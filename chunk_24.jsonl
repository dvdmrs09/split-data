{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_scaling_function(y_m, y_M, x_m, x_M):\n    if x_M <= x_m:\n        return lambda _: y_m\n    k = (y_M - y_m) / (x_M - x_m)\n    b = y_m - k * x_m\n    return lambda _: int(k * _ + b)", "response": "Returns a linear function y = k x + b where y = y_m x = x_M"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_as_cytoscape_html(tree, out_html, column2states, layout='dagre', name_feature='name',\n                           name2colour=None, n2tooltip=None, years=None, is_compressed=True, n2date=None,\n                           date_column='Dist. to root'):\n    \"\"\"\n    Converts a tree to an html representation using Cytoscape.js.\n\n    If categories are specified they are visualised as pie-charts inside the nodes,\n    given that each node contains features corresponding to these categories with values being the percentage.\n    For instance, given categories ['A', 'B', 'C'], a node with features {'A': 50, 'B': 50}\n    will have a half-half pie-chart (half-colored in a colour of A, and half B).\n\n    If dist_step is specified, the edges are rescaled accordingly to their dist (node.dist / dist_step),\n    otherwise all edges are drawn of the same length.\n\n    otherwise all edges are drawn of the same length.\n    :param name_feature: str, a node feature whose value will be used as a label\n    returns a key to be used for sorting nodes on the same level in the tree.\n    :param n2tooltip: dict, TreeNode to str mapping tree nodes to tooltips.\n    :param layout: str, name of the layout for Cytoscape.js\n    :param name2colour: dict, str to str, category name to HEX colour mapping \n    :param categories: a list of categories for the pie-charts inside the nodes\n    :param tree: ete3.Tree\n    :param out_html: path where to save the resulting html file.\n    \"\"\"\n    graph_name = os.path.splitext(os.path.basename(out_html))[0]\n\n    json_dict, clazzes \\\n        = _tree2json(tree, column2states, name_feature=name_feature,\n                     node2tooltip=n2tooltip, years=years, is_compressed=is_compressed, n2date=n2date)\n    env = Environment(loader=PackageLoader('pastml'))\n    template = env.get_template('pie_tree.js') if is_compressed else env.get_template('pie_tree_simple.js')\n\n    clazz2css = {}\n    for clazz_list in clazzes:\n        n = len(clazz_list)\n        css = ''\n        for i, cat in enumerate(clazz_list, start=1):\n            css += \"\"\"\n                'pie-{i}-background-color': \"{colour}\",\n                'pie-{i}-background-size': '{percent}\\%',\n            \"\"\".format(i=i, percent=round(100 / n, 2), colour=name2colour[cat])\n        clazz2css[_clazz_list2css_class(clazz_list)] = css\n    graph = template.render(clazz2css=clazz2css.items(), elements=json_dict, layout=layout, title=graph_name,\n                            years=['{:g}'.format(_) for _ in years])\n    slider = env.get_template('time_slider.html').render(min_date=n2date[tree.name], max_date=len(years) - 1,\n                                                         name=date_column) \\\n        if len(years) > 1 else ''\n\n    template = env.get_template('index.html')\n    page = template.render(graph=graph, title=graph_name, slider=slider)\n\n    os.makedirs(os.path.abspath(os.path.dirname(out_html)), exist_ok=True)\n    with open(out_html, 'w+') as fp:\n        fp.write(page)", "response": "Converts a tree to an html representation using Cytoscape. js."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the jsg in infilename and save the results in outfilename", "response": "def do_parse(infilename: str, jsonfilename: Optional[str], rdffilename: Optional[str], rdffmt: str,\n             context: Optional[str] = None) -> bool:\n    \"\"\"\n    Parse the jsg in infilename and save the results in outfilename\n    :param infilename: name of the file containing the ShExC\n    :param jsonfilename: target ShExJ equivalent\n    :param rdffilename: target ShExR equivalent\n    :param rdffmt: target RDF format\n    :param context: @context to use for rdf generation. If None use what is in the file\n    :return: true if success\n    \"\"\"\n    shexj = parse(FileStream(infilename, encoding=\"utf-8\"))\n    if shexj is not None:\n        shexj['@context'] = context if context else \"http://www.w3.org/ns/shex.jsonld\"\n        if jsonfilename:\n            with open(jsonfilename, 'w') as outfile:\n                outfile.write(shexj._as_json_dumps())\n        if rdffilename:\n            g = Graph().parse(data=shexj._as_json, format=\"json-ld\")\n            g.serialize(open(rdffilename, \"wb\"), format=rdffmt)\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(input_: Union[str, FileStream], default_base: Optional[str]=None) -> Optional[Schema]:\n\n    # Step 1: Tokenize the input stream\n    error_listener = ParseErrorListener()\n    if not isinstance(input_, FileStream):\n        input_ = InputStream(input_)\n    lexer = ShExDocLexer(input_)\n    lexer.addErrorListener(error_listener)\n    tokens = CommonTokenStream(lexer)\n    tokens.fill()\n    if error_listener.n_errors:         # Lexer prints errors directly\n        return None\n\n    # Step 2: Generate the parse tree\n    parser = ShExDocParser(tokens)\n    parser.addErrorListener(error_listener)\n    parse_tree = parser.shExDoc()\n    if error_listener.n_errors:\n        print('\\n'.join(error_listener.errors), file=sys.stderr)\n        return None\n\n    # Step 3: Transform the results the results\n    parser = ShexDocParser(default_base=default_base)\n    parser.visit(parse_tree)\n\n    return parser.context.schema", "response": "Parse the input text and return the resulting schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rdf_suffix(fmt: str) -> str:\n    for k, v in SUFFIX_FORMAT_MAP.items():\n        if fmt == v:\n            return k\n    return 'rdf'", "response": "Map the RDF format to the approproate suffix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef genargs() -> ArgumentParser:\n    parser = ArgumentParser()\n    parser.add_argument(\"infile\", help=\"Input ShExC specification\")\n    parser.add_argument(\"-nj\", \"--nojson\", help=\"Do not produce json output\", action=\"store_true\")\n    parser.add_argument(\"-nr\", \"--nordf\", help=\"Do not produce rdf output\", action=\"store_true\")\n    parser.add_argument(\"-j\", \"--jsonfile\", help=\"Output ShExJ file (Default: {infile}.json)\")\n    parser.add_argument(\"-r\", \"--rdffile\", help=\"Output ShExR file (Default: {infile}.{fmt suffix})\")\n    parser.add_argument(\"--context\", help=\"Alternative @context\")\n    parser.add_argument(\"-f\", \"--format\",\n                        choices=list(set(x.name for x in rdflib_plugins(None, rdflib_Serializer)\n                                         if '/' not in str(x.name))),\n                        help=\"Output format (Default: turtle)\", default=\"turtle\")\n    return parser", "response": "Create a command line parser for the\n    command line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntransforming ShExC to ShExJ", "response": "def generate(argv: List[str]) -> bool:\n    \"\"\" \n    Transform ShExC to ShExJ\n    :param argv: Command line arguments\n    :return: True if successful\n    \"\"\"\n    opts = genargs().parse_args(argv)\n    filebase = os.path.dirname(opts.infile) + str(os.path.basename(opts.infile).rsplit('.', 1)[0])\n    if opts.nojson:\n        opts.jsonfile = None\n    elif not opts.jsonfile:\n        opts.jsonfile = filebase + \".json\"\n    if opts.nordf:\n        opts.rdffile = None\n    elif not opts.rdffile:\n        opts.rdffile = filebase + \".\" + rdf_suffix(opts.format)\n    if do_parse(opts.infile, opts.jsonfile, opts.rdffile, opts.format, opts.context):\n        if opts.jsonfile:\n            print(\"JSON output written to {}\".format(opts.jsonfile))\n        if opts.rdffile:\n            print(\"{} output written to {}\".format(opts.format, opts.rdffile))\n        return True\n    else:\n        print(\"Conversion failed\")\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_bert(data, electrodes, filename):\n    # Check for multiple timesteps\n    if has_multiple_timesteps(data):\n        for i, timestep in enumerate(split_timesteps(data)):\n            export_bert(timestep, electrodes,\n                        filename.replace(\".\", \"_%.3d.\" % i))\n\n        # TODO: Make ABMN consistent\n        # index_full = ert.data.groupby(list(\"abmn\")).groups.keys()\n        # g = ert.data.groupby('timestep')\n        # q = ert.data.pivot_table(values='r', index=list(\"abmn\"), columns=\"timestep\", dropna=True)\n        # ert.data.reset_index(list(\"abmn\"))\n\n    f = open(filename, 'w')\n    f.write(\"%d\\n\" % len(electrodes))\n    f.write(\"# \")\n\n    # Make temporary copies for renaming\n    electrodes = electrodes.copy()\n    data = data.copy()\n\n    electrodes.columns = electrodes.columns.str.lower()\n    data.columns = data.columns.str.lower()\n\n    # Remove unnecessary columns and rename according to bert conventions\n    # https://gitlab.com/resistivity-net/bert#the-unified-data-format\n    cols_to_export = [\"a\", \"b\", \"m\", \"n\", \"u\", \"i\", \"r\", \"rho_a\", \"error\"]\n    data.drop(data.columns.difference(cols_to_export), 1, inplace=True)\n    data.rename(columns={\"rho_a\": \"rhoa\", \"error\": \"err\"}, inplace=True)\n\n    for key in electrodes.keys():\n        f.write(\"%s \" % key)\n    f.write(\"\\n\")\n    for row in electrodes.itertuples(index=False):\n        for val in row:\n            f.write(\"%5.3f \" % val)\n        f.write(\"\\n\")\n    f.write(\"%d\\n\" % len(data))\n    f.write(\"# \")\n\n    # Make sure that a, b, m, n are the first 4 columns\n    columns = data.columns.tolist()\n    for c in \"abmn\":\n        columns.remove(c)\n    columns = list(\"abmn\") + columns\n    data = data[columns]\n\n    for key in data.keys():\n        f.write(\"%s \" % key)\n    f.write(\"\\n\")\n    for row in data.itertuples(index=False):\n        for i, val in enumerate(row):\n            if i < 4:\n                f.write(\"%d \" % val)\n            else:\n                f.write(\"%E \" % val)\n\n        f.write(\"\\n\")\n    f.close()", "response": "Export to unified data format used in pyGIMLi & BERT."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visitShapeOr(self, ctx: ShExDocParser.ShapeOrContext):\n        if len(ctx.shapeAnd()) > 1:\n            self.expr = ShapeOr(id=self.label, shapeExprs=[])\n            for sa in ctx.shapeAnd():\n                sep = ShexShapeExpressionParser(self.context)\n                sep.visit(sa)\n                self.expr.shapeExprs.append(sep.expr)\n        else:\n            self.visit(ctx.shapeAnd(0))", "response": "shapeOr: shapeAnd (KW_OR shapeAnd)*"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visitInlineShapeOr(self, ctx: ShExDocParser.InlineShapeOrContext):\n        if len(ctx.inlineShapeAnd()) > 1:\n            self.expr = ShapeOr(id=self.label, shapeExprs=[])\n            for sa in ctx.inlineShapeAnd():\n                sep = ShexShapeExpressionParser(self.context)\n                sep.visit(sa)\n                self.expr.shapeExprs.append(sep.expr)\n        else:\n            self.visit(ctx.inlineShapeAnd(0))", "response": "inlineShapeAnd is a list of keywords? | |"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _and_collapser(container: ShapeAnd, containee: shapeExpr) -> None:\n        if isinstance(containee, ShapeAnd):\n            for expr in containee.shapeExprs:\n                container.shapeExprs.append(expr)\n        else:\n            container.shapeExprs.append(containee)", "response": "Collapser the containee with the container."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visitShapeNot(self, ctx: ShExDocParser.ShapeNotContext):\n        if ctx.negation():\n            self.expr = ShapeNot(id=self.label)\n            sn = ShexShapeExpressionParser(self.context)\n            sn.visit(ctx.shapeAtom())\n            self.expr.shapeExpr = sn.expr if sn.expr is not None else Shape()\n        else:\n            self.visitChildren(ctx)", "response": "shapeNot is a helper method that creates a shapeNot expression if it is not a negation shapeAtom"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visitInlineShapeNot(self, ctx: ShExDocParser.InlineShapeNotContext):\n        if ctx.negation():\n            self.expr = ShapeNot(id=self.label)\n            sn = ShexShapeExpressionParser(self.context)\n            ctx.inlineShapeAtom()\n            sn.visit(ctx.inlineShapeAtom())\n            self.expr.shapeExpr = sn.expr if sn.expr is not None else Shape()\n        else:\n            self.visitChildren(ctx)", "response": "inlineShapeNot is a helper method to create a shapeNot expression"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visitInlineShapeOrRef(self, ctx: ShExDocParser.InlineShapeOrRefContext):\n        if ctx.inlineShapeDefinition():\n            from pyshexc.parser_impl.shex_shape_definition_parser import ShexShapeDefinitionParser\n            shdef_parser = ShexShapeDefinitionParser(self.context, self.label)\n            shdef_parser.visitChildren(ctx)\n            self.expr = shdef_parser.shape\n        else:\n            self.expr = self.context.shapeRef_to_iriref(ctx.shapeRef())", "response": "inlineShapeOrRef is a special case for inlineShapeDefinition | shapeRef"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreset the points for the specified index position.", "response": "def reset(self, index=None):\n        '''\n        Reset the points for the specified index position.  If no index is\n        specified, reset points for all point handlers.\n        '''\n        points_handler_count = len(self.registration_view.points)\n        if index is None:\n            indexes = range(points_handler_count)\n        else:\n            indexes = [index]\n\n        indexes = [i for i in indexes if i < points_handler_count]\n\n        for i in indexes:\n            self.registration_view.points[i].reset()\n        if indexes:\n            self.registration_view.update_transform()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_file(filename):\n    # read data\n    with open(filename, 'r') as fid2:\n        abem_data_orig = fid2.read()\n\n    fid = StringIO()\n    fid.write(abem_data_orig)\n    fid.seek(0)\n\n    # determine type of array\n    fid.readline()\n    fid.readline()\n    file_type = int(fid.readline().strip())\n\n    # reset file pointer\n    fid.seek(0)\n    return file_type, fid", "response": "Read a res2dinv - file and return the header and the file object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_general_type(content, settings):\n    header_raw = []\n    index = 0\n    while index < 9:\n        line = content.readline()\n        # filter comments\n        if line.startswith('//'):\n            continue\n        else:\n            header_raw.append(line.strip())\n            index += 1\n    # parse header\n    header = {\n        'name': header_raw[0],\n        # unit is meters?\n        'unit_spacing': float(header_raw[1]),\n        'type': int(header_raw[2]),\n        'type2': int(header_raw[3]),\n        'type_of_measurements': int(header_raw[5]),\n        'nr_measurements': int(header_raw[6]),\n        'type_of_x_location': int(header_raw[7]),\n        'ip_data': int(header_raw[8]),\n    }\n\n    if header['type_of_measurements'] == 0:\n        raise Exception('Reading in app. resistivity not supported yet')\n\n    df = pd.read_csv(\n        content,\n        delim_whitespace=True,\n        header=None,\n        names=(\n            'nr_elecs',\n            'x1',\n            'z1',\n            'x2',\n            'z2',\n            'x3',\n            'z3',\n            'x4',\n            'z4',\n            'value',\n        ),\n    )\n\n    # print('xxx', df.ix[10, ['x1', 'x2', 'x3', 'x4']])\n    # for now ignore the z coordinates and compute simple electrode denotations\n    df['a'] = df['x1'] / header['unit_spacing'] + 1\n    df['b'] = df['x2'] / header['unit_spacing'] + 1\n    df['m'] = df['x3'] / header['unit_spacing'] + 1\n    df['n'] = df['x4'] / header['unit_spacing'] + 1\n    # print('abmn', df.ix[10, ['a', 'b', 'm', 'n']])\n\n    # for now assume value in resistances\n    df['r'] = df['value']\n\n    # remove any nan values\n    df.dropna(axis=0, subset=['a', 'b', 'm', 'n', 'r'], inplace=True)\n\n    # ABMN are integers\n    df['a'] = df['a'].astype(int)\n    df['b'] = df['b'].astype(int)\n    df['m'] = df['m'].astype(int)\n    df['n'] = df['n'].astype(int)\n\n    # drop unused columns\n    df.drop(\n        [\n            'nr_elecs',\n            'x1', 'z1',\n            'x2', 'z2',\n            'x3', 'z3',\n            'x4', 'z4',\n            'value',\n        ], axis=1, inplace=True\n    )\n    return header, df", "response": "Read a general RES2DINV data block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a RES2DINV - style file produced by the ABEM export program.", "response": "def add_dat_file(filename, settings, container=None, **kwargs):\n    \"\"\" Read a RES2DINV-style file produced by the ABEM export program.\n    \"\"\"\n    # each type is read by a different function\n    importers = {\n        # general array type\n        11: _read_general_type,\n    }\n\n    file_type, content = _read_file(filename)\n\n    if file_type not in importers:\n        raise Exception(\n            'type of RES2DINV data file not recognized: {0}'.format(file_type)\n        )\n\n    header, data = importers[file_type](content, settings)\n\n    timestep = settings.get('timestep', 0)\n\n    # add timestep column\n    data['timestep'] = timestep\n\n    if container is None:\n        container = ERT(data)\n    else:\n        container.data = pd.concat((container.data, data))\n\n    return container"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef console_input(default, validation=None, allow_empty=False):\n    value = raw_input(\"> \") or default\n    if value == \"\" and not allow_empty:\n        print \"Invalid: Empty value is not permitted.\"\n        return console_input(default, validation)\n    if validation:\n        try:\n            return validation(value)\n        except ValidationError, e:\n            print \"Invalid: \", e\n            return console_input(default, validation)\n    return value", "response": "Get user input value from stdin."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute weC from weT aeT", "response": "def correct(self, calib, temp, we_t, ae_t):\n        \"\"\"\n        Compute weC from weT, aeT\n        \"\"\"\n        if not A4TempComp.in_range(temp):\n            return None\n\n        if self.__algorithm == 1:\n            return self.__eq1(temp, we_t, ae_t)\n\n        if self.__algorithm == 2:\n            return self.__eq2(temp, we_t, ae_t, calib.we_cal_mv, calib.ae_cal_mv)\n\n        if self.__algorithm == 3:\n            return self.__eq3(temp, we_t, ae_t, calib.we_cal_mv, calib.ae_cal_mv)\n\n        if self.__algorithm == 4:\n            return self.__eq4(temp, we_t, calib.we_cal_mv)\n\n        raise ValueError(\"A4TempComp.conv: unrecognised algorithm: %d.\" % self.__algorithm)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cf_t(self, temp):\n        index = int((temp - A4TempComp.__MIN_TEMP) // A4TempComp.__INTERVAL)        # index of start of interval\n\n        # on boundary...\n        if temp % A4TempComp.__INTERVAL == 0:\n            return self.__values[index]\n\n        # all others...\n        y1 = self.__values[index]                                                   # y value at start of interval\n        y2 = self.__values[index + 1]                                               # y value at end of interval\n\n        delta_y = y2 - y1\n\n        delta_x = float(temp % A4TempComp.__INTERVAL) / A4TempComp.__INTERVAL       # proportion of interval\n\n        cf_t = y1 + (delta_y * delta_x)\n\n        # print(\"A4TempComp.cf_t: alg:%d, temp:%f y1:%f y2:%f delta_y:%f delta_x:%f cf_t:%f \" %\n        #       (self.__algorithm, temp, y1, y2, delta_y, delta_x, cf_t), file=sys.stderr)\n\n        return cf_t", "response": "Compute the linear - interpolated temperature compensation factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new client for the HNV REST API.", "response": "def get_client(url, username, password, allow_insecure, ca_bundle):\n    \"\"\"Create a new client for the HNV REST API.\"\"\"\n    return _HNVClient(url, username, password, allow_insecure, ca_bundle)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _session(self):\n        if self._http_session is None:\n            self._http_session = requests.Session()\n            self._http_session.headers.update(self._get_headers())\n            self._http_session.verify = self._verify_https_request()\n\n            if all(self._credentials):\n                username, password = self._credentials\n                self._http_session.auth = requests_ntlm.HttpNtlmAuth(\n                    username=username, password=password)\n\n        return self._http_session", "response": "Returns a new requests. Session object that can be used to access the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_resource(self, path):\n        response = self._http_request(path)\n        try:\n            return response.json()\n        except ValueError:\n            raise exception.ServiceException(\"Invalid service response.\")", "response": "Gets the required information from the API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_resource(self, path, data, if_match=None):\n        response = self._http_request(resource=path, method=\"PUT\", body=data,\n                                      if_match=if_match)\n        try:\n            return response.json()\n        except ValueError:\n            raise exception.ServiceException(\"Invalid service response.\")", "response": "Update the required resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summarize(self):\n\n        s = str(self.allval())\n\n        return self.parse(s[:2]+ ''.join(['Z']*len(s[2:])))", "response": "Convert all of the values to their max values. This form is used to represent the summary level"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters Schlumberger configurations based on the given configurations.", "response": "def _filter_schlumberger(configs):\n    \"\"\"Filter Schlumberger configurations\n\n    Schlumberger configurations are selected using the following criteria:\n\n    * For a given voltage dipole, there need to be at least two current\n      injections with electrodes located on the left and the right of the\n      voltage dipole.\n    * The distance between the current electrodes and the next voltage\n      electrode is the same on both sides.\n\n    Parameters\n    ----------\n    configs: numpy.ndarray\n        Nx4 array with N measurement configurations\n\n    Returns\n    -------\n    configs: numpy.ndarray\n        Remaining configurations, all Schlumberger configurations are set to\n        numpy.nan\n    schl_indices: dict with one entry: numpy.ndarray\n        indices of Schlumberger configurations\n    \"\"\"\n    # sort configs\n    configs_sorted = np.hstack((\n        np.sort(configs[:, 0:2], axis=1),\n        np.sort(configs[:, 2:4], axis=1),\n    )).astype(int)\n\n    # determine unique voltage dipoles\n    MN = configs_sorted[:, 2:4].copy()\n    MN_unique = np.unique(\n        MN.view(\n            MN.dtype.descr * 2\n        )\n    )\n    MN_unique_reshape = MN_unique.view(\n        MN.dtype\n    ).reshape(-1, 2)\n\n    schl_indices_list = []\n    for mn in MN_unique_reshape:\n        # check if there are more than one associated current injections\n        nr_current_binary = (\n            (configs_sorted[:, 2] == mn[0]) &\n            (configs_sorted[:, 3] == mn[1])\n        )\n        if len(np.where(nr_current_binary)[0]) < 2:\n            continue\n\n        # now which of these configurations have current electrodes on both\n        # sides of the voltage dipole\n        nr_left_right = (\n            (configs_sorted[:, 0] < mn[0]) &\n            (configs_sorted[:, 1] > mn[0]) &\n            nr_current_binary\n        )\n\n        # now check that the left/right distances are equal\n        distance_left = np.abs(\n            configs_sorted[nr_left_right, 0] - mn[0]\n        ).squeeze()\n        distance_right = np.abs(\n            configs_sorted[nr_left_right, 1] - mn[1]\n        ).squeeze()\n\n        nr_equal_distances = np.where(distance_left == distance_right)[0]\n\n        indices = np.where(nr_left_right)[0][nr_equal_distances]\n\n        if indices.size > 2:\n            schl_indices_list.append(indices)\n\n    # set Schlumberger configs to nan\n    if len(schl_indices_list) == 0:\n        return configs, {0: np.array([])}\n    else:\n        schl_indices = np.hstack(schl_indices_list).squeeze()\n        configs[schl_indices, :] = np.nan\n        return configs, {0: schl_indices}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfilter dipole - dipole configurations based on the given configurations.", "response": "def _filter_dipole_dipole(configs):\n    \"\"\"Filter dipole-dipole configurations\n\n    A dipole-dipole configuration is defined using the following criteria:\n\n        * equal distance between the two current electrodes and between the two\n          voltage electrodes\n        * no overlap of dipoles\n\n    Parameters\n    ----------\n    configs: numpy.ndarray\n        Nx4 array with N measurement configurations\n\n    Returns\n    -------\n    configs: numpy.ndarray\n        Remaining configurations, all dipole-dipole configurations are set to\n        numpy.nan\n    dd_indices: numpy.ndarray\n        indices of dipole-dipole configurations\n\n    \"\"\"\n    # check that dipoles have equal size\n    dist_ab = np.abs(configs[:, 0] - configs[:, 1])\n    dist_mn = np.abs(configs[:, 2] - configs[:, 3])\n\n    distances_equal = (dist_ab == dist_mn)\n\n    # check that they are not overlapping\n    not_overlapping = (\n        # either a,b < m,n\n        (\n            (configs[:, 0] < configs[:, 2]) &\n            (configs[:, 1] < configs[:, 2]) &\n            (configs[:, 0] < configs[:, 3]) &\n            (configs[:, 1] < configs[:, 3])\n        ) |\n        # or m,n < a,b\n        (\n            (configs[:, 2] < configs[:, 0]) &\n            (configs[:, 3] < configs[:, 0]) &\n            (configs[:, 2] < configs[:, 1]) &\n            (configs[:, 3] < configs[:, 1])\n        )\n    )\n\n    is_dipole_dipole = (distances_equal & not_overlapping)\n\n    dd_indices = np.where(is_dipole_dipole)[0]\n    dd_indices_sorted = _sort_dd_skips(configs[dd_indices, :], dd_indices)\n\n    # set all dd configs to nan\n    configs[dd_indices, :] = np.nan\n\n    return configs, dd_indices_sorted"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sort_dd_skips(configs, dd_indices_all):\n    config_current_skips = np.abs(configs[:, 1] - configs[:, 0])\n    if np.all(np.isnan(config_current_skips)):\n        return {0: []}\n\n    # determine skips\n    available_skips_raw = np.unique(config_current_skips)\n    available_skips = available_skips_raw[\n        ~np.isnan(available_skips_raw)\n    ].astype(int)\n\n    # now determine the configurations\n    dd_configs_sorted = {}\n    for skip in available_skips:\n        indices = np.where(config_current_skips == skip)[0]\n        dd_configs_sorted[skip - 1] = dd_indices_all[indices]\n\n    return dd_configs_sorted", "response": "Given a set of dipole - dipole configurations sort them according to\n    their current skip."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter(configs, settings):\n    if isinstance(configs, pd.DataFrame):\n        configs = configs[['a', 'b', 'm', 'n']].values\n\n    # assign short labels to Python functions\n    filter_funcs = {\n        'dd': _filter_dipole_dipole,\n        'schlumberger': _filter_schlumberger,\n    }\n\n    # we need a list to fix the call order of filter functions\n    keys = ['dd', 'schlumberger', ]\n\n    allowed_keys = settings.get('only_types', filter_funcs.keys())\n\n    results = {}\n    # we operate iteratively on the configs, set the first round here\n    # rows are iteratively set to nan when filters remove them!\n    configs_filtered = configs.copy().astype(float)\n\n    for key in keys:\n        if key in allowed_keys:\n            configs_filtered, indices_filtered = filter_funcs[key](\n                configs_filtered,\n            )\n            if len(indices_filtered) > 0:\n                results[key] = indices_filtered\n\n    # add all remaining indices to the results dict\n    results['not_sorted'] = np.where(\n        ~np.all(np.isnan(configs_filtered), axis=1)\n    )[0]\n    return results", "response": "Filter the configuration types for all registered A - B - M - N configurations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_prologs(filename):\n    results = {}\n    prolog = {}\n    heading_re = re.compile(r\"^\\*  ([A-Z].*):$\")\n    heading = \"\"\n    content = \"\"\n    counter = 0\n\n    for line in open(filename):\n        line = line.strip()\n\n        # Start of a completely new prolog so reset everything\n        if line.startswith(\"*+\"):\n            if counter != 0:\n                raise ValueError(\"Started prologue without closing previous prologue\")\n            prolog = {}\n            heading = \"\"\n            content = \"\"\n            counter = counter + 1\n            continue\n\n        # End of a prolog. Must store the current dict\n        if line.startswith(\"*-\"):\n            counter = 0\n            if len(heading):\n                # Flush current heading\n                prolog[heading] = content\n                content = \"\"\n            name = prolog['name'].strip()\n            results[name] = prolog\n            prolog = None\n            continue\n\n        # If we are not in a prologue then nothing further is needed\n        if counter == 0:\n            continue\n\n        counter = counter + 1\n\n        # Completely blank lines are ignored\n        if len(line) == 0:\n            continue\n\n        # Look for a new section heading\n        match_head = heading_re.search(line)\n        if match_head is not None:\n            if len(heading):\n                # Flush previous heading\n                prolog[heading] = content\n            heading = match_head.group(1).lower()\n            content = \"\"\n            continue\n\n        if line.startswith(\"*     \"):\n            content = content + line[6:] + \"\\n\"\n            continue\n        elif line == \"*\":\n            content = content + \"\\n\"\n            continue\n\n        if counter:\n            raise ValueError(\"Error parsing SST prologue line \"+str(counter)+\":'\" + line + \"'\")\n    return results", "response": "Given a filename search for SST prologues\n    and returns a dict where the keys are the name of the SST prolog and the keys are the content of the SST prologues containing the SST labels."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving a block of data to a CRTomo - compatible. crt file.", "response": "def save_block_to_crt(filename, group, norrec='all', store_errors=False):\n    \"\"\"Save a dataset to a CRTomo-compatible .crt file\n\n    Parameters\n    ----------\n    filename : string\n        Output filename\n    group : pandas.group\n        Data group\n    norrec : string\n        Which data to export Possible values: all|nor|rec\n    store_errors : bool\n        If true, store errors of the data in a separate column\n\n    \"\"\"\n    if norrec != 'all':\n        group = group.query('norrec == \"{0}\"'.format(norrec))\n\n    # todo: we need to fix the global naming scheme for columns!\n    with open(filename, 'wb') as fid:\n        fid.write(\n            bytes('{0}\\n'.format(len(group)), 'UTF-8')\n        )\n\n        AB = group['a'] * 1e4 + group['b']\n        MN = group['m'] * 1e4 + group['n']\n        line = [\n            AB.values.astype(int),\n            MN.values.astype(int),\n            group['r'].values,\n        ]\n\n        if 'rpha' in group:\n            line.append(group['rpha'].values)\n        else:\n            line.append(group['r'].values * 0.0)\n\n        fmt = '%i %i %f %f'\n        if store_errors:\n            line += (\n                group['d|Z|_[Ohm]'].values,\n                group['dphi_[mrad]'].values,\n            )\n            fmt += ' %f %f'\n\n        subdata = np.array(line).T\n        np.savetxt(fid, subdata, fmt=fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite sEIT data ta files to a directory.", "response": "def write_files_to_directory(df, directory, **kwargs):\n    \"\"\"Write sEIT data ta files. Data of each frequency is written in a\n    separate file that conforms to the CRMod/CRTomo standard, and can directly\n    be used for inversions using CRTomo.\n\n    Parameters\n    ----------\n    df : pandas.DataFrame\n        Data\n    directory : string\n        Output directory. Will be created if not existant\n\n    Other Parameters\n    ----------------\n    store_errors: bool, optional\n        store the device generated errors in the output files (as additional\n        columns). Default: False\n    norrec: string, optional\n        all|nor|rec which normal-reciprocal data set to use. Default: all\n\n    \"\"\"\n    if 'frequency' in df.columns:\n        group_key = 'frequency'\n    elif 'frequency_[Hz]' in df.columns:\n        group_key = 'frequency_[Hz]'\n    else:\n        group_key = None\n\n    if not os.path.isdir(directory):\n        os.makedirs(directory)\n\n    pwd = os.getcwd()\n    os.chdir(directory)\n\n    if group_key is not None:\n        g = df.groupby(group_key)\n\n        nr = 1\n        frequencies_used = []\n        for frequency, group in sorted(g):\n            if group.shape[0] > 0:\n                frequencies_used.append(frequency)\n            filename = 'volt_{0:02}_{1:.6}Hz.crt'.format(nr, frequency)\n            save_block_to_crt(\n                filename,\n                group,\n                norrec=kwargs.get('norrec', 'all'),\n                store_errors=kwargs.get('store_errors', False),\n            )\n\n            nr += 1\n        np.savetxt('frequencies.dat', frequencies_used)\n    else:\n        save_block_to_crt(\n            'volt.dat',\n            df,\n            norrec=kwargs.get('norrec', 'all'),\n            store_errors=kwargs.get('store_errors', False),\n        )\n\n    os.chdir(pwd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_label(parameter, ptype, flavor=None, mpl=None):\n    # determine flavor\n    if flavor is not None:\n        if flavor not in ('latex', 'mathml'):\n            raise Exception('flavor not recognized: {}'.format(flavor))\n    else:\n        if mpl is None:\n            raise Exception('either the flavor or mpl must be provided')\n        rendering = mpl.rcParams['text.usetex']\n        if rendering:\n            flavor = 'latex'\n        else:\n            flavor = 'mathml'\n\n    # check if the requested label is present\n    if parameter not in labels:\n        raise Exception('parameter not known')\n    if ptype not in labels[parameter]:\n        raise Exception('ptype not known')\n    if flavor not in labels[parameter][ptype]:\n        raise Exception('flavor not known')\n\n    return labels[parameter][ptype][flavor]", "response": "Returns the label of a given parameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_labels(self, axes, dtype):\n        for ax in axes[1, :].flat:\n            ax.set_xlabel('frequency [Hz]')\n\n        if dtype == 'rho':\n            axes[0, 0].set_ylabel(r'$|\\rho| [\\Omega m]$')\n            axes[0, 1].set_ylabel(r'$-\\phi [mrad]$')\n            axes[1, 0].set_ylabel(r\"$\\sigma' [S/m]$\")\n            axes[1, 1].set_ylabel(r\"$\\sigma'' [S/m]$\")\n        elif dtype == 'r':\n            axes[0, 0].set_ylabel(r'$|R| [\\Omega]$')\n            axes[0, 1].set_ylabel(r'$-\\phi [mrad]$')\n            axes[1, 0].set_ylabel(r\"$Y' [S]$\")\n            axes[1, 1].set_ylabel(r\"$Y'' [S]$\")\n        else:\n            raise Exception('dtype not known: {}'.format(dtype))", "response": "Adds x and y labels to the axes that are used to plot the logarithmic logarithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _plot(self, title=None, reciprocal=None, limits=None, dtype='rho',\n              **kwargs):\n        \"\"\"Standard plot of spectrum\n\n        Parameters\n        ----------\n        title: string|None, optional\n            Title of plot\n        reciprocal: sip_response object|None, optional\n            If provided, plot this spectrum with another color\n        limits: dict|None, optional\n            used to set ylimits of the plots. Possible entries: rmag_min,\n            rmag_max, rpha_min, rpha_max, cre_min, cre_max, cim_min, cim_max\n        dtype: string, optional\n            Possible values: [rho|R]. Determines the label types. 'rho':\n                resistivity/conductivity, 'r': resistance/conductance\n        label_nor:\n            label for normal data (default: \"normal\")\n        label_rec:\n            label for reciprocal data (default: \"reciprocal\")\n\n        Returns\n        -------\n        fig: figure object\n            the generated matplotlib figure\n        axes: list\n            matplotlib axes objects\n        \"\"\"\n        if limits is None:\n            limits = {}\n\n        fig, axes = plt.subplots(\n            2, 2, figsize=(15 / 2.54, 6 / 2.54), sharex=True\n        )\n        if title is not None:\n            fig.suptitle(title)\n\n        # resistivity magnitude\n        if limits is None:\n            limits = {}\n\n        ax = axes[0, 0]\n        ax.semilogx(\n            self.frequencies, self.rmag, '.-', color='k',\n            label=kwargs.get('label_nor', 'normal'),\n        )\n        ax.set_ylim(\n            limits.get('rmag_min', None),\n            limits.get('rmag_max', None)\n        )\n\n        # resistivity phase\n        ax = axes[0, 1]\n        ax.semilogx(self.frequencies, -self.rpha, '.-', color='k')\n        # note the switch of _min/_max because we change the sign while\n        # plotting\n        ymin = limits.get('rpha_max', None)\n        if ymin is not None:\n            ymin *= -1\n        ymax = limits.get('rpha_min', None)\n        if ymax is not None:\n            ymax *= -1\n        ax.set_ylim(\n            ymin,\n            ymax,\n        )\n\n        # conductivity real part\n        ax = axes[1, 0]\n        ax.loglog(self.frequencies, self.cre, '.-', color='k')\n        ax.set_ylim(\n            limits.get('cre_min', None),\n            limits.get('cre_max', None)\n        )\n\n        # conductivity imaginary part\n        ax = axes[1, 1]\n        ax.loglog(self.frequencies, self.cim, '.-', color='k')\n        ax.set_ylim(\n            limits.get('cim_min', None),\n            limits.get('cim_max', None)\n        )\n\n        self._add_labels(axes, dtype)\n\n        for ax in axes.flatten()[0:2]:\n            ax.xaxis.set_major_locator(mpl.ticker.LogLocator(numticks=5))\n            ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(5))\n\n        for ax in axes.flatten()[2:]:\n            ax.xaxis.set_major_locator(mpl.ticker.LogLocator(numticks=5))\n            ax.yaxis.set_major_locator(mpl.ticker.LogLocator(numticks=5))\n\n        fig.tight_layout()\n        # plot reciprocal spectrum\n        if reciprocal is not None:\n            axes[0, 0].semilogx(\n                reciprocal.frequencies,\n                reciprocal.rmag,\n                '.-',\n                color='k',\n                linestyle='dashed',\n                label=kwargs.get('label_rec', 'reciprocal'),\n            )\n            axes[0, 1].semilogx(\n                reciprocal.frequencies,\n                -reciprocal.rpha,\n                '.-',\n                color='k',\n                linestyle='dashed',\n            )\n            axes[1, 0].loglog(\n                reciprocal.frequencies,\n                reciprocal.cre,\n                '.-',\n                color='k',\n                linestyle='dashed',\n            )\n            axes[1, 1].loglog(\n                reciprocal.frequencies,\n                reciprocal.cim,\n                '.-',\n                color='k',\n                linestyle='dashed',\n            )\n\n            fig.subplots_adjust(\n                bottom=0.3,\n            )\n\n            axes[0, 0].legend(\n                loc=\"lower center\",\n                ncol=4,\n                bbox_to_anchor=(0, 0, 1, 1),\n                bbox_transform=fig.transFigure,\n                fontsize=7.0,\n            )\n\n        return fig, axes", "response": "General function to plot the object of the specific type of object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot the current state of the object in the file.", "response": "def plot(self, filename, title=None, reciprocal=None, limits=None,\n             dtype='rho', return_fig=False, **kwargs):\n        \"\"\"Standard plot of spectrum\n\n        Parameters\n        ----------\n        filename: string\n            Output filename. Include the ending to specify the filetype\n            (usually .pdf or .png)\n        title: string, optional\n            Title for the plot\n        reciprocal: :class:`reda.eis.plots.sip_response`, optional\n            If another :class:`reda.eis.plots.sip_response` object is provided\n            here, use this as the reciprocal spectrum.\n        limits: dict, optional\n            A dictionary which contains plot limits. See code example below.\n        dtype: string, optional\n            Determines if the data plotted included geometric factors ('rho')\n            or not ('r'). Default: 'rho'\n        return_fig: bool, optional\n            If True, then do not delete the figure object after saving to file\n            and return the figure object. Default: False\n        **kwargs: dict\n            kwargs is piped through to the _plot function\n\n        Returns\n        -------\n        fig: :class:`matplotlib.Figure`\n            The figure object. Only returned if return_fig is set to True\n\n        Examples\n        --------\n        >>> from reda.eis.plots import sip_response\n        >>> import numpy as np\n        >>> frequencies = np.array([\n        ...     1.00000000e-03, 1.77827941e-03, 3.16227766e-03, 5.62341325e-03,\n        ...     1.00000000e-02, 1.77827941e-02, 3.16227766e-02, 5.62341325e-02,\n        ...     1.00000000e-01, 1.77827941e-01, 3.16227766e-01, 5.62341325e-01,\n        ...     1.00000000e+00, 1.77827941e+00, 3.16227766e+00, 5.62341325e+00,\n        ...     1.00000000e+01, 1.77827941e+01, 3.16227766e+01, 5.62341325e+01,\n        ...     1.00000000e+02, 1.77827941e+02, 3.16227766e+02, 5.62341325e+02,\n        ...     1.00000000e+03])\n        >>> rcomplex = np.array([\n        ...     49.34369772-0.51828971j, 49.11781581-0.59248806j,\n        ...     48.85819872-0.6331137j , 48.58762806-0.62835135j,\n        ...     48.33331113-0.57965851j, 48.11599009-0.50083533j,\n        ...     47.94405036-0.41005275j, 47.81528917-0.32210768j,\n        ...     47.72215469-0.24543425j, 47.65607773-0.18297794j,\n        ...     47.60962191-0.13433101j, 47.57706229-0.09755774j,\n        ...     47.55424286-0.07031682j, 47.53822912-0.05041399j,\n        ...     47.52697253-0.03601005j, 47.51904718-0.02565412j,\n        ...     47.51345965-0.01824266j, 47.50951606-0.01295546j,\n        ...     47.50673042-0.00919217j, 47.50476152-0.0065178j ,\n        ...     47.50336925-0.00461938j, 47.50238442-0.00327285j,\n        ...     47.50168762-0.00231829j, 47.50119454-0.00164187j,\n        ...     47.50084556-0.00116268j])\n        >>> spectrum = sip_response(frequencies=frequencies, rcomplex=rcomplex)\n        >>> fig = spectrum.plot('spectrum.pdf', return_fig=True)\n\n        \"\"\"\n        fig, axes = self._plot(\n            reciprocal=reciprocal,\n            limits=limits,\n            title=title,\n            dtype=dtype,\n            **kwargs\n        )\n        fig.savefig(filename, dpi=300)\n        if return_fig:\n            return fig\n        else:\n            plt.close(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd one response object to the list", "response": "def add(self, response, label=None):\n        \"\"\"add one response object to the list\n        \"\"\"\n        if not isinstance(response, sip_response.sip_response):\n            raise Exception(\n                'can only add sip_reponse.sip_response objects'\n            )\n        self.objects.append(response)\n\n        if label is None:\n            self.labels.append('na')\n        else:\n            self.labels.append(label)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_rpha(self, filename, pmin=None, pmax=None, title=None):\n        cmap = mpl.cm.get_cmap('viridis')\n        SM = mpl.cm.ScalarMappable(norm=None, cmap=cmap)\n        colors = SM.to_rgba(np.linspace(0, 1, len(self.objects)))\n        fig, ax = plt.subplots(1, 1, figsize=(15 / 2.54, 15 / 2.54))\n        for nr, item in enumerate(self.objects):\n            ax.semilogx(\n                item.frequencies,\n                -item.rpha,\n                '.-',\n                color=colors[nr],\n                label=self.labels[nr],\n            )\n        ax.set_xlim(*self.xlim)\n        ax.set_ylabel(sip_labels.get_label('rpha', 'meas', 'mathml'))\n        ax.set_xlabel('frequency [Hz]')\n        ax.set_ylim(pmin, pmax)\n        if title is not None:\n            ax.set_title(title)\n        self._add_legend(ax)\n        fig.tight_layout()\n        fig.subplots_adjust(bottom=0.5)\n        fig.savefig(filename, dpi=300)\n        plt.close(fig)", "response": "plot all resistance and resistivity phase spectra"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_pastml_parameter_file(method, model, column):\n    ml = is_ml(method)\n    template = PASTML_ML_PARAMS_TAB if ml else PASTML_MP_PARAMS_TAB\n    column, method = get_column_method(column, method)\n    return template.format(state=column, method=method, model=model)", "response": "Get the filename where the PastML parameters are saved."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_pastml_marginal_prob_file(method, model, column):\n    if not is_marginal(method):\n        return None\n    column, method = get_column_method(column, method)\n    return PASTML_MARGINAL_PROBS_TAB.format(state=column, model=model)", "response": "Get the filename where the PastML marginal probabilities of node states are saved."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_data(data, squeeze=False):\n    vdata = np.atleast_2d(data)\n    nr_freqs = int(vdata.shape[1] / 2)\n    part1 = vdata[:, 0:nr_freqs]\n    part2 = vdata[:, nr_freqs:]\n    if(squeeze):\n        part1 = part1.squeeze()\n        part2 = part2.squeeze()\n    return part1, part2", "response": "Split data into two parts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generic_magpha_to_reim(mag, pha):\n    complex_nr = to_complex(mag, pha)\n    real_part = np.real(complex_nr)\n    imag_part = np.imag(complex_nr)\n    return real_part, imag_part", "response": "Convert magnitude and phase to real and imaginary part using the formula."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert(input_format, output_format, data, one_spectrum=False):\n    if input_format == output_format:\n        return data\n\n    if input_format not in from_converters:\n        raise KeyError('Input format {0} not known!'.format(input_format))\n\n    if output_format not in to_converters:\n        raise KeyError('Output format {0} not known!'.format(output_format))\n\n    # internally we always work with the second axis of double the frequency\n    # size\n    if len(data.shape) == 2 and data.shape[0] == 2 and one_spectrum:\n        work_data = np.hstack((data[0, :], data[1, :]))\n        one_spec_2d = True\n    else:\n        work_data = data\n        one_spec_2d = False\n\n    cre, cim = from_converters[input_format](work_data)\n    converted_data = to_converters[output_format](cre, cim)\n\n    if one_spec_2d:\n        part1, part2 = split_data(converted_data, True)\n        converted_data = np.vstack((part1, part2))\n\n    # reshape to input size (this should only be necessary for 1D data)\n    if len(data.shape) == 1:\n        converted_data = np.squeeze(converted_data)\n    return converted_data", "response": "Convert from the input format to the output format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, params, standardize=False):\n        resp = self._request(ENDPOINTS['SEARCH'], params)\n        if not standardize:\n            return resp\n        # Standardization logic\n        for res in resp['result_data']:\n            res = self.standardize(res)\n        return resp", "response": "Search for a list of person objects for the given search parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a detailed list of person objects for the given search params.", "response": "def detail_search(self, params, standardize=False):\n        \"\"\"Get a detailed list of person objects for the given search params.\n\n        :param params:\n            Dictionary specifying the query parameters\n\n        >>> people_detailed = d.detail_search({'first_name': 'tobias', 'last_name': 'funke'})\n        \"\"\"\n\n        response = self._request(ENDPOINTS['SEARCH'], params)\n        result_data = []\n        for person in response['result_data']:\n            try:\n                detail = self.person_details(person['person_id'],\n                                             standardize=standardize)\n            except ValueError:\n                pass\n            else:\n                result_data.append(detail)\n\n        response['result_data'] = result_data\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a detailed person object", "response": "def person_details(self, person_id, standardize=False):\n        \"\"\"Get a detailed person object\n\n        :param person_id:\n            String corresponding to the person's id.\n\n        >>> instructor = d.person('jhs878sfd03b38b0d463b16320b5e438')\n        \"\"\"\n        resp = self._request(path.join(ENDPOINTS['DETAILS'], person_id))\n        if standardize:\n            resp['result_data'] = [self.standardize(res) for res in resp['result_data']]\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_radic_header(header_group, dipole_mode=\"all\"):\n    header = ''.join(header_group[1])\n\n    nr_of_electrodes = _get_nr_of_electrodes(header.split('\\n'))\n    groups = itertools.groupby(\n        header.split('\\n'),\n        lambda line: (\n            line.startswith('[End Readings]') or\n            line.startswith('[Begin Readings]')\n        )\n    )\n\n    for key, group in groups:\n        # there is only one group we are interested in: the first one\n        if key:\n            tmp = np.array(\n                [\n                    np.fromstring(\n                        x, sep=' ', dtype=int\n                    )\n                    for x in next(groups)[1]\n                ],\n            )\n\n            # curpots = tmp.T.copy()\n            # curpots.resize(nr_of_electrodes + 2, tmp.shape[0])\n            break\n\n    # now we have the raw readings, pad them with zeros\n    readings = []\n    for raw_reading in tmp:\n        voltage_rus = raw_reading[3:]\n        # note that we now have the last electrode in here, too (so one more\n        # than we have RUs. This electrode is always 0\n        normed_reading = np.zeros(nr_of_electrodes)\n        for i in voltage_rus[np.where(voltage_rus > 0)]:\n            normed_reading[i - 1] = i\n        readings.append(np.hstack((raw_reading[0:3], normed_reading)))\n\n    # now generate the available configurations\n    # add calibration\n    reading_configs = {}\n    for nr, reading in enumerate(readings):\n        # nr = reading[0]\n        A = reading[1]\n        B = reading[2]\n\n        voltages = []\n\n        # loop over the RU-settings\n        # we always measure to the next zero electrode\n        for i in range(3, nr_of_electrodes + 3):\n            for j in range(i + 1, nr_of_electrodes + 3):\n                if reading[j] == 0:\n                    M = i - 2\n                    N = j - 2\n                    all_active = ((reading[i] == 0) and (reading[j] == 0))\n                    # print('a', 'b', 'm, n', A, B, M, N, all_active)\n                    voltages.append((A, B, M, N, all_active))\n                    break\n\n        reading_configs[nr + 1] = np.array(voltages)\n    return reading_configs", "response": "Parses the radic header into a single tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single result file as produced by the SIP256c SIP measuring device and return a pandas. DataFrame containing the data contained in the SIP256c SIP measuring device and the SIP - specific information about the device.", "response": "def parse_radic_file(filename, settings, selection_mode=\"after\",\n                     reciprocal=None):\n    \"\"\"Import one result file as produced by the SIP256c SIP measuring device\n    (Radic Research)\n\n    Full settings dictionary: ::\n\n        settings = {\n            'filter_skip': (integer) skip dipoles we are interested in\n            'quadrupole_mode': ['after'|'between'|'before'| 'all']\n                               which dipoles to use from the file\n        }\n\n\n    Parameters\n    ----------\n    filename: string\n        input filename, usually with the ending \".RES\"\n    settings: dict\n        Settings for the data import, see code snippet above\n    selection_mode: dict\n        which voltage dipoles should be returned. Possible choices:\n        \"all\"|\"before\"|\"after\"\n    reciprocal: int|None\n        If this is an integer, then assume this was a reciprocal measurement\n        and the number denotes the largest RU number, N. Electrode numbers\n        (a,b,m,n) will then be transformed to (N1 - a, N1 - b, N1 - m, N1 - n),\n        with N1 = N + 1\n\n    Returns\n    -------\n    sip_data: :py:pandas:`pandas.DataFrame`\n        The data contained in a data frame\n    electrodes : None\n        No electrode positions are imported\n    topography : None\n        No topography is imported\n\n    \"\"\"\n    try:\n        with open(filename, 'r', encoding='latin-1') as fid:\n            lines = fid.readlines()\n    except:\n        print('file not found', filename)\n        import pdb\n        pdb.set_trace()\n\n    groups = itertools.groupby(\n        lines,\n        lambda line: line.startswith('Reading:')\n    )\n\n    # parse header\n    group = next(groups)\n    header_data = _parse_radic_header(group, dipole_mode='between')\n\n    # parse readings\n    reading_blocks = {}\n    for key, group in groups:\n        # determine reading number\n        line = next(group)\n        reading_nr = int(line[8: line.find('/')].strip())\n        # print('reading nr', reading_nr)\n        reading_blocks[reading_nr] = [x for x in next(groups)[1]]\n        # print reading_blocks[reading_nr]\n\n    # print(sorted(reading_blocks.keys()))\n    # now parse the readings\n    print('number of readings', len(reading_blocks))\n    print('keys', sorted(reading_blocks.keys()))\n    readings = {}\n    for key in sorted(reading_blocks):\n        # print('KEY/Reading', key)\n        reading = reading_blocks[key]\n        tmp = parse_reading(reading)\n        # except Exception as e:\n        #     print('Parsing of reading failed')\n        #     print(''.join(reading))\n        #     print('error message')\n        #     print(e)\n        #     exit()\n        readings[key] = tmp\n    # print('reading keys', sorted(readings.keys()))\n\n    logging.debug('removing calibration reading')\n    # remove calibration reading\n    if 0 in readings:\n        del(readings[0])\n\n    # print('readings', readings)\n    sip_data_raw = compute_quadrupoles(header_data, readings, settings)\n\n    sip_data = pd.concat(sip_data_raw)\n\n    if reciprocal is not None and isinstance(reciprocal, int):\n        sip_data['a'] = (reciprocal + 1) - sip_data['a']\n        sip_data['b'] = (reciprocal + 1) - sip_data['b']\n        sip_data['m'] = (reciprocal + 1) - sip_data['m']\n        sip_data['n'] = (reciprocal + 1) - sip_data['n']\n\n    return sip_data, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a pseudosection plot of type 2.", "response": "def plot_pseudosection_type2(dataobj, column, **kwargs):\n    \"\"\"Create a pseudosection plot of type 2.\n\n    For a given measurement data set, create plots that graphically show\n    the data in a 2D color plot. Hereby, x and y coordinates in the plot\n    are determined by the current dipole (x-axis) and voltage dipole\n    (y-axis) of the corresponding measurement configurations.\n\n    This type of rawdata plot can plot any type of measurement\n    configurations, i.e., it is not restricted to certain types of\n    configurations such as Dipole-dipole or Wenner configurations. However,\n    spatial inferences cannot be made from the plots for all configuration\n    types.\n\n    Coordinates are generated by separately sorting the dipoles\n    (current/voltage) along the first electrode, and then subsequently\n    sorting by the difference (skip) between both electrodes.\n\n    Note that this type of raw data plot does not take into account the\n    real electrode spacing of the measurement setup.\n\n    Type 2 plots can show normal and reciprocal data at the same time.\n    Hereby the upper left triangle of the plot area usually contains normal\n    data, and the lower right triangle contains the corresponding\n    reciprocal data. Therefore a quick assessment of normal-reciprocal\n    differences can be made by visually comparing the symmetry on the 1:1\n    line going from the lower left corner to the upper right corner.\n\n    Note that this interpretation usually only holds for Dipole-Dipole data\n    (and the related Wenner configurations).\n\n    Parameters\n    ----------\n\n    dataobj: ERT container|pandas.DataFrame\n        Container or DataFrame with data to plot\n    column: string\n        Column key to plot\n    ax: matplotlib.Axes object, optional\n        axes object to plot to. If not provided, a new figure and axes\n        object will be created and returned\n    nocb: bool, optional\n        if set to False, don't plot the colorbar\n    cblabel: string, optional\n        label for the colorbar\n    cbmin: float, optional\n        colorbar minimum\n    cbmax: float, optional\n        colorbar maximum\n    xlabel: string, optional\n        xlabel for the plot\n    ylabel: string, optional\n        ylabel for the plot\n    do_not_saturate: bool, optional\n        if set to True, then values outside the colorbar range will not\n        saturate with the respective limit colors. Instead, values lower\n        than the CB are colored \"cyan\" and vaues above the CB limit are\n        colored \"red\"\n    log10: bool, optional\n        if set to True, plot the log10 values of the provided data\n\n    Returns\n    -------\n    fig:\n        figure object\n    ax:\n        axes object\n    cb:\n        colorbar object\n\n    Examples\n    --------\n\n    You can just supply a pandas.DataFrame to the plot function:\n\n    .. plot::\n        :include-source:\n\n        import numpy as np\n        configs = np.array((\n            (1, 2, 4, 3),\n            (1, 2, 5, 4),\n            (1, 2, 6, 5),\n            (2, 3, 5, 4),\n            (2, 3, 6, 5),\n            (3, 4, 6, 5),\n        ))\n        measurements = np.random.random(configs.shape[0])\n        import pandas as pd\n        df = pd.DataFrame(configs, columns=['a', 'b', 'm', 'n'])\n        df['measurements'] = measurements\n\n        from reda.plotters.pseudoplots import plot_pseudosection_type2\n        fig, ax, cb = plot_pseudosection_type2(\n           dataobj=df,\n           column='measurements',\n        )\n\n    You can also supply axes to plot to:\n\n    .. plot::\n        :include-source:\n\n        import numpy as np\n        configs = np.array((\n            (1, 2, 4, 3),\n            (1, 2, 5, 4),\n            (1, 2, 6, 5),\n            (2, 3, 5, 4),\n            (2, 3, 6, 5),\n            (3, 4, 6, 5),\n        ))\n        measurements = np.random.random(configs.shape[0])\n        measurements2 = np.random.random(configs.shape[0])\n\n        import pandas as pd\n        df = pd.DataFrame(configs, columns=['a', 'b', 'm', 'n'])\n        df['measurements'] = measurements\n        df['measurements2'] = measurements2\n\n        from reda.plotters.pseudoplots import plot_pseudosection_type2\n\n        fig, axes = plt.subplots(1, 2)\n\n        plot_pseudosection_type2(\n            df,\n            column='measurements',\n            ax=axes[0],\n            cblabel='this label',\n            xlabel='xlabel',\n            ylabel='ylabel',\n        )\n        plot_pseudosection_type2(\n            df,\n            column='measurements2',\n            ax=axes[1],\n            cblabel='measurement 2',\n            xlabel='xlabel',\n            ylabel='ylabel',\n        )\n        fig.tight_layout()\n\n    >>> from reda.testing.containers import ERTContainer_nr\n    >>> import reda.plotters.pseudoplots as ps\n    >>> fig, axes, cb = ps.plot_pseudosection_type2(ERTContainer_nr, 'r')\n\n    \"\"\"\n    if isinstance(dataobj, pd.DataFrame):\n        df = dataobj\n    else:\n        df = dataobj.data\n\n    c = df[['a', 'b', 'm', 'n']].values\n\n    AB_ids = _get_unique_identifiers(c[:, 0:2])\n    MN_ids = _get_unique_identifiers(c[:, 2:4])\n\n    ab_sorted = np.sort(c[:, 0:2], axis=1)\n    mn_sorted = np.sort(c[:, 2:4], axis=1)\n\n    AB_coords = [\n        AB_ids[x] for x in\n        (ab_sorted[:, 0] * 1e5 + ab_sorted[:, 1]).astype(int)\n    ]\n    MN_coords = [\n        MN_ids[x] for x in\n        (mn_sorted[:, 0] * 1e5 + mn_sorted[:, 1]).astype(int)\n    ]\n\n    # check for duplicate positions\n    ABMN_coords = np.vstack((AB_coords, MN_coords)).T.copy()\n    _, counts = np.unique(\n        ABMN_coords.view(\n            ABMN_coords.dtype.descr * 2\n        ),\n        return_counts=True,\n    )\n    # import IPython\n    # IPython.embed()\n    # exit()\n    if np.any(counts > 1):\n        print('found duplicate coordinates!')\n        # duplicate_configs = np.where(counts > 1)[0]\n        # print('duplicate configs:')\n        # print('A B M N')\n        # for i in duplicate_configs:\n        #     print(c[i, :])\n\n    # prepare matrix\n    plot_values = np.squeeze(df[column].values)\n\n    if kwargs.get('log10', False):\n        plot_values = np.log10(plot_values)\n\n    C = np.zeros((len(MN_ids.items()), len(AB_ids))) * np.nan\n    C[MN_coords, AB_coords] = plot_values\n\n    # for display purposes, reverse the first dimension\n    C = C[::-1, :]\n\n    ax = kwargs.get('ax', None)\n    if ax is None:\n        fig, ax = plt.subplots(1, 1, figsize=(15 / 2.54, 10 / 2.54))\n    fig = ax.get_figure()\n\n    cmap = mpl.cm.get_cmap('viridis')\n    if kwargs.get('do_not_saturate', False):\n        cmap.set_over(\n            color='r'\n        )\n        cmap.set_under(\n            color='c'\n        )\n    im = ax.matshow(\n        C,\n        interpolation='none',\n        cmap=cmap,\n        aspect='auto',\n        vmin=kwargs.get('cbmin', None),\n        vmax=kwargs.get('cbmax', None),\n        extent=[\n            0, max(AB_coords),\n            0, max(MN_coords),\n        ],\n    )\n\n    max_xy = max((max(AB_coords), max(MN_coords)))\n    ax.plot(\n        (0, max_xy),\n        (0, max_xy),\n        '-',\n        color='k',\n        linewidth=1.0,\n    )\n\n    cb = None\n    if not kwargs.get('nocb', False):\n        cb = fig.colorbar(im, ax=ax)\n        cb.set_label(\n            kwargs.get('cblabel', units.get_label(column))\n        )\n\n    ax.set_xlabel(\n        kwargs.get('xlabel', 'current dipoles')\n    )\n    ax.set_ylabel(\n        kwargs.get('ylabel', 'voltage dipoles')\n    )\n\n    return fig, ax, cb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating grouped pseudoplots for one or more time steps.", "response": "def plot_ps_extra(dataobj, key, **kwargs):\n    \"\"\"Create grouped pseudoplots for one or more time steps\n\n    Parameters\n    ----------\n    dataobj: :class:`reda.containers.ERT`\n        An ERT container with loaded data\n    key: string\n        The column name to plot\n    subquery: string, optional\n    cbmin: float, optional\n    cbmax: float, optional\n\n    Examples\n    --------\n    >>> import reda.testing.containers\n    >>> ert = reda.testing.containers.ERTContainer_nr\n    >>> import reda.plotters.pseudoplots as PS\n    >>> fig = PS.plot_ps_extra(ert, key='r')\n    \"\"\"\n    if isinstance(dataobj, pd.DataFrame):\n        df_raw = dataobj\n    else:\n        df_raw = dataobj.data\n\n    if kwargs.get('subquery', False):\n        df = df_raw.query(kwargs.get('subquery'))\n    else:\n        df = df_raw\n\n    def fancyfy(axes, N):\n        for ax in axes[0:-1, :].flat:\n            ax.set_xlabel('')\n        for ax in axes[:, 1:].flat:\n            ax.set_ylabel('')\n\n    g = df.groupby('timestep')\n    N = len(g.groups.keys())\n    nrx = min((N, 5))\n    nry = int(np.ceil(N / nrx))\n    # the sizes are heuristics [inches]\n    sizex = nrx * 3\n    sizey = nry * 4 - 1\n    fig, axes = plt.subplots(\n        nry, nrx,\n        sharex=True,\n        sharey=True,\n        figsize=(sizex, sizey),\n    )\n    axes = np.atleast_2d(axes)\n\n    cbs = []\n    for ax, (name, group) in zip(axes.flat, g):\n        fig1, axes1, cb1 = plot_pseudosection_type2(\n            group,\n            key,\n            ax=ax,\n            log10=False,\n            cbmin=kwargs.get('cbmin', None),\n            cbmax=kwargs.get('cbmax', None),\n        )\n        cbs.append(cb1)\n        ax.set_title('timestep: {0}'.format(int(name)))\n        ax.xaxis.set_ticks_position('bottom')\n        ax.set_aspect('equal')\n\n    for cb in np.array(cbs).reshape(axes.shape)[:, 0:-1].flat:\n        cb.ax.set_visible(False)\n\n    fancyfy(axes, N)\n    fig.tight_layout()\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhacks to fix twisted not accepting absolute URIs", "response": "def twisted_absolute_path(path, request):\n    \"\"\"Hack to fix twisted not accepting absolute URIs\"\"\"\n    parsed = urlparse.urlparse(request.uri)\n    if parsed.scheme != '':\n        path_parts = parsed.path.lstrip('/').split('/')\n        request.prepath = path_parts[0:1]\n        request.postpath = path_parts[1:]\n        path = request.prepath[0]\n    return path, request"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_rhoa(df, spacing):\n    df['k'] = redaK.compute_K_analytical(df, spacing=spacing)\n    df['rho_a'] = df['r'] * df['k']\n    if 'Zt' in df.columns:\n        df['rho_a_complex'] = df['Zt'] * df['k']\n    return df", "response": "a simple wrapper to compute K factors and add rhoa\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a. mat file with single potentials A B M and return a pandas DataFrame containing the MNUs.", "response": "def _read_mat_mnu0(filename):\n    \"\"\"Import a .mat file with single potentials (A B M) into a pandas\n    DataFrame\n\n    Also export some variables of the md struct into a separate structure\n    \"\"\"\n    print('read_mag_single_file')\n\n    mat = sio.loadmat(filename)\n\n    df_emd = _extract_emd(mat)\n\n    return df_emd"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of geoids reduce it to a simpler set.", "response": "def simplify(geoids):\n    \"\"\"\n    Given a list of geoids, reduce it to a simpler set. If there are five or more geoids at one summary level\n    convert them to a single geoid at the higher level.\n\n    :param geoids:\n    :return:\n    \"\"\"\n\n    from collections import defaultdict\n\n    aggregated = defaultdict(set)\n\n    d = {}\n\n    for g in geoids:\n\n        if not bool(g):\n            continue\n\n        av = g.allval()\n\n        d[av] = None\n\n        aggregated[av].add(g)\n\n    compiled = set()\n\n    for k, v in aggregated.items():\n        if len(v) >= 5:\n            compiled.add(k)\n            compiled.add(k.promote())\n        else:\n            compiled |= v\n\n    return compiled"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_input(self):\n        app_split = self.args[0].split('.')\n        app = app_split[0]\n        model_name = app_split[1].lower()\n        \n        self.model = get_model(app, model_name)\n\n        # String field name to re-generate.\n        self.field = self.args[1]", "response": "Go through the user input and get some important values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle re - generating the thumbnails for the current language of the object.", "response": "def regenerate_thumbs(self):\n        \"\"\"\n        Handle re-generating the thumbnails. All this involves is reading the\n        original file, then saving the same exact thing. Kind of annoying, but\n        it's simple.\n        \"\"\"\n        Model = self.model\n        instances = Model.objects.all()\n        num_instances = instances.count()\n        # Filenames are keys in here, to help avoid re-genning something that\n        # we have already done.\n        regen_tracker = {}\n\n        counter = 1\n        for instance in instances:\n            file = getattr(instance, self.field)\n            if not file:\n                print \"(%d/%d) ID: %d -- Skipped -- No file\" % (counter,\n                                                                num_instances,\n                                                                instance.id)\n                counter += 1\n                continue\n\n            file_name = os.path.basename(file.name)\n\n            if regen_tracker.has_key(file_name):\n                print \"(%d/%d) ID: %d -- Skipped -- Already re-genned %s\" % (\n                                                    counter,\n                                                    num_instances,\n                                                    instance.id,\n                                                    file_name)\n                counter += 1\n                continue\n\n            # Keep them informed on the progress.\n            print \"(%d/%d) ID: %d -- %s\" % (counter, num_instances,\n                                            instance.id, file_name)\n\n            try:\n                fdat = file.read()\n                file.close()\n                del file.file\n            except IOError:\n                # Key didn't exist.\n                print \"(%d/%d) ID %d -- Error -- File missing on S3\" % (\n                                                              counter,\n                                                              num_instances,\n                                                              instance.id)\n                counter += 1\n                continue\n\n            try:\n                file_contents = ContentFile(fdat)\n            except ValueError:\n                # This field has no file associated with it, skip it.\n                print \"(%d/%d) ID %d --  Skipped -- No file on field)\" % (\n                                                              counter,\n                                                              num_instances,\n                                                              instance.id)\n                counter += 1\n                continue\n\n            # Saving pumps it back through the thumbnailer, if this is a\n            # ThumbnailField. If not, it's still pretty harmless.\n\n            try:\n                file.generate_thumbs(file_name, file_contents)\n            except IOError, e:\n                print \"(%d/%d) ID %d --  Error -- Image may be corrupt)\" % (\n                    counter,\n                    num_instances,\n                    instance.id)\n                counter += 1\n                continue\n\n            regen_tracker[file_name] = True\n            counter += 1"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_vowels(text):\n    count = 0\n    for i in text:\n        if i.lower() in config.AVRO_VOWELS:\n            count += 1\n    return count", "response": "Count the number of occurrences of vowels in a given string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncounting the number of occurrences of consonants in a given string", "response": "def count_consonants(text):\n    \"\"\"Count number of occurrences of consonants in a given string\"\"\"\n    count = 0\n    for i in text:\n        if i.lower() in config.AVRO_CONSONANTS:\n            count += 1\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the pseudodepths for the Wenner electrodes.", "response": "def _pseudodepths_wenner(configs, spacing=1, grid=None):\n    \"\"\"Given distances between electrodes, compute Wenner pseudo\n    depths for the provided configuration\n\n    The pseudodepth is computed after Roy & Apparao, 1971, as 0.11 times\n    the distance between the two outermost electrodes. It's not really\n    clear why the Wenner depths are different from the Dipole-Dipole\n    depths, given the fact that Wenner configurations are a complete subset\n    of the Dipole-Dipole configurations.\n\n    \"\"\"\n    if grid is None:\n        xpositions = (configs - 1) * spacing\n    else:\n        xpositions = grid.get_electrode_positions()[configs - 1, 0]\n\n    z = np.abs(np.max(xpositions, axis=1) - np.min(xpositions, axis=1)) * -0.11\n    x = np.mean(xpositions, axis=1)\n    return x, z"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting pseudodepths for the measurements.", "response": "def plot_pseudodepths(configs, nr_electrodes, spacing=1, grid=None,\n                      ctypes=None, dd_merge=False, **kwargs):\n    \"\"\"Plot pseudodepths for the measurements. If grid is given, then the\n    actual electrode positions are used, and the parameter 'spacing' is\n    ignored'\n\n    Parameters\n    ----------\n    configs: :class:`numpy.ndarray`\n        Nx4 array containing the quadrupoles for different measurements\n    nr_electrodes: int\n        The overall number of electrodes of the dataset. This is used to plot\n        the surface electrodes\n    spacing: float, optional\n        assumed distance between electrodes. Default=1\n    grid: crtomo.grid.crt_grid instance, optional\n        grid instance. Used to infer real electrode positions\n    ctypes: list of strings, optional\n        a list of configuration types that will be plotted. All\n        configurations that can not be sorted into these types will not be\n        plotted! Possible types:\n\n        * dd\n        * schlumberger\n\n    dd_merge: bool, optional\n        if True, merge all skips. Otherwise, generate individual plots for\n        each skip\n\n    Returns\n    -------\n    figs: matplotlib.figure.Figure instance or list of Figure instances\n        if only one type was plotted, then the figure instance is returned.\n        Otherwise, return a list of figure instances.\n    axes: axes object or list of axes ojects\n        plot axes\n\n    Examples\n    --------\n\n    .. plot::\n        :include-source:\n\n        from reda.plotters.plots2d import plot_pseudodepths\n        # define a few measurements\n        import numpy as np\n        configs = np.array((\n            (1, 2, 4, 3),\n            (1, 2, 5, 4),\n            (1, 2, 6, 5),\n            (2, 3, 5, 4),\n            (2, 3, 6, 5),\n            (3, 4, 6, 5),\n        ))\n        # plot\n        fig, axes = plot_pseudodepths(configs, nr_electrodes=6, spacing=1,\n                                      ctypes=['dd', ])\n\n    .. plot::\n        :include-source:\n\n        from reda.plotters.plots2d import plot_pseudodepths\n        # define a few measurements\n        import numpy as np\n        configs = np.array((\n            (4, 7, 5, 6),\n            (3, 8, 5, 6),\n            (2, 9, 5, 6),\n            (1, 10, 5, 6),\n        ))\n        # plot\n        fig, axes = plot_pseudodepths(configs, nr_electrodes=10, spacing=1,\n                                      ctypes=['schlumberger', ])\n\n    \"\"\"\n    # for each configuration type we have different ways of computing\n    # pseudodepths\n    pseudo_d_functions = {\n        'dd': _pseudodepths_dd_simple,\n        'schlumberger': _pseudodepths_schlumberger,\n        'wenner': _pseudodepths_wenner,\n    }\n\n    titles = {\n        'dd': 'dipole-dipole configurations',\n        'schlumberger': 'Schlumberger configurations',\n        'wenner': 'Wenner configurations',\n    }\n\n    # sort the configurations into the various types of configurations\n    only_types = ctypes or ['dd', ]\n    results = fT.filter(configs, settings={'only_types': only_types, })\n\n    # loop through all measurement types\n    figs = []\n    axes = []\n    for key in sorted(results.keys()):\n        print('plotting: ', key)\n        if key == 'not_sorted':\n            continue\n        index_dict = results[key]\n        # it is possible that we want to generate multiple plots for one\n        # type of measurement, i.e., to separate skips of dipole-dipole\n        # measurements. Therefore we generate two lists:\n        # 1) list of list of indices to plot\n        # 2) corresponding labels\n        if key == 'dd' and not dd_merge:\n            plot_list = []\n            labels_add = []\n            for skip in sorted(index_dict.keys()):\n                plot_list.append(index_dict[skip])\n                labels_add.append(' - skip {0}'.format(skip))\n        else:\n            # merge all indices\n            plot_list = [np.hstack(index_dict.values()), ]\n            print('schlumberger', plot_list)\n            labels_add = ['', ]\n\n        grid = None\n        # generate plots\n        for indices, label_add in zip(plot_list, labels_add):\n            if len(indices) == 0:\n                continue\n            ddc = configs[indices]\n            px, pz = pseudo_d_functions[key](ddc, spacing, grid)\n\n            fig, ax = plt.subplots(figsize=(15 / 2.54, 5 / 2.54))\n            ax.scatter(px, pz, color='k', alpha=0.5)\n\n            # plot electrodes\n            if grid is not None:\n                electrodes = grid.get_electrode_positions()\n                ax.scatter(\n                    electrodes[:, 0],\n                    electrodes[:, 1],\n                    color='b',\n                    label='electrodes', )\n            else:\n                ax.scatter(\n                    np.arange(0, nr_electrodes) * spacing,\n                    np.zeros(nr_electrodes),\n                    color='b',\n                    label='electrodes', )\n            ax.set_title(titles[key] + label_add)\n            ax.set_aspect('equal')\n            ax.set_xlabel('x [m]')\n            ax.set_ylabel('x [z]')\n\n            fig.tight_layout()\n            figs.append(fig)\n            axes.append(ax)\n\n    if len(figs) == 1:\n        return figs[0], axes[0]\n    else:\n        return figs, axes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_pseudosection(df, plot_key, spacing=1, ctypes=None, dd_merge=False,\n                       cb=False, **kwargs):\n    \"\"\"Create a pseudosection plot for a given measurement\n\n    Parameters\n    ----------\n    df: dataframe\n        measurement dataframe, one measurement frame (i.e., only one frequency\n        etc)\n    key:\n        which key to colorcode\n    spacing: float, optional\n        assumed electrode spacing\n    ctypes: list of strings\n        which configurations to plot, default: dd\n    dd_merge: bool, optional\n        ?\n    cb: bool, optional\n        ?\n\n    \"\"\"\n    grid = None\n\n    pseudo_d_functions = {\n        'dd': _pseudodepths_dd_simple,\n        'schlumberger': _pseudodepths_schlumberger,\n        'wenner': _pseudodepths_wenner,\n    }\n\n    titles = {\n        'dd': 'dipole-dipole configurations',\n        'schlumberger': 'Schlumberger configurations',\n        'wenner': 'Wenner configurations',\n    }\n\n    # for now sort data and only plot dipole-dipole\n    only_types = ctypes or ['dd', ]\n    if 'schlumberger' in only_types:\n        raise Exception('plotting of pseudosections not implemented for ' +\n                        'Schlumberger configurations!')\n\n    configs = df[['a', 'b', 'm', 'n']].values\n    results = fT.filter(\n        configs,\n        settings={'only_types': only_types, }, )\n    values = df[plot_key].values\n\n    plot_objects = []\n    for key in sorted(results.keys()):\n        print('plotting: ', key)\n        if key == 'not_sorted':\n            continue\n        index_dict = results[key]\n        # it is possible that we want to generate multiple plots for one\n        # type of measurement, i.e., to separate skips of dipole-dipole\n        # measurements. Therefore we generate two lists:\n        # 1) list of list of indices to plot\n        # 2) corresponding labels\n        if key == 'dd' and not dd_merge:\n            plot_list = []\n            labels_add = []\n            for skip in sorted(index_dict.keys()):\n                plot_list.append(index_dict[skip])\n                labels_add.append(' - skip {0}'.format(skip))\n        else:\n            # merge all indices\n            plot_list = [np.hstack(index_dict.values()), ]\n            print('schlumberger', plot_list)\n            labels_add = ['', ]\n\n        # generate plots\n        for indices, label_add in zip(plot_list, labels_add):\n            if len(indices) == 0:\n                continue\n\n            ddc = configs[indices]\n            plot_data = values[indices]\n            px, pz = pseudo_d_functions[key](ddc, spacing, grid)\n            # we need at least four points for a spatial interpolation, I\n            # think...\n            if px.size <= 4:\n                continue\n\n            # take 200 points for the new grid in every direction. Could be\n            # adapted to the actual ratio\n            xg = np.linspace(px.min(), px.max(), 200)\n            zg = np.linspace(pz.min(), pz.max(), 200)\n\n            x, z = np.meshgrid(xg, zg)\n\n            cmap_name = kwargs.get('cmap_name', 'jet')\n            cmap = mpl.cm.get_cmap(cmap_name)\n\n            # normalize data\n            data_min = kwargs.get('cbmin', plot_data.min())\n            data_max = kwargs.get('cbmax', plot_data.max())\n            cnorm = mpl.colors.Normalize(vmin=data_min, vmax=data_max)\n            scalarMap = mpl.cm.ScalarMappable(norm=cnorm, cmap=cmap)\n            fcolors = scalarMap.to_rgba(plot_data)\n\n            try:\n                image = si.griddata(\n                    (px, pz),\n                    fcolors,\n                    (x, z),\n                    method='linear', )\n            except siq.QhullError as e:\n                print('Ex', e)\n                continue\n\n            cmap = mpl.cm.get_cmap('jet_r')\n\n            data_ratio = np.abs(px.max() - px.min()) / np.abs(pz.min())\n\n            fig_size_y = 15 / data_ratio + 6 / 2.54\n            fig = plt.figure(figsize=(15, fig_size_y))\n\n            fig_top = 1 / 2.54 / fig_size_y\n            fig_left = 2 / 2.54 / 15\n            fig_right = 1 / 2.54 / 15\n            if cb:\n                fig_bottom = 3 / 2.54 / fig_size_y\n            else:\n                fig_bottom = 0.05\n\n            ax = fig.add_axes([\n                fig_left, fig_bottom + fig_top * 2, 1 - fig_left - fig_right,\n                1 - fig_top - fig_bottom - fig_top * 2\n            ])\n\n            im = ax.imshow(\n                image[::-1],\n                extent=(xg.min(), xg.max(), zg.min(), zg.max()),\n                interpolation='none',\n                aspect='auto',\n                # vmin=10,\n                # vmax=300,\n                cmap=cmap, )\n            ax.set_ylim(pz.min(), 0)\n\n            # colorbar\n            if cb:\n                print('plotting colorbar')\n                # the colorbar has 3 cm on the bottom\n                ax_cb = fig.add_axes([\n                    fig_left * 4, fig_top * 2,\n                    1 - fig_left * 4 - fig_right * 4, fig_bottom - fig_top * 2\n                ])\n                # from mpl_toolkits.axes_grid1 import make_axes_locatable\n                # divider = make_axes_locatable(ax)\n                # ax_cb = divider.append_axes(\"bottom\", \"5%\", pad=\"3%\")\n                # (ax_cb, kw) = mpl.colorbar.make_axes_gridspec(\n                #     ax,\n                #     orientation='horizontal',\n                #     fraction=fig_bottom,\n                #     pad=0.3,\n                #     shrink=0.9,\n                #     # location='bottom',\n                # )\n                cb = mpl.colorbar.ColorbarBase(\n                    ax=ax_cb,\n                    cmap=cmap,\n                    norm=cnorm,\n                    orientation='horizontal',\n                    # **kw\n                )\n                cb.set_label('cb label')\n            else:\n                fig_bottom = 0.05\n\n            # 1cm on top\n\n            # # 3 cm on bottom for colorbar\n            # fig.subplots_adjust(\n            #     top=1 - fig_top,\n            #     bottom=fig_bottom,\n            # )\n\n            ax.set_title(titles[key] + label_add)\n            ax.set_aspect('equal')\n            ax.set_xlabel('x [m]')\n            ax.set_ylabel('x [z]')\n            plot_objects.append((fig, ax, im))\n\n    return plot_objects", "response": "Create a pseudosection plot for a given measurement."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef matplot(x, y, z, ax=None, colorbar=True, **kwargs):\n    xmin = x.min()\n    xmax = x.max()\n    dx = np.abs(x[0, 1] - x[0, 0])\n\n    ymin = y.min()\n    ymax = y.max()\n    dy = np.abs(y[1, 0] - y[0, 0])\n\n    x2, y2 = np.meshgrid(\n        np.arange(xmin, xmax + 2 * dx, dx) - dx / 2.,\n        np.arange(ymin, ymax + 2 * dy, dy) - dy / 2.)\n\n    if not ax:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.figure\n\n    im = ax.pcolormesh(x2, y2, z, **kwargs)\n    ax.axis([x2.min(), x2.max(), y2.min(), y2.max()])\n    ax.set_xticks(np.arange(xmin, xmax + dx, dx))\n    ax.set_yticks(np.arange(ymin, ymax + dx, dy))\n\n    if colorbar:\n        cbar = fig.colorbar(im, ax=ax)\n    else:\n        cbar = None\n\n    return ax, cbar", "response": "Plot x y z as expected with correct axis labels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_f81_pij(t, frequencies, mu):\n\n    # if mu == inf (e.g. just one state) and t == 0, we should prioritise mu\n    exp_mu_t = 0. if (mu == np.inf) else np.exp(-mu * t)\n    return (1 - exp_mu_t) * frequencies + np.eye(len(frequencies)) * exp_mu_t", "response": "Calculates the probability of substitution i - > j over time t given the mutation rate mu."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ledger(self):\n        self.balance()  # make sure the transaction balances\n\n        s = \"{0}/{1:02}/{2:02}  {3}\\n\".format(\n            self.date.year,\n            self.date.month,\n            self.date.day,\n            self.desc.replace('\\n', ' ')\n        )\n        for src in self.src:\n            s += \"  {0.account}  ${0.amount}\\n\".format(src)\n        for dst in self.dst:\n            s += \"  {0.account}  ${0.amount}\\n\".format(dst)\n        return s", "response": "Convert to a Ledger transaction"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string summary of the current state of the current object", "response": "def summary(self):\n        \"\"\"Return a string summary of transaction\"\"\"\n        return \"\\n\".join([\n            \"Transaction:\",\n            \"  When:        \" + self.date.strftime(\"%a %d %b %Y\"),\n            \"  Description: \" + self.desc.replace('\\n', ' '),\n            \"  For amount:  {}\".format(self.amount),\n            \"  From:        {}\".format(\n                \", \".join(map(lambda x: x.account, self.src)) if self.src \\\n                    else \"UNKNOWN\"\n            ),\n            \"  To:          {}\".format(\n                \", \".join(map(lambda x: x.account, self.dst)) if self.dst \\\n                    else \"UNKNOWN\"\n            ),\n            \"\"\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(self):\n        if not self.date:\n            raise XnDataError(\"Missing date\")\n        if not self.desc:\n            raise XnDataError(\"Missing description\")\n        if not self.dst:\n            raise XnDataError(\"No destination accounts\")\n        if not self.src:\n            raise XnDataError(\"No source accounts\")\n        if not self.amount:\n            raise XnDataError(\"No transaction amount\")", "response": "Check this transaction for completeness"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef balance(self):\n        self.check()\n        if not sum(map(lambda x: x.amount, self.src)) == -self.amount:\n            raise XnBalanceError(\"Sum of source amounts \"\n                                 \"not equal to transaction amount\")\n        if not sum(map(lambda x: x.amount, self.dst)) == self.amount:\n            raise XnBalanceError(\"Sum of destination amounts \"\n                                 \"not equal to transaction amount\")\n        return True", "response": "Check this transaction for correctness"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing this transaction against the given ruleset returning a dictionary of fields with ScoreSet values which may be empty.", "response": "def match_rules(self, rules):\n        \"\"\"Process this transaction against the given ruleset\n\n        Returns a dict of fields with ScoreSet values, which may be empty.\n        Notably, the rule processing will be shortcircuited if the Xn is\n        already complete - in this case, None is returned.\n        \"\"\"\n        try:\n            self.check()\n            return None\n        except XnDataError:\n            pass\n\n        scores = {}\n\n        for r in rules:\n            outcomes = r.match(self)\n            if not outcomes:\n                continue\n            for outcome in outcomes:\n                if isinstance(outcome, rule.SourceOutcome):\n                    key = 'src'\n                elif isinstance(outcome, rule.DestinationOutcome):\n                    key = 'dst'\n                elif isinstance(outcome, rule.DescriptionOutcome):\n                    key = 'desc'\n                elif isinstance(outcome, rule.DropOutcome):\n                    key = 'drop'\n                elif isinstance(outcome, rule.RebateOutcome):\n                    key = 'rebate'\n                else:\n                    raise KeyError\n                if key not in scores:\n                    scores[key] = score.ScoreSet()  # initialise ScoreSet\n                scores[key].append((outcome.value, outcome.score))\n\n        return scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the given outcomes to this rule.", "response": "def apply_outcomes(self, outcomes, uio, dropped=False, prevxn=None):\n        \"\"\"Apply the given outcomes to this rule.\n\n        If user intervention is required, outcomes are not applied\n        unless a ui.UI is supplied.\n        \"\"\"\n        if self.dropped and not dropped:\n            # do nothing for dropped xn, unless specifically told to\n            return\n\n        if 'drop' in outcomes:\n            highscore = score.score(outcomes['drop'].highest()[0])\n            if highscore >= threshold['y']:\n                # drop without prompting\n                self.dropped = True\n            elif highscore < threshold['n?']:\n                # do NOT drop, and don't even ask\n                pass\n            else:\n                uio.show('DROP was determined for transaction:')\n                uio.show('')\n                uio.show(self.summary())\n                if highscore >= threshold['y?']:\n                    default = True\n                elif highscore >= threshold['?']:\n                    default = None\n                else:\n                    default = False\n                try:\n                    self.dropped = uio.yn('DROP this transaction?', default)\n                except ui.RejectWarning:\n                    # we assume they mean \"no\"\n                    pass\n\n        if self.dropped and not dropped:\n            # do nothing further for dropped xn, unless specifically told to\n            return\n\n        # rebate outcomes\n        #\n        # A rebate is a rebate of the previous transaction.\n        # The proportions of credits in the prev xn are kept,\n        # inverted (i.e. made debits) and scaled to the rebate\n        # amount credit amount.\n        if 'rebate' in outcomes and not self.src and prevxn is not None:\n            ratio = self.amount / prevxn.amount\n            def scale(dst_ep):\n                amount = (dst_ep.amount * ratio).quantize(dst_ep.amount)\n                return Endpoint(dst_ep.account, -amount)\n            self.src = map(scale, prevxn.dst)\n            # handle rounding errors\n            self.src[0].amount -= self.amount + sum(x.amount for x in self.src)\n\n        # account outcomes\n        for outcome in ['src', 'dst']:\n            if outcome not in outcomes or getattr(self, outcome):\n                # no outcome, or the attribute was already set\n                continue\n\n            endpoints = []\n            highest = outcomes[outcome].highest()\n            try:\n                highscore = score.score(highest[0])\n                if len(highest) == 1:\n                    if highscore >= threshold['y']:\n                        # do it\n                        endpoints = [\n                            Endpoint(score.value(highest[0]), self.amount)\n                        ]\n                    else:\n                        uio.show('Choose ' + outcome + ' for transaction:')\n                        uio.show('')\n                        uio.show(self.summary())\n\n                        prompt = 'Is the account {0}?'.format(\n                            score.value(highest[0])\n                        )\n                        if highscore >= threshold['y?']:\n                            default = True\n                        elif highscore >= threshold['?']:\n                            default = None\n                        else:\n                            default = False\n                        if uio.yn(prompt, default):\n                            endpoints = [\n                                Endpoint(\n                                    score.value(highest[0]),\n                                    self.amount\n                                )\n                            ]\n                        else:\n                            raise ui.RejectWarning('top score declined')\n                else:\n                    # tied highest score, let user pick\n                    uio.show('Choose ' + outcome + ' for transaction:')\n                    uio.show('')\n                    uio.show(self.summary())\n\n                    prompt = 'Choose an account'\n                    endpoints = [\n                        Endpoint(\n                            uio.choose(prompt, map(score.value, highest)),\n                            self.amount\n                        )\n                    ]\n\n            except ui.RejectWarning:\n                # user has rejected our offer(s)\n                uio.show(\"\\n\")\n                uio.show('Enter ' + outcome + ' endpoints:')\n                try:\n                    endpoints = []\n                    remaining = self.amount\n                    while remaining:\n                        uio.show('\\n${0} remaining'.format(remaining))\n                        account = uio.text(\n                            ' Enter account',\n                            score.value(highest[0]) if highest else None\n                        )\n                        amount = uio.decimal(\n                            ' Enter amount',\n                            default=remaining,\n                            lower=0,\n                            upper=remaining\n                        )\n                        endpoints.append(Endpoint(account, amount))\n                        remaining = self.amount \\\n                            - sum(map(lambda x: x.amount, endpoints))\n                except ui.RejectWarning:\n                    # bail out\n                    sys.exit(\"bye!\")\n\n            # flip amounts if it was a src outcome\n            if outcome == 'src':\n                endpoints = map(\n                    lambda x: Endpoint(x.account, -x.amount),\n                    endpoints\n                )\n\n            # set endpoints\n            setattr(self, outcome, endpoints)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries for all missing information in the transaction", "response": "def complete(self, uio, dropped=False):\n        \"\"\"Query for all missing information in the transaction\"\"\"\n        if self.dropped and not dropped:\n            # do nothing for dropped xn, unless specifically told to\n            return\n\n        for end in ['src', 'dst']:\n            if getattr(self, end):\n                continue  # we have this information\n\n            uio.show('\\nEnter ' + end + ' for transaction:')\n            uio.show('')\n            uio.show(self.summary())\n            try:\n                endpoints = []\n                remaining = self.amount\n                while remaining:\n                    account = uio.text(' Enter account', None)\n                    amount = uio.decimal(\n                        ' Enter amount',\n                        default=remaining,\n                        lower=0,\n                        upper=remaining\n                    )\n                    endpoints.append(Endpoint(account, amount))\n                    remaining = self.amount \\\n                        - sum(map(lambda x: x.amount, endpoints))\n            except ui.RejectWarning:\n                # bail out\n                sys.exit(\"bye!\")\n\n            # flip amounts if it was a src outcome\n            if end == 'src':\n                endpoints = map(\n                    lambda x: Endpoint(x.account, -x.amount),\n                    endpoints\n                )\n\n            # set endpoints\n            setattr(self, end, endpoints)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmatch rules and applies outcomes", "response": "def process(self, rules, uio, prevxn=None):\n        \"\"\"Matches rules and applies outcomes\"\"\"\n        self.apply_outcomes(self.match_rules(rules), uio, prevxn=prevxn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_payment(self, *, reference_code, description, tx_value, tx_tax, tx_tax_return_base, currency, buyer,\n                     payer, credit_card, payment_method, payment_country, device_session_id, ip_address, cookie,\n                     user_agent, language=None, shipping_address=None, extra_parameters=None, notify_url=None,\n                     transaction_type=TransactionType.AUTHORIZATION_AND_CAPTURE):\n        \"\"\"\n        Authorization: used to verify if a credit card is active, if it has funds, etc.\n        The transaction is not complete until a transaction capture is sent (only available for accounts in Argentina,\n        Brazil, Peru).\n\n        Capture: terminates a previously authorized transaction.\n        This is when the account makes a debit to the card (only available for accounts in Argentina, Brazil, Peru).\n\n        Authorization and capture: this is the most used type of transaction.\n        This option sends the transaction amount to authorization and if it is approved immediately capture is performed.\n\n        Args:\n            reference_code: The reference code of the order. It represents the identifier of the transaction\n            in the shop\u2019s system.\n            Alphanumeric. Min: 1 Max: 255.\n\n            description: The description of the order.\n            Alphanumeric. Min: 1 Max: 255.\n\n            tx_value: TX_VALUE, it is the total amount of the transaction. It can contain two decimal digits.\n            For example 10000.00 and 10000.\n            Alphanumeric. 64.\n\n            tx_tax: TX_TAX, it is the value of the VAT (Value Added Tax only valid for Colombia) of the transaction,\n            if no VAT is sent, the system will apply 19% automatically. It can contain two decimal digits.\n            Example 19000.00. In case you have no VAT you should fill out 0.\n            Alphanumeric. 64.\n\n            tx_tax_return_base: TX_TAX_RETURN_BASE, it is the base value on which VAT (only valid for Colombia)\n            is calculated. If you do not have VAT should be sent to 0.\n            Alphanumeric. 64.\n\n            currency: The ISO currency code associated with the amount.\n            http://developers.payulatam.com/en/api/variables_table.html\n            Alphanumeric. 3.\n\n            buyer: Buyer\u2019s shipping address.\n            Example.\n            {\n                \"merchantBuyerId\": \"1\",\n                \"fullName\": \"First name and second buyer  name\",\n                \"emailAddress\": \"buyer_test@test.com\",\n                \"contactPhone\": \"7563126\",\n                \"dniNumber\": \"5415668464654\",\n                \"shippingAddress\": {\n                    \"street1\": \"calle 100\",\n                    \"street2\": \"5555487\",\n                    \"city\": \"Medellin\",\n                    \"state\": \"Antioquia\",\n                    \"country\": \"CO\",\n                    \"postalCode\": \"000000\",\n                    \"phone\": \"7563126\"\n                }\n            }\n\n            payer: Payer\u2019s data.\n            Example.\n            {\n                \"merchantPayerId\": \"1\",\n                \"fullName\": \"First name and second payer name\",\n                \"emailAddress\": \"payer_test@test.com\",\n                \"contactPhone\": \"7563126\",\n                \"dniNumber\": \"5415668464654\",\n                \"billingAddress\": {\n                    \"street1\": \"calle 93\",\n                    \"street2\": \"125544\",\n                    \"city\": \"Bogota\",\n                    \"state\": \"Bogota DC\",\n                    \"country\": \"CO\",\n                    \"postalCode\": \"000000\",\n                    \"phone\": \"7563126\"\n                }\n            }\n\n            credit_card: Debit card\u2019s data.\n            Example.\n            {\n                \"number\": \"4097440000000004\",\n                \"securityCode\": \"321\",\n                \"expirationDate\": \"2022/12\",\n                \"name\": \"APPROVED\"\n            }\n\n            payment_method: Payment method.\n            Alphanumeric. 32.\n\n            payment_country: Payment countries.\n            http://developers.payulatam.com/en/api/variables_table.html\n\n            device_session_id: The session identifier of the device where the transaction was performed from.\n            Alphanumeric. Max: 255.\n\n            ip_address: The IP address of the device where the transaction was performed from.\n            Alphanumeric. Max: 39.\n\n            cookie: The cookie stored on the device where the transaction was performed from.\n            Alphanumeric. Max: 255.\n\n            user_agent: The user agent of the browser from which the transaction was performed.\n            Alphanumeric. Max: 1024\n\n            language: The language used in the emails that are sent to the buyer and seller.\n            Alphanumeric. 2\n\n            shipping_address: The shipping address.\n            Example.\n            {\n                \"street1\": \"calle 100\",\n                \"street2\": \"5555487\",\n                \"city\": \"Medellin\",\n                \"state\": \"Antioquia\",\n                \"country\": \"CO\",\n                \"postalCode\": \"0000000\",\n                \"phone\": \"7563126\"\n            }\n\n            extra_parameters: Additional parameters or data associated with a transaction. These parameters may vary\n            according to the payment means or shop\u2019s preferences.\n            Example.\n            {\n                \"INSTALLMENTS_NUMBER\": 1\n            }\n\n            notify_url: The URL notification or order confirmation.\n            Alphanumeric. Max: 2048.\n\n            transaction_type:\n\n        Returns:\n\n        \"\"\"\n        if not isinstance(payment_country, Country):\n            payment_country = Country(payment_country)\n\n        if not isinstance(transaction_type, TransactionType):\n            transaction_type = TransactionType(transaction_type)\n\n        if not isinstance(payment_method, Franchise):\n            payment_method = Franchise(payment_method)\n\n        if not isinstance(currency, Currency):\n            currency = Currency(currency)\n\n        franchises = get_available_franchise_for_payment(payment_country, transaction_type)\n        if not franchises or payment_method not in franchises:\n            fmt = 'The credit card franchise {} with transaction type {} is not available for {}.'\n            raise CVVRequiredError(fmt.format(payment_method.value, transaction_type.value, payment_country.name))\n\n        payload = {\n            \"language\": self.client.language.value,\n            \"command\": PaymentCommand.SUBMIT_TRANSACTION.value,\n            \"merchant\": {\n                \"apiKey\": self.client.api_key,\n                \"apiLogin\": self.client.api_login\n            },\n            \"transaction\": {\n                \"order\": {\n                    \"accountId\": self.client.account_id,\n                    \"referenceCode\": reference_code,\n                    \"description\": description,\n                    \"language\": language or self.client.language.value,\n                    \"signature\": self.client._get_signature(reference_code, tx_value, currency.value),\n                    \"notifyUrl\": notify_url,\n                    \"additionalValues\": {\n                        \"TX_VALUE\": {\n                            \"value\": tx_value,\n                            \"currency\": currency.value\n                        },\n                        \"TX_TAX\": {\n                            \"value\": tx_tax,\n                            \"currency\": currency.value\n                        },\n                        \"TX_TAX_RETURN_BASE\": {\n                            \"value\": tx_tax_return_base,\n                            \"currency\": currency.value\n                        }\n                    },\n                    \"buyer\": buyer,\n                    \"shippingAddress\": shipping_address\n                },\n                \"payer\": payer,\n                \"creditCard\": credit_card,\n                \"extraParameters\": extra_parameters,\n                \"type\": transaction_type.value,\n                \"paymentMethod\": payment_method.value,\n                \"paymentCountry\": payment_country.value,\n                \"deviceSessionId\": device_session_id,\n                \"ipAddress\": ip_address,\n                \"cookie\": cookie,\n                \"userAgent\": user_agent\n            },\n            \"test\": self.client.is_test\n        }\n        return self.client._post(self.url, json=payload)", "response": "This function is used to make a payment for an order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvisualize time -lapse evolution of a single quadropole.", "response": "def plot_quadpole_evolution(dataobj, quadpole, cols, threshold=5,\n                            rolling=False, ax=None):\n    \"\"\"Visualize time-lapse evolution of a single quadropole.\n\n    Parameters\n    ----------\n    dataobj : :py:class:`pandas.DataFrame`\n        DataFrame containing the data. Please refer to the documentation for\n        required columns.\n    quadpole : list of integers\n        Electrode numbers of the the quadropole.\n    cols : str\n        The column/parameter to plot over time.\n    threshold : float\n        Allowed percentage deviation from the rolling standard deviation.\n    rolling : bool\n        Calculate rolling median values (the default is False).\n    ax : mpl.axes\n        Optional axes object to plot to.\n    \"\"\"\n    if isinstance(dataobj, pd.DataFrame):\n        df = dataobj\n    else:\n        df = dataobj.data\n\n    subquery = df.query(\n        'a == {0} and b == {1} and m == {2} and n == {3}'.format(*quadpole))\n    # rhoa = subquery['rho_a'].values\n    # rhoa[30] = 300\n    # subquery['rho_a'] = rhoa\n\n    if ax is not None:\n        fig = ax.get_figure()\n    else:\n        fig, ax = plt.subplots(1, 1, figsize=(20 / 2.54, 7 / 2.54))\n\n    ax.plot(\n        subquery['timestep'],\n        subquery[cols],\n        '.',\n        color='blue',\n        label='valid data',\n    )\n    if rolling:\n        # rolling mean\n        rolling_m = subquery.rolling(3, center=True, min_periods=1).median()\n\n        ax.plot(\n            rolling_m['timestep'].values,\n            rolling_m['rho_a'].values,\n            '-',\n            label='rolling median',\n        )\n\n        ax.fill_between(\n            rolling_m['timestep'].values,\n            rolling_m['rho_a'].values * (1 - threshold),\n            rolling_m['rho_a'].values * (1 + threshold),\n            alpha=0.4,\n            color='blue',\n            label='{0}\\% confidence region'.format(threshold * 100),\n        )\n\n        # find all values that deviate by more than X percent from the\n        # rolling_m\n        bad_values = (np.abs(\n            np.abs(subquery['rho_a'].values - rolling_m['rho_a'].values) /\n            rolling_m['rho_a'].values) > threshold)\n\n        bad = subquery.loc[bad_values]\n        ax.plot(\n            bad['timestep'].values,\n            bad['rho_a'].values,\n            '.',\n            # s=15,\n            color='r',\n            label='discarded data',\n        )\n\n    ax.legend(loc='upper center', fontsize=6)\n    # ax.set_xlim(10, 20)\n\n    ax.set_ylabel(r'$\\rho_a$ [$\\Omega$m]')\n    ax.set_xlabel('timestep')\n    return fig, ax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring Logr with the given level and handler and formatter.", "response": "def configure(level=logging.WARNING, handler=None, formatter=None):\n        \"\"\"Configure Logr\n\n        @param handler: Logger message handler\n        @type handler: logging.Handler or None\n\n        @param formatter: Logger message Formatter\n        @type formatter: logging.Formatter or None\n        \"\"\"\n        if formatter is None:\n            formatter = LogrFormatter()\n\n        if handler is None:\n            handler = logging.StreamHandler()\n\n        handler.setFormatter(formatter)\n        handler.setLevel(level)\n        Logr.handler = handler"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets or create a logger.", "response": "def get_logger():\n        \"\"\"Get or create logger (if it does not exist)\n\n        @rtype: RootLogger\n        \"\"\"\n        name = Logr.get_logger_name()\n        if name not in Logr.loggers:\n            Logr.configure_check()\n            Logr.loggers[name] = logging.Logger(name)\n            Logr.loggers[name].addHandler(Logr.handler)\n        return Logr.loggers[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visitUnaryShape(self, ctx: ShExDocParser.UnaryShapeContext):\n        if ctx.include():\n            self.expression = self.context.tripleexprlabel_to_iriref(ctx.include().tripleExprLabel())\n        else:\n            lbl = self.context.tripleexprlabel_to_iriref(ctx.tripleExprLabel()) if ctx.tripleExprLabel() else None\n            if ctx.tripleConstraint():\n                self.expression = TripleConstraint(lbl)\n                self.visit(ctx.tripleConstraint())\n            elif ctx.encapsulatedShape():\n                self.visit(ctx.encapsulatedShape())\n                self.expression.id = lbl", "response": "visit unaryShape returns unaryShape"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visitEncapsulatedShape(self, ctx: ShExDocParser.EncapsulatedShapeContext):\n        enc_shape = ShexOneOfShapeParser(self.context)\n        enc_shape.visit(ctx.innerShape())\n        self.expression = enc_shape.expression\n        self._card_annotations_and_semacts(ctx)", "response": "EncapsulatedShape is an encapsulated shape expression."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visitStarCardinality(self, ctx: ShExDocParser.StarCardinalityContext):\n        self.expression.min = 0\n        self.expression.max = -1", "response": "Set min and max to 0."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the minimum and maximum of the cardinality of the current entry.", "response": "def visitPlusCardinality(self, ctx: ShExDocParser.PlusCardinalityContext):\n        \"\"\" '+' \"\"\"\n        self.expression.min = 1\n        self.expression.max = -1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset min and max to 0 if cardinality is not defined", "response": "def visitOptionalCardinality(self, ctx: ShExDocParser.OptionalCardinalityContext):\n        \"\"\" '?' \"\"\"\n        self.expression.min = 0\n        self.expression.max = 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visitSenseFlags(self, ctx: ShExDocParser.SenseFlagsContext):\n        if '!' in ctx.getText():\n            self.expression.negated = True\n        if '^' in ctx.getText():\n            self.expression.inverse = True", "response": "Set expression negated and inverse to True."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the predicate of the taxonomy.", "response": "def visitPredicate(self, ctx: ShExDocParser.PredicateContext):\n        \"\"\" predicate: iri | rdfType \"\"\"\n        self.expression.predicate = self.context.predicate_to_IRI(ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(cls, datestr):\n        m = DATE_RE.match(datestr)\n        if m is not None:\n            day = None if m.group(2) is None else int(m.group(2))\n            return cls(m.group(4), m.group(3), day, m.group(1))", "response": "Parse a GEDCOM date string and make a CalendarDate object from it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a three - tuple of numbers.", "response": "def as_tuple(self):\n        \"\"\"Date as three-tuple of numbers\"\"\"\n        if self._tuple is None:\n            # extract leading digits from year\n            year = 9999\n            if self.year:\n                m = self.DIGITS.match(self.year)\n                if m:\n                    year = int(m.group(0))\n            month = self.month_num or 99\n            day = self.day if self.day is not None else 99\n\n            # should we include calendar name in tuple too?\n            self._tuple = year, month, day\n\n        return self._tuple"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a printable representation of the instance.", "response": "def fmt(self):\n        \"\"\"Make printable representation out of this instance.\n        \"\"\"\n        val = str(self.year)\n        if self.month is not None:\n            val += ' ' + str(self.month)\n            if self.day is not None:\n                val += ' ' + str(self.day)\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, datestr):\n        # some apps generate DATE recods without any value, which is\n        # non-standard, return empty DateValue for those\n        if not datestr:\n            return cls()\n        for regex, tmpl in DATES:\n            m = regex.match(datestr)\n            if m is not None:\n                groups = {}\n                for key, val in m.groupdict().items():\n                    if key != 'phrase':\n                        val = CalendarDate.parse(val)\n                    groups[key] = val\n                return cls(tmpl, groups)\n        # if cannot parse string assume it is a phrase\n        return cls(\"($phrase)\", dict(phrase=datestr))", "response": "Parse string <DATE_VALUE > string and return a new instance of DateValue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn CalendarDate used for comparison.", "response": "def _cmp_date(self):\n        \"\"\"Returns Calendar date used for comparison.\n\n        Use the earliest date out of all CalendarDates in this instance,\n        or some date in the future if there are no CalendarDates (e.g.\n        when Date is a phrase).\n        \"\"\"\n        dates = sorted(val for val in self.kw.values()\n                       if isinstance(val, CalendarDate))\n        if dates:\n            return dates[0]\n        # return date very far in the future\n        return CalendarDate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes printable representation of this instance.", "response": "def fmt(self):\n        \"\"\"Make printable representation out of this instance.\n        \"\"\"\n        tmpl = string.Template(self.template)\n        kw = {}\n        for key, val in self.kw.items():\n            if key == 'phrase':\n                kw[key] = val\n            else:\n                kw[key] = val.fmt()\n        return tmpl.substitute(kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef better_sentences(func):\n\n    @wraps(func)\n    def wrapped(*args):\n\n            sentences = func(*args)\n            new_sentences = []\n            for i, l in enumerate(sentences):\n                if '\\n\\n' in l:\n                    splits = l.split('\\n\\n')\n                    if len(splits)>1:\n                        for ind,spl in enumerate(splits):\n                            if len(spl) <20:\n                                #if DEBUG: print \"Discarding: \", spl\n                                del splits[ind]\n                    new_sentences.extend(splits)\n                else:\n                    new_sentences.append(l)\n            return new_sentences\n    return wrapped", "response": "takes care of some edge cases of sentence\n    tokenization for cases when websites don t have close sentences properly"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __we_c(cls, calib, tc, temp, we_v):\n        offset_v = calib.pid_elc_mv / 1000.0\n\n        response_v = we_v - offset_v                # remove electronic zero\n        response_c = tc.correct(temp, response_v)   # correct the response component\n\n        if response_c is None:\n            return None\n\n        we_c = response_c + offset_v                # replace electronic zero\n\n        return we_c", "response": "Compute weC from weV and weV"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __cnc(cls, calib, we_c):\n        if we_c is None:\n            return None\n\n        offset_v = calib.pid_elc_mv / 1000.0\n\n        response_c = we_c - offset_v                # remove electronic zero\n        cnc = response_c / calib.pid_sens_mv        # pid_sens_mv set for ppb or ppm - see PID.init()\n\n        return cnc", "response": "Compute cnc from weC\n"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request(self, method, url, headers=None, **kwargs):\n        _headers = {\n            'Accept': 'application/json',\n            'Content-Type': 'application/json'\n        }\n        if headers:\n            _headers.update(headers)\n\n        if self.is_debug:\n            self.logger.debug('{} {} {} {}'.format(method, url, headers, kwargs))\n        return self._parse(requests.request(method, url, headers=_headers, timeout=60, **kwargs))", "response": "Internal method for making a request to PayU."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_to_class(self, model_class):\n        model_class._meta.add_field(self)\n        setattr(model_class, self.name, _FieldDescriptor(self))", "response": "Replace the Field attribute with a named _FieldDescriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_field(self, field):\n        self.remove_field(field.name)\n        self._fields[field.name] = field\n\n        if field.default is not None:\n            if six.callable(field.default):\n                self._default_callables[field.key] = field.default\n            else:\n                self._defaults[field.key] = field.default", "response": "Add the received field to the model."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove the field with the received field name from model.", "response": "def remove_field(self, field_name):\n        \"\"\"Remove the field with the received field name from model.\"\"\"\n        field = self._fields.pop(field_name, None)\n        if field is not None and field.default is not None:\n            if six.callable(field.default):\n                self._default_callables.pop(field.key, None)\n            else:\n                self._defaults.pop(field.key, None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_defaults(self):\n        defaults = self._defaults.copy()\n        for field_key, default in self._default_callables.items():\n            defaults[field_key] = default()\n        return defaults", "response": "Get a dictionary that contains all the available defaults."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef speak(self, text, lang, gender, format):\n\n        namemap = {\n            \"ar-EG,Female\": \"Microsoft Server Speech Text to Speech Voice (ar-EG, Hoda)\",\n            \"de-DE,Female\": \"Microsoft Server Speech Text to Speech Voice (de-DE, Hedda)\",\n            \"de-DE,Male\": \"Microsoft Server Speech Text to Speech Voice (de-DE, Stefan, Apollo)\",\n            \"en-AU,Female\": \"Microsoft Server Speech Text to Speech Voice (en-AU, Catherine)\",\n            \"en-CA,Female\": \"Microsoft Server Speech Text to Speech Voice (en-CA, Linda)\",\n            \"en-GB,Female\": \"Microsoft Server Speech Text to Speech Voice (en-GB, Susan, Apollo)\",\n            \"en-GB,Male\": \"Microsoft Server Speech Text to Speech Voice (en-GB, George, Apollo)\",\n            \"en-IN,Male\": \"Microsoft Server Speech Text to Speech Voice (en-IN, Ravi, Apollo)\",\n            \"en-US,Male\": \"Microsoft Server Speech Text to Speech Voice (en-US, BenjaminRUS)\",\n            \"en-US,Female\": \"Microsoft Server Speech Text to Speech Voice (en-US, ZiraRUS)\",\n            \"es-ES,Female\": \"Microsoft Server Speech Text to Speech Voice (es-ES, Laura, Apollo)\",\n            \"es-ES,Male\": \"Microsoft Server Speech Text to Speech Voice (es-ES, Pablo, Apollo)\",\n            \"es-MX,Male\": \"Microsoft Server Speech Text to Speech Voice (es-MX, Raul, Apollo)\",\n            \"fr-CA,Female\": \"Microsoft Server Speech Text to Speech Voice (fr-CA, Caroline)\",\n            \"fr-FR,Female\": \"Microsoft Server Speech Text to Speech Voice (fr-FR, Julie, Apollo)\",\n            \"fr-FR,Male\": \"Microsoft Server Speech Text to Speech Voice (fr-FR, Paul, Apollo)\",\n            \"it-IT,Male\": \"Microsoft Server Speech Text to Speech Voice (it-IT, Cosimo, Apollo)\",\n            \"ja-JP,Female\": \"Microsoft Server Speech Text to Speech Voice (ja-JP, Ayumi, Apollo)\",\n            \"ja-JP,Male\": \"Microsoft Server Speech Text to Speech Voice (ja-JP, Ichiro, Apollo)\",\n            \"pt-BR,Male\": \"Microsoft Server Speech Text to Speech Voice (pt-BR, Daniel, Apollo)\",\n            \"ru-RU,Female\": \"Microsoft Server Speech Text to Speech Voice (ru-RU, Irina, Apollo)\",\n            \"ru-RU,Male\": \"Microsoft Server Speech Text to Speech Voice (ru-RU, Pavel, Apollo)\",\n            \"zh-CN,Female\": \"Microsoft Server Speech Text to Speech Voice (zh-CN, HuihuiRUS)\",\n            \"zh-CN,Male\": \"Microsoft Server Speech Text to Speech Voice (zh-CN, Kangkang, Apollo)\",\n            \"zh-HK,Male\": \"Microsoft Server Speech Text to Speech Voice (zh-HK, Danny, Apollo)\",\n            \"zh-TW,Female\": \"Microsoft Server Speech Text to Speech Voice (zh-TW, Yating, Apollo)\",\n            \"zh-TW,Male\": \"Microsoft Server Speech Text to Speech Voice (zh-TW, Zhiwei, Apollo)\",\n            \"nl-NL,Female\": \"Microsoft Server Speech Text to Speech Voice (nl-NL, HannaRUS)\",\n            \"id-ID,Male\": \"Microsoft Server Speech Text to Speech Voice (id-ID, Andika)\",\n            \"id-ID,Female\": \"Microsoft Server Speech Text to Speech Voice (id-ID, Andika)\",\n            \"ar-EG,Female\": \"Microsoft Server Speech Text to Speech Voice (ar-EG, Hoda)\",\n            \"hi-IN,Female\": \"Microsoft Server Speech Text to Speech Voice (hi-IN, Kalpana)\",\n            \"ru-RU,Female\": \"Microsoft Server Speech Text to Speech Voice (ru-RU, Irina)\"\n        }\n        if not gender:\n            gender = 'Female'\n        else:\n            gender = gender.capitalize()\n\n        if not lang:\n            lang = 'en-US'\n\n        if not format:\n            format = 'riff-8khz-8bit-mono-mulaw'\n\n        try:\n            servicename = namemap[lang + ',' + gender]\n        except (Exception):\n            raise LanguageException(\"Invalid language/gender combination: %s, %s\" % (lang, gender))\n\n        headers = {\n            \"Content-type\": \"application/ssml+xml; charset=utf-8\",\n            \"X-Microsoft-OutputFormat\": format,\n            # \"X-Search-AppId\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n            # \"X-Search-ClientID\": \"YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\",\n            \"User-Agent\": \"TTSForPython\"\n        }\n\n        body = \"<speak version='1.0' xml:lang='%s'><voice xml:lang='%s' xml:gender='%s' name='%s'>%s</voice></speak>\" % (lang, lang, gender, servicename, str(text))\n\n        return self.make_request(headers, body)", "response": "Speech the specified text using the specified language and gender and format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef speak(self, textstr, lang='en-US', gender='female', format='riff-16khz-16bit-mono-pcm'):\n        # print(\"speak(textstr=%s, lang=%s, gender=%s, format=%s)\" % (textstr, lang, gender, format))\n        concatkey = '%s-%s-%s-%s' % (textstr, lang.lower(), gender.lower(), format)\n        key = self.tts_engine + '' + str(hash(concatkey))\n        self.filename = '%s-%s.mp3' % (key, lang)\n\n        # check if file exists\n        fileloc = self.directory + self.filename\n        if self.cache and os.path.isfile(self.directory + self.filename):\n            return self.filename\n        else:\n            with open(fileloc, 'wb') as f:\n                self.speech.speak_to_file(f, textstr, lang, gender, format)\n                return self.filename\n            return False", "response": "This method will call Microsoft Translate API and produce audio using the specified textstr and lang and gender and format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall a command and return exit_code and stdout", "response": "def call(args):\n    \"\"\"\n    Call terminal command and return exit_code and stdout\n\n    Parameters\n    ----------\n    args : list\n        A command and arguments list\n\n    Returns\n    -------\n    list : [exit_code, stdout]\n        exit_code indicate the exit code of the command and stdout indicate the\n        output of the command\n    \"\"\"\n    b = StringIO()\n    p = subprocess.Popen(args,\n                        stdout=subprocess.PIPE,\n                        stderr=subprocess.STDOUT)\n    encoding = getattr(sys.stdout, 'encoding', None) or 'utf-8'\n    # old python has bug in p.stdout, so the following little\n    # hack is required.\n    for stdout in iter(p.stdout.readline, ''):\n        if len(stdout) == 0:\n            break\n        # translate non unicode to unicode\n        stdout = force_unicode(stdout, encoding)\n        # StringIO store unicode\n        b.write(stdout)\n        # stdout require non unicode\n        sys.stdout.write(from_unicode(stdout, encoding))\n        sys.stdout.flush()\n    buf = b.getvalue()\n    p.stdout.close()\n    return p.returncode or 0, buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_command_str(args):\n    single_quote = \"'\"\n    double_quote = '\"'\n    for i, value in enumerate(args):\n        if \" \" in value and double_quote not in value:\n            args[i] = '\"%s\"' % value\n        elif \" \" in value and single_quote not in value:\n            args[i] = \"'%s'\" % value\n    return \" \".join(args)", "response": "Get terminal command string from list of command and arguments"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef receive_data_chunk(self, raw_data, start):\n        self.file.write(raw_data)\n        # CHANGED: This un-hangs us long enough to keep things rolling.\n        eventlet.sleep(0)", "response": "This method is called when the worker is listening for data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stoptimes(self, start_date, end_date):\n        params = {\n            'start': self.format_date(start_date),\n            'end': self.format_date(end_date)\n        }\n        response = self._request(ENDPOINTS['STOPTIMES'], params)\n        return response", "response": "Return all stop times in the date range\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_logger(self):\n        self.log_list = []\n        handler = ListHandler(self.log_list)\n\n        formatter = logging.Formatter(\n            '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n\n        logger = logging.getLogger()\n        logger.addHandler(handler)\n\n        logger.setLevel(logging.INFO)\n\n        self.handler = handler\n        self.logger = logger", "response": "Setup a logger that logs to the log_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_to_dict(match):\n    balance, indent, account_fragment = match.group(1, 2, 3)\n    return {\n        'balance': decimal.Decimal(balance),\n        'indent': len(indent),\n        'account_fragment': account_fragment,\n        'parent': None,\n        'children': [],\n    }", "response": "Convert a match object into a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef balance(output):\n    lines = map(pattern.search, output.splitlines())\n\n    stack = []\n    top = []\n    for item in map(match_to_dict, itertools.takewhile(lambda x: x, lines)):\n        # pop items off stack while current item has indent <=\n        while stack and item['indent'] <= stack[-1]['indent']:\n            stack.pop()\n\n        # check if this is a top-level item\n        if not stack:\n            stack.append(item)\n            top.append(item)\n        else:\n            item['parent'] = stack[-1]\n            stack[-1]['children'].append(item)\n            stack.append(item)\n\n    return top", "response": "Convert ledger balance output into an hierarchical data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_punctuation(text):\n    return not (text.lower() in config.AVRO_VOWELS or\n                text.lower() in config.AVRO_CONSONANTS)", "response": "Check if given string is a punctuation"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking exact occurrence of needle in haystack", "response": "def is_exact(needle, haystack, start, end, matchnot):\n    \"\"\"Check exact occurrence of needle in haystack\"\"\"\n    return ((start >= 0 and end < len(haystack) and\n             haystack[start:end] == needle) ^ matchnot)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fix_string_case(text):\n    fixed = []\n    for i in text:\n        if is_case_sensitive(i):\n            fixed.append(i)\n        else:\n            fixed.append(i.lower())\n    return ''.join(fixed)", "response": "Converts case - insensitive characters to lower case and returns a string with phonetic - compatible case."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts crmod - style configurations to a Nx4 array containing the unique values", "response": "def _crmod_to_abmn(self, configs):\n        \"\"\"convert crmod-style configurations to a Nx4 array\n\n        CRMod-style configurations merge A and B, and M and N, electrode\n        numbers into one large integer each:\n\n        .. math ::\n\n            AB = A \\cdot 10^4 + B\n\n            MN = M \\cdot 10^4 + N\n\n        Parameters\n        ----------\n        configs: numpy.ndarray\n            Nx2 array holding the configurations to convert\n\n        Examples\n        --------\n\n        >>> import numpy as np\n        >>> from reda.configs.configManager import ConfigManager\n        >>> config = ConfigManager(nr_of_electrodes=5)\n        >>> crmod_configs = np.array((\n        ...     (10002, 40003),\n        ...     (10010, 30004),\n        ... ))\n        >>> abmn = config._crmod_to_abmn(crmod_configs)\n        >>> print(abmn)\n        [[ 2  1  3  4]\n         [10  1  4  3]]\n\n        \"\"\"\n        A = configs[:, 0] % 1e4\n        B = np.floor(configs[:, 0] / 1e4).astype(int)\n        M = configs[:, 1] % 1e4\n        N = np.floor(configs[:, 1] / 1e4).astype(int)\n        ABMN = np.hstack((A[:, np.newaxis], B[:, np.newaxis], M[:, np.newaxis],\n                          N[:, np.newaxis])).astype(int)\n        return ABMN"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_configs(self, filename):\n        configs = np.loadtxt(filename)\n        self.add_to_configs(configs)", "response": "Load configurations from a file with four columns a b m n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_crmod_config(self, filename):\n        with open(filename, 'r') as fid:\n            nr_of_configs = int(fid.readline().strip())\n            configs = np.loadtxt(fid)\n            print('loaded configs:', configs.shape)\n            if nr_of_configs != configs.shape[0]:\n                raise Exception(\n                    'indicated number of measurements does not equal ' +\n                    'to actual number of measurements')\n            ABMN = self._crmod_to_abmn(configs[:, 0:2])\n            self.configs = ABMN", "response": "Load a CRMod configuration file and store it in self. configs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a CRMod measurement file into a list of unique ids.", "response": "def load_crmod_volt(self, filename):\n        \"\"\"Load a CRMod measurement file (commonly called volt.dat)\n\n        Parameters\n        ----------\n        filename: string\n            path to filename\n\n        Returns\n        -------\n        list\n            list of measurement ids\n        \"\"\"\n        with open(filename, 'r') as fid:\n            nr_of_configs = int(fid.readline().strip())\n            measurements = np.loadtxt(fid)\n            if nr_of_configs != measurements.shape[0]:\n                raise Exception(\n                    'indicated number of measurements does not equal ' +\n                    'to actual number of measurements')\n        ABMN = self._crmod_to_abmn(measurements[:, 0:2])\n        if self.configs is None:\n            self.configs = ABMN\n        else:\n            # check that configs match\n            if not np.all(ABMN == self.configs):\n                raise Exception(\n                    'previously stored configurations do not match new ' +\n                    'configurations')\n\n        # add data\n        cid_mag = self.add_measurements(measurements[:, 2])\n        cid_pha = self.add_measurements(measurements[:, 3])\n        return [cid_mag, cid_pha]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_crmod_abmn(self):\n        ABMN = np.vstack((\n            self.configs[:, 0] * 1e4 + self.configs[:, 1],\n            self.configs[:, 2] * 1e4 + self.configs[:, 3],\n        )).T.astype(int)\n        return ABMN", "response": "return a Nx2 array with the measurement configurations formatted\n            CRTomo style\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_crmod_volt(self, filename, mid):\n        ABMN = self._get_crmod_abmn()\n\n        if isinstance(mid, (list, tuple)):\n            mag_data = self.measurements[mid[0]]\n            pha_data = self.measurements[mid[1]]\n        else:\n            mag_data = self.measurements[mid]\n            pha_data = np.zeros(mag_data.shape)\n\n        all_data = np.hstack((ABMN, mag_data[:, np.newaxis],\n                              pha_data[:, np.newaxis]))\n\n        with open(filename, 'wb') as fid:\n            fid.write(bytes(\n                '{0}\\n'.format(ABMN.shape[0]),\n                'utf-8',\n            ))\n            np.savetxt(fid, all_data, fmt='%i %i %f %f')", "response": "Write the measurements to the output file in the volt. dat file format that can be read by CRTomo."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_crmod_config(self, filename):\n        ABMN = self._get_crmod_abmn()\n\n        with open(filename, 'wb') as fid:\n            fid.write(bytes(\n                '{0}\\n'.format(ABMN.shape[0]),\n                'utf-8',\n            ))\n            np.savetxt(fid, ABMN.astype(int), fmt='%i %i')", "response": "Write the configuration file in the CRMod format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the dipole - dipole configurations for a given set of electrodes and current energy.", "response": "def gen_dipole_dipole(self,\n                          skipc,\n                          skipv=None,\n                          stepc=1,\n                          stepv=1,\n                          nr_voltage_dipoles=10,\n                          before_current=False,\n                          start_skip=0,\n                          N=None):\n        \"\"\"Generate dipole-dipole configurations\n\n        Parameters\n        ----------\n        skipc: int\n            number of electrode positions that are skipped between electrodes\n            of a given dipole\n        skipv: int\n            steplength between subsequent voltage dipoles. A steplength of 0\n            will produce increments by one, i.e., 3-4, 4-5, 5-6 ...\n        stepc: int\n            steplength between subsequent current dipoles. A steplength of 0\n            will produce increments by one, i.e., 3-4, 4-5, 5-6 ...\n        stepv: int\n            steplength between subsequent voltage dipoles. A steplength of 0\n            will produce increments by one, i.e., 3-4, 4-5, 5-6 ...\n        nr_voltage_dipoles: int\n            the number of voltage dipoles to generate for each current\n            injection dipole\n        before_current: bool, optional\n            if set to True, also generate voltage dipoles in front of current\n            dipoles.\n        start_skip: int, optional\n            how many electrode to skip before/after the first/second current\n            electrode.\n        N: int, optional\n            number of electrodes, must be given if not already known by the\n            config instance\n\n        Examples\n        --------\n\n        >>> from reda.configs.configManager import ConfigManager\n        >>> config = ConfigManager(nr_of_electrodes=10)\n        >>> config.gen_dipole_dipole(skipc=2)\n        array([[ 1,  4,  5,  8],\n               [ 1,  4,  6,  9],\n               [ 1,  4,  7, 10],\n               [ 2,  5,  6,  9],\n               [ 2,  5,  7, 10],\n               [ 3,  6,  7, 10]])\n\n\n        \"\"\"\n        if N is None and self.nr_electrodes is None:\n            raise Exception('You must provide the number of electrodes')\n        elif N is None:\n            N = self.nr_electrodes\n\n        # by default, current voltage dipoles have the same size\n        if skipv is None:\n            skipv = skipc\n\n        configs = []\n        # current dipoles\n        for a in range(0, N - skipv - skipc - 3, stepc):\n            b = a + skipc + 1\n            nr = 0\n            # potential dipoles before current injection\n            if before_current:\n                for n in range(a - start_skip - 1, -1, -stepv):\n                    nr += 1\n                    if nr > nr_voltage_dipoles:\n                        continue\n                    m = n - skipv - 1\n                    if m < 0:\n                        continue\n                    quadpole = np.array((a, b, m, n)) + 1\n                    configs.append(quadpole)\n\n            # potential dipoles after current injection\n            nr = 0\n            for m in range(b + start_skip + 1, N - skipv - 1, stepv):\n                nr += 1\n                if nr > nr_voltage_dipoles:\n                    continue\n                n = m + skipv + 1\n                quadpole = np.array((a, b, m, n)) + 1\n                configs.append(quadpole)\n\n        configs = np.array(configs)\n        # now add to the instance\n        if self.configs is None:\n            self.configs = configs\n        else:\n            self.configs = np.vstack((self.configs, configs))\n        return configs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gen_gradient(self, skip=0, step=1, vskip=0, vstep=1):\n        N = self.nr_electrodes\n        quadpoles = []\n        for a in range(1, N - skip, step):\n            b = a + skip + 1\n            for m in range(a + 1, b - vskip - 1, vstep):\n                n = m + vskip + 1\n                quadpoles.append((a, b, m, n))\n\n        configs = np.array(quadpoles)\n        if configs.size == 0:\n            return None\n\n        self.add_to_configs(configs)\n        return configs", "response": "Generate gradient measurements for the current electrodes and voltage electrodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating all possible current dipoles for the given number of electrodes.", "response": "def gen_all_current_dipoles(self):\n        \"\"\"Generate all possible current dipoles for the given number of\n        electrodes (self.nr_electrodes). Duplicates are removed in the process.\n\n        After Noel and Xu, 1991, for N electrodes, the number of possible\n        unique configurations is :math:`N \\cdot (N - 1) / 2`. This excludes\n        duplicates in the form of switches current/voltages electrodes, as well\n        as reciprocal measurements.\n\n        Returns\n        -------\n        configs: Nx2 numpy.ndarray\n            all possible current dipoles A-B\n        \"\"\"\n        N = self.nr_electrodes\n        celecs = list(range(1, N + 1))\n        AB_list = itertools.permutations(celecs, 2)\n        AB = np.array([ab for ab in AB_list])\n        AB.sort(axis=1)\n\n        # now we need to filter duplicates\n        AB = np.unique(AB.view(AB.dtype.descr * 2)).view(AB.dtype).reshape(\n            -1, 2)\n\n        return AB"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove duplicate entries from 4 - point configurations. If no duplicates are provided use self. configs.", "response": "def remove_duplicates(self, configs=None):\n        \"\"\"remove duplicate entries from 4-point configurations. If no\n        configurations are provided, then use self.configs. Unique\n        configurations are only returned if configs is not None.\n\n        Parameters\n        ----------\n        configs: Nx4 numpy.ndarray, optional\n            remove duplicates from these configurations instead from\n            self.configs.\n\n        Returns\n        -------\n        configs_unique: Kx4 numpy.ndarray\n            unique configurations. Only returned if configs is not None\n\n        \"\"\"\n        if configs is None:\n            c = self.configs\n        else:\n            c = configs\n        struct = c.view(c.dtype.descr * 4)\n        configs_unique = np.unique(struct).view(c.dtype).reshape(-1, 4)\n        if configs is None:\n            self.configs = configs_unique\n        else:\n            return configs_unique"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates one Schlumberger sounding configuration that is one set of configurations for one potential dipole MN.", "response": "def gen_schlumberger(self, M, N, a=None):\n        \"\"\"generate one Schlumberger sounding configuration, that is, one set\n        of configurations for one potential dipole MN.\n\n        Parameters\n        ----------\n        M: int\n            electrode number for the first potential electrode\n        N: int\n            electrode number for the second potential electrode\n        a: int, optional\n            stepping between subsequent voltage electrodes. If not set,\n            determine it as a = abs(M - N)\n\n        Returns\n        -------\n        configs: Kx4 numpy.ndarray\n            array holding the configurations\n\n        Examples\n        --------\n\n            import reda.configs.configManager as CRconfig\n            config = CRconfig.ConfigManager(nr_of_electrodes=40)\n            config.gen_schlumberger(M=20, N=21)\n\n        \"\"\"\n        if a is None:\n            a = np.abs(M - N)\n\n        nr_of_steps_left = int(min(M, N) - 1 / a)\n        nr_of_steps_right = int((self.nr_electrodes - max(M, N)) / a)\n        configs = []\n        for i in range(0, min(nr_of_steps_left, nr_of_steps_right)):\n            A = min(M, N) - (i + 1) * a\n            B = max(M, N) + (i + 1) * a\n            configs.append((A, B, M, N))\n        configs = np.array(configs)\n        self.add_to_configs(configs)\n        return configs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_to_configs(self, configs):\n        if len(configs) == 0:\n            return None\n\n        if self.configs is None:\n            self.configs = np.atleast_2d(configs)\n        else:\n            configs = np.atleast_2d(configs)\n            self.configs = np.vstack((self.configs, configs))\n        return self.configs", "response": "Adds one or more measurement configurations to the stored\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit the stored configurations into normal and reciprocal measurements ** *Rule 1: the normal configuration contains the smallest electrode number of the four involved electrodes in the current dipole* ** Parameters ---------- pad: bool, optional if True, add numpy.nan values to the reciprocals for non-existent measuremnts return_indices: bool, optional if True, also return the indices of normal and reciprocal measurments. This can be used to extract corresponding measurements. Returns ------- normal: numpy.ndarray Nnx4 array. If pad is True, then Nn == N (total number of unique measurements). Otherwise Nn is the number of normal measurements. reciprocal: numpy.ndarray Nrx4 array. If pad is True, then Nr == N (total number of unique measurements). Otherwise Nr is the number of reciprocal measurements. nor_indices: numpy.ndarray, optional Nnx1 array containing the indices of normal measurements. Only returned if return_indices is True. rec_indices: numpy.ndarray, optional Nrx1 array containing the indices of normal measurements. Only returned if return_indices is True.", "response": "def split_into_normal_and_reciprocal(self, pad=False,\n                                         return_indices=False):\n        \"\"\"Split the stored configurations into normal and reciprocal\n        measurements\n\n        ** *Rule 1: the normal configuration contains the smallest electrode\n        number of the four involved electrodes in the current dipole* **\n\n        Parameters\n        ----------\n        pad: bool, optional\n            if True, add numpy.nan values to the reciprocals for non-existent\n            measuremnts\n        return_indices: bool, optional\n            if True, also return the indices of normal and reciprocal\n            measurments. This can be used to extract corresponding\n            measurements.\n\n        Returns\n        -------\n        normal: numpy.ndarray\n            Nnx4 array. If pad is True, then Nn == N (total number of\n            unique measurements). Otherwise Nn is the number of normal\n            measurements.\n        reciprocal: numpy.ndarray\n            Nrx4 array. If pad is True, then Nr == N (total number of\n            unique measurements). Otherwise Nr is the number of reciprocal\n            measurements.\n        nor_indices: numpy.ndarray, optional\n            Nnx1 array containing the indices of normal measurements. Only\n            returned if return_indices is True.\n        rec_indices: numpy.ndarray, optional\n            Nrx1 array containing the indices of normal measurements. Only\n            returned if return_indices is True.\n\n        \"\"\"\n        # for simplicity, we create an array where AB and MN are sorted\n        configs = np.hstack((np.sort(self.configs[:, 0:2], axis=1),\n                             np.sort(self.configs[:, 2:4], axis=1)))\n\n        ab_min = configs[:, 0]\n        mn_min = configs[:, 2]\n\n        # rule 1\n        indices_normal = np.where(ab_min < mn_min)[0]\n\n        # now look for reciprocals\n        indices_used = []\n        normal = []\n        normal_indices = []\n        reciprocal_indices = []\n        reciprocal = []\n        duplicates = []\n        for index in indices_normal:\n            indices_used.append(index)\n            normal.append(self.configs[index, :])\n            normal_indices.append(index)\n\n            # look for reciprocal configuration\n            index_rec = np.where(\n                # A == M, B == N, M == A, N == B\n                (configs[:, 0] == configs[index, 2]) &\n                (configs[:, 1] == configs[index, 3]) &\n                (configs[:, 2] == configs[index, 0]) &\n                (configs[:, 3] == configs[index, 1]))[0]\n            if len(index_rec) == 0 and pad:\n                reciprocal.append(np.ones(4) * np.nan)\n            elif len(index_rec) == 1:\n                reciprocal.append(self.configs[index_rec[0], :])\n                indices_used.append(index_rec[0])\n                reciprocal_indices.append(index_rec[0])\n            elif len(index_rec > 1):\n                # take the first one\n                reciprocal.append(self.configs[index_rec[0], :])\n                reciprocal_indices.append(index_rec[0])\n                duplicates += list(index_rec[1:])\n                indices_used += list(index_rec)\n\n        # now determine all reciprocal-only parameters\n        set_all_indices = set(list(range(0, configs.shape[0])))\n        set_used_indices = set(indices_used)\n        reciprocal_only_indices = set_all_indices - set_used_indices\n        for index in reciprocal_only_indices:\n            if pad:\n                normal.append(np.ones(4) * np.nan)\n            reciprocal.append(self.configs[index, :])\n\n        normals = np.array(normal)\n        reciprocals = np.array(reciprocal)\n\n        if return_indices:\n            return normals, reciprocals, normal_indices, reciprocal_indices\n        else:\n            return normals, reciprocals"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gen_reciprocals(self, append=False):\n        # Switch AB and MN\n        reciprocals = self.configs.copy()[:, ::-1]\n        reciprocals[:, 0:2] = np.sort(reciprocals[:, 0:2], axis=1)\n        reciprocals[:, 2:4] = np.sort(reciprocals[:, 2:4], axis=1)\n        # # Sort by current dipoles\n        ind = np.lexsort((reciprocals[:, 3], reciprocals[:, 2],\n                          reciprocals[:, 1], reciprocals[:, 0]))\n        reciprocals = reciprocals[ind]\n\n        if append:\n            self.configs = np.vstack((self.configs, reciprocals))\n        return reciprocals", "response": "Generate reciprocal configurations sort by AB and optionally append to configurations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_configs_permutate(self,\n                              injections_raw,\n                              only_same_dipole_length=False,\n                              ignore_crossed_dipoles=False,\n                              silent=False):\n        \"\"\" Create measurement configurations out of a pool of current\n        injections.  Use only the provided dipoles for potential dipole\n        selection. This means that we have always reciprocal measurements.\n\n        Remove quadpoles where electrodes are used both as current and voltage\n        dipoles.\n\n        Parameters\n        ----------\n        injections_raw : Nx2 array\n            current injections\n        only_same_dipole_length : bool, optional\n            if True, only generate permutations for the same dipole length\n        ignore_crossed_dipoles : bool, optional\n            If True, potential dipoles will be ignored that lie between current\n            dipoles,  e.g. 1-4 3-5. In this case it is possible to not have\n            full normal-reciprocal coverage.\n        silent: bool, optional\n            if True, do not print information on ignored configs (default:\n            False)\n\n        Returns\n        -------\n        configs : Nx4 array\n            quadrupoles generated out of the current injections\n\n        \"\"\"\n        injections = np.atleast_2d(injections_raw).astype(int)\n        N = injections.shape[0]\n\n        measurements = []\n\n        for injection in range(0, N):\n            dipole_length = np.abs(injections[injection][1] -\n                                   injections[injection][0])\n\n            # select all dipole EXCEPT for the injection dipole\n            for i in set(range(0, N)) - set([injection]):\n                test_dipole_length = np.abs(injections[i, :][1] -\n                                            injections[i, :][0])\n                if (only_same_dipole_length\n                        and test_dipole_length != dipole_length):\n                    continue\n                quadpole = np.array(\n                    [injections[injection, :], injections[i, :]]).flatten()\n                if ignore_crossed_dipoles is True:\n                    # check if we need to ignore this dipole\n                    # Note: this could be wrong if electrode number are not\n                    # ascending!\n                    if (quadpole[2] > quadpole[0]\n                            and quadpole[2] < quadpole[1]):\n                        if not silent:\n                            print('A - ignoring', quadpole)\n                    elif (quadpole[3] > quadpole[0]\n                          and quadpole[3] < quadpole[1]):\n                        if not silent:\n                            print('B - ignoring', quadpole)\n                    else:\n                        measurements.append(quadpole)\n                else:\n                    # add very quadpole\n                    measurements.append(quadpole)\n\n        # check and remove double use of electrodes\n        filtered = []\n        for quadpole in measurements:\n            if (not set(quadpole[0:2]).isdisjoint(set(quadpole[2:4]))):\n                if not silent:\n                    print('Ignoring quadrupole because of ',\n                          'repeated electrode use:', quadpole)\n            else:\n                filtered.append(quadpole)\n        self.add_to_configs(filtered)\n        return np.array(filtered)", "response": "Generates the measurement configurations for the given set of current energy modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving configurations with dipole separations higher than maxsep.", "response": "def remove_max_dipole_sep(self, maxsep=10):\n        \"\"\"Remove configurations with dipole separations higher than `maxsep`.\n\n        Parameters\n        ----------\n        maxsep : int\n            Maximum separation between both dipoles (the default is 10).\n        \"\"\"\n        sep = np.abs(self.configs[:, 1] - self.configs[:, 2])\n        self.configs = self.configs[sep <= maxsep]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_pg_scheme(self, container=None, positions=None):\n        if container is None and positions is None:\n            raise Exception('electrode positions are required for BERT export')\n\n        if container is not None and container.electrodes is None:\n            raise Exception('container does not contain electrode positions')\n\n        if container is not None and positions is not None:\n            raise Exception(\n                'only one of container OR positions must be provided')\n\n        if container is not None:\n            elec_positions = container.electrodes.values\n        elif positions is not None:\n            elec_positions = positions\n\n        opt_import(\"pybert\", requiredFor=\"\")\n        import pybert\n\n        # Initialize BERT DataContainer\n        data = pybert.DataContainerERT()\n\n        # Define electrodes (48 electrodes spaced by 0.5 m)\n        for nr, (x, y, z) in enumerate(elec_positions):\n            data.createSensor((x, y, z))\n\n        # Define number of measurements\n        data.resize(self.configs.shape[0])\n\n        for index, token in enumerate(\"abmn\"):\n            data.set(token, self.configs[:, index].tolist())\n\n        # account for zero indexing\n        for token in \"abmn\":\n            data.set(token, data(token) - 1)\n\n        # np.vstack([data.get(x).array() for x in (\"abmn\")]).T\n        return data", "response": "Convert the configuration to a pygimli measurement scheme."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexporting to IRIS Instrument configuration file", "response": "def to_iris_syscal(self, filename):\n        \"\"\"Export to IRIS Instrument configuration file\n\n        Parameters\n        ----------\n        filename : string\n            Path to output filename\n        \"\"\"\n        with open(filename, 'w') as fid:\n            # fprintf(fod, '#\\t X\\t Y\\t Z\\n');\n            fid.write('#\\t X\\t Y\\t Z\\n')\n            # fprintf(fod, '%d\\t %.1f\\t %d\\t %d\\n', D');\n            # loop over electrodes and assign increasing x-positions\n            # TODO: use proper electrode positions, if available\n            for nr in range(0, self.configs.max()):\n                fid.write('{}   {}  0   0\\n'.format(nr + 1, nr))\n\n            # fprintf(fod, '#\\t A\\t B\\t M\\t N\\n');\n            fid.write('#\\t A\\t B\\t M\\t N\\n')\n\n            # fprintf(fod, '%d\\t %d\\t %d\\t %d\\t %d\\n', C');\n            for nr, config in enumerate(self.configs):\n                fid.write('{}  {}  {}  {}  {}\\n'.format(nr + 1, *config))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_plan(self, *, plan_code, description, interval, interval_count, max_payments_allowed,\n                    payment_attempts_delay, plan_value, plan_tax, plan_tax_return_base, currency,\n                    max_payment_attempts=None, max_pending_payments=None, trial_days=None):\n        \"\"\"\n        Creating a new plan for subscriptions associated with the merchant.\n\n        Args:\n\n            plan_code: Unique code assigned by the merchant to the plan in order to identify it.\n            Alphanumeric. Min: 1 Max: 255.\n\n            description: Plan description.\n            Alphanumeric. Min: 1 Max: 255.\n\n            interval: Interval that defines how often the suscription payment is performed.\n            The possible values are: DAY, WEEK, MONTH y YEAR.\n            Alphanumeric. Min: 3 Max: 5.\n\n            interval_count: Interval count that defines how often the suscription payment is performed.\n            Numeric.\n\n            max_payments_allowed: Total amount of payments for the suscription.\n            Numeric.\n\n            payment_attempts_delay: Total amount of waiting days between the payment attempts of the suscription.\n            Numeric.\n\n            plan_value: total value of the plan.\n            Alphanumeric. Min: 1 Max: 255.\n\n            plan_tax: tax value associated to the value of the plan.\n            Alphanumeric. Min: 1 Max: 255.\n\n            plan_tax_return_base: tax return base value associated to the value of the plan.\n            Alphanumeric. Min: 1 Max: 255.\n\n            currency: The ISO currency code associated with the amount.\n            http://developers.payulatam.com/en/api/variables_table.html\n\n            max_payment_attempts: Total amount of payment attempts performed when a suscription payment is declined.\n            Numeric. Max: 3.\n\n            max_pending_payments: Total amount of pending payments that a suscription can have before it is cancelled.\n            Numeric.\n\n            trial_days: Total amount of trial days of the suscription.\n            Numeric.\n\n        Returns:\n\n        \"\"\"\n        payload = {\n            \"accountId\": self.client.account_id,\n            \"planCode\": plan_code,\n            \"description\": description,\n            \"interval\": interval,\n            \"intervalCount\": interval_count,\n            \"maxPaymentsAllowed\": max_payments_allowed,\n            \"paymentAttemptsDelay\": payment_attempts_delay,\n            \"additionalValues\": [\n                {\n                    \"name\": \"PLAN_VALUE\",\n                    \"value\": plan_value,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"PLAN_TAX\",\n                    \"value\": plan_tax,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"PLAN_TAX_RETURN_BASE\",\n                    \"value\": plan_tax_return_base,\n                    \"currency\": currency\n                }\n            ],\n            \"maxPaymentAttempts\": max_payment_attempts,\n            \"maxPendingPayments\": max_pending_payments,\n            \"trialDays\": trial_days\n        }\n        return self.client._post(self.url + 'plans', json=payload, headers=self.get_headers())", "response": "Creates a new plan for subscriptions associated with the merchant."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_plan(self, plan_code):\n        return self.client._get(self.url + 'plans/{}'.format(plan_code), headers=self.get_headers())", "response": "Check all the information of a plan for subscriptions associated with the merchant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an entire subscription plan associated with the merchant.", "response": "def delete_plan(self, plan_code):\n        \"\"\"\n        Delete an entire subscription plan associated with the merchant.\n\n        Args:\n            plan_code: Plan\u2019s identification code for the merchant.\n\n        Returns:\n\n        \"\"\"\n        return self.client._delete(self.url + 'plans/{}'.format(plan_code), headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_customer(self, *, full_name, email):\n        payload = {\n            \"fullName\": full_name,\n            \"email\": email\n        }\n        return self.client._post(self.url + 'customers', json=payload, headers=self.get_headers())", "response": "This method creates a customer in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_customer(self, customer_id):\n        return self.client._get(self.url + 'customers/{}'.format(customer_id), headers=self.get_headers())", "response": "Queries the information related to the customer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_customer(self, customer_id):\n        return self.client._delete(self.url + 'customers/{}'.format(customer_id), headers=self.get_headers())", "response": "Removes a user from the system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_credit_card(self, *, customer_id, name, document, number, exp_month, exp_year, type, address):\n        payload = {\n            \"name\": name,\n            \"document\": document,\n            \"number\": number,\n            \"expMonth\": exp_month,\n            \"expYear\": exp_year,\n            \"type\": type,\n            \"address\": address\n        }\n        fmt = 'customers/{}/creditCards'.format(customer_id)\n        return self.client._post(self.url + fmt, json=payload, headers=self.get_headers())", "response": "Creates a credit card for the user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_credit_card(self, credit_card_id):\n        return self.client._get(self.url + 'creditCards/{}'.format(credit_card_id), headers=self.get_headers())", "response": "Check the information of a credit card token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a credit card associated with a user.", "response": "def delete_credit_card(self, *, customer_id, credit_card_id):\n        \"\"\"\n        Delete a credit card (Token) associated with a user.\n\n        Args:\n            customer_id: Identifier of the client of whom you are going to delete the token.\n            credit_card_id: Identifier of the token to be deleted.\n\n        Returns:\n\n        \"\"\"\n        fmt = 'customers/{}/creditCards/{}'.format(customer_id, credit_card_id)\n        return self.client._delete(self.url + fmt, headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_subscription(self, *, customer_id, credit_card_token, plan_code, quantity=None, installments=None,\n                            trial_days=None, immediate_payment=None, extra1=None, extra2=None, delivery_address=None,\n                            notify_url=None, recurring_bill_items=None):\n        \"\"\"\n        Creating a new subscription of a client to a plan.\n\n        Args:\n            customer_id: Customer that will be associated to the subscription.\n            You can find more information in the \"Customer\" section of this page.\n\n            credit_card_token: Customer's credit card that is selected to make the payment.\n            You can find more information in the \"Credit card\" section of this page.\n\n            plan_code: Plan that will be associated to the subscription.\n            You can find more information in the \"Plan\" section of this page.\n\n            quantity: Total amount of plans that will be acquired with the subscription.\n            Numeric.\n\n            installments: Total amount of installments to defer the payment.\n            Numeric.\n\n            trial_days: Total amount of trial days of the subscription.\n            This variable has preference over the plan's trial days.\n            Numeric.\n\n            immediate_payment:\n\n            extra1:\n\n            extra2:\n\n            delivery_address:\n\n            notify_url:\n\n            recurring_bill_items:\n\n        Returns:\n\n        \"\"\"\n        payload = {\n            \"quantity\": quantity,\n            \"installments\": installments,\n            \"trialDays\": trial_days,\n            \"immediatePayment\": immediate_payment,\n            \"extra1\": extra1,\n            \"extra2\": extra2,\n            \"customer\": {\n                \"id\": customer_id,\n                \"creditCards\": [\n                    {\n                        \"token\": credit_card_token\n                    }\n                ]\n            },\n            \"plan\": {\n                \"planCode\": plan_code\n            },\n            \"deliveryAddress\": delivery_address,\n            \"notifyUrl\": notify_url,\n            \"recurringBillItems\": recurring_bill_items\n        }\n        return self.client._post(self.url + 'subscriptions', json=payload, headers=self.get_headers())", "response": "This method creates a new subscription of a client to a plan."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subscription(self, subscription_id):\n        return self.client._put(self.url + 'subscriptions/{}'.format(subscription_id), headers=self.get_headers())", "response": "Check the basic information associated with the specified subscription."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_subscription(self, *, subscription_id, credit_card_token):\n        payload = {\n            \"creditCardToken\": credit_card_token\n        }\n        fmt = 'subscriptions/{}'.format(subscription_id)\n        return self.client._put(self.url + fmt, json=payload, headers=self.get_headers())", "response": "Update the information associated with the specified subscription."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_additional_charge(self, *, subscription_id, description, plan_value, plan_tax, plan_tax_return_base,\n                                 currency):\n        \"\"\"\n        Adds extra charges to the respective invoice for the current period.\n\n        Args:\n            subscription_id: Identification of the subscription\n            description:\n            plan_value:\n            plan_tax:\n            plan_tax_return_base:\n            currency:\n\n        Returns:\n\n        \"\"\"\n        payload = {\n            \"description\": description,\n            \"additionalValues\": [\n                {\n                    \"name\": \"ITEM_VALUE\",\n                    \"value\": plan_value,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"ITEM_TAX\",\n                    \"value\": plan_tax,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"ITEM_TAX_RETURN_BASE\",\n                    \"value\": plan_tax_return_base,\n                    \"currency\": currency\n                }\n            ]\n        }\n        fmt = 'subscriptions/{}/recurringBillItems'.format(subscription_id)\n        return self.client._post(self.url + fmt, json=payload, headers=self.get_headers())", "response": "This method creates an additional charge for the current period."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the additional charge information of an invoice from its identifier.", "response": "def get_additional_charge_by_identifier(self, recurring_billing_id):\n        \"\"\"\n        Query extra charge information of an invoice from its identifier.\n\n        Args:\n            recurring_billing_id: Identifier of the additional charge.\n\n        Returns:\n\n        \"\"\"\n        fmt = 'recurringBillItems/{}'.format(recurring_billing_id)\n        return self.client._get(self.url + fmt, headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries extra charges of shop\u2019s invoices that meet the stipulated filters.", "response": "def get_additional_charge_by_description(self, description):\n        \"\"\"\n        Query extra charges of shop\u2019s invoices that meet the stipulated filters.\n\n        Args:\n            description: Description entered in the extra charge.\n\n        Returns:\n\n        \"\"\"\n        params = {\n            \"description\": description\n        }\n        return self.client._get(self.url + 'recurringBillItems/', params=params, headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_additional_charge(self, *, recurring_billing_id, description, plan_value, plan_tax, plan_tax_return_base,\n                                 currency):\n        \"\"\"\n        Updates the information from an additional charge in an invoice.\n\n        Args:\n            recurring_billing_id: Identifier of the additional charge.\n            description:\n            plan_value:\n            plan_tax:\n            plan_tax_return_base:\n            currency:\n\n        Returns:\n\n        \"\"\"\n        payload = {\n            \"description\": description,\n            \"additionalValues\": [\n                {\n                    \"name\": \"ITEM_VALUE\",\n                    \"value\": plan_value,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"ITEM_TAX\",\n                    \"value\": plan_tax,\n                    \"currency\": currency\n                },\n                {\n                    \"name\": \"ITEM_TAX_RETURN_BASE\",\n                    \"value\": plan_tax_return_base,\n                    \"currency\": currency\n                }\n            ]\n        }\n        fmt = 'recurringBillItems/{}'.format(recurring_billing_id)\n        return self.client._put(self.url + fmt, payload=payload, headers=self.get_headers())", "response": "Updates the information from an additional charge in an invoice."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove an additional charge from an invoice.", "response": "def delete_additional_charge(self, recurring_billing_id):\n        \"\"\"\n        Remove an extra charge from an invoice.\n\n        Args:\n            recurring_billing_id: Identifier of the additional charge.\n\n        Returns:\n\n        \"\"\"\n        fmt = 'recurringBillItems/{}'.format(recurring_billing_id)\n        return self.client._delete(self.url + fmt, headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_recurring_bill_by_client(self, *, customer_id, date_begin=None, date_final=None):\n        params = {\n            \"customerId\": customer_id,\n        }\n        if date_begin and date_final:\n            params['dateBegin'] = date_begin.strftime('%Y-%m-%d')\n            params['dateFinal'] = date_final.strftime('%Y-%m-%d')\n        return self.client._get(self.url + 'recurringBill', params=params, headers=self.get_headers())", "response": "Get a list of all the recurring bills for a given customer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a list of all the recurring bills for a given subscription.", "response": "def get_recurring_bill_by_subscription(self, subscription_id):\n        \"\"\"\n        Consulta de las facturas que est\u00e1n pagadas o pendientes por pagar. Se puede consultar por cliente,\n        por suscripci\u00f3n o por rango de fechas.\n\n        Args:\n            subscription_id:\n\n        Returns:\n\n        \"\"\"\n        params = {\n            \"subscriptionId\": subscription_id,\n        }\n        return self.client._get(self.url + 'recurringBill', params=params, headers=self.get_headers())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_args(args):\n    if not args:\n        return {}\n\n    # Handle the old comma separated argument format.\n    if len(args) == 1 and not REGEXP_ARGS.search(args[0]):\n        args = args[0].split(',')\n\n    # Separate out the key and value for each argument.\n    args_dict = {}\n    for arg in args:\n        split_arg = arg.split('=', 1)\n        value = len(split_arg) > 1 and split_arg[1] or None\n        args_dict[split_arg[0]] = value\n\n    return args_dict", "response": "Split a list of argument strings into a dictionary where each key is an anonymized argument name and each value is the value of the argument."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a thumbnail of an image field.", "response": "def thumbnail(parser, token):\n    \"\"\"\n    Creates a thumbnail of for an ImageField.\n\n    To just output the absolute url to the thumbnail::\n\n        {% thumbnail image 80x80 %}\n\n    After the image path and dimensions, you can put any options::\n\n        {% thumbnail image 80x80 force_ssl=True %}\n\n    To put the thumbnail URL on the context instead of just rendering\n    it, finish the tag with ``as [context_var_name]``::\n\n        {% thumbnail image 80x80 as thumb %}\n        <img src=\"{{thumb}}\" />\n    \"\"\"\n    args = token.split_contents()\n    tag = args[0]\n    # Check to see if we're setting to a context variable.\n    if len(args) > 4 and args[-2] == 'as':\n        context_name = args[-1]\n        args = args[:-2]\n    else:\n        context_name = None\n\n    if len(args) < 3:\n        raise TemplateSyntaxError(\"Invalid syntax. Expected \"\n            \"'{%% %s source size [option1 option2 ...] %%}' or \"\n            \"'{%% %s source size [option1 option2 ...] as variable %%}'\" %\n            (tag, tag))\n\n    # Get the source image path and requested size.\n\n    source_var = args[1]\n    # If the size argument was a correct static format, wrap it in quotes so\n    # that it is compiled correctly.\n    m = REGEXP_THUMB_SIZES.match(args[2])\n    if m:\n        args[2] = '\"%s\"' % args[2]\n    size_var = args[2]\n\n    # Get the options.\n    args_list = split_args(args[3:]).items()\n\n    # Check the options.\n    opts = {}\n    kwargs = {} # key,values here override settings and defaults\n\n    for arg, value in args_list:\n        value = value and parser.compile_filter(value)\n        if arg in TAG_SETTINGS and value is not None:\n            kwargs[str(arg)] = value\n            continue\n        else:\n            raise TemplateSyntaxError(\"'%s' tag received a bad argument: \"\n                                      \"'%s'\" % (tag, arg))\n    return ThumbnailNode(source_var, size_var, opts=opts,\n                         context_name=context_name, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nswapping electrode denotations so that geometrical factors become positive.", "response": "def fix_sign_with_K(dataframe):\n    \"\"\"Swap electrode denotations so that geometrical (K) factors become\n    positive. Also, swap signs of all parameters affected by this process.\n\n    Affected parameters, at the moment, are:\n\n        * K\n        * r\n        * Vmn\n        * Zt\n        * rho_a\n        * rpha\n\n    Parameters\n    ----------\n    dataframe : pandas.DateFrame\n        dataframe holding the data\n\n    Returns\n    -------\n    dataframe : pandas.DateFrame\n        the fixed dataframe\n\n    \"\"\"\n    # check for required columns\n    if 'k' not in dataframe or 'r' not in dataframe:\n        raise Exception('k and r columns required!')\n\n    indices_negative = (dataframe['k'] < 0) & (dataframe['r'] < 0)\n    if np.where(indices_negative)[0].size == 0:\n        # nothing to do here\n        return dataframe\n\n    dataframe.ix[indices_negative, ['k', 'r']] *= -1\n\n    # switch potential electrodes\n    indices_switched_ab = indices_negative & (dataframe['a'] > dataframe['b'])\n    indices_switched_mn = indices_negative & (dataframe['a'] < dataframe['b'])\n\n    dataframe.ix[indices_switched_ab, ['a', 'b']] = dataframe.ix[\n        indices_switched_ab, ['b', 'a']\n    ].values\n\n    dataframe.ix[indices_switched_mn, ['m', 'n']] = dataframe.ix[\n        indices_switched_mn, ['n', 'm']\n    ].values\n\n    # switch sign of voltages\n    if 'Vmn' in dataframe:\n        dataframe.ix[indices_negative, 'Vmn'] *= -1\n\n    if 'Zt' in dataframe:\n        dataframe.ix[indices_negative, 'Zt'] *= -1\n\n    if 'rho_a' in dataframe:\n        dataframe['rho_a'] = dataframe['r'] * dataframe['k']\n\n    if 'Mx' in dataframe:\n        # for now we have to loop here because we store numpy arrays within\n        # each cell\n        for index in np.where(indices_negative)[0]:\n            # import IPython\n            # IPython.embed()\n            # exit()\n            dataframe.at[index, 'Mx'] *= -1\n\n    # recompute phase values\n    if 'rpha' in dataframe:\n        if 'Zt' in dataframe:\n            # recompute\n            dataframe['rpha'] = np.arctan2(\n                dataframe['Zt'].imag, dataframe['Zt'].real\n            ) * 1e3\n        else:\n            raise Exception(\n                'Recomputation of phase without Zt not implemented yet. ' +\n                'See source code for more information'\n            )\n            \"\"\"\n            when the complex number is located in the fourth sector instead of\n            the first, this corresponds to a phase shift by pi. For all values\n            where magnitude < 0 and phase < 3000 mrad reverse this shift by pi\n            by multiplying the complex number by -1:\n            new_value = - 1 * (Magnitude * exp(i phi))\n            Test this function by setting one measurement to\n            -85.02069 -183.25 in radic column 6 and 7, should get -58 mrad when\n            converted\n            \"\"\"\n    # Make sure a, b, m, n stay integers.\n    for col in ('a', 'b', 'm', 'n'):\n        dataframe[col] = dataframe[col].astype(int)\n\n    return dataframe"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_cropping_offset(crop, epsilon):\n    m = _CROP_PERCENT_PATTERN.match(crop)\n    if not m:\n        raise ThumbnailParseError('Unrecognized crop option: %s' % crop)\n    value = int(m.group('value')) # we only take ints in the regexp\n    unit = m.group('unit')\n    if unit == '%':\n        value = epsilon * value / 100.0\n        # return \u2208 [0, epsilon]\n    return int(max(0, min(value, epsilon)))", "response": "Calculates the cropping offset for the cropped image."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the crop string and returns x y offsets for the image and window area of the image.", "response": "def parse_crop(crop, xy_image, xy_window):\n    \"\"\"\n    Returns x, y offsets for cropping. The window area should fit inside\n    image but it works out anyway\n\n    :param str crop: A cropping offset string. This is either one or two\n        space-separated values. If only one value is specified, the cropping\n        amount (pixels or percentage) for both X and Y dimensions is the\n        amount given. If two values are specified, X and Y dimension cropping\n        may be set independently. Some examples: '50% 50%', '50px 20px',\n        '50%', '50px'.\n    :param tuple xy_image: The (x,y) dimensions of the image.\n    :param tuple xy_window: The desired dimensions (x,y) of the cropped image.\n    :raises: ThumbnailParseError in the event of invalid input.\n    :rtype: tuple of ints\n    :returns: A tuple of of offsets for cropping, in (x,y) format.\n    \"\"\"\n    # Cropping percentages are space-separated by axis. For example:\n    # '50% 75%' would be a 50% cropping ratio for X, and 75% for Y.\n    xy_crop = crop.split(' ')\n    if len(xy_crop) == 1:\n        # Only one dimension was specified, use the same for both planes.\n        if crop in _X_ALIAS_PERCENT:\n            x_crop = _X_ALIAS_PERCENT[crop]\n            y_crop = '50%'\n        elif crop in _Y_ALIAS_PERCENT:\n            y_crop = _Y_ALIAS_PERCENT[crop]\n            x_crop = '50%'\n        else:\n            x_crop, y_crop = crop, crop\n    elif len(xy_crop) == 2:\n        # Separate X and Y cropping percentages specified.\n        x_crop, y_crop = xy_crop\n        x_crop = _X_ALIAS_PERCENT.get(x_crop, x_crop)\n        y_crop = _Y_ALIAS_PERCENT.get(y_crop, y_crop)\n    else:\n        raise ThumbnailParseError('Unrecognized crop option: %s' % crop)\n\n    # We now have cropping percentages for the X and Y planes.\n    # Calculate the cropping offsets (in pixels) for each plane.\n    offset_x = get_cropping_offset(x_crop, xy_image[0] - xy_window[0])\n    offset_y = get_cropping_offset(y_crop, xy_image[1] - xy_window[1])\n    return offset_x, offset_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _computeStats(self):\n\n        # Initialize the local max and min\n        _clipmin = self.lower\n        _clipmax = self.upper\n\n        # Compute the clipped mean iterating the user specified numer of iterations\n        for iter in range(self.nclip+1):\n\n            try:\n                _npix,_mean,_stddev,_min,_max = computeMean(self.image,_clipmin,_clipmax)\n                #print(\"_npix,_mean,_stddev,_min,_max = \",_npix,_mean,_stddev,_min,_max)\n            except:\n                raise SystemError(\"An error processing the array object information occured in \\\n                                    the computeMean module of imagestats.\")\n\n            if _npix <= 0:\n                # Compute Global minimum and maximum\n                errormsg = self._error_no_valid_pixels(\n                    iter, self.min, self.max,\n                    _clipmin, _clipmax\n                )\n                print(errormsg)\n                raise ValueError(\"Not enough data points to compute statistics.\")\n\n            if iter < self.nclip:\n                # Re-compute limits for iterations\n                _clipmin = max(self.lower, _mean - self.lsig * _stddev)\n                _clipmax = min(self.upper, _mean + self.usig * _stddev)\n\n        if self.fields.find('median') != -1:\n            # Use the clip range to limit the data before computing\n            #  the median value using numpy\n            if self.nclip > 0:\n                _image = self.image[(self.image <= _clipmax) & (self.image >= _clipmin)]\n            else:\n                _image = self.image\n            self.median = np.median(_image)\n            # clean-up intermediate product since it is no longer needed\n            del _image\n\n        if ( (self.fields.find('mode') != -1) or (self.fields.find('midpt') != -1) ):\n            # Populate the historgram\n            _hwidth = self.binwidth * _stddev\n            _drange = _max - _min\n            _minfloatval = 10.0 * np.finfo(dtype=np.float32).eps\n            if _hwidth < _minfloatval or abs(_drange) < _minfloatval or \\\n               _hwidth > _drange:\n                _nbins = 1\n                _dz = _drange\n                print(\"! WARNING: Clipped data falls within 1 histogram bin\")\n            else:\n                _nbins = int( (_max - _min) / _hwidth ) + 1\n                _dz = float(_max - _min) / float(_nbins - 1)\n\n            _hist = histogram1d(self.image, _nbins, _dz, _min)\n            self._hist = _hist\n            _bins = _hist.histogram\n\n            if (self.fields.find('mode') != -1):\n                # Compute the mode, taking into account special cases\n                if _nbins == 1:\n                    _mode = _min + 0.5 *_hwidth\n                elif _nbins == 2:\n                    if _bins[0] > _bins[1]:\n                        _mode = _min + 0.5 *_hwidth\n                    elif _bins[0] < _bins[1]:\n                        _mode = _min + 1.5 *_hwidth\n                    else:\n                        _mode = _min + _hwidth\n                else:\n                    _peakindex = np.where(_bins == np.maximum.reduce(_bins))[0].tolist()[0]\n\n                    if _peakindex == 0:\n                        _mode = _min + 0.5 * _hwidth\n                    elif _peakindex == (_nbins - 1):\n                        _mode = _min + (_nbins - 0.5) * _hwidth\n                    else:\n                        _dh1 = _bins[_peakindex] - _bins[_peakindex - 1]\n                        _dh2 = _bins[_peakindex] - _bins[_peakindex + 1]\n                        _denom = _dh1 + _dh2\n                        if _denom == 0:\n                            _mode = _min + (_peakindex + 0.5) * _hwidth\n                        else:\n                            _mode = _peakindex + 1 + (0.5 * (int(_dh1) - int(_dh2))/_denom)\n                            _mode = _min + ((_mode - 0.5) * _hwidth)\n                # Return the mode\n                self.mode = _mode\n\n            if (self.fields.find('midpt') != -1):\n                # Compute a pseudo-Median Value using IRAF's algorithm\n                _binSum = np.cumsum(_bins).astype(np.float32)\n                _binSum = _binSum/_binSum[-1]\n                _lo = np.where(_binSum >= 0.5)[0][0]\n                _hi = _lo + 1\n\n                _h1 = _min + _lo * _hwidth\n                if (_lo == 0):\n                    _hdiff = _binSum[_hi-1]\n                else:\n                    _hdiff = _binSum[_hi-1] - _binSum[_lo-1]\n\n                if (_hdiff == 0):\n                    _midpt = _h1\n                elif (_lo == 0):\n                    _midpt = _h1 + 0.5 / _hdiff * _hwidth\n                else:\n                    _midpt = _h1 + (0.5 - _binSum[_lo-1])/_hdiff * _hwidth\n                self.midpt = _midpt\n\n            # These values will only be returned if the histogram is computed.\n            self.hmin = _min + 0.5 * _hwidth\n            self.hwidth = _hwidth\n            self.histogram  = _bins\n\n        #Return values\n        self.stddev = _stddev\n        self.mean = _mean\n        self.npix = _npix\n        self.min = _min\n        self.max = _max", "response": "Compute all the basic statistics from the array object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef printStats(self):\n        print(\"--- Imagestats Results ---\")\n\n        if (self.fields.find('npix') != -1 ):\n            print(\"Number of pixels  :  \",self.npix)\n        if (self.fields.find('min') != -1 ):\n            print(\"Minimum value     :  \",self.min)\n        if (self.fields.find('max') != -1 ):\n            print(\"Maximum value     :  \",self.max)\n        if (self.fields.find('stddev') != -1 ):\n            print(\"Standard Deviation:  \",self.stddev)\n        if (self.fields.find('mean') != -1 ):\n            print(\"Mean              :  \",self.mean)\n        if (self.fields.find('mode') != -1 ):\n            print(\"Mode              :  \",self.mode)\n        if (self.fields.find('median') != -1 ):\n            print(\"Median            :  \",self.median)\n        if (self.fields.find('midpt') != -1 ):\n            print(\"Midpt            :  \",self.midpt)", "response": "Prints the requested statistics values for those fields specified on input."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raw_request(self, method, uri, **kwargs):\n        with warnings.catch_warnings():  # catch warning about certs not being verified\n            warnings.simplefilter(\"ignore\", urllib3.exceptions.InsecureRequestWarning)\n            warnings.simplefilter(\"ignore\", urllib3.exceptions.InsecurePlatformWarning)\n            try:\n                response = self._get_session().request(method, self._get_ws_url(uri), **kwargs)\n            except requests.RequestException as e:\n                # e.g. raise new_exc from old_exc\n                six.raise_from(WVAHttpRequestError(e), e)\n            else:\n                return response", "response": "Perform a WVA request and return the raw response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a WVA request and return the decoded value if successful.", "response": "def request(self, method, uri, **kwargs):\n        \"\"\"Perform a WVA web services request and return the decoded value if successful\n\n        :param method: The HTTP method to use when making this request\n        :param uri: The path past /ws to request.  That is, the path requested for\n            a relpath of `a/b/c` would be `/ws/a/b/c`.\n        :raises WVAHttpError: if a response is received but the success is non-success\n        :raises WVAHttpSocketError: if there was an error making the HTTP request.  That is,\n            the request was unable to make it to the WVA for some reason.\n        :return: If the response content type is JSON, it will be deserialized and a\n            python dictionary containing the information from the json document will\n            be returned.  If not a JSON response, a unicode string of the response\n            text will be returned.\n        \"\"\"\n        response = self.raw_request(method, uri, **kwargs)\n        if response.status_code != 200:\n            exception_class = HTTP_STATUS_EXCEPTION_MAP.get(response.status_code, WVAHttpError)\n            raise exception_class(response)\n\n        if response.headers.get(\"content-type\") == \"application/json\":\n            return json.loads(response.text)\n        else:\n            return response.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post(self, uri, data, **kwargs):\n        return self.request(\"POST\", uri, data=data, **kwargs)", "response": "POST the provided data to the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post_json(self, uri, data, **kwargs):\n        encoded_data = json.dumps(data)\n        kwargs.setdefault(\"headers\", {}).update({\n            \"Content-Type\": \"application/json\",  # tell server we are sending json\n        })\n        return self.post(uri, data=encoded_data, **kwargs)", "response": "POST the provided data as json to the specified path\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put(self, uri, data, **kwargs):\n        return self.request(\"PUT\", uri, data=data, **kwargs)", "response": "PUT the provided data to the specified path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef put_json(self, uri, data, **kwargs):\n        encoded_data = json.dumps(data)\n        kwargs.setdefault(\"headers\", {}).update({\n            \"Content-Type\": \"application/json\",  # tell server we are sending json\n        })\n        return self.put(uri, data=encoded_data, **kwargs)", "response": "PUT the provided data as json to the specified path\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntranslates a MAC address into an IPv6 address in the prefixed network.", "response": "def mac_to_ipv6_linklocal(mac,prefix=\"fe80::\"):\n    \"\"\" Translate a MAC address into an IPv6 address in the prefixed network.\n\n    This function calculates the EUI (Extended Unique Identifier) from the given\n    MAC address and prepend the needed prefix to come up with a valid IPv6 address.\n    The default prefix is the link local prefix defined by RFC 4291 .\n\n        :param mac: the mac address of the device\n        :type mac: str\n        :param prefix: the IPv6 network prefix\n        :type prefix: str\n        :returns: IPv6 address\n        :rtype: str\n\n    \"\"\"\n\n    # Remove the most common delimiters; dots, dashes, etc.\n    mac_value = int(mac.translate(str.maketrans(dict([(x,None) for x in [\" \",\".\",\":\",\"-\"]]))),16)\n    # Split out the bytes that slot into the IPv6 address\n    # XOR the most significant byte with 0x02, inverting the\n    # Universal / Local bit\n    high2 = mac_value >> 32 & 0xffff ^ 0x0200\n    high1 = mac_value >> 24 & 0xff\n    low1 = mac_value >> 16 & 0xff\n    low2 = mac_value & 0xffff\n    return prefix+':{:04x}:{:02x}ff:fe{:02x}:{:04x}'.format(\n        high2, high1, low1, low2)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datagram_received(self, data, addr):\n        self.register()\n        response = unpack_lifx_message(data)\n        self.lastmsg=datetime.datetime.now()\n        if response.seq_num in self.message:\n            response_type,myevent,callb = self.message[response.seq_num]\n            if type(response) == response_type:\n                if response.source_id == self.source_id:\n                    if \"State\" in response.__class__.__name__:\n                        setmethod=\"resp_set_\"+response.__class__.__name__.replace(\"State\",\"\").lower()\n                        if setmethod in dir(self) and callable(getattr(self,setmethod)):\n                            getattr(self,setmethod)(response)\n                    if callb:\n                        callb(self,response)\n                    myevent.set()\n                del(self.message[response.seq_num])\n            elif type(response) == Acknowledgement:\n                pass\n            else:\n                del(self.message[response.seq_num])\n        elif self.default_callb:\n            self.default_callb(response)", "response": "This method is run when data is received from the device. It will unpack the data according to the LIFX protocol and then execute the appropriate callback corresponding\n            to the message sequence number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(self):\n        if not self.registered:\n            self.registered = True\n            if self.parent:\n                self.parent.register(self)", "response": "Proxy method to register the device with the parent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cleanup(self):\n        if self.transport:\n            self.transport.close()\n            self.transport = None\n        if self.task:\n            self.task.cancel()\n            self.task = None", "response": "Method to cleanly terminate the connection to the device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def fire_sending(self,msg,num_repeats):\n        if num_repeats is None:\n            num_repeats = self.retry_count\n        sent_msg_count = 0\n        sleep_interval = 0.05\n        while(sent_msg_count < num_repeats):\n            if self.transport:\n                self.transport.sendto(msg.packed_message)\n            sent_msg_count += 1\n            await aio.sleep(sleep_interval)", "response": "Coroutine used to send a message to the device when no response is needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fire_and_forget(self, msg_type, payload={}, timeout_secs=None, num_repeats=None):\n        msg = msg_type(self.mac_addr, self.source_id, seq_num=0, payload=payload, ack_requested=False, response_requested=False)\n        xx=self.loop.create_task(self.fire_sending(msg,num_repeats))\n        return True", "response": "Method used to send a message to the device when no response or ack is needed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef req_with_ack(self, msg_type, payload, callb = None, timeout_secs=None, max_attempts=None):\n        msg = msg_type(self.mac_addr, self.source_id, seq_num=self.seq_next(), payload=payload, ack_requested=True, response_requested=False)\n        self.message[msg.seq_num]=[Acknowledgement,None,callb]\n        xx=self.loop.create_task(self.try_sending(msg,timeout_secs, max_attempts))\n        return True", "response": "Method to send a message expecting to receive an ACK."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_label(self, value,callb=None):\n        if len(value) > 32:\n            value = value[:32]\n        mypartial=partial(self.resp_set_label,label=value)\n        if callb:\n            self.req_with_ack(SetLabel, {\"label\": value},lambda x,y:(mypartial(y),callb(x,y)) )\n        else:\n            self.req_with_ack(SetLabel, {\"label\": value},lambda x,y:mypartial(y) )", "response": "Convenience method to set the label of the device."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting callback for get_label and set_label", "response": "def resp_set_label(self, resp, label=None):\n        \"\"\"Default callback for get_label/set_label\n        \"\"\"\n        if label:\n            self.label=label\n        elif resp:\n            self.label=resp.label.decode().replace(\"\\x00\", \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resp_set_location(self, resp, location=None):\n        if location:\n            self.location=location\n        elif resp:\n            self.location=resp.label.decode().replace(\"\\x00\", \"\")", "response": "Default callback for get_location and set_location"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_group(self,callb=None):\n        if self.group is None:\n            mypartial=partial(self.resp_set_group)\n            if callb:\n                mycallb=lambda x,y:(mypartial(y),callb(x,y))\n            else:\n                mycallb=lambda x,y:mypartial(y)\n            response = self.req_with_resp(GetGroup, StateGroup, callb=callb )\n        return self.group", "response": "Convenience method to request the group from the device"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefault callback for get_group and set_group", "response": "def resp_set_group(self, resp, group=None):\n        \"\"\"Default callback for get_group/set_group\n        \"\"\"\n        if group:\n            self.group=group\n        elif resp:\n            self.group=resp.label.decode().replace(\"\\x00\", \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefaults callback for get_power and set_power", "response": "def resp_set_power(self, resp, power_level=None):\n        \"\"\"Default callback for get_power/set_power\n        \"\"\"\n        if power_level is not None:\n            self.power_level=power_level\n        elif resp:\n            self.power_level=resp.power_level"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefault callback for get_wififirmware", "response": "def resp_set_wififirmware(self, resp):\n        \"\"\"Default callback for get_wififirmware\n        \"\"\"\n        if resp:\n            self.wifi_firmware_version = float(str(str(resp.version >> 16) + \".\" + str(resp.version & 0xff)))\n            self.wifi_firmware_build_timestamp = resp.build"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_wifiinfo(self,callb=None):\n        response = self.req_with_resp(GetWifiInfo, StateWifiInfo,callb=callb )\n        return None", "response": "Convenience method to request the wifi info from the device"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hostfirmware(self,callb=None):\n        if self.host_firmware_version is None:\n            mypartial=partial(self.resp_set_hostfirmware)\n            if callb:\n                mycallb=lambda x,y:(mypartial(y),callb(x,y))\n            else:\n                mycallb=lambda x,y:mypartial(y)\n            response = self.req_with_resp(GetHostFirmware, StateHostFirmware,mycallb )\n        return (self.host_firmware_version,self.host_firmware_build_timestamp)", "response": "Convenience method to request the device firmware info from the device."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resp_set_hostfirmware(self, resp):\n        if resp:\n            self.host_firmware_version = float(str(str(resp.version >> 16) + \".\" + str(resp.version & 0xff)))\n            self.host_firmware_build_timestamp = resp.build", "response": "Default callback for get_hostfirmware\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_hostinfo(self,callb=None):\n        response = self.req_with_resp(GetInfo, StateInfo,callb=callb )\n        return None", "response": "Convenience method to request the device info from the device and return the host info."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resp_set_version(self, resp):\n        if resp:\n            self.vendor = resp.vendor\n            self.product = resp.product\n            self.version = resp.version", "response": "Default callback for get_version\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef device_product_str(self, indent):\n        s = \"Vendor: {}\\n\".format(self.vendor)\n        s += indent + \"Product: {}\\n\".format((self.product and product_map[self.product]) or \"Unknown\")\n        s += indent + \"Version: {}\\n\".format(self.version)\n        return s", "response": "Convenience to string method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef device_radio_str(self, resp, indent=\"  \"):\n        signal = resp.signal\n        tx = resp.tx\n        rx = resp.rx\n        s = \"Wifi Signal Strength (mW): {}\\n\".format(signal)\n        s += indent + \"Wifi TX (bytes): {}\\n\".format(tx)\n        s += indent + \"Wifi RX (bytes): {}\\n\".format(rx)\n        return s", "response": "Convenience to string method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefaulting callback for set_power", "response": "def resp_set_lightpower(self, resp, power_level=None):\n        \"\"\"Default callback for set_power\n        \"\"\"\n        if power_level is not None:\n            self.power_level=power_level\n        elif resp:\n            self.power_level=resp.power_level"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_color(self,callb=None):\n        response = self.req_with_resp(LightGet, LightState, callb=callb)\n        return self.color", "response": "Convenience method to request the colour status from the device"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefault callback for set_color", "response": "def resp_set_light(self, resp, color=None):\n        \"\"\"Default callback for set_color\n        \"\"\"\n        if color:\n            self.color=color\n        elif resp:\n            self.power_level = resp.power_level\n            self.color = resp.color\n            self.label = resp.label.decode().replace(\"\\x00\", \"\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_color_zones(self, start_index, end_index=None, callb=None):\n        if end_index is None:\n            end_index = start_index + 7\n        args = {\n            \"start_index\": start_index,\n            \"end_index\": end_index,\n        }\n        self.req_with_resp(MultiZoneGetColorZones, MultiZoneStateMultiZone, payload=args, callb=callb)", "response": "Convenience method to request the state of colour by zones from the device."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resp_set_multizonemultizone(self, resp, args=None):\n        if args:\n            if self.color_zones:\n                for i in range(args[\"start_index\"], args[\"end_index\"]+1):\n                    self.color_zones[i] = args[\"color\"]\n        elif resp:\n            if self.color_zones is None:\n                self.color_zones = [None] * resp.count\n            for i in range(resp.index, min(resp.index+8, resp.count)):\n                self.color_zones[i] = resp.color[i-resp.index]", "response": "Default callback for get - color_zones and set - color_zones"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_waveform(self, value, callb=None, rapid=False):\n        if \"color\" in value and len(value[\"color\"]) == 4:\n            if rapid:\n                self.fire_and_forget(LightSetWaveform, value, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetWaveform, value, callb=callb)", "response": "This method will animate the light and send a SetPower message to the device."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_waveform_optional(self, value, callb=None, rapid=False):\n        if \"color\" in value and len(value[\"color\"]) == 4:\n            if rapid:\n                self.fire_and_forget(LightSetWaveformOptional, value, num_repeats=1)\n            else:\n                self.req_with_ack(LightSetWaveformOptional, value, callb=callb)", "response": "This method will animate the light to a specific waveform."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_infrared(self,callb=None):\n        response = self.req_with_resp(LightGetInfrared, LightStateInfrared,callb=callb)\n        return self.infrared_brightness", "response": "Convenience method to request the infrared brightness from the device"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefault callback for set_infrared", "response": "def resp_set_infrared(self, resp, infrared_brightness=None):\n        \"\"\"Default callback for set_infrared/get_infrared\n        \"\"\"\n        if infrared_brightness is not None:\n            self.infrared_brightness = infrared_brightness\n        elif resp:\n            self.infrared_brightness = resp.infrared_brightness"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connection_made(self, transport):\n        #print('started')\n        self.transport = transport\n        sock = self.transport.get_extra_info(\"socket\")\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_BROADCAST, 1)\n        self.loop.call_soon(self.discover)", "response": "Method run when the UDP broadcast server is started\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef datagram_received(self, data, addr):\n        response = unpack_lifx_message(data)\n        response.ip_addr = addr[0]\n\n        mac_addr = response.target_addr\n        if mac_addr == BROADCAST_MAC:\n            return\n\n        if type(response) == StateService and response.service == 1: # only look for UDP services\n            # discovered\n            remote_port = response.port\n        elif type(response) == LightState:\n            # looks like the lights are volunteering LigthState after booting\n            remote_port = UDP_BROADCAST_PORT\n        else:\n            return\n\n        if self.ipv6prefix:\n            family = socket.AF_INET6\n            remote_ip = mac_to_ipv6_linklocal(mac_addr, self.ipv6prefix)\n        else:\n            family = socket.AF_INET\n            remote_ip = response.ip_addr\n\n        if mac_addr in self.lights:\n            # rediscovered\n            light = self.lights[mac_addr]\n\n            # nothing to do\n            if light.registered:\n                return\n\n            light.cleanup()\n            light.ip_addr = remote_ip\n            light.port = remote_port\n        else:\n            # newly discovered\n            light = Light(self.loop, mac_addr, remote_ip, remote_port, parent=self)\n            self.lights[mac_addr] = light\n\n        coro = self.loop.create_datagram_endpoint(\n            lambda: light, family=family, remote_addr=(remote_ip, remote_port))\n\n        light.task = self.loop.create_task(coro)", "response": "This method is called when data is received from the devices and returns the object that represents the new instance of the class that is associated with the new device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discover(self):\n        if self.transport:\n            if self.discovery_countdown <= 0:\n                self.discovery_countdown = self.discovery_interval\n                msg = GetService(BROADCAST_MAC, self.source_id, seq_num=0, payload={}, ack_requested=False, response_requested=True)\n                self.transport.sendto(msg.generate_packed_message(), (self.broadcast_ip, UDP_BROADCAST_PORT))\n            else:\n                self.discovery_countdown -= self.discovery_step\n            self.loop.call_later(self.discovery_step, self.discover)", "response": "Method to send a discovery message to the source"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cleanup(self):\n        if self.transport:\n            self.transport.close()\n            self.transport = None\n        if self.task:\n            self.task.cancel()\n            self.task = None\n        for light in self.lights.values():\n            light.cleanup()\n        self.lights = {}", "response": "Method to cleanly terminate the connection to the device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of local IP addresses on interfaces with LIFX bulbs.", "response": "async def scan(self, timeout=1):\n        \"\"\"Return a list of local IP addresses on interfaces with LIFX bulbs.\"\"\"\n        adapters = await self.loop.run_in_executor(None, ifaddr.get_adapters)\n        ips = [ip.ip for adapter in ifaddr.get_adapters() for ip in adapter.ips if ip.is_IPv4]\n\n        if not ips:\n            return []\n\n        tasks = []\n        discoveries = []\n        for ip in ips:\n            manager = ScanManager(ip)\n            lifx_discovery = LifxDiscovery(self.loop, manager)\n            discoveries.append(lifx_discovery)\n            lifx_discovery.start(listen_ip=ip)\n            tasks.append(self.loop.create_task(manager.lifx_ip()))\n\n        (done, pending) = await aio.wait(tasks, timeout=timeout)\n\n        for discovery in discoveries:\n            discovery.cleanup()\n\n        for task in pending:\n            task.cancel()\n\n        return [task.result() for task in done]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_file_version(filename):\n    mat = sio.loadmat(filename, squeeze_me=True)\n    version = mat['MP']['Version'].item()\n    del(mat)\n\n    return version", "response": "High level import function that tries to determine the specific version of the file used by the emmt_pp. exe postprocessing program."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a MD DataFrame return a Nx4 array which permutes the current injection dipoles.", "response": "def MD_ConfigsPermutate(df_md):\n    \"\"\"Given a MD DataFrame, return a Nx4 array which permutes the current\n    injection dipoles.\n    \"\"\"\n    g_current_injections = df_md.groupby(['a', 'b'])\n    ab = np.array(list(g_current_injections.groups.keys()))\n    config_mgr = ConfigManager(nr_of_electrodes=ab.max())\n    config_mgr.gen_configs_permutate(ab, silent=True)\n    return config_mgr.configs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport data post - processed as 3P data ( NMU0. mat file and returns the data in a 2D pandas. DataFrame.", "response": "def get_mnu0_data(filename, configs, return_3p=False, **kwargs):\n    \"\"\"Import data post-processed as 3P data (NMU0), i.e., measured towards\n    common ground.\n\n    Parameters\n    ----------\n    filename : string (usually: eit_data_mnu0.mat)\n        filename of matlab file\n    configs : Nx4 numpy.ndarray|filename|function\n        4P measurements configurations (ABMN) to generate out of the data. If\n        this parameter is a callable, then call it with the MD DataFrame as its\n        sole parameter and expect a Nx4 numpy.ndarray as return value\n    return_3p : bool, optional\n        also return 3P data\n    **kwargs : dict, optional\n        all other parameters will be handled down to the extract functions\n\n    Returns\n    -------\n    data_emd_4p : pandas.DataFrame\n        The generated 4P data\n    data_md_raw : pandas.DataFrame|None\n        MD data (sometimes this data is not imported, then we return None here)\n    data_emd_3p : pandas.DataFrame\n        The imported 3P data (only if return_3p==True)\n    \"\"\"\n    if not os.path.isfile(filename):\n        print('File not found: {}'.format(filename))\n        exit()\n    version = _get_file_version(filename)\n    importer = mat_version_importers.get(version, None)\n    if importer is not None:\n        mat = sio.loadmat(filename, squeeze_me=True)\n        data_md_raw = importer._extract_md(mat, **kwargs)\n        data_emd_3p = importer._extract_emd(mat, **kwargs)\n\n        # check configs\n        if callable(configs):\n            configs_abmn = configs(data_md_raw)\n        else:\n            configs_abmn = configs\n\n        if data_emd_3p is not None:\n            data_emd_4p = compute_quadrupoles(data_emd_3p, configs_abmn)\n        else:\n            data_emd_4p = None\n    else:\n        raise Exception(\n            'The file version \"{}\" is not supported yet.'.format(\n                version)\n        )\n\n    if return_3p:\n        return data_emd_4p, data_md_raw, data_emd_3p\n    else:\n        return data_emd_4p, data_md_raw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply correction factors for a pseudo - 2D measurement setup.", "response": "def apply_correction_factors(df, correction_file):\n    \"\"\"Apply correction factors for a pseudo-2D measurement setup. See Weigand\n    and Kemna, 2017, Biogeosciences, for detailed information.\n    \"\"\"\n    if isinstance(correction_file, (list, tuple)):\n        corr_data_raw = np.vstack(\n            [np.loadtxt(x) for x in correction_file]\n        )\n    else:\n        corr_data_raw = np.loadtxt(correction_file)\n\n        if corr_data_raw.shape[1] == 3:\n            A = (corr_data_raw[:, 0] / 1e4).astype(int)\n            B = (corr_data_raw[:, 0] % 1e4).astype(int)\n            M = (corr_data_raw[:, 1] / 1e4).astype(int)\n            N = (corr_data_raw[:, 1] % 1e4).astype(int)\n            corr_data = np.vstack((A, B, M, N, corr_data_raw[:, 2])).T\n\n        elif corr_data_raw.shape[1] == 5:\n            corr_data = corr_data_raw\n        else:\n            raise Exception('error')\n    corr_data[:, 0:2] = np.sort(corr_data[:, 0:2], axis=1)\n    corr_data[:, 2:4] = np.sort(corr_data[:, 2:4], axis=1)\n\n    if 'frequency' not in df.columns:\n        raise Exception(\n            'No frequency data found. Are you sure this is a seit data set?'\n        )\n\n    df = df.reset_index()\n    gf = df.groupby(['a', 'b', 'm', 'n'])\n    for key, item in gf.indices.items():\n        # print('key', key)\n        # print(item)\n        item_norm = np.hstack((np.sort(key[0:2]), np.sort(key[2:4])))\n        # print(item_norm)\n        index = np.where(\n            (corr_data[:, 0] == item_norm[0]) &\n            (corr_data[:, 1] == item_norm[1]) &\n            (corr_data[:, 2] == item_norm[2]) &\n            (corr_data[:, 3] == item_norm[3])\n        )[0]\n        # print(index, corr_data[index])\n        if len(index) == 0:\n            print(key)\n            import IPython\n            IPython.embed()\n            raise Exception(\n                'No correction factor found for this configuration'\n            )\n\n        factor = corr_data[index, 4]\n        # if key == (1, 4, 2, 3):\n        #     print(key)\n        #     print(factor)\n        #     print(df['R'])\n        #     print(df['k'])\n        #     import IPython\n        #     IPython.embed()\n        #     exit()\n        # apply correction factor\n        for col in ('r', 'Zt', 'Vmn', 'rho_a'):\n            if col in df.columns:\n                df.ix[item, col] *= factor\n        df.ix[item, 'corr_fac'] = factor\n    return df, corr_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function for calculation of probability matrix of substitutions i - > j over time t.", "response": "def get_pij_method(model=F81, frequencies=None, kappa=None):\n    \"\"\"\n    Returns a function for calculation of probability matrix of substitutions i->j over time t.\n\n    :param kappa: kappa parameter for HKY model\n    :type kappa: float\n    :param frequencies: array of state frequencies \\pi_i\n    :type frequencies: numpy.array\n    :param model: model of character evolution\n    :type model: str\n    :return: probability matrix\n    :rtype: function\n    \"\"\"\n    if is_f81_like(model):\n        mu = get_mu(frequencies)\n        return lambda t: get_f81_pij(t, frequencies, mu)\n    if JTT == model:\n        return get_jtt_pij\n    if HKY == model:\n        return lambda t: get_hky_pij(t, frequencies, kappa)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the bottom - up likelihood for the given tree.", "response": "def get_bottom_up_likelihood(tree, character, frequencies, sf, kappa=None, is_marginal=True, model=F81):\n    \"\"\"\n    Calculates the bottom-up likelihood for the given tree.\n    The likelihood for each node is stored in the corresponding feature,\n    given by get_personalised_feature_name(feature, BU_LH).\n\n    :param model: model of character evolution\n    :type model: str\n    :param is_marginal: whether the likelihood reconstruction is marginal (true) or joint (false)\n    :type is_marginal: bool\n    :param tree: tree of interest\n    :type tree: ete3.Tree\n    :param character: character for which the likelihood is calculated\n    :type character: str\n    :param frequencies: array of state frequencies \\pi_i\n    :type frequencies: numpy.array\n    :param sf: scaling factor\n    :type sf: float\n    :return: log likelihood\n    :rtype: float\n    \"\"\"\n    lh_sf_feature = get_personalized_feature_name(character, BU_LH_SF)\n    lh_feature = get_personalized_feature_name(character, BU_LH)\n    lh_joint_state_feature = get_personalized_feature_name(character, BU_LH_JOINT_STATES)\n    allowed_state_feature = get_personalized_feature_name(character, ALLOWED_STATES)\n\n    get_pij = get_pij_method(model, frequencies, kappa)\n    for node in tree.traverse('postorder'):\n        likelihood_array = np.ones(len(frequencies), dtype=np.float64) * getattr(node, allowed_state_feature)\n        factors = 0\n        for child in node.children:\n            child_likelihoods = get_pij(child.dist * sf) * getattr(child, lh_feature)\n            if is_marginal:\n                child_likelihoods = child_likelihoods.sum(axis=1)\n            else:\n                child_states = child_likelihoods.argmax(axis=1)\n                child.add_feature(lh_joint_state_feature, child_states)\n                child_likelihoods = child_likelihoods.max(axis=1)\n\n            factors += rescale(child_likelihoods, fraction_of_limit=len(node.children))\n            likelihood_array *= child_likelihoods\n\n        if np.all(likelihood_array == 0):\n            return -np.inf\n\n        factors += rescale(likelihood_array, fraction_of_limit=len(node.up.children) if not node.is_root() else 1)\n        node.add_feature(lh_feature, likelihood_array)\n        node.add_feature(lh_sf_feature, factors + sum(getattr(_, lh_sf_feature) for _ in node.children))\n    root_likelihoods = getattr(tree, lh_feature) * frequencies\n    root_likelihoods = root_likelihoods.sum() if is_marginal else root_likelihoods.max()\n    return np.log(root_likelihoods) - getattr(tree, lh_sf_feature) * np.log(10)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rescale(likelihood_array, fraction_of_limit):\n\n    max_limit = MAX_VALUE / fraction_of_limit\n    min_limit = MIN_VALUE / fraction_of_limit\n\n    min_lh_value = np.log10(np.min(likelihood_array[np.nonzero(likelihood_array)]))\n    max_lh_value = np.log10(np.max(likelihood_array[np.nonzero(likelihood_array)]))\n\n    factors = 0\n    if max_lh_value > max_limit:\n        factors = max_limit - max_lh_value - 1\n        likelihood_array *= np.power(10, factors)\n    elif min_lh_value < min_limit:\n        factors = min(-min_lh_value, max_limit - max_lh_value)\n        likelihood_array *= np.power(10, factors)\n    return factors", "response": "Rescales the likelihood array if it gets too small or large by multiplying it by a factor of 10."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimize_likelihood_params(tree, character, frequencies, sf, kappa, avg_br_len,\n                               optimise_sf=True, optimise_frequencies=True, optimise_kappa=True,\n                               model=F81):\n    \"\"\"\n    Optimizes the likelihood parameters (state frequencies and scaling factor) for the given tree.\n\n    :param model: model of character evolution\n    :type model: str\n    :param avg_br_len: avg branch length\n    :type avg_br_len: float\n    :param tree: tree of interest\n    :type tree: ete3.Tree\n    :param character: character for which the likelihood is optimised\n    :type character: str\n    :param frequencies: array of initial state frequencies\n    :type frequencies: numpy.array\n    :param sf: initial scaling factor\n    :type sf: float\n    :param optimise_sf: whether the scaling factor needs to be optimised\n    :type optimise_sf: bool\n    :param optimise_frequencies: whether the state frequencies need to be optimised\n    :type optimise_frequencies: bool\n    :return: optimized parameters and log likelihood: ((frequencies, scaling_factor), optimum)\n    :rtype: tuple\n    \"\"\"\n    bounds = []\n    if optimise_frequencies:\n        bounds += [np.array([1e-6, 10e6], np.float64)] * (len(frequencies) - 1)\n    if optimise_sf:\n        bounds += [np.array([0.001 / avg_br_len, 10. / avg_br_len])]\n    if optimise_kappa:\n        bounds += [np.array([1e-6, 20.])]\n    bounds = np.array(bounds, np.float64)\n\n    def get_real_params_from_optimised(ps):\n        freqs = frequencies\n        if optimise_frequencies:\n            freqs = np.hstack((ps[: (len(frequencies) - 1)], [1.]))\n            freqs /= freqs.sum()\n        sf_val = ps[(len(frequencies) - 1) if optimise_frequencies else 0] if optimise_sf else sf\n        kappa_val = ps[((len(frequencies) - 1) if optimise_frequencies else 0) + (1 if optimise_sf else 0)] \\\n            if optimise_kappa else kappa\n        return freqs, sf_val, kappa_val\n\n    def get_v(ps):\n        if np.any(pd.isnull(ps)):\n            return np.nan\n        freqs, sf_val, kappa_val = get_real_params_from_optimised(ps)\n        res = get_bottom_up_likelihood(tree=tree, character=character, frequencies=freqs,\n                                       sf=sf_val, kappa=kappa_val, is_marginal=True, model=model)\n        return np.inf if pd.isnull(res) else -res\n\n    for i in range(10):\n        if i == 0:\n            vs = np.hstack((frequencies[:-1] / frequencies[-1] if optimise_frequencies else [],\n                            [sf] if optimise_sf else [],\n                            [kappa] if optimise_kappa else []))\n        else:\n            vs = np.random.uniform(bounds[:, 0], bounds[:, 1])\n        fres = minimize(get_v, x0=vs, method='L-BFGS-B', bounds=bounds)\n        if fres.success and not np.any(np.isnan(fres.x)):\n            return get_real_params_from_optimised(fres.x), -fres.fun", "response": "This function optimizes the likelihood parameters for a given character evolution tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the top - down likelihood of a given character.", "response": "def calculate_top_down_likelihood(tree, character, frequencies, sf, kappa=None, model=F81):\n    \"\"\"\n    Calculates the top-down likelihood for the given tree.\n    The likelihood for each node is stored in the corresponding feature,\n    given by get_personalised_feature_name(feature, TD_LH).\n\n    To calculate the top-down likelihood of a node, we assume that the tree is rooted in this node\n    and combine the likelihoods of the \u201cup-subtrees\u201d,\n    e.g. to calculate the top-down likelihood of a node N1 being in a state i,\n    given that its parent node is P and its brother node is N2, we imagine that the tree is re-rooted in N1,\n    therefore P becoming the child of N1, and N2 its grandchild.\n    We then calculate the bottom-up likelihood from the P subtree:\n    L_top_down(N1, i) = \\sum_j P(i -> j, dist(N1, P)) * L_top_down(P) * \\sum_k P(j -> k, dist(N2, P)) * L_bottom_up (N2).\n\n    For the root node we assume its top-down likelihood to be 1 for all the states.\n\n    :param model: model of character evolution\n    :type model: str\n    :param sf: scaling factor\n    :type sf: float\n    :param character: character whose ancestral state likelihood is being calculated\n    :type character: str\n    :param tree: tree of interest (with bottom-up likelihood pre-calculated)\n    :type tree: ete3.Tree\n    :param frequencies: state frequencies\n    :type frequencies: numpy.array\n    :return: void, stores the node top-down likelihoods in the get_personalised_feature_name(feature, TD_LH) feature.\n    \"\"\"\n\n    lh_feature = get_personalized_feature_name(character, TD_LH)\n    lh_sf_feature = get_personalized_feature_name(character, TD_LH_SF)\n    bu_lh_feature = get_personalized_feature_name(character, BU_LH)\n    bu_lh_sf_feature = get_personalized_feature_name(character, BU_LH_SF)\n\n    get_pij = get_pij_method(model, frequencies, kappa)\n    for node in tree.traverse('preorder'):\n        if node.is_root():\n            node.add_feature(lh_feature, np.ones(len(frequencies), np.float64))\n            node.add_feature(lh_sf_feature, 0)\n            continue\n\n        parent = node.up\n        parent_bu_likelihood = getattr(parent, bu_lh_feature)\n\n        node_pjis = np.transpose(get_pij(node.dist * sf))\n        node_contribution = getattr(node, bu_lh_feature).dot(node_pjis)\n\n        parent_likelihood = getattr(parent, lh_feature) * parent_bu_likelihood\n        parent_likelihood[np.nonzero(parent_likelihood)] /= node_contribution[np.nonzero(parent_likelihood)]\n        factors = getattr(parent, lh_sf_feature) + getattr(parent, bu_lh_sf_feature) - getattr(node, bu_lh_sf_feature)\n\n        td_likelihood = parent_likelihood.dot(node_pjis)\n        factors += rescale(td_likelihood, fraction_of_limit=len(node.children) if not node.is_leaf() else 1)\n\n        node.add_feature(lh_feature, td_likelihood)\n        node.add_feature(lh_sf_feature, factors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef initialize_allowed_states(tree, feature, states):\n    allowed_states_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n    state2index = dict(zip(states, range(len(states))))\n\n    for node in tree.traverse():\n        node_states = getattr(node, feature, set())\n        if not node_states:\n            allowed_states = np.ones(len(state2index), dtype=np.int)\n        else:\n            allowed_states = np.zeros(len(state2index), dtype=np.int)\n            for state in node_states:\n                allowed_states[state2index[state]] = 1\n        node.add_feature(allowed_states_feature, allowed_states)", "response": "Initializes the allowed states for the tree with the given states."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nalters the bottom-up likelihood arrays for zero-distance tips to make sure they do not contradict with other zero-distance tip siblings. :param tree: ete3.Tree, the tree of interest :param feature: str, character for which the likelihood is altered :return: void, modifies the get_personalised_feature_name(feature, BU_LH) feature to zero-distance tips.", "response": "def alter_zero_tip_allowed_states(tree, feature):\n    \"\"\"\n    Alters the bottom-up likelihood arrays for zero-distance tips\n    to make sure they do not contradict with other zero-distance tip siblings.\n\n    :param tree: ete3.Tree, the tree of interest\n    :param feature: str, character for which the likelihood is altered\n    :return: void, modifies the get_personalised_feature_name(feature, BU_LH) feature to zero-distance tips.\n    \"\"\"\n    zero_parent2tips = defaultdict(list)\n\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n\n    for tip in tree:\n        if tip.dist == 0:\n            state = getattr(tip, feature, None)\n            if state is not None and state != '':\n                zero_parent2tips[tip.up].append(tip)\n\n    # adjust zero tips to contain all the zero tip options as states\n    for parent, zero_tips in zero_parent2tips.items():\n        # If there is a common state do nothing\n        counts = None\n        for tip in zero_tips:\n            if counts is None:\n                counts = getattr(tip, allowed_state_feature).copy()\n            else:\n                counts += getattr(tip, allowed_state_feature)\n        if counts.max() == len(zero_tips):\n            continue\n\n        # Otherwise set all tip states to state union\n        allowed_states = None\n        for tip in zero_tips:\n            if allowed_states is None:\n                allowed_states = getattr(tip, allowed_state_feature).copy()\n            else:\n                tip_allowed_states = getattr(tip, allowed_state_feature)\n                allowed_states[np.nonzero(tip_allowed_states)] = 1\n            tip.add_feature(allowed_state_feature, allowed_states)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unalter_zero_tip_allowed_states(tree, feature, state2index):\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n    for tip in tree:\n        if tip.dist > 0:\n            continue\n        state = getattr(tip, feature, set())\n        if state:\n            initial_allowed_states = np.zeros(len(state2index), np.int)\n            for _ in state:\n                initial_allowed_states[state2index[_]] = 1\n            allowed_states = getattr(tip, allowed_state_feature) & initial_allowed_states\n            tip.add_feature(allowed_state_feature, (allowed_states\n                                                    if np.any(allowed_states > 0) else initial_allowed_states))", "response": "Unalters the bottom - up likelihood arrays for zero - distance tips and ones only in their states."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unalter_zero_tip_joint_states(tree, feature, state2index):\n    lh_joint_state_feature = get_personalized_feature_name(feature, BU_LH_JOINT_STATES)\n    for tip in tree:\n        if tip.dist > 0:\n            continue\n        state = getattr(tip, feature, set())\n        if len(state) > 1:\n            allowed_indices = {state2index[_] for _ in state}\n            allowed_index = next(iter(allowed_indices))\n            joint_states = getattr(tip, lh_joint_state_feature)\n            for i in range(len(state2index)):\n                if joint_states[i] not in allowed_indices:\n                    joint_states[i] = allowed_index\n        elif len(state) == 1:\n            tip.add_feature(lh_joint_state_feature, np.ones(len(state2index), np.int) * state2index[next(iter(state))])", "response": "Unalters the joint tip states for zero - distance tips."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_marginal_likelihoods(tree, feature, frequencies):\n    bu_lh_feature = get_personalized_feature_name(feature, BU_LH)\n    bu_lh_sf_feature = get_personalized_feature_name(feature, BU_LH_SF)\n    td_lh_feature = get_personalized_feature_name(feature, TD_LH)\n    td_lh_sf_feature = get_personalized_feature_name(feature, TD_LH_SF)\n    lh_feature = get_personalized_feature_name(feature, LH)\n    lh_sf_feature = get_personalized_feature_name(feature, LH_SF)\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n\n    for node in tree.traverse('preorder'):\n        likelihood = getattr(node, bu_lh_feature) * getattr(node, td_lh_feature) * frequencies \\\n                     * getattr(node, allowed_state_feature)\n        node.add_feature(lh_feature, likelihood)\n        node.add_feature(lh_sf_feature, getattr(node, td_lh_sf_feature) + getattr(node, bu_lh_sf_feature))\n\n        node.del_feature(bu_lh_feature)\n        node.del_feature(bu_lh_sf_feature)\n        node.del_feature(td_lh_feature)\n        node.del_feature(td_lh_sf_feature)", "response": "Calculates the marginal likelihoods for each node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_marginal_likelihoods(tree, feature):\n    lh_feature = get_personalized_feature_name(feature, LH)\n    lh_sf_feature = get_personalized_feature_name(feature, LH_SF)\n\n    for node in tree.traverse():\n        if not node.is_root() and not (node.is_leaf() and node.dist == 0):\n            node_loglh = np.log10(getattr(node, lh_feature).sum()) - getattr(node, lh_sf_feature)\n            parent_loglh = np.log10(getattr(node.up, lh_feature).sum()) - getattr(node.up, lh_sf_feature)\n            assert (round(node_loglh, 2) == round(parent_loglh, 2))", "response": "Sanity check for marginal likelihoods of each node of the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_likelihoods_to_probabilities(tree, feature, states):\n    lh_feature = get_personalized_feature_name(feature, LH)\n\n    name2probs = {}\n\n    for node in tree.traverse():\n        lh = getattr(node, lh_feature)\n        name2probs[node.name] = lh / lh.sum()\n\n    return pd.DataFrame.from_dict(name2probs, orient='index', columns=states)", "response": "Normalizes each node marginal likelihoods to convert them to marginal probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects the ancestral states for the given feature and states.", "response": "def choose_ancestral_states_mppa(tree, feature, states, force_joint=True):\n    \"\"\"\n    Chooses node ancestral states based on their marginal probabilities using MPPA method.\n\n    :param force_joint: make sure that Joint state is chosen even if it has a low probability.\n    :type force_joint: bool\n    :param tree: tree of interest\n    :type tree: ete3.Tree\n    :param feature: character for which the ancestral states are to be chosen\n    :type feature: str\n    :param states: possible character states in order corresponding to the probabilities array\n    :type states: numpy.array\n    :return: number of ancestral scenarios selected,\n        calculated by multiplying the number of selected states for all nodes.\n        Also modified the get_personalized_feature_name(feature, ALLOWED_STATES) feature of each node\n        to only contain the selected states.\n    :rtype: int\n    \"\"\"\n    lh_feature = get_personalized_feature_name(feature, LH)\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n    joint_state_feature = get_personalized_feature_name(feature, JOINT_STATE)\n\n    n = len(states)\n    _, state2array = get_state2allowed_states(states, False)\n\n    num_scenarios = 1\n    unresolved_nodes = 0\n    num_states = 0\n\n    # If force_joint == True,\n    # we make sure that the joint state is always chosen,\n    # for this we sort the marginal probabilities array as [lowest_non_joint_mp, ..., highest_non_joint_mp, joint_mp]\n    # select k in 1:n such as the correction between choosing 0, 0, ..., 1/k, ..., 1/k and our sorted array is min\n    # and return the corresponding states\n    for node in tree.traverse():\n        marginal_likelihoods = getattr(node, lh_feature)\n        marginal_probs = marginal_likelihoods / marginal_likelihoods.sum()\n        if force_joint:\n            joint_index = getattr(node, joint_state_feature)\n            joint_prob = marginal_probs[joint_index]\n            marginal_probs = np.hstack((np.sort(np.delete(marginal_probs, joint_index)), [joint_prob]))\n        else:\n            marginal_probs = np.sort(marginal_probs)\n        best_k = n\n        best_correstion = np.inf\n        for k in range(1, n + 1):\n            correction = np.hstack((np.zeros(n - k), np.ones(k) / k)) - marginal_probs\n            correction = correction.dot(correction)\n            if correction < best_correstion:\n                best_correstion = correction\n                best_k = k\n\n        num_scenarios *= best_k\n        num_states += best_k\n        if force_joint:\n            indices_selected = sorted(range(n),\n                                      key=lambda _: (0 if n == joint_index else 1, -marginal_likelihoods[_]))[:best_k]\n        else:\n            indices_selected = sorted(range(n), key=lambda _: -marginal_likelihoods[_])[:best_k]\n        if best_k == 1:\n            allowed_states = state2array[indices_selected[0]]\n        else:\n            allowed_states = np.zeros(len(states), dtype=np.int)\n            allowed_states[indices_selected] = 1\n            unresolved_nodes += 1\n        node.add_feature(allowed_state_feature, allowed_states)\n\n    return num_scenarios, unresolved_nodes, num_states"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect node ancestral states based on their marginal probabilities using MAP method.", "response": "def choose_ancestral_states_map(tree, feature, states):\n    \"\"\"\n    Chooses node ancestral states based on their marginal probabilities using MAP method.\n\n    :param tree: ete3.Tree, the tree of interest\n    :param feature: str, character for which the ancestral states are to be chosen\n    :param states: numpy.array of possible character states in order corresponding to the probabilities array\n    :return: void, modified the get_personalized_feature_name(feature, ALLOWED_STATES) feature of each node\n        to only contain the selected states.\n    \"\"\"\n    lh_feature = get_personalized_feature_name(feature, LH)\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n    _, state2array = get_state2allowed_states(states, False)\n\n    for node in tree.traverse():\n        marginal_likelihoods = getattr(node, lh_feature)\n        node.add_feature(allowed_state_feature, state2array[marginal_likelihoods.argmax()])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nselect node ancestral states based on their marginal probabilities using joint method.", "response": "def choose_ancestral_states_joint(tree, feature, states, frequencies):\n    \"\"\"\n    Chooses node ancestral states based on their marginal probabilities using joint method.\n\n    :param frequencies: numpy array of state frequencies\n    :param tree: ete3.Tree, the tree of interest\n    :param feature: str, character for which the ancestral states are to be chosen\n    :param states: numpy.array of possible character states in order corresponding to the probabilities array\n    :return: void, modified the get_personalized_feature_name(feature, ALLOWED_STATES) feature of each node\n        to only contain the selected states.\n    \"\"\"\n    lh_feature = get_personalized_feature_name(feature, BU_LH)\n    lh_state_feature = get_personalized_feature_name(feature, BU_LH_JOINT_STATES)\n    allowed_state_feature = get_personalized_feature_name(feature, ALLOWED_STATES)\n    joint_state_feature = get_personalized_feature_name(feature, JOINT_STATE)\n    _, state2array = get_state2allowed_states(states, False)\n\n    def chose_consistent_state(node, state_index):\n        node.add_feature(joint_state_feature, state_index)\n        node.add_feature(allowed_state_feature, state2array[state_index])\n\n        for child in node.children:\n            chose_consistent_state(child, getattr(child, lh_state_feature)[state_index])\n\n    chose_consistent_state(tree, (getattr(tree, lh_feature) * frequencies).argmax())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the ML states on the tree and stores them in the corresponding feature.", "response": "def ml_acr(tree, character, prediction_method, model, states, avg_br_len, num_nodes, num_tips, freqs=None, sf=None,\n           kappa=None, force_joint=True):\n    \"\"\"\n    Calculates ML states on the tree and stores them in the corresponding feature.\n\n    :param states: numpy array of possible states\n    :param prediction_method: str, MPPA (marginal approximation), MAP (max a posteriori) or JOINT\n    :param tree: ete3.Tree, the tree of interest\n    :param character: str, character for which the ML states are reconstructed\n    :param model: str, evolutionary model, F81 (Felsenstein 81-like), JC (Jukes-Cantor-like) or EFT (estimate from tips)\n    :param avg_br_len: float, average non-zero branch length of the tree.\n    :param freqs: numpy array of predefined frequencies (or None if they are to be estimated)\n    :param sf: float, predefined scaling factor (or None if it is to be estimated)\n    :return: dict, mapping between reconstruction parameters and values\n    \"\"\"\n    n = len(states)\n    state2index = dict(zip(states, range(n)))\n    missing_data = 0.\n    observed_frequencies = np.zeros(n, np.float64)\n    for _ in tree:\n        state = getattr(_, character, set())\n        if state:\n            num_node_states = len(state)\n            for _ in state:\n                observed_frequencies[state2index[_]] += 1. / num_node_states\n        else:\n            missing_data += 1\n    total_count = observed_frequencies.sum() + missing_data\n    observed_frequencies /= observed_frequencies.sum()\n    missing_data /= total_count\n\n    logger = logging.getLogger('pastml')\n    logger.debug('Observed frequencies for {}:{}{}.'\n                 .format(character,\n                         ''.join('\\n\\tfrequency of {}:\\t{:.3f}'.format(state, observed_frequencies[\n                             state2index[state]])\n                                 for state in states),\n                         '\\n\\tfraction of missing data:\\t{:.3f}'.format(\n                             missing_data) if missing_data else ''))\n\n    if freqs is not None and model not in {F81, HKY}:\n        logging.warning('Some frequencies were specified in the parameter file, '\n                        'but the selected model ({}) ignores them. '\n                        'Use F81 (or HKY for nucleotide characters only) '\n                        'for taking user-specified frequencies into account.'.format(model))\n    optimise_frequencies = model in {F81, HKY} and freqs is None\n    if JTT == model:\n        frequencies = JTT_FREQUENCIES\n    elif EFT == model:\n        frequencies = observed_frequencies\n    elif model in {F81, HKY} and freqs is not None:\n        frequencies = freqs\n    else:\n        frequencies = np.ones(n, dtype=np.float64) / n\n\n    initialize_allowed_states(tree, character, states)\n    alter_zero_tip_allowed_states(tree, character)\n    if sf:\n        optimise_sf = False\n    else:\n        sf = 1. / avg_br_len\n        optimise_sf = True\n    if HKY == model:\n        if kappa:\n            optimise_kappa = False\n        else:\n            optimise_kappa = True\n            kappa = 4.\n    else:\n        optimise_kappa = False\n\n    likelihood = get_bottom_up_likelihood(tree=tree, character=character, frequencies=frequencies, sf=sf, kappa=kappa,\n                                          is_marginal=True, model=model)\n    if not optimise_sf and not optimise_frequencies and not optimise_kappa:\n        logger.debug('All the parameters are fixed for {}:{}{}{}{}.'\n                     .format(character,\n                             ''.join('\\n\\tfrequency of {}:\\t{:.3f}'.format(state, frequencies[\n                                 state2index[state]])\n                                     for state in states),\n                             '\\n\\tSF:\\t{:.3f}, i.e. {:.3f} changes per avg branch'\n                             .format(sf, sf * avg_br_len),\n                             '\\n\\tkappa:\\t{:.3f}'.format(kappa) if HKY == model else '',\n                             '\\n\\tlog likelihood:\\t{:.3f}'.format(likelihood)))\n    else:\n        logger.debug('Initial values for {} parameter optimisation:{}{}{}{}.'\n                     .format(character,\n                             ''.join('\\n\\tfrequency of {}:\\t{:.3f}'.format(state, frequencies[\n                                 state2index[state]])\n                                     for state in states),\n                             '\\n\\tSF:\\t{:.3f}, i.e. {:.3f} changes per avg branch'\n                             .format(sf, sf * avg_br_len),\n                             '\\n\\tkappa:\\t{:.3f}'.format(kappa) if HKY == model else '',\n                             '\\n\\tlog likelihood:\\t{:.3f}'.format(likelihood)))\n        if optimise_sf:\n            (_, sf, _), likelihood = optimize_likelihood_params(tree=tree, character=character, frequencies=frequencies,\n                                                                sf=sf, kappa=kappa,\n                                                                optimise_frequencies=False, optimise_sf=optimise_sf,\n                                                                optimise_kappa=False, avg_br_len=avg_br_len,\n                                                                model=model)\n            if optimise_frequencies or optimise_kappa:\n                logger.debug('Pre-optimised SF for {}:{}{}.'\n                             .format(character,\n                                     '\\n\\tSF:\\t{:.3f}, i.e. {:.3f} changes per avg branch'\n                                     .format(sf, sf * avg_br_len),\n                                     '\\n\\tlog likelihood:\\t{:.3f}'.format(likelihood)))\n        if optimise_frequencies or optimise_kappa:\n            (frequencies, sf, kappa), likelihood = \\\n                optimize_likelihood_params(tree=tree, character=character, frequencies=frequencies, sf=sf, kappa=kappa,\n                                           optimise_frequencies=optimise_frequencies, optimise_sf=optimise_sf,\n                                           optimise_kappa=optimise_kappa, avg_br_len=avg_br_len, model=model)\n\n        logger.debug('Optimised {} values:{}{}{}{}'\n                     .format(character,\n                             ''.join('\\n\\tfrequency of {}:\\t{:.3f}'.format(state, frequencies[\n                                 state2index[state]])\n                                     for state in states) if optimise_frequencies else '',\n                             '\\n\\tSF:\\t{:.3f}, i.e. {:.3f} changes per avg branch'\n                             .format(sf, sf * avg_br_len),\n                             '\\n\\tkappa:\\t{:.3f}'.format(kappa) if HKY == model else '',\n                             '\\n\\tlog likelihood:\\t{:.3f}'.format(likelihood)))\n\n    result = {LOG_LIKELIHOOD: likelihood, CHARACTER: character, METHOD: prediction_method, MODEL: model,\n              FREQUENCIES: frequencies, SCALING_FACTOR: sf, CHANGES_PER_AVG_BRANCH: sf * avg_br_len, STATES: states,\n              NUM_NODES: num_nodes, NUM_TIPS: num_tips}\n    if HKY == model:\n        result[KAPPA] = kappa\n\n    results = []\n\n    def process_reconstructed_states(method):\n        if method == prediction_method or is_meta_ml(prediction_method):\n            method_character = get_personalized_feature_name(character, method) \\\n                if prediction_method != method else character\n            convert_allowed_states2feature(tree, character, states, method_character)\n            res = result.copy()\n            res[CHARACTER] = method_character\n            res[METHOD] = method\n            results.append(res)\n\n    def process_restricted_likelihood_and_states(method):\n        alter_zero_tip_allowed_states(tree, character)\n        restricted_likelihood = get_bottom_up_likelihood(tree=tree, character=character,\n                                                         frequencies=frequencies, sf=sf, kappa=kappa,\n                                                         is_marginal=True, model=model)\n        unalter_zero_tip_allowed_states(tree, character, state2index)\n        note_restricted_likelihood(method, restricted_likelihood)\n        process_reconstructed_states(method)\n\n    def note_restricted_likelihood(method, restricted_likelihood):\n        logger.debug('Log likelihood for {} after {} state selection:\\t{:.3f}'\n                     .format(character, method, restricted_likelihood))\n        result[RESTRICTED_LOG_LIKELIHOOD_FORMAT_STR.format(method)] = restricted_likelihood\n\n    if prediction_method != MAP:\n        # Calculate joint restricted likelihood\n        restricted_likelihood = get_bottom_up_likelihood(tree=tree, character=character,\n                                                         frequencies=frequencies, sf=sf, kappa=kappa,\n                                                         is_marginal=False, model=model)\n        note_restricted_likelihood(JOINT, restricted_likelihood)\n        unalter_zero_tip_joint_states(tree, character, state2index)\n        choose_ancestral_states_joint(tree, character, states, frequencies)\n        process_reconstructed_states(JOINT)\n\n    if is_marginal(prediction_method):\n        initialize_allowed_states(tree, character, states)\n        alter_zero_tip_allowed_states(tree, character)\n        get_bottom_up_likelihood(tree=tree, character=character, frequencies=frequencies, sf=sf, kappa=kappa,\n                                 is_marginal=True, model=model)\n        calculate_top_down_likelihood(tree, character, frequencies, sf, kappa=kappa, model=model)\n        unalter_zero_tip_allowed_states(tree, character, state2index)\n        calculate_marginal_likelihoods(tree, character, frequencies)\n        # check_marginal_likelihoods(tree, feature)\n        result[MARGINAL_PROBABILITIES] = convert_likelihoods_to_probabilities(tree, character, states)\n\n        choose_ancestral_states_map(tree, character, states)\n        process_restricted_likelihood_and_states(MAP)\n\n        if MPPA == prediction_method or is_meta_ml(prediction_method):\n\n            if ALL == prediction_method:\n                pars_acr_results = parsimonious_acr(tree, character, MP, states, num_nodes, num_tips)\n                results.extend(pars_acr_results)\n                for pars_acr_res in pars_acr_results:\n                    _parsimonious_states2allowed_states(tree, pars_acr_res[CHARACTER], character, state2index)\n                    alter_zero_tip_allowed_states(tree, character)\n                    restricted_likelihood = get_bottom_up_likelihood(tree=tree, character=character,\n                                                                     frequencies=frequencies, sf=sf, kappa=kappa,\n                                                                     is_marginal=True, model=model)\n                    note_restricted_likelihood(pars_acr_res[METHOD], restricted_likelihood)\n\n            result[NUM_SCENARIOS], result[NUM_UNRESOLVED_NODES], result[NUM_STATES_PER_NODE] = \\\n                choose_ancestral_states_mppa(tree, character, states, force_joint=force_joint)\n            result[NUM_STATES_PER_NODE] /= num_nodes\n            result[PERC_UNRESOLVED] = result[NUM_UNRESOLVED_NODES] * 100 / num_nodes\n            logger.debug('{} node{} unresolved ({:.2f}%) for {} by {}, '\n                         'i.e. {:.4f} state{} per node in average.'\n                         .format(result[NUM_UNRESOLVED_NODES], 's are' if result[NUM_UNRESOLVED_NODES] != 1 else ' is',\n                                 result[PERC_UNRESOLVED], character, MPPA,\n                                 result[NUM_STATES_PER_NODE], 's' if result[NUM_STATES_PER_NODE] > 1 else ''))\n            process_restricted_likelihood_and_states(MPPA)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreformats the column name to make sure it contains only numerical letter characters or underscore.", "response": "def col_name2cat(column):\n    \"\"\"\n    Reformats the column string to make sure it contains only numerical, letter characters or underscore.\n\n    :param column: column name to be reformatted\n    :type column: str\n    :return: column name with illegal characters removed\n    :rtype: str\n    \"\"\"\n    column_string = ''.join(s for s in column.replace(' ', '_') if s.isalnum() or '_' == s)\n    return column_string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_user_config_filename(appname='notify'):\n    import platform\n    system = platform.system()\n    if system == 'Windows':\n        rootname = os.path.join(os.environ['APPDATA'], appname)\n        filename = appname + \".cfg\"\n        prefix = ''\n    elif system == 'Linux':\n        XDG_CONFIG_HOME = os.environ.get('XDG_CONFIG_HOME', None)\n        rootname = XDG_CONFIG_HOME or os.path.join('~', '.config')\n        rootname = os.path.expanduser(rootname)\n        # check if XDG_CONFIG_HOME exists\n        if not os.path.exists(rootname) and XDG_CONFIG_HOME is None:\n            # XDG_CONFIG_HOME is not used\n            rootname = os.path.expanduser('~')\n            filename = appname + \".cfg\"\n            prefix = '.'\n        else:\n            rootname = os.path.join(rootname, appname)\n            filename = appname + \".cfg\"\n            prefix = ''\n    elif system == 'Darwin':\n        rootname = os.path.expanduser('~')\n        filename = appname + \".cfg\"\n        prefix = '.'\n    else:\n        # Unknown\n        rootname = os.path.expanduser('~')\n        filename = appname + \".cfg\"\n        prefix = ''\n    return os.path.join(rootname, prefix + filename)", "response": "Get the user config filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef config_to_options(config):\n    class Options:\n        host=config.get('smtp', 'host', raw=True)\n        port=config.getint('smtp', 'port')\n        to_addr=config.get('mail', 'to_addr', raw=True)\n        from_addr=config.get('mail', 'from_addr', raw=True)\n        subject=config.get('mail', 'subject', raw=True)\n        encoding=config.get('mail', 'encoding', raw=True)\n        username=config.get('auth', 'username')\n    opts = Options()\n    # format\n    opts.from_addr % {'host': opts.host, 'prog': 'notify'}\n    opts.to_addr % {'host': opts.host, 'prog': 'notify'}\n    return opts", "response": "Convert ConfigParser instance to argparse. Namespace object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_default_config():\n    import codecs\n    config = ConfigParser.SafeConfigParser()\n    config.readfp(StringIO(DEFAULT_CONFIG))\n\n    # Load user settings\n    filename = get_user_config_filename()\n    if not os.path.exists(filename):\n        from wizard import setup_wizard\n        setup_wizard(config)\n    else:\n        try:\n            fi = codecs.open(filename, 'r', encoding='utf-8')\n            config.readfp(fi)\n        finally:\n            fi.close()\n    return config", "response": "Create default ConfigParser instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef opt_import(module, requiredFor=\"use the full functionality\"):\n    # set default message for common imports\n    if not requiredFor and \"crtomo\" in module:\n        requiredFor = (\n            \"modelling and inversion with CRTOMO. Check\"\n            \"http://geo.uni-bonn.de/~mweigand/dashboard for details.\")\n\n    if not requiredFor and \"pygimli\" in module:\n        requiredFor = (\"modelling and inversion with pygimli. Check\"\n                       \"www.pygimli.org for installation details.\")\n\n    if module.count(\".\") > 2:\n        raise ImportError(\"Can only import modules and sub-packages.\")\n\n    try:\n        mod = import_module(module)\n    except ImportError:\n        msg = (\"No module named \\'%s\\'.\\nYou need to install this optional \"\n               \"dependency to %s.\")\n        print(msg % (module, requiredFor))\n        mod = None\n\n    return mod", "response": "Import and return module only if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if data container has multiple timesteps.", "response": "def has_multiple_timesteps(data):\n    \"\"\"Return True if `data` container has multiple timesteps.\"\"\"\n    if \"timestep\" in data.keys():\n        if len(np.unique(data[\"timestep\"])) > 1:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_timesteps(data, consistent_abmn=False):\n    if has_multiple_timesteps(data):\n        grouped = data.groupby(\"timestep\")\n        return [group[1] for group in grouped]\n    else:\n        return data", "response": "Split data into multiple timesteps."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(text):\n    # Sanitize text case to meet phonetic comparison standards\n    fixed_text = validate.fix_string_case(utf(text))\n    # prepare output list\n    output = []\n    # cursor end point\n    cur_end = 0\n    # iterate through input text\n    for cur, i in enumerate(fixed_text):\n        # Trap characters with unicode encoding errors\n        try:\n            i.encode('utf-8')\n        except UnicodeDecodeError:\n            uni_pass = False\n        else:\n            uni_pass = True\n        # Default value for match\n        match = {'matched': False}\n        # Check cur is greater than or equals cur_end. If cursor is in\n        # a position that has alread been processed/replaced, we don't\n        # process anything at all\n        if not uni_pass:\n            cur_end = cur + 1\n            output.append(i)\n        elif cur >= cur_end and uni_pass:\n            # Try looking in non rule patterns with current string portion\n            match = match_non_rule_patterns(fixed_text, cur)\n            # Check if non rule patterns have matched\n            if match[\"matched\"]:\n                output.append(match[\"replaced\"])\n                cur_end = cur + len(match[\"found\"])\n            else:\n            # if non rule patterns have not matched, try rule patterns\n                match = match_rule_patterns(fixed_text, cur)\n                # Check if rule patterns have matched\n                if match[\"matched\"]:\n                    # Update cur_end as cursor + length of match found\n                    cur_end =  cur + len(match[\"found\"])\n                    # Process its rules\n                    replaced = process_rules(rules = match[\"rules\"],\n                                             fixed_text = fixed_text,\n                                             cur = cur, cur_end = cur_end)\n                    # If any rules match, output replacement from the\n                    # rule, else output it's default top-level/default\n                    # replacement\n                    if replaced is not None:\n                        # Rule has matched\n                        output.append(replaced)\n                    else:\n                        # No rules have matched\n                        # output common match\n                        output.append(match[\"replaced\"])\n\n            # If none matched, append present cursor value\n            if not match[\"matched\"]:\n                cur_end = cur + 1\n                output.append(i)\n\n    # End looping through input text and produce output\n    return ''.join(output)", "response": "Parses input text and returns a list of avrodict objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmatching given text at cursor position with non - rule patterns.", "response": "def match_non_rule_patterns(fixed_text, cur=0):\n    \"\"\"Matches given text at cursor position with non rule patterns\n\n    Returns a dictionary of three elements:\n\n    - \"matched\" - Bool: depending on if match found\n    - \"found\" - string/None: Value of matched pattern's 'find' key or none\n    - \"replaced\": string Replaced string if match found else input string at\n    cursor\n\n     \"\"\"\n    pattern = exact_find_in_pattern(fixed_text, cur, NON_RULE_PATTERNS)\n    if len(pattern) > 0:\n        return {\"matched\": True, \"found\": pattern[0]['find'],\n                \"replaced\": pattern[0]['replace']}\n    else:\n        return {\"matched\": False, \"found\": None,\n                \"replaced\": fixed_text[cur]}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_rule_patterns(fixed_text, cur=0):\n    pattern = exact_find_in_pattern(fixed_text, cur, RULE_PATTERNS)\n    # if len(pattern) == 1:\n    if len(pattern) > 0:\n        return {\"matched\": True, \"found\": pattern[0]['find'],\n                \"replaced\": pattern[0]['replace'], \"rules\": pattern[0]['rules']}\n    else:\n        return {\"matched\": False, \"found\": None,\n                \"replaced\": fixed_text[cur], \"rules\": None}", "response": "Matches given text at cursor position with rule patterns\n    Returns a dictionary of four elements that are returned by the function match_rule_patterns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of patterns that match given text cur position and pattern", "response": "def exact_find_in_pattern(fixed_text, cur=0, patterns=PATTERNS):\n    \"\"\"Returns pattern items that match given text, cur position and pattern\"\"\"\n    return [x for x in patterns if (cur + len(x['find']) <= len(fixed_text))\n             and x['find'] == fixed_text[cur:(cur + len(x['find']))]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_rules(rules, fixed_text, cur = 0, cur_end = 1):\n    replaced = ''\n    # iterate through rules\n    for rule in rules:\n        matched = False\n        # iterate through matches\n        for match in rule['matches']:\n            matched = process_match(match, fixed_text, cur, cur_end)\n            # Break out of loop if we dont' have a match. Here we are\n            # trusting avrodict to have listed matches sequentially\n            if not matched:\n                break\n        # If a match is found, stop looping through rules any further\n        if matched:\n            replaced = rule['replace']\n            break\n\n    # if any match has been found return replace value\n    if matched:\n        return replaced\n    else:\n        return None", "response": "Process rules matched in pattern and returns suitable replacement\n    If any rule s condition is satisfied output the rules replace else output None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_match(match, fixed_text, cur, cur_end):\n    # Set our tools\n    # -- Initial/default value for replace\n    replace = True\n    # -- Set check cursor depending on match['type']\n    if match['type'] == 'prefix':\n        chk = cur - 1\n    else:\n        # suffix\n        chk = cur_end\n    # -- Set scope based on whether scope is negative\n    if match['scope'].startswith('!'):\n        scope = match['scope'][1:]\n        negative = True\n    else:\n        scope = match['scope']\n        negative = False\n\n    # Let the matching begin\n    # -- Punctuations\n    if scope == 'punctuation':\n        # Conditions: XORd with negative\n        if (not ((chk < 0 and match['type'] == 'prefix') or\n                 (chk >= len(fixed_text) and match['type'] == 'suffix') or\n                 validate.is_punctuation(fixed_text[chk]))\n            ^ negative):\n            replace = False\n    # -- Vowels -- Checks: 1. Cursor should not be at first character\n    # -- if prefix or last character if suffix, 2. Character at chk\n    # -- should be a vowel. 3. 'negative' will invert the value of 1\n    # -- AND 2\n    elif scope == 'vowel':\n        if (not (((chk >= 0 and match['type'] == 'prefix') or\n                  (chk < len(fixed_text) and match['type'] == 'suffix'))\n                 and validate.is_vowel(fixed_text[chk]))\n            ^ negative):\n            replace =  False\n    # -- Consonants -- Checks: 1. Cursor should not be at first\n    # -- character if prefix or last character if suffix, 2. Character\n    # -- at chk should be a consonant. 3. 'negative' will invert the\n    # -- value of 1 AND 2\n    elif scope == 'consonant':\n        if (not (((chk >= 0 and match['type'] == 'prefix') or\n                  (chk < len(fixed_text) and match['type'] == 'suffix'))\n                 and validate.is_consonant(fixed_text[chk]))\n            ^ negative):\n            replace = False\n    # -- Exacts\n    elif scope == 'exact':\n        # Prepare cursor for exact search\n        if match['type'] == 'prefix':\n            exact_start = cur - len(match['value'])\n            exact_end = cur\n        else:\n            # suffix\n            exact_start = cur_end\n            exact_end = cur_end + len(match['value'])\n        # Validate exact find.\n        if not validate.is_exact(match['value'], fixed_text, exact_start,\n                                 exact_end, negative):\n            replace = False\n    # Return replace, which will be true if none of the checks above match\n    return replace", "response": "Processes a single match in rules and returns the value of the match in rules."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cli(ctx, hostname, username, password, config_dir, https):\n    ctx.is_root = True\n    ctx.user_values_entered = False\n    ctx.config_dir = os.path.abspath(os.path.expanduser(config_dir))\n    ctx.config = load_config(ctx)\n    ctx.hostname = hostname\n    ctx.username = username\n    ctx.password = password\n    ctx.https = https\n\n    # Creating the WVA object is deferred as some commands like clearconfig\n    # should not require a username/password to perform them\n    ctx.wva = None", "response": "Command - line interface for interacting with a WVA device"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms an HTTP GET of the provided URI", "response": "def get(ctx, uri):\n    \"\"\"Perform an HTTP GET of the provided URI\n\nThe URI provided is relative to the /ws base to allow for easy navigation of\nthe resources exposed by the WVA.  Example Usage::\n\n\\b\n    $ wva get /\n    {'ws': ['vehicle',\n             'hw',\n             'config',\n             'state',\n             'files',\n             'alarms',\n             'subscriptions',\n             'password']}\n    $ wva get /vehicle\n    {'vehicle': ['vehicle/ecus', 'vehicle/data', 'vehicle/dtc']}\n    $ wva get /vehicle/ecus\n    {'ecus': ['vehicle/ecus/can0ecu0', 'vehicle/ecus/can0ecu251']}\n    $ wva get /vehicle/ecus/can0ecu0\n    {'can0ecu0': ['vehicle/ecus/can0ecu0/name',\n           'vehicle/ecus/can0ecu0/address',\n           'vehicle/ecus/can0ecu0/function',\n           'vehicle/ecus/can0ecu0/bus',\n           'vehicle/ecus/can0ecu0/channel',\n           'vehicle/ecus/can0ecu0/make',\n           'vehicle/ecus/can0ecu0/model',\n           'vehicle/ecus/can0ecu0/serial_number',\n           'vehicle/ecus/can0ecu0/unit_number',\n           'vehicle/ecus/can0ecu0/VIN']}\n    $ wva get /vehicle/ecus/can0ecu0/bus\n    {'bus': 'J1939'}\n    \"\"\"\n    http_client = get_wva(ctx).get_http_client()\n    cli_pprint(http_client.get(uri))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(ctx, uri):\n    http_client = get_wva(ctx).get_http_client()\n    cli_pprint(http_client.delete(uri))", "response": "DELETE the specified URI\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(ctx, uri, input_file):\n    http_client = get_wva(ctx).get_http_client()\n    cli_pprint(http_client.post(uri, input_file.read()))", "response": "POST file data to a specific URI"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(ctx, element, timestamp, repeat, delay):\n    element = get_wva(ctx).get_vehicle_data_element(element)\n    for i in xrange(repeat):\n        curval = element.sample()\n        if timestamp:\n            print(\"{} at {}\".format(curval.value, curval.timestamp.ctime()))\n        else:\n            print(\"{}\".format(curval.value))\n\n        if i + 1 < repeat:  # do not delay on last iteration\n            time.sleep(delay)", "response": "Sample the value of a vehicle data element in a base vehicle data element."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists short name of all current subscriptions", "response": "def list(ctx):\n    \"\"\"List short name of all current subscriptions\"\"\"\n    wva = get_wva(ctx)\n    for subscription in wva.get_subscriptions():\n        print(subscription.short_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(ctx, short_name):\n    wva = get_wva(ctx)\n    subscription = wva.get_subscription(short_name)\n    subscription.delete()", "response": "Delete a specific subscription by short name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all registered subscriptions", "response": "def clear(ctx):\n    \"\"\"Remove all registered subscriptions\n\nExample:\n\n\\b\n    $ wva subscriptions clear\n    Deleting engineload... Done\n    Deleting fuelrate... Done\n    Deleting throttle... Done\n    Deleting rpm... Done\n    Deleting speedy... Done\n\nTo remove a specific subscription, use 'wva subscription remove <name>' instead.\n\"\"\"\n    wva = get_wva(ctx)\n    for subscription in wva.get_subscriptions():\n        sys.stdout.write(\"Deleting {}... \".format(subscription.short_name))\n        sys.stdout.flush()\n        subscription.delete()\n        print(\"Done\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows metadata for a specific subscription", "response": "def show(ctx, short_name):\n    \"\"\"Show metadata for a specific subscription\n\nExample:\n\n\\b\n    $ wva subscriptions show speed\n    {'buffer': 'queue', 'interval': 5, 'uri': 'vehicle/data/VehicleSpeed'}\n\"\"\"\n    wva = get_wva(ctx)\n    subscription = wva.get_subscription(short_name)\n    cli_pprint(subscription.get_metadata())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a subscription with a given short_name for a given uri with a given interval and a buffer.", "response": "def add(ctx, short_name, uri, interval, buffer):\n    \"\"\"Add a subscription with a given short_name for a given uri\n\nThis command can be used to create subscriptions to receive new pieces\nof vehicle data on the stream channel on a periodic basis.  By default,\nsubscriptions are buffered and have a 5 second interval:\n\n\\b\n    $ wva subscriptions add speed vehicle/data/VehicleSpeed\n    $ wva subscriptions show speed\n    {'buffer': 'queue', 'interval': 5, 'uri': 'vehicle/data/VehicleSpeed'}\n\nThese parameters can be modified by the use of optional arguments:\n\n    $ wva subscriptions add rpm vehicle/data/EngineSpeed --interval 1 --buffer discard\n    $ wva subscriptions show rpm\n    {'buffer': 'discard', 'interval': 1, 'uri': 'vehicle/data/EngineSpeed'}\n\nTo view the data coming in as a result of these subscriptions, one can use\neither 'wva subscriptions listen' or 'wva subscriptions graph <name>'.\n\"\"\"\n    wva = get_wva(ctx)\n    subscription = wva.get_subscription(short_name)\n    subscription.create(uri, buffer, interval)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef listen(ctx):\n    wva = get_wva(ctx)\n    es = wva.get_event_stream()\n\n    def cb(event):\n        cli_pprint(event)\n\n    es.add_event_listener(cb)\n    es.enable()\n    while True:\n        time.sleep(5)", "response": "This command shows the contents of the WVA event stream that has been set up and the data on the vehicle bus. This command is useful for debugging and debugging."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef graph(ctx, items, seconds, ylim):\n    wva = get_wva(ctx)\n    es = wva.get_event_stream()\n\n    try:\n        from wva import grapher\n    except ImportError:\n        print(\"Unable to graph... you must have matplotlib installed\")\n    else:\n        stream_grapher = grapher.WVAStreamGrapher(wva, items, seconds=seconds, ylim=ylim)\n        es.enable()\n        stream_grapher.run()", "response": "Presents a live graph of the incoming streaming data for the specified items within some time window."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenable ssh login as the current user for the current user", "response": "def authorize(ctx, public_key, append):\n    \"\"\"Enable ssh login as the Python user for the current user\n\nThis command will create an authorized_keys file on the target device\ncontaining the current users public key.  This will allow ssh to\nthe WVA from this machine.\n\"\"\"\n    wva = get_wva(ctx)\n\n    http_client = wva.get_http_client()\n    authorized_keys_uri = \"/files/userfs/WEB/python/.ssh/authorized_keys\"\n    authorized_key_contents = public_key\n    if append:\n        try:\n            existing_contents = http_client.get(authorized_keys_uri)\n            authorized_key_contents = \"{}\\n{}\".format(existing_contents, public_key)\n        except WVAHttpNotFoundError:\n            pass  # file doesn't exist, just write the public key\n    http_client.put(authorized_keys_uri, authorized_key_contents)\n\n    print(\"Public key written to authorized_keys for python user.\")\n    print(\"You should now be able to ssh to the device by doing the following:\")\n    print(\"\")\n    print(\"  $ ssh python@{}\".format(get_root_ctx(ctx).hostname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name_tree(tree):\n    existing_names = Counter((_.name for _ in tree.traverse() if _.name))\n    if sum(1 for _ in tree.traverse()) == len(existing_names):\n        return\n    i = 0\n    existing_names = Counter()\n    for node in tree.traverse('preorder'):\n        name = node.name if node.is_leaf() else ('root' if node.is_root() else None)\n        while name is None or name in existing_names:\n            name = '{}{}'.format('t' if node.is_leaf() else 'n', i)\n            i += 1\n        node.name = name\n        existing_names[name] += 1", "response": "Names all the nodes that are not named or have non - unique names."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_certain_leaves(tr, to_remove=lambda node: False):\n\n    tips = [tip for tip in tr if to_remove(tip)]\n    for node in tips:\n        if node.is_root():\n            return None\n        parent = node.up\n        parent.remove_child(node)\n        # If the parent node has only one child now, merge them.\n        if len(parent.children) == 1:\n            brother = parent.children[0]\n            brother.dist += parent.dist\n            if parent.is_root():\n                brother.up = None\n                tr = brother\n            else:\n                grandparent = parent.up\n                grandparent.remove_child(parent)\n                grandparent.add_child(brother)\n    return tr", "response": "Removes all branches leading to leaves identified positively by to_remove function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xy_simple_dipole_dipole(dataframe, spacing=1, indices=None):\n    if indices is None:\n        indices = slice(None)\n    abmn = dataframe.ix[indices, ['a', 'b', 'm', 'n']].values\n    posx = np.mean(abmn[:, 0:4], axis=1)\n    posz = np.abs(\n        np.min(abmn[:, 0:2], axis=1) - np.max(abmn[:, 2:4], axis=1)\n    ) * -0.192\n\n    # scale the positions with the electrode spacing\n    posx *= spacing\n    posz *= spacing\n    print(abmn.shape, posx.shape)\n    print('posxz', np.vstack((abmn.T, posx, posz)).T)\n    return posx, posz", "response": "Compute the x and z pseudo locations for each configuration in the dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_iso8601(self, include_millis=False):\n        date = self.__datetime.strftime(\"%Y-%m-%d\")\n        time = self.__datetime.strftime(\"%H:%M:%S\")\n\n        # millis...\n        if include_millis:\n            micros = float(self.__datetime.strftime(\"%f\"))\n            millis = \".%03d\" % (micros // 1000)\n\n        else:\n            millis = \"\"\n\n        # time zone...\n        zone = self.__datetime.strftime(\"%z\")\n\n        # Z format...\n        if float(zone[1:]) == 0.0:\n            return \"%sT%s%sZ\" % (date, time, millis)\n\n        # numeric format...\n        zone_hours = zone[:3]\n        zone_mins = zone[3:]\n\n        return \"%sT%s%s%s:%s\" % (date, time, millis, zone_hours, zone_mins)", "response": "returns ISO 8601 format of the object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrefreshing the config of the current instance of the base class.", "response": "def refresh_config(self):\n        '''\n        __NB__ This *must* be called from a *different* thread than the GUI/Gtk thread.\n        '''\n        if self.config_requested is not None:\n            while self.video_view.xid is None:\n                print 'waiting for GUI...'\n                time.sleep(1)\n            self.pipeline_manager.set_config(self.video_view.xid,\n                                             self.config_requested,\n                                             record_path=self.record_path)\n            self.config_requested = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef record_path(self):\n        '''\n        If recording is not enabled, return `None` as record path.\n        '''\n        if self.record_button.get_property('active') and (self.record_path_selector\n                                                          .selected_path):\n            return self.record_path_selector.selected_path\n        else:\n            return None", "response": "Return the record path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_crmod_file(filename):\n    crmod_lines = [\n        '***FILES***',\n        '../grid/elem.dat',\n        '../grid/elec.dat',\n        '../rho/rho.dat',\n        '../config/config.dat',\n        'F                ! potentials ?',\n        '../mod/pot/pot.dat',\n        'T                ! measurements ?',\n        '../mod/volt.dat',\n        'F               ! sensitivities ?',\n        '../mod/sens/sens.dat',\n        'F                ! another dataset ?',\n        '1                ! 2D (=0) or 2.5D (=1)',\n        'F                ! fictitious sink ?',\n        '1660             ! fictitious sink node number',\n        'F                ! boundary values ?',\n        'boundary.dat',\n    ]\n\n    with open(filename, 'w') as fid:\n        [fid.write(line + '\\n') for line in crmod_lines]", "response": "Write a valid crmod configuration file to filename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compute_K(dataframe, settings, keep_dir=False):\n    if settings is None:\n        print('using default settings')\n        settings = get_default_settings()\n\n    if not os.path.isfile(settings['elem']):\n        raise IOError(\n            'elem file not found: {0}'.format(settings['elem'])\n        )\n\n    if not os.path.isfile(settings['elec']):\n        raise IOError(\n            'elec file not found: {0}'.format(settings['elec'])\n        )\n\n    # read grid file and determine nr of cells\n    with open(settings['elem'], 'r') as fid:\n        fid.readline()\n        cell_type, cell_number, edge_number = np.fromstring(\n            fid.readline().strip(),\n            sep=' ',\n            dtype=int,\n        )\n\n    # generate forward model as a string\n    forward_model = '{0}\\n'.format(cell_number)\n    forward_model += '{0} {1}\\n'.format(settings['rho'], 0) * cell_number\n\n    full_path_elem = os.path.abspath(settings['elem'])\n    full_path_elec = os.path.abspath(settings['elec'])\n\n    pwd = os.getcwd()\n    with tempfile.TemporaryDirectory() as invdir:\n        os.chdir(invdir)\n        # create tomodir directory structure\n        for dir in [\n            'exe',\n            'mod',\n            'config',\n            'inv',\n            'grid',\n            'rho',\n        ]:\n            os.makedirs(dir)\n\n        # save forward model\n        with open('rho/rho.dat', 'w') as fid:\n            fid.write(forward_model)\n\n        shutil.copy(full_path_elem, 'grid/elem.dat')\n        shutil.copy(full_path_elec, 'grid/elec.dat')\n\n        print('SETTINGS')\n        print(settings)\n\n        cfg = CRcfg.crmod_config()\n        if settings.get('2D', False):\n            # activate 2D mode\n            print('2D modeling')\n            cfg['2D'] = '0'\n            cfg['fictitious_sink'] = 'T'\n            cfg['sink_node'] = settings.get('sink_node')\n        else:\n            cfg['2D'] = 1\n\n        cfg.write_to_file('exe/crmod.cfg')\n        subprocess.call('cat exe/crmod.cfg', shell=True)\n\n        config_orig = _write_config_file('config/config.dat', dataframe)\n\n        os.chdir('exe')\n        binary = CRbinaries.get('CRMod')\n        subprocess.call(binary, shell=True)\n        os.chdir('..')\n\n        # read in results\n        modeled_resistances = np.loadtxt(\n            'mod/volt.dat',\n            skiprows=1,\n        )\n\n        # now we have to make sure CRMod didn't change the signs\n        changed_sign = (config_orig[:, 1] == modeled_resistances[:, 1])\n        modeled_resistances[~changed_sign, 2] *= -1\n\n        if settings.get('norm_factor', None) is not None:\n            modeled_resistances[:, 2] /= settings.get('norm_factor')\n\n        K = settings['rho'] / modeled_resistances[:, 2]\n        if isinstance(dataframe, pd.DataFrame):\n            dataframe['k'] = K\n        if keep_dir is not None and not os.path.isdir(keep_dir):\n            shutil.copytree('.', keep_dir)\n            print('Copy of modeling dir stored here: {}'.format(keep_dir))\n\n    os.chdir(pwd)\n    return K", "response": "Compute K - vector for the current language of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining file codec from its BOM record.", "response": "def check_bom(file):\n    \"\"\"Determines file codec from from its BOM record.\n\n    If file starts with BOM record encoded with UTF-8 or UTF-16(BE/LE)\n    then corresponding encoding name is returned, otherwise None is returned.\n    In both cases file current position is set to after-BOM bytes. The file\n    must be open in binary mode and positioned at offset 0.\n    \"\"\"\n\n    # try to read first three bytes\n    lead = file.read(3)\n    if len(lead) == 3 and lead == codecs.BOM_UTF8:\n        # UTF-8, position is already OK, use canonical name\n        return codecs.lookup('utf-8').name\n    elif len(lead) >= 2 and lead[:2] == codecs.BOM_UTF16_BE:\n        # need to backup one character\n        if len(lead) == 3:\n            file.seek(-1, os.SEEK_CUR)\n        return codecs.lookup('utf-16-be').name\n    elif len(lead) >= 2 and lead[:2] == codecs.BOM_UTF16_LE:\n        # need to backup one character\n        if len(lead) == 3:\n            file.seek(-1, os.SEEK_CUR)\n        return codecs.lookup('utf-16-le').name\n    else:\n        # no BOM, rewind\n        file.seek(-len(lead), os.SEEK_CUR)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nguessing current line number in a file.", "response": "def guess_lineno(file):\n    \"\"\"Guess current line number in a file.\n\n    Guessing is done in a very crude way - scanning file from beginning\n    until current offset and counting newlines. Only meant to be used in\n    exceptional cases - generating line number for error message.\n    \"\"\"\n    offset = file.tell()\n    file.seek(0)\n    startpos = 0\n    lineno = 1\n    # looks like file.read() return bytes in python3\n    # so I need more complicated algorithm here\n    while True:\n        line = file.readline()\n        if not line:\n            break\n        endpos = file.tell()\n        if startpos <= offset < endpos:\n            break\n        lineno += 1\n    file.seek(offset)\n    return lineno"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute weC from sensor temperature compensation of weV aeV", "response": "def __we_c(cls, calib, tc, temp, we_v, ae_v):\n        \"\"\"\n        Compute weC from sensor temperature compensation of weV, aeV\n        \"\"\"\n        we_t = we_v - (calib.we_elc_mv / 1000.0)        # remove electronic we zero\n        ae_t = ae_v - (calib.ae_elc_mv / 1000.0)        # remove electronic ae zero\n\n        we_c = tc.correct(calib, temp, we_t, ae_t)\n\n        # print(\"A4Datum__we_c: we_t:%f ae_t:%f we_c:%s\" % (we_t, ae_t, we_c), file=sys.stderr)\n\n        return we_c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch Penn Libraries Franklin for documents", "response": "def search(query):\n    \"\"\"Search Penn Libraries Franklin for documents\n    The maximum pagesize currently is 50.\n    \"\"\"\n    params = {\n        's.cmd': 'setTextQuery(%s)setPageSize(50)setHoldingsOnly(true)' % query\n    }\n    return requests.get(BASE_URL, params=params, timeout=10).json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new record based on parameters.", "response": "def make_record(level, xref_id, tag, value, sub_records, offset, dialect,\n                parser=None):\n    \"\"\"Create Record instance based on parameters.\n\n    :param int level: Record level number.\n    :param str xref_id: Record reference ID, possibly empty.\n    :param str tag: Tag name.\n    :param value: Record value, possibly empty. Value can be None, bytes, or\n        string object, if it is bytes then it should be decoded into strings\n        before calling freeze(), this is normally done by the parser which\n        knows about encodings.\n    :param list sub_records: Initial list of subordinate records,\n        possibly empty. List can be updated later.\n    :param int offset: Record location in a file.\n    :param dialect: One of DIALECT_* constants.\n    :param parser: Instance of `GedcomReader` class, only needed for\n        records whose walue is a pointer.\n    :return: Instance of :py:class:`Record` (or one of its subclasses).\n    \"\"\"\n    # value can be bytes or string so we check for both, 64 is code for '@'\n    if value and len(value) > 2 and \\\n        ((value[0] == '@' and value[-1] == '@') or\n         (value[0] == 64 and value[-1] == 64)):\n        # this looks like a <pointer>, make a Pointer record\n        klass = Pointer\n        rec = klass(parser)\n    else:\n        klass = _tag_class.get(tag, Record)\n        rec = klass()\n\n    rec.level = level\n    rec.xref_id = xref_id\n    rec.tag = tag\n    rec.value = value\n    rec.sub_records = sub_records\n    rec.offset = offset\n    rec.dialect = dialect\n    return rec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the direct sub - record with given tag name or None.", "response": "def sub_tag(self, path, follow=True):\n        \"\"\"Returns direct sub-record with given tag name or None.\n\n        Path can be a simple tag name, in which case the first direct\n        sub-record of this record with the matching tag is returned. Path\n        can also consist of several tags separated by slashes, in that case\n        sub-records are searched recursively.\n\n        If `follow` is True then pointer records are resolved and pointed\n        record is used instead of pointer record, this also works for all\n        intermediate records in a path.\n\n        :param str path: tag names separated by slashes.\n        :param boolean follow: If True then resolve pointers.\n        :return: `Record` instance or `None` if sub-record with a given\n            tag does not exist.\n        \"\"\"\n        tags = path.split('/')\n        rec = self\n        for tag in tags:\n            recs = [x for x in (rec.sub_records or []) if x.tag == tag]\n            if not recs:\n                return None\n            rec = recs[0]\n            if follow and isinstance(rec, Pointer):\n                rec = rec.ref\n        return rec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value of a direct sub - record with a given tag name.", "response": "def sub_tag_value(self, path, follow=True):\n        \"\"\"Returns value of a direct sub-record or None.\n\n        Works as :py:meth:`sub_tag` but returns value of a sub-record\n        instead of sub-record itself.\n\n        :param str path: tag names separated by slashes.\n        :param boolean follow: If True then resolve pointers.\n        :return: String or `None` if sub-record with a given\n            tag does not exist.\n        \"\"\"\n        rec = self.sub_tag(path, follow)\n        if rec:\n            return rec.value\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn list of direct sub - records matching any tag name.", "response": "def sub_tags(self, *tags, **kw):\n        \"\"\"Returns list of direct sub-records matching any tag name.\n\n        Unlike :py:meth:`sub_tag` method this method does not support\n        hierarchical paths and does not resolve pointers.\n\n        :param str tags: Names of the sub-record tag\n        :param kw: Keyword arguments, only recognized keyword is `follow`\n            with the same meaning as in :py:meth:`sub_tag`.\n        :return: List of `Records`, possibly empty.\n        \"\"\"\n        records = [x for x in self.sub_records if x.tag in tags]\n        if kw.get('follow', True):\n            records = [rec.ref if isinstance(rec, Pointer) else rec\n                       for rec in records]\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef freeze(self):\n        # None is the same as empty string\n        if self.value is None:\n            self.value = \"\"\n        if self.dialect in [DIALECT_ALTREE]:\n            name_tuple = parse_name_altree(self)\n        elif self.dialect in [DIALECT_MYHERITAGE]:\n            name_tuple = parse_name_myher(self)\n        elif self.dialect in [DIALECT_ANCESTRIS]:\n            name_tuple = parse_name_ancestris(self)\n        else:\n            name_tuple = split_name(self.value)\n        self.value = name_tuple\n        return self", "response": "Method called by parser when updates to this record finish."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive name could include both first and middle name", "response": "def given(self):\n        \"\"\"Given name could include both first and middle name\"\"\"\n        if self._primary.value[0] and self._primary.value[2]:\n            return self._primary.value[0] + ' ' + self._primary.value[2]\n        return self._primary.value[0] or self._primary.value[2]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn name order key.", "response": "def order(self, order):\n        \"\"\"Returns name order key.\n\n        Returns tuple with two strings that can be compared to other such\n        tuple obtained from different name. Note that if you want\n        locale-dependent ordering then you need to compare strings using\n        locale-aware method (e.g. ``locale.strxfrm``).\n\n        :param order: One of the ORDER_* constants.\n        :returns: tuple of two strings\n        \"\"\"\n        given = self.given\n        surname = self.surname\n        if order in (ORDER_MAIDEN_GIVEN, ORDER_GIVEN_MAIDEN):\n            surname = self.maiden or self.surname\n\n        # We are collating empty names to come after non-empty,\n        # so instead of empty we return \"2\" and add \"1\" as prefix to others\n        given = (\"1\" + given) if given else \"2\"\n        surname = (\"1\" + surname) if surname else \"2\"\n\n        if order in (ORDER_SURNAME_GIVEN, ORDER_MAIDEN_GIVEN):\n            return (surname, given)\n        elif order in (ORDER_GIVEN_SURNAME, ORDER_GIVEN_MAIDEN):\n            return (given, surname)\n        else:\n            raise ValueError(\"unexpected order: {}\".format(order))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format(self):\n        name = self._primary.value[0]\n        if self.surname:\n            if name:\n                name += ' '\n            name += self.surname\n        if self._primary.value[2]:\n            if name:\n                name += ' '\n            name += self._primary.value[2]\n        return name", "response": "Format name for output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the individual s mother tag.", "response": "def mother(self):\n        \"\"\"Parent of this individual\"\"\"\n        if self._mother == []:\n            self._mother = self.sub_tag(\"FAMC/WIFE\")\n        return self._mother"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the parent of this individual", "response": "def father(self):\n        \"\"\"Parent of this individual\"\"\"\n        if self._father == []:\n            self._father = self.sub_tag(\"FAMC/HUSB\")\n        return self._father"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(self, xn):\n        if all(map(lambda x: x.match(xn), self.conditions)):\n            return self.outcomes\n        return None", "response": "Processes a transaction against this rule\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport RELEVANT data from the. mat or. csv file containing SIP -04 measurement results.", "response": "def import_sip04_data(data_filename):\n    \"\"\"Import RELEVANT data from the result files. Refer to the function\n    :func:`reda.importers.sip04.import_sip04_data_all` for an importer that\n    imports ALL data.\n\n    Exported parameters:\n\n    ================== ========================================================\n    key                description\n    ================== ========================================================\n    a                  First current electrode\n    b                  Second current electrode\n    m                  First potential electrode\n    n                  Second potential electrode\n    frequency          Measurement frequency\n    Temp_1             Temperature sensor 1 (optional)\n    Temp_2             Temperature sensor 2 (optional)\n    zt                 Complex Transfer Impedance (the measurement), mean value\n    r                  Magnitude of mean measurements (=|zt|)\n    rpha               Resistance phase [mrad]\n    zt_1               Complex Transfer Impedance, first repetition\n    zt_2               Complex Transfer Impedance, second repetition\n    zt_3               Complex Transfer Impedance, third repetition\n    ContactResistance  Contact resistance (mean value)\n    ShuntResistance    Shunt resistance used [Ohm]\n    ================== ========================================================\n\n    Parameters\n    ----------\n    data_filename : string\n        Path to .mat or .csv file containing SIP-04 measurement results. Note\n        that the .csv file does not contain all data contained in the .mat\n        file!\n\n    Returns\n    -------\n    df : :py:class:`pandas.DataFrame`\n        The data, contained in a DataFrame\n\n    \"\"\"\n    df_all = import_sip04_data_all(data_filename)\n    columns_to_keep = [\n        'a', 'b', 'm', 'n',\n        'frequency',\n        'Temp_1', 'Temp_2',\n        'Zm_1', 'Zm_2', 'Zm_3',\n        'Zg_m',\n        'zt',\n        'Rs',\n        'r',\n        'rpha',\n    ]\n    df = df_all[columns_to_keep]\n    df = df.rename(columns={\n        'Rs': 'ShuntResistance',\n        'Zg_m': 'ContactResistance',\n        'Zm_1': 'zt_1',\n        'Zm_2': 'zt_2',\n        'Zm_3': 'zt_3',\n    })\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_sip04_data_all(data_filename):\n    filename, fformat = os.path.splitext(data_filename)\n\n    if fformat == '.csv':\n        print('Import SIP04 data from .csv file')\n        df_all = _import_csv_file(data_filename)\n    elif fformat == '.mat':\n        print('Import SIP04 data from .mat file')\n        df_all = _import_mat_file(data_filename)\n    else:\n        print('Please use .csv or .mat format.')\n        df_all = None\n\n    return df_all", "response": "Import ALL data from the result files containing SIP -04 measurement results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_label(key):\n    if key in rel:\n        if mpl.rcParams['text.usetex']:\n            return rel[key]['label_latex']\n        else:\n            return rel[key]['label_mpl']\n    else:\n        return key", "response": "Convenience function to get the label of a given key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_session(db_url=None, echo=False, engine=None, settings=None):\n    if engine is None:\n        engine = init_engine(db_url=db_url, echo=echo, settings=settings)\n    return sessionmaker(bind=engine)", "response": "A SQLAlchemy Session requires that an engine be initialized if one isn t provided."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_crtomo(self, directory, frequency_file='frequencies.dat',\n                      data_prefix='volt_', **kwargs):\n        \"\"\"CRTomo importer\"\"\"\n\n        # we get not electrode positions (dummy1) and no topography data\n        # (dummy2)\n        df, dummy1, dumm2 = reda_crtomo_exporter.load_seit_data(\n            directory, frequency_file, data_prefix, **kwargs)\n        self._add_to_container(df)\n\n        print('Summary:')\n        self._describe_data(df)", "response": "Import CRTomo data from seit. dat into the current container."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_sip256c(self, filename, settings=None, reciprocal=None,\n                       **kwargs):\n        \"\"\"Radic SIP256c data import\"\"\"\n        if settings is None:\n            settings = {}\n        # we get not electrode positions (dummy1) and no topography data\n        # (dummy2)\n        df, dummy1, dummy2 = reda_sip256c.parse_radic_file(\n            filename, settings, reciprocal=reciprocal, **kwargs)\n        self._add_to_container(df)\n\n        print('Summary:')\n        self._describe_data(df)", "response": "Radic SIP256c data import"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_eit_fzj(self, filename, configfile, correction_file=None,\n                       timestep=None, **kwargs):\n        \"\"\"EIT data import for FZJ Medusa systems\"\"\"\n        # we get not electrode positions (dummy1) and no topography data\n        # (dummy2)\n        df_emd, dummy1, dummy2 = eit_fzj.read_3p_data(\n            filename,\n            configfile,\n            **kwargs\n        )\n        if correction_file is not None:\n            eit_fzj_utils.apply_correction_factors(df_emd, correction_file)\n\n        if timestep is not None:\n            df_emd['timestep'] = timestep\n\n        self._add_to_container(df_emd)\n\n        print('Summary:')\n        self._describe_data(df_emd)", "response": "Import EIT data from 3P file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the given dataframe for the required columns", "response": "def check_dataframe(self, dataframe):\n        \"\"\"Check the given dataframe for the required columns\n        \"\"\"\n        required_columns = (\n            'a',\n            'b',\n            'm',\n            'n',\n            'r',\n        )\n        for column in required_columns:\n            if column not in dataframe:\n                raise Exception('Required column not in dataframe: {0}'.format(\n                    column\n                ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(self, query, inplace=True):\n        # TODO: add to queue\n        result = self.data.query(query, inplace=inplace)\n        return result", "response": "Query the data for the given query."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves frequencies from the dataset", "response": "def remove_frequencies(self, fmin, fmax):\n        \"\"\"Remove frequencies from the dataset\n        \"\"\"\n        self.data.query(\n            'frequency > {0} and frequency < {1}'.format(fmin, fmax),\n            inplace=True\n        )\n        g = self.data.groupby('frequency')\n        print('Remaining frequencies:')\n        print(sorted(g.groups.keys()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the K - factor over the homogeneous half - space of the data.", "response": "def compute_K_analytical(self, spacing):\n        \"\"\"Assuming an equal electrode spacing, compute the K-factor over a\n        homogeneous half-space.\n\n        For more complex grids, please refer to the module:\n        reda.utils.geometric_factors\n\n        Parameters\n        ----------\n        spacing: float\n            Electrode spacing\n\n        \"\"\"\n        assert isinstance(spacing, Number)\n        K = geometric_factors.compute_K_analytical(self.data, spacing)\n        self.data = geometric_factors.apply_K(self.data, K)\n        fix_sign_with_K(self.data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scatter_norrec(self, filename=None, individual=False):\n        # if not otherwise specified, use these column pairs:\n        std_diff_labels = {\n            'r': 'rdiff',\n            'rpha': 'rphadiff',\n        }\n\n        diff_labels = std_diff_labels\n\n        # check which columns are present in the data\n        labels_to_use = {}\n        for key, item in diff_labels.items():\n            # only use if BOTH columns are present\n            if key in self.data.columns and item in self.data.columns:\n                labels_to_use[key] = item\n\n        g_freq = self.data.groupby('frequency')\n        frequencies = list(sorted(g_freq.groups.keys()))\n\n        if individual:\n            figures = {}\n            axes_all = {}\n        else:\n            Nx = len(labels_to_use.keys())\n            Ny = len(frequencies)\n            fig, axes = plt.subplots(\n                Ny, Nx,\n                figsize=(Nx * 2.5, Ny * 2.5)\n            )\n\n        for row, (name, item) in enumerate(g_freq):\n            if individual:\n                fig, axes_row = plt.subplots(\n                    1, 2, figsize=(16 / 2.54, 6 / 2.54))\n            else:\n                axes_row = axes[row, :]\n            # loop over the various columns\n            for col_nr, (key, diff_column) in enumerate(\n                    sorted(labels_to_use.items())):\n                indices = np.where(~np.isnan(item[diff_column]))[0]\n                ax = axes_row[col_nr]\n                ax.scatter(\n                    item[key],\n                    item[diff_column],\n                )\n                ax.set_xlabel(key)\n                ax.set_ylabel(diff_column)\n                ax.set_title('N: {}'.format(len(indices)))\n            if individual:\n                fig.tight_layout()\n                figures[name] = fig\n                axes_all[name] = axes_row\n\n        if individual:\n            return figures, axes_all\n        else:\n            fig.tight_layout()\n            return fig, axes", "response": "Create a scatter plot for all diff pairs of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_incomplete_spectra(self, flimit=1000, percAccept=85):\n        assert percAccept > 0 and percAccept < 100\n\n        def _retain_only_complete_spectra(item, fmax, acceptN):\n            \"\"\"Function called using pd.filter, applied to all spectra in the\n            data set. Return true if the number of data points <= **fmax** in\n            item is equal, or larger, than **acceptN**.\n\n            Parameters\n            ----------\n            item : :py:class:`pandas.DataFrame`\n                dataframe containing one spectrum\n            fmax : float\n                maximum frequency up to which data points are counted\n            acceptN : int\n                the number of data points required to pass this test\n\n            Returns\n            -------\n            true : bool\n                if enough data points are present\n            false : bool\n                if not enough data points are present\n            \"\"\"\n            frequencies = item['frequency'].loc[item['frequency'] < fmax]\n            fN = frequencies.size\n            if fN >= acceptN:\n                return True\n            return False\n\n        group_abmn = self.data.groupby(['a', 'b', 'm', 'n'])\n        frequencies = np.array(\n            list(sorted(self.data.groupby('frequency').groups.keys()))\n        )\n        assert flimit >= frequencies.min() and flimit <= frequencies.max()\n        Nlimit = len(np.where(frequencies <= flimit)[0])\n        Naccept = np.ceil(Nlimit * percAccept / 100.0)\n        self.data = group_abmn.filter(\n            _retain_only_complete_spectra, fmax=flimit, acceptN=Naccept\n        ).copy()", "response": "Filter incomplete data points in the current set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a spectrum and its reciprocal counter part if present in the available set of reciprocals.", "response": "def get_spectrum(self, nr_id=None, abmn=None, plot_filename=None):\n        \"\"\"Return a spectrum and its reciprocal counter part, if present in the\n        dataset. Optimally, refer to the spectrum by its normal-reciprocal id.\n\n        Returns\n        -------\n        spectrum_nor : :py:class:`reda.eis.plots.sip_response`\n            Normal spectrum. None if no normal spectrum is available\n        spectrum_rec : :py:class:`reda.eis.plots.sip_response` or None\n            Reciprocal spectrum. None if no reciprocal spectrum is available\n        fig : :py:class:`matplotlib.Figure.Figure` , optional\n            Figure object (only if plot_filename is set)\n\n        \"\"\"\n        assert nr_id is None or abmn is None\n        # determine nr_id for given abmn tuple\n        if abmn is not None:\n            subdata = self.data.query(\n                'a == {} and b == {} and m == {} and n == {}'.format(*abmn)\n            ).sort_values('frequency')\n            if subdata.shape[0] == 0:\n                return None, None\n\n            # determine the norrec-id of this spectrum\n            nr_id = subdata['id'].iloc[0]\n\n        # get spectra\n        subdata_nor = self.data.query(\n            'id == {} and norrec==\"nor\"'.format(nr_id)\n        ).sort_values('frequency')\n\n        subdata_rec = self.data.query(\n            'id == {} and norrec==\"rec\"'.format(nr_id)\n        ).sort_values('frequency')\n\n        # create spectrum objects\n        spectrum_nor = None\n        spectrum_rec = None\n\n        if subdata_nor.shape[0] > 0:\n            spectrum_nor = eis_plot.sip_response(\n                frequencies=subdata_nor['frequency'].values,\n                rmag=subdata_nor['r'],\n                rpha=subdata_nor['rpha'],\n            )\n        if subdata_rec.shape[0] > 0:\n            spectrum_rec = eis_plot.sip_response(\n                frequencies=subdata_rec['frequency'].values,\n                rmag=subdata_rec['r'],\n                rpha=subdata_rec['rpha'],\n            )\n        if plot_filename is not None:\n            if spectrum_nor is not None:\n                fig = spectrum_nor.plot(\n                    plot_filename,\n                    reciprocal=spectrum_rec,\n                    return_fig=True,\n                    title='a: {} b: {} m: {}: n: {}'.format(\n                        *subdata_nor[['a', 'b', 'm', 'n']].values[0, :]\n                    )\n                )\n                return spectrum_nor, spectrum_rec, fig\n        return spectrum_nor, spectrum_rec"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_all_spectra(self, outdir):\n        os.makedirs(outdir, exist_ok=True)\n\n        g = self.data.groupby('id')\n        for nr, (name, item) in enumerate(g):\n            print(\n                'Plotting spectrum with id {} ({} / {})'.format(\n                    name, nr, len(g.groups.keys()))\n            )\n            plot_filename = ''.join((\n                outdir + os.sep,\n                '{:04}_spectrum_id_{}.png'.format(nr, name)\n            ))\n            spec_nor, spec_rec, spec_fig = self.get_spectrum(\n                nr_id=name,\n                plot_filename=plot_filename\n            )\n            plt.close(spec_fig)", "response": "This function plots all the spectra currently stored in the container. It will iterate over all ids and plot the spectrum for each."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a multi - plot with one pseudosection for each frequency.", "response": "def plot_pseudosections(self, column, filename=None, return_fig=False):\n        \"\"\"Create a multi-plot with one pseudosection for each frequency.\n\n        Parameters\n        ----------\n        column : string\n            which column to plot\n        filename : None|string\n            output filename. If set to None, do not write to file. Default:\n            None\n        return_fig : bool\n            if True, return the generated figure object. Default: False\n\n        Returns\n        -------\n        fig : None|matplotlib.Figure\n            if return_fig is set to True, return the generated Figure object\n        \"\"\"\n        assert column in self.data.columns\n\n        g = self.data.groupby('frequency')\n        fig, axes = plt.subplots(\n            4, 2,\n            figsize=(15 / 2.54, 20 / 2.54),\n            sharex=True, sharey=True\n        )\n        for ax, (key, item) in zip(axes.flat, g):\n            fig, ax, cb = PS.plot_pseudosection_type2(\n                item, ax=ax, column=column\n            )\n            ax.set_title('f: {} Hz'.format(key))\n        fig.tight_layout()\n        if filename is not None:\n            fig.savefig(filename, dpi=300)\n\n        if return_fig:\n            return fig\n        else:\n            plt.close(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting the sEIT data into a directory.", "response": "def export_to_directory_crtomo(self, directory, norrec='norrec'):\n        \"\"\"Export the sEIT data into data files that can be read by CRTomo.\n\n        Parameters\n        ----------\n        directory : string\n            output directory. will be created if required\n        norrec : string (nor|rec|norrec)\n            Which data to export. Default: norrec\n\n        \"\"\"\n        exporter_crtomo.write_files_to_directory(\n            self.data, directory, norrec=norrec\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting the data to a ready - initialized seit - manager object.", "response": "def export_to_crtomo_seit_manager(self, grid):\n        \"\"\"Return a ready-initialized seit-manager object from the CRTomo\n        tools. This function only works if the crtomo_tools are installed.\n        \"\"\"\n        import crtomo\n        g = self.data.groupby('frequency')\n        seit_data = {}\n        for name, item in g:\n            print(name, item.shape, item.size)\n            if item.shape[0] > 0:\n                seit_data[name] = item[\n                    ['a', 'b', 'm', 'n', 'r', 'rpha']\n                ].values\n        seit = crtomo.eitMan(grid=grid, seit_data=seit_data)\n        return seit"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_hky_pij(t, frequencies, kappa):\n    pi_a, pi_c, pi_g, pi_t = frequencies\n    pi_ag = pi_a + pi_g\n    pi_ct = pi_c + pi_t\n    beta = .5 / (pi_ag * pi_ct + kappa * (pi_a * pi_g + pi_c * pi_t))\n\n    exp_min_beta_t = np.exp(-beta * t)\n    exp_ct = np.exp(-beta * t * (1. + pi_ct * (kappa - 1.))) / pi_ct\n    exp_ag = np.exp(-beta * t * (1. + pi_ag * (kappa - 1.))) / pi_ag\n    ct_sum = (pi_ct + pi_ag * exp_min_beta_t) / pi_ct\n    ag_sum = (pi_ag + pi_ct * exp_min_beta_t) / pi_ag\n    p = np.ones((4, 4), dtype=np.float64) * (1 - exp_min_beta_t)\n    p *= frequencies\n\n    p[T, T] = pi_t * ct_sum + pi_c * exp_ct\n    p[T, C] = pi_c * ct_sum - pi_c * exp_ct\n\n    p[C, T] = pi_t * ct_sum - pi_t * exp_ct\n    p[C, C] = pi_c * ct_sum + pi_t * exp_ct\n\n    p[A, A] = pi_a * ag_sum + pi_g * exp_ag\n    p[A, G] = pi_g * ag_sum - pi_g * exp_ag\n\n    p[G, A] = pi_a * ag_sum - pi_a * exp_ag\n    p[G, G] = pi_g * ag_sum + pi_a * exp_ag\n\n    return p", "response": "Calculates the probability matrix of substitutions i - > j over time t with HKY model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tape(self, start=0, end=10):\n        '''Pretty prints the tape values'''\n        self.tape_start = start\n        self.tape_end = end\n        self.tape_length = end - start\n        tmp = '\\n'+\"|\"+str(start)+\"|  \"\n        for i in xrange(len(self.tape[start:end])):\n            if i == self.cur_cell:\n                tmp += \"[\" + str(self.tape[i]) + \"] \"\n            else: tmp += \":\" + str(self.tape[i]) + \": \"\n        tmp += \" |\"+str(end)+\"|\"\n        return tmp", "response": "Pretty prints the tape values"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports SIP -04 measurement data from. mat or. csv file containing SIP -04 measurement results", "response": "def import_sip04(self, filename, timestep=None):\n        \"\"\"SIP04 data import\n\n        Parameters\n        ----------\n        filename: string\n            Path to .mat or .csv file containing SIP-04 measurement results\n\n        Examples\n        --------\n\n        ::\n\n            import tempfile\n            import reda\n            with tempfile.TemporaryDirectory() as fid:\n                reda.data.download_data('sip04_fs_01', fid)\n                sip = reda.SIP()\n                sip.import_sip04(fid + '/sip_dataA.mat')\n\n        \"\"\"\n        df = reda_sip04.import_sip04_data(filename)\n        if timestep is not None:\n            print('adding timestep')\n            df['timestep'] = timestep\n\n        self._add_to_container(df)\n        print('Summary:')\n        self._describe_data(df)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_dataframe(self, dataframe):\n        if dataframe is None:\n            return None\n\n        # is this a DataFrame\n        if not isinstance(dataframe, pd.DataFrame):\n            raise Exception(\n                'The provided dataframe object is not a pandas.DataFrame'\n            )\n\n        for column in self.required_columns:\n            if column not in dataframe:\n                raise Exception('Required column not in dataframe: {0}'.format(\n                    column\n                ))\n        return dataframe", "response": "Check the given dataframe for the required type and columns\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreducing the duplicate frequencies of the current record.", "response": "def reduce_duplicate_frequencies(self):\n        \"\"\"In case multiple frequencies were measured, average them and compute\n        std, min, max values for zt.\n\n        In case timesteps were added (i.e., multiple separate measurements),\n        group over those and average for each timestep.\n\n        Examples\n        --------\n\n        ::\n\n            import tempfile\n            import reda\n            with tempfile.TemporaryDirectory() as fid:\n                reda.data.download_data('sip04_fs_06', fid)\n                sip = reda.SIP()\n                sip.import_sip04(fid + '/sip_dataA.mat', timestep=0)\n                # well, add the spectrum again as another timestep\n                sip.import_sip04(fid + '/sip_dataA.mat', timestep=1)\n            df = sip.reduce_duplicate_frequencies()\n\n        \"\"\"\n        group_keys = ['frequency', ]\n        if 'timestep' in self.data.columns:\n            group_keys = group_keys + ['timestep', ]\n\n        g = self.data.groupby(group_keys)\n\n        def group_apply(item):\n            y = item[['zt_1', 'zt_2', 'zt_3']].values.flatten()\n            zt_imag_std = np.std(y.imag)\n            zt_real_std = np.std(y.real)\n            zt_imag_min = np.min(y.imag)\n            zt_real_min = np.min(y.real)\n            zt_imag_max = np.max(y.imag)\n            zt_real_max = np.max(y.real)\n            zt_imag_mean = np.mean(y.imag)\n            zt_real_mean = np.mean(y.real)\n            dfn = pd.DataFrame(\n                {\n                    'zt_real_mean': zt_real_mean,\n                    'zt_real_std': zt_real_std,\n                    'zt_real_min': zt_real_min,\n                    'zt_real_max': zt_real_max,\n                    'zt_imag_mean': zt_imag_mean,\n                    'zt_imag_std': zt_imag_std,\n                    'zt_imag_min': zt_imag_min,\n                    'zt_imag_max': zt_imag_max,\n                },\n                index=[0, ]\n            )\n\n            dfn['count'] = len(y)\n            dfn.index.name = 'index'\n            return dfn\n\n        p = g.apply(group_apply)\n        p.index = p.index.droplevel('index')\n        if len(group_keys) > 1:\n            p = p.swaplevel(0, 1).sort_index()\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the module and return the required class.", "response": "def _load_class(class_path):\n    \"\"\"Load the module and return the required class.\"\"\"\n    parts = class_path.rsplit('.', 1)\n    module = __import__(parts[0], fromlist=parts[1])\n    return getattr(module, parts[1])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollapsing a child node into its parent if they are in the same state.", "response": "def collapse_vertically(tree, columns):\n    \"\"\"\n    Collapses a child node into its parent if they are in the same state.\n    :param columns: a list of characters\n    :param tree: ete3.Tree\n    :return: void, modifies the input tree\n    \"\"\"\n\n    def _same_states(node1, node2, columns):\n        for column in columns:\n            if getattr(node1, column, set()) != getattr(node2, column, set()):\n                return False\n        return True\n\n    num_collapsed = 0\n    for n in tree.traverse('postorder'):\n        if n.is_leaf():\n            continue\n\n        children = list(n.children)\n        for child in children:\n            # merge the child into this node if their states are the same\n            if _same_states(n, child, columns):\n                getattr(n, TIPS_INSIDE).extend(getattr(child, TIPS_INSIDE))\n                getattr(n, INTERNAL_NODES_INSIDE).extend(getattr(child, INTERNAL_NODES_INSIDE))\n\n                n.remove_child(child)\n                grandchildren = list(child.children)\n                for grandchild in grandchildren:\n                    n.add_child(grandchild)\n                num_collapsed += 1\n    if num_collapsed:\n        logging.getLogger('pastml').debug('Collapsed vertically {} internal nodes without state change.'\n                                          .format(num_collapsed))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_mediators(tree, columns):\n    num_removed = 0\n    for n in tree.traverse('postorder'):\n        if getattr(n, METACHILD, False) or n.is_leaf() or len(n.children) > 1 or n.is_root() \\\n                or getattr(n, NUM_TIPS_INSIDE) > 0:\n            continue\n\n        parent = n.up\n        child = n.children[0]\n\n        compatible = True\n        for column in columns:\n            states = getattr(n, column, set())\n            parent_states = getattr(parent, column, set())\n            child_states = getattr(child, column, set())\n            # if mediator has unresolved states, it should hesitate between the parent and the child:\n            if states != child_states | parent_states:\n                compatible = False\n                break\n\n        if compatible:\n            parent.remove_child(n)\n            parent.add_child(child)\n            num_removed += 1\n    if num_removed:\n        logging.getLogger('pastml').debug(\"Removed {} internal node{}\"\n                                          \" with the state unresolved between the parent's and the only child's.\"\n                                          .format(num_removed, '' if num_removed == 1 else 's'))", "response": "Removes intermediate nodes that are just mediators between their parent and child states."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rev_comp( seq, molecule='dna' ):\n\t\n\tif molecule == 'dna':\n\t\tnuc_dict = { \"A\":\"T\", \"B\":\"V\", \"C\":\"G\", \"D\":\"H\", \"G\":\"C\", \"H\":\"D\", \"K\":\"M\", \"M\":\"K\", \"N\":\"N\", \"R\":\"Y\", \"S\":\"S\", \"T\":\"A\", \"V\":\"B\", \"W\":\"W\", \"Y\":\"R\" }\n\telif molecule == 'rna':\n\t\tnuc_dict = { \"A\":\"U\", \"B\":\"V\", \"C\":\"G\", \"D\":\"H\", \"G\":\"C\", \"H\":\"D\", \"K\":\"M\", \"M\":\"K\", \"N\":\"N\", \"R\":\"Y\", \"S\":\"S\", \"U\":\"A\", \"V\":\"B\", \"W\":\"W\", \"Y\":\"R\" }\n\telse:\n\t\traise ValueError( \"rev_comp requires molecule to be dna or rna\" )\n\t\n\tif not isinstance( seq, six.string_types ):\n\t\traise TypeError( \"seq must be a string!\" )\n\t\n\treturn ''.join( [ nuc_dict[c] for c in seq.upper()[::-1] ] )", "response": "Reverse complement a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndegenerating sequence -> regex Example: NNYCGAARN -> [ACGT]{2}[CT]CGA{2}[AG][ACGT]", "response": "def make_degenerate_regex( motif_seq, molecule='dna' ):\n\t\"\"\" Degenerate sequence -> regex\n\tExample: NNYCGAARN -> [ACGT]{2}[CT]CGA{2}[AG][ACGT]\n\t\"\"\"\n\t\n\tif not isinstance( motif_seq, six.string_types ):\n\t\traise TypeError( \"motif_seq must be a string!\" )\n\t\t\n\tif molecule == 'dna':\n\t\tdegenerate_code = { \"A\":\"A\", \"B\":\"[CGT]\", \"C\":\"C\", \"D\":\"[AGT]\", \"G\":\"G\", \"H\":\"[ACT]\", \"K\":\"[GT]\", \"M\":\"[AC]\", \"N\":\"[ACGT]\", \"R\":\"[AG]\", \"S\":\"[GC]\", \"T\":\"T\", \"V\":\"[ACG]\", \"W\":\"[AT]\", \"Y\":\"[CT]\" }\n\telif molecule == 'rna':\n\t\tdegenerate_code = { \"A\":\"A\", \"B\":\"[CGU]\", \"C\":\"C\", \"D\":\"[AGU]\", \"G\":\"G\", \"H\":\"[ACU]\", \"K\":\"[GU]\", \"M\":\"[AC]\", \"N\":\"[ACGU]\", \"R\":\"[AG]\", \"S\":\"[GC]\", \"U\":\"U\", \"V\":\"[ACG]\", \"W\":\"[AU]\", \"Y\":\"[CU]\" }\n\telse:\n\t\traise ValueError( \"make_degenerate_regex requires molecule to be dna or rna\" )\n\t\n\tregex_string = ''\n\t\n\tidx = 0\n\t\n\twhile idx < len( motif_seq ):\n\t\tcurr = motif_seq[idx]\n\t\t\n\t\tcount = 1\n\t\t\n\t\tfor next_idx in range( idx+1, len( motif_seq ) ):\n\t\t\tnext = motif_seq[next_idx]\n\t\t\t\n\t\t\tif next == curr:\n\t\t\t\tcount += 1\n\t\t\telse:\n\t\t\t\tbreak\n\t\t\n\t\tregex_string += degenerate_code[curr]\n\t\t\n\t\tif count > 1:\n\t\t\tidx = idx + count - 1\n\t\t\tregex_string += \"{%s}\" % ( count )\n\t\t\n\t\tidx += 1\n\t\n\treturn regex_string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visitBaseDecl(self, ctx: ShExDocParser.BaseDeclContext):\n        self.context.base = None\n        self.context.base = self.context.iriref_to_shexj_iriref(ctx.IRIREF())", "response": "baseDecl is a base decl"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef visitPrefixDecl(self, ctx: ShExDocParser.PrefixDeclContext):\n        iri = self.context.iriref_to_shexj_iriref(ctx.IRIREF())\n        prefix = ctx.PNAME_NS().getText()\n        if iri not in self.context.ld_prefixes:\n            self.context.prefixes.setdefault(prefix, iri.val)", "response": "visitPrefixDecl : Add prefix to the context if it is not already there."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvisit a START context and set the schema. start attribute", "response": "def visitStart(self, ctx: ShExDocParser.StartContext):\n        \"\"\" start: KW_START '=' shapeExpression \"\"\"\n        shexpr = ShexShapeExpressionParser(self.context, None)\n        shexpr.visit(ctx.shapeExpression())\n        self.context.schema.start = shexpr.expr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visitShapeExprDecl(self, ctx: ShExDocParser.ShapeExprDeclContext):\n        label = self.context.shapeexprlabel_to_IRI(ctx.shapeExprLabel())\n        if self.context.schema.shapes is None:\n            self.context.schema.shapes = []\n        if ctx.KW_EXTERNAL():\n            shape = ShapeExternal(id=label)\n        else:\n            shexpr = ShexShapeExpressionParser(self.context, label)\n            shexpr.visit(ctx.shapeExpression())\n            shape = shexpr.expr\n        self.context.schema.shapes.append(shape)", "response": "ShapeExprDecl is a wrapper for ShexShapeExpressionParser. shapeExpr"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nalternate constructor intended for using JSON format of private key.", "response": "def from_json(cls, key, scopes, subject=None):\n        \"\"\"Alternate constructor intended for using JSON format of private key.\n\n        Args:\n            key (dict) - Parsed JSON with service account credentials.\n            scopes (Union[str, collections.Iterable[str]]) -\n                List of permissions that the application requests.\n            subject (str) - The email address of the user for which\n                the application is requesting delegated access.\n\n        Returns:\n            ServiceAccount\n        \"\"\"\n        credentials_type = key['type']\n        if credentials_type != 'service_account':\n            raise ValueError('key: expected type service_account '\n                             '(got %s)' % credentials_type)\n        email = key['client_email']\n        key = OpenSSL.crypto.load_privatekey(OpenSSL.crypto.FILETYPE_PEM,\n                                             key['private_key'])\n\n        return cls(key=key, email=email, scopes=scopes, subject=subject)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nalternates constructor intended for using PKCS12 files.", "response": "def from_pkcs12(cls, key, email, scopes, subject=None,\n                    passphrase=PKCS12_PASSPHRASE):\n        \"\"\"Alternate constructor intended for using .p12 files.\n\n        Args:\n            key (dict) - Parsed JSON with service account credentials.\n            email (str) - Service account email.\n            scopes (Union[str, collections.Iterable[str]]) -\n                List of permissions that the application requests.\n            subject (str) - The email address of the user for which\n                the application is requesting delegated access.\n            passphrase (str) - Passphrase of private key file.\n                Google generates .p12 files secured with fixed 'notasecret'\n                passphrase, so if you didn't change it it's fine to omit\n                this parameter.\n\n        Returns:\n            ServiceAccount\n        \"\"\"\n        key = OpenSSL.crypto.load_pkcs12(key, passphrase).get_privatekey()\n        return cls(key=key, email=email, scopes=scopes, subject=subject)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the time when the access token was requested.", "response": "def issued_at(self):\n        \"\"\"Time when access token was requested, as seconds since epoch.\n\n        Note:\n            Accessing this property when there wasn't any request attempts\n            will return current time.\n\n        Returns:\n            int\n        \"\"\"\n        issued_at = self._issued_at\n        if issued_at is None:\n            self._issued_at = int(time.time())\n        return self._issued_at"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef access_token(self):\n        if (self._access_token is None or\n                self.expiration_time <= int(time.time())):\n            resp = self.make_access_request()\n            self._access_token = resp.json()['access_token']\n\n        return self._access_token", "response": "Stores always valid OAuth2 access token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_access_request(self):\n        del self.issued_at\n\n        assertion = b'.'.join((self.header(), self.claims(), self.signature()))\n        post_data = {\n            'grant_type': GRANT_TYPE,\n            'assertion': assertion,\n        }\n\n        resp = requests.post(AUDIENCE, post_data)\n\n        if resp.status_code != 200:\n            raise AuthenticationError(resp)\n\n        return resp", "response": "Makes an OAuth2 access token request with crafted JWT and signature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authorized_request(self, method, url, **kwargs):\n        headers = kwargs.pop('headers', {})\n        if headers.get('Authorization') or kwargs.get('auth'):\n            raise ValueError(\"Found custom Authorization header, \"\n                             \"method call would override it.\")\n        headers['Authorization'] = 'Bearer ' + self.access_token\n\n        return requests.request(method, url, headers=headers, **kwargs)", "response": "Shortcut for requests. request with proper Authorization header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert the x - coordinates of a syscal system into a single systemcal system that can be used in the abmn - X system.", "response": "def _convert_coords_to_abmn_X(data, **kwargs):\n    \"\"\"The syscal only stores positions for the electrodes. Yet, we need to\n    infer electrode numbers for (a,b,m,n) by means of some heuristics. This\n    heuristic uses the x-coordinates to infer an electrode spacing (y/z\n    coordinates are ignored). We also assume a constant spacing of electrodes\n    (i.e., a gap in electrode positions would indicate unused electrodes). This\n    is usually a good estimate as hardly anybody does change the electrode\n    positions stored in the Syscal system (talk to us if you do).\n\n    Note that this function can use user input to simplify the process by using\n    a user-supplied x0 value for the smallest electrode position (corresponding\n    to electrode 1) and a user-supplied spacing (removing the need to infer\n    from the positions).\n\n    Parameters\n    ----------\n    data : Nx4 array|Nx4 :py:class:`pandas.DataFrame`\n        The x positions of a, b, m, n electrodes. N is the number of\n        measurements\n    x0 : float, optional\n        position of first electrode. If not given, then use the smallest\n        x-position in the data as the first electrode.\n    spacing : float\n        electrode spacing. This is important if not all electrodes are used in\n        a given measurement setup. If not given, then the smallest distance\n        between electrodes is assumed to be the electrode spacing. Naturally,\n        this requires measurements (or injections) with subsequent electrodes.\n\n    Returns\n    -------\n    data_new : Nx4 :py:class:`pandas.DataFrame`\n        The electrode number columns a,b,m,n\n\n    \"\"\"\n    assert data.shape[1] == 4, 'data variable must only contain four columns'\n\n    x0 = kwargs.get(\n        'x0',\n        data.min().min()\n    )\n    electrode_spacing = kwargs.get('spacing', None)\n\n    # try to determine from the data itself\n    if electrode_spacing is None:\n        electrode_positions = data.values\n        electrode_spacing = np.abs(\n            electrode_positions[:, 1:] - electrode_positions[:, 0:-1]\n        ).min()\n\n    data_new = pd.DataFrame()\n    data_new['a'] = (data.iloc[:, 0] - x0) / electrode_spacing + 1\n    data_new['b'] = (data.iloc[:, 1] - x0) / electrode_spacing + 1\n    data_new['m'] = (data.iloc[:, 2] - x0) / electrode_spacing + 1\n    data_new['n'] = (data.iloc[:, 3] - x0) / electrode_spacing + 1\n\n    # convert to integers\n    for col in (('a', 'b', 'm', 'n')):\n        data_new[col] = data_new[col].astype(int)\n\n    return data_new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_txt(filename, **kwargs):\n    # read in text file into a buffer\n    with open(filename, 'r') as fid:\n        text = fid.read()\n    strings_to_replace = {\n        'Mixed / non conventional': 'Mixed/non-conventional',\n        'Date': 'Date Time AM-PM',\n    }\n    for key in strings_to_replace.keys():\n        text = text.replace(key, strings_to_replace[key])\n\n    buffer = StringIO(text)\n\n    # read data file\n    data_raw = pd.read_csv(\n        buffer,\n        # sep='\\t',\n        delim_whitespace=True,\n    )\n\n    # clean up column names\n    data_raw.columns = [x.strip() for x in data_raw.columns.tolist()]\n\n    # generate electrode positions\n    data = _convert_coords_to_abmn_X(\n        data_raw[['Spa.1', 'Spa.2', 'Spa.3', 'Spa.4']],\n        **kwargs\n    )\n\n    # [mV] / [mA]\n    data['r'] = data_raw['Vp'] / data_raw['In']\n    data['Vmn'] = data_raw['Vp']\n    data['Iab'] = data_raw['In']\n\n    # rename electrode denotations\n    rec_max = kwargs.get('reciprocals', None)\n    if rec_max is not None:\n        print('renumbering electrode numbers')\n        data[['a', 'b', 'm', 'n']] = rec_max + 1 - data[['a', 'b', 'm', 'n']]\n\n    return data, None, None", "response": "Import Syscal measurements from a text file exported as Spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_bin(filename, **kwargs):\n    metadata, data_raw = _import_bin(filename)\n\n    skip_rows = kwargs.get('skip_rows', 0)\n    if skip_rows > 0:\n        data_raw.drop(data_raw.index[range(0, skip_rows)], inplace=True)\n        data_raw = data_raw.reset_index()\n\n    if kwargs.get('check_meas_nums', True):\n        # check that first number is 0\n        if data_raw['measurement_num'].iloc[0] != 0:\n            print('WARNING: Measurement numbers do not start with 0 ' +\n                  '(did you download ALL data?)')\n\n        # check that all measurement numbers increase by one\n        if not np.all(np.diff(data_raw['measurement_num'])) == 1:\n            print(\n                'WARNING '\n                'Measurement numbers are not consecutive. '\n                'Perhaps the first measurement belongs to another measurement?'\n                ' Use the skip_rows parameter to skip those measurements'\n            )\n\n        # now check if there is a jump in measurement numbers somewhere\n        # ignore first entry as this will always be nan\n        diff = data_raw['measurement_num'].diff()[1:]\n        jump = np.where(diff != 1)[0]\n        if len(jump) > 0:\n            print('WARNING: One or more jumps in measurement numbers detected')\n            print('The jump indices are:')\n            for jump_nr in jump:\n                print(jump_nr)\n\n            print('Removing data points subsequent to the first jump')\n            data_raw = data_raw.iloc[0:jump[0] + 1, :]\n\n    if data_raw.shape[0] == 0:\n        # no data present, return a bare DataFrame\n        return pd.DataFrame(columns=['a', 'b', 'm', 'n', 'r']), None, None\n\n    data = _convert_coords_to_abmn_X(\n        data_raw[['x_a', 'x_b', 'x_m', 'x_n']],\n        **kwargs\n    )\n    # [mV] / [mA]\n    data['r'] = data_raw['vp'] / data_raw['Iab']\n    data['Vmn'] = data_raw['vp']\n    data['vab'] = data_raw['vab']\n    data['Iab'] = data_raw['Iab']\n\n    data['mdelay'] = data_raw['mdelay']\n    data['Tm'] = data_raw['Tm']\n    data['Mx'] = data_raw['Mx']\n    data['chargeability'] = data_raw['m']\n    data['q'] = data_raw['q']\n\n    # rename electrode denotations\n    rec_max = kwargs.get('reciprocals', None)\n    if rec_max is not None:\n        print('renumbering electrode numbers')\n        data[['a', 'b', 'm', 'n']] = rec_max + 1 - data[['a', 'b', 'm', 'n']]\n\n    # print(data)\n    return data, None, None", "response": "Read a. bin file generated by the IRIS Instruments Syscal Pro System and return a curated dataframe for further processing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _import_bin(filename):\n    fid = open(filename, 'rb')\n\n    def fget(fid, fmt, tsize):\n        buffer = fid.read(tsize)\n        result_raw = struct.unpack(fmt, buffer)\n        if len(result_raw) == 1:\n            return result_raw[0]\n        else:\n            return result_raw\n\n    # determine overall size\n    fid.seek(0, 2)\n    total_size = fid.tell()\n    # print('total size', total_size)\n\n    # start from the beginning\n    fid.seek(0)\n\n    # read version\n    buffer = fid.read(4)\n    version = struct.unpack('I', buffer)\n    # print('version', version)\n    buffer = fid.read(1)\n    typedesyscal = struct.unpack('c', buffer)[0]\n    syscal_type = int.from_bytes(typedesyscal, 'big')\n    # print('Syscal type: {}'.format(syscal_type))\n\n    # comment\n    buffer = fid.read(1024)\n    comment_raw = struct.iter_unpack('c', buffer)\n    comment = ''.join([x[0].decode('utf-8') for x in comment_raw])\n    # print('comment', comment)\n\n    metadata = {\n        'version': version,\n        'syscal_type': syscal_type,\n        'comment': comment,\n    }\n\n    measurements = []\n    # for each measurement\n    counter = 0\n    while(fid.tell() < total_size):\n        # print('COUNTER', counter)\n        buffer = fid.read(2)\n        array_type = struct.unpack('h', buffer)\n        array_type\n        # print(array_type)\n        # not used\n        moretmeasure = fget(fid, 'h', 2)\n        moretmeasure\n        # print('moretmeasure', moretmeasure)\n        # measurement time [ms]\n        mtime = fget(fid, 'f', 4)\n        # print('measurement time', mtime)\n        # delay before IP measurements start [ms]\n        mdelay = fget(fid, 'f', 4)\n        # print('mdelay', mdelay)\n        TypeCpXyz = fget(fid, 'h', 2)\n        # our file format documentation always assumes this value to be == 1\n        assert TypeCpXyz == 1\n        # print('TypeCpXyz', TypeCpXyz)\n        # ignore\n        fget(fid, 'h', 2)\n        # positions: a b m n [m]\n        xpos = fget(fid, '4f', 16)\n        # print('xpos', xpos)\n        ypos = fget(fid, '4f', 16)\n        # print('ypos', ypos)\n        zpos = fget(fid, '4f', 16)\n        # print('zpos', zpos)\n        # self-potential [mV]\n        sp = fget(fid, 'f', 4)\n        # print('sp', sp)\n        # measured voltage at voltage electrodes m and n [mV]\n        vp = fget(fid, 'f', 4)\n        # print('vp', vp)\n        Iab = fget(fid, 'f', 4)\n        # print('iab', Iab)\n        rho = fget(fid, 'f', 4)\n        # print('rho', rho)\n        m = fget(fid, 'f', 4)\n        # print('m', m)\n        # standard deviation\n        q = fget(fid, 'f', 4)\n        # print('q', q)\n        # timing windows\n        Tm = fget(fid, '20f', 20 * 4)\n        Tm = np.array(Tm)\n        # print('Tm', Tm)\n        # chargeabilities\n        Mx = fget(fid, '20f', 20 * 4)\n        Mx = np.array(Mx)\n        # print('Mx', Mx)\n        # this is 4 bytes used to store information on the measured channel\n        # Channel + NbChannel\n        buffer = fid.read(1)\n        buffer_bin = bin(ord(buffer))[2:].rjust(8, '0')\n        # print(buffer_bin)\n        channel = int(buffer_bin[4:], 2)\n        channelnb = int(buffer_bin[0:4], 2)\n        # print('ChannelNB:', channelnb)\n        # print('Channel:', channel)\n        # 4 binaries + unused\n        buffer = fid.read(1)\n        buffer_bin = bin(ord(buffer))[2:].rjust(8, '0')\n        # print(buffer_bin)\n        overload = bool(int(buffer_bin[4]))\n        channel_valid = bool(int(buffer_bin[5]))\n        channel_sync = bool(int(buffer_bin[6]))\n        gap_filler = bool(int(buffer_bin[7]))\n        # print(overload, channel_valid, channel_sync, gap_filler)\n        measurement_num = fget(fid, 'H', 2)\n        # print('measurement_num', measurement_num)\n        filename = fget(fid, '12s', 12)\n        # print('filename', filename)\n        latitude = fget(fid, 'f', 4)\n        # print('lat', latitude)\n        longitude = fget(fid, 'f', 4)\n        # print('long', longitude)\n        # number of stacks\n        NbCren = fget(fid, 'f', 4)\n        # print('Stacks', NbCren)\n        # RS check\n        RsChk = fget(fid, 'f', 4)\n        # print('RsChk', RsChk)\n        # absolute applied voltage\n        vab = fget(fid, 'f', 4)\n        # print('Vab', vab)\n        # tx battery voltage [V]\n        batTX = fget(fid, 'f', 4)\n        # print('batTX', batTX)\n        # rx battery voltage [V]\n        batRX = fget(fid, 'f', 4)\n        # print('batRX', batRX)\n        temperature = fget(fid, 'f', 4)\n        # print('Temp.', temperature)\n        # TODO: date and time not analyzed\n        b = struct.unpack('2f', fid.read(2 * 4))\n        # print('b', b)\n        b\n\n        measurements.append({\n            'version': version,\n            'mtime': mtime,\n            'x_a': xpos[0],\n            'x_b': xpos[1],\n            'x_m': xpos[2],\n            'x_n': xpos[3],\n            'y_a': ypos[0],\n            'y_b': ypos[1],\n            'y_m': ypos[2],\n            'y_n': ypos[3],\n            'z_a': zpos[0],\n            'z_b': zpos[1],\n            'z_m': zpos[2],\n            'z_n': zpos[3],\n            'mdelay': mdelay,\n            'vp': vp,\n            'q': q,\n            'overload': overload,\n            'channel_valid': channel_valid,\n            'channel_sync': channel_sync,\n            'gap_filler': gap_filler,\n            'NbStacks': NbCren,\n            'm': m,\n            'Tm': Tm,\n            'Mx': Mx,\n            'nr': measurement_num,\n            'vab': vab,\n            'channel': channel,\n            'sp': sp,\n            'Iab': Iab,\n            'rho': rho,\n            'latitude': latitude,\n            'longitude': longitude,\n            'channelnb': channelnb,\n            'RsCHk': RsChk,\n            'batTX': batTX,\n            'batRX': batRX,\n            'temperature': temperature,\n            'measurement_num': measurement_num,\n        })\n\n        counter += 1\n\n    # create a dataframe with all primary data\n    df = pd.DataFrame(\n        measurements\n    ).reset_index()\n    return metadata, df", "response": "Read a. bin file generated by the IRIS Instruments Syscal Pro System and return a dataframe containing all measurement data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef call_and_notificate(args, opts):\n    # store starttime\n    stctime = time.clock()\n    stttime = time.time()\n    stdtime = datetime.datetime.now()\n    # call subprocess\n    exit_code, output = call(args)\n    # calculate delta\n    cdelta = time.clock() - stctime\n    tdelta = time.time() - stttime\n    endtime = datetime.datetime.now()\n    if exit_code == 0:\n        status = u\"Success\"\n    else:\n        status = u\"Fail (%d)\" % exit_code\n    # create email body\n    body = EMAIL_BODY % {\n        'prog': get_command_str(args),\n        'status': status,\n        'stdtime': stdtime,\n        'endtime': endtime,\n        'tdelta': tdelta,\n        'cdelta': cdelta,\n        'output': output,\n        'cwd': os.getcwd(),\n    }\n    # create email subject\n    subject = opts.subject % {\n        'prog': get_command_str(args),\n        'status': status.lower(),\n    }\n    # create email message\n    msg = create_message(opts.from_addr,\n                         opts.to_addr,\n                         subject,\n                         body,\n                         opts.encoding)\n    # obtain password from keyring\n    password = keyring.get_password('notify', opts.username)\n    # send email\n    send_email(msg, opts.host, opts.port, opts.username, password)", "response": "Execute specified arguments and send notification email to the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates and returns an array of num_unique_values HEX colours.", "response": "def get_enough_colours(num_unique_values):\n    \"\"\"\n    Generates and returns an array of `num_unique_values` HEX colours.\n    :param num_unique_values: int, number of colours to be generated.\n    :return: array of str, containing colours in HEX format.\n    \"\"\"\n    if num_unique_values in NUM2COLOURS:\n        return NUM2COLOURS[num_unique_values]\n    vs = ['#%02x%02x%02x' % tuple(rgb) for rgb in\n          (map(lambda x: int(x * 255), colorsys.hsv_to_rgb(*hsv)) for hsv in\n           ((_ / num_unique_values, 0.25 * (1 + (_ % 3)), .8) for _ in range(1, num_unique_values + 1)))]\n    if num_unique_values < 20:\n        return vs[::5] + vs[1::5] + vs[2::5] + vs[3::5] + vs[4::5]\n    return vs[::10] + vs[1::10] + vs[2::10] + vs[3::10] + vs[4::10] \\\n           + vs[5::10] + vs[6::10] + vs[7::10] + vs[8::10] + vs[9::10]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the thumbnail format of the file.", "response": "def get_thumbnail_format(self):\n        \"\"\"\n        Determines the target thumbnail type either by looking for a format\n        override specified at the model level, or by using the format the\n        user uploaded.\n        \"\"\"\n        if self.field.thumbnail_format:\n            # Over-ride was given, use that instead.\n            return self.field.thumbnail_format.lower()\n        else:\n            # Use the existing extension from the file.\n            filename_split = self.name.rsplit('.', 1)\n            return filename_split[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the image with thumbnails.", "response": "def save(self, name, content, save=True):\n        \"\"\"\n        Handles some extra logic to generate the thumbnails when the original\n        file is uploaded.\n        \"\"\"\n        super(ImageWithThumbsFieldFile, self).save(name, content, save)\n        try:\n            self.generate_thumbs(name, content)\n        except IOError, exc:\n            if 'cannot identify' in exc.message or \\\n               'bad EPS header' in exc.message:\n                raise UploadedImageIsUnreadableError(\n                    \"We were unable to read the uploaded image. \"\n                    \"Please make sure you are uploading a valid image file.\"\n                )\n            else:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the correct filename for a would - be thumbnail of the given size.", "response": "def _calc_thumb_filename(self, thumb_name):\n        \"\"\"\n        Calculates the correct filename for a would-be (or potentially\n        existing) thumbnail of the given size.\n        \n        NOTE: This includes the path leading up to the thumbnail. IE:\n        uploads/cbid_images/photo.png\n        \n        size: (tuple) In the format of (width, height)\n        \n        Returns a string filename.\n        \"\"\"\n        filename_split = self.name.rsplit('.', 1)\n        file_name = filename_split[0]\n        file_extension = self.get_thumbnail_format()\n\n        return '%s_%s.%s' % (file_name, thumb_name, file_extension)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a thumbnail for the given image and store it in the storage backend.", "response": "def create_and_store_thumb(self, image, thumb_name, thumb_options):\n        \"\"\"\n        Given that 'image' is a PIL Image object, create a thumbnail for the\n        given size tuple and store it via the storage backend.\n        \n        image: (Image) PIL Image object.\n        size: (tuple) Tuple in form of (width, height). Image will be\n            thumbnailed to this size.\n        \"\"\"\n        size = thumb_options['size']\n        upscale = thumb_options.get('upscale', True)\n        crop = thumb_options.get('crop')\n        if crop is True:\n            # We'll just make an assumption here. Center cropping is the\n            # typical default.\n            crop = 'center'\n\n        thumb_filename = self._calc_thumb_filename(thumb_name)\n        file_extension = self.get_thumbnail_format()\n\n        # The work starts here.\n        thumbed_image = THUMBNAIL_ENGINE.create_thumbnail(\n            image,\n            size,\n            crop=crop,\n            upscale=upscale\n        )\n\n        # TODO: Avoiding hitting the disk here, but perhaps we should use temp\n        # files down the road? Big images might choke us as we do part in\n        # RAM then hit swap.\n        img_fobj = cStringIO.StringIO()\n        # This writes the thumbnailed PIL.Image to the file-like object.\n        THUMBNAIL_ENGINE.write(thumbed_image, img_fobj, format=file_extension)\n        # Save the result to the storage backend.\n        thumb_content = ContentFile(img_fobj.getvalue())\n        self.storage.save(thumb_filename, thumb_content)\n        img_fobj.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, save=True):\n        for thumb in self.field.thumbs:\n            thumb_name, thumb_options = thumb\n            thumb_filename = self._calc_thumb_filename(thumb_name)\n            self.storage.delete(thumb_filename)\n\n        super(ImageWithThumbsFieldFile, self).delete(save)", "response": "Deletes the original plus any thumbnails."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self):\n    data = data={\"db-name\":self.db}\n    self.rest('POST', self.uri_str, status_codes=(200,201), data=data)\n    return True", "response": "Creates the database\n    >>> db.create()\n    True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the data structure edn and puts it in the db", "response": "def tx_schema(self, **kwargs):\n    \"\"\" Builds the data structure edn, and puts it in the db\n    \"\"\"\n    for s in self.schema.schema: \n      tx = self.tx(s, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes a raw TX string or get a new TX object to work with. Returns a dict of the response.", "response": "def tx(self, *args, **kwargs):\n    \"\"\" Executes a raw tx string, or get a new TX object to work with.\n\n    Passing a raw string or list of strings will immedately transact \n    and return the API response as a dict.\n    >>> resp = tx('{:db/id #db/id[:db.part/user] :person/name \"Bob\"}')\n    {db-before: db-after: tempids: }\n\n    This gets a fresh `TX()` to prepare a transaction with.\n    >>> tx = db.tx()\n\n    New `E()` object with person/fname and person/lname attributes\n    >>> person = tx.add('person/',   {'fname':'John', 'lname':'Doe'})\n\n    New state and city objects referencing the state\n    >>> state  = tx.add('loc/state', 'WA')\n    >>> city   = tx.add('loc/city',  'Seattle', 'isin', state)\n\n    Add person/city, person/state, and person/likes refs to the person entity\n    >>> person.add('person/', {'city': city, 'state': state, 'likes': [city, state]})\n\n    Excute the transaction\n    >>> resp = tx.tx()        \n\n    The resolved entity ids for our person\n    >>> person.eid, state.eid, city.eid\n\n    Fetch all attributes, behave like a dict\n    >>> person.items()\n    >>> person.iteritems()\n\n    Access attribute as an attribute\n    >>> person['person/name']\n\n    See `TX()` for options.\n\n    \"\"\"\n    if 0 == len(args): return TX(self) \n    ops = []\n    for op in args:\n      if isinstance(op, list):            ops += op\n      elif isinstance(op, (str,unicode)): ops.append(op)\n    if 'debug' in kwargs: pp(ops)\n    tx_proc =\"[ %s ]\" % \"\".join(ops)\n    x = self.rest('POST', self.uri_db, data={\"tx-data\": tx_proc})\n    return x"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retract(self, e, a, v):\n    ta = datetime.datetime.now()\n    ret = u\"[:db/retract %i :%s %s]\" % (e, a, dump_edn_val(v))\n    rs = self.tx(ret)\n    tb = datetime.datetime.now() - ta\n    print cl('<<< retracted %s,%s,%s in %sms' % (e,a,v, tb.microseconds/1000.0), 'cyan')\n    return rs", "response": "redact the value of an attribute"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a generator that returns datoms in a specified chunk size.", "response": "def datoms(self, index='aevt', e='', a='', v='', \n                   limit=0, offset=0, chunk=100, \n                   start='', end='', since='', as_of='', history='', **kwargs):\n    \"\"\" Returns a lazy generator that will only fetch groups of datoms\n        at the chunk size specified.\n\n    http://docs.datomic.com/clojure/index.html#datomic.api/datoms\n    \"\"\"\n    assert index in ['aevt','eavt','avet','vaet'], \"non-existant index\"\n    data = {'index':   index, \n            'a':       ':{0}'.format(a) if a else '',\n            'v':       dump_edn_val(v) if v else '',\n            'e':       int(e) if e else '', \n            'offset':  offset or 0,\n            'start':   start,\n            'end':     end,\n            'limit':   limit,\n            'history': 'true' if history else '',\n            'as-of':   int(as_of) if as_of else '',\n            'since':   int(since) if since else '',\n            }\n    data['limit'] = offset + chunk\n    rs = True\n    while rs and (data['offset'] < (limit or 1000000000)):\n      ta = datetime.datetime.now()\n      rs = self.rest('GET', self.uri_db + '-/datoms', data=data, parse=True)\n      if not len(rs):\n        rs = False\n      tb = datetime.datetime.now() - ta\n      print cl('<<< fetched %i datoms at offset %i in %sms' % (\n        len(rs), data['offset'], tb.microseconds/1000.0), 'cyan')\n      for r in rs: yield r\n      data['offset'] += chunk"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndebugs timing terminal output", "response": "def debug(self, defn, args, kwargs, fmt=None, color='green'):\n    \"\"\" debug timing, colored terminal output\n    \"\"\"\n    ta = datetime.datetime.now()\n    rs = defn(*args, **kwargs)  \n    tb = datetime.datetime.now() - ta\n    fmt = fmt or \"processed {defn} in {ms}ms\"\n    logmsg = fmt.format(ms=tb.microseconds/1000.0, defn=defn)\n    \"terminal output\"\n    print cl(logmsg, color)\n    \"logging output\"\n    logging.debug(logmsg)\n    return rs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef q(self, q, inputs=None, limit='', offset='', history=False):\n    if not q.strip().startswith(\"[\"): q = \"[ {0} ]\".format(q)\n    args     = u'[ {:db/alias \"%(store)s/%(db)s\" %(hist)s} %(inputs)s ]' % dict(\n      store  = self.store,\n      db     = self.db,\n      hist   = ':history true' if history==True else '',\n      inputs = \" \".join(inputs or []))\n    data = {\"args\":   args, \n            \"q\":      q, \n            \"offset\": offset or '',\n            \"limit\":  limit  or '',\n            }\n    return self.rest('GET', self.uri_q, data=data, parse=True)", "response": "Query the national von von."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef where(self, *args, **kwargs):\n    \" :where \"\n    [(self._where.append(x)) for x in args]\n    return self", "response": "where \" : where"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef param(self, *args, **kwargs):\n    \" :in   \"\n    for first, second in pairwise(args):\n      if isinstance(second, list):\n        if not isinstance(second[0], list):\n          \" add a logical _or_ \" \n          self._input.append((\n            u\"[{0} ...]\".format(first), second)) \n        else:\n          \" relations, list of list\"\n          self._input.append((\n            u\"[[{0}]]\".format(first), second)) \n      elif isinstance(second, tuple):\n        \" tuple \"\n        self._input.append((\n          u\"[{0}]\".format(first), list(second))) \n      else:\n        \" nothing special \"\n        self._input.append((first,second)) \n    return self", "response": "add a logical or list of list"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute query get back", "response": "def hashone(self):\n    \"execute query, get back\"\n    rs = self.one()\n    if not rs: \n      return {}\n    else:\n      finds = \" \".join(self._find).split(' ')\n      return dict(zip((x.replace('?','') for x in finds), rs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes query get all list of lists", "response": "def all(self):\n    \" execute query, get all list of lists\"\n    query,inputs = self._toedn()\n    return self.db.q(query,\n      inputs  = inputs,\n      limit   = self._limit,\n      offset  = self._offset,\n      history = self._history)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _toedn(self):\n    finds  = u\"\"\n    inputs = u\"\"\n    wheres = u\"\"\n    args   = []\n    \": in and args\"\n    for a,b in self._input:\n      inputs += \" {0}\".format(a)\n      args.append(dump_edn_val(b))\n    if inputs:\n      inputs = u\":in ${0}\".format(inputs)\n    \" :where \"\n    for where in self._where:\n      if isinstance(where, (str,unicode)): \n        wheres += u\"[{0}]\".format(where)\n      elif isinstance(where, (list)):\n        wheres += u\" \".join([u\"[{0}]\".format(w) for w in where])\n    \" find: \"\n    if self._find == []: #find all\n      fs = set()\n      for p in wheres.replace('[',' ').replace(']',' ').split(' '):\n        if p.startswith('?'):\n          fs.add(p)\n      self._find = list(fs)\n    finds = \" \".join(self._find)\n    \" all togethr now...\"\n    q = u\"\"\"[ :find {0} {1} :where {2} ]\"\"\".\\\n        format( finds, inputs, wheres)\n    return q,args", "response": "prepare the query for the rest api\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, *args, **kwargs):\n\n    assert self.resp is None, \"Transaction already committed\"\n    entity, av_pairs, args = None, [], list(args)\n    if len(args):\n      if isinstance(args[0], (int, long)): \n        \" first arg is an entity or tempid\"\n        entity = E(args[0], tx=self)\n      elif isinstance(args[0], E):\n        \" dont resuse entity from another tx\"\n        if args[0]._tx is self:\n          entity  = args[0]\n        else:\n          if int(args[0]) > 0:\n            \" use the entity id on a new obj\"\n            entity = E(int(args[0]), tx=self)\n          args[0] = None\n      \" drop the first arg\"\n      if entity is not None or args[0] in (None, False, 0):\n        v = args.pop(0)\n    \" auto generate a temp id?\"\n    if entity is None:\n      entity       = E(self.ctmpid, tx=self)\n      self.ctmpid -= 1\n    \" a,v from kwargs\"\n    if len(args) == 0 and kwargs: \n      for a,v in kwargs.iteritems():\n        self.addeav(entity, a, v)\n    \" a,v from args \"\n    if len(args):\n      assert len(args) % 2 == 0, \"imbalanced a,v in args: \" % args\n      for first, second in pairwise(args):\n        if not first.startswith(':'): \n          first = ':' + first\n        if not first.endswith('/'):\n          \" longhand used: blah/blah \"\n          if isinstance(second, list):\n            for v in second:\n              self.addeav(entity, first, v)\n          else:\n            self.addeav(entity, first, second)\n          continue\n        elif isinstance(second, dict):\n          \" shorthand used: blah/, dict \"\n          for a,v in second.iteritems():\n            self.addeav(entity, \"%s%s\" % (first, a), v)\n            continue\n        elif isinstance(second, (list, tuple)):\n          \" shorthand used: blah/, list|tuple \"\n          for a,v in pairwise(second):\n            self.addeav(entity, \"%s%s\" % (first, a), v)\n            continue\n        else:\n          raise Exception, \"invalid pair: %s : %s\" % (first,second)\n    \"pass back the entity so it can be resolved after tx()\"\n    return entity", "response": "Add a new object to the list of objects in the order they appear in the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, **kwargs):\n    assert self.resp is None, \"Transaction already committed\"\n    try:\n      self.resp = self.db.tx(list(self.edn_iter), **kwargs)\n    except Exception:\n      self.resp = False\n      raise\n    else:\n      self.resolve()\n      self.adds = None\n      self.tmpents = None\n    return self.resp", "response": "Commit the current statements from add"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self):\n    assert isinstance(self.resp, dict), \"Transaction in uncommitted or failed state\"\n    rids = [(v) for k,v in self.resp['tempids'].items()]\n    self.txid = self.resp['tx-data'][0]['tx']\n    rids.reverse()\n    for t in self.tmpents:\n      pos = self.tmpents.index(t)\n      t._eid, t._txid = rids[pos], self.txid\n    for t in self.realents:\n      t._txid = self.txid", "response": "Resolve one or more tempids. \n    Automatically takes place after transaction is executed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate over the user - specified additions.", "response": "def edn_iter(self):\n    \"\"\" yields edns\n    \"\"\"\n    for e,a,v in self.adds:\n      yield u\"{%(a)s %(v)s :db/id #db/id[:db.part/user %(e)s ]}\" % \\\n            dict(a=a, v=dump_edn_val(v), e=int(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_usage(self):\n\n        resp = requests.get(FITNESS_URL, timeout=30)\n        resp.raise_for_status()\n\n        soup = BeautifulSoup(resp.text, \"html5lib\")\n        eastern = pytz.timezone('US/Eastern')\n        output = []\n        for item in soup.findAll(\"div\", {\"class\": \"barChart\"}):\n            data = [x.strip() for x in item.get_text(\"\\n\").strip().split(\"\\n\")]\n            data = [x for x in data if x]\n            name = re.sub(r\"\\s*(Hours)?\\s*-?\\s*(CLOSED|OPEN)?$\", \"\", data[0], re.I).strip()\n            output.append({\n                \"name\": name,\n                \"open\": \"Open\" in data[1],\n                \"count\": int(data[2].rsplit(\" \", 1)[-1]),\n                \"updated\": eastern.localize(datetime.datetime.strptime(data[3][8:].strip(), '%m/%d/%Y %I:%M %p')).isoformat(),\n                \"percent\": int(data[4][:-1])\n            })\n        return output", "response": "Get fitness locations and their current usage."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self, keyword):\n        params = {\n            \"source\": \"map\",\n            \"description\": keyword\n        }\n        data = self._request(ENDPOINTS['SEARCH'], params)\n        data['result_data'] = [res for res in data['result_data'] if isinstance(res, dict)]\n        return data", "response": "Search for buildings related to the provided keyword."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_K(df, k):\n    if 'k' not in df.columns:\n        df['k'] = k\n\n    if 'rho_a' not in df.columns:\n        df['rho_a'] = df['r'] * df['k']\n\n    if 'sigma_a' not in df.columns:\n        df['sigma_a'] = 1.0 / df['rho_a']\n\n    if 'Zt' in df.columns:\n        df['rho_a_complex'] = df['Zt'] * df['k']\n    return df", "response": "Apply the geometric factors to the dataset and compute the parent s resistivities and conductivities"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse a finite - element modeling code to infer geometric factors for a single object in the current directory.", "response": "def compute_K_numerical(dataframe, settings=None, keep_dir=None):\n    \"\"\"Use a finite-element modeling code to infer geometric factors for meshes\n    with topography or irregular electrode spacings.\n\n    Parameters\n    ----------\n    dataframe : pandas.DataFrame\n        the data frame that contains the data\n    settings : dict\n        The settings required to compute the geometric factors. See examples\n        down below for more information in the required content.\n    keep_dir : path\n        if not None, copy modeling dir here\n\n    Returns\n    -------\n    K : :class:`numpy.ndarray`\n        K factors (are also directly written to the dataframe)\n\n    Examples\n    --------\n    ::\n\n        settings = {\n            'rho': 100,\n            'elem': 'elem.dat',\n            'elec': 'elec.dat',\n            'sink_node': '100',\n            '2D': False,\n        }\n\n\n    \"\"\"\n    inversion_code = reda.rcParams.get('geom_factor.inversion_code', 'crtomo')\n    if inversion_code == 'crtomo':\n        import reda.utils.geom_fac_crtomo as geom_fac_crtomo\n        if keep_dir is not None:\n            keep_dir = os.path.abspath(keep_dir)\n        K = geom_fac_crtomo.compute_K(\n            dataframe, settings, keep_dir)\n    else:\n        raise Exception(\n            'Inversion code {0} not implemented for K computation'.format(\n                inversion_code\n            ))\n    return K"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_K_analytical(dataframe, spacing):\n    if isinstance(dataframe, pd.DataFrame):\n        configs = dataframe[['a', 'b', 'm', 'n']].values\n    else:\n        configs = dataframe\n\n    r_am = np.abs(configs[:, 0] - configs[:, 2]) * spacing\n    r_an = np.abs(configs[:, 0] - configs[:, 3]) * spacing\n    r_bm = np.abs(configs[:, 1] - configs[:, 2]) * spacing\n    r_bn = np.abs(configs[:, 1] - configs[:, 3]) * spacing\n\n    K = 2 * np.pi / (1 / r_am - 1 / r_an - 1 / r_bm + 1 / r_bn)\n\n    if isinstance(dataframe, pd.DataFrame):\n        dataframe['k'] = K\n\n    return K", "response": "Compute geometrical factors for homogeneous half - space electrodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget key from object", "response": "def _get_object_key(self, p_object):\n        \"\"\"Get key from object\"\"\"\n        matched_key = None\n        matched_index = None\n\n        if hasattr(p_object, self._searchNames[0]):\n            return getattr(p_object, self._searchNames[0])\n\n        for x in xrange(len(self._searchNames)):\n            key = self._searchNames[x]\n            if hasattr(p_object, key):\n                matched_key = key\n                matched_index = x\n\n        if matched_key is None:\n            raise KeyError()\n\n        if matched_index != 0 and self._searchOptimize:\n            self._searchNames.insert(0, self._searchNames.pop(matched_index))\n\n        return getattr(p_object, matched_key)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextending the list by appending elements from the iterable", "response": "def extend(self, collection):\n        \"\"\" L.extend(iterable) -- extend list by appending elements from the iterable \"\"\"\n        if type(collection) is list:\n            if self._col_dict is None and self._col_list is None:\n                self._col_list = collection\n            else:\n                self._col_list += collection\n            self._sync_list_to_dict(collection)\n        elif type(collection) is dict:\n            if self._col_dict is None and self._col_list is None:\n                self._col_dict = collection\n                self._col_list = collection.values()\n            else:\n                for key, value in collection.items():\n                    self._set_key(key, value)\n        else:\n            raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the first occurrence of value from the list of entries in the column. Raises ValueError if the value is not present.", "response": "def remove(self, value):\n        \"\"\"\n        L.remove(value) -- remove first occurrence of value.\n        Raises ValueError if the value is not present.\n        \"\"\"\n        self._col_list.remove(value)\n        self._col_dict.pop(self._get_object_key(value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort(self, cmp=None, key=None, reverse=False):\n        self._col_list.sort(cmp, key, reverse)", "response": "sort - Sort the log entries in the log"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef popitem(self):\n        key, value = self._col_dict.popitem()\n        if value is not None:\n            self._col_list.remove(value)\n        return key, value", "response": "D. popitem - remove and return some key - value pair as a\n        2 - tuple ; but raise KeyError if D is empty."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset default value for the key k to d", "response": "def setdefault(self, k, d=None):\n        \"\"\" D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D \"\"\"\n        if k not in self._col_dict:\n            self._set_key(k, d)\n        return self._col_dict.get(k)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, E=None, **F):\n        if hasattr(E, 'keys'):\n            self.extend(E)\n        else:\n            for key, value in E:\n                self._set_key(key, value)\n        self.extend(F)", "response": "Update the internal dictionary with the contents of E and F."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef correct(self, temp, we_t):\n        if not PIDTempComp.in_range(temp):\n            return None\n\n        n_t = self.cf_t(temp)\n\n        if n_t is None:\n            return None\n\n        we_c = we_t * n_t\n\n        return we_c", "response": "Compute weC from weT and weT"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef average_repetitions(df, keys_mean):\n    if 'norrec' not in df.columns:\n        raise Exception(\n            'The \"norrec\" column is required for this function to work!'\n        )\n\n    # Get column order to restore later\n    cols = list(df.columns.values)\n\n    keys_keep = list(set(df.columns.tolist()) - set(keys_mean))\n    agg_dict = {x: _first for x in keys_keep}\n    agg_dict.update({x: np.mean for x in keys_mean})\n    for key in ('id', 'timestep', 'frequency', 'norrec'):\n        if key in agg_dict:\n            del(agg_dict[key])\n    # print(agg_dict)\n\n    # average over duplicate measurements\n    extra_dimensions_raw = ['id', 'norrec', 'frequency', 'timestep']\n    extra_dimensions = [x for x in extra_dimensions_raw if x in df.columns]\n    df = df.groupby(extra_dimensions).agg(agg_dict)\n    df.reset_index(inplace=True)\n    return df[cols]", "response": "average duplicate measurements over a single node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the differences between the norrec and the normal - reciprocal differences", "response": "def compute_norrec_differences(df, keys_diff):\n    \"\"\"DO NOT USE ANY MORE - DEPRECIATED!\n\n    \"\"\"\n    raise Exception('This function is depreciated!')\n    print('computing normal-reciprocal differences')\n    # df.sort_index(level='norrec')\n\n    def norrec_diff(x):\n        \"\"\"compute norrec_diff\"\"\"\n        if x.shape[0] != 2:\n            return np.nan\n        else:\n            return np.abs(x.iloc[1] - x.iloc[0])\n\n    keys_keep = list(set(df.columns.tolist()) - set(keys_diff))\n    agg_dict = {x: _first for x in keys_keep}\n    agg_dict.update({x: norrec_diff for x in keys_diff})\n    for key in ('id', 'timestep', 'frequency'):\n        if key in agg_dict:\n            del(agg_dict[key])\n\n    # for frequencies, we could (I think) somehow prevent grouping by\n    # frequencies...\n    df = df.groupby(('timestep', 'frequency', 'id')).agg(agg_dict)\n    # df.rename(columns={'r': 'Rdiff'}, inplace=True)\n    df.reset_index()\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a normalized version of abmn", "response": "def _normalize_abmn(abmn):\n    \"\"\"return a normalized version of abmn\n    \"\"\"\n    abmn_2d = np.atleast_2d(abmn)\n    abmn_normalized = np.hstack((\n        np.sort(abmn_2d[:, 0:2], axis=1),\n        np.sort(abmn_2d[:, 2:4], axis=1),\n    ))\n    return abmn_normalized"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef assign_norrec_to_df(df):\n    if df.shape[0] == 0:\n        # empty dataframe, just return a copy\n        return df.copy()\n\n    c = df[['a', 'b', 'm', 'n']].values.copy()\n    # unique injections\n    cu = np.unique(c, axis=0)\n\n    print('generating ids')\n    # now assign unique IDs to each config in normal and reciprocal\n    running_index = 0\n    normal_ids = {}\n    reciprocal_ids = {}\n    # loop through all configurations\n    for i in range(0, cu.shape[0]):\n        # print('testing', cu[i], i, cu.shape[0])\n        # normalize configuration\n        cu_norm = _normalize_abmn(cu[i, :]).squeeze()\n        if tuple(cu_norm) in normal_ids:\n            # print('already indexed')\n            continue\n\n        # find pairs\n        indices = np.where((\n            # current electrodes\n            (\n                (\n                    (cu[:, 0] == cu[i, 2]) & (cu[:, 1] == cu[i, 3])\n                ) |\n                (\n                    (cu[:, 0] == cu[i, 3]) & (cu[:, 1] == cu[i, 2])\n                )\n            ) &\n            # voltage electrodes\n            (\n                (\n                    (cu[:, 2] == cu[i, 0]) & (cu[:, 3] == cu[i, 1])\n                ) |\n                (\n                    (cu[:, 2] == cu[i, 1]) & (cu[:, 3] == cu[i, 0])\n                )\n            )\n        ))[0]\n\n        # we found no pair\n        if len(indices) == 0:\n            # print('no reciprocals, continuing')\n            if not tuple(cu_norm) in normal_ids:\n                if np.min(cu_norm[0:2]) < np.min(cu_norm[2:3]):\n                    # treat as normal\n                    normal_ids[tuple(cu_norm)] = running_index\n                else:\n                    reciprocal_ids[tuple(cu_norm)] = running_index\n                running_index += 1\n            continue\n\n        # if len(indices) > 1:\n        #     print('found more than one reciprocals')\n\n        # normalize the first reciprocal\n        cu_rec_norm = _normalize_abmn(cu[indices[0], :]).squeeze()\n\n        # decide on normal or reciprocal\n        # print('ABREC', cu_norm[0:2], cu_rec_norm[0:2])\n        if np.min(cu_norm[0:2]) < np.min(cu_rec_norm[0:2]):\n            # print('is normal')\n            # normal\n            normal_ids[tuple(cu_norm)] = running_index\n            reciprocal_ids[tuple(cu_rec_norm)] = running_index\n        else:\n            normal_ids[tuple(cu_rec_norm)] = running_index\n            reciprocal_ids[tuple(cu_norm)] = running_index\n        running_index += 1\n\n    print('assigning ids')\n    # print(df.shape)\n    # print(df.columns)\n    # print('normal_ids', normal_ids)\n    # print('reciprocal_ids', reciprocal_ids)\n    # now convert the indices into a dataframe so we can use pd.merge\n    # note that this code was previously written in another way, so the\n    # conversion is quite cumbersome\n    # at one point we need to rewrite everything here...\n    df_nor = {item: key for key, item in normal_ids.items()}\n    df_nor = pd.DataFrame(df_nor).T.reset_index().rename(\n        {'index': 'id'}, axis=1)\n    df_nor['norrec'] = 'nor'\n\n    if len(normal_ids) > 0:\n        df_nor.columns = ('id', 'a', 'b', 'm', 'n', 'norrec')\n        df_nor2 = df_nor.copy()\n        df_nor2.columns = ('id', 'b', 'a', 'm', 'n', 'norrec')\n        df_nor3 = df_nor.copy()\n        df_nor3.columns = ('id', 'b', 'a', 'n', 'm', 'norrec')\n        df_nor4 = df_nor.copy()\n        df_nor4.columns = ('id', 'a', 'b', 'n', 'm', 'norrec')\n        df_ids = pd.concat(\n            (\n                df_nor,\n                df_nor2,\n                df_nor3,\n                df_nor4,\n            ),\n            sort=True\n        )\n    else:\n        df_ids = pd.DataFrame()\n\n    if len(reciprocal_ids) > 0:\n        df_rec = {item: key for key, item in reciprocal_ids.items()}\n        df_rec = pd.DataFrame(df_rec).T.reset_index().rename(\n            {'index': 'id'}, axis=1)\n        df_rec['norrec'] = 'rec'\n        df_rec.columns = ('id', 'a', 'b', 'm', 'n', 'norrec')\n        df_rec2 = df_rec.copy()\n        df_rec2.columns = ('id', 'b', 'a', 'm', 'n', 'norrec')\n        df_rec3 = df_rec.copy()\n        df_rec3.columns = ('id', 'b', 'a', 'n', 'm', 'norrec')\n        df_rec4 = df_rec.copy()\n        df_rec4.columns = ('id', 'a', 'b', 'n', 'm', 'norrec')\n\n        df_ids = pd.concat(\n            (\n                df_ids,\n                df_rec,\n                df_rec2,\n                df_rec3,\n                df_rec4,\n            ),\n            sort=True\n        )\n\n    df_new = pd.merge(df, df_ids, how='left', on=('a', 'b', 'm', 'n'))\n    df_new.rename(\n        {'id_y': 'id',\n         'norrec_y': 'norrec'\n         }, axis=1,\n        inplace=True\n    )\n    return df_new\n    import IPython\n    IPython.embed()\n    exit()\n\n    df_new[['a', 'b', 'm', 'n', 'id_y', 'norrec_y']]\n    # x.iloc[[0, 1978], :]\n\n    # now assign to all measurements\n    for key, item in normal_ids.items():\n        df.loc[\n            ((df.a == key[0]) & (df.b == key[1]) &\n             (df.m == key[2]) & (df.n == key[3])) |\n            ((df.a == key[1]) & (df.b == key[0]) &\n             (df.m == key[2]) & (df.n == key[3])) |\n            ((df.a == key[0]) & (df.b == key[1]) &\n             (df.m == key[3]) & (df.n == key[2])) |\n            ((df.a == key[1]) & (df.b == key[0]) &\n             (df.m == key[3]) & (df.n == key[2])),\n            ('id', 'norrec')\n        ] = (item, 'nor')\n    for key, item in reciprocal_ids.items():\n        df.loc[\n            ((df.a == key[0]) & (df.b == key[1]) &\n             (df.m == key[2]) & (df.n == key[3])) |\n            ((df.a == key[1]) & (df.b == key[0]) &\n             (df.m == key[2]) & (df.n == key[3])) |\n            ((df.a == key[0]) & (df.b == key[1]) &\n             (df.m == key[3]) & (df.n == key[2])) |\n            ((df.a == key[1]) & (df.b == key[0]) &\n             (df.m == key[3]) & (df.n == key[2])),\n            ('id', 'norrec')\n        ] = [item, 'rec']\n\n    # cast norrec-column to string\n    df['norrec'] = df['norrec'].astype(str)\n\n    return df", "response": "Assign the norrec to the current or current electrodes for a given dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute and write the difference between normal and reciprocal values for all columns specified in the diff_list parameter.", "response": "def assign_norrec_diffs(df, diff_list):\n    \"\"\"Compute and write the difference between normal and reciprocal values\n    for all columns specified in the diff_list parameter.\n\n    Note that the DataFrame is directly written to. That is, it is changed\n    during the call of this function. No need to use the returned object.\n\n    Parameters\n    ----------\n    df: pandas.DataFrame\n        Dataframe containing the data\n    diff_list: list\n        list of columns to compute differences for.\n\n    Returns\n    -------\n    df_new: pandas.DataFrame\n        The data with added columns\n    \"\"\"\n    extra_dims = [\n        x for x in ('timestep', 'frequency', 'id') if x in df.columns\n    ]\n    g = df.groupby(extra_dims)\n\n    def subrow(row):\n        if row.size == 2:\n            return row.iloc[1] - row.iloc[0]\n        else:\n            return np.nan\n\n    for diffcol in diff_list:\n        diff = g[diffcol].agg(subrow).reset_index()\n        # rename the column\n        cols = list(diff.columns)\n        cols[-1] = diffcol + 'diff'\n        diff.columns = cols\n\n        df = df.drop(\n            cols[-1], axis=1, errors='ignore'\n        ).merge(diff, on=extra_dims, how='outer')\n\n    df = df.sort_values(extra_dims)\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the response from the ULogin API to the user API.", "response": "def handle_authenticated_user(self, response):\n        \"\"\"\n        Handles the ULogin response if user is already\n        authenticated\n        \"\"\"\n        current_user = get_user(self.request)\n\n        ulogin, registered = ULoginUser.objects.get_or_create(\n            uid=response['uid'],\n            network=response['network'],\n            defaults={'identity': response['identity'],\n                      'user': current_user})\n\n        if not registered:\n            ulogin_user = ulogin.user\n            logger.debug('uLogin user already exists')\n\n            if current_user != ulogin_user:\n                logger.debug(\n                    \"Mismatch: %s is not a %s. Take over it!\" % (current_user,\n                                                                 ulogin_user)\n                )\n                ulogin.user = current_user\n                ulogin.save()\n\n        return get_user(self.request), ulogin, registered"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_anonymous_user(self, response):\n        try:\n            ulogin = ULoginUser.objects.get(network=response['network'],\n                                            uid=response['uid'])\n        except ULoginUser.DoesNotExist:\n            user = create_user(request=self.request,\n                               ulogin_response=response)\n            ulogin = ULoginUser.objects.create(user=user,\n                                               network=response['network'],\n                                               identity=response['identity'],\n                                               uid=response['uid'])\n            registered = True\n        else:\n            user = ulogin.user\n            registered = False\n\n        # Authenticate user\n        if not hasattr(user, 'backend'):\n            user.backend = settings.AUTHENTICATION_BACKEND\n        login(self.request, user)\n\n        return user, ulogin, registered", "response": "Handles the anonymous response from the ULogin server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef form_valid(self, form):\n        response = self.ulogin_response(form.cleaned_data['token'],\n                                        self.request.get_host())\n\n        if 'error' in response:\n            return render(self.request, self.error_template_name,\n                          {'json': response})\n\n        if user_is_authenticated(get_user(self.request)):\n            user, identity, registered = \\\n                self.handle_authenticated_user(response)\n        else:\n            user, identity, registered = \\\n                self.handle_anonymous_user(response)\n\n        assign.send(sender=ULoginUser,\n                    user=get_user(self.request),\n                    request=self.request,\n                    registered=registered,\n                    ulogin_user=identity,\n                    ulogin_data=response)\n        return redirect(self.request.GET.get(REDIRECT_FIELD_NAME) or '/')", "response": "Check if the request from ulogin service is correct"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ulogin_response(self, token, host):\n        response = requests.get(\n            settings.TOKEN_URL,\n            params={\n                'token': token,\n                'host': host\n            })\n        content = response.content\n\n        if sys.version_info >= (3, 0):\n            content = content.decode('utf8')\n\n        return json.loads(content)", "response": "Makes a request to ULOGIN\n            and returns the ULOGIN\n            object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a string containing a set of caps into a pandas. DataFrame.", "response": "def caps_str_to_df(caps_str):\n    '''\n    Parse caps string (as returned by `Gst.Pad.query_caps().to_string()`) into\n    `pandas.DataFrame` table, with one row per configuration.\n    '''\n    structures = [dict([process_dict(v.groupdict())\n                   for v in re.finditer(\n                       r'(?P<key>[^ ]*)=\\((?P<type>.*?)\\)((?P<value>[^{].*?)|{(?P<values>.*?)})(,|$)', s)])\n                  for s in caps_str.split(';')]\n\n    df = pd.DataFrame(structures)\n    df.reset_index(drop=True, inplace=True)\n\n    def compute_multi(df):\n        multi_values = [[(c, k) for k, v in df[c].iteritems()\n                         if isinstance(v, list)] for c in df]\n        value_lists = [m for m in multi_values if m]\n        if value_lists:\n            return pd.DataFrame(np.concatenate(value_lists),\n                             columns=['label', 'index'])\n        else:\n            return pd.DataFrame()\n\n    df_multi = compute_multi(df)\n\n    while df_multi.shape[0] > 0:\n        df = resolve_multi(df, df_multi)\n        df_multi = compute_multi(df)\n\n    if 'framerate' in df:\n        df['framerate_numerator'] = df['framerate'].map(lambda v: v.num)\n        df['framerate_denominator'] = df['framerate'].map(lambda v: v.denom)\n        df['framerate'] = df['framerate_numerator'] / df['framerate_denominator']\n    if 'pixel-aspect-ratio' in df:\n        df['pixel-aspect-ratio_numerator'] = df['pixel-aspect-ratio'].map(lambda v: v.num)\n        df['pixel-aspect-ratio_denominator'] = df['pixel-aspect-ratio'].map(lambda v: v.denom)\n        df['pixel-aspect-ratio'] = (df['pixel-aspect-ratio_numerator'] /\n                                    df['pixel-aspect-ratio_numerator'])\n\n    return df.sort(['framerate', 'width', 'height']).reset_index(drop=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a pandas. DataFrame where each row corresponds to an available device configuration including the device.", "response": "def get_device_configs():\n    '''\n    Return a `pandas.DataFrame`, where each row corresponds to an available\n    device configuration, including the `device` (i.e., the name of the\n    device).\n    '''\n    frames = []\n\n    for i in range(2):\n        for device in get_video_sources():\n            df_device_i = get_configs(device)\n            df_device_i.insert(0, 'device', str(device))\n            frames.append(df_device_i)\n\n    device_configs = pd.concat(frames).drop_duplicates()\n    device_configs['label'] = device_configs.device.map(\n        lambda x: x.split('/')[-1].split('-')[1].split('_')[0])\n    device_configs['bitrate'] = device_configs.height.map(get_bitrate)\n    return device_configs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialise_parsimonious_states(tree, feature, states):\n    ps_feature_down = get_personalized_feature_name(feature, BU_PARS_STATES)\n    ps_feature = get_personalized_feature_name(feature, PARS_STATES)\n    all_states = set(states)\n\n    for node in tree.traverse():\n        state = getattr(node, feature, set())\n        if not state:\n            node.add_feature(ps_feature_down, all_states)\n        else:\n            node.add_feature(ps_feature_down, state)\n        node.add_feature(ps_feature, getattr(node, ps_feature_down))", "response": "Initialises the bottom - up state arrays for tips based on their states given by the feature."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_most_common_states(state_iterable):\n    state_counter = Counter()\n    for states in state_iterable:\n        state_counter.update(states)\n    max_count = state_counter.most_common(1)[0][1]\n    return {state for (state, count) in state_counter.items() if count == max_count}", "response": "Gets the set of most common states among the state sets contained in the iterable argument\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the parsimonious states on the tree and stores them in the corresponding feature.", "response": "def parsimonious_acr(tree, character, prediction_method, states, num_nodes, num_tips):\n    \"\"\"\n    Calculates parsimonious states on the tree and stores them in the corresponding feature.\n\n    :param states: numpy array of possible states\n    :param prediction_method: str, ACCTRAN (accelerated transformation), DELTRAN (delayed transformation) or DOWNPASS\n    :param tree: ete3.Tree, the tree of interest\n    :param character: str, character for which the parsimonious states are reconstructed\n    :return: dict, mapping between reconstruction parameters and values\n    \"\"\"\n    initialise_parsimonious_states(tree, character, states)\n    uppass(tree, character)\n\n    results = []\n    result = {STATES: states, NUM_NODES: num_nodes, NUM_TIPS: num_tips}\n\n    logger = logging.getLogger('pastml')\n\n    def process_result(method, feature):\n        out_feature = get_personalized_feature_name(character, method) if prediction_method != method else character\n        res = result.copy()\n        res[NUM_SCENARIOS], res[NUM_UNRESOLVED_NODES], res[NUM_STATES_PER_NODE] \\\n            = choose_parsimonious_states(tree, feature, out_feature)\n        res[NUM_STATES_PER_NODE] /= num_nodes\n        res[PERC_UNRESOLVED] = res[NUM_UNRESOLVED_NODES] * 100 / num_nodes\n        logger.debug('{} node{} unresolved ({:.2f}%) for {} by {}, '\n                     'i.e. {:.4f} state{} per node in average.'\n                     .format(res[NUM_UNRESOLVED_NODES], 's are' if res[NUM_UNRESOLVED_NODES] != 1 else ' is',\n                             res[PERC_UNRESOLVED], character, method,\n                             res[NUM_STATES_PER_NODE], 's' if res[NUM_STATES_PER_NODE] > 1 else ''))\n        res[CHARACTER] = out_feature\n        res[METHOD] = method\n        results.append(res)\n\n    if prediction_method in {ACCTRAN, MP}:\n        feature = get_personalized_feature_name(character, PARS_STATES)\n        if prediction_method == MP:\n            feature = get_personalized_feature_name(feature, ACCTRAN)\n        acctran(tree, character, feature)\n        result[STEPS] = get_num_parsimonious_steps(tree, feature)\n        process_result(ACCTRAN, feature)\n\n        bu_feature = get_personalized_feature_name(character, BU_PARS_STATES)\n        for node in tree.traverse():\n            if prediction_method == ACCTRAN:\n                node.del_feature(bu_feature)\n            node.del_feature(feature)\n\n    if prediction_method != ACCTRAN:\n        downpass(tree, character, states)\n        feature = get_personalized_feature_name(character, PARS_STATES)\n        if prediction_method == DOWNPASS:\n            result[STEPS] = get_num_parsimonious_steps(tree, feature)\n        if prediction_method in {DOWNPASS, MP}:\n            process_result(DOWNPASS, feature)\n        if prediction_method in {DELTRAN, MP}:\n            deltran(tree, character)\n            if prediction_method == DELTRAN:\n                result[STEPS] = get_num_parsimonious_steps(tree, feature)\n            process_result(DELTRAN, feature)\n        for node in tree.traverse():\n            node.del_feature(feature)\n\n    logger.debug(\"Parsimonious reconstruction for {} requires {} state changes.\"\n                 .format(character, result[STEPS]))\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the content of the get_personalized_feature_name(feature, PARS_STATES) node feature to the predicted states and stores them in the `feature` feature to each node. The get_personalized_feature_name(feature, PARS_STATES) is deleted. :param feature: str, character for which the parsimonious states are reconstructed :param tree: ete3.Tree, the tree of interest :return: int, number of ancestral scenarios selected, calculated by multiplying the number of selected states for all nodes. Also adds parsimonious states as the `feature` feature to each node", "response": "def choose_parsimonious_states(tree, ps_feature, out_feature):\n    \"\"\"\n    Converts the content of the get_personalized_feature_name(feature, PARS_STATES) node feature to the predicted states\n    and stores them in the `feature` feature to each node.\n    The get_personalized_feature_name(feature, PARS_STATES) is deleted.\n\n    :param feature: str, character for which the parsimonious states are reconstructed\n    :param tree: ete3.Tree, the tree of interest\n    :return: int, number of ancestral scenarios selected,\n    calculated by multiplying the number of selected states for all nodes.\n    Also adds parsimonious states as the `feature` feature to each node\n    \"\"\"\n    num_scenarios = 1\n    unresolved_nodes = 0\n    num_states = 0\n    for node in tree.traverse():\n        states = getattr(node, ps_feature)\n        node.add_feature(out_feature, states)\n        n = len(states)\n        num_scenarios *= n\n        unresolved_nodes += 1 if n > 1 else 0\n        num_states += n\n    return num_scenarios, unresolved_nodes, num_states"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef balance_to_ringchart_items(balance, account='', show=SHOW_CREDIT):\n    show = show if show else SHOW_CREDIT  # cannot show all in ring chart\n    rcis = []\n    for item in balance:\n        subaccount = item['account_fragment'] if not account \\\n            else ':'.join((account, item['account_fragment']))\n        ch = balance_to_ringchart_items(item['children'], subaccount, show)\n        amount = item['balance'] if show == SHOW_CREDIT else -item['balance']\n        if amount < 0:\n            continue  # omit negative amounts\n        wedge_amount = max(amount, sum(map(float, ch)))\n        rci = gtkchartlib.ringchart.RingChartItem(\n            wedge_amount,\n            tooltip='{}\\n{}'.format(subaccount, wedge_amount),\n            items=ch\n        )\n        rcis.append(rci)\n    return rcis", "response": "Convert a balance data structure into a list of RingChartItem objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding file_handler to logger", "response": "def log_to_file(log_path, log_urllib=False, limit=None):\n    \"\"\" Add file_handler to logger\"\"\"\n    log_path = log_path\n    file_handler = logging.FileHandler(log_path)\n    if limit:\n        file_handler = RotatingFileHandler(\n            log_path,\n            mode='a',\n            maxBytes=limit * 1024 * 1024,\n            backupCount=2,\n            encoding=None,\n            delay=0)\n    fmt = '[%(asctime)s %(filename)18s] %(levelname)-7s - %(message)7s'\n    date_fmt = '%Y-%m-%d %H:%M:%S'\n    formatter = logging.Formatter(fmt, datefmt=date_fmt)\n    file_handler.setFormatter(formatter)\n    logger.addHandler(file_handler)\n    if log_urllib:\n        urllib_logger.addHandler(file_handler)\n        urllib_logger.setLevel(logging.DEBUG)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef session_context(fn):\n    @functools.wraps(fn)\n    def wrap(*args, **kwargs):\n        session = args[0].Session()  # obtain from self\n        result = fn(*args, session=session, **kwargs)\n        session.close()\n        return result\n    return wrap", "response": "A decorator that returns a function that opens a session and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all the permissions for the given identifier.", "response": "def _get_permissions_query(self, session, identifier):\n        \"\"\"\n        select domain, json_agg(parts) as permissions from\n            (select domain, row_to_json(r) as parts from\n                    (select domain, action, array_agg(distinct target) as target from\n                        (select (case when domain is null then '*' else domain end) as domain,\n                                (case when target is null then '*' else target end) as target,\n                                array_agg(distinct (case when action is null then '*' else action end)) as action\n                           from permission\n                          group by domain, target\n                         ) x\n                      group by domain, action)\n              r) parts\n        group by domain;\n        \"\"\"\n        thedomain = case([(Domain.name == None, '*')], else_=Domain.name)\n        theaction = case([(Action.name == None, '*')], else_=Action.name)\n        theresource = case([(Resource.name == None, '*')], else_=Resource.name)\n\n        action_agg = func.array_agg(theaction.distinct())\n\n        stmt1 = (\n            session.query(Permission.domain_id,\n                          thedomain.label('domain'),\n                          Permission.resource_id,\n                          theresource.label('resource'),\n                          action_agg.label('action')).\n            select_from(User).\n            join(role_membership_table, User.pk_id == role_membership_table.c.user_id).\n            join(role_permission_table, role_membership_table.c.role_id == role_permission_table.c.role_id).\n            join(Permission, role_permission_table.c.permission_id == Permission.pk_id).\n            outerjoin(Domain, Permission.domain_id == Domain.pk_id).\n            outerjoin(Action, Permission.action_id == Action.pk_id).\n            outerjoin(Resource, Permission.resource_id == Resource.pk_id).\n            filter(User.identifier == identifier).\n            group_by(Permission.domain_id, Domain.name, Permission.resource_id, Resource.name)).subquery()\n\n        stmt2 = (session.query(stmt1.c.domain,\n                               stmt1.c.action,\n                               func.array_agg(stmt1.c.resource.distinct()).label('resource')).\n                 select_from(stmt1).\n                 group_by(stmt1.c.domain, stmt1.c.action)).subquery()\n\n        stmt3 = (session.query(stmt2.c.domain,\n                               func.row_to_json(as_row(stmt2)).label('parts')).\n                 select_from(stmt2)).subquery()\n\n        final = (session.query(stmt3.c.domain, cast(func.json_agg(stmt3.c.parts), Text)).\n                 select_from(stmt3).\n                 group_by(stmt3.c.domain))\n\n        return final"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_roles_query(self, session, identifier):\n        return (session.query(Role).\n                join(role_membership_table, Role.pk_id == role_membership_table.c.role_id).\n                join(User, role_membership_table.c.user_id == User.pk_id).\n                filter(User.identifier == identifier))", "response": "Get the query for the roles table."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_authc_info(self, identifier, session=None):\n        user = self._get_user_query(session, identifier).first()\n\n        creds = self._get_credential_query(session, identifier).all()\n        if not creds:\n            return None\n        authc_info = {cred_type: {'credential': cred_value, 'failed_attempts': []}\n                      for cred_type, cred_value in creds}\n\n        if 'totp_key' in authc_info:\n            authc_info['totp_key']['2fa_info'] = {'phone_number': user.phone_number}\n\n        return dict(account_locked=user.account_lock_millis, authc_info=authc_info)", "response": "Get the authentication information for a single account."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _syscal_write_electrode_coords(fid, spacing, N):\n    fid.write('# X Y Z\\n')\n    for i in range(0, N):\n        fid.write('{0} {1} {2} {3}\\n'.format(i + 1, i * spacing, 0, 0))", "response": "helper function that writes out electrode positions to a file descriptor fid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting configurations to a Syscal ascii file that can be read by the Electre Pro program.", "response": "def syscal_save_to_config_txt(filename, configs, spacing=1):\n    \"\"\"Write configurations to a Syscal ascii file that can be read by the\n    Electre Pro program.\n\n    Parameters\n    ----------\n    filename: string\n        output filename\n    configs: numpy.ndarray\n        Nx4 array with measurement configurations A-B-M-N\n\n    \"\"\"\n    print('Number of measurements: ', configs.shape[0])\n    number_of_electrodes = configs.max().astype(int)\n\n    with open(filename, 'w') as fid:\n        _syscal_write_electrode_coords(fid, spacing, number_of_electrodes)\n        _syscal_write_quadpoles(fid, configs.astype(int))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup(use_latex=False, overwrite=False):\n    # just make sure we can access matplotlib as mpl\n    import matplotlib as mpl\n\n    # general settings\n    if overwrite:\n        mpl.rcParams[\"lines.linewidth\"] = 2.0\n        mpl.rcParams[\"lines.markeredgewidth\"] = 3.0\n        mpl.rcParams[\"lines.markersize\"] = 3.0\n        mpl.rcParams[\"font.size\"] = 12\n        mpl.rcParams['mathtext.default'] = 'regular'\n    if latex and use_latex:\n        mpl.rcParams['text.usetex'] = True\n\n        mpl.rc(\n            'text.latex',\n            preamble=''.join((\n                #         r'\\usepackage{droidsans}\n                r'\\usepackage[T1]{fontenc} ',\n                r'\\usepackage{sfmath} \\renewcommand{\\rmfamily}{\\sffamily}',\n                r'\\renewcommand\\familydefault{\\sfdefault} ',\n                r'\\usepackage{mathastext} '\n            ))\n        )\n    else:\n        mpl.rcParams['text.usetex'] = False\n\n    import matplotlib.pyplot as plt\n    return plt, mpl", "response": "Set up matplotlib imports and settings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_mod_file(filename):\n    df_raw = pd.read_csv(\n        filename, skiprows=1, delim_whitespace=True,\n        names=['ab', 'mn', 'r', 'rpha']\n    )\n    df_raw['Zt'] = df_raw['r'] * np.exp(1j * df_raw['rpha'] / 1000.0)\n    df_raw['a'] = np.floor(df_raw['ab'] / 1e4).astype(int)\n    df_raw['b'] = (df_raw['ab'] % 1e4).astype(int)\n    df_raw['m'] = np.floor(df_raw['mn'] / 1e4).astype(int)\n    df_raw['n'] = (df_raw['mn'] % 1e4).astype(int)\n\n    df = df_raw.drop(['ab', 'mn'], axis=1)\n    return df", "response": "Load a. mod file and return a DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_seit_data(directory, frequency_file='frequencies.dat',\n                   data_prefix='volt_', **kwargs):\n    \"\"\"Load sEIT data from data directory. This function loads data previously\n    exported from reda using reda.exporters.crtomo.write_files_to_directory\n\n    Parameters\n    ----------\n    directory : string\n        input directory\n    frequency_file : string, optional\n        file (located in directory) that contains the frequencies\n    data_prefix: string, optional\n        for each frequency a corresponding data file must be present in the\n        input directory. Frequencies and files are matched by sorting the\n        frequencies AND the filenames, retrieved using glob and the\n        data_prefix\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        A DataFrame suitable for the sEIT container\n    electrodes : None\n        No electrode data is imported\n    topography : None\n        No topography data is imported\n\n    \"\"\"\n    frequencies = np.loadtxt(directory + os.sep + frequency_file)\n    data_files = sorted(glob(directory + os.sep + data_prefix + '*'))\n    # check that the number of frequencies matches the number of data files\n    if frequencies.size != len(data_files):\n        raise Exception(\n            'number of frequencies does not match number of data files')\n\n    # load data\n    data_list = []\n    for frequency, filename in zip(frequencies, data_files):\n        subdata = load_mod_file(filename)\n        subdata['frequency'] = frequency\n        data_list.append(subdata)\n    df = pd.concat(data_list)\n    return df, None, None", "response": "Loads data from a directory containing the sEIT electrodes and topography data files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the diagonalisation matrix for a single character state.", "response": "def get_diagonalisation(frequencies, rate_matrix=None):\n    \"\"\"\n    Normalises and diagonalises the rate matrix.\n\n    :param frequencies: character state frequencies.\n    :type frequencies: numpy.array\n    :param rate_matrix: (optional) rate matrix (by default an all-equal-rate matrix is used)\n    :type rate_matrix: numpy.ndarray\n    :return: matrix diagonalisation (d, A, A^{-1})\n        such that A.dot(np.diag(d))).dot(A^{-1}) = 1/mu Q (normalised generator)\n    :rtype: tuple\n    \"\"\"\n    Q = get_normalised_generator(frequencies, rate_matrix)\n    d, A = np.linalg.eig(Q)\n    return d, A, np.linalg.inv(A)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_normalised_generator(frequencies, rate_matrix=None):\n    if rate_matrix is None:\n        n = len(frequencies)\n        rate_matrix = np.ones(shape=(n, n), dtype=np.float64) - np.eye(n)\n    generator = rate_matrix * frequencies\n    generator -= np.diag(generator.sum(axis=1))\n    mu = -generator.diagonal().dot(frequencies)\n    generator /= mu\n    return generator", "response": "Calculates the normalised generator from the rate matrix and character state frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the probability matrix of substitutions i - > j over time t given the normalised generator diagonalisation.", "response": "def get_pij_matrix(t, diag, A, A_inv):\n    \"\"\"\n    Calculates the probability matrix of substitutions i->j over time t,\n    given the normalised generator diagonalisation.\n\n\n    :param t: time\n    :type t: float\n    :return: probability matrix\n    :rtype: numpy.ndarray\n    \"\"\"\n    return A.dot(np.diag(np.exp(diag * t))).dot(A_inv)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting specified arguments into two lists.", "response": "def split_arguments(args):\n    \"\"\"\n    Split specified arguments to two list.\n\n    This is used to distinguish the options of the program and\n    execution command/arguments.\n\n    Parameters\n    ----------\n    args : list\n        Command line arguments\n\n    Returns\n    -------\n    list : options, arguments\n        options indicate the optional arguments for the program and\n        arguments indicate the execution command/arguments\n    \"\"\"\n    prev = False\n    for i, value in enumerate(args[1:]):\n        if value.startswith('-'):\n            prev = True\n        elif prev:\n            prev = False\n        else:\n            return args[:i+1], args[i+1:]\n    return args, []"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_arguments(args, config):\n    import notify\n    from conf import config_to_options\n    opts = config_to_options(config)\n\n    usage = (\"%(prog)s \"\n             \"[-h] [-t TO_ADDR] [-f FROM_ADDR] [-e ENCODING] [-s SUBJECT]\\n\"\n             \"              \"\n             \"[-o HOST] [-p PORT] [--username USERNAME] [--password PASSWORD]\\n\"\n             \"              \"\n             \"[--setup] [--check] COMMAND ARGUMENTS\") % {'prog': \"notify\"}\n    description = \"\"\"\n    Call COMMAND with ARGUMENTS and send notification email to TO_ADDR\n    \"\"\"\n    parser = optparse.OptionParser(\n            usage=usage,\n            description=description,\n            version=notify.__version__)\n    parser.add_option('-t', '--to-addr',\n                      default=opts.to_addr,\n                      help=('Destination of the email.'))\n    parser.add_option('-f', '--from-addr',\n                      default=opts.from_addr,\n                      help=('Source of the email.'))\n    parser.add_option('-s', '--subject',\n                      default=opts.subject,\n                      help=('Subject of the email'))\n    parser.add_option('-e', '--encoding',\n                      default=opts.encoding,\n                      help=('Encoding of the email'))\n    parser.add_option('-o', '--host',\n                      default=opts.host,\n                      help=('Host address of MUA'))\n    parser.add_option('-p', '--port', type='int',\n                      default=opts.port,\n                      help=('Port number of MUA'))\n    parser.add_option('--username',\n                      default=opts.username,\n                      help=('Username for authentication'))\n    parser.add_option('--password',\n                      help=('Password for authentication'))\n    parser.add_option('--setup', default=False,\n                      action='store_true',\n                      help=('Setup %(prog)s configuration'))\n    parser.add_option('--check', default=False,\n                      action='store_true',\n                      help=('Send %(prog)s configuration via email for '\n                            'checking. Only for Unix system.'))\n\n    # display help and exit\n    if len(args) == 1:\n        parser.print_help()\n        sys.exit(0)\n    else:\n        # translate all specified arguments to unicode\n        if sys.version_info < (3,):\n            encoding = sys.stdout.encoding\n            args = map(lambda x: unicode(x, encoding), args)\n\n        # split argv to two array\n        lhs, rhs = split_arguments(args)\n\n        # parse options\n        opts = parser.parse_args(args=lhs[1:])[0]\n        return rhs, opts", "response": "Parse command line arguments via config and return a list of ArgumentParser and options"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reconstruct_uri(environ):\n    uri = environ.get('SCRIPT_NAME', '') + environ['PATH_INFO']\n    if environ.get('QUERY_STRING'):\n        uri += '?' + environ['QUERY_STRING']\n    return uri", "response": "Reconstruct the relative part of the request URI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if we should require authentication for the given URL", "response": "def should_require_authentication(self, url):\n        \"\"\" Returns True if we should require authentication for the URL given \"\"\"\n        return (not self.routes # require auth for all URLs\n                or any(route.match(url) for route in self.routes))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef authenticate(self, environ):\n        try:\n            hd = parse_dict_header(environ['HTTP_AUTHORIZATION'])\n        except (KeyError, ValueError):\n            return False\n\n        return self.credentials_valid(\n            hd['response'],\n            environ['REQUEST_METHOD'],\n            environ['httpauth.uri'],\n            hd['nonce'],\n            hd['Digest username'],\n        )", "response": "Returns True if the credentials passed in the Authorization header are valid False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the HA1 hash file and return the realm and user_HA1_map.", "response": "def parse_htdigest_file(self, filelike):\n        \"\"\"\n        .htdigest files consist of lines in the following format::\n\n            username:realm:passwordhash\n\n        where both `username` and `realm` are plain-text without any colons\n        and `passwordhash` is the result of ``md5(username : realm : password)``\n        and thus `passwordhash` == HA1.\n        \"\"\"\n        realm = None\n        user_HA1_map = {}\n\n        for lineno, line in enumerate(filter(None, filelike.read().splitlines()), 1):\n            try:\n                username, realm2, password_hash = line.split(':')\n            except ValueError:\n                raise ValueError(\"Line %d invalid: %r (username/password may not contain ':')\" % (lineno, line))\n            if realm is not None and realm != realm2:\n                raise ValueError(\"Line %d: realm may not vary (got %r and %r)\" % (lineno, realm, realm2))\n            else:\n                realm = realm2\n                user_HA1_map[username] = password_hash\n\n        return realm, user_HA1_map"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the next transaction object. StopIteration will be propagated from self. csvreader. next", "response": "def next(self):\n        \"\"\"Return the next transaction object.\n\n        StopIteration will be propagated from self.csvreader.next()\n        \"\"\"\n        try:\n            return self.dict_to_xn(self.csvreader.next())\n        except MetadataException:\n            # row was metadata; proceed to next row\n            return next(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the date and return a datetime object.", "response": "def parse_date(self, date):\n        \"\"\"Parse the date and return a datetime object\n\n        The heuristic for determining the date is:\n         - if ``date_format`` is set, parse using strptime\n         - if one field of 8 digits, YYYYMMDD\n         - split by '-' or '/'\n         - (TODO: substitute string months with their numbers)\n         - if (2, 2, 4), DD-MM-YYYY (not the peculiar US order)\n         - if (4, 2, 2), YYYY-MM-DD\n         - ka-boom!\n\n        The issue of reliably discerning between DD-MM-YYYY (sane) vs.\n        MM-DD-YYYY (absurd, but Big In America), without being told what's\n        being used,  is intractable.\n\n        Return a datetime.date object.\n\n        \"\"\"\n        if self.date_format is not None:\n            return datetime.datetime.strptime(date, self.date_format).date()\n\n        if re.match('\\d{8}$', date):\n            # assume YYYYMMDD\n            return datetime.date(*map(int, (date[:4], date[4:6], date[6:])))\n        try:\n            # split by '-' or '/'\n            parts = date_delim.split(date, 2)   # maxsplit=2\n            if len(parts) == 3:\n                if len(parts[0]) == 4:\n                    # YYYY, MM, DD\n                    return datetime.date(*map(int, parts))\n                elif len(parts[2]) == 4:\n                    # DD, MM, YYYY\n                    return datetime.date(*map(int, reversed(parts)))\n        # fail\n        except TypeError, ValueError:\n            raise reader.DataError('Bad date format: \"{}\"'.format(date))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the result line of a log - in request.", "response": "def parse_result(line):\n    \"\"\"\n    Parse the result line of a phenomizer request.\n    \n    Arguments:\n        line (str): A raw output line from phenomizer\n    \n    Returns:\n         result (dict): A dictionary with the phenomizer info:\n             {\n                'p_value': float,\n                'gene_symbols': list(str),\n                'disease_nr': int,\n                'disease_source': str,\n                'description': str,\n                'raw_line': str\n             }\n             \n    \"\"\"\n    \n    if line.startswith(\"Problem\"):\n        raise RuntimeError(\"Login credentials seems to be wrong\")\n\n    result = {\n        'p_value': None,\n        'gene_symbols': [],\n        'disease_nr': None,\n        'disease_source': None,\n        'description': None,\n        'raw_line': line\n    }\n    \n    result['raw_line'] = line.rstrip()\n    result_line = line.rstrip().split('\\t')\n    \n    try:\n        result['p_value'] = float(result_line[0])\n    except ValueError:\n        pass\n\n    try:\n        medical_litterature = result_line[2].split(':')\n        result['disease_source'] = medical_litterature[0]\n        result['disease_nr'] = int(medical_litterature[1])\n    except IndexError:\n        pass\n\n    try:\n        description = result_line[3]\n        result['description'] = description\n    except IndexError:\n        pass\n\n    if len(result_line) > 4:\n        for gene_symbol in result_line[4].split(','):\n            result['gene_symbols'].append(gene_symbol.strip())\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the phenomizer web tool for a list of hpo terms.", "response": "def query_phenomizer(usr, pwd,  *hpo_terms):\n    \"\"\"\n    Query the phenomizer web tool\n    \n    Arguments:\n        usr (str): A username for phenomizer\n        pwd (str): A password for phenomizer\n        hpo_terms (list): A list with hpo terms\n    \n    Returns:\n        raw_answer : The raw result from phenomizer\n    \"\"\"\n    base_string = 'http://compbio.charite.de/phenomizer/phenomizer/PhenomizerServiceURI'\n    questions = {'mobilequery':'true', 'terms':','.join(hpo_terms), 'username':usr, 'password':pwd}\n    try:\n        r = requests.get(base_string, params=questions, timeout=10)\n    except requests.exceptions.Timeout:\n        raise RuntimeError(\"The request timed out.\")\n        \n    if not r.status_code == requests.codes.ok:\n        raise RuntimeError(\"Phenomizer returned a bad status code: %s\" % r.status_code)\n    \n    r.encoding = 'utf-8'\n    \n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the phenomizer web tool and return a list of parsed information for each entry in the result set.", "response": "def query(usr, pwd, *hpo_terms):\n    \"\"\"\n    Query the phenomizer web tool\n    \n    Arguments:\n        usr (str): A username for phenomizer\n        pwd (str): A password for phenomizer\n        hpo_terms (list): A list with hpo terms\n    \n    yields:\n        parsed_term (dict): A dictionary with the parsed information\n                            from phenomizer\n     \n    \"\"\"\n    raw_result = query_phenomizer(usr, pwd, *hpo_terms)\n    \n    for line in raw_result.text.split('\\n'):\n        if len(line) > 1:\n            if not line.startswith('#'):\n                yield parse_result(line)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_term(usr, pwd, hpo_term):\n    \n    result = True\n    try:\n        for line in query(usr, pwd, hpo_term):\n            pass\n    except RuntimeError as err:\n        raise err\n    \n    return result", "response": "Validate if the HPO term exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a signed request to the API raise any API errors and and raise an APIError", "response": "def _request(self, url, params=None):\n        \"\"\"Make a signed request to the API, raise any API errors, and\n        returning a tuple of (data, metadata)\n        \"\"\"\n\n        response = get(url, params=params, headers=self.headers, timeout=30)\n\n        if response.status_code != 200:\n            raise APIError('Request to {} returned {}'.format(response.url, response.status_code))\n\n        response = response.json()\n\n        error_text = response['service_meta']['error_text']\n        if error_text:\n            raise APIError(error_text)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new subscription with this short name and the provided parameters.", "response": "def create(self, uri, buffer=\"queue\", interval=10):\n        \"\"\"Create a subscription with this short name and the provided parameters\n\n        For more information on what the parameters required here mean, please\n        refer to the `WVA Documentation <http://goo.gl/DRcOQf>`_.\n\n        :raises WVAError: If there is a problem creating the new subscription\n        \"\"\"\n        return self._http_client.put_json(\"subscriptions/{}\".format(self.short_name), {\n            \"subscription\": {\n                \"uri\": uri,\n                \"buffer\": buffer,\n                \"interval\": interval,\n            }\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef import_ohm(filename, verbose=False, reciprocals=False):\n    if verbose:\n        print((\"Reading in %s... \\n\" % filename))\n    file = open(filename)\n\n    eleccount = int(file.readline().split(\"#\")[0])\n    elecs_str = file.readline().split(\"#\")[1]\n    elecs_dim = len(elecs_str.split())\n    elecs_ix = elecs_str.split()\n\n    elecs = np.zeros((eleccount, elecs_dim), 'float')\n    for i in range(eleccount):\n        line = file.readline().split(\"#\")[0]  # Account for comments\n        elecs[i] = line.rsplit()\n\n    datacount = int(file.readline().split(\"#\")[0])\n    data_str = file.readline().split(\"#\")[1]\n    data_dim = len(data_str.split())\n    data_ix = data_str.split()\n\n    _string_ = \"\"\"\n    Number of electrodes: %s\n    Dimension: %s\n    Coordinates: %s\n    Number of data points: %s\n    Data header: %s\n    \"\"\" % (eleccount, elecs_dim, elecs_str, datacount, data_str)\n\n    data = np.zeros((datacount, data_dim), 'float')\n    for i in range(datacount):\n        line = file.readline()\n        data[i] = line.rsplit()\n\n    file.close()\n\n    data = pd.DataFrame(data, columns=data_ix)\n    # rename columns to the reda standard\n    data_reda = data.rename(\n        index=str,\n        columns={\n            'rhoa': 'rho_a',\n            # 'k': 'k',\n            # 'u': 'U',\n            # 'i': 'I'\n        }\n    )\n    if ('r' not in data_reda.keys()) and \\\n       ('rho_a' in data_reda.keys() and 'k' in data_reda.keys()):\n        data_reda['r'] = data_reda['rho_a'] / data_reda['k']\n        print(\n            \"Calculating resistance from apparent resistivity and \"\n            \"geometric factors. (r = rhoa_ / k)\")\n\n    elecs = pd.DataFrame(elecs, columns=elecs_ix)\n    # Ensure uppercase labels (X, Y, Z) in electrode positions\n    elecs.columns = elecs.columns.str.upper()\n\n    # rename electrode denotations\n    if type(reciprocals) == int:\n        print('renumbering electrode numbers')\n        data_reda[['a', 'b', 'm', 'n']] = reciprocals + 1 - data_reda[\n            ['a', 'b', 'm', 'n']]\n\n    if verbose:\n        print((_string_))\n\n    return data_reda, elecs, None", "response": "Read in a BERT - style unified data file and return a pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn ISO 8601 date string", "response": "def as_iso8601(self):\n        \"\"\"\n        example: 2016-08-13T00:38:05.210+00:00\n        \"\"\"\n        if self.__date is None or self.__time is None:\n            return None\n\n        return \"20%s-%s-%sT%s:%s:%s0Z\" % \\\n               (self.__date[4:], self.__date[2:4], self.__date[:2], self.__time[:2], self.__time[2:4], self.__time[4:])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __we_c(cls, calib, tc, temp, we_v):\n        we_t = we_v - (calib.we_elc_mv / 1000.0)        # remove electronic we zero\n\n        we_c = tc.correct(calib, temp, we_t)\n\n        # print(\"D4Datum__we_c: we_t:%f we_c:%s\" % (we_t, we_c), file=sys.stderr)\n\n        return we_c", "response": "Compute weC from sensor temperature compensation of weV"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the version number from the PAL configure. ac file and returns it as a string major minor and patchlevel version integers.", "response": "def read_pal_version():\n    \"\"\"\n    Scans the PAL configure.ac looking for the version number.\n\n    (vers, maj, min, patchlevel) = read_pal_version()\n\n    Returns the version as a string and the major, minor\n    and patchlevel version integers\n\n    \"\"\"\n    verfile = os.path.join(\"cextern\", \"pal\", \"configure.ac\")\n    verstring = \"-1.-1.-1\"\n    for line in open(verfile):\n        if line.startswith(\"AC_INIT\"):\n            # Version will be in string [nn.mm.pp]\n            match = re.search(r\"\\[(\\d+\\.\\d+\\.\\d+)\\]\", line)\n            if match:\n                verstring = match.group(1)\n            break\n    (major, minor, patch) = verstring.split(\".\")\n    return (verstring, major, minor, patch)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the fields value with the received information.", "response": "def _reset_model(self, response):\n        \"\"\"Update the fields value with the received information.\"\"\"\n\n        # pylint: disable=no-member\n\n        # Reset the model to the initial state\n        self._provision_done = False    # Set back the provision flag\n        self._changes.clear()           # Clear the changes\n\n        # Process the raw data from the update response\n        fields = self.process_raw_data(response)\n        # Update the current model representation\n        self._set_fields(fields)\n\n        # Lock the current model\n        self._provision_done = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the current model is ready to be used.", "response": "def is_ready(self):\n        \"\"\"Check if the current model is ready to be used.\"\"\"\n        if not self.provisioning_state:\n            raise exception.ServiceException(\"The object doesn't contain \"\n                                             \"`provisioningState`.\")\n        elif self.provisioning_state == constant.FAILED:\n            raise exception.ServiceException(\n                \"Failed to complete the required operation.\")\n        elif self.provisioning_state == constant.SUCCEEDED:\n            LOG.debug(\"The model %s: %s was successfully updated \"\n                      \"(or created).\",\n                      self.__class__.__name__, self.resource_id)\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_client():\n        return utils.get_client(url=CONFIG.HNV.url,\n                                username=CONFIG.HNV.username,\n                                password=CONFIG.HNV.password,\n                                allow_insecure=CONFIG.HNV.https_allow_insecure,\n                                ca_bundle=CONFIG.HNV.https_ca_bundle)", "response": "Create a new client for the HNV REST API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_all(cls, parent_id=None, grandparent_id=None):\n        client = cls._get_client()\n        endpoint = cls._endpoint.format(resource_id=\"\",\n                                        parent_id=parent_id or \"\",\n                                        grandparent_id=grandparent_id or \"\")\n        resources = []\n        while True:\n            response = client.get_resource(endpoint)\n            for raw_data in response.get(\"value\", []):\n                raw_data[\"parentResourceID\"] = parent_id\n                raw_data[\"grandParentResourceID\"] = grandparent_id\n                resources.append(cls.from_raw_data(raw_data))\n            endpoint = response.get(\"nextLink\")\n            if not endpoint:\n                break\n        return resources", "response": "Retrives all the required resources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get(cls, resource_id, parent_id, grandparent_id):\n        client = cls._get_client()\n        endpoint = cls._endpoint.format(resource_id=resource_id or \"\",\n                                        parent_id=parent_id or \"\",\n                                        grandparent_id=grandparent_id or \"\")\n        raw_data = client.get_resource(endpoint)\n        raw_data[\"parentResourceID\"] = parent_id\n        raw_data[\"grandParentResourceID\"] = grandparent_id\n        return cls.from_raw_data(raw_data)", "response": "Retrieves the required resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the required resources.", "response": "def get(cls, resource_id=None, parent_id=None, grandparent_id=None):\n        \"\"\"Retrieves the required resources.\n\n        :param resource_id:      The identifier for the specific resource\n                                 within the resource type.\n        :param parent_id:        The identifier for the specific ancestor\n                                 resource within the resource type.\n        :param grandparent_id:   The identifier that is associated with\n                                 network objects that are ancestors of the\n                                 parent of the necessary resource.\n        \"\"\"\n\n        if not resource_id:\n            return cls._get_all(parent_id, grandparent_id)\n        else:\n            return cls._get(resource_id, parent_id, grandparent_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting the required resource.", "response": "def remove(cls, resource_id, parent_id=None, grandparent_id=None,\n               wait=True, timeout=None):\n        \"\"\"Delete the required resource.\n\n        :param resource_id:      The identifier for the specific resource\n                                 within the resource type.\n        :param parent_id:        The identifier for the specific ancestor\n                                 resource within the resource type.\n        :param grandparent_id:   The identifier that is associated with\n                                 network objects that are ancestors of the\n                                 parent of the necessary resource.\n        :param wait:             Whether to wait until the operation is\n                                 completed\n        :param timeout:          The maximum amount of time required for this\n                                 operation to be completed.\n\n        If optional :param wait: is True and timeout is None (the default),\n        block if necessary until the resource is available. If timeout is a\n        positive number, it blocks at most timeout seconds and raises the\n        `TimeOut` exception if no item was available within that time.\n\n        Otherwise (block is false), return a resource if one is immediately\n        available, else raise the `NotFound` exception (timeout is ignored\n        in that case).\n        \"\"\"\n        client = cls._get_client()\n        endpoint = cls._endpoint.format(resource_id=resource_id or \"\",\n                                        parent_id=parent_id or \"\",\n                                        grandparent_id=grandparent_id or \"\")\n        client.remove_resource(endpoint)\n\n        elapsed_time = 0\n        while wait:\n            try:\n                resource = cls._get(resource_id=resource_id,\n                                    parent_id=parent_id,\n                                    grandparent_id=grandparent_id)\n                resource.is_ready()\n                LOG.debug(\"The resource is still available. %r\", resource)\n            except exception.NotFound:\n                LOG.debug(\"The resource was successfully removed.\")\n                break\n\n            elapsed_time += CONFIG.HNV.retry_interval\n            if timeout and elapsed_time > timeout:\n                raise exception.TimeOut(\"The request timed out.\")\n            time.sleep(CONFIG.HNV.retry_interval)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self):\n        client = self._get_client()\n        endpoint = self._endpoint.format(\n            resource_id=self.resource_id or \"\",\n            parent_id=self.parent_id or \"\",\n            grandparent_id=self.grandparent_id or \"\")\n        response = client.get_resource(endpoint)\n        self._reset_model(response)", "response": "Get the latest representation of the current model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef commit(self, if_match=None, wait=True, timeout=None):\n        if not self._changes:\n            LOG.debug(\"No changes available for %s: %s\",\n                      self.__class__.__name__, self.resource_id)\n            return\n\n        LOG.debug(\"Apply all the changes on the current %s: %s\",\n                  self.__class__.__name__, self.resource_id)\n        client = self._get_client()\n        endpoint = self._endpoint.format(\n            resource_id=self.resource_id or \"\",\n            parent_id=self.parent_id or \"\",\n            grandparent_id=self.grandparent_id or \"\")\n        request_body = self.dump(include_read_only=False)\n        response = client.update_resource(endpoint, data=request_body,\n                                          if_match=if_match)\n\n        elapsed_time = 0\n        while wait:\n            self.refresh()  # Update the representation of the current model\n            if self.is_ready():\n                break\n            elapsed_time += CONFIG.HNV.retry_interval\n            if timeout and elapsed_time > timeout:\n                raise exception.TimeOut(\"The request timed out.\")\n            time.sleep(CONFIG.HNV.retry_interval)\n        else:\n            self._reset_model(response)\n\n        # NOTE(alexcoman): In order to keep backwards compatibility the\n        # `method: commit` will return a reference to itself.\n        # An example for that can be the following use case:\n        # label = client.Model().commit()\n        return self", "response": "Apply all the changes on the current model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        raw_metadata = raw_data.get(\"resourceMetadata\", None)\n        if raw_metadata is not None:\n            metadata = ResourceMetadata.from_raw_data(raw_metadata)\n            raw_data[\"resourceMetadata\"] = metadata\n\n        raw_state = properties.get(\"configurationState\", None)\n        if raw_state is not None:\n            configuration = ConfigurationState.from_raw_data(raw_state)\n            properties[\"configurationState\"] = configuration\n\n        return super(_BaseHNVModel, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_fields(self, fields):\n        super(_BaseHNVModel, self)._set_fields(fields)\n        if not self.resource_ref:\n            endpoint = self._endpoint.format(\n                resource_id=self.resource_id, parent_id=self.parent_id,\n                grandparent_id=self.grandparent_id)\n            self.resource_ref = re.sub(\"(/networking/v[0-9]+)\", \"\", endpoint)", "response": "Set or update the fields value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the associated resource.", "response": "def get_resource(self):\n        \"\"\"Return the associated resource.\"\"\"\n        references = {\"resource_id\": None, \"parent_id\": None,\n                      \"grandparent_id\": None}\n        for model_cls, regexp in self._regexp.iteritems():\n            match = regexp.search(self.resource_ref)\n            if match is not None:\n                references.update(match.groupdict())\n                return model_cls.get(**references)\n\n        raise exception.NotFound(\"No model available for %(resource_ref)r\",\n                                 resource_ref=self.resource_ref)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_raw_data(cls, raw_data):\n        properties = raw_data[\"properties\"]\n\n        ip_pools = []\n        for raw_content in properties.get(\"ipPools\", []):\n            raw_content[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            raw_content[\"grandParentResourceID\"] = raw_data[\"parentResourceID\"]\n            ip_pools.append(IPPools.from_raw_data(raw_content))\n        properties[\"ipPools\"] = ip_pools\n\n        ip_configurations = []\n        for raw_content in properties.get(\"ipConfigurations\", []):\n            resource = Resource.from_raw_data(raw_content)\n            ip_configurations.append(resource)\n        properties[\"ipConfigurations\"] = ip_configurations\n\n        network_interfaces = []\n        for raw_content in properties.get(\"networkInterfaces\", []):\n            resource = Resource.from_raw_data(raw_content)\n            network_interfaces.append(resource)\n        properties[\"networkInterfaces\"] = network_interfaces\n\n        return super(LogicalSubnetworks, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data[\"properties\"]\n\n        subnetworks = []\n        for raw_subnet in properties.get(\"subnets\", []):\n            raw_subnet[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            subnetworks.append(LogicalSubnetworks.from_raw_data(raw_subnet))\n        properties[\"subnets\"] = subnetworks\n\n        virtual_networks = []\n        for raw_network in properties.get(\"virtualNetworks\", []):\n            virtual_networks.append(Resource.from_raw_data(raw_network))\n        properties[\"virtualNetworks\"] = virtual_networks\n\n        return super(LogicalNetworks, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data[\"properties\"]\n\n        address_pools = []\n        for content in properties.get(\"loadBalancerBackendAddressPools\", []):\n            resource = Resource.from_raw_data(content)\n            address_pools.append(resource)\n        properties[\"loadBalancerBackendAddressPools\"] = address_pools\n\n        nat_rules = []\n        for content in properties.get(\"loadBalancerInboundNatRules\", None):\n            resource = Resource.from_raw_data(content)\n            nat_rules.append(resource)\n        properties[\"loadBalancerInboundNatRules\"] = nat_rules\n\n        raw_content = properties.get(\"publicIPAddress\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"publicIPAddress\"] = resource\n\n        raw_content = properties.get(\"serviceInsertion\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"serviceInsertion\"] = resource\n\n        raw_content = properties.get(\"subnet\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"subnet\"] = resource\n\n        return super(IPConfiguration, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        raw_settings = raw_data.get(\"qosSettings\", {})\n        qos_settings = QosSettings.from_raw_data(raw_settings)\n        raw_data[\"qosSettings\"] = qos_settings\n        return super(PortSettings, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data[\"properties\"]\n\n        ip_configurations = []\n        raw_settings = properties.get(\"ipConfigurations\", [])\n        for raw_configuration in raw_settings:\n            raw_configuration[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            ip_configuration = IPConfiguration.from_raw_data(raw_configuration)\n            ip_configurations.append(ip_configuration)\n        properties[\"ipConfigurations\"] = ip_configurations\n\n        raw_settings = properties.get(\"dnsSettings\", {})\n        dns_settings = DNSSettings.from_raw_data(raw_settings)\n        properties[\"dnsSettings\"] = dns_settings\n\n        raw_settings = properties.get(\"portSettings\", {})\n        port_settings = PortSettings.from_raw_data(raw_settings)\n        properties[\"portSettings\"] = port_settings\n\n        return super(NetworkInterfaces, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data[\"properties\"]\n\n        raw_content = properties.get(\"accessControlList\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"accessControlList\"] = resource\n\n        # TODO(alexcoman): Add model for ServiceInsertion\n        raw_content = properties.get(\"serviceInsertion\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"serviceInsertion\"] = resource\n\n        raw_content = properties.get(\"routeTable\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"routeTable\"] = resource\n\n        ip_configurations = []\n        for raw_config in properties.get(\"ipConfigurations\", []):\n            ip_configurations.append(Resource.from_raw_data(raw_config))\n        properties[\"ipConfigurations\"] = ip_configurations\n\n        return super(SubNetworks, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_raw_data(cls, raw_data):\n        properties = raw_data[\"properties\"]\n\n        raw_content = properties.get(\"addressSpace\", None)\n        if raw_content is not None:\n            address_space = AddressSpace.from_raw_data(raw_content)\n            properties[\"addressSpace\"] = address_space\n\n        raw_content = properties.get(\"dhcpOptions\")\n        if raw_content is not None:\n            dhcp_options = DHCPOptions.from_raw_data(raw_content)\n            properties[\"dhcpOptions\"] = dhcp_options\n\n        raw_content = properties.get(\"logicalNetwork\", None)\n        if raw_content is not None:\n            properties[\"logicalNetwork\"] = Resource.from_raw_data(raw_content)\n\n        subnetworks = []\n        for raw_subnet in properties.get(\"subnets\", []):\n            raw_subnet[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            subnetworks.append(SubNetworks.from_raw_data(raw_subnet))\n        properties[\"subnets\"] = subnetworks\n\n        return super(VirtualNetworks, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_raw_data(cls, raw_data):\n        properties = raw_data[\"properties\"]\n\n        ip_configurations = []\n        for raw_content in properties.get(\"ipConfigurations\", []):\n            resource = Resource.from_raw_data(raw_content)\n            ip_configurations.append(resource)\n        properties[\"ipConfigurations\"] = ip_configurations\n\n        subnetworks = []\n        for raw_subnet in properties.get(\"subnets\", []):\n            subnetworks.append(Resource.from_raw_data(raw_subnet))\n        properties[\"subnets\"] = subnetworks\n\n        acl_rules = []\n        for raw_rule in properties.get(\"aclRules\", []):\n            raw_rule[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            acl_rules.append(ACLRules.from_raw_data(raw_rule))\n        properties[\"aclRules\"] = acl_rules\n\n        return super(AccessControlLists, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the required resource.", "response": "def get(cls, resource_id=None, parent_id=None, grandparent_id=None):\n        \"\"\"\"Retrieves the required resource.\"\"\"\n        return cls._get(resource_id, parent_id, grandparent_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_raw_data(cls, raw_data):\n        properties = raw_data[\"properties\"]\n        qos_settings = properties.get(\"qosSettings\", {})\n        properties[\"qosSettings\"] = VirtualSwtichQosSettings.from_raw_data(\n            raw_data=qos_settings)\n        return super(VirtualSwitchManager, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the required resource.", "response": "def remove(cls, resource_id, parent_id=None, grandparent_id=None,\n               wait=True, timeout=None):\n        \"\"\"Delete the required resource.\"\"\"\n        raise exception.NotSupported(feature=\"DELETE\",\n                                     context=\"VirtualSwitchManager\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_raw_data(cls, raw_data):\n        properties = raw_data[\"properties\"]\n\n        routes = []\n        raw_routes = properties.get(\"routes\", [])\n        for raw_route in raw_routes:\n            raw_route[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            routes.append(Routes.from_raw_data(raw_route))\n        properties[\"routes\"] = routes\n\n        subnets = []\n        raw_subnets = properties.get(\"subnets\", [])\n        for raw_subnet in raw_subnets:\n            subnets.append(Resource.from_raw_data(raw_subnet))\n        properties[\"subnets\"] = subnets\n\n        return super(RouteTables, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_raw_data(cls, raw_data):\n        raw_main = raw_data.get(\"mainMode\", None)\n        if raw_main is not None:\n            main_mode = MainMode.from_raw_data(raw_main)\n            raw_data[\"mainMode\"] = main_mode\n\n        raw_quick = raw_data.get(\"quickMode\", None)\n        if raw_quick is not None:\n            quick_mode = QuickMode.from_raw_data(raw_quick)\n            raw_data[\"quickMode\"] = quick_mode\n\n        local_vpn_ts = []\n        for raw_local_vpn in raw_data.get(\"localVpnTrafficSelector\", []):\n            local_vpn_ts.append(LocalVpnTrafficSelector.from_raw_data(\n                raw_local_vpn))\n        raw_data[\"localVpnTrafficSelector\"] = local_vpn_ts\n\n        remote_vpn_ts = []\n        for raw_remote_vpn in raw_data.get(\"remoteVpnTrafficSelector\", []):\n            remote_vpn_ts.append(RemoteVpnTrafficSelector.from_raw_data(\n                raw_remote_vpn))\n        raw_data[\"remoteVpnTrafficSelector\"] = remote_vpn_ts\n\n        return super(IPSecConfiguration, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        raw_content = properties.get(\"ipSecConfiguration\", None)\n        if raw_content is not None:\n            ip_sec = IPSecConfiguration.from_raw_data(raw_content)\n            properties[\"ipSecConfiguration\"] = ip_sec\n\n        ip_addresses = []\n        for raw_content in properties.get(\"ipAddresses\", []):\n            ip_addresses.append(IPAddress.from_raw_data(raw_content))\n        properties[\"ipAddresses\"] = ip_addresses\n\n        routes = []\n        for raw_content in properties.get(\"routes\", []):\n            routes.append(NetworkInterfaceRoute.from_raw_data(raw_content))\n        properties[\"routes\"] = routes\n\n        raw_content = properties.get(\"statistics\", None)\n        if raw_content is not None:\n            statistics = NetworkInterfaceStatistics.from_raw_data(\n                raw_content)\n            properties[\"statistics\"] = statistics\n\n        raw_content = properties.get(\"greConfiguration\", None)\n        if raw_content is not None:\n            gre_configuration = GREConfiguration.from_raw_data(raw_content)\n            properties[\"greConfiguration\"] = gre_configuration\n\n        raw_content = properties.get(\"l3Configuration\", None)\n        if raw_content is not None:\n            l3_configuration = L3Configuration.from_raw_data(raw_content)\n            properties[\"l3Configuration\"] = l3_configuration\n\n        raw_content = properties.get(\"gateway\", None)\n        if raw_content is not None:\n            gateway = Resource.from_raw_data(raw_content)\n            properties[\"gateway\"] = gateway\n\n        return super(NetworkConnections, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef process_raw_data(cls, raw_data):\n        properties = raw_data.get(\"properties\", {})\n\n        raw_content = properties.get(\"ipConfiguration\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"ipConfiguration\"] = resource\n\n        return super(PublicIPAddresses, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        backend_ip_configurations = []\n        for raw_content in properties.get(\"backendIPConfigurations\", []):\n            resource = Resource.from_raw_data(raw_content)\n            backend_ip_configurations.append(resource)\n        properties[\"backendIPConfigurations\"] = backend_ip_configurations\n\n        load_balancing_rules = []\n        for raw_content in properties.get(\"loadBalancingRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            load_balancing_rules.append(resource)\n        properties[\"loadBalancingRules\"] = load_balancing_rules\n\n        outbound_nat_rules = []\n        for raw_content in properties.get(\"outboundNatRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            outbound_nat_rules.append(resource)\n        properties[\"outboundNatRules\"] = outbound_nat_rules\n\n        return super(BackendAddressPools, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_raw_data(cls, raw_data):\n        properties = raw_data.get(\"properties\", {})\n\n        load_balancing_rules = []\n        for raw_content in properties.get(\"loadBalancingRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            load_balancing_rules.append(resource)\n        properties[\"loadBalancingRules\"] = load_balancing_rules\n\n        inbound_nat_rules = []\n        for raw_content in properties.get(\"inboundNatRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            inbound_nat_rules.append(resource)\n        properties[\"inboundNatRules\"] = inbound_nat_rules\n\n        outbound_nat_rules = []\n        for raw_content in properties.get(\"outboundNatRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            outbound_nat_rules.append(resource)\n        properties[\"outboundNatRules\"] = outbound_nat_rules\n\n        raw_content = properties.get(\"subnet\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"subnet\"] = resource\n\n        return super(FrontendIPConfigurations, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        raw_ip_configuration = properties.get(\"backendIPConfiguration\", [])\n        if isinstance(raw_ip_configuration, dict):\n            raw_ip_configuration = [raw_ip_configuration]\n\n        for raw_content in raw_ip_configuration:\n            backend_ip_configuration = Resource.from_raw_data(raw_content)\n            properties[\"backendIPConfiguration\"] = backend_ip_configuration\n\n        frontend_ip_configurations = []\n        for raw_content in properties.get(\"frontendIPConfigurations\", []):\n            resource = Resource.from_raw_data(raw_content)\n            frontend_ip_configurations.append(resource)\n        properties[\"frontendIPConfigurations\"] = frontend_ip_configurations\n\n        return super(InboundNATRules, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_raw_data(cls, raw_data):\n        properties = raw_data.get(\"properties\", {})\n\n        frontend_ip_configurations = []\n        for raw_content in properties.get(\"frontendIPConfigurations\", []):\n            resource = Resource.from_raw_data(raw_content)\n            frontend_ip_configurations.append(resource)\n        properties[\"frontendIPConfigurations\"] = frontend_ip_configurations\n\n        raw_content = properties.get(\"backendAddressPool\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"backendAddressPool\"] = resource\n\n        raw_content = properties.get(\"probe\", None)\n        if raw_content is not None:\n            resource = Resource.from_raw_data(raw_content)\n            properties[\"probe\"] = resource\n\n        return super(LoadBalancingRules, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        load_balancing_rules = []\n        for raw_content in properties.get(\"loadBalancingRules\", []):\n            resource = Resource.from_raw_data(raw_content)\n            load_balancing_rules.append(resource)\n        properties[\"loadBalancingRules\"] = load_balancing_rules\n\n        return super(Probes, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_raw_data(cls, raw_data):\n\n        # pylint: disable=redefined-variable-type\n\n        raw_content = raw_data.get(\"updateMessageStats\", None)\n        if raw_content is not None:\n            statistics = UpdateMessageStatistics.from_raw_data(raw_content)\n            raw_data[\"updateMessageStats\"] = statistics\n\n        raw_content = raw_data.get(\"routeRefreshMessageStats\", None)\n        if raw_content is not None:\n            statistics = RouteRefreshMessageStatistics.from_raw_data(\n                raw_content)\n            raw_data[\"routeRefreshMessageStats\"] = statistics\n\n        raw_content = raw_data.get(\"keepAliveMessageStats\", None)\n        if raw_content is not None:\n            statistics = KeepAliveMessageStatistics.from_raw_data(raw_content)\n            raw_data[\"keepAliveMessageStats\"] = statistics\n\n        raw_content = raw_data.get(\"notificationMessageStats\", None)\n        if raw_content is not None:\n            statistics = NotificationMessageStatistics.from_raw_data(\n                raw_content)\n            raw_data[\"notificationMessageStats\"] = statistics\n\n        raw_content = raw_data.get(\"openMessageStats\", None)\n        if raw_content is not None:\n            statistics = OpenMessageStatistics.from_raw_data(raw_content)\n            raw_data[\"openMessageStats\"] = statistics\n\n        raw_content = raw_data.get(\"ipv4Route\", None)\n        if raw_content is not None:\n            statistics = IPV4Route.from_raw_data(raw_content)\n            raw_data[\"ipv4Route\"] = statistics\n\n        raw_content = raw_data.get(\"ipv6Route\", None)\n        if raw_content is not None:\n            statistics = IPV6Route.from_raw_data(raw_content)\n            raw_data[\"ipv6Route\"] = statistics\n\n        return super(BGPPeersStatistics, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_raw_data(cls, raw_data):\n        properties = raw_data.get(\"properties\", {})\n\n        raw_content = properties.get(\"statistics\", None)\n        if raw_content is not None:\n            statistics = BGPPeersStatistics.from_raw_data(raw_content)\n            properties[\"statistics\"] = statistics\n\n        return super(BGPPeers, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_raw_data(cls, raw_data):\n        properties = raw_data.get(\"properties\", {})\n\n        bgp_peers = []\n        for raw_content in properties.get(\"bgpPeers\", []):\n            raw_content[\"parentResourceID\"] = raw_data[\"resourceId\"]\n            raw_content[\"grandParentResourceID\"] = raw_data[\"parentResourceID\"]\n            bgp_peers.append(BGPPeers.from_raw_data(raw_content))\n        properties[\"bgpPeers\"] = bgp_peers\n\n        return super(BGPRouters, cls).process_raw_data(raw_data)", "response": "Create a new model using raw API response."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new model using raw API response.", "response": "def process_raw_data(cls, raw_data):\n        \"\"\"Create a new model using raw API response.\"\"\"\n        properties = raw_data.get(\"properties\", {})\n\n        vip_ip_pools = []\n        for raw_content in properties.get(\"vipIpPools\", []):\n            resource = Resource.from_raw_data(raw_content)\n            vip_ip_pools.append(resource)\n        properties[\"vipIpPools\"] = vip_ip_pools\n\n        return super(LoadBalancerManager, cls).process_raw_data(raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_nr_bins(count):\n    if count <= 30:\n        # use the square-root choice, used by Excel and Co\n        k = np.ceil(np.sqrt(count))\n    else:\n        # use Sturges' formula\n        k = np.ceil(np.log2(count)) + 1\n    return int(k)", "response": "Compute the number of bins for a given number of data points"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_histograms(ertobj, keys, **kwargs):\n    # you can either provide a DataFrame or an ERT object\n    if isinstance(ertobj, pd.DataFrame):\n        df = ertobj\n    else:\n        df = ertobj.data\n\n    if df.shape[0] == 0:\n        raise Exception('No data present, cannot plot')\n\n    if isinstance(keys, str):\n        keys = [keys, ]\n\n    figures = {}\n    merge_figs = kwargs.get('merge', True)\n    if merge_figs:\n        nr_x = 2\n        nr_y = len(keys)\n        size_x = 15 / 2.54\n        size_y = 5 * nr_y / 2.54\n        fig, axes_all = plt.subplots(nr_y, nr_x, figsize=(size_x, size_y))\n        axes_all = np.atleast_2d(axes_all)\n\n    for row_nr, key in enumerate(keys):\n        print('Generating histogram plot for key: {0}'.format(key))\n        subdata_raw = df[key].values\n        subdata = subdata_raw[~np.isnan(subdata_raw)]\n        subdata = subdata[np.isfinite(subdata)]\n\n        subdata_log10_with_nan = np.log10(subdata[subdata > 0])\n        subdata_log10 = subdata_log10_with_nan[~np.isnan(\n            subdata_log10_with_nan)\n        ]\n\n        subdata_log10 = subdata_log10[np.isfinite(subdata_log10)]\n\n        if merge_figs:\n            axes = axes_all[row_nr].squeeze()\n        else:\n            fig, axes = plt.subplots(1, 2, figsize=(10 / 2.54, 5 / 2.54))\n\n        ax = axes[0]\n        ax.hist(\n            subdata,\n            _get_nr_bins(subdata.size),\n        )\n        ax.set_xlabel(\n            units.get_label(key)\n        )\n        ax.set_ylabel('count')\n        ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(5))\n        ax.tick_params(axis='both', which='major', labelsize=6)\n        ax.tick_params(axis='both', which='minor', labelsize=6)\n\n        if subdata_log10.size > 0:\n            ax = axes[1]\n            ax.hist(\n                subdata_log10,\n                _get_nr_bins(subdata.size),\n            )\n            ax.set_xlabel(r'$log_{10}($' + units.get_label(key) + ')')\n            ax.set_ylabel('count')\n            ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(5))\n        else:\n            pass\n            # del(axes[1])\n\n        fig.tight_layout()\n\n        if not merge_figs:\n            figures[key] = fig\n\n    if merge_figs:\n        figures['all'] = fig\n    return figures", "response": "Generate histograms for one or more keys in the given container."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nproduces histograms grouped by the extra dimensions.", "response": "def plot_histograms_extra_dims(dataobj, keys, **kwargs):\n    \"\"\"Produce histograms grouped by the extra dimensions.\n\n    Extra dimensions are:\n\n    * timesteps\n    * frequency\n\n    Parameters\n    ----------\n    dataobj : :py:class:`pandas.DataFrame` or reda container\n        The data container/data frame which holds the data\n    keys:   list|tuple|iterable\n        The keys (columns) of the dataobj to plot\n    subquery : string, optional\n        ?\n\n    log10plot: bool\n        if True, generate linear and log10 versions of the histogram\n    Nx : int, optional\n        ?\n\n    Returns\n    -------\n\n    Examples\n    --------\n    >>> import reda.testing.containers\n    >>> ert = reda.testing.containers.ERTContainer_nr\n    >>> import reda.plotters.histograms as RH\n    >>> fig = RH.plot_histograms_extra_dims(ert, ['r', ])\n\n    >>> import reda.testing.containers\n    >>> ert = reda.testing.containers.ERTContainer_nr\n    >>> import reda.plotters.histograms as RH\n    >>> fig = RH.plot_histograms_extra_dims(ert, ['r', 'a'])\n    \"\"\"\n    if isinstance(dataobj, pd.DataFrame):\n        df_raw = dataobj\n    else:\n        df_raw = dataobj.data\n\n    if kwargs.get('subquery', False):\n        df = df_raw.query(kwargs.get('subquery'))\n    else:\n        df = df_raw\n\n    split_timestamps = True\n    if split_timestamps:\n        group_timestamps = df.groupby('timestep')\n        N_ts = len(group_timestamps.groups.keys())\n    else:\n        group_timestamps = ('all', df)\n        N_ts = 1\n\n    columns = keys\n    N_c = len(columns)\n\n    plot_log10 = kwargs.get('log10plot', False)\n    if plot_log10:\n        transformers = ['lin', 'log10']\n        N_log10 = 2\n    else:\n        transformers = ['lin', ]\n        N_log10 = 1\n\n    # determine layout of plots\n    Nx_max = kwargs.get('Nx', 4)\n    N = N_ts * N_c * N_log10\n    Nx = min(Nx_max, N)\n    Ny = int(np.ceil(N / Nx))\n\n    size_x = 5 * Nx / 2.54\n    size_y = 5 * Ny / 2.54\n    fig, axes = plt.subplots(Ny, Nx, figsize=(size_x, size_y), sharex=True,\n                             sharey=True)\n    axes = np.atleast_2d(axes)\n\n    index = 0\n    for ts_name, tgroup in group_timestamps:\n        for column in columns:\n            for transformer in transformers:\n                # print('{0}-{1}-{2}'.format(ts_name, column, transformer))\n\n                subdata_raw = tgroup[column].values\n                subdata = subdata_raw[~np.isnan(subdata_raw)]\n                subdata = subdata[np.isfinite(subdata)]\n\n                if transformer == 'log10':\n                    subdata_log10_with_nan = np.log10(subdata[subdata > 0])\n                    subdata_log10 = subdata_log10_with_nan[~np.isnan(\n                        subdata_log10_with_nan)\n                    ]\n\n                    subdata_log10 = subdata_log10[np.isfinite(subdata_log10)]\n                    subdata = subdata_log10\n\n                ax = axes.flat[index]\n                ax.hist(\n                    subdata,\n                    _get_nr_bins(subdata.size),\n                )\n                ax.set_xlabel(\n                    units.get_label(column)\n                )\n                ax.set_ylabel('count')\n                ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(3))\n                ax.tick_params(axis='both', which='major', labelsize=6)\n                ax.tick_params(axis='both', which='minor', labelsize=6)\n                ax.set_title(\"timestep: %d\" % ts_name)\n\n                index += 1\n\n    # remove some labels\n    for ax in axes[:, 1:].flat:\n        ax.set_ylabel('')\n    for ax in axes[:-1, :].flat:\n        ax.set_xlabel('')\n    fig.tight_layout()\n    return fig"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nproducing histograms for each group of extra dimensions.", "response": "def plot_histograms_it_extra_dims(dataobj, keys, extra_dims, **kwargs):\n    \"\"\"Produce histograms for each group of extra dimensions. \\*\\*kwargs are\n    directly passed on to plot_histograms().\n\n\n    \"\"\"\n    if isinstance(dataobj, pd.DataFrame):\n        df = dataobj\n    else:\n        df = dataobj.df\n\n    g = df.groupby(extra_dims)\n\n    results = {}\n    for name in sorted(g.groups.keys()):\n        item = g.get_group(name)\n        plot_results = plot_histograms(item, keys, **kwargs)\n        results[name] = plot_results\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_wenner_file(filename, settings):\n    # read data\n    with open(filename, 'r') as fid2:\n        geotom_data_orig = fid2.read()\n\n    # replace all ';' by ' ; '\n    geotom_data = geotom_data_orig.replace(';', ' ; ')\n\n    fid = StringIO()\n    fid.write(geotom_data)\n    fid.seek(0)\n\n    header = [fid.readline() for i in range(0, 16)]\n    header\n\n    df = pd.read_csv(\n        fid,\n        delim_whitespace=True,\n        header=None,\n        names=(\n            'elec1_wenner',\n            'a_w',\n            'rho_a',\n            'c4',\n            'c5',\n            'c6',\n            'c6',\n            'c7',\n            'c8',\n            'c9',\n        ),\n    )\n\n    # compute geometric factor using the Wenner formula\n    df['k'] = 2 * np.pi * df['a_w']\n    df['r'] = df['rho_a'] / df['k']\n\n    Am = df['elec1_wenner']\n    Bm = df['elec1_wenner'] + df['a_w']\n    Mm = df['elec1_wenner'] + 3 * df['a_w']\n    Nm = df['elec1_wenner'] + 2 * df['a_w']\n\n    df['a'] = Am / 2.0 + 1\n    df['b'] = Bm / 2.0 + 1\n    df['m'] = Mm / 2.0 + 1\n    df['n'] = Nm / 2.0 + 1\n\n    # remove any nan values\n    df.dropna(axis=0, subset=['a', 'b', 'm', 'n', 'r'], inplace=True)\n\n    return df", "response": "Parse a Geotom. wen file and return a dictionary of the data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_medusa_data(mat_filename, config_file):\n    df_emd, df_md = _read_mat_mnu0(mat_filename)\n\n    # 'configs' can be a numpy array or a filename\n    if not isinstance(config_file, np.ndarray):\n        configs = np.loadtxt(config_file).astype(int)\n    else:\n        configs = config_file\n\n    # construct four-point measurements via superposition\n    print('constructing four-point measurements')\n    quadpole_list = []\n\n    if df_emd is not None:\n        index = 0\n        for Ar, Br, M, N in configs:\n            # print('constructing', Ar, Br, M, N)\n            # the order of A and B doesn't concern us\n            A = np.min((Ar, Br))\n            B = np.max((Ar, Br))\n\n            # first choice: correct ordering\n            query_M = df_emd.query('a=={0} and b=={1} and p=={2}'.format(\n                A, B, M\n            ))\n            query_N = df_emd.query('a=={0} and b=={1} and p=={2}'.format(\n                A, B, N\n            ))\n\n            if query_M.size == 0 or query_N.size == 0:\n                continue\n\n            index += 1\n\n            # keep these columns as they are (no subtracting)\n            keep_cols = [\n                'datetime',\n                'frequency',\n                'a', 'b',\n                'Zg1', 'Zg2', 'Zg3',\n                'Is',\n                'Il',\n                'Zg',\n                'Iab',\n            ]\n\n            df4 = pd.DataFrame()\n            diff_cols = ['Zt', ]\n            df4[keep_cols] = query_M[keep_cols]\n            for col in diff_cols:\n                df4[col] = query_M[col].values - query_N[col].values\n            df4['m'] = query_M['p'].values\n            df4['n'] = query_N['p'].values\n\n            quadpole_list.append(df4)\n\n    if quadpole_list:\n        dfn = pd.concat(quadpole_list)\n        Rsign = np.sign(dfn['Zt'].real)\n        dfn['r'] = Rsign * np.abs(dfn['Zt'])\n        dfn['Vmn'] = dfn['r'] * dfn['Iab']\n        dfn['rpha'] = np.arctan2(\n            np.imag(dfn['Zt'].values),\n            np.real(dfn['Zt'].values)\n        ) * 1e3\n    else:\n        dfn = pd.DataFrame()\n\n    return dfn, df_md", "response": "Imports the MNU0 measurement data from the given MNU0 file into a list of the n - point ones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a. mat file with single potentials and return a DataFrame containing the EMD and EMDs.", "response": "def _read_mat_mnu0(filename):\n    \"\"\"Import a .mat file with single potentials (a b m) into a pandas\n    DataFrame\n\n    Also export some variables of the MD struct into a separate structure\n    \"\"\"\n    print('read_mag_single_file: {0}'.format(filename))\n\n    mat = sio.loadmat(filename, squeeze_me=True)\n    # check the version\n    version = mat['MP']['Version'].item()\n    if version != 'FZJ-EZ-2017':\n        raise Exception(\n            'This data format is not supported (expected: FZJ-EZ-2017)' +\n            ' got: {}'.format(version)\n        )\n\n    df_emd = _extract_emd(mat, filename=filename)\n    df_md = _extract_md(mat)\n\n    return df_emd, df_md"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the data from the EMD substruct given a MNU0 - mat file.", "response": "def _extract_emd(mat, filename):\n    \"\"\"Extract the data from the EMD substruct, given a medusa-created MNU0-mat\n    file\n\n    Parameters\n    ----------\n\n    mat: matlab-imported struct\n\n    \"\"\"\n    emd = mat['EMD'].squeeze()\n    # Labview epoch\n    epoch = datetime.datetime(1904, 1, 1)\n\n    def convert_epoch(x):\n        timestamp = epoch + datetime.timedelta(seconds=x.astype(float))\n        return timestamp\n\n    dfl = []\n    # loop over frequencies\n    for f_id in range(0, emd.size):\n        # print('Frequency: ', emd[f_id]['fm'])\n        fdata = emd[f_id]\n        # some consistency checks\n        if len(fdata['nu']) == 2 and fdata['nu'].shape[1] == 2:\n            raise Exception(\n                'Need MNU0 file, not a quadpole .mat file: {0}'.format(\n                    filename\n                )\n            )\n\n        # fdata_md = md[f_id]\n\n        timestamp = np.atleast_2d(\n            [convert_epoch(x) for x in fdata['Time'].squeeze()]\n        ).T\n        df = pd.DataFrame(\n            np.hstack((\n                timestamp,\n                fdata['ni'],\n                fdata['nu'][:, np.newaxis],\n                fdata['Zt3'],\n                fdata['Is3'],\n                fdata['Il3'],\n                fdata['Zg3'],\n                fdata['As3'][:, 0, :].squeeze(),\n                fdata['As3'][:, 1, :].squeeze(),\n                fdata['As3'][:, 2, :].squeeze(),\n                fdata['As3'][:, 3, :].squeeze(),\n                fdata['Yg13'],\n                fdata['Yg23'],\n            )),\n        )\n        df.columns = (\n            'datetime',\n            'a',\n            'b',\n            'p',\n            'Z1',\n            'Z2',\n            'Z3',\n            'Is1',\n            'Is2',\n            'Is3',\n            'Il1',\n            'Il2',\n            'Il3',\n            'Zg1',\n            'Zg2',\n            'Zg3',\n            'ShuntVoltage1_1',\n            'ShuntVoltage1_2',\n            'ShuntVoltage1_3',\n            'ShuntVoltage2_1',\n            'ShuntVoltage2_2',\n            'ShuntVoltage2_3',\n            'ShuntVoltage3_1',\n            'ShuntVoltage3_2',\n            'ShuntVoltage3_3',\n            'ShuntVoltage4_1',\n            'ShuntVoltage4_2',\n            'ShuntVoltage4_3',\n            'Yg13_1',\n            'Yg13_2',\n            'Yg13_3',\n            'Yg23_1',\n            'Yg23_2',\n            'Yg23_3',\n        )\n\n        df['frequency'] = np.ones(df.shape[0]) * fdata['fm']\n\n        # cast to correct type\n        df['datetime'] = pd.to_datetime(df['datetime'])\n        df['a'] = df['a'].astype(int)\n        df['b'] = df['b'].astype(int)\n        df['p'] = df['p'].astype(int)\n\n        df['Z1'] = df['Z1'].astype(complex)\n        df['Z2'] = df['Z2'].astype(complex)\n        df['Z3'] = df['Z3'].astype(complex)\n\n        df['Zg1'] = df['Zg1'].astype(complex)\n        df['Zg2'] = df['Zg2'].astype(complex)\n        df['Zg3'] = df['Zg3'].astype(complex)\n\n        df['Is1'] = df['Is1'].astype(complex)\n        df['Is2'] = df['Is2'].astype(complex)\n        df['Is3'] = df['Is3'].astype(complex)\n\n        df['Il1'] = df['Il1'].astype(complex)\n        df['Il2'] = df['Il2'].astype(complex)\n        df['Il3'] = df['Il3'].astype(complex)\n\n        df['ShuntVoltage1_1'] = df['ShuntVoltage1_1'].astype(complex)\n        df['ShuntVoltage1_2'] = df['ShuntVoltage1_2'].astype(complex)\n        df['ShuntVoltage1_3'] = df['ShuntVoltage1_3'].astype(complex)\n\n        df['ShuntVoltage2_1'] = df['ShuntVoltage2_1'].astype(complex)\n        df['ShuntVoltage2_2'] = df['ShuntVoltage2_2'].astype(complex)\n        df['ShuntVoltage2_3'] = df['ShuntVoltage2_3'].astype(complex)\n\n        df['ShuntVoltage3_1'] = df['ShuntVoltage3_1'].astype(complex)\n        df['ShuntVoltage3_2'] = df['ShuntVoltage3_2'].astype(complex)\n        df['ShuntVoltage3_3'] = df['ShuntVoltage3_3'].astype(complex)\n\n        df['ShuntVoltage4_1'] = df['ShuntVoltage4_1'].astype(complex)\n        df['ShuntVoltage4_2'] = df['ShuntVoltage4_2'].astype(complex)\n        df['ShuntVoltage4_3'] = df['ShuntVoltage4_3'].astype(complex)\n\n        dfl.append(df)\n\n    if len(dfl) == 0:\n        return None\n    df = pd.concat(dfl)\n\n    # average swapped current injections here!\n    # TODO\n\n    # sort current injections\n    condition = df['a'] > df['b']\n    df.loc[condition, ['a', 'b']] = df.loc[condition, ['b', 'a']].values\n    # change sign because we changed A and B\n    df.loc[condition, ['Z1', 'Z2', 'Z3']] *= -1\n\n    # average of Z1-Z3\n    df['Zt'] = np.mean(df[['Z1', 'Z2', 'Z3']].values, axis=1)\n    # we need to keep the sign of the real part\n    sign_re = df['Zt'].real / np.abs(df['Zt'].real)\n    df['r'] = np.abs(df['Zt']) * sign_re\n    # df['Zt_std'] = np.std(df[['Z1', 'Z2', 'Z3']].values, axis=1)\n\n    df['Is'] = np.mean(df[['Is1', 'Is2', 'Is3']].values, axis=1)\n    df['Il'] = np.mean(df[['Il1', 'Il2', 'Il3']].values, axis=1)\n    df['Zg'] = np.mean(df[['Zg1', 'Zg2', 'Zg3']].values, axis=1)\n\n    # \"standard\" injected current, in [mA]\n    df['Iab'] = np.abs(df['Is']) * 1e3\n    df['Iab'] = df['Iab'].astype(float)\n    # df['Is_std'] = np.std(df[['Is1', 'Is2', 'Is3']].values, axis=1)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a log line into a dict containing the values parsed from the log line.", "response": "def parse(line):\n    \"\"\"\n    Parses a log line using the regexices above.\n    :param line: A log line to be parsed.\n    :return: If no match is found, None, otherwise a dict containing the values parsed from the log line.\n    \"\"\"\n    values = None\n    matches = re.search(regex, line)\n    if matches:\n\n        # Standard values\n        values = {\n            'date_time': matches.group(DATE_TIME),\n            'log_level': matches.group(LOG_LEVEL),\n            'process_id': matches.group(PROCESS_ID),\n            'thread_name': matches.group(THREAD_NAME),\n            'logger_name': matches.group(LOGGER_NAME),\n            'log_message': matches.group(LOG_MESSAGE)\n        }\n\n        # Optional transaction tracking information\n        if matches.group(TRANSACTION):\n            values[\"transaction\"] = {\n                \"app\": matches.group(TRANSACTION_APP),\n                \"id\": matches.group(TRANSACTION_ID),\n                \"span\": matches.group(TRANSACTION_SPAN),\n                \"exported\": matches.group(TRANSACTION_EXPORTED)\n            }\n\n    return values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remote_path(self):\n        # TODO: move this property into independent object for PG\n        if len(self.book_id) > 1:\n            path_parts = list(self.book_id[:-1])\n        else:\n            path_parts = ['0']\n        path_parts.append(self.book_id)\n        return os.path.join(*path_parts) + '/'", "response": "turns an ebook_id into a path on PG s server s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make(self):\n        logger.debug(\"preparing to add all git files\")\n        num_added = self.local_repo.add_all_files()\n        if num_added:\n            self.local_repo.commit(\"Initial import from Project Gutenberg\")\n\n        file_handler = NewFilesHandler(self)\n        file_handler.add_new_files()\n\n        num_added = self.local_repo.add_all_files()\n        if num_added:\n            self.local_repo.commit(\n                \"Updates Readme, contributing, license files, cover, metadata.\"\n            )", "response": "turn fetched files into a local repo make auxiliary files\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef push(self):\n        self.github_repo.create_and_push()\n        self._repo = self.github_repo.repo\n        return self._repo", "response": "create a github repo and push the local repo into it\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tag(self, version='bump', message=''):\n        self.clone_from_github()\n        self.github_repo.tag(version, message=message)", "response": "tag the current branch"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a string and sanitizes it for Github s url name format", "response": "def format_title(self):\n        def asciify(_title):\n            _title = unicodedata.normalize('NFD', unicode(_title))\n            ascii = True\n            out = []\n            ok = u\"1234567890qwertyuiopasdfghjklzxcvbnmQWERTYUIOPASDFGHJKLZXCVBNM- ',\"\n            for ch in _title:\n                if ch in ok:\n                    out.append(ch)\n                elif unicodedata.category(ch)[0] == (\"L\"): #a letter\n                    out.append(hex(ord(ch)))\n                    ascii = False\n                elif ch in u'\\r\\n\\t':\n                    out.append(u'-')\n            return (ascii, sub(\"[ ',-]+\", '-', \"\".join(out)) )\n\n        \"\"\" Takes a string and sanitizes it for Github's url name format \"\"\"\n        (ascii, _title) = asciify(self.meta.title)\n        if not ascii and self.meta.alternative_title:\n            (ascii, _title2) = asciify(self.meta.alternative_title)\n            if ascii:\n                _title = _title2\n        title_length = 99 - len(str(self.book_id)) - 1\n        if len(_title) > title_length:\n            # if the title was shortened, replace the trailing _ with an ellipsis\n            repo_title = \"{0}__{1}\".format(_title[:title_length], self.book_id)\n        else:\n            repo_title = \"{0}_{1}\".format(_title[:title_length], self.book_id)\n        logger.debug(\"%s %s\" % (len(repo_title), repo_title))\n        self.meta.metadata['_repo'] = repo_title\n        return repo_title"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strip_xss(html, whitelist=None, replacement=\"(removed)\"):\n    re_html_tag = re.compile( # This matches HTML tags (if used correctly)\n      \"(?i)<\\/?\\w+((\\s+\\w+(\\s*=\\s*(?:\\\".*?\\\"|'.*?'|[^'\\\">\\s]+))?)+\\s*|\\s*)\\/?>\")\n    # This will match things like 'onmouseover=' ('on<whatever>=')\n    on_events_re = re.compile('.*\\s+(on[a-z]+\\s*=).*')\n    if not whitelist:\n        # These are all pretty safe and covers most of what users would want in\n        # terms of formatting and sharing media (images, audio, video, etc).\n        whitelist = set([\n            'a', 'abbr', 'aside', 'audio', 'bdi', 'bdo', 'blockquote', 'canvas',\n            'caption', 'code', 'col', 'colgroup', 'data', 'dd', 'del',\n            'details', 'div', 'dl', 'dt', 'em', 'figcaption', 'figure', 'h1',\n            'h2', 'h3', 'h4', 'h5', 'h6', 'hr', 'i', 'img', 'ins', 'kbd', 'li',\n            'mark', 'ol', 'p', 'pre', 'q', 'rp', 'rt', 'ruby', 's', 'samp',\n            'small', 'source', 'span', 'strong', 'sub', 'summary', 'sup',\n            'table', 'td', 'th', 'time', 'tr', 'track', 'u', 'ul', 'var',\n            'video', 'wbr'\n        ])\n    elif whitelist == \"off\":\n        whitelist = None # Disable it altogether\n    bad_tags = set()\n    for tag in re_html_tag.finditer(html):\n        tag = tag.group()\n        tag_lower = tag.lower()\n        short_tag = tag_lower.split()[0].lstrip('</').rstrip('>')\n        if whitelist and short_tag not in whitelist:\n            bad_tags.add(tag)\n            continue\n        # Make sure the tag can't execute any JavaScript\n        if \"javascript:\" in tag_lower:\n            bad_tags.add(tag)\n            continue\n        # on<whatever> events are not allowed (just another XSS vuln)\n        if on_events_re.search(tag_lower):\n            bad_tags.add(tag)\n            continue\n        # Flash sucks\n        if \"fscommand\" in tag_lower:\n            bad_tags.add(tag)\n            continue\n        # I'd be impressed if an attacker tried this one (super obscure)\n        if \"seeksegmenttime\" in tag_lower:\n            bad_tags.add(tag)\n            continue\n        # Yes we'll protect IE users from themselves...\n        if \"vbscript:\" in tag_lower:\n            bad_tags.add(tag)\n            continue\n    if replacement == \"entities\":\n        for bad_tag in bad_tags:\n            escaped = cgi.escape(bad_tag).encode('ascii', 'xmlcharrefreplace')\n            html = html.replace(bad_tag, escaped.decode('ascii'))\n    else:\n        for bad_tag in bad_tags:\n            html = html.replace(bad_tag, replacement)\n    return (html, bad_tags)", "response": "This function strips the given HTML string and returns a tuple containing all non - whitelisted HTML tags replaced with replacement."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd any number of supplied strings to self and returns a new ~htmltag. HTML instance with the result.", "response": "def append(self, *strings):\n        \"\"\"\n        Adds any number of supplied *strings* to `self` (we're a subclass of\n        `str` remember) just before the last closing tag and returns a new\n        instance of `~htmltag.HTML` with the result.\n        Example::\n\n            >>> from htmltag import span, b\n            >>> html = span('Test:')\n            >>> print(html)\n            <span>Test:</span>\n            >>> html = html.append(' ', b('appended'))\n            >>> print(html)\n            <span>Test: <b>appended</b></span>\n\n        In the case of self-closing tags like '<img>' the string will simply be\n        appended after the tag::\n\n            >>> from htmltag import img\n            >>> image = img(src=\"http://company.com/image.png\")\n            >>> print(image.append(\"Appended string\"))\n            <img src=\"http://company.com/image.png\">Appended string\n\n        .. note:: Why not update ourselves in-place?  Because we're a subclass\n            of `str`; in Python strings are immutable.\n        \"\"\"\n        close_tag_start = self.rfind('</')\n        if self.tagname: # More accurate\n            close_tag_start = self.rfind('</'+self.tagname)\n        if close_tag_start == -1: # Couldn't find closing tag\n            return self + \"\".join(strings) # Just tack on to the end\n        ending = self[close_tag_start:]\n        beginning = self[:close_tag_start]\n        if self.tagname: # Preserve it\n            tagname = self.tagname\n            new = HTML(beginning + \"\".join(strings) + ending)\n            new.tagname = tagname\n            return new\n        else:\n            return HTML(beginning + \"\".join(strings) + ending)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string with all instances of < > and '&' converted into HTML entities.", "response": "def escape(self, string):\n        \"\"\"\n        Returns *string* with all instances of '<', '>', and '&' converted into\n        HTML entities.\n        \"\"\"\n        html_entities = {\"&\": \"&amp;\", '<': '&lt;', '>': '&gt;'}\n        return HTML(\"\".join(html_entities.get(c, c) for c in string))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps the given tag with the given arguments and returns the resulting HTML string.", "response": "def wrap(self, tag, *args, **kwargs):\n        \"\"\"\n        Returns all *args* (strings) wrapped in HTML tags like so::\n\n            >>> b = TagWrap('b')\n            >>> print(b('bold text'))\n            <b>bold text</b>\n\n        To add attributes to the tag you can pass them as keyword arguments::\n\n            >>> a = TagWrap('a')\n            >>> print(a('awesome software', href='http://liftoffsoftware.com/'))\n            <a href=\"http://liftoffsoftware.com/\">awesome software</a>\n\n        .. note:: :meth:`~TagWrap.wrap` will automatically convert '<', '>', \\\n        and '&' into HTML entities unless the wrapped string has an `__html__` \\\n        method\n        \"\"\"\n        template = \"<{tagstart}>{content}</{tag}>\"\n        if tag in self_closing_tags:\n            template = \"<{tagstart}>\" # self-closing tags don't have content\n            if self.ending_slash:\n                template = \"<{tagstart} />\"\n        content = \"\"\n        for string in args:\n            if not hasattr(string, '__html__'): # Indicates already escaped\n                string = self.escape(string)\n            content += string.__html__()\n        tagstart = tag\n        if kwargs:\n            tagstart += ' '\n            for key, value in kwargs.items():\n                key = key.lstrip('_')\n                if value == True:\n                    tagstart = tagstart + key + ' '\n                elif value == False:\n                    continue # skip it altogether\n                else:\n                    tagstart = tagstart + '{key}=\"{value}\" '.format(\n                        key=key, value=value)\n            tagstart = tagstart.rstrip()\n        html = template.format(tagstart=tagstart, content=content, tag=tag)\n        if self.safe_mode:\n            html, rejected = strip_xss(\n                html, whitelist=self.whitelist, replacement=self.replacement)\n            if self.log_rejects:\n                logging.error(\n                    \"{name} rejected unsafe HTML: '{rejected}'\".format(\n                    name=self.__class__.__name__, rejected=rejected))\n        html = HTML(html)\n        html.tagname = tag # So we can easily append()\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self, tagname, **kwargs):\n        new_kwargs = {\n            'replacement': self.replacement,\n            'whitelist': self.whitelist,\n            'safe_mode': self.safe_mode,\n            'log_rejects': self.log_rejects,\n            'ending_slash': self.ending_slash\n        }\n        new_kwargs.update(**kwargs)\n        return TagWrap(tagname, **new_kwargs)", "response": "Returns a copy of this instance with the given tagname and any additional keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _request(self, path, method, body=None):\n        url = '/'.join([_SERVER, path])\n        (resp, content) = _HTTP.request(url, method,\n                                        headers=self._headers, body=body)\n\n        content_type = resp.get('content-type')\n        if content_type and content_type.startswith('application/json'):\n            content = json.loads(content.decode('UTF-8'))\n\n        return (resp, content)", "response": "Make a request from the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a PUT request from the API.", "response": "def put(self, path, payload):\n        \"\"\"Make a PUT request from the API.\"\"\"\n        body = json.dumps(payload)\n        return self._request(path, 'PUT', body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, path, payload):\n        body = json.dumps(payload)\n        return self._request(path, 'POST', body)", "response": "Make a POST request from the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_child(self, modules):\n        binder = self._binder.create_child()\n        return Injector(modules, binder=binder, stage=self._stage)", "response": "Create a new injector that inherits the state from this injector."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_data(self):\n        try:\n            with async_timeout.timeout(5, loop=self._loop):\n                response = await self._session.get(\n                    '{}/{}/'.format(self.url, self.sensor_id))\n\n            _LOGGER.debug(\n                \"Response from luftdaten.info: %s\", response.status)\n            self.data = await response.json()\n            _LOGGER.debug(self.data)\n        except (asyncio.TimeoutError, aiohttp.ClientError):\n            _LOGGER.error(\"Can not load data from luftdaten.info\")\n            raise exceptions.LuftdatenConnectionError()\n\n        if not self.data:\n            self.values = self.meta = None\n            return\n\n        try:\n            sensor_data = sorted(\n                self.data, key=lambda timestamp: timestamp['timestamp'],\n                reverse=True)[0]\n            print(sensor_data)\n\n            for entry in sensor_data['sensordatavalues']:\n                for measurement in self.values.keys():\n                    if measurement == entry['value_type']:\n                        self.values[measurement] = float(entry['value'])\n\n            self.meta['sensor_id'] = self.sensor_id\n            self.meta['longitude'] = float(\n                sensor_data['location']['longitude'])\n            self.meta['latitude'] = float(sensor_data['location']['latitude'])\n        except (TypeError, IndexError):\n            raise exceptions.LuftdatenError()", "response": "Retrieve the data from the Luftdaten."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate a message given a schema.", "response": "def validate(self, message, schema_name):\n        \"\"\"Validate a message given a schema.\n\n        Args:\n            message (dict): Loaded JSON of pulled message from Google\n                PubSub.\n            schema_name (str): Name of schema to validate ``message``\n                against. ``schema_name`` will be used to look up\n                schema from :py:attr:`.MessageValidator.schemas` dict\n        Raises:\n            InvalidMessageError: if message is invalid against the\n                given schema.\n            InvalidMessageError: if given schema name can not be found.\n        \"\"\"\n        err = None\n        try:\n            jsonschema.validate(message, self.schemas[schema_name])\n\n        except KeyError:\n            msg = (f'Schema \"{schema_name}\" was not found (available: '\n                   f'{\", \".join(self.schemas.keys())})')\n            err = {'msg': msg}\n\n        except jsonschema.ValidationError as e:\n            msg = (f'Given message was not valid against the schema '\n                   f'\"{schema_name}\": {e.message}')\n            err = {'msg': msg}\n\n        if err:\n            logging.error(**err)\n            raise exceptions.InvalidMessageError(err['msg'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compose(*functions):\n    ''' evaluates functions from right to left.\n\n        >>> add = lambda x, y: x + y\n        >>> add3 = lambda x: x + 3\n        >>> divide2 = lambda x: x/2\n        >>> subtract4 = lambda x: x - 4\n        >>> subtract1 = compose(add3, subtract4)\n        >>> subtract1(1)\n        0\n        >>> compose(subtract1, add3)(4)\n        6\n        >>> compose(int, add3, add3, divide2)(4)\n        8\n        >>> compose(int, divide2, add3, add3)(4)\n        5\n        >>> compose(int, divide2, compose(add3, add3), add)(7, 3)\n        8\n    '''\n    def inner(func1, func2):\n        return lambda *x, **y: func1(func2(*x, **y))\n    return functools.reduce(inner, functions)", "response": "evaluates functions from right to left."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary resulting from adding all keys and values of the second to the first CTYPE", "response": "def updated_schema(old, new):\n    ''' Creates a dictionary resulting from adding all keys/values of the second to the first\n        The second dictionary will overwrite the first.\n\n        >>> old, new = {'name': 'ric', 'job': None}, {'name': 'Rick'}\n        >>> updated = updated_schema(old, new)\n        >>> len(updated.keys())\n        2\n        >>> print(updated['name'])\n        Rick\n        >>> updated['job'] is None\n        True\n\n    '''\n    d = deepcopy(old)\n    for key, value in new.items():\n        if isinstance(value, dict) and old.get(key) and isinstance(old[key], dict):\n            d[key] = updated_schema(old[key], new[key])\n        else:\n            d[key] = value\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_mouse_allele_name(name):\n    original = name\n    if name.upper().startswith(\"H2\"):\n        name = name[2:]\n    elif name.upper().startswith(\"H-2\"):\n        name = name[3:]\n    _, name = parse_separator(name)\n\n    # special logic for mouse alleles\n    if name.upper().startswith(\"I\"):\n        # class II mouse allele\n        if len(name) < 2:\n            raise AlleleParseError(\"Incomplete mouse MHC allele: %s\" % original)\n        gene_name = name[:2]\n        name = name[2:]\n    else:\n        # class I mouse allele\n        if len(name) < 1:\n            raise AlleleParseError(\"Incomplete mouse MHC allele: %s\" % original)\n        gene_name = name[0]\n        name = name[1:]\n    _, name = parse_separator(name)\n\n    if len(name) != 1:\n        raise AlleleParseError(\n            \"Malformed mouse MHC allele: %s, parse error at %s\" % (\n                original, name))\n    allele = name[0]\n    return gene_name.upper(), allele.lower()", "response": "Parses a mouse MHC allele name into a gene and allele code pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates if the instance should be logged or is excluded", "response": "def validate_instance(instance):\n    \"\"\"Validating if the instance should be logged, or is excluded\"\"\"\n    excludes = settings.AUTOMATED_LOGGING['exclude']['model']\n\n    for excluded in excludes:\n        if (excluded in [instance._meta.app_label.lower(),\n                         instance.__class__.__name__.lower()] or\n                instance.__module__.lower().startswith(excluded)):\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets current user object from middleware", "response": "def get_current_user():\n    \"\"\"Get current user object from middleware\"\"\"\n    thread_local = AutomatedLoggingMiddleware.thread_local\n    if hasattr(thread_local, 'current_user'):\n        user = thread_local.current_user\n        if isinstance(user, AnonymousUser):\n            user = None\n    else:\n        user = None\n\n    return user"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets current application and path object from middleware", "response": "def get_current_environ():\n    \"\"\"Get current application and path object from middleware\"\"\"\n    thread_local = AutomatedLoggingMiddleware.thread_local\n    if hasattr(thread_local, 'request_uri'):\n        request_uri = thread_local.request_uri\n    else:\n        request_uri = None\n\n    if hasattr(thread_local, 'application'):\n        application = thread_local.application\n        application = Application.objects.get_or_create(name=application)[0]\n    else:\n        application = None\n\n    if hasattr(thread_local, 'method'):\n        method = thread_local.method\n    else:\n        method = None\n\n    if hasattr(thread_local, 'status'):\n        status = thread_local.status\n    else:\n        status = None\n\n    return request_uri, application, method, status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all the current category s parents.", "response": "def parents(self):\n        \"\"\" Returns a list of all the current category's parents.\"\"\"\n        parents = []\n\n        if self.parent is None:\n            return []\n\n        category = self\n        while category.parent is not None:\n            parents.append(category.parent)\n            category = category.parent\n\n        return parents[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef root_parent(self, category=None):\n        return next(filter(lambda c: c.is_root, self.hierarchy()))", "response": "Returns the root parent of the current category."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef active(self) -> bool:\n        states = self._client.get_state(self._state_url)['states']\n        for state in states:\n            state = state['State']\n            if int(state['Id']) == self._state_id:\n                # yes, the ZM API uses the *string* \"1\" for this...\n                return state['IsActive'] == \"1\"\n        return False", "response": "Indicate if this RunState is currently active."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a value to the most reasonable unit.", "response": "def to_reasonable_unit(value, units, round_digits=2):\n    \"\"\"Convert a value to the most reasonable unit.\n\n    The most reasonable unit is roughly the one with the smallest exponent\n    absolute value when written in scientific notation. For example\n    `1.5` is more reasonable that `.0015` and `22` is more reasonable\n    than `22000`. There is a bias towards numbers > 1, so `3.2` is\n    considered more reasonable that `.32`.\n    \"\"\"\n    def to_unit(unit):\n        return float(value) / unit[1]\n    exponents = [abs(Decimal(to_unit(u)).adjusted() - 1) for u in units]\n    best = min(enumerate(exponents), key=itemgetter(1))[0]\n    return dict(val=round(to_unit(units[best]), round_digits),\n                label=units[best][0],\n                multiplier=units[best][1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning extended progress bar text", "response": "def get_text(self):\n        \"\"\"Return extended progress bar text\"\"\"\n        done_units = to_reasonable_unit(self.done, self.units)\n        current = round(self.current / done_units['multiplier'], 2)\n        percent = int(self.current * 100 / self.done)\n        return '{0:.2f} of {1:.2f} {2} ({3}%)'.format(current,\n                                                      done_units['val'],\n                                                      done_units['label'],\n                                                      percent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_progress(self, delta, done=None):\n        if done is not None:\n            self.done = done\n        self.bar.current = max(min(self.done, self.current + delta), 0)\n        self.rate_display.set_text(self.rate_text)\n        self.remaining_time_display.set_text(self.remaining_time_text)\n        return self.current == self.done", "response": "Add delta to the current progress amount."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        self.bar.current = 0\n        self.start_time = time.time()\n        self.add_progress(0)", "response": "Reset the current value and restart the progress bar timer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremains time until complete at current rate", "response": "def remaining_time(self):\n        \"\"\"remaining time (as a timedelta) until complete at current rate\"\"\"\n        if self.rate == 0:\n            return None\n        else:\n            remaining_progress = self.done - self.current\n            if self.rate == 0:\n                if self.done == 0:\n                    return 0\n                else:\n                    return None\n\n            return remaining_progress / self.rate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrecovering a public key from a signature.", "response": "def _recover_public_key(G, order, r, s, i, e):\n    \"\"\"Recover a public key from a signature.\n    See SEC 1: Elliptic Curve Cryptography, section 4.1.6, \"Public\n    Key Recovery Operation\".\n    http://www.secg.org/sec1-v2.pdf\n    \"\"\"\n\n    c = G.curve()\n\n    # 1.1 Let x = r + jn\n    x = r + (i // 2) * order\n\n    # 1.3 point from x\n    alpha = (x * x * x + c.a() * x + c.b()) % c.p()\n    beta = pycoin.ecdsa.numbertheory.modular_sqrt(alpha, c.p())\n    y = beta if (beta - i) % 2 == 0 else c.p() - beta\n\n    # 1.4 Check that nR is at infinity\n    R = pycoin.ecdsa.ellipticcurve.Point(c, x, y, order)\n\n    rInv = pycoin.ecdsa.numbertheory.inverse_mod(r, order)  # r^-1\n    eNeg = -e % order  # -e\n\n    # 1.6 compute Q = r^-1 (sR - eG)\n    Q = rInv * (s * R + eNeg * G)\n    return Q"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def valid_token_set(self):\n        is_valid = False\n\n        if self._auth_client.token:\n            # Account for a token near expiration\n            now = datetime.datetime.utcnow()\n            skew = datetime.timedelta(seconds=60)\n            if self._auth_client.expiry > (now + skew):\n                is_valid = True\n        return is_valid", "response": "Check if the token is valid and refresh if none or expired."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def request(self, method, url, params=None, headers=None,\n                      data=None, json=None, token_refresh_attempts=2,\n                      **kwargs):\n        \"\"\"Make an asynchronous HTTP request.\n\n        Args:\n            method (str): HTTP method to use for the request.\n            url (str): URL to be requested.\n            params (dict): (optional) Query parameters for the request.\n                Defaults to ``None``.\n            headers (dict): (optional) HTTP headers to send with the\n                request. Headers pass through to the request will\n                include :attr:`DEFAULT_REQUEST_HEADERS`.\n            data (obj): (optional) A dictionary, bytes, or file-like\n                object to send in the body of the request.\n            json (obj): (optional) Any json compatible python\n                object.\n                NOTE: json and body parameters cannot be used at the same time.\n            token_refresh_attempts (int): (optional) Number of attempts a token\n                refresh should be performed.\n        Returns:\n            (str) HTTP response body.\n        Raises:\n            :exc:`.GCPHTTPError`: if any exception occurred,\n                specifically a :exc:`.GCPHTTPResponseError`, if the\n                exception is associated with a response status code.\n\n        \"\"\"\n        if all([data, json]):\n            msg = ('\"data\" and \"json\" request parameters can not be used '\n                   'at the same time')\n            logging.warn(msg)\n            raise exceptions.GCPHTTPError(msg)\n\n        req_headers = headers or {}\n        req_headers.update(_utils.DEFAULT_REQUEST_HEADERS)\n        req_kwargs = {\n                'params': params,\n                'headers': req_headers,\n            }\n\n        if data:\n            req_kwargs['data'] = data\n        if json:\n            req_kwargs['json'] = json\n\n        if token_refresh_attempts:\n            if not await self.valid_token_set():\n                await self._auth_client.refresh_token()\n                token_refresh_attempts -= 1\n\n        req_headers.update(\n            {'Authorization': f'Bearer {self._auth_client.token}'}\n        )\n\n        request_id = kwargs.get('request_id', uuid.uuid4())\n        logging.debug(_utils.REQ_LOG_FMT.format(\n            request_id=request_id,\n            method=method.upper(),\n            url=url,\n            kwargs=req_kwargs))\n        try:\n            async with self._session.request(method, url, **req_kwargs) as resp:\n                log_kw = {\n                    'request_id': request_id,\n                    'method': method.upper(),\n                    'url': resp.url,\n                    'status': resp.status,\n                    'reason': resp.reason\n                }\n                logging.debug(_utils.RESP_LOG_FMT.format(**log_kw))\n\n                if resp.status in REFRESH_STATUS_CODES:\n                    logging.warning(\n                        f'[{request_id}] HTTP Status Code {resp.status}'\n                        f' returned requesting {resp.url}: {resp.reason}')\n                    if token_refresh_attempts:\n                        logging.info(\n                            f'[{request_id}] Attempting request to {resp.url} '\n                            'again.')\n                        return await self.request(\n                            method, url,\n                            token_refresh_attempts=token_refresh_attempts,\n                            request_id=request_id,\n                            **req_kwargs)\n\n                    logging.warning(\n                        f'[{request_id}] Max attempts refreshing auth token '\n                        f'exhausted while requesting {resp.url}')\n\n                resp.raise_for_status()\n\n                return await resp.text()\n        except aiohttp.ClientResponseError as e:\n            # bad HTTP status; avoid leaky abstractions and wrap HTTP errors\n            # with our own\n            msg = f'[{request_id}] HTTP error response from {resp.url}: {e}'\n            logging.error(msg, exc_info=e)\n            raise exceptions.GCPHTTPResponseError(msg, resp.status)\n        except exceptions.GCPHTTPResponseError as e:\n            # from recursive call\n            raise e\n        except Exception as e:\n            msg = f'[{request_id}] Request call failed: {e}'\n            logging.error(msg, exc_info=e)\n            raise exceptions.GCPHTTPError(msg)", "response": "Makes an asynchronous HTTP request to the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a URL and return its JSON response.", "response": "async def get_json(self, url, json_callback=None, **kwargs):\n        \"\"\"Get a URL and return its JSON response.\n\n        Args:\n            url (str): URL to be requested.\n            json_callback (func): Custom JSON loader function. Defaults\n                to :meth:`json.loads`.\n            kwargs (dict): Additional arguments to pass through to the\n                request.\n        Returns:\n            response body returned by :func:`json_callback` function.\n        \"\"\"\n        if not json_callback:\n            json_callback = json.loads\n        response = await self.request(method='get', url=url, **kwargs)\n        return json_callback(response)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naggregating data from all pages of an API endpoint URL.", "response": "async def get_all(self, url, params=None):\n        \"\"\"Aggregate data from all pages of an API query.\n\n        Args:\n            url (str): Google API endpoint URL.\n            params (dict): (optional) URL query parameters.\n\n        Returns:\n            list: Parsed JSON query response results.\n        \"\"\"\n        if not params:\n            params = {}\n        items = []\n        next_page_token = None\n\n        while True:\n            if next_page_token:\n                params['pageToken'] = next_page_token\n            response = await self.get_json(url, params=params)\n\n            items.append(response)\n            next_page_token = response.get('nextPageToken')\n            if not next_page_token:\n                break\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_config():\n    configfile = ConfigFile()\n    global data\n    if data.keys() > 0:\n        # FIXME: run a better check of this file\n        print(\"gitberg config file exists\")\n        print(\"\\twould you like to edit your gitberg config file?\")\n    else:\n        print(\"No config found\")\n        print(\"\\twould you like to create a gitberg config file?\")\n    answer = input(\"-->  [Y/n]\")\n    # By default, the answer is yes, as denoted by the capital Y\n    if not answer:\n        answer = 'Y'\n\n    # If yes, generate a new configuration\n    # to be written out as yaml\n\n    if answer in 'Yy':\n        print(\"Running gitberg config generator ...\")\n        # config.exists_or_make()\n        config_gen = ConfigGenerator(current=data)\n        config_gen.ask()\n        # print(config_gen.answers)\n        data = config_gen.answers\n        configfile.write()\n        print(\"Config written to {}\".format(configfile.file_path))", "response": "Report if there is an existing config file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsampling code to retrieve the data.", "response": "async def main():\n    \"\"\"Sample code to retrieve the data.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        data = Luftdaten(SENSOR_ID, loop, session)\n        await data.get_data()\n\n        if not await data.validate_sensor():\n            print(\"Station is not available:\", data.sensor_id)\n            return\n\n        if data.values and data.meta:\n            # Print the sensor values\n            print(\"Sensor values:\", data.values)\n\n            # Print the coordinates fo the sensor\n            print(\"Location:\", data.meta['latitude'], data.meta['longitude'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a string from a list of chinese items.", "response": "def _build_str_from_chinese(chinese_items):\n    \"\"\"\n    \u6839\u636e\u89e3\u6790\u51fa\u7684\u4e2d\u6587\u65f6\u95f4\u5b57\u7b26\u4e32\u7684\u5173\u952e\u5b57\u8fd4\u56de\u5bf9\u5e94\u7684\u6807\u51c6\u683c\u5f0f\u5b57\u7b26\u4e32\n    \"\"\"\n    year, month, day = chinese_items\n    year = reduce(lambda a, b: a*10+b, map(CHINESE_NUMS.find, year))\n    return '%04d-%02d-%02d 00:00:00' % (year, _parse_chinese_field(month), _parse_chinese_field(day))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a string from an expression.", "response": "def _build_str_from_expression(exp_items):\n    \"\"\"\n    \u6839\u636e\u89e3\u6790\u51fa\u7684\u65f6\u95f4\u8868\u8fbe\u5f0f\u7684\u5173\u952e\u5b57\u8ba1\u7b97\u5bf9\u5e94\u7684\u8868\u8fbe\u5f0f\u503c\n    :return: \u8868\u8fbe\u5f0f\u5bf9\u5e94\u65f6\u95f4\u7684\u6807\u51c6\u65f6\u95f4\u683c\u5f0f\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\n    \"\"\"\n    time_delta_base, unit, method = exp_items\n    if unit not in _KEY_TIME_MAPPING:\n        return None\n    delta = datetime.timedelta(seconds=int(time_delta_base) * _KEY_TIME_MAPPING[unit])\n    now = datetime.datetime.now()\n    return datetime.datetime.strftime(now + (delta, -delta)[u'\u524d' == method], STARDAND_FORMAT)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a string from a list of time items.", "response": "def _build_str_from_time_items(items):\n    \"\"\"\n    \u6839\u636e\u89e3\u6790\u51fa\u7684\u65f6\u95f4\u5b57\u7b26\u4e32\u5173\u952e\u5b57\u8ba1\u7b97\u6807\u51c6\u65f6\u95f4\u8868\u793a\u683c\u5f0f\u7684\u5b57\u7b26\u4e32\n    :return: \u6807\u51c6\u65f6\u95f4\u683c\u5f0f\u5b57\u7b26\u4e32\u8868\u793a\u5f62\u5f0f\n    \"\"\"\n    if not items:\n        return None\n    items = [int(item) for item in items if item]\n    items = items + [0 for _ in xrange(6-len(items))]\n    return '%d-%02d-%02d %02d:%02d:%02d' % (items[0], items[1], items[2], items[3], items[4], items[5])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_time(time_str, iso_time_obj=False):\n    time_str = _parse_time(time_str)\n    if not time_str:\n        return \"\"\n    if iso_time_obj:\n        return datetime.datetime.strptime(time_str, STARDAND_FORMAT)\n    return time_str", "response": "Parse a time string into a datetime object or time string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef now(obj=True, utc=False, precise=False):\n    t = (datetime.datetime.now(), datetime.datetime.utcnow())[utc]\n    return (t.strftime(STARDAND_FORMAT + ('', '.%f')[precise]), t)[obj]", "response": "Get the current time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching all instances in a GCE project.", "response": "async def list_instances(self,\n                             project,\n                             page_size=100,\n                             instance_filter=None):\n        \"\"\"Fetch all instances in a GCE project.\n\n        You can find the endpoint documentation `here <https://cloud.\n        google.com/compute/docs/reference/latest/instances/\n        aggregatedList>`__.\n\n        Args:\n            project (str): unique, user-provided project ID.\n            page_size (int): hint for the client to only retrieve up to\n                this number of results per API call.\n            instance_filter (str): endpoint-specific filter string used\n                to retrieve a subset of instances. This is passed\n                directly to the endpoint's \"filter\" URL query parameter.\n        Returns:\n            list(dicts): data of all instances in the given\n                :obj:`project`\n        \"\"\"\n        url = (f'{self.BASE_URL}{self.api_version}/projects/{project}'\n               '/aggregated/instances')\n        params = {'maxResults': page_size}\n        if instance_filter:\n            params['filter'] = instance_filter\n\n        responses = await self.list_all(url, params)\n        instances = self._parse_rsps_for_instances(responses)\n        return instances"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef infer_alpha_chain(beta):\n    if beta.gene.startswith(\"DRB\"):\n        return AlleleName(species=\"HLA\", gene=\"DRA1\", allele_family=\"01\", allele_code=\"01\")\n    elif beta.gene.startswith(\"DPB\"):\n        # Most common alpha chain for DP is DPA*01:03 but we really\n        # need to change this logic to use a lookup table of pairwise\n        # frequencies for inferring the alpha-beta pairing\n        return AlleleName(\n            species=\"HLA\", gene=\"DPA1\", allele_family=\"01\", allele_code=\"03\")\n    elif beta.gene.startswith(\"DQB\"):\n        # Most common DQ alpha (according to wikipedia)\n        # DQA1*01:02\n        return AlleleName(\n            species=\"HLA\", gene=\"DQA1\", allele_family=\"01\", allele_code=\"02\")\n    return None", "response": "Infer the most frequent alpha chain for a given beta chain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_classi_or_classii_allele_name(name, infer_pair=True):\n    species, name = split_species_prefix(name)\n\n    # Handle the case where alpha/beta pairs are separated with a /.\n    name = name.replace(\"/\", \"-\")\n\n    # Ignored underscores, such as with DRB1_0102\n    name = name.replace(\"_\", \"*\")\n\n    parts = name.split(\"-\")\n\n    if len(parts) == 2:\n        alpha_string, beta_string = parts\n        alpha = parse_allele_name(alpha_string)\n        beta = parse_allele_name(beta_string)\n        return (alpha, beta)\n    elif len(parts) == 1:\n        parsed = parse_allele_name(name, species)\n        if parsed.species == \"HLA\" and infer_pair:\n            alpha = infer_alpha_chain(parsed)\n            if alpha is not None:\n                return (alpha, parsed)\n        return (parsed,)\n    else:\n        raise AlleleParseError(\n            \"Allele has too many parts: %s\" % name)", "response": "Parse a single or alpha - beta allele name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef producer(cfg_uri, queue, logger=None):\n    _info, _exception = _deal_logger(logger)\n    cfg_uri = _deal_uri(cfg_uri)\n\n    def decorator(function):\n        @wraps(function)\n        def wapper(*args, **kwargs):\n            client = _conn(cfg_uri, queue, _info)\n            for item in function(*args, **kwargs):\n                try:\n                    if not isinstance(item, dict):\n                        item = {'data': item}\n                    data = dumps(item, ensure_ascii=False)\n                    client.send(queue, data, headers={'persistent': 'true'})\n                    _info('Producer insert %s - %s' % (queue, item))\n                except ProducerFatalError, e:\n                    _exception(e)\n                    break\n                except Exception, e:\n                    _exception(e)\n            client.disconnect()\n            _info('disconnected %s' % cfg_uri)\n        return wapper\n    return decorator", "response": "Decorator for the producer function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process(data_stream):\n    import xml.etree.cElementTree as ElementTree\n\n    def parse_diff(source, handle):\n        for event, elem in ElementTree.iterparse(source,\n                                                 events=('start', 'end')):\n            if event == 'start':\n                handle.start_element(elem.tag, elem.attrib)\n            elif event == 'end':\n                handle.end_element(elem.tag)\n            elem.clear()\n\n    class osc_decoder():\n        def __init__(self):\n            self.changes = {}\n            self.nodes = {}\n            self.ways = {}\n            self.relations = {}\n            self.action = \"\"\n            self.primitive = {}\n            self.missingNds = set()\n\n        def start_element(self, name, attributes):\n            if name in ('modify', 'delete', 'create'):\n                self.action = name\n            if name in ('node', 'way', 'relation'):\n                self.primitive['id'] = int(attributes['id'])\n                self.primitive['version'] = int(attributes['version'])\n                self.primitive['changeset'] = int(attributes['changeset'])\n                self.primitive['username'] = attributes['user']\n                self.primitive['uid'] = attributes['uid']\n                self.primitive['timestamp'] = attributes['timestamp']\n                self.primitive['tags'] = {}\n                self.primitive['action'] = self.action\n            if name == 'node':\n                self.primitive['lat'] = float(attributes['lat'])\n                self.primitive['lon'] = float(attributes['lon'])\n            elif name == 'tag':\n                key = attributes['k']\n                val = attributes['v']\n                self.primitive['tags'][key] = val\n            elif name == 'way':\n                self.primitive['nodes'] = []\n            elif name == 'relation':\n                self.primitive['members'] = []\n            elif name == 'nd':\n                ref = int(attributes['ref'])\n                self.primitive['nodes'].append(ref)\n                if ref not in self.nodes:\n                    self.missingNds.add(ref)\n            elif name == 'member':\n                self.primitive['members'].append({\n                    'type': attributes['type'],\n                    'role': attributes['role'],\n                    'ref': attributes['ref']\n                })\n\n        def end_element(self, name):\n            if name == 'node':\n                self.nodes[self.primitive['id']] = self.primitive\n            elif name == 'way':\n                self.ways[self.primitive['id']] = self.primitive\n            elif name == 'relation':\n                self.relations[self.primitive['id']] = self.primitive\n            if name in ('node', 'way', 'relation'):\n                self.primitive = {}\n\n    data_object = osc_decoder()\n    parse_diff(data_stream, data_object)\n\n    return data_object", "response": "Process a diff file stream into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Zendesk ticket", "response": "def create_ticket(subject, tags, ticket_body, requester_email=None,\n                  custom_fields=[]):\n    \"\"\" Create a new Zendesk ticket \"\"\"\n\n    payload = {'ticket': {\n        'subject': subject,\n        'comment': {\n            'body': ticket_body\n        },\n        'group_id': settings.ZENDESK_GROUP_ID,\n        'tags': tags,\n        'custom_fields': custom_fields\n    }}\n\n    if requester_email:\n        payload['ticket']['requester'] = {\n            'name': 'Sender: %s' % requester_email.split('@')[0],\n            'email': requester_email,\n        }\n    else:\n        payload['ticket']['requester_id'] = settings.ZENDESK_REQUESTER_ID\n\n    requests.post(\n        get_ticket_endpoint(),\n        data=json.dumps(payload),\n        auth=zendesk_auth(),\n        headers={'content-type': 'application/json'}).raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a message in the current window.", "response": "def message(message, title=''):\n    \"\"\"\n    Display a message\n\n    :ref:`screenshots<message>`\n\n    :param message: message to be displayed.\n    :param title: window title\n    :rtype: None\n    \"\"\"\n    return backend_api.opendialog(\"message\", dict(message=message, title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text(text, message='', title=''):\n    return backend_api.opendialog(\"text\", dict(text=text, message=message, title=title))", "response": "Display a text window."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a dialog with some text.", "response": "def ask_string(message='Enter something.', default='', title=''):\n    \"\"\"\n    Show a box in which a user can enter some text.\n\n    You may optionally specify some default text, which will appear in the\n    entry-box when it is displayed.\n\n    Returns the text that the user entered, or None if he cancels the operation\n\n    :ref:`screenshots<ask_string>`\n\n    :param message: message to be displayed.\n    :param title: window title\n    :param default: entry-box default string\n    :rtype: None or string\n    \"\"\"\n    return backend_api.opendialog(\"ask_string\", dict(message=message, default=default, title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay a list of choices.", "response": "def choice(choices=[], message='Pick something.', default=None, title=''):\n    \"\"\"\n    Present the user with a list of choices.\n    return the choice that he selects.\n    return None if he cancels the selection selection.\n\n    :ref:`screenshots<choice>`\n\n    :param choices: a list of the choices to be displayed\n    :param message: message to be displayed.\n    :param title: window title\n    :param default: default string of choice\n    :rtype: None or string\n    \"\"\"\n    return backend_api.opendialog(\"choice\", dict(choices=choices, message=message, default=default, title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a list of items in a window with a message.", "response": "def multi_choice(choices=[], message='Pick as many items as you like.', default=None, title=''):\n    \"\"\"\n    Present the user with a list of choices.\n    allow him to select multiple items and return them in a list.\n    if the user doesn't choose anything from the list, return the empty list.\n    return None if he cancelled selection.\n\n    :ref:`screenshots<multi_choice>`\n\n    :param choices: a list of the choices to be displayed\n    :param message: message to be displayed.\n    :param title: window title\n    :param default: default list of strings\n    :rtype: None or list of strings\n    \"\"\"\n    return backend_api.opendialog(\"multi_choice\", dict(choices=choices, message=message, default=default, title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndisplaying a message with choices of OK and Cancel.", "response": "def ask_ok_cancel(message='', default=0, title=''):\n    \"\"\"\n    Display a message with choices of OK and Cancel.\n\n    returned value:\n        OK       -> True\n        Cancel   -> False\n\n    :ref:`screenshots<ask_ok_cancel>`\n\n    :param message: message to be displayed.\n    :param title: window title\n    :param default: default button as boolean (OK=True, Cancel=False)\n    :rtype: bool\n    \"\"\"\n    return backend_api.opendialog(\"ask_ok_cancel\", dict(message=message, default=default, title=title))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ask_yes_no(message='', default=0, title=''):\n    return backend_api.opendialog(\"ask_yes_no\", dict(message=message, default=default, title=title))", "response": "Display a message with choices of Yes and No."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads actions from an entry point group.", "response": "def load_entry_point_group(self, entry_point_group):\n        \"\"\"Load actions from an entry point group.\"\"\"\n        for ep in pkg_resources.iter_entry_points(group=entry_point_group):\n            self.register(ep.name, ep.load())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_app(self, app, entry_point_group='invenio_webhooks.receivers'):\n        self.init_config(app)\n        state = _WebhooksState(app, entry_point_group=entry_point_group)\n        self._state = app.extensions['invenio-webhooks'] = state", "response": "Initialize the Flask application."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef skip(self, sched_rule_id):\n        path = 'schedulerule/skip'\n        payload = {'id': sched_rule_id}\n        return self.rachio.put(path, payload)", "response": "Skip a schedule rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self, sched_rule_id):\n        path = 'schedulerule/start'\n        payload = {'id': sched_rule_id}\n        return self.rachio.put(path, payload)", "response": "Start a schedule rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef seasonalAdjustment(self, sched_rule_id, adjustment):\n        path = 'schedulerule/seasonal_adjustment'\n        payload = {'id': sched_rule_id, 'adjustment': adjustment}\n        return self.rachio.put(path, payload)", "response": "Seasonal adjustment for a schedule rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the information for a scheduleRule entity.", "response": "def get(self, sched_rule_id):\n        \"\"\"Retrieve the information for a scheduleRule entity.\"\"\"\n        path = '/'.join(['schedulerule', sched_rule_id])\n        return self.rachio.get(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the message according to schema.", "response": "def parse(self, message, schema):\n        \"\"\"Parse message according to schema.\n\n        `message` should already be validated against the given schema.\n        See :ref:`schemadef` for more information.\n\n        Args:\n            message (dict): message data to parse.\n            schema (str): valid message schema.\n        Returns:\n            (dict): parsed message\n        \"\"\"\n        func = {\n            'audit-log': self._parse_audit_log_msg,\n            'event': self._parse_event_msg,\n        }[schema]\n        return func(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, zone_id):\n        path = '/'.join(['zone', zone_id])\n        return self.rachio.get(path)", "response": "Retrieve the information for a zone entity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clean_translation(self):\n        translation = self.cleaned_data['translation']\n\n        if self.instance and self.instance.content_object:\n            # do not allow string longer than translatable field\n            obj = self.instance.content_object\n            field = obj._meta.get_field(self.instance.field)\n            max_length = field.max_length\n\n            if max_length and len(translation) > max_length:\n                raise forms.ValidationError(\n                    _('The entered translation is too long. You entered '\n                      '%(entered)s chars, max length is %(maxlength)s') % {\n                        'entered': len(translation),\n                        'maxlength': max_length,\n                    }\n                )\n        else:\n            raise forms.ValidationError(\n                _('Can not store translation. First create all translation'\n                  ' for this object')\n            )\n        return translation", "response": "Check if the translation is longer than the max_lenght of the field to\n           . If so raise forms. ValidationError."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_safe_return_to(request, return_to):\n    if return_to and is_safe_url(url=return_to, host=request.get_host()) and return_to != request.build_absolute_uri():\n        return return_to", "response": "Ensure the user - originating redirection url is safe."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding merge rules as key - value pairs in which the first element is a JSON path as a tuple and the second element is a list of merge properties whose values are true.", "response": "def _get_merge_rules(properties, path=None):\n    \"\"\"\n    Yields merge rules as key-value pairs, in which the first element is a JSON path as a tuple, and the second element\n    is a list of merge properties whose values are `true`.\n    \"\"\"\n    if path is None:\n        path = ()\n\n    for key, value in properties.items():\n        new_path = path + (key,)\n        types = _get_types(value)\n\n        # `omitWhenMerged` supersedes all other rules.\n        # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#omit-when-merged\n        if value.get('omitWhenMerged') or value.get('mergeStrategy') == 'ocdsOmit':\n            yield (new_path, {'omitWhenMerged'})\n        # `wholeListMerge` supersedes any nested rules.\n        # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#whole-list-merge\n        elif 'array' in types and (value.get('wholeListMerge') or value.get('mergeStrategy') == 'ocdsVersion'):\n            yield (new_path, {'wholeListMerge'})\n        elif 'object' in types and 'properties' in value:\n            yield from _get_merge_rules(value['properties'], path=new_path)\n        elif 'array' in types and 'items' in value:\n            item_types = _get_types(value['items'])\n            # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#objects\n            if any(item_type != 'object' for item_type in item_types):\n                yield (new_path, {'wholeListMerge'})\n            elif 'object' in item_types and 'properties' in value['items']:\n                # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#whole-list-merge\n                if 'id' not in value['items']['properties']:\n                    yield (new_path, {'wholeListMerge'})\n                else:\n                    yield from _get_merge_rules(value['items']['properties'], path=new_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning merge rules as key - value pairs in which the key is a JSON path as a tuple and the value is a list of merge properties whose values are true.", "response": "def get_merge_rules(schema=None):\n    \"\"\"\n    Returns merge rules as key-value pairs, in which the key is a JSON path as a tuple, and the value is a list of\n    merge properties whose values are `true`.\n    \"\"\"\n    schema = schema or get_release_schema_url(get_tags()[-1])\n    if isinstance(schema, dict):\n        deref_schema = jsonref.JsonRef.replace_refs(schema)\n    else:\n        deref_schema = _get_merge_rules_from_url_or_path(schema)\n    return dict(_get_merge_rules(deref_schema['properties']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten(obj, merge_rules=None, path=None, flattened=None):\n    if merge_rules is None:\n        merge_rules = {}\n    if path is None:\n        path = ()\n    if flattened is None:\n        flattened = OrderedDict()\n\n    if isinstance(obj, dict):\n        iterable = obj.items()\n        if not iterable:\n            flattened[path] = OrderedDict()\n    else:\n        iterable = enumerate(obj)\n        if not iterable:\n            flattened[path] = []\n\n    for key, value in iterable:\n        new_path = path + (key,)\n        # Remove array indices to find the merge rule for this JSON path in the data.\n        new_path_merge_rules = merge_rules.get(tuple(part for part in new_path if not isinstance(part, int)), [])\n\n        if 'omitWhenMerged' in new_path_merge_rules:\n            continue\n        # If it is neither an object nor an array, if it is `wholeListMerge`, or if it is an array containing\n        # non-objects (even if `wholeListMerge` is `false`), use the whole list merge strategy.\n        # Note: Behavior is undefined and inconsistent if the array is not in the schema and contains objects in some\n        # cases but not in others.\n        # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#whole-list-merge\n        # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#objects\n        elif (not isinstance(value, (dict, list)) or 'wholeListMerge' in new_path_merge_rules or\n                isinstance(value, list) and any(not isinstance(item, dict) for item in value)):\n            flattened[new_path] = value\n        # Recurse into non-empty objects, and arrays of objects that aren't `wholeListMerge`.\n        elif value:\n            flatten(value, merge_rules, new_path, flattened)\n\n    return flattened", "response": "Flattens a JSON object into key - value pairs in which the key is the JSON path as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unflatten(processed, merge_rules):\n    unflattened = OrderedDict()\n\n    for key in processed:\n        current_node = unflattened\n        for end, part in enumerate(key, 1):\n            # If this is a path to an item of an array.\n            # See http://standard.open-contracting.org/1.1-dev/en/schema/merging/#identifier-merge\n            if isinstance(part, IdValue):\n                # If the `id` of an object in the array matches, change into it.\n                for node in current_node:\n                    if isinstance(node, IdDict) and node.identifier == part.identifier:\n                        current_node = node\n                        break\n                # Otherwise, append a new object, and change into it.\n                else:\n                    new_node = IdDict()\n                    new_node.identifier = part.identifier\n                    # If the original object had an `id` value, set it.\n                    if part.original_value is not None:\n                        new_node['id'] = part.original_value\n                    current_node.append(new_node)\n                    current_node = new_node\n                continue\n\n            # Otherwise, this is a path to a property of an object.\n            node = current_node.get(part)\n\n            # If this is a path to a node we visited before, change into it. If it's an `id` field, it's already been\n            # set to its original value.\n            if node is not None:\n                current_node = node\n                continue\n\n            # If this is a full path, copy the data.\n            if len(key) == end:\n                # Omit null'ed fields.\n                if processed[key] is not None:\n                    current_node[part] = processed[key]\n                continue\n\n            # If the path is to a new array, start a new array, and change into it.\n            if isinstance(key[end], IdValue):\n                new_node = []\n            # If the path is to a new object, start a new object, and change into it.\n            else:\n                new_node = OrderedDict()\n\n            current_node[part] = new_node\n            current_node = new_node\n\n    return unflattened", "response": "Unflattens a processed object into a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess a flattened object into a single object.", "response": "def process_flattened(flattened):\n    \"\"\"\n    Replace numbers in JSON paths (representing positions in arrays) with special objects. This ensures that objects\n    in arrays with different `id` values have different JSON paths \u2013\u00a0and makes it easy to identify such arrays.\n    \"\"\"\n    # Keep arrays in order.\n    processed = OrderedDict()\n\n    # Cache identifiers, to avoid minting a new ID for each field of the same object.\n    identifiers = {}\n\n    for key in flattened:\n        new_key = []\n        for end, part in enumerate(key, 1):\n            # If this is a path to an item in an array.\n            if isinstance(part, int):\n                if key[:end] in identifiers:\n                    part = identifiers[key[:end]]\n                else:\n                    # If it is an array of objects, get the `id` value to apply the identifier merge strategy.\n                    # http://standard.open-contracting.org/latest/en/schema/merging/#identifier-merge\n                    id_value = flattened.get(key[:end] + ('id',))\n\n                    # If the object contained no top-level `id` value, set a unique value.\n                    if id_value is None:\n                        identifier = uuid.uuid4()\n                    else:\n                        identifier = id_value\n\n                    # Save the original value. (If the value is an integer, this avoids coercing it to a string.)\n                    part = IdValue(identifier)\n                    part.original_value = id_value\n\n                    identifiers[key[:end]] = part\n            new_key.append(part)\n        processed[tuple(new_key)] = flattened[key]\n\n    return processed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge(releases, schema=None, merge_rules=None):\n    if not merge_rules:\n        merge_rules = get_merge_rules(schema)\n\n    merged = OrderedDict({('tag',): ['compiled']})\n    for release in sorted(releases, key=lambda release: release['date']):\n        release = release.copy()\n\n        ocid = release['ocid']\n        date = release['date']\n        # Prior to OCDS 1.1.4, `tag` didn't set \"omitWhenMerged\": true.\n        release.pop('tag', None)  # becomes [\"compiled\"]\n\n        flat = flatten(release, merge_rules)\n        processed = process_flattened(flat)\n\n        # Add an `id` and `date`.\n        merged[('id',)] = '{}-{}'.format(ocid, date)\n        merged[('date',)] = date\n\n        # In OCDS 1.0, `ocid` incorrectly sets \"mergeStrategy\": \"ocdsOmit\".\n        merged[('ocid',)] = ocid\n\n        merged.update(processed)\n\n    return unflatten(merged, merge_rules)", "response": "Merges a list of releases into a compiledRelease."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_versioned(releases, schema=None, merge_rules=None):\n    if not merge_rules:\n        merge_rules = get_merge_rules(schema)\n\n    merged = OrderedDict()\n    for release in sorted(releases, key=lambda release: release['date']):\n        release = release.copy()\n\n        # Don't version the OCID.\n        ocid = release.pop('ocid')\n        merged[('ocid',)] = ocid\n\n        releaseID = release['id']\n        date = release['date']\n        # Prior to OCDS 1.1.4, `tag` didn't set \"omitWhenMerged\": true.\n        tag = release.pop('tag', None)\n\n        flat = flatten(release, merge_rules)\n        processed = process_flattened(flat)\n\n        for key, value in processed.items():\n            # If value is unchanged, don't add to history.\n            if key in merged and value == merged[key][-1]['value']:\n                continue\n\n            if key not in merged:\n                merged[key] = []\n\n            merged[key].append(OrderedDict([\n                ('releaseID', releaseID),\n                ('releaseDate', date),\n                ('releaseTag', tag),\n                ('value', value),\n            ]))\n\n    return unflatten(merged, merge_rules)", "response": "Merge a list of releases into a versionedRelease."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef AskFileForSave(message=None, savedFileName=None, version=None, defaultLocation=None, dialogOptionFlags=None, location=None, clientName=None, windowTitle=None, actionButtonLabel=None, cancelButtonLabel=None, preferenceKey=None, popupExtension=None, eventProc=None, fileType=None, fileCreator=None, wanted=None, multiple=None):\n    return psidialogs.ask_file(message=message, save=True)", "response": "A dialog that asks the user for a filename to save to."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ask_folder(message=None, version=None, defaultLocation=None, dialogOptionFlags=None, location=None, clientName=None, windowTitle=None, actionButtonLabel=None, cancelButtonLabel=None, preferenceKey=None, popupExtension=None, eventProc=None, filterProc=None, wanted=None, multiple=None):\n    return psidialogs.ask_folder(message=message, title=windowTitle, ok=actionButtonLabel, cancel=cancelButtonLabel)", "response": "Display a dialog asking the user for a folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef AskString(prompt, default='', id=261, ok=None, cancel=None):\n    return psidialogs.ask_string(message=prompt, default=default, ok=ok, cancel=cancel)", "response": "Original doc : Display a PROMPT string and a text entry field with a DEFAULT string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Message(msg, id=260, ok=None):\n    return psidialogs.message(message=msg, ok=ok)", "response": "Display a MESSAGE string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsplit list into chunks of the given size.", "response": "def chunks(items, size):\n    \"\"\" Split list into chunks of the given size.\n    Original order is preserved.\n\n    Example:\n        > chunks([1,2,3,4,5,6,7,8], 3)\n        [[1, 2, 3], [4, 5, 6], [7, 8]]\n    \"\"\"\n    return [items[i:i+size] for i in range(0, len(items), size)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging-in to the ZoneMinder API.", "response": "def login(self):\n        \"\"\"Login to the ZoneMinder API.\"\"\"\n        _LOGGER.debug(\"Attempting to login to ZoneMinder\")\n\n        login_post = {'view': 'console', 'action': 'login'}\n        if self._username:\n            login_post['username'] = self._username\n        if self._password:\n            login_post['password'] = self._password\n\n        req = requests.post(urljoin(self._server_url, 'index.php'),\n                            data=login_post, verify=self._verify_ssl)\n        self._cookies = req.cookies\n\n        # Login calls returns a 200 response on both failure and success.\n        # The only way to tell if you logged in correctly is to issue an api\n        # call.\n        req = requests.get(\n            urljoin(self._server_url, 'api/host/getVersion.json'),\n            cookies=self._cookies,\n            timeout=ZoneMinder.DEFAULT_TIMEOUT,\n            verify=self._verify_ssl)\n\n        if not req.ok:\n            _LOGGER.error(\"Connection error logging into ZoneMinder\")\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a request to the ZoneMinder API.", "response": "def _zm_request(self, method, api_url, data=None,\n                    timeout=DEFAULT_TIMEOUT) -> dict:\n        \"\"\"Perform a request to the ZoneMinder API.\"\"\"\n        try:\n            # Since the API uses sessions that expire, sometimes we need to\n            # re-auth if the call fails.\n            for _ in range(ZoneMinder.LOGIN_RETRIES):\n                req = requests.request(\n                    method, urljoin(self._server_url, api_url), data=data,\n                    cookies=self._cookies, timeout=timeout,\n                    verify=self._verify_ssl)\n\n                if not req.ok:\n                    self.login()\n                else:\n                    break\n\n            else:\n                _LOGGER.error('Unable to get API response from ZoneMinder')\n\n            try:\n                return req.json()\n            except ValueError:\n                _LOGGER.exception('JSON decode exception caught while'\n                                  'attempting to decode \"%s\"', req.text)\n                return {}\n        except requests.exceptions.ConnectionError:\n            _LOGGER.exception('Unable to connect to ZoneMinder')\n            return {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_monitors(self) -> List[Monitor]:\n        raw_monitors = self._zm_request('get', ZoneMinder.MONITOR_URL)\n        if not raw_monitors:\n            _LOGGER.warning(\"Could not fetch monitors from ZoneMinder\")\n            return []\n\n        monitors = []\n        for raw_result in raw_monitors['monitors']:\n            _LOGGER.debug(\"Initializing camera %s\",\n                          raw_result['Monitor']['Id'])\n            monitors.append(Monitor(self, raw_result))\n\n        return monitors", "response": "Get a list of Monitors from the ZoneMinder API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a list of RunStates from the ZoneMinder API.", "response": "def get_run_states(self) -> List[RunState]:\n        \"\"\"Get a list of RunStates from the ZoneMinder API.\"\"\"\n        raw_states = self.get_state('api/states.json')\n        if not raw_states:\n            _LOGGER.warning(\"Could not fetch runstates from ZoneMinder\")\n            return []\n\n        run_states = []\n        for i in raw_states['states']:\n            raw_state = i['State']\n            _LOGGER.info(\"Initializing runstate %s\", raw_state['Id'])\n            run_states.append(RunState(self, raw_state))\n\n        return run_states"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the name of the active run state from the ZoneMinder API.", "response": "def get_active_state(self) -> Optional[str]:\n        \"\"\"Get the name of the active run state from the ZoneMinder API.\"\"\"\n        for state in self.get_run_states():\n            if state.active:\n                return state.name\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the active state of the ZoneMinder.", "response": "def set_active_state(self, state_name):\n        \"\"\"\n        Set the ZoneMinder run state to the given state name, via ZM API.\n\n        Note that this is a long-running API call; ZoneMinder changes the state\n        of each camera in turn, and this GET does not receive a response until\n        all cameras have been updated. Even on a reasonably powerful machine,\n        this call can take ten (10) or more seconds **per camera**. This method\n        sets a timeout of 120, which should be adequate for most users.\n        \"\"\"\n        _LOGGER.info('Setting ZoneMinder run state to state %s', state_name)\n        return self._zm_request('GET',\n                                'api/states/change/{}.json'.format(state_name),\n                                timeout=120)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_url_with_auth(self, url) -> str:\n        if not self._username:\n            return url\n\n        url += '&user={:s}'.format(self._username)\n\n        if not self._password:\n            return url\n\n        return url + '&pass={:s}'.format(self._password)", "response": "Add the auth credentials to a url ( if needed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_available(self) -> bool:\n        status_response = self.get_state(\n            'api/host/daemonCheck.json'\n        )\n\n        if not status_response:\n            return False\n\n        return status_response.get('result') == 1", "response": "Indicate if this ZoneMinder service is currently available."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_server_url(server_host, server_path) -> str:\n        server_url = urljoin(server_host, server_path)\n        if server_url[-1] == '/':\n            return server_url\n        return '{}/'.format(server_url)", "response": "Build the server url making sure it ends in a trailing slash."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the information for a flexscheduleRule entity.", "response": "def get(self, flex_sched_rule_id):\n        \"\"\"Retrieve the information for a flexscheduleRule entity.\"\"\"\n        path = '/'.join(['flexschedulerule', flex_sched_rule_id])\n        return self.rachio.get(path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing the fetch make push subcommands to github3 api", "response": "def upload_all_books(book_id_start, book_id_end, rdf_library=None):\n    \"\"\" Uses the fetch, make, push subcommands to\n        mirror Project Gutenberg to a github3 api\n    \"\"\"\n\n    # TODO refactor appname into variable\n    logger.info(\n        \"starting a gitberg mass upload: {0} -> {1}\".format(\n            book_id_start, book_id_end\n        )\n    )\n\n    for book_id in range(int(book_id_start), int(book_id_end) + 1):\n        cache = {}\n        errors = 0\n        try:\n            if int(book_id) in missing_pgid:\n                print(u'missing\\t{}'.format(book_id))\n                continue\n            upload_book(book_id, rdf_library=rdf_library, cache=cache)\n        except Exception as e:\n            print(u'error\\t{}'.format(book_id))\n            logger.error(u\"Error processing: {}\\r{}\".format(book_id, e))\n            errors += 1\n            if errors > 10:\n                print('error limit reached!')\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef upload_list(book_id_list, rdf_library=None):\n    with open(book_id_list, 'r') as f:\n        cache = {}\n        for book_id in f:\n            book_id = book_id.strip()\n            try:\n                if int(book_id) in missing_pgid:\n                    print(u'missing\\t{}'.format(book_id))\n                    continue\n                upload_book(book_id, rdf_library=rdf_library, cache=cache)\n            except Exception as e:\n                print(u'error\\t{}'.format(book_id))\n                logger.error(u\"Error processing: {}\\r{}\".format(book_id, e))", "response": "Adds a list of pg books to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate(self):\n        translations = []\n        for lang in settings.LANGUAGES:\n            # do not create an translations for default language.\n            # we will use the original model for this\n            if lang[0] == self._get_default_language():\n                continue\n            # create translations for all fields of each language\n            if self.translatable_slug is not None:\n                if self.translatable_slug not in self.translatable_fields:\n                    self.translatable_fields = self.translatable_fields + (self.translatable_slug,)\n\n            for field in self.translatable_fields:\n                trans, created = Translation.objects.get_or_create(\n                    object_id=self.id,\n                    content_type=ContentType.objects.get_for_model(self),\n                    field=field,\n                    lang=lang[0],\n                )\n                translations.append(trans)\n\n        return translations", "response": "Creates all translations objects for this Translatable instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the complete list of translation objects of a Translatable instance", "response": "def translations_objects(self, lang):\n        \"\"\"\n        Return the complete list of translation objects of a Translatable\n        instance\n\n        @type lang: string\n        @param lang: a string with the name of the language\n\n        @rtype: list of Translation\n        @return: Returns a list of translations objects\n        \"\"\"\n        return Translation.objects.filter(\n            object_id=self.id,\n            content_type=ContentType.objects.get_for_model(self),\n            lang=lang\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the list of translation strings of a Translatable instance in a dictionary form", "response": "def translations(self, lang):\n        \"\"\"\n        Return the list of translation strings of a Translatable\n        instance in a dictionary form\n\n        @type lang: string\n        @param lang: a string with the name of the language\n\n        @rtype: python Dictionary\n        @return: Returns a all fieldname / translations (key / value)\n        \"\"\"\n        key = self._get_translations_cache_key(lang)\n        trans_dict = cache.get(key, {})\n\n        if self.translatable_slug is not None:\n            if self.translatable_slug not in self.translatable_fields:\n                self.translatable_fields = self.translatable_fields + (self.translatable_slug,)\n\n        if not trans_dict:\n            for field in self.translatable_fields:\n                # we use get_translation method to be sure that it will\n                # fall back and get the default value if needed\n                trans_dict[field] = self.get_translation(lang, field)\n\n            cache.set(key, trans_dict)\n        return trans_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_translation_obj(self, lang, field, create=False):\n        trans = None\n        try:\n            trans = Translation.objects.get(\n                object_id=self.id,\n                content_type=ContentType.objects.get_for_model(self),\n                lang=lang,\n                field=field,\n            )\n        except Translation.DoesNotExist:\n            if create:\n                trans = Translation.objects.create(\n                    object_id=self.id,\n                    content_type=ContentType.objects.get_for_model(self),\n                    lang=lang,\n                    field=field,\n                )\n        return trans", "response": "Returns the translation object of an specific field in a Translatable\n        istance\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the translation string of an specific field in a Translatable object", "response": "def get_translation(self, lang, field):\n        \"\"\"\n        Return the translation string of an specific field in a Translatable\n        istance\n\n        @type lang: string\n        @param lang: a string with the name of the language\n\n        @type field: string\n        @param field: a string with the name that we try to get\n\n        @rtype: string\n        @return: Returns a translation string\n        \"\"\"\n        # Read from cache\n        key = self._get_translation_cache_key(lang, field)\n        trans = cache.get(key, '')\n\n        if not trans:\n            trans_obj = self.get_translation_obj(lang, field)\n            trans = getattr(trans_obj, 'translation', '')\n            # if there's no translation text fall back to the model field\n            if not trans:\n                trans = getattr(self, field, '')\n            # update cache\n            cache.set(key, trans)\n        return trans"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the translation string in the specified field for a specific language.", "response": "def set_translation(self, lang, field, text):\n        \"\"\"\n        Store a translation string in the specified field for a Translatable\n        istance\n\n        @type lang: string\n        @param lang: a string with the name of the language\n\n        @type field: string\n        @param field: a string with the name that we try to get\n\n        @type text: string\n        @param text: a string to be stored as translation of the field\n        \"\"\"\n        # Do not allow user to set a translations in the default language\n        auto_slug_obj = None\n\n        if lang == self._get_default_language():\n            raise CanNotTranslate(\n                _('You are not supposed to translate the default language. '\n                  'Use the model fields for translations in default language')\n            )\n\n        # Get translation, if it does not exits create one\n        trans_obj = self.get_translation_obj(lang, field, create=True)\n        trans_obj.translation = text\n        trans_obj.save()\n\n        # check if the field has an autoslugfield and create the translation\n        if INSTALLED_AUTOSLUG:\n            if self.translatable_slug:\n                try:\n                    auto_slug_obj = self._meta.get_field(self.translatable_slug).populate_from\n                except AttributeError:\n                    pass\n\n        if auto_slug_obj:\n            tobj = self.get_translation_obj(lang, self.translatable_slug, create=True)\n            translation = self.get_translation(lang, auto_slug_obj)\n            tobj.translation = slugify(translation)\n            tobj.save()\n\n        # Update cache for this specif translations\n        key = self._get_translation_cache_key(lang, field)\n        cache.set(key, text)\n        # remove cache for translations dict\n        cache.delete(self._get_translations_cache_key(lang))\n        return trans_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef translations_link(self):\n        translation_type = ContentType.objects.get_for_model(Translation)\n        link = urlresolvers.reverse('admin:%s_%s_changelist' % (\n            translation_type.app_label,\n            translation_type.model),\n        )\n\n        object_type = ContentType.objects.get_for_model(self)\n        link += '?content_type__id__exact=%s&object_id=%s' % (object_type.id, self.id)\n        return '<a href=\"%s\">translate</a>' % link", "response": "Print on admin change list the link to see all translations for this object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares old and new object to determine which fields changed how", "response": "def comparison_callback(sender, instance, **kwargs):\n    \"\"\"Comparing old and new object to determin which fields changed how\"\"\"\n    if validate_instance(instance) and settings.AUTOMATED_LOGGING['to_database']:\n        try:\n            old = sender.objects.get(pk=instance.pk)\n        except Exception:\n            return None\n\n        try:\n            mdl = ContentType.objects.get_for_model(instance)\n            cur, ins = old.__dict__, instance.__dict__\n            old, new = {}, {}\n            for k in cur.keys():\n                # _ fields are not real model fields, only state or cache fields\n                # getting filtered\n                if re.match('(_)(.*?)', k):\n                    continue\n\n                changed = False\n                if k in ins.keys():\n                    if cur[k] != ins[k]:\n                        changed = True\n                        new[k] = ModelObject()\n                        new[k].value = str(ins[k])\n                        new[k].save()\n\n                        try:\n                            new[k].type = ContentType.objects.get_for_model(ins[k])\n                        except Exception:\n                            logger = logging.getLogger(__name__)\n                            logger.debug('Could not dermin the content type of the field')\n\n                        new[k].field = Field.objects.get_or_create(name=k, model=mdl)[0]\n                        new[k].save()\n                else:\n                    changed = True\n\n                if changed:\n                    old[k] = ModelObject()\n                    old[k].value = str(cur[k])\n                    old[k].save()\n\n                    try:\n                        old[k].type = ContentType.objects.get_for_model(cur[k])\n                    except Exception:\n                        logger = logging.getLogger(__name__)\n                        logger.debug('Could not dermin the content type of the field')\n\n                    old[k].field = Field.objects.get_or_create(name=k, model=mdl)[0]\n                    old[k].save()\n\n            if old or new:\n                changelog = ModelChangelog()\n                changelog.save()\n\n                changelog.modification = ModelModification()\n                changelog.modification.save()\n                changelog.modification.previously.add(*old.values())\n                changelog.modification.currently.add(*new.values())\n\n                changelog.information = ModelObject()\n                changelog.information.save()\n                changelog.information.value = repr(instance)\n                changelog.information.type = ContentType.objects.get_for_model(instance)\n                changelog.information.save()\n                changelog.save()\n\n                instance.al_chl = changelog\n\n                return instance\n\n        except Exception as e:\n            print(e)\n            logger = logging.getLogger(__name__)\n            logger.warning('automated_logging recorded an exception that should not have happended')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving object & link logging entry", "response": "def save_callback(sender, instance, created, update_fields, **kwargs):\n    \"\"\"Save object & link logging entry\"\"\"\n    if validate_instance(instance):\n        status = 'add' if created is True else 'change'\n        change = ''\n\n        if status == 'change' and 'al_chl' in instance.__dict__.keys():\n            changelog = instance.al_chl.modification\n            change = ' to following changed: {}'.format(changelog)\n\n        processor(status, sender, instance, update_fields, addition=change)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef requires_refcount(cls, func):\n        @functools.wraps(func)\n        def requires_active_handle(*args, **kwargs):\n            if cls.refcount() == 0:\n                raise NoHandleException()  # You probably want to encase your code in a 'with LibZFSHandle():' block...\n            return func(*args, **kwargs)\n        return requires_active_handle", "response": "Decorator to make sure that a function is called with no active handle."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a GPubsubPublisher client.", "response": "def get_gpubsub_publisher(config, metrics, changes_channel, **kw):\n    \"\"\"Get a GPubsubPublisher client.\n\n    A factory function that validates configuration, creates an auth\n    and pubsub API client, and returns a Google Pub/Sub Publisher\n    provider.\n\n    Args:\n        config (dict): Google Cloud Pub/Sub-related configuration.\n        metrics (obj): :interface:`IMetricRelay` implementation.\n        changes_channel (asyncio.Queue): Queue to publish message to\n            make corrections to Cloud DNS.\n        kw (dict): Additional keyword arguments to pass to the\n            Publisher.\n    Returns:\n        A :class:`GPubsubPublisher` instance.\n    \"\"\"\n    builder = gpubsub_publisher.GPubsubPublisherBuilder(\n        config, metrics, changes_channel, **kw)\n    return builder.build_publisher()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a GDNSReconciler client.", "response": "def get_reconciler(config, metrics, rrset_channel, changes_channel, **kw):\n    \"\"\"Get a GDNSReconciler client.\n\n    A factory function that validates configuration, creates an auth\n    and :class:`GDNSClient` instance, and returns a GDNSReconciler\n    provider.\n\n    Args:\n        config (dict): Google Cloud Pub/Sub-related configuration.\n        metrics (obj): :interface:`IMetricRelay` implementation.\n        rrset_channel (asyncio.Queue): Queue from which to consume\n            record set messages to validate.\n        changes_channel (asyncio.Queue): Queue to publish message to\n            make corrections to Cloud DNS.\n        kw (dict): Additional keyword arguments to pass to the\n            Reconciler.\n    Returns:\n        A :class:`GDNSReconciler` instance.\n    \"\"\"\n    builder = reconciler.GDNSReconcilerBuilder(\n        config, metrics, rrset_channel, changes_channel, **kw)\n    return builder.build_reconciler()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_authority(config, metrics, rrset_channel, **kwargs):\n    builder = authority.GCEAuthorityBuilder(\n        config, metrics, rrset_channel, **kwargs)\n    return builder.build_authority()", "response": "Returns a GCEAuthority client."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nenables folder in the git repository", "response": "def enable(self, folder):\n\t\thost_name = self.base.options.ip\n\t\temail = self.base.options.user\n\t\tisHost(host_name)\n\t\tself.__userExists(email)\n\n\t\tdata = self.getHostData(host_name)\n\t\twebsite_dir = data['website_dir']\n\t\tfull_path = '%s/%s' % (website_dir, folder)\n\t\tfull_path_git = '%s/.git' % full_path\n\t\trepository = '%s/%s/%s.git' % (self.data[email]['dir'], host_name, folder)\n\t\treal_repository = '%s/%s.git' % (self.base.git['repositories'], host_name)\n\t\tuser_hook = '%s/hooks/post-receive' % repository\n\t\trepo_hooks = '%s/.git/hooks' % website_dir\n\t\torigin = md5(email)\n\n\t\tif not fileExists(full_path):\n\t\t\terror_message('Folder %s not exists!' % full_path)\n\n\t\tif not host_name in self.data[email]['projects']:\n\t\t\tself.data[email]['projects'][host_name] = []\n\n\t\tif folder in self.data[email]['projects'][host_name]:\n\t\t\terror_message('Repository already exists for this user!')\n\t\tself.data[email]['projects'][host_name].append(folder)\n\n\t\tself.__makeDir(repository)\n\t\tos.system('cd %s && git init --bare 1> /dev/null' % repository)\n\t\tputFile(\n\t\t\t'%s/.gitignore' % full_path,\n\t\t\tgetTemplate('git-jail-gitignore-default')\n\t\t)\n\n\t\tos.system(\n\t\t\t'cd %(full_path)s && git init 1> /dev/null && '\n\t\t\t'git remote add %(origin)s %(repository)s && '\n\t\t\t'git add . && '\n\t\t\t'git commit -am \"Initial commit\" 1> /dev/null && '\n\t\t\t'git push %(origin)s master 1> /dev/null' % locals()\n\t\t)\n\n\t\tos.system('chown git:git -R %s' % full_path)\n\t\tos.system('chown git:git -R %s' % repository)\n\t\tos.system('chown git:git -R %s' % real_repository)\n\t\tos.system('chown git:git -R %s/.git' % website_dir)\n\n\n\t\t'''\n\t\tos.system(\n\t\t\t'cd %s; git rm --cached %s; git add ./%s/*' % (\n\t\t\twebsite_dir, folder, folder\n\t\t))\n\t\t'''\n\n\t\tkey = hash('%(email)s-%(host_name)s-%(folder)s' % locals())\n\n\t\ttemplates = {\n\t\t\tuser_hook: 'git-jail-post-receive-user-repo',\n\t\t\t'%s/post-commit' % repo_hooks: 'git-jail-post-commit-repo',\n\t\t\t'%s/post-receive' % repo_hooks: 'git-jail-post-receive-repo',\n\t\t\t'%s/hooks/post-receive' % real_repository: 'git-jail-post-receive-real-repo',\n\t\t}\n\n\t\tputFile(\n\t\t\t'%s/hooks/post-receive.db' % real_repository,\n\t\t\t'%(website_dir)s;%(full_path)s;.;%(key)s;%(origin)s;' % locals(),\n\t\t\t'a'\n\t\t)\n\n\t\tputFile(\n\t\t\t'%s.db' % user_hook,\n\t\t\t'%(full_path)s;%(website_dir)s;./%(folder)s/*;%(key)s;%(origin)s;' % locals(),\n\t\t\t'a'\n\t\t)\n\n\t\tputFile(\n\t\t\t'%s/post-receive.db' % repo_hooks,\n\t\t\t'%(full_path)s;%(real_repository)s;%(key)s;%(origin)s;' % locals(),\n\t\t\t'a'\n\t\t)\n\n\t\tfor f,t in templates.items():\n\t\t\tputFile(f, getTemplate(t) % locals())\n\t\t\tos.system('chmod +x %s' % f)\n\n\t\tinfo_message('Successful!')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrefresh the oauth access token attached to this HTTP session.", "response": "async def refresh_token(self):\n        \"\"\"Refresh oauth access token attached to this HTTP session.\n\n        Raises:\n            :exc:`.GCPAuthError`: if no token was found in the\n                response.\n            :exc:`.GCPHTTPError`: if any exception occurred,\n                specifically a :exc:`.GCPHTTPResponseError`, if the\n                exception is associated with a response status code.\n        \"\"\"\n        url, headers, body = self._setup_token_request()\n        request_id = uuid.uuid4()\n        logging.debug(_utils.REQ_LOG_FMT.format(\n            request_id=request_id, method='POST', url=url, kwargs=None))\n        async with self._session.post(url, headers=headers, data=body) as resp:\n            log_kw = {\n                'request_id': request_id,\n                'method': 'POST',\n                'url': resp.url,\n                'status': resp.status,\n                'reason': resp.reason,\n            }\n            logging.debug(_utils.RESP_LOG_FMT.format(**log_kw))\n\n            # avoid leaky abstractions and wrap http errors with our own\n            try:\n                resp.raise_for_status()\n            except aiohttp.ClientResponseError as e:\n                msg = f'[{request_id}] Issue connecting to {resp.url}: {e}'\n                logging.error(msg, exc_info=e)\n                raise exceptions.GCPHTTPResponseError(msg, resp.status)\n\n            response = await resp.json()\n            try:\n                self.token = response['access_token']\n            except KeyError:\n                msg = '[{request_id}] No access token in response.'\n                logging.error(msg)\n                raise exceptions.GCPAuthError(msg)\n\n        self.expiry = _client._parse_expiry(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a list of dictionaries to SeabornTable object", "response": "def list_to_obj(cls, list_, columns, row_columns=None, tab='',\n                    key_on=None, no_header=False):\n        \"\"\"\n        :param list_:         list of list or list of dictionary to use as the\n                              source\n        :param columns:       list of strings to label the columns when\n                              converting to str\n        :param row_columns:   list of columns in the actually data\n        :param tab:           str of the tab to use before the row when\n                              converting to str\n        :param key_on:        str of the column to key each row on\n        :param no_header:     bool if false then the first row is the headers\n        :return: SeabornTable\n        \"\"\"\n        if not list_:\n            return cls(columns=columns, row_columns=row_columns, tab=tab,\n                       key_on=key_on)\n        if getattr(list_[0], 'keys', None) and not isinstance(list_[0], dict):\n            row_columns = row_columns or columns or list_[0].keys()\n            column_index = cls._create_column_index(row_columns)\n            table = [SeabornRow(\n                column_index,\n                [getattr(row, col, None) for col in row_columns])\n                for row in list_]\n        elif isinstance(list_[0], dict):\n            row_columns = row_columns or columns or \\\n                          cls._key_on_columns(key_on,\n                                              cls._get_normalized_columns(\n                                                  list_))\n            column_index = cls._create_column_index(row_columns)\n            table = [SeabornRow(column_index,\n                                [row.get(c, None) for c in row_columns])\n                     for row in list_]\n\n        elif isinstance(list_[0], (list, tuple)) and no_header:\n            row_columns = row_columns or columns or \\\n                          cls._key_on_columns(key_on, [\n                              'Column %s' % i for i in range(len(list_[0]))])\n            column_index = cls._create_column_index(row_columns)\n            table = [SeabornRow(column_index, row) for row in list_]\n\n        elif isinstance(list_[0], (list, tuple)):\n            row_columns = row_columns or list_[0]\n            if list_[0] == row_columns:\n                list_ = list_[1:]\n            column_index = cls._create_column_index(row_columns)\n            size = len(row_columns)\n            table = [SeabornRow(column_index, row + [None] * (size - len(row)))\n                     for row in list_]\n        else:\n            column_index = cls._create_column_index(columns or [])\n            table = [SeabornRow(column_index, [row]) for row in list_]\n\n        return cls(table, columns, row_columns, tab, key_on)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dict_to_obj(cls, dict_, columns, row_columns, tab='', key_on=None):\n        if isinstance(list(dict_.values())[0], dict):\n            row_columns = row_columns or columns or cls._key_on_columns(\n                key_on, cls._ordered_keys(dict_.values()[0]))\n            column_index = cls._create_column_index(row_columns)\n            if key_on is None:\n                table = [\n                    SeabornRow(column_index, [row[c] for c in row_columns])\n                    for row in dict_.values()]\n            else:\n                table = [SeabornRow(column_index,\n                                    [row.get(c, c == key_on and key or None)\n                                     for c in row_columns])\n                         for key, row in dict_.items()]\n\n        elif isinstance(list(dict_.values())[0], list):\n            row_columns = row_columns or columns or \\\n                          cls._key_on_columns(key_on, sorted(dict_.keys()))\n            column_index = cls._create_column_index(row_columns)\n            if key_on is None:\n                table = [\n                    SeabornRow(column_index, [dict_[c][i] for c in columns])\n                    for i in range(len(dict_[columns[0]]))]\n            else:\n                table = [\n                    SeabornRow(column_index, [dict_[c][i] for c in columns])\n                    for i in range(len(dict_[columns[0]]))]\n\n        else:\n            row_columns = row_columns or columns or ['KEY', 'VALUE']\n            column_index = cls._create_column_index(row_columns)\n            table = [SeabornRow(column_index, [k, v]) for k, v in\n                     dict_.items()]\n\n        return cls(table, columns, row_columns, tab, key_on)", "response": "Convert a dictionary of Seaborn SeabornRecord objects into SeabornTable objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef csv_to_obj(cls, file_path=None, text='', columns=None,\n                   remove_empty_rows=True, key_on=None, deliminator=',',\n                   eval_cells=True):\n        \"\"\"\n        This will convert a csv file or csv text into a seaborn table\n        and return it\n        :param file_path: str of the path to the file\n        :param text: str of the csv text\n        :param columns: list of str of columns to use\n        :param remove_empty_rows: bool if True will remove empty rows\n                which can happen in non-trimmed file\n        :param key_on: list of str of columns to key on\n        :param deliminator: str to use as a deliminator, defaults to ,\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        lines = cls._get_lines(file_path, text, replace=u'\\ufeff')\n        for i in range(len(lines)):\n            lines[i] = lines[i].replace('\\r', '\\n')\n            lines[i] = lines[i].replace('\\\\r', '\\r').split(',')\n        data = cls._merge_quoted_cells(lines, deliminator, remove_empty_rows,\n                                       eval_cells)\n        row_columns = data[0]\n        if len(row_columns) != len(set(row_columns)):  # make unique\n            for i, col in enumerate(row_columns):\n                count = row_columns[:i].count(col)\n                row_columns[i] = '%s_%s' % (col, count) if count else col\n\n        return cls.list_to_obj(data[1:], columns=columns,\n                               row_columns=row_columns, key_on=key_on)", "response": "This will convert a csv file or csv text into a Seaborn table object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grid_to_obj(cls, file_path=None, text='', edges=None,\n                    columns=None, eval_cells=True, key_on=None):\n        \"\"\"\n        This will convert a grid file or grid text into a seaborn table\n        and return it\n        :param file_path:   str of the path to the file\n        :param text:        str of the grid text\n        :param columns:     list of str of columns to use\n        :param key_on:      list of str of columns to key on\n        :param eval_cells:  bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        edges = edges if edges else cls.FANCY\n        lines = cls._get_lines(file_path, text)\n        data = []\n        for i in range(len(lines)-1):\n            if i % 2 == 1:\n                row = lines[i].split(edges['internal vertical edge'])[1:-1]\n                data.append([cls._eval_cell(r, _eval=eval_cells) for r in row])\n\n        # todo should be refactored to list_to_obj\n        row_columns = data[0]\n        if len(row_columns) != len(set(row_columns)):  # make unique\n            for i, col in enumerate(row_columns):\n                count = row_columns[:i].count(col)\n                row_columns[i] = '%s_%s' % (col, count) if count else col\n        return cls.list_to_obj(data[1:], columns=columns,\n                               row_columns=row_columns, key_on=key_on)", "response": "This will convert a grid file or text into a Seaborn table object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef txt_to_obj(cls, file_path=None, text='', columns=None,\n                   remove_empty_rows=True, key_on=None,\n                   row_columns=None, deliminator='\\t', eval_cells=True):\n        \"\"\"\n        This will convert text file or text to a seaborn table\n        and return it\n        :param file_path: str of the path to the file\n        :param text: str of the csv text\n        :param columns: list of str of columns to use\n        :param row_columns: list of str of columns in data but not to use\n        :param remove_empty_rows: bool if True will remove empty rows\n        :param key_on: list of str of columns to key on\n        :param deliminator: str to use as a deliminator\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        return cls.str_to_obj(file_path=file_path, text=text, columns=columns,\n                              remove_empty_rows=remove_empty_rows,\n                              key_on=key_on, row_columns=row_columns,\n                              deliminator=deliminator, eval_cells=eval_cells)", "response": "This will convert a text file or text to a Seaborn table object and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef str_to_obj(cls, file_path=None, text='', columns=None,\n                   remove_empty_rows=True, key_on=None,\n                   row_columns=None, deliminator='\\t', eval_cells=True):\n        \"\"\"\n        This will convert text file or text to a seaborn table\n        and return it\n        :param file_path: str of the path to the file\n        :param text: str of the csv text\n        :param columns: list of str of columns to use\n        :param row_columns: list of str of columns in data but not to use\n        :param remove_empty_rows: bool if True will remove empty rows\n        :param key_on: list of str of columns to key on\n        :param deliminator: str to use as a deliminator\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        text = cls._get_lines(file_path, text)\n        if len(text) == 1:\n            text = text[0].split('\\r')\n\n        list_of_list = [[cls._eval_cell(cell, _eval=eval_cells)\n                         for cell in row.split(deliminator)]\n                        for row in text if not remove_empty_rows or\n                        True in [bool(r) for r in row]]\n\n        if list_of_list[0][0] == '' and list_of_list[0][-1] == '':\n            list_of_list = [row[1:-1] for row in list_of_list]\n        return cls.list_to_obj(list_of_list, key_on=key_on, columns=columns,\n                               row_columns=row_columns)", "response": "This will convert a text file or text to a Seaborn table object and return it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef psql_to_obj(cls, file_path=None, text='', columns=None,\n                    remove_empty_rows=True, key_on=None,\n                    deliminator=' | ', eval_cells=True):\n        \"\"\"\n        This will convert a psql file or text to a seaborn table\n        :param file_path: str of the path to the file\n        :param text: str of the csv text\n        :param columns: list of str of columns to use\n        :param remove_empty_rows: bool if True will remove empty rows\n        :param key_on: list of str of columns to key on\n        :param deliminator: str to use as a deliminator\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        text = cls._get_lines(file_path, text)\n        if len(text) == 1:\n            text = text[0].split('\\r')\n\n        if not text[1].replace('+', '').replace('-', '').strip():\n            text.pop(1)  # get rid of bar\n\n        list_of_list = [[cls._eval_cell(cell, _eval=eval_cells)\n                         for cell in row.split(deliminator)]\n                        for row in text if not remove_empty_rows or\n                        True in [bool(r) for r in row]]\n\n        return cls.list_to_obj(list_of_list, key_on=key_on, columns=columns)", "response": "This will convert a psql file or text to a Seaborn table object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef html_to_obj(cls, file_path=None, text='', columns=None,\n                    key_on=None, ignore_code_blocks=True, eval_cells=True):\n        \"\"\"\n        This will convert a html file or text to a seaborn table\n        :param file_path: str of the path to the file\n        :param text: str of the csv text\n        :param columns: list of str of columns to use\n        :param remove_empty_rows: bool if True will remove empty rows\n        :param key_on: list of str of columns to key on\n        :param deliminator: str to use as a deliminator\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: SeabornTable\n        \"\"\"\n        raise NotImplemented", "response": "This will convert a html file or text to a seaborn table object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mark_down_to_dict_of_obj(cls, file_path=None, text='', columns=None,\n                                 key_on=None, eval_cells=True):\n        \"\"\"\n        This will read multiple tables separated by a #### Header\n        and return it as a dictionary of headers\n        :param file_path: str of the path to the file\n        :param text: str of the mark down text\n        :param columns: list of str of columns to use\n        :param key_on: list of str of columns to key on\n        :param eval_cells: bool if True will try to evaluate numbers\n        :return: OrderedDict of {<header>: SeabornTable}\n        \"\"\"\n        text = cls._get_lines(file_path, text, split_lines=False)\n        ret = OrderedDict()\n        paragraphs = text.split('####')\n        for paragraph in paragraphs[1:]:\n            header, text = paragraph.split('\\n', 1)\n            ret[header.strip()] = cls.mark_down_to_obj(\n                text=text, columns=columns, key_on=key_on,\n                eval_cells=eval_cells)\n        return ret", "response": "This will read multiple tables separated by a #### Header\n        and return a dictionary of headers and SeabornTable objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef objs_to_mark_down(cls, tables, file_path=None, keys=None,\n                          pretty_columns=True, quote_numbers=True):\n        \"\"\"\n        This will return a str of multiple mark down tables.\n        :param tables:         dict of {str <name>:SeabornTable}\n        :param file_path:      str of the path to the file\n        :param keys:           list of str of the order of keys to use\n        :param pretty_columns: bool if True will make the columns pretty\n        :param quote_numbers:  bool if True will quote numbers that are strings\n        :return:               str of the converted markdown tables\n        \"\"\"\n        keys = keys or tables.keys()\n        ret = ['#### ' + key + '\\n' + tables[key].obj_to_mark_down(\n            pretty_columns=pretty_columns, quote_numbers=quote_numbers)\n               for key in keys]\n        ret = '\\n\\n'.join(ret)\n        cls._save_file(file_path, ret)\n        return ret", "response": "This will return a str of multiple mark down tables."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef obj_to_json(self, file_path=None, indent=2, sort_keys=False,\n                    quote_numbers=True):\n        \"\"\"\n        This will return a str of a json list.\n        :param file_path:      path to data file, defaults to\n                               self's contents if left alone\n        :param indent:         int if set to 2 will indent to spaces and include\n                               line breaks.\n        :param sort_keys:      sorts columns as oppose to column order.\n        :param quote_numbers:  bool if True will quote numbers that are strings\n        :return:               string representing the grid formation\n                               of the relevant data\n        \"\"\"\n        data = [row.obj_to_ordered_dict(self.columns) for row in self]\n\n        if not quote_numbers:\n            for row in data:\n                for k, v in row.items():\n                    if isinstance(v, (bool, int, float)):\n                        row[k] = str(row[k])\n        ret = json.dumps(data, indent=indent, sort_keys=sort_keys)\n        if sys.version_info[0] == 2:\n            ret = ret.replace(', \\n', ',\\n')\n        self._save_file(file_path, ret)\n        return ret", "response": "This will return a json list of the relevant data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef obj_to_grid(self, file_path=None, delim=None, tab=None,\n                    quote_numbers=True, quote_empty_str=False):\n        \"\"\"\n        This will return a str of a grid table.\n        :param file_path:       path to data file, defaults to\n                                self's contents if left alone\n        :param delim:           dict of deliminators, defaults to\n                                obj_to_str's method:\n        :param tab:             string of offset of the table\n        :param quote_numbers:   bool if True will quote numbers that are strings\n        :param quote_empty_str: bool if True will quote empty strings\n        :return:                string representing the grid formation\n                                of the relevant data\n        \"\"\"\n\n        div_delims = {\"top\": ['top left corner', 'top intersect',\n                              'top edge', 'top right corner'],\n                      \"divide\": ['left major intersect',\n                                 'internal major intersect',\n                                 'bottom edge', 'right major intersect'],\n                      \"middle\": ['left intersect', 'internal intersect',\n                                 'internal horizontal edge', 'right intersect'],\n                      \"bottom\": ['bottom left intersect', 'bottom intersect',\n                                 'bottom edge', 'bottom right corner']}\n        delim = delim if delim else {}\n        for tag in self.FANCY.keys():\n            delim[tag] = delim[tag] if tag in delim.keys() \\\n                else self.FANCY[tag]\n\n        tab = self.tab if tab is None else tab\n        list_of_list, column_widths = self.get_data_and_shared_column_widths(\n            data_kwargs=dict(quote_numbers=quote_numbers,\n                             quote_empty_str=quote_empty_str),\n            width_kwargs=dict(padding=0, pad_last_column=True))\n\n        ret = [[cell.ljust(column_widths[i]) for i, cell in enumerate(row)]\n               for row in list_of_list]\n        grid_row = {}\n\n        for key in div_delims.keys():\n            draw = div_delims[key]\n            grid_row[key] = delim[draw[0]]\n            grid_row[key] += delim[draw[1]].join(\n                [delim[draw[2]] * width\n                 for width in column_widths])\n            grid_row[key] += delim[draw[3]]\n\n        ret = [delim['left edge'] + delim['internal vertical edge'].join(row) +\n               delim['right edge'] for row in ret]\n        header = [grid_row[\"top\"], ret[0], grid_row[\"divide\"]]\n        body = [[row, grid_row[\"middle\"]] for row in ret[1:]]\n        body = [item for pair in body for item in pair][:-1]\n        ret = header + body + [grid_row[\"bottom\"]]\n        ret = tab + (u'\\n' + tab).join(ret)\n        self._save_file(file_path, ret)\n        return ret", "response": "This method will return a string representing the relevant data in a table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obj_to_csv(self, file_path=None, quote_everything=False,\n                   space_columns=True, quote_numbers=True):\n        \"\"\"\n        This will return a str of a csv text that is friendly to excel\n        :param file_path:        str to the path\n        :param quote_everything: bool if True will quote everything if it needs\n                                 it or not, this is so it looks pretty in excel.\n        :param quote_numbers:    bool if True will quote numbers that are\n                                 strings\n        :param space_columns:    bool if True it will align columns with spaces\n        :return: str\n        \"\"\"\n        list_of_list, column_widths = self.get_data_and_shared_column_widths(\n            data_kwargs=dict(quote_numbers=quote_numbers,\n                             quote_everything=quote_everything,\n                             safe_str=self._excel_cell),\n            width_kwargs=dict(padding=0))\n        if space_columns:\n            csv = [','.join([cell.ljust(column_widths[i])\n                             for i, cell in enumerate(row)])\n                   for row in list_of_list]\n        else:\n            csv = [','.join(row) for row in list_of_list]\n\n        if os.name == 'posix':\n            ret = '\\r\\n'.join(csv)\n        else:\n            ret = '\\n'.join(csv)\n\n        self._save_file(file_path, ret)\n        return ret", "response": "This will return a str of a csv text that is friendly to excel"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef obj_to_html(self, file_path=None, tab='', border=1, cell_padding=5,\n                    cell_spacing=1, border_color='black', align='center',\n                    row_span=None, quote_numbers=True, quote_empty_str=False):\n        \"\"\"\n        This will return a str of an html table.\n        :param file_path:       str for path to the file\n        :param tab:             str to insert before each line e.g. '    '\n        :param border:          int of the thickness of the table lines\n        :param cell_padding:    int of the padding for the cells\n        :param cell_spacing:    int of the spacing for hte cells\n        :param border_color:    str of the color for the border\n        :param align:           str for cell alignment, center, left, right\n        :param row_span:        list of rows to span\n        :param quote_numbers:   bool if True will quote numbers that are strings\n        :param quote_empty_str: bool if True will quote empty strings\n        :return: str of html code\n        \"\"\"\n        html_table = self._html_link_cells()\n        html_table._html_row_respan(row_span)\n        data = [self._html_row(html_table.columns, tab + '  ', '#bcbcbc',\n                               align=align, quote_numbers=quote_numbers)]\n        for i, row in enumerate(html_table):\n            color = '#dfe7f2' if i % 2 else None\n            row = [row[c] for c in html_table.columns]\n            data.append(self._html_row(row, tab + '  ', color, align=align,\n                                       quote_numbers=quote_numbers))\n\n        ret = '''\n            <table border=\"%s\" cellpadding=\"%s\" cellspacing=\"%s\"\n                   bordercolor=\"%s\" >\n              %s\n            </table>'''.strip().replace('\\n            ', '\\n')\n\n        data = ('\\n%s  ' % tab).join(data)\n        ret = (ret % (border, cell_padding, cell_spacing, border_color, data)\n               ).replace('\\n', '\\n%s' % tab)\n        self._save_file(file_path, ret)\n        return ret", "response": "This function will return a html table of the object that is in the format of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef share_column_widths(self, tables, shared_limit=None):\n        for table in tables:\n            record = (table, shared_limit)\n            if not record in self.shared_tables and table is not self:\n                self.shared_tables.append(record)", "response": "Share column widths of Seaborn tables with this table"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the value of the _key_on attribute of the object.", "response": "def key_on(self, value):\n        \"\"\"\n        :param value: str of which column to key the rows on like a dictionary\n        :return: None\n        \"\"\"\n        if isinstance(value, BASESTRING):\n            value = (value,)\n        self._key_on = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef map(self, func):\n        for row in self.table:\n            for i, cell in enumerate(row):\n                row[i] = func(cell)", "response": "This will replace every cell in the function with func ( cell )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef naming_convention_columns(self, convention='underscore',\n                                  remove_empty=True):\n        \"\"\"\n        This will change the column names to a particular naming convention.\n            underscore: lower case all letters and replaces spaces with _\n            title: uppercase first letter and replaces _ with spaces\n        :param convention: str enum of \"lowercase_underscore\"\n        :param remove_empty: bool if true will remove column header of value ''\n        :return: None\n        \"\"\"\n        converter = getattr(self, '_%s_column' % convention, None)\n        assert converter is not None, \\\n            'Convention \"%s\" is not a valid convention' % convention\n        self.row_columns = [converter(col) for col in self.row_columns]\n        self._columns = [converter(col) for col in self._columns]\n        if remove_empty and '' in self.row_columns:\n            self.remove_column('')", "response": "This will change the column names to a particular naming convention."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves the column with the given key from every row in the table.", "response": "def remove_column(self, key):\n        \"\"\"\n        :param key: str of the column to remove from every row in the table\n        :return: None\n        \"\"\"\n        if isinstance(key, int):\n            index = key\n            key = self.row_columns[key]\n        else:\n            index = self._column_index[key]\n        for row in self.table:\n            row.pop(index)\n        self.row_columns = self.row_columns[:index] + self.row_columns[\n                                                      index + 1:]\n        self.pop_column(key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_by(self, **kwargs):\n        ret = self.__class__(\n            columns=self.columns, row_columns=self.row_columns, tab=self.tab,\n            key_on=self.key_on)\n        for row in self:\n            if False not in [row[k] == v for k, v in kwargs.items()]:\n                ret.append(row)\n        return ret", "response": "Returns a new SeabornTable with only the rows that match the given kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new SeabornTable with only the rows that match the condition and the value.", "response": "def filter(self, column, condition='!=', value=None):\n        \"\"\"\n        :param column: str or index of the column\n        :param condition: str of the python operator\n        :param value: obj of the value to test for\n        :return: SeabornTable\n        \"\"\"\n        ret = self.__class__(\n            columns=self.columns, row_columns=self.row_columns, tab=self.tab,\n            key_on=self.key_on)\n        for row in self:\n            if getattr(row[column], condition, None):\n                if eval('row[column].%s(%s)' % (condition, value)):\n                    ret.append(row)\n            if eval('row[column] %s value' % condition):\n                ret.append(row)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef append(self, row=None):\n        self.table.append(self._normalize_row(row))\n        return self.table[-1]", "response": "This will add a row to the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pop_empty_columns(self, empty=None):\n        empty = ['', None] if empty is None else empty\n        if len(self) == 0:\n            return\n        for col in list(self.columns):\n            if self[0][col] in empty:\n                if not [v for v in self.get_column(col) if v not in empty]:\n                    self.pop_column(col)", "response": "This will pop columns from the printed columns if they only contain\n            '' or None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_by_key(self, keys=None):\n        keys = keys or self.key_on\n        keys = keys if isinstance(keys, (list, tuple)) else [keys]\n        for key in reversed(keys):\n            reverse, key = (True, key[1:]) if key[0] == '-' else (False, key)\n            self.table.sort(key=lambda row: row[key], reverse=reverse)", "response": "Sort the table by the given keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_data_and_column_widths(self, data_kwargs, width_kwargs):\n        data_kwargs = data_kwargs.copy()\n        safe_str = data_kwargs.pop('safe_str', self._safe_str)\n        list_of_list = [[safe_str(col, _is_header=True, **data_kwargs)\n                         for col in self.columns]]\n        list_of_list+= [[safe_str(row[col], **data_kwargs)\n                         for col in self.columns] for row in self]\n        column_widths = self._get_column_widths(list_of_list, **width_kwargs)\n        return list_of_list, column_widths", "response": "Returns a tuple of data and column widths for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a string to a unicode string.", "response": "def _safe_str(cls, cell, quote_numbers=True, repr_line_break=False,\n                  deliminator=None, quote_empty_str=False,\n                  title_columns=False, _is_header=False):\n        \"\"\"\n        :param cell: obj to turn in to a string\n        :param quote_numbers:  bool if True will quote numbers that are strings\n        :param repr_line_break: if True will replace \\n with \\\\n\n        :param deliminator: if the deliminator is in the cell it will be quoted\n        :param quote_empty_str: bool if True will quote empty strings\n        \"\"\"\n        if cell is None:\n            cell = ''\n\n        ret = str(cell) if not isinstance(cell, BASESTRING) else cell\n        if isinstance(cell, BASESTRING):\n            if title_columns and _is_header:\n                ret = cls._title_column(ret)\n            if quote_numbers and (ret.replace(u'.', u'').isdigit() or\n                                          ret in [u'False', u'True', 'False',\n                                                  'True']):\n                ret = u'\"%s\"' % ret\n            elif u'\"' in ret or (deliminator and deliminator in ret):\n                ret = u'\"%s\"' % ret\n            elif quote_empty_str and cell == u'':\n                ret = u'\"\"'\n        if repr_line_break:\n            ret = ret.replace(u'\\n', u'\\\\n')\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_normalized_columns(cls, list_):\n        ret = []\n        for row in list_:\n            if len(row.keys()) > len(ret):\n                ret = cls._ordered_keys(row)\n\n        for row in list_:\n            for key in row.keys():\n                if key not in ret:\n                    ret.append(key)\n                    if not isinstance(row, OrderedDict):\n                        ret.sort()\n        return ret", "response": "returns a list of all the columns that are not in the list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ordered_keys(dict_):\n        return isinstance(dict_, OrderedDict) and dict_.keys() or \\\n               dict_ and sorted(dict_.keys()) or []", "response": "Returns a list of str of keys in the original order\n            or in alphabetical order\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _key_on_columns(key_on, columns):\n        if key_on is not None:\n            if key_on in columns:\n                columns.remove(key_on)\n            columns = [key_on] + columns\n        return columns", "response": "Return a list of columns where the key_on is in the front of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _index_iterator(column_size, max_size, mix_index=False):\n        # todo implement a proper partial factorial design\n        indexes = [0] * len(column_size)\n\n        index_order = [0]\n        if mix_index:\n            for i in range(1, max(column_size)):\n                index_order += [-1 * i, i]\n        else:\n            index_order += range(1, max(column_size))\n\n        for i in range(max_size):\n            yield [index_order[indexes[i]] for i in range(len(indexes))]\n\n            for index in range(len(column_size)):\n                indexes[index] += 1\n                if indexes[index] < column_size[index]:\n                    break\n\n                indexes[index] = 0\n                if index == len(column_size) - 1:\n                    if sys.version_info[0] == 2:\n                        raise StopIteration()\n                    else:\n                        return", "response": "This will iterate over the indexes and return a list of indexes that are in the given column size."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _html_link_cells(self):\n        new_table = self.copy()\n        for row in new_table:\n            for c in new_table.columns:\n                link = '%s <Link>' % c\n                if row.get(link, None):\n                    row[c] = '<a href=\"%s\">%s</a>' % (row[link], row[c])\n\n        new_table.columns = [c for c in self.columns if '<Link>' not in c]\n        return new_table", "response": "This will return a new table with the columns that have <Link > in the name\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _column_width(self, index=None, name=None, max_width=300, **kwargs):\n        assert name is not None or index is not None\n        if name and name not in self._column_index:\n            return min(max_width, name)\n\n        if index is not None:\n            name = self.columns[index]\n        else:\n            index = self._column_index[name]\n\n        values_width = [len(name)]\n        if isinstance(self._parameters.get(name, None), list):\n            values_width += [len(self._safe_str(p, **kwargs))\n                             for p in self._parameters[name]]\n\n        values_width += [len(self._safe_str(row[index], **kwargs))\n                         for row in self.table]\n\n        ret = max(values_width)\n        return min(max_width, ret) if max_width else ret", "response": "Returns the width of the column."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, dict_):\n        for key, value in dict_.items():\n            index = self.column_index.get(key, None)\n            if index is not None:\n                list.__setitem__(self, index, value)", "response": "This will update the row values if the columns exist in the dict_."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the information for a device entity.", "response": "def get(self, dev_id):\n        \"\"\"Retrieve the information for a device entity.\"\"\"\n        path = '/'.join(['device', dev_id])\n        return self.rachio.get(path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getEvent(self, dev_id, starttime, endtime):\n        path = 'device/%s/event?startTime=%s&endTime=%s' % \\\n            (dev_id, starttime, endtime)\n        return self.rachio.get(path)", "response": "Retrieve events for a device entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves current and predicted forecast.", "response": "def getForecast(self, dev_id, units):\n        \"\"\"Retrieve current and predicted forecast.\"\"\"\n        assert units in ['US', 'METRIC'], 'units must be either US or METRIC'\n        path = 'device/%s/forecast?units=%s' % (dev_id, units)\n        return self.rachio.get(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops all watering on the device.", "response": "def stopWater(self, dev_id):\n        \"\"\"Stop all watering on device.\"\"\"\n        path = 'device/stop_water'\n        payload = {'id': dev_id}\n        return self.rachio.put(path, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn ON all features of the device.", "response": "def on(self, dev_id):\n        \"\"\"Turn ON all features of the device.\n\n        schedules, weather intelligence, water budget, etc.\n        \"\"\"\n        path = 'device/on'\n        payload = {'id': dev_id}\n        return self.rachio.put(path, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nturning OFF all features of the device.", "response": "def off(self, dev_id):\n        \"\"\"Turn OFF all features of the device.\n\n        schedules, weather intelligence, water budget, etc.\n        \"\"\"\n        path = 'device/off'\n        payload = {'id': dev_id}\n        return self.rachio.put(path, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_wallet(self, master_secret=b\"\"):\n        master_secret = deserialize.bytes_str(master_secret)\n        bip32node = control.create_wallet(self.testnet,\n                                          master_secret=master_secret)\n        return bip32node.hwif(as_private=True)", "response": "Create a BIP0032 - style hierarchical wallet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate new private key and return in wif format.", "response": "def create_key(self, master_secret=b\"\"):\n        \"\"\"Create new private key and return in wif format.\n\n        @param: master_secret Create from master secret, otherwise random.\n        \"\"\"\n        master_secret = deserialize.bytes_str(master_secret)\n        bip32node = control.create_wallet(self.testnet,\n                                          master_secret=master_secret)\n        return bip32node.wif()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_tx(self, txins=None, txouts=None, lock_time=0):\n        txins = [] if txins is None else txins\n        txouts = [] if txouts is None else txouts\n        lock_time = deserialize.positive_integer(lock_time)\n        txins = deserialize.txins(txins)\n        txouts = deserialize.txouts(self.testnet, txouts)\n        tx = control.create_tx(self.service, self.testnet, txins, txouts,\n                               lock_time=lock_time)\n        return serialize.tx(tx)", "response": "Create unsigned rawtx with given txins and txouts as json data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, wifs, txouts, change_address=None, lock_time=0, fee=10000):\n        # FIXME test!!\n        rawtx = self.create_tx(txouts=txouts, lock_time=lock_time)\n        rawtx = self.add_inputs(rawtx, wifs, change_address=change_address,\n                                fee=fee)\n        return self.publish(rawtx)", "response": "Send a set of txouts to the blockchain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_inputs(self, rawtx, wifs, change_address=None, fee=10000,\n                   dont_sign=False):\n        \"\"\"Add sufficient inputs from given <wifs> to cover <rawtx> outputs\n        and <fee>. If no <change_address> is given, change will be sent to\n        first wif.\n        \"\"\"\n        tx = deserialize.tx(rawtx)\n        keys = deserialize.keys(self.testnet, wifs)\n        fee = deserialize.positive_integer(fee)\n        if change_address is not None:\n            change_address = deserialize.address(self.testnet, change_address)\n        tx = control.add_inputs(self.service, self.testnet, tx, keys,\n                                change_address=change_address, fee=fee)\n\n        if not dont_sign:\n            tx = control.sign_tx(self.service, self.testnet, tx, keys)\n\n        return serialize.tx(tx)", "response": "Add sufficient inputs from given rawtx to cover outputs\n        and fee."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign_tx(self, rawtx, wifs):\n        tx = deserialize.tx(rawtx)\n        keys = deserialize.keys(self.testnet, wifs)\n        tx = control.sign_tx(self.service, self.testnet, tx, keys)\n        return serialize.tx(tx)", "response": "Sign rawtx with given wifs as json data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve_tx(self, txid):\n        txid = deserialize.txid(txid)\n        tx = self.service.get_tx(txid)\n        return serialize.tx(tx)", "response": "Returns rawtx for txid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_utxos(self, addresses):\n        addresses = deserialize.addresses(self.testnet, addresses)\n        spendables = control.retrieve_utxos(self.service, addresses)\n        return serialize.utxos(spendables)", "response": "Retrieve current utxos for addresses."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publish(self, rawtx):\n        tx = deserialize.signedtx(rawtx)\n        if not self.dryrun:\n            self.service.send_tx(tx)\n        return serialize.txid(tx.hash())", "response": "Publish signed tx to bitcoin network."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsign data with private key.", "response": "def sign_data(self, wif, hexdata):\n        \"\"\"Signing <hexdata> with <wif> private key.\"\"\"\n        data = deserialize.binary(hexdata)\n        key = deserialize.key(self.testnet, wif)\n        sigdata = control.sign_data(self.testnet, data, key)\n        return serialize.signature(sigdata)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_signature(self, address, signature, hexdata):\n        try:\n            address = deserialize.address(self.testnet, address)\n            data = deserialize.binary(hexdata)\n            signature = deserialize.signature(signature)\n            return control.verify_signature(self.testnet, address,\n                                            signature, data)\n        except exceptions.InvalidAddress:\n            return False", "response": "Verify the signature of a given hexdata by the specified address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sign_unicode(self, wif, message):\n        hexdata = binascii.hexlify(message.encode(\"utf-8\"))\n        return self.sign_data(wif, hexdata)", "response": "Signs a message with a private key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_signature_unicode(self, address, signature, message):\n        hexdata = binascii.hexlify(message.encode(\"utf-8\"))\n        return self.verify_signature(address, signature, hexdata)", "response": "Verify the signature of a message by an unicode address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_hash160data(self, rawtx, hexdata, dust_limit=common.DUST_LIMIT):\n        tx = deserialize.unsignedtx(rawtx)\n        dust_limit = deserialize.positive_integer(dust_limit)\n        hash160data_txout = deserialize.hash160data_txout(hexdata, dust_limit)\n        tx = control.add_hash160data_output(tx, hash160data_txout)\n        return serialize.tx(tx)", "response": "Writes hexdata as new Pay - to - PubkeyHash output to rawtx."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_hash160data(self, rawtx, output_index):\n        tx = deserialize.unsignedtx(rawtx)\n        output_index = deserialize.positive_integer(output_index)\n        data = control.get_hash160_data(tx, output_index)\n        return serialize.data(data)", "response": "Get the data for a hash160 transaction"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the hash160 data for a transaction.", "response": "def retrieve_hash160data(self, txid, output_index):\n        \"\"\"TODO doc string\"\"\"\n        rawtx = self.retrieve_tx(txid)\n        return self.get_hash160_data(rawtx, output_index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites <hexdata > as new nulldata output to rawtx.", "response": "def add_nulldata(self, rawtx, hexdata):\n        \"\"\"Writes <hexdata> as new nulldata output to <rawtx>.\"\"\"\n        tx = deserialize.unsignedtx(rawtx)\n        nulldata_txout = deserialize.nulldata_txout(hexdata)\n        tx = control.add_nulldata_output(tx, nulldata_txout)\n        return serialize.tx(tx)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nulldata(self, rawtx):\n        tx = deserialize.tx(rawtx)\n        index, data = control.get_nulldata(tx)\n        return serialize.data(data)", "response": "Returns nulldata from rawtx as hexdata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_nulldata(self, hexdata, wifs, change_address=None, txouts=None,\n                       fee=10000, lock_time=0):\n        \"\"\"Store <hexdata> in blockchain and return new txid.\n        Utxos taken from <wifs> and change sent to <change_address>.\n        <wifs>: '[\"privatekey_in_wif_format\", ...]'\n        \"\"\"\n        rawtx = self.create_tx(txouts=txouts, lock_time=lock_time)\n        rawtx = self.add_nulldata(rawtx, hexdata)\n        rawtx = self.add_inputs(rawtx, wifs, change_address=change_address,\n                                fee=fee)\n        return self.publish(rawtx)", "response": "Store a null data in blockchain and return new txid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef retrieve_nulldata(self, txid):\n        rawtx = self.retrieve_tx(txid)\n        return self.get_nulldata(rawtx)", "response": "Returns nulldata stored in blockchain <txid > as hexdata."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the data blob for a raw transaction", "response": "def get_data_blob(self, rawtx):\n        \"\"\"TODO add docstring\"\"\"\n        tx = deserialize.tx(rawtx)\n        data = control.get_data_blob(tx)\n        return serialize.data(data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_data_blob(self, rawtx, hexdata, dust_limit=common.DUST_LIMIT):\n        tx = deserialize.tx(rawtx)\n        data = deserialize.binary(hexdata)\n        tx = control.add_data_blob(tx, data, dust_limit=dust_limit)\n        return serialize.tx(tx)", "response": "Add data blob to the current object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef store_data_blob(self, hexdata, wifs, change_address=None,\n                        txouts=None, fee=10000, lock_time=0,\n                        dust_limit=common.DUST_LIMIT):\n        \"\"\"TODO add docstring\"\"\"\n        rawtx = self.create_tx(txouts=txouts, lock_time=lock_time)\n        rawtx = self.add_data_blob(rawtx, hexdata, dust_limit=dust_limit)\n        rawtx = self.add_inputs(rawtx, wifs, change_address=change_address,\n                                fee=fee)\n        return self.publish(rawtx)", "response": "Store data in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retrieve_data_blob(self, txid):\n        rawtx = self.retrieve_tx(txid)\n        return self.get_data_blob(rawtx)", "response": "Retrieve the data blob for a given transaction id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_broadcast_message(self, rawtx, message, sender_wif,\n                              dust_limit=common.DUST_LIMIT):\n        \"\"\"TODO add docstring\"\"\"\n        tx = deserialize.tx(rawtx)\n        message = deserialize.unicode_str(message)\n        sender_key = deserialize.key(self.testnet, sender_wif)\n        tx = control.add_broadcast_message(self.testnet, tx, message,\n                                           sender_key, dust_limit=dust_limit)\n        return serialize.tx(tx)", "response": "Add a broadcast message to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstore a broadcast message in the database.", "response": "def store_broadcast_message(self, message, sender_wif, wifs,\n                                change_address=None, txouts=None, fee=10000,\n                                lock_time=0, dust_limit=common.DUST_LIMIT):\n        \"\"\"TODO add docstring\"\"\"\n        rawtx = self.create_tx(txouts=txouts, lock_time=lock_time)\n        rawtx = self.add_broadcast_message(rawtx, message, sender_wif,\n                                           dust_limit=dust_limit)\n        rawtx = self.add_inputs(rawtx, wifs, change_address=change_address,\n                                fee=fee)\n        return self.publish(rawtx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_broadcast_message(self, txid):\n        rawtx = self.retrieve_tx(txid)\n        return self.get_broadcast_message(rawtx)", "response": "Retrieve a broadcast message from the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of confirms for a given transaction id.", "response": "def confirms(self, txid):\n        \"\"\"Returns number of confirms or None if unpublished.\"\"\"\n        txid = deserialize.txid(txid)\n        return self.service.confirms(txid)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit utxos of wif into limit and fee.", "response": "def split_utxos(self, wif, limit, fee=10000, max_outputs=100):\n        \"\"\"Split utxos of <wif> unitil <limit> or <max_outputs> reached.\"\"\"\n        key = deserialize.key(self.testnet, wif)\n        limit = deserialize.positive_integer(limit)\n        fee = deserialize.positive_integer(fee)\n        max_outputs = deserialize.positive_integer(max_outputs)\n        spendables = control.retrieve_utxos(self.service, [key.address()])\n        txids = control.split_utxos(self.service, self.testnet, key,\n                                    spendables, limit, fee=fee,\n                                    max_outputs=max_outputs,\n                                    publish=(not self.dryrun))\n        return serialize.txids(txids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef forwards(self, orm):\n        \"Write your forwards methods here.\"\n        # Note: Remember to use orm['appname.ModelName'] rather than \"from appname.models...\"\n        for category in Category.objects.all():\n            category.slug = category._generate_slug()\n            category.save()", "response": "Write your forwards methods here."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite your backwards methods here.", "response": "def backwards(self, orm):\n        \"Write your backwards methods here.\"\n        for category in Category.objects.all():\n            category.slug = ''\n            category.save()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_time_period(value):\n        for time_period in TimePeriod:\n            if time_period.period == value:\n                return time_period\n        raise ValueError('{} is not a valid TimePeriod'.format(value))", "response": "Get the corresponding TimePeriod from the value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the monitor status from the ZM server.", "response": "def update_monitor(self):\n        \"\"\"Update the monitor and monitor status from the ZM server.\"\"\"\n        result = self._client.get_state(self._monitor_url)\n        self._raw_result = result['monitor']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the MonitorState of this Monitor.", "response": "def function(self, new_function):\n        \"\"\"Set the MonitorState of this Monitor.\"\"\"\n        self._client.change_state(\n            self._monitor_url,\n            {'Monitor[Function]': new_function.value})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nindicating if this Monitor is currently recording.", "response": "def is_recording(self) -> Optional[bool]:\n        \"\"\"Indicate if this Monitor is currently recording.\"\"\"\n        status_response = self._client.get_state(\n            'api/monitors/alarm/id:{}/command:status.json'.format(\n                self._monitor_id\n            )\n        )\n\n        if not status_response:\n            _LOGGER.warning('Could not get status for monitor {}'.format(\n                self._monitor_id\n            ))\n            return None\n\n        status = status_response.get('status')\n        # ZoneMinder API returns an empty string to indicate that this monitor\n        # cannot record right now\n        if status == '':\n            return False\n        return int(status) == STATE_ALARM"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nindicates if this Monitor is currently available.", "response": "def is_available(self) -> bool:\n        \"\"\"Indicate if this Monitor is currently available.\"\"\"\n        status_response = self._client.get_state(\n            'api/monitors/daemonStatus/id:{}/daemon:zmc.json'.format(\n                self._monitor_id\n            )\n        )\n\n        if not status_response:\n            _LOGGER.warning('Could not get availability for monitor {}'.format(\n                self._monitor_id\n            ))\n            return False\n\n        # Monitor_Status was only added in ZM 1.32.3\n        monitor_status = self._raw_result.get('Monitor_Status', None)\n        capture_fps = monitor_status and monitor_status['CaptureFPS']\n\n        return status_response.get('status', False) and capture_fps != \"0.00\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the number of events that have occurred on this Monitor.", "response": "def get_events(self, time_period, include_archived=False) -> Optional[int]:\n        \"\"\"Get the number of events that have occurred on this Monitor.\n\n        Specifically only gets events that have occurred within the TimePeriod\n        provided.\n        \"\"\"\n        date_filter = '1%20{}'.format(time_period.period)\n        if time_period == TimePeriod.ALL:\n            # The consoleEvents API uses DATE_SUB, so give it\n            # something large\n            date_filter = '100%20year'\n\n        archived_filter = '/Archived=:0'\n        if include_archived:\n            archived_filter = ''\n\n        event = self._client.get_state(\n            'api/events/consoleEvents/{}{}.json'.format(\n                date_filter,\n                archived_filter\n            )\n        )\n\n        try:\n            events_by_monitor = event['results']\n            if isinstance(events_by_monitor, list):\n                return 0\n            return events_by_monitor.get(str(self._monitor_id), 0)\n        except (TypeError, KeyError, AttributeError):\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding and return a ZoneMinder camera image url.", "response": "def _build_image_url(self, monitor, mode) -> str:\n        \"\"\"Build and return a ZoneMinder camera image url.\"\"\"\n        query = urlencode({\n            'mode': mode,\n            'buffer': monitor['StreamReplayBuffer'],\n            'monitor': monitor['Id'],\n        })\n        url = '{zms_url}?{query}'.format(\n            zms_url=self._client.get_zms_url(), query=query)\n        _LOGGER.debug('Monitor %s %s URL (without auth): %s',\n                      monitor['Id'], mode, url)\n        return self._client.get_url_with_auth(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nask for a filename to open and returned the opened file", "response": "def askopenfile(mode=\"r\", **options):\n    \"Ask for a filename to open, and returned the opened file\"\n\n    filename = askopenfilename(**options)\n    if filename:\n        return open(filename, mode)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nasking for multiple filenames and return the open file objects.", "response": "def askopenfiles(mode=\"r\", **options):\n    \"\"\"Ask for multiple filenames and return the open file\n    objects\n\n    returns a list of open file objects or an empty list if\n    cancel selected\n    \"\"\"\n\n    files = askopenfilenames(**options)\n    if files:\n        ofiles = []\n        for filename in files:\n            ofiles.append(open(filename, mode))\n        files = ofiles\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asksaveasfile(mode=\"w\", **options):\n    \"Ask for a filename to save as, and returned the opened file\"\n\n    filename = asksaveasfilename(**options)\n    if filename:\n        return open(filename, mode)\n    return None", "response": "Ask for a filename to save as and returned the opened file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, a, b):\n        self.send_add(a, b)\n        return self.recv_add()", "response": "Parameters:\n         - a\n         - b"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_allele_name(name, species_prefix=None):\n    original = name\n    name = name.strip()\n\n    if len(name) == 0:\n        raise ValueError(\"Can't normalize empty MHC allele name\")\n\n    species_from_name, name = split_species_prefix(name)\n\n    if species_prefix:\n        if species_from_name:\n            raise ValueError(\"If a species is passed in, we better not have another \"\n                             \"species in the name itself.\")\n        species = species_prefix\n    else:\n        species = species_from_name\n\n    if species in (\"H-2\", \"H2\"):\n        gene, allele_code = parse_mouse_allele_name(\"H-2-\" + name)\n        # mice don't have allele families\n        return AlleleName(\"H-2\", gene, \"\", allele_code)\n\n    if len(name) == 0:\n        raise AlleleParseError(\"Incomplete MHC allele name: %s\" % (original,))\n    elif not species:\n        # assume that a missing species name means we're dealing with a\n        # human HLA allele\n        if \"-\" in name:\n            raise AlleleParseError(\"Can't parse allele name: %s\" % original)\n        species = \"HLA\"\n\n    if name[0].upper() == \"D\":\n        if len(name) == 7:\n            # sometimes we get very compact names like DRB0101\n            gene, name = parse_letters(name, 3)\n        else:\n            # MHC class II genes like \"DQA1\" need to be parsed with both\n            # letters and numbers\n            gene, name = parse_alphanum(name, 4)\n        # TODO: make a list of known species/gene pairs, along with\n        # gene synonyms. That should significantly imporve on this kind of\n        # ad-hoc synonym handling.\n\n        if gene.isalpha():\n            # expand e.g. DRA -> DRA1, DQB -> DQB1\n            gene = gene + \"1\"\n    elif len(name) == 5:\n        # example: SLA-30101\n        gene, name = name[0], name[1:]\n    elif name[0].isalpha():\n        # if there are more separators to come, then assume the gene names\n        # can have the form \"DQA1\"\n        gene, name = parse_letters(name)\n    elif name[0].isdigit():\n        gene, name = parse_numbers(name)\n    elif len(name) in (6, 7) and (\"*\" in name or \"-\" in name or \":\" in name):\n        # example: SLA-3*0101 or SLA-3*01:01\n        gene, name = parse_alphanum(name)\n        _, name = parse_separator(name)\n    else:\n        raise AlleleParseError(\n            \"Can't parse gene name from allele: %s\" % original)\n\n    if len(gene) == 0:\n        raise AlleleParseError(\"No MHC gene name given in %s\" % original)\n    if len(name) == 0:\n        raise AlleleParseError(\"Malformed MHC type %s\" % original)\n\n    gene = gene.upper()\n    # skip initial separator\n    sep, name = parse_separator(name)\n\n    if species == \"SLA\":\n        if \":\" in name:\n            parts = name.split(\":\")\n            if len(parts) != 2:\n                raise AlleleParseError(\n                    \"Unexpected number of ':' characters in '%s'\" % original)\n            family, name = parts\n        elif len(name) < 2:\n                raise AlleleParseError(\"Unable to parse '%s'\" % original)\n        elif name.isalpha() or len(name) == 2:\n            # parse sequences serotypes like SLA-1-HB\n            # as shorthand for SLA-1-HB01\n            family = name\n            name = \"01\"\n        else:\n            # the family names for pigs can be weirdly complicated\n            # such as 'w13sm' but the alleles still always\n            # end with two digits e.g. SLA-2*w13sm20\n            family = name[:-2]\n            name = name[-2:]\n    elif len(name) == 4 or (species == \"HLA\" and gene in (\"A\", \"B\", \"C\")):\n        # If all that's left is e.g. \"0201\" then only parse the\n        # first two digits as the family code. Also, human Class I alleles\n        # seem to be exceptional in that they have only 2 digit allele\n        # families but 3 digit allele codes\n        # (other species like sheep have 3 digits followed by 2 digits)\n        family, name = parse_numbers(name, max_len=2)\n    else:\n        family, name = parse_numbers(name, max_len=3)\n\n    sep, name = parse_separator(name)\n\n    allele_code, rest_of_text = parse_numbers(name)\n\n    rest_of_text = rest_of_text.strip()\n\n    if len(rest_of_text) > 0:\n        raise AlleleParseError(\"The suffix '%s' of '%s' was not parsed\" % (\n            rest_of_text,\n            original))\n\n    if len(family) == 1:\n        family = \"0\" + family\n    elif len(family) == 3 and family[0] == \"0\":\n        family = family[1:]\n    if len(allele_code) == 0:\n        allele_code = \"01\"\n    elif len(allele_code) == 3 and allele_code[0] == \"0\":\n        # normalize HLA-A*02:001 into HLA-A*02:01\n        allele_code = allele_code[1:]\n\n    return AlleleName(species, gene, family, allele_code)", "response": "Takes an allele name and returns a AlleleName object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef spaced_coordinate(name, keys, ordered=True):\n    def validate(self):\n        \"\"\"Raise a ValueError if the instance's keys are incorrect\"\"\"\n        if set(keys) != set(self):\n            raise ValueError('{} needs keys {} and got {}'.format(type(self).__name__, keys, tuple(self)))\n\n    new_class = type(name, (Coordinate, ), {'default_order': keys if ordered else None, '_validate': validate})\n    return new_class", "response": "Returns a new class which is spaced with the given keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the vector norm with the given order of the values", "response": "def norm(self, order=2):\n        \"\"\"Find the vector norm, with the given order, of the values\"\"\"\n        return (sum(val**order for val in abs(self).values()))**(1/order)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding a sequence of Mappings or a sequence of Mappings.", "response": "def from_sequence(cls, seq, order=None, **kwargs):\n        \"\"\"Yield from a sequence of Mappings, or (if order is given), sequences\"\"\"\n        for arg in seq:\n            yield cls(arg, order=order, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef jsonify(o, max_depth=-1, parse_enums=PARSE_KEEP):\n    if max_depth == 0:\n        return o\n    max_depth -= 1\n    if isinstance(o, dict):\n        keyattrs = getattr(o.__class__, '_altnames', {})\n\n        def _getter(key, value):\n            key = keyattrs.get(key, key)\n            other = getattr(o, key, value)\n            if callable(other):\n                other = value\n            if isinstance(key, Enum):  # Make sure we use a name as the key... if we don't it might mess some things up.\n                key = key.name\n            return key, jsonify(other, max_depth=max_depth, parse_enums=parse_enums)\n\n        return dict(_getter(key, value) for key, value in six.iteritems(o))\n    elif isinstance(o, list):\n        return [jsonify(x, max_depth=max_depth, parse_enums=parse_enums) for x in o]\n    elif isinstance(o, tuple):\n        return (jsonify(x, max_depth=max_depth, parse_enums=parse_enums) for x in o)\n    elif isinstance(o, Enum):\n        o = _parse_enum(o, parse_enums=parse_enums)\n    return o", "response": "Returns a JSON - formatted version of the object o."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy the LICENSE and CONTRIBUTING files to each folder repo Copy the metadata rdf file to each folder repo Copy the metadata rdf file to each folder repo Copy the metadata if needed. Dump the metadata.", "response": "def copy_files(self):\n        \"\"\" Copy the LICENSE and CONTRIBUTING files to each folder repo \n        Generate covers if needed. Dump the metadata.\n        \"\"\"\n        files = [u'LICENSE', u'CONTRIBUTING.rst']\n        this_dir = dirname(abspath(__file__))\n        for _file in files:\n            sh.cp(\n                '{0}/templates/{1}'.format(this_dir, _file),\n                '{0}/'.format(self.book.local_path)\n            )\n\n        # copy metadata rdf file\n        if self.book.meta.rdf_path: # if None, meta is from yaml file\n            sh.cp(\n                self.book.meta.rdf_path,\n                '{0}/'.format(self.book.local_path)\n            )\n            \n        if 'GITenberg' not in self.book.meta.subjects:\n            if not self.book.meta.subjects:\n                self.book.meta.metadata['subjects'] = []\n            self.book.meta.metadata['subjects'].append('GITenberg')\n        self.save_meta()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollates the data of the object or changeset with the data from the object or changeset.", "response": "def _collate_data(collation, first_axis, second_axis):\n    \"\"\"\n    Collects information about the number of edit actions belonging to keys in\n    a supplied dictionary of object or changeset ids.\n\n    Parameters\n    ----------\n    collation : dict\n        A dictionary of OpenStreetMap object or changeset ids.\n\n    first_axis : string\n        An object or changeset key for the collation to be performed on.\n\n    second_axis : {'create','modify','delete'}\n        An action key to be added to the first_axis key.\n    \"\"\"\n    if first_axis not in collation:\n        collation[first_axis] = {}\n        collation[first_axis][\"create\"] = 0\n        collation[first_axis][\"modify\"] = 0\n        collation[first_axis][\"delete\"] = 0\n\n    first = collation[first_axis]\n\n    first[second_axis] = first[second_axis] + 1\n\n    collation[first_axis] = first"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new OpenStreetMap diff containing information about each changeset present in a diff file.", "response": "def extract_changesets(objects):\n    \"\"\"\n    Provides information about each changeset present in an OpenStreetMap diff\n    file.\n\n    Parameters\n    ----------\n    objects : osc_decoder class\n        A class containing OpenStreetMap object dictionaries.\n\n    Returns\n    -------\n    changeset_collation : dict\n        A dictionary of dictionaries with each changeset as a separate key,\n        information about each changeset as attributes in that dictionary,\n        and the actions performed in the changeset as keys.\n    \"\"\"\n    def add_changeset_info(collation, axis, item):\n        \"\"\"\n        \"\"\"\n        if axis not in collation:\n            collation[axis] = {}\n\n        first = collation[axis]\n\n        first[\"id\"] = axis\n        first[\"username\"] = item[\"username\"]\n        first[\"uid\"] = item[\"uid\"]\n        first[\"timestamp\"] = item[\"timestamp\"]\n\n        collation[axis] = first\n\n    changeset_collation = {}\n\n    for node in objects.nodes.values():\n        _collate_data(changeset_collation, node['changeset'], node['action'])\n        add_changeset_info(changeset_collation, node['changeset'], node)\n    for way in objects.ways.values():\n        _collate_data(changeset_collation, way['changeset'], way['action'])\n        add_changeset_info(changeset_collation, way['changeset'], way)\n    for relation in objects.relations.values():\n        _collate_data(changeset_collation, relation['changeset'], relation['action'])\n        add_changeset_info(changeset_collation, relation['changeset'], relation)\n\n    return changeset_collation"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_str(obj):\n    if isinstance(obj, str):\n        return obj\n    if isinstance(obj, unicode):\n        return obj.encode('utf-8')\n    return str(obj)", "response": "convert a object to string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the GDNS managed zone name for a DNS zone.", "response": "def get_managed_zone(self, zone):\n        \"\"\"Get the GDNS managed zone name for a DNS zone.\n\n        Google uses custom string names with specific `requirements\n        <https://cloud.google.com/dns/api/v1/managedZones#resource>`_\n        for storing records. The scheme implemented here chooses a\n        managed zone name which removes the trailing dot and replaces\n        other dots with dashes, and in the case of reverse records,\n        uses only the two most significant octets, prepended with\n        'reverse'. At least two octets are required for reverse DNS zones.\n\n        Example:\n           get_managed_zone('example.com.') = 'example-com'\n           get_managed_zone('20.10.in-addr.arpa.) = 'reverse-20-10'\n           get_managed_zone('30.20.10.in-addr.arpa.) = 'reverse-20-10'\n           get_managed_zone('40.30.20.10.in-addr.arpa.) = 'reverse-20-10'\n\n        Args:\n            zone (str): DNS zone.\n        Returns:\n            str of managed zone name.\n\n        \"\"\"\n        if zone.endswith('.in-addr.arpa.'):\n            return self.reverse_prefix + '-'.join(zone.split('.')[-5:-3])\n        return self.forward_prefix + '-'.join(zone.split('.')[:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget all resource record sets for a managed zone.", "response": "async def get_records_for_zone(self, dns_zone, params=None):\n        \"\"\"Get all resource record sets for a managed zone, using the DNS zone.\n\n        Args:\n            dns_zone (str): Desired DNS zone to query.\n            params (dict): (optional) Additional query parameters for HTTP\n                requests to the GDNS API.\n        Returns:\n            list of dicts representing rrsets.\n        \"\"\"\n        managed_zone = self.get_managed_zone(dns_zone)\n        url = f'{self._base_url}/managedZones/{managed_zone}/rrsets'\n\n        if not params:\n            params = {}\n\n        if 'fields' not in params:\n            # Get only the fields we care about\n            params['fields'] = ('rrsets/name,rrsets/kind,rrsets/rrdatas,'\n                                'rrsets/type,rrsets/ttl,nextPageToken')\n        next_page_token = None\n\n        records = []\n        while True:\n            if next_page_token:\n                params['pageToken'] = next_page_token\n            response = await self.get_json(url, params=params)\n            records.extend(response['rrsets'])\n            next_page_token = response.get('nextPageToken')\n            if not next_page_token:\n                break\n\n        logging.info(f'Found {len(records)} rrsets for zone \"{dns_zone}\".')\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a DNS change has completed.", "response": "async def is_change_done(self, zone, change_id):\n        \"\"\"Check if a DNS change has completed.\n\n        Args:\n            zone (str): DNS zone of the change.\n            change_id (str): Identifier of the change.\n        Returns:\n            Boolean\n        \"\"\"\n        zone_id = self.get_managed_zone(zone)\n        url = f'{self._base_url}/managedZones/{zone_id}/changes/{change_id}'\n        resp = await self.get_json(url)\n        return resp['status'] == self.DNS_CHANGES_DONE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nposting changes to a zone.", "response": "async def publish_changes(self, zone, changes):\n        \"\"\"Post changes to a zone.\n\n        Args:\n            zone (str): DNS zone of the change.\n            changes (dict): JSON compatible dict of a `Change\n                <https://cloud.google.com/dns/api/v1/changes>`_.\n        Returns:\n            string identifier of the change.\n        \"\"\"\n        zone_id = self.get_managed_zone(zone)\n        url = f'{self._base_url}/managedZones/{zone_id}/changes'\n        resp = await self.request('post', url, json=changes)\n        return json.loads(resp)['id']"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls a remote procedure.", "response": "def call(self, procedure, *args, **kwargs):\n        \"\"\"Call a remote procedure.\n\n        Replace :meth:`autobahn.wamp.interface.IApplicationSession.call`\n        \"\"\"\n        return self._async_session.call(procedure, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a procedure for remote calling.", "response": "def register(self, endpoint, procedure=None, options=None):\n        \"\"\"Register a procedure for remote calling.\n\n        Replace :meth:`autobahn.wamp.interface.IApplicationSession.register`\n        \"\"\"\n        def proxy_endpoint(*args, **kwargs):\n            return self._callbacks_runner.put(partial(endpoint, *args, **kwargs))\n        return self._async_session.register(proxy_endpoint, procedure=procedure, options=options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npublishes an event to a topic.", "response": "def publish(self, topic, *args, **kwargs):\n        \"\"\"Publish an event to a topic.\n\n        Replace :meth:`autobahn.wamp.interface.IApplicationSession.publish`\n        \"\"\"\n        return self._async_session.publish(topic, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubscribing to a topic for receiving events.", "response": "def subscribe(self, handler, topic=None, options=None):\n        \"\"\"Subscribe to a topic for receiving events.\n\n        Replace :meth:`autobahn.wamp.interface.IApplicationSession.subscribe`\n        \"\"\"\n        def proxy_handler(*args, **kwargs):\n            return self._callbacks_runner.put(partial(handler, *args, **kwargs))\n        return self._async_session.subscribe(proxy_handler, topic=topic, options=options)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef b58encode(val, charset=DEFAULT_CHARSET):\n\n    def _b58encode_int(int_, default=bytes([charset[0]])):\n        if not int_ and default:\n            return default\n        output = b''\n        while int_:\n            int_, idx = divmod(int_, base)\n            output = charset[idx:idx+1] + output\n        return output\n\n    if not isinstance(val, bytes):\n        raise TypeError(\n            \"a bytes-like object is required, not '%s', \"\n            \"use .encode('ascii') to encode unicode strings\" %\n            type(val).__name__)\n\n    if isinstance(charset, str):\n        charset = charset.encode('ascii')\n\n    base = len(charset)\n\n    if not base == 58:\n        raise ValueError('charset base must be 58, not %s' % base)\n\n    pad_len = len(val)\n    val = val.lstrip(b'\\0')\n    pad_len -= len(val)\n\n    p, acc = 1, 0\n    for char in deque(reversed(val)):\n        acc += p * char\n        p = p << 8\n\n    result = _b58encode_int(acc, default=False)\n    prefix = bytes([charset[0]]) * pad_len\n    return prefix + result", "response": "Encodes the input to base58check encoding."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef b58decode(val, charset=DEFAULT_CHARSET):\n\n    def _b58decode_int(val):\n        output = 0\n        for char in val:\n            output = output * base + charset.index(char)\n        return output\n\n    if isinstance(val, str):\n        val = val.encode()\n\n    if isinstance(charset, str):\n        charset = charset.encode()\n\n    base = len(charset)\n\n    if not base == 58:\n        raise ValueError('charset base must be 58, not %s' % base)\n\n    pad_len = len(val)\n    val = val.lstrip(bytes([charset[0]]))\n    pad_len -= len(val)\n\n    acc = _b58decode_int(val)\n\n    result = deque()\n    while acc > 0:\n        acc, mod = divmod(acc, 256)\n        result.appendleft(mod)\n\n    prefix = b'\\0' * pad_len\n    return prefix + bytes(result)", "response": "Decode base58check encoded input to original raw bytes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nallowing for a change event callback", "response": "def change(self, fn, edge=edges.BOTH):\n        \"\"\"Allow for `@change` decorator\"\"\"\n\n        def wrapped(pin):\n            fn(self.value)\n\n        GPIO.add_event_callback(self._pin, wrapped)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for_edge(self):\n        GPIO.remove_event_detect(self._pin)\n        GPIO.wait_for_edge(self._pin, self._edge)", "response": "Wait for the edge to be set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request_exception(sender, request, **kwargs):\n    if not isinstance(request, WSGIRequest):\n        logger = logging.getLogger(__name__)\n        level = CRITICAL if request.status_code <= 500 else WARNING\n\n        logger.log(level, '%s exception occured (%s)',\n                   request.status_code, request.reason_phrase)\n\n    else:\n        logger = logging.getLogger(__name__)\n        logger.log(WARNING, 'WSGIResponse exception occured')", "response": "A function to log the exception occured in the request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the full path to the first source file that exists in the base directory", "response": "def source_start(base='', book_id='book'):\n    \"\"\"\n    chooses a starting source file in the 'base' directory for id = book_id\n    \n    \"\"\"\n    repo_htm_path = \"{book_id}-h/{book_id}-h.htm\".format(book_id=book_id)\n    possible_paths = [\"book.asciidoc\",\n                      repo_htm_path,\n                      \"{}-0.txt\".format(book_id),\n                      \"{}-8.txt\".format(book_id),\n                      \"{}.txt\".format(book_id),\n                      \"{}-pdf.pdf\".format(book_id),\n                     ]\n\n    # return the first match\n\n    for path in possible_paths:\n        fullpath = os.path.join(base, path)\n        if os.path.exists(fullpath):\n            return path\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pretty_dump(fn):\n    @wraps(fn)\n    def pretty_dump_wrapper(*args, **kwargs):\n        response.content_type = \"application/json; charset=utf-8\"\n\n        return json.dumps(\n            fn(*args, **kwargs),\n\n            # sort_keys=True,\n            indent=4,\n            separators=(',', ': ')\n        )\n\n    return pretty_dump_wrapper", "response": "Decorator used to output prettified JSON."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode_json_body():\n    raw_data = request.body.read()\n\n    try:\n        return json.loads(raw_data)\n    except ValueError as e:\n        raise HTTPError(400, e.__str__())", "response": "Decode bottle. request. body to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_json_body(data):\n    # support for StringIO / file - like objects\n    if hasattr(data, \"read\"):\n        return data\n\n    response.content_type = \"application/json; charset=utf-8\"\n\n    return json.dumps(\n        data,\n        indent=4,\n        separators=(',', ': ')\n    )", "response": "Encode JSON data into a prettified JSON string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_type_error(fn):\n    @wraps(fn)\n    def handle_type_error_wrapper(*args, **kwargs):\n        def any_match(string_list, obj):\n            return filter(lambda x: x in obj, string_list)\n\n        try:\n            return fn(*args, **kwargs)\n        except TypeError as e:\n            message = e.__str__()\n            str_list = [\n                \"takes exactly\",\n                \"got an unexpected\",\n                \"takes no argument\",\n            ]\n            if fn.__name__ in message and any_match(str_list, message):\n                raise HTTPError(400, message)\n\n            raise  # This will cause 500: Internal server error\n\n    return handle_type_error_wrapper", "response": "A decorator that handles type errors in the base class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_to_params(fn=None, return_json=True):\n    def json_to_params_decorator(fn):\n        @handle_type_error\n        @wraps(fn)\n        def json_to_params_wrapper(*args, **kwargs):\n            data = decode_json_body()\n\n            if type(data) in [tuple, list]:\n                args = list(args) + data\n            elif type(data) == dict:\n                # transport only items that are not already in kwargs\n                allowed_keys = set(data.keys()) - set(kwargs.keys())\n                for key in allowed_keys:\n                    kwargs[key] = data[key]\n            elif type(data) in PRIMITIVE_TYPES:\n                args = list(args)\n                args.append(data)\n\n            if not return_json:\n                return fn(*args, **kwargs)\n\n            return encode_json_body(\n                fn(*args, **kwargs)\n            )\n\n        return json_to_params_wrapper\n\n    if fn:  # python decorator with optional parameters bukkake\n        return json_to_params_decorator(fn)\n\n    return json_to_params_decorator", "response": "Decorator to convert JSON in the body of the request to the parameters for the wrapped function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_to_data(fn=None, return_json=True):\n    def json_to_data_decorator(fn):\n        @handle_type_error\n        @wraps(fn)\n        def get_data_wrapper(*args, **kwargs):\n            kwargs[\"data\"] = decode_json_body()\n\n            if not return_json:\n                return fn(*args, **kwargs)\n\n            return encode_json_body(\n                fn(*args, **kwargs)\n            )\n\n        return get_data_wrapper\n\n    if fn:  # python decorator with optional parameters bukkake\n        return json_to_data_decorator(fn)\n\n    return json_to_data_decorator", "response": "Decorator to convert a JSON object to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts bottle forms request to parameters for the wrapped function. Args: return_json (bool, default True): Should the decorator automatically convert returned value to JSON?", "response": "def form_to_params(fn=None, return_json=True):\n    \"\"\"\n    Convert bottle forms request to parameters for the wrapped function.\n\n    Args:\n        return_json (bool, default True): Should the decorator automatically\n                    convert returned value to JSON?\n    \"\"\"\n    def forms_to_params_decorator(fn):\n        @handle_type_error\n        @wraps(fn)\n        def forms_to_params_wrapper(*args, **kwargs):\n            kwargs.update(\n                dict(request.forms)\n            )\n\n            if not return_json:\n                return fn(*args, **kwargs)\n\n            return encode_json_body(\n                fn(*args, **kwargs)\n            )\n\n        return forms_to_params_wrapper\n\n    if fn:  # python decorator with optional parameters bukkake\n        return forms_to_params_decorator(fn)\n\n    return forms_to_params_decorator"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch(sequence, time='hour'):\n    import StringIO\n    import gzip\n    import requests\n\n    if time not in ['minute','hour','day']:\n        raise ValueError('The supplied type of replication file does not exist.')\n\n    sqn = str(sequence).zfill(9)\n    url = \"https://planet.osm.org/replication/%s/%s/%s/%s.osc.gz\" %\\\n          (time, sqn[0:3], sqn[3:6], sqn[6:9])\n    content = requests.get(url)\n\n    if content.status_code == 404:\n        raise EnvironmentError('Diff file cannot be found.')\n    \n    content = StringIO.StringIO(content.content)\n    data_stream = gzip.GzipFile(fileobj=content)\n\n    return data_stream", "response": "Fetch an OpenStreetMap diff file from the replication server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef m2m_callback(sender, instance, action, reverse, model, pk_set, using, **kwargs):\n    if validate_instance(instance) and settings.AUTOMATED_LOGGING['to_database']:\n        if action in [\"post_add\", 'post_remove']:\n            modification = [model.objects.get(pk=x) for x in pk_set]\n\n            if 'al_chl' in instance.__dict__.keys() and instance.al_chl:\n                changelog = instance.al_chl\n            else:\n                changelog = ModelChangelog()\n\n                changelog.information = ModelObject()\n                changelog.information.value = repr(instance)\n                changelog.information.type = ContentType.objects.get_for_model(instance)\n                changelog.information.save()\n                changelog.save()\n\n            for f in modification:\n                obj = ModelObject()\n                obj.value = repr(f)\n\n                try:\n                    obj.type = ContentType.objects.get_for_model(f)\n                except Exception:\n                    logger = logging.getLogger(__name__)\n                    logger.debug('Could not determin the type of the modification.')\n\n                obj.save()\n                if action == 'post_add':\n                    changelog.inserted.add(obj)\n                else:\n                    changelog.removed.add(obj)\n\n            changelog.save()\n            instance.al_chl = changelog\n\n            if 'al_evt' in instance.__dict__.keys():\n                target = instance.al_evt\n            else:\n                target = Model()\n                target.user = get_current_user()\n                target.action = 2 if action == 'post_add' else 2\n                target.save()\n\n                ct = ContentType.objects.get_for_model(instance).app_label\n                target.application = Application.objects.get_or_create(name=ct)[0]\n                target.information = ModelObject()\n                target.information.value = repr(instance)\n                target.information.type = ContentType.objects.get_for_model(instance)\n                target.information.save()\n\n                instance.al_evt = target\n\n            target.modification = changelog\n            target.save()", "response": "This callback is used to handle the M2M relationship signall receivver."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _clip(value, lower, upper):\n    return lower if value < lower else upper if value > upper else value", "response": "Helper function to clip a given value based on a lower and upper bound."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    # Helper function.\n    def _draw_and_save(title, subtitle, author, filename):\n        \"\"\"\n        Draw a cover and write it to a file. Note that only PNG is supported.\n        \"\"\"\n        cover_image = draw(title, subtitle, author)\n        if filename == \"-\":\n            assert not \"Implement.\"\n        else:\n            _, ext = os.path.splitext(os.path.basename(filename))\n            if ext.upper() == \".PNG\":\n                try:\n                    with open(filename, \"wb\") as f:\n                        cover_image.save(f)\n                except FileNotFoundError:\n                    print(\"Error opening target file \" + filename)\n                    return 1\n            else:\n                print(\"Unsupported image file format '\" + ext + \"', use PNG\")\n                return 1\n        return 0\n\n    # Set up and parse the command line arguments passed to the program.\n    usage = \"Python implementation of the 10PRINT Cover image generator.\"\n    parser = argparse.ArgumentParser(usage=usage)\n\n    parser.add_argument(\"-t\", \"--title\", dest=\"title\", help=\"Book title\")\n    parser.add_argument(\"-s\", \"--subtitle\", dest=\"subtitle\", help=\"Book subtitle\", default=\"\")\n    parser.add_argument(\"-a\", \"--author\", dest=\"author\", help=\"Author(s) of the book\")\n    parser.add_argument(\"-o\", \"--cover\", dest=\"outfile\", help=\"Filename of the cover image in PNG format\")\n    parser.add_argument(\"-j\", \"--json-covers\", dest=\"json_covers\", help=\"JSON file containing cover information\")\n    args = parser.parse_args()\n\n    # A JSON file is given as command line parameter; ignore the other ones.\n    # Read the file line by line and use the given information to generate the\n    # book covers. The file contains lines of JSON maps of the format\n    #\n    #   {\"authors\": \"..\", \"identifier\": \"..\", \"subtitle\": null, \"title\": \"..\",\n    #    \"identifier_type\": \"Gutenberg ID\", \"filename\": \"..\"}\n    if args.json_covers:\n        try:\n            with open(args.json_covers, \"r\") as f:\n                for line in f:\n                    data = json.loads(line)\n                    print(\"Generating cover for \" + data[\"identifier\"])\n                    status = _draw_and_save(\n                        data[\"title\"],\n                        data[\"subtitle\"],\n                        data[\"authors\"],\n                        data[\"filename\"]\n                    )\n                    if status:\n                        print(\"Error generating book cover image, skipping\")\n            return 0\n        except ValueError:\n            print(\"Error reading from JSON file, exiting\")\n        except FileNotFoundError:\n            print(\"JSON cover file does not exist: \" + args.json_covers)\n\n    # Generate only a single cover based on the given command line arguments.\n    else:\n        if not args.title or not args.author:\n            print(\"Missing --title or --author argument, exiting\")\n        elif not args.outfile:\n            print(\"No outfile specified, exiting\")\n        else:\n            return _draw_and_save(args.title, args.subtitle, args.author, args.outfile)\n    return 1", "response": "This function is the main function of the 10PRINT Cover image generator. It is the main function of the 10PRINT Cover image generator. It is the main function of the 10PRINT Cover image generator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef triangle(self, x1, y1, x2, y2, x3, y3, color):\n        self.context.set_source_rgb(*color)\n        self.context.move_to(self.tx(x1), self.ty(y1))\n        self.context.line_to(self.tx(x2), self.ty(y2))\n        self.context.line_to(self.tx(x3), self.ty(y3))\n        self.context.line_to(self.tx(x1), self.ty(y1))\n        self.context.fill()", "response": "See the Processing function triangle():\n        https://processing.org/reference/triangle_.html"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw a rectangle on the current context", "response": "def rect(self, x, y, width, height, color):\n        \"\"\"\n        See the Processing function rect():\n        https://processing.org/reference/rect_.html\n        \"\"\"\n        self.context.set_source_rgb(*color)\n        self.context.rectangle(self.tx(x), self.ty(y), self.tx(width), self.ty(height))\n        self.context.fill()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsee the Processing function ellipse(): https://processing.org/reference/ellipse_.html", "response": "def ellipse(self, x, y, width, height, color):\n        \"\"\"\n        See the Processing function ellipse():\n        https://processing.org/reference/ellipse_.html\n        \"\"\"\n        self.context.set_source_rgb(*color)\n        self.context.save()\n        self.context.translate(self.tx(x + (width / 2.0)), self.ty(y + (height / 2.0)))\n        self.context.scale(self.tx(width / 2.0), self.ty(height / 2.0))\n        self.context.arc(0.0, 0.0, 1.0, 0.0, 2 * math.pi)\n        self.context.fill()\n        self.context.restore()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw an arc of the specified size.", "response": "def arc(self, x, y, width, height, start, end, color, thick=1, _=None):\n        \"\"\"\n        This is different than the Processing function arc():\n        https://processing.org/reference/arc_.html\n\n        Use the Cairo arc() function to draw an arc with a given line thickness.\n        \"\"\"\n        thick *= 4\n        self.context.set_source_rgb(*color)\n        self.context.save()\n        self.context.translate(self.tx(x+(width/2)), self.ty(y+(height/2)))\n        self.context.scale(self.tx(width/2), self.ty(height/2))\n        self.context.arc(0.0, 0.0, 1.0 - (self.tx(thick)/2),\n            (2*math.pi*start)/360,\n            (2*math.pi*end)/360\n        )\n        self.context.set_line_width(self.tx(thick))\n        self.context.stroke()\n        self.context.restore()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef text(self, text, x, y, width, height, color, font):\n        # Helper function.\n        def chop(word):\n            \"\"\"\n            Take a word longer than the bounding box's width and chop off as many\n            letters in the beginning as fit, followed by an ellipsis.\n            \"\"\"\n            total_str = \"\"\n            for c in word:\n                _, _, total_width, _, _, _ = self.context.text_extents(total_str + c + \"\u2026\")\n                if total_width >= width:\n                    return total_str + \"\u2026\"\n                total_str += c\n            assert not \"Should not be here, else 'word' fit into the bounding box\"\n        # Prepare the context for text rendering.\n        self.context.set_source_rgb(*color)\n        font_name, (font_size, font_slant, font_weight) = (font)\n        self.context.select_font_face(font_name, font_slant, font_weight)\n        self.context.set_font_size(font_size)\n        self.context.set_antialias(cairo.ANTIALIAS_DEFAULT)\n        # Get some font metrics.\n        font_asc, _, font_height, _, _ = self.context.font_extents()\n        # Initialize text cursor to the baseline of the font.\n        width, height = self.tx(width), self.ty(height)\n        w_x, w_y = self.tx(x), font_asc + self.ty(y)\n        # Draw the text one line at a time and ensure the bounding box.\n        line = \"\"\n        nlines = 1\n        for word in text.split(\" \"):\n            _, _, line_width, _, _, _ = self.context.text_extents(_join(line, word))\n            if line_width < width:\n                line = _join(line, word)\n            else:\n                if not line:\n                    # First word of the line extends beyond the line: chop and done.\n                    self.context.move_to(w_x, w_y)\n                    self.context.show_text(chop(word))\n                    return nlines, font_height\n                else:\n                    # Filled a line, render it, and move on to the next line.\n                    self.context.move_to(w_x, w_y)\n                    self.context.show_text(line)\n                    line = word\n                    w_y += font_height\n\n                    if w_y > height:\n                        return nlines, font_height\n                    nlines += 1\n        self.context.move_to(w_x, w_y)\n        self.context.show_text(line)\n        return nlines, font_height", "response": "Render a text in the context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef font(self, name, properties):\n        size, slant, weight = (properties)\n        return (name, (self.ty(size), slant, weight))", "response": "Return a tuple that contains font name size slant weight"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive the H S and B values for the HSB color mode convert them into the R G B values and return the color tuple.", "response": "def colorHSB(h, s, b):\n        \"\"\"\n        Given the H,S,B (equivalent to H,S,V) values for the HSB color mode,\n        convert them into the R,G,B values for the RGB color mode and return a\n        color tuple. This conversion is necessary because Cairo understands\n        only RGB(A).\n        \"\"\"\n        H, S, B = float(h), float(s/100), float(b/100)\n        if S == 0.0:\n            return (B, B, B) # achromatic (grey)\n        h = H / 60\n        i = math.floor(h)\n        f = h - i\n        v = B\n        p = v * (1 - S)\n        q = v * (1 - S * f)\n        t = v * (1 - S * (1 - f))\n        if i == 0:\n            return (v, t, b)\n        elif i == 1:\n            return (q, v, p)\n        elif i == 2:\n            return (p, v, t)\n        elif i == 3:\n            return (p, q, v)\n        elif i == 4:\n            return (t, p, v)\n        else: # i == 5 (or i == 6 for the case of H == 360)\n            return (v, p, q)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colorRGB(r, g, b):\n        return (float(r / 255), float(g / 255), float(b / 255))", "response": "Returns a tuple with the RGB values in the range [ 0.. 255 )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging-in to Alarm. com.", "response": "def async_login(self):\n       \"\"\"Login to Alarm.com.\"\"\"\n       _LOGGER.debug('Attempting to log into Alarm.com...')\n\n       # Get the session key for future logins.\n       response = None\n       try:\n           with async_timeout.timeout(10, loop=self._loop):\n               response = yield from self._websession.get(\n                   self.ALARMDOTCOM_URL + '/default.aspx',\n               headers={'User-Agent': 'Mozilla/5.0 '\n                                      '(Windows NT 6.1;'\n                                      ' WOW64; rv:40.0) '\n                                      'Gecko/20100101 '\n                                      'Firefox/40.1'})\n\n           _LOGGER.debug(\n               'Response status from Alarm.com: %s',\n               response.status)\n           text = yield from response.text()\n           _LOGGER.debug(text)\n           tree = BeautifulSoup(text, 'html.parser')\n           self._login_info = {\n               'sessionkey': self.SESSION_KEY_RE.match(\n                   str(response.url)).groupdict()['sessionKey'],\n               self.VIEWSTATE: tree.select(\n                   '#{}'.format(self.VIEWSTATE))[0].attrs.get('value'),\n               self.VIEWSTATEGENERATOR: tree.select(\n                   '#{}'.format(self.VIEWSTATEGENERATOR))[0].attrs.get('value'),\n               self.EVENTVALIDATION: tree.select(\n                   '#{}'.format(self.EVENTVALIDATION))[0].attrs.get('value')\n           }\n\n           _LOGGER.debug(self._login_info)\n           _LOGGER.info('Attempting login to Alarm.com')\n\n       except (asyncio.TimeoutError, aiohttp.ClientError):\n           _LOGGER.error('Can not get login page from Alarm.com')\n           return False\n       except AttributeError:\n           _LOGGER.error('Unable to get sessionKey from Alarm.com')\n           raise\n\n        # Login params to pass during the post\n       params = {\n           self.USERNAME: self._username,\n           self.PASSWORD: self._password,\n           self.VIEWSTATE: self._login_info[self.VIEWSTATE],\n           self.VIEWSTATEGENERATOR: self._login_info[self.VIEWSTATEGENERATOR],\n           self.EVENTVALIDATION: self._login_info[self.EVENTVALIDATION]\n       }\n\n       try:\n           # Make an attempt to log in.\n           with async_timeout.timeout(10, loop=self._loop):\n               response = yield from self._websession.post(\n                   self.ALARMDOTCOM_URL + '{}/default.aspx'.format(\n                       self._login_info['sessionkey']),\n                   data=params,\n                   headers={'User-Agent': 'Mozilla/5.0 '\n                                          '(Windows NT 6.1; '\n                                          'WOW64; rv:40.0) '\n                                          'Gecko/20100101 '\n                                          'Firefox/40.1'}\n               )\n           _LOGGER.debug(\n               'Status from Alarm.com login %s', response.status)\n\n           # Alarm.com changed their redirect so hit this page directly to get the status of login.\n           with async_timeout.timeout(10, loop=self._loop):\n               response = yield from self._websession.get(\n                   self.ALARMDOTCOM_URL + '{}/main.aspx'.format(\n                       self._login_info['sessionkey']),\n                   headers={'User-Agent': 'Mozilla/5.0 '\n                                          '(Windows NT 6.1; '\n                                          'WOW64; rv:40.0) '\n                                          'Gecko/20100101 '\n                                          'Firefox/40.1'}\n               )\n\n           # Get the text from the login to ensure that we are logged in.\n           text = yield from response.text()\n           _LOGGER.debug(text)\n           tree = BeautifulSoup(text, 'html.parser')\n           try:\n               self.state = tree.select(self.ALARM_STATE)[0].get_text()\n               _LOGGER.debug(\n                   'Current alarm state: %s', self.state)\n               self.sensor_status = tree.select(self.SENSOR_STATUS)[0].get_text()\n               _LOGGER.debug(\n                   'Current sensor status: %s', self.sensor_status)\n           except IndexError:\n               try:\n                   error_control = tree.select(\n                       '#{}'.format(self.ERROR_CONTROL))[0].attrs.get('value')\n                   if 'Login failure: Bad Credentials' in error_control:\n                       _LOGGER.error(error_control)\n                       return False\n               except AttributeError:\n                   _LOGGER.error('Error while trying to log into Alarm.com')\n                   return False\n       except (asyncio.TimeoutError, aiohttp.ClientError):\n           _LOGGER.error(\"Can not load login page from Alarm.com\")\n           return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches the latest state of the alarm.", "response": "def async_update(self):\n        \"\"\"Fetch the latest state.\"\"\"\n        _LOGGER.debug('Calling update on Alarm.com')\n        response = None\n        if not self._login_info:\n            yield from self.async_login()\n        try:\n            with async_timeout.timeout(10, loop=self._loop):\n                response = yield from self._websession.get(\n                    self.ALARMDOTCOM_URL + '{}/main.aspx'.format(\n                        self._login_info['sessionkey']),\n                    headers={'User-Agent': 'Mozilla/5.0 '\n                                           '(Windows NT 6.1; '\n                                           'WOW64; rv:40.0) '\n                                           'Gecko/20100101 '\n                                           'Firefox/40.1'}\n                )\n\n            _LOGGER.debug('Response from Alarm.com: %s', response.status)\n            text = yield from response.text()\n            _LOGGER.debug(text)\n            tree = BeautifulSoup(text, 'html.parser')\n            try:\n                self.state = tree.select(self.ALARM_STATE)[0].get_text()\n                _LOGGER.debug(\n                    'Current alarm state: %s', self.state)\n                self.sensor_status = tree.select(self.SENSOR_STATUS)[0].get_text()\n                _LOGGER.debug(\n                    'Current sensor status: %s', self.sensor_status)\n            except IndexError:\n                # We may have timed out. Re-login again\n                self.state = None\n                self.sensor_status = None\n                self._login_info = None\n                yield from self.async_update()\n        except (asyncio.TimeoutError, aiohttp.ClientError):\n            _LOGGER.error(\"Can not load login page from Alarm.com\")\n            return False\n        finally:\n            if response is not None:\n                yield from response.release()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_api_handler(self):\n        try:\n            self.github = github3.login(username=config.data['gh_user'],\n                                    password=config.data['gh_password'])\n        except KeyError as e:\n            raise config.NotConfigured(e)\n        logger.info(\"ratelimit remaining: {}\".format(self.github.ratelimit_remaining))\n        if hasattr(self.github, 'set_user_agent'):\n            self.github.set_user_agent('{}: {}'.format(self.org_name, self.org_homepage))\n        try:\n            self.org = self.github.organization(self.org_name)\n        except github3.GitHubError:\n            logger.error(\"Possibly the github ratelimit has been exceeded\")\n            logger.info(\"ratelimit: \" + str(self.github.ratelimit_remaining))", "response": "Creates an api handler and sets it on self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate_func_args(func, kwargs):\n\n    args, varargs, varkw, defaults = inspect.getargspec(func)\n    if set(kwargs.keys()) != set(args[1:]): # chop off self\n        raise TypeError(\"decorator kwargs do not match %s()'s kwargs\"\n                        % func.__name__)", "response": "Validate decorator args when used to decorate a function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an enclosing frame that skips decorator code", "response": "def enclosing_frame(frame=None, level=2):\n    \"\"\"Get an enclosing frame that skips decorator code\"\"\"\n    frame = frame or sys._getframe(level)\n    while frame.f_globals.get('__name__') == __name__: frame = frame.f_back\n    return frame"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a GPSEventConsumer client.", "response": "def get_event_consumer(config, success_channel, error_channel, metrics,\n                       **kwargs):\n    \"\"\"Get a GPSEventConsumer client.\n\n    A factory function that validates configuration, creates schema\n    validator and parser clients, creates an auth and a pubsub client,\n    and returns an event consumer (:interface:`gordon.interfaces.\n    IRunnable` and :interface:`gordon.interfaces.IMessageHandler`)\n    provider.\n\n    Args:\n        config (dict): Google Cloud Pub/Sub-related configuration.\n        success_channel (asyncio.Queue): Queue to place a successfully\n            consumed message to be further handled by the ``gordon``\n            core system.\n        error_channel (asyncio.Queue): Queue to place a message met\n            with errors to be further handled by the ``gordon`` core\n            system.\n        metrics (obj): :interface:`IMetricRelay` implementation.\n        kwargs (dict): Additional keyword arguments to pass to the\n            event consumer.\n    Returns:\n        A :class:`GPSEventConsumer` instance.\n    \"\"\"\n    builder = event_consumer.GPSEventConsumerBuilder(\n        config, success_channel, error_channel, metrics, **kwargs)\n    return builder.build_event_consumer()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_enricher(config, metrics, **kwargs):\n    builder = enricher.GCEEnricherBuilder(\n        config, metrics, **kwargs)\n    return builder.build_enricher()", "response": "Returns a GCEEnricher client."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a GDNSPublisher client.", "response": "def get_gdns_publisher(config, metrics, **kwargs):\n    \"\"\"Get a GDNSPublisher client.\n\n    A factory function that validates configuration and returns a\n    publisher client (:interface:`gordon.interfaces.IMessageHandler`)\n    provider.\n\n    Args:\n        config (dict): Google Cloud DNS API related configuration.\n        metrics (obj): :interface:`IMetricRelay` implementation.\n        kwargs (dict): Additional keyword arguments to pass to the\n            publisher.\n    Returns:\n        A :class:`GDNSPublisher` instance.\n    \"\"\"\n    builder = gdns_publisher.GDNSPublisherBuilder(\n        config, metrics, **kwargs)\n    return builder.build_publisher()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef no_exception(on_exception, logger=None):\n\n    def decorator(function):\n        def wrapper(*args, **kwargs):\n            try:\n                result = function(*args, **kwargs)\n            except Exception, e:\n                if hasattr(logger, 'exception'):\n                    logger.exception(e)\n                else:\n                    print traceback.format_exc()\n                result = on_exception\n            return result\n\n        return wrapper\n\n    return decorator", "response": "Decorator to catch exception and print traceback"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeout(seconds, err_msg=\"xtls: function run too long.\"):\n\n    def decorator(function):\n        def _on_timeout(signum, frame):\n            raise TimeoutError(err_msg)\n\n        @wraps(function)\n        def wrapper(*args, **kwargs):\n            signal.signal(signal.SIGALRM, _on_timeout)\n            signal.alarm(seconds)\n            try:\n                result = function(*args, **kwargs)\n            finally:\n                signal.alarm(0)\n            return result\n\n        return wrapper\n\n    return decorator", "response": "A decorator that raises TimeoutError if the function runs too long."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting IP from the NIC.", "response": "def get_ip():\n    \"\"\"\n    \u83b7\u53d6\u672c\u673aIP\n    \"\"\"\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    # 0x8915 -> SIOCGIFADDR\n    try:\n        return socket.inet_ntoa(fcntl.ioctl(s.fileno(), 0x8915, struct.pack('256s', 'eth1'))[20:24])\n    except Exception, e:\n        try:\n            return socket.inet_ntoa(fcntl.ioctl(s.fileno(), 0x8915, struct.pack('256s', 'wlan0'))[20:24])\n        except Exception, e:\n            try:\n                return socket.inet_ntoa(fcntl.ioctl(s.fileno(), 0x8915, struct.pack('256s', 'eth0'))[20:24])\n            except Exception, e:\n                return socket.gethostbyname(socket.getfqdn(socket.gethostname()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef singleton(cls):\n    INSTANCES = {}\n\n    def _singleton(*args, **kwargs):\n        if cls not in INSTANCES:\n            INSTANCES[cls] = cls(*args, **kwargs)\n        return INSTANCES[cls]\n\n    return _singleton", "response": "\u5355\u4f8b\u6a21\u5f0f\u7684\u88c5\u9970\u5668\uff1a \u5728\u9700\u8981\u5355\u4f8b\u7684\u7c7b\u5b9a\u4e49\u4e0a\u52a0 @singleton \u5373\u53ef"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalizing a MHC allele name.", "response": "def normalize_allele_name(raw_allele, omit_dra1=False, infer_class2_pair=True):\n    \"\"\"MHC alleles are named with a frustratingly loose system. It's not uncommon\n    to see dozens of different forms for the same allele.\n\n    Note: this function works with both class I and class II allele names (including\n    alpha/beta pairs).\n\n    For example, these all refer to the same MHC sequence:\n        - HLA-A*02:01\n        - HLA-A02:01\n        - HLA-A:02:01\n        - HLA-A0201\n        - HLA-A00201\n\n    Additionally, for human alleles, the species prefix is often omitted:\n        - A*02:01\n        - A*00201\n        - A*0201\n        - A02:01\n        - A:02:01\n        - A:002:01\n        - A0201\n        - A00201\n\n    We might also encounter \"6 digit\" and \"8 digit\" MHC types (which specify\n    variants that don't affect amino acid sequence), for our purposes these\n    should be truncated to their \"4-digit\" forms:\n        - A*02:01:01\n        - A*02:01:01:01\n    There are also suffixes which we're going to ignore:\n        - HLA-A*02:01:01G\n\n    And lastly, for human alleles, there are serotypes which we'll treat\n    as approximately equal to a 4-digit type.\n        - HLA-A2\n        - A2\n\n    These should all be normalized to:\n        HLA-A*02:01\n    \"\"\"\n    cache_key = (raw_allele, omit_dra1, infer_class2_pair)\n    if cache_key in _normalized_allele_cache:\n        return _normalized_allele_cache[cache_key]\n\n    parsed_alleles = parse_classi_or_classii_allele_name(\n        raw_allele, infer_pair=infer_class2_pair)\n    species = parsed_alleles[0].species\n    normalized_list = [species]\n    # Optionally omit the alpha allele, e.g. for IEDB predictors.\n    if omit_dra1 and len(parsed_alleles) == 2:\n        alpha, beta = parsed_alleles\n        # by convention the alpha allelle is omitted since it's assumed\n        # to be DRA1*01:01\n        if alpha == _DRA1_0101:\n            parsed_alleles = [beta]\n    for parsed_allele in parsed_alleles:\n        if len(parsed_allele.allele_family) > 0:\n            normalized_list.append(\"%s*%s:%s\" % (\n                parsed_allele.gene,\n                parsed_allele.allele_family,\n                parsed_allele.allele_code))\n        else:\n            # mice don't have allele families\n            # e.g. H-2-Kd\n            # species = H-2\n            # gene = K\n            # allele = d\n            normalized_list.append(\"%s%s\" % (\n                parsed_allele.gene,\n                parsed_allele.allele_code))\n    normalized = \"-\".join(normalized_list)\n\n    _normalized_allele_cache[cache_key] = normalized\n    return normalized"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a raw allele name into a compact allele name.", "response": "def compact_allele_name(raw_allele):\n    \"\"\"\n    Turn HLA-A*02:01 into A0201 or H-2-D-b into H-2Db or\n    HLA-DPA1*01:05-DPB1*100:01 into DPA10105-DPB110001\n    \"\"\"\n    parsed_alleles = parse_classi_or_classii_allele_name(raw_allele)\n    normalized_list = []\n    if len(parsed_alleles) == 2:\n        alpha, beta = parsed_alleles\n        # by convention the alpha allelle is omitted since it's assumed\n        # to be DRA1*01:01\n        if alpha == _DRA1_0101:\n            parsed_alleles = [beta]\n\n    for parsed_allele in parsed_alleles:\n        if len(parsed_allele.allele_family) > 0:\n            normalized_list.append(\"%s%s%s\" % (\n                parsed_allele.gene,\n                parsed_allele.allele_family,\n                parsed_allele.allele_code))\n        else:\n            # mice don't have allele families\n            normalized_list.append(\"%s%s\" % (\n                parsed_allele.gene,\n                parsed_allele.allele_code))\n    return \"-\".join(normalized_list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getVersion(data):\n    data = data.splitlines()\n    return next((\n        v\n        for v, u in zip(data, data[1:])  # v = version, u = underline\n        if len(v) == len(u) and allSame(u) and hasDigit(v) and \".\" in v\n    ))", "response": "Parse version from changelog written in RST format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits off the species component of the allele name from the rest of it. Given HLA - A * 02 : 01 returns HLA - A * 02 : 01.", "response": "def split_species_prefix(name, seps=\"-:_ \"):\n    \"\"\"\n    Splits off the species component of the allele name from the rest of it.\n\n    Given \"HLA-A*02:01\", returns (\"HLA\", \"A*02:01\").\n    \"\"\"\n    species = None\n    name_upper = name.upper()\n    name_len = len(name)\n    for curr_prefix in _all_prefixes:\n        n = len(curr_prefix)\n        if name_len <= n:\n            continue\n        if name_upper.startswith(curr_prefix.upper()):\n            species = curr_prefix\n            name = name[n:].strip(seps)\n            break\n    return (species, name)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing to make get calls to mattermost api", "response": "def get(self, request):\n        \"\"\"\n        Used to make get calls to mattermost api\n        :param request:\n        :return:\n        \"\"\"\n        headers = {\"Authorization\": \"Bearer \" + self.token }\n        g = requests.get(self.url + request, headers=headers)\n        return json.loads(g.text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse to make post calls to mattermost api", "response": "def post(self, request, data=None):\n        \"\"\"\n        Used to make post calls to mattermost api\n        :param request:\n        :param data:\n        :return:\n        \"\"\"\n        headers = {\"Authorization\": \"Bearer \" + self.token }\n        logging.debug(json.dumps(data, indent=4))\n        p = requests.post(self.url + request, headers=headers, data=json.dumps(data))\n        return json.loads(p.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs-in to the corresponding mattermost instance.", "response": "def login(self, login, password):\n        \"\"\"Login to the corresponding (self.url) mattermost instance.\"\"\"\n        props = {'login_id': login, 'password': password}\n        p = requests.post(self.url + '/users/login', data=json.dumps(props))\n        self.token = p.headers[\"Token\"] # Store the token for further requests\n        self.get_team_id()\n        return json.loads(p.text)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_channel_listing(self):\n        teams = self.get('/teams')\n        for team in teams:\n            if team['name'].lower() == self.team:\n                channel_listing = self.get('/teams/' + team['id'] + '/channels')\n                return channel_listing", "response": "This function takes in the display_name of your team and gets the channel listing for that team."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new post to a channel", "response": "def post_channel(self, channel_id, message):\n        \"\"\"\n        Creates a new post to a channel\n        :param channel_id:\n        :param message:\n        :return:\n        \"\"\"\n        headers = {\"Authorization\": \"Bearer \" + self.token}\n        props = {'channel_id': channel_id, 'message': message}\n        p = requests.post(self.url + '/posts', headers=headers, data=json.dumps(props))\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclones a book from GITenberg s repo into the library_book_dir returns True if successful False if not", "response": "def clone(self):\n        \"\"\" clones a book from GITenberg's repo into the library\n        assumes you are authenticated to git clone from repo?\n        returns True/False, message\n        \"\"\"\n        logger.debug(\"Attempting to clone {0}\".format(self.book_repo_name))\n\n        if self.path_exists():\n            return False, \"Error: Local clone of {0} already exists\".format(self.book_repo_name)\n\n        try:\n            self.local_repo = git.Repo.clone_from(self.get_clone_url_ssh(), self.library_book_dir())\n            return True, \"Success! Cloned {0}\".format(self.book_repo_name)\n        except git.exc.GitCommandError as e:\n            print(e)\n            logger.debug(\"clone ran into an issue, likely remote doesn't exist\")\n            return False, \"Error git returned  a fail code\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef formatFlow(s):\n    result = \"\"\n    shifts = []     # positions of opening '<'\n    pos = 0         # symbol position in a line\n    nextIsList = False\n\n    def IsNextList(index, maxIndex, buf):\n        if index == maxIndex:\n            return False\n        if buf[index + 1] == '<':\n            return True\n        if index < maxIndex - 1:\n            if buf[index + 1] == '\\n' and buf[index + 2] == '<':\n                return True\n        return False\n\n    maxIndex = len(s) - 1\n    for index in range(len(s)):\n        sym = s[index]\n        if sym == \"\\n\":\n            lastShift = shifts[-1]\n            result += sym + lastShift * \" \"\n            pos = lastShift\n            if index < maxIndex:\n                if s[index + 1] not in \"<>\":\n                    result += \" \"\n                    pos += 1\n            continue\n        if sym == \"<\":\n            if nextIsList == False:\n                shifts.append(pos)\n            else:\n                nextIsList = False\n            pos += 1\n            result += sym\n            continue\n        if sym == \">\":\n            shift = shifts[-1]\n            result += '\\n'\n            result += shift * \" \"\n            pos = shift\n            result += sym\n            pos += 1\n            if IsNextList(index, maxIndex, s):\n                nextIsList = True\n            else:\n                del shifts[-1]\n                nextIsList = False\n            continue\n        result += sym\n        pos += 1\n    return result", "response": "Reformats the control flow output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntraining itself using the sequence data.", "response": "def train(self, training_set, iterations=500):\n        \"\"\"Trains itself using the sequence data.\"\"\"\n        if len(training_set) > 2:\n            self.__X = np.matrix([example[0] for example in training_set])\n            if self.__num_labels == 1:\n                self.__y = np.matrix([example[1] for example in training_set]).reshape((-1, 1))\n            else:\n                eye = np.eye(self.__num_labels)\n                self.__y = np.matrix([eye[example[1]] for example in training_set])\n        else:\n            self.__X = np.matrix(training_set[0])\n            if self.__num_labels == 1:\n                self.__y = np.matrix(training_set[1]).reshape((-1, 1))\n            else:\n                eye = np.eye(self.__num_labels)\n                self.__y = np.matrix([eye[index] for sublist in training_set[1] for index in sublist])\n        self.__m = self.__X.shape[0]\n        self.__input_layer_size = self.__X.shape[1]\n        self.__sizes = [self.__input_layer_size]\n        self.__sizes.extend(self.__hidden_layers)\n        self.__sizes.append(self.__num_labels)\n        initial_theta = []\n        for count in range(len(self.__sizes) - 1):\n            epsilon = np.sqrt(6) / np.sqrt(self.__sizes[count]+self.__sizes[count+1])\n            initial_theta.append(np.random.rand(self.__sizes[count+1],self.__sizes[count]+1)*2*epsilon-epsilon)\n        initial_theta = self.__unroll(initial_theta)\n        self.__thetas = self.__roll(fmin_bfgs(self.__cost_function, initial_theta, fprime=self.__cost_grad_function, maxiter=iterations))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict(self, X):\n        return self.__cost(self.__unroll(self.__thetas), 0, np.matrix(X))", "response": "Returns predictions of input test cases."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __cost(self, params, phase, X):\n        params = self.__roll(params)\n        a = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1) # This is a1\n        calculated_a = [a] # a1 is at index 0, a_n is at index n-1\n        calculated_z = [0] # There is no z1, z_n is at index n-1\n        for i, theta in enumerate(params): # calculated_a now contains a1, a2, a3 if there was only one hidden layer (two theta matrices)\n            z = calculated_a[-1] * theta.transpose() # z_n = a_n-1 * Theta_n-1'\n            calculated_z.append(z) # Save the new z_n\n            a = self.sigmoid(z) # a_n = sigmoid(z_n)\n            if i != len(params) - 1: # Don't append extra ones for the output layer\n                a = np.concatenate((np.ones((a.shape[0], 1)), a), axis=1) # Append the extra column of ones for all other layers\n            calculated_a.append(a) # Save the new a\n\n        if phase == 0:\n            if self.__num_labels > 1:\n                return np.argmax(calculated_a[-1], axis=1)\n            return np.round(calculated_a[-1])\n\n        J = np.sum(-np.multiply(self.__y, np.log(calculated_a[-1]))-np.multiply(1-self.__y, np.log(1-calculated_a[-1])))/self.__m; # Calculate cost\n        if self.__lambda != 0: # If we're using regularization...\n            J += np.sum([np.sum(np.power(theta[:,1:], 2)) for theta in params])*self.__lambda/(2.0*self.__m) # ...add it from all theta matrices\n\n        if phase == 1:\n            return J\n\n        reversed_d = []\n        reversed_theta_grad = []\n        for i in range(len(params)): # For once per theta matrix...\n            if i == 0: # ...if it's the first one...\n                d = calculated_a[-1] - self.__y # ...initialize the error...\n            else: # ...otherwise d_n-1 = d_n * Theta_n-1[missing ones] .* sigmoid(z_n-1)\n                d = np.multiply(reversed_d[-1]*params[-i][:,1:], self.sigmoid_grad(calculated_z[-1-i])) # With i=1/1 hidden layer we're getting Theta2 at index -1, and z2 at index -2\n            reversed_d.append(d)\n            theta_grad = reversed_d[-1].transpose() * calculated_a[-i-2] / self.__m\n            if self.__lambda != 0:\n                theta_grad += np.concatenate((np.zeros((params[-1-i].shape[0], 1)), params[-1-i][:,1:]), axis=1) * self.__lambda / self.__m# regularization\n            reversed_theta_grad.append(theta_grad)\n        theta_grad = self.__unroll(reversed(reversed_theta_grad))\n        return theta_grad", "response": "Computes activation cost function and derivative."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __roll(self, unrolled):\n        rolled = []\n        index = 0\n        for count in range(len(self.__sizes) - 1):\n            in_size = self.__sizes[count]\n            out_size = self.__sizes[count+1]\n            theta_unrolled = np.matrix(unrolled[index:index+(in_size+1)*out_size])\n            theta_rolled = theta_unrolled.reshape((out_size, in_size+1))\n            rolled.append(theta_rolled)\n            index += (in_size + 1) * out_size\n        return rolled", "response": "Converts parameter array back into matrices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting parameter matrices into an array.", "response": "def __unroll(self, rolled):\n        \"\"\"Converts parameter matrices into an array.\"\"\"\n        return np.array(np.concatenate([matrix.flatten() for matrix in rolled], axis=1)).reshape(-1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing to check gradient estimation through slope approximation.", "response": "def grad(self, params, epsilon=0.0001):\n        \"\"\"Used to check gradient estimation through slope approximation.\"\"\"\n        grad = []\n        for x in range(len(params)):\n            temp = np.copy(params)\n            temp[x] += epsilon\n            temp2 = np.copy(params)\n            temp2[x] -= epsilon\n            grad.append((self.__cost_function(temp)-self.__cost_function(temp2))/(2*epsilon))\n        return np.array(grad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef postWebhook(self, dev_id, external_id, url, event_types):\n        path = 'notification/webhook'\n        payload = {'device': {'id': dev_id}, 'externalId': external_id,\n                   'url': url, 'eventTypes': event_types}\n        return self.rachio.post(path, payload)", "response": "Add a webhook to a device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect(self):\n        self.con = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.con.connect((self.ip, self.port))\n\n        log.debug('Connected with set-top box at %s:%s.',\n                  self.ip, self.port)", "response": "Connects to the Horizon box."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisconnect the connection to the Horizon box.", "response": "def disconnect(self):\n        \"\"\" Disconnect closes the connection to the Horizon box. \"\"\"\n        if self.con is not None:\n            self.con.close()\n            log.debug('Closed connection with with set-top box at %s:%s.',\n                      self.ip, self.port)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef authorize(self):\n        # Read the version of the set-top box and write it back. Why? I've no\n        # idea.\n        version = self.con.makefile().readline()\n        self.con.send(version.encode())\n\n        # The set-top box returns with 2 bytes. I've no idea what they mean.\n        self.con.recv(2)\n\n        # The following reads and writes are used to authenticate. But I don't\n        # fully understand what is going on.\n        self.con.send(struct.pack('>B', 1))\n        msg = self.con.recv(4)\n        response = struct.unpack(\">I\", msg)\n\n        if response[0] != 0:\n            log.debug(\"Failed to authorize with set-top at %s:%s.\",\n                      self.ip, self.port)\n            raise AuthenticationError()\n\n        # Dunno where this is good for. But otherwise the client doesn't work.\n        self.con.send(b'0')\n        log.debug('Authorized succesfully with set-top box at %s:%s.',\n                  self.ip, self.port)", "response": "This function is used to authenticate with the set - top box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_key(self, key):\n        cmd = struct.pack(\">BBBBBBH\", 4, 1, 0, 0, 0, 0, key)\n        self.con.send(cmd)\n\n        cmd = struct.pack(\">BBBBBBH\", 4, 0, 0, 0, 0, 0, key)\n        self.con.send(cmd)", "response": "Send a key to the Horizon box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_powered_on(self):\n        host = '{0}:62137'.format(self.ip)\n        try:\n            HTTPConnection(host, timeout=2).\\\n                request('GET', '/DeviceDescription.xml')\n        except (ConnectionRefusedError, socket.timeout):\n            log.debug('Set-top box at %s:%s is powered off.',\n                      self.ip, self.port)\n\n            return False\n\n        log.debug('Set-top box at %s:%s is powered on.', self.ip, self.port)\n        return True", "response": "Return True if the device is powered on."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npowers on the set - top box.", "response": "def power_on(self):\n        \"\"\" Power on the set-top box. \"\"\"\n        if not self.is_powered_on():\n            log.debug('Powering on set-top box at %s:%s.', self.ip, self.port)\n            self.send_key(keys.POWER)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select_channel(self, channel):\n        for i in str(channel):\n            key = int(i) + 0xe300\n            self.send_key(key)", "response": "Select a channel.\n\n        :param channel: Number of channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_hmac(message):\n    key = current_app.config['WEBHOOKS_SECRET_KEY']\n    hmac_value = hmac.new(\n        key.encode('utf-8') if hasattr(key, 'encode') else key,\n        message.encode('utf-8') if hasattr(message, 'encode') else message,\n        sha1\n    ).hexdigest()\n    return hmac_value", "response": "Calculate HMAC value of message using WEBHOOKS_SECRET_KEY."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_x_hub_signature(signature, message):\n    hmac_value = get_hmac(message)\n    if hmac_value == signature or \\\n       (signature.find('=') > -1 and\n            hmac_value == signature[signature.find('=') + 1:]):\n        return True\n    return False", "response": "Check X - Hub - Signature used by GitHub to sign requests."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef textbox(message='', title='', text='', codebox=0):\n    return psidialogs.text(message=message, title=title, text=text)", "response": "Original doc: Display some text in a proportional font with line wrapping at word breaks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filesavebox(msg=None, title=None, argInitialFile=None):\n    return psidialogs.ask_file(message=msg, title=title, default=argInitialFile, save=True)", "response": "Original doc : A file to save. Returns None if user chose to cancel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choicebox(message='Pick something.', title='', choices=['program logic error - no choices specified']):\n    return psidialogs.choice(message=message, title=title, choices=choices)", "response": "Presents the user with a list of choices."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef msgbox(message='Shall I continue?', title='', buttonMessage='OK'):\n    return psidialogs.message(message=message, title=title, ok=buttonMessage)", "response": "Original doc: Display a messagebox"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npresenting a user with a list of choices and returns a list of items.", "response": "def multchoicebox(message='Pick as many items as you like.', title='', choices=['program logic error - no choices specified']):\n    \"\"\"Original doc: Present the user with a list of choices.\n        allow him to select multiple items and return them in a list.\n        if the user doesn't choose anything from the list, return the empty list.\n        return None if he cancelled selection.\n        \"\"\"\n    return psidialogs.multi_choice(message=message, title=title, choices=choices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef integerbox(message='Enter something.', title='', argDefault=None, argLowerBound=0, argUpperBound=99):\n    return psidialogs.ask_string(message=message, title=title, default=str(argDefault))", "response": "Original doc : Shows a box in which a user can enter an integer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays a message box with choices of Continue and Cancel.", "response": "def ccbox(message=\"Shall I continue?\", title=\"\"):\n    \"\"\"\n    Original doc:\n    Display a message box with choices of Continue and Cancel.\n    The default is \"Continue\".\n    Returns returns 1 if \"Continue\" is chosen, or if\n    the dialog is cancelled (which is interpreted as\n    choosing the default).  Otherwise returns 0.\n\n    If invoked without a message parameter, displays a generic request for a confirmation\n    that the user wishes to continue.  So it can be used this way:\n\n        if ccbox(): pass # continue\n        else: sys.exit(0)  # exit the program\n    \"\"\"\n    choices = [\"Continue\", \"Cancel\"]\n    if title == None:\n        title = \"\"\n    return boolbox(message, title, choices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef boolbox(message=\"Shall I continue?\", title=\"\", choices=[\"Yes\", \"No\"]):\n    if title == None:\n        if message == \"Shall I continue?\":\n            title = \"Confirmation\"\n        else:\n            title = \"\"\n\n    reply = buttonbox(message, title, choices)\n    if reply == choices[0]:\n        return 1\n    else:\n        return 0", "response": "Display a boolean message box."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay a buttonbox with the specified choices.", "response": "def indexbox(message=\"Shall I continue?\", title=\"\", choices=[\"Yes\", \"No\"]):\n    \"\"\"\n    Original doc:\n    Display a buttonbox with the specified choices.\n    Return the index of the choice selected.\n    \"\"\"\n    reply = buttonbox(message, title, choices)\n    index = -1\n    for choice in choices:\n        index = index + 1\n        if reply == choice:\n            return index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all active projects.", "response": "async def list_all_active_projects(self, page_size=1000):\n        \"\"\"Get all active projects.\n\n        You can find the endpoint documentation `here <https://cloud.\n        google.com/resource-manager/reference/rest/v1/projects/list>`__.\n\n        Args:\n            page_size (int): hint for the client to only retrieve up to\n                this number of results per API call.\n        Returns:\n            list(dicts): all active projects\n        \"\"\"\n        url = f'{self.BASE_URL}/{self.api_version}/projects'\n        params = {'pageSize': page_size}\n\n        responses = await self.list_all(url, params)\n        projects = self._parse_rsps_for_projects(responses)\n        return [\n            project for project in projects\n            if project.get('lifecycleState', '').lower() == 'active'\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls another module s bindings.", "response": "def install(self, binder, module):\n        \"\"\"Add another module's bindings to a binder.\"\"\"\n        ModuleAdapter(module, self._injector).configure(binder)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexposing the child injector to the parent inject for a binding.", "response": "def expose(self, binder, interface, annotation=None):\n        \"\"\"Expose the child injector to the parent inject for a binding.\"\"\"\n        private_module = self\n        class Provider(object):\n            def get(self):\n                return private_module.private_injector.get_instance(\n                        interface, annotation)\n\n        self.original_binder.bind(interface, annotated_with=annotation,\n                                  to_provider=Provider)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of tuples that are used to lookup the user s entry - level attributes.", "response": "def lookups(self, request, model_admin):\n        \"\"\"\n        Returns a list of tuples. The first element in each\n        tuple is the coded value for the option that will\n        appear in the URL query. The second element is the\n        human-readable name for the option that will appear\n        in the right sidebar.\n        \"\"\"\n\n        User = get_user_model()\n        output = []\n        for i in models.Model.objects.values('user__pk').distinct():\n            pk = i['user__pk']\n\n            if pk is not None:\n                output.append([pk, User.objects.get(pk=pk).__str__])\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef queryset(self, request, queryset):\n        if self.value() is not None:\n            return queryset.filter(user__pk=self.value())\n        else:\n            return queryset", "response": "Returns the filtered queryset based on the value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of tuples that are used to identify the items in the database that will appear in the right sidebar.", "response": "def lookups(self, request, model_admin):\n        \"\"\"\n        Returns a list of tuples. The first element in each\n        tuple is the coded value for the option that will\n        appear in the URL query. The second element is the\n        human-readable name for the option that will appear\n        in the right sidebar.\n        \"\"\"\n\n        output = []\n        for i in models.Model.objects.values('information__type').distinct():\n            ct = i['information__type']\n\n            if ct is not None:\n                ct = ContentType.objects.get(pk=ct)\n                output.append([ct.pk, ct.app_label + '.' + ct.model])\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the filtered queryset based on the value of the information type.", "response": "def queryset(self, request, queryset):\n        \"\"\"\n        Returns the filtered queryset based on the value\n        provided in the query string and retrievable via\n        `self.value()`.\n        \"\"\"\n        if self.value() is not None:\n            return queryset.filter(information__type__pk=self.value())\n        else:\n            return queryset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_rdf(self):\n        try:\n            self.metadata = pg_rdf_to_json(self.rdf_path)\n        except IOError as e:\n            raise NoRDFError(e)\n\n        if not self.authnames():\n            self.author = ''\n        elif len(self.authnames()) == 1:\n            self.author = self.authnames()[0]\n        else:\n            self.author = \"Various\"", "response": "Parses the relevant PG rdf file and sets the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading the RDF file and extracts it if necessary. Returns True on success.", "response": "def download_rdf(self, force=False):\n        \"\"\"Ensures a fresh-enough RDF file is downloaded and extracted.\n\n        Returns True on error.\"\"\"\n        if self.downloading:\n            return True\n\n        if not force and (os.path.exists(RDF_PATH) and\n                (time.time() - os.path.getmtime(RDF_PATH)) < RDF_MAX_AGE):\n            return False\n        self.downloading = True\n        logging.info('Re-downloading RDF library from %s' % RDF_URL)\n        try:\n            shutil.rmtree(os.path.join(self.rdf_library_dir, 'cache'))\n        except OSError as e:\n            # Ignore not finding the directory to remove.\n            if e.errno != errno.ENOENT:\n                raise\n\n        try:\n            with open(RDF_PATH, 'w') as f:\n                with requests.get(RDF_URL, stream=True) as r:\n                    shutil.copyfileobj(r.raw, f)\n        except requests.exceptions.RequestException as e:\n            logging.error(e)\n            return True\n\n        try:\n            with tarfile.open(RDF_PATH, 'r') as f:\n                f.extractall(self.rdf_library_dir)\n        except tarfile.TarError as e:\n            logging.error(e)\n            try:\n                os.unlink(RDF_PATH)\n            except:\n                pass\n            return True\n        self.downloading = False\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_in_twisted(self, url=DEFAULT_AUTOBAHN_ROUTER, realm=DEFAULT_AUTOBAHN_REALM,\n                       authmethods=None, authid=None, authrole=None, authextra=None,\n                       callback=None, **kwargs):\n        \"\"\"\n        Start the WAMP connection. Given we cannot run synchronous stuff inside the\n        twisted thread, use this function (which returns immediately) to do the\n        initialization from a spawned thread.\n\n        :param callback: function that will be called inside the spawned thread.\n        Put the rest of you init (or you main loop if you have one) inside it\n\n        :param authmethods: Passed to :meth:`autobahn.wamp.protocol.ApplicationSession.join`\n        :param authid: Passed to :meth:`autobahn.wamp.protocol.ApplicationSession.join`\n        :param authrole: Passed to :meth:`autobahn.wamp.protocol.ApplicationSession.join`\n        :param authextra: Passed to :meth:`autobahn.wamp.protocol.ApplicationSession.join`\n\n        .. note::\n            This function must be called instead of :meth:`AutobahnSync.run`\n            if we are calling from twisted application (typically if we are running\n            our application inside crossbar as a `wsgi` component)\n        \"\"\"\n        _init_crochet(in_twisted=True)\n        logger.debug('run_in_crossbar, bootstraping')\n        # No need to go non-blocking if no callback has been provided\n        blocking = callback is None\n\n        def bootstrap_and_callback():\n            self._bootstrap(blocking, url=url, realm=realm,\n                authmethods=authmethods, authid=authid, authrole=authrole,\n                authextra=authextra, **kwargs)\n            if callback:\n                callback()\n            self._callbacks_runner.start()\n\n        threads.deferToThread(bootstrap_and_callback)", "response": "Start the WAMP connection and run the callback in a twisted thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart the WAMP connection and create the WAMP connection", "response": "def run(self, url=DEFAULT_AUTOBAHN_ROUTER, realm=DEFAULT_AUTOBAHN_REALM,\n            authmethods=None, authid=None, authrole=None, authextra=None,\n            blocking=False, callback=None, **kwargs):\n        \"\"\"\n        Start the background twisted thread and create the WAMP connection\n\n        :param blocking: If ``False`` (default) this method will spawn a new\n        thread that will be used to run the callback events (e.i. registered and\n        subscribed functions). If ``True`` this method will not returns and\n        use the current thread to run the callbacks.\n        :param callback: This callback will be called once init is done, use it\n        with ``blocking=True`` to put your WAMP related init\n        \"\"\"\n        _init_crochet(in_twisted=False)\n        self._bootstrap(blocking, url=url, realm=realm,\n                authmethods=authmethods, authid=authid, authrole=authrole,\n                authextra=authextra, **kwargs)\n        if callback:\n            callback()\n        self._callbacks_runner.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nterminate the WAMP session", "response": "def stop(self):\n        \"\"\"\n        Terminate the WAMP session\n\n        .. note::\n            If the :meth:`AutobahnSync.run` has been run with ``blocking=True``,\n            it will returns then.\n        \"\"\"\n        if not self._started:\n            raise NotRunningError(\"This AutobahnSync instance is not started\")\n        self._callbacks_runner.stop()\n        self._started = False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bootstrap(self, blocking, **kwargs):\n        join_config = {}\n        for key in ('authid', 'authmethods', 'authrole', 'authextra'):\n            val = kwargs.pop(key)\n            if val:\n                join_config[key] = val\n        if self._started:\n            raise AlreadyRunningError(\"This AutobahnSync instance is already started\")\n        if blocking:\n            self._callbacks_runner = CallbacksRunner()\n        else:\n            self._callbacks_runner = ThreadedCallbacksRunner()\n\n        @crochet.wait_for(timeout=30)\n        def start_runner():\n            ready_deferred = defer.Deferred()\n            logger.debug('[CrochetReactor] start bootstrap')\n\n            def register_session(config):\n                logger.debug('[CrochetReactor] start register_session')\n                self._async_session = _AsyncSession(config=config, join_config=join_config)\n                self._session = SyncSession(self._callbacks_runner, self._on_challenge_callback)\n                self._async_session.connect_to_sync(self._session)\n                self._session.connect_to_async(self._async_session)\n\n                def resolve(result):\n                    logger.debug('[CrochetReactor] callback resolve: %s' % result)\n                    ready_deferred.callback(result)\n                    return result\n\n                self._async_session.on_join_defer.addCallback(resolve)\n\n                def resolve_error(failure):\n                    logger.debug('[CrochetReactor] errback resolve_error: %s' % failure)\n                    ready_deferred.errback(failure)\n\n                self._async_session.on_join_defer.addErrback(resolve_error)\n                return self._async_session\n\n            self._async_runner = ApplicationRunner(**kwargs)\n            d = self._async_runner.run(register_session, start_reactor=False)\n\n            def connect_error(failure):\n                ready_deferred.errback(failure)\n\n            d.addErrback(connect_error)\n            logger.debug('[CrochetReactor] end bootstrap')\n            return ready_deferred\n\n        logger.debug('[MainThread] call bootstrap')\n        start_runner()\n        logger.debug('[MainThread] call decorated register/subscribe')\n        for cb in self._on_running_callbacks:\n            cb()\n        self._on_running_callbacks = []\n        self._started = True\n        logger.debug('[MainThread] start callbacks runner')", "response": "Create the WAMP session and configure the _callbacks_runner."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(self, procedure=None, options=None):\n\n        def decorator(func):\n            if self._started:\n                self.session.register(endpoint=func, procedure=procedure, options=options)\n            else:\n\n                def registerer():\n                    self.session.register(endpoint=func, procedure=procedure, options=options)\n\n                # Wait for the WAMP session to be started\n                self._on_running_callbacks.append(registerer)\n            return func\n\n        return decorator", "response": "Decorator for the AutobahnSync. session. register method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subscribe(self, topic, options=None):\n\n        def decorator(func):\n            if self._started:\n                self.session.subscribe(handler=func, topic=topic, options=options)\n            else:\n\n                def subscriber():\n                    self.session.subscribe(handler=func, topic=topic, options=options)\n\n                # Wait for the WAMP session to be started\n                self._on_running_callbacks.append(subscriber)\n            return func\n\n        return decorator", "response": "Decorator for the AutobahnSync. session. subscribe method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_event(self, event_id):\n    with db.session.begin_nested():\n        event = Event.query.get(event_id)\n        event._celery_task = self  # internal binding to a Celery task\n        event.receiver.run(event)  # call run directly to avoid circular calls\n        flag_modified(event, 'response')\n        flag_modified(event, 'response_headers')\n        db.session.add(event)\n    db.session.commit()", "response": "Process an event in Celery."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a JSON column.", "response": "def _json_column(**kwargs):\n    \"\"\"Return JSON column.\"\"\"\n    return db.Column(\n        JSONType().with_variant(\n            postgresql.JSON(none_as_null=True),\n            'postgresql',\n        ),\n        nullable=True,\n        **kwargs\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, event):\n        assert self.receiver_id == event.receiver_id\n        event.response = {'status': 410, 'message': 'Gone.'}\n        event.response_code = 410", "response": "Mark the event as deleted."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hook_url(self, access_token):\n        # Allow overwriting hook URL in debug mode.\n        if (current_app.debug or current_app.testing) and \\\n           current_app.config.get('WEBHOOKS_DEBUG_RECEIVER_URLS', None):\n            url_pattern = current_app.config[\n                'WEBHOOKS_DEBUG_RECEIVER_URLS'].get(self.receiver_id, None)\n            if url_pattern:\n                return url_pattern % dict(token=access_token)\n        return url_for(\n            'invenio_webhooks.event_list',\n            receiver_id=self.receiver_id,\n            access_token=access_token,\n            _external=True\n        )", "response": "Get URL for webhook."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_signature(self):\n        if not self.signature:\n            return True\n        signature_value = request.headers.get(self.signature, None)\n        if signature_value:\n            validator = 'check_' + re.sub(r'[-]', '_', self.signature).lower()\n            check_signature = getattr(signatures, validator)\n            if check_signature(signature_value, request.data):\n                return True\n        return False", "response": "Check signature of signed request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract payload from request.", "response": "def extract_payload(self):\n        \"\"\"Extract payload from request.\"\"\"\n        if not self.check_signature():\n            raise InvalidSignature('Invalid Signature')\n        if request.is_json:\n            # Request.get_json() could be first called with silent=True.\n            delete_cached_json_for(request)\n            return request.get_json(silent=False, cache=False)\n        elif request.content_type == 'application/x-www-form-urlencoded':\n            return dict(request.form)\n        raise InvalidPayload(request.content_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple with current processing status code and message.", "response": "def status(self, event):\n        \"\"\"Return a tuple with current processing status code and message.\"\"\"\n        result = AsyncResult(str(event.id))\n        return (\n            self.CELERY_STATES_TO_HTTP.get(result.state),\n            result.info.get('message')\n            if result.state in self.CELERY_RESULT_INFO_FOR and result.info\n            else event.response.get('message')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete(self, event):\n        super(CeleryReceiver, self).delete(event)\n        AsyncResult(event.id).revoke(terminate=True)", "response": "Abort running task if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate an event instance.", "response": "def create(cls, receiver_id, user_id=None):\n        \"\"\"Create an event instance.\"\"\"\n        event = cls(id=uuid.uuid4(), receiver_id=receiver_id, user_id=user_id)\n        event.payload = event.receiver.extract_payload()\n        return event"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef status(self):\n        status = self.receiver.status(self)\n        return status if status else (\n            self.response_code, self.response.get('message')\n        )", "response": "Return a tuple with current processing status code and message."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_bootstrap_functions():\n    '''Discover and register all post import hooks named in the\n    'AUTOWRAPT_BOOTSTRAP' environment variable. The value of the\n    environment variable must be a comma separated list.\n\n    '''\n\n    # This can be called twice if '.pth' file bootstrapping works and\n    # the 'autowrapt' wrapper script is still also used. We therefore\n    # protect ourselves just in case it is called a second time as we\n    # only want to force registration once.\n\n    global _registered\n\n    if _registered:\n        return \n\n    _registered = True\n\n    # It should be safe to import wrapt at this point as this code will\n    # be executed after all Python module search directories have been\n    # added to the module search path.\n\n    from wrapt import discover_post_import_hooks\n\n    for name in os.environ.get('AUTOWRAPT_BOOTSTRAP', '').split(','):\n        discover_post_import_hooks(name)", "response": "Discover and register all post import hooks named in the AUTOWRAPT_BOOTSTRAP environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bootstrap():\n    '''Patches the 'site' module such that the bootstrap functions for\n    registering the post import hook callback functions are called as\n    the last thing done when initialising the Python interpreter. This\n    function would normally be called from the special '.pth' file.\n\n    '''\n\n    global _patched\n\n    if _patched:\n        return\n\n    _patched = True\n\n    # We want to do our real work as the very last thing in the 'site'\n    # module when it is being imported so that the module search path is\n    # initialised properly. What is the last thing executed depends on\n    # whether the 'usercustomize' module support is enabled. Support for\n    # the 'usercustomize' module will not be enabled in Python virtual\n    # enviromments. We therefore wrap the functions for the loading of\n    # both the 'sitecustomize' and 'usercustomize' modules but detect\n    # when 'usercustomize' support is disabled and in that case do what\n    # we need to after the 'sitecustomize' module is loaded.\n    #\n    # In wrapping these functions though, we can't actually use wrapt to\n    # do so. This is because depending on how wrapt was installed it may\n    # technically be dependent on '.pth' evaluation for Python to know\n    # where to import it from. The addition of the directory which\n    # contains wrapt may not yet have been done. We thus use a simple\n    # function wrapper instead.\n\n    site.execsitecustomize = _execsitecustomize_wrapper(site.execsitecustomize)\n    site.execusercustomize = _execusercustomize_wrapper(site.execusercustomize)", "response": "Patches the site module so that the bootstrap functions for the user - defined modules are called as the last thing done when initialising the Python interpreter."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures and call the AutobahnSync. start method.", "response": "def init_app(self, app, router=None, realm=None, in_twisted=None):\n        \"\"\"Configure and call the :meth:`AutobahnSync.start` method\n\n        :param app: Flask app to configure\n        :param router: WAMP router to connect to\n        :param realm: WAMP realm to connect to\n        :param in_twisted: Is the code is going to run inside a Twisted application\n\n        .. Note:: The config provided as argument will overwrite the one privided by ``app.config``\n        \"\"\"\n        router = router or app.config.get('AUTHOBAHN_ROUTER')\n        realm = realm or app.config.get('AUTHOBAHN_REALM')\n        in_twisted = in_twisted or app.config.get('AUTHOBAHN_IN_TWISTED')\n        if router:\n            self.config['router'] = router\n        if realm:\n            self.config['realm'] = realm\n        if in_twisted:\n            self.run_in_twisted(url=self.config['router'], realm=self.config['realm'])\n        else:\n            self.run(url=self.config['router'], realm=self.config['realm'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef askokcancel(title=None, message=None, **options):\n    return psidialogs.ask_ok_cancel(title=title, message=message)", "response": "Original doc : Ask if operation should proceed ; return true if the answer is ok"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef askyesno(title=None, message=None, **options):\n    return psidialogs.ask_yes_no(title=title, message=message)", "response": "Original doc : Ask a question ; return true if the answer is yes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef showwarning(title=None, message=None, **options):\n    return psidialogs.warning(title=title, message=message)", "response": "Original doc : Show a warning message"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef showerror(title=None, message=None, **options):\n    return psidialogs.error(title=title, message=message)", "response": "Original doc : Show an error message"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_summary(content):\n  soup = BeautifulSoup(content, \"html.parser\")\n  summary = soup.find_all(attrs={\n      'class': re.compile(r\".*\\bsummary-content\\b.*\")})\n\n  if summary[0].p.string is None:\n    return summary[0].p.p.getText()\n  return summary[0].p.getText()", "response": "Gets the summary of a license from tldrlegal. com"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rules(license):\n\n  can = []\n  cannot = []\n  must = []\n  req = requests.get(\"{base_url}/licenses/{license}\".format(\n                     base_url=BASE_URL, license=license), headers=_HEADERS)\n\n  if req.status_code == requests.codes.ok:\n    data = req.json()\n    can = data[\"permitted\"]\n    cannot = data[\"forbidden\"]\n    must = data[\"required\"]\n\n  return can, cannot, must", "response": "Gets can cannot and must rules from github license API"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the license information and stores it in json format", "response": "def main():\n  \"\"\"Gets all the license information and stores it in json format\"\"\"\n\n  all_summary = {}\n\n  for license in RESOURCES:\n\n    req = requests.get(RESOURCES[license])\n\n    if req.status_code == requests.codes.ok:\n      summary = get_summary(req.text)\n      can, cannot, must = get_rules(license)\n\n      all_summary[license] = {\n          \"summary\": summary,\n          \"source\": RESOURCES[license],\n          \"can\": can,\n          \"cannot\": cannot,\n          \"must\": must\n      }\n\n  with open('summary.json', 'w+') as f:\n    f.write(json.dumps(all_summary, indent=4))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a response from webhook event.", "response": "def make_response(event):\n    \"\"\"Make a response from webhook event.\"\"\"\n    code, message = event.status\n    response = jsonify(**event.response)\n    response.headers['X-Hub-Event'] = event.receiver_id\n    response.headers['X-Hub-Delivery'] = event.id\n    if message:\n        response.headers['X-Hub-Info'] = message\n    add_link_header(response, {'self': url_for(\n        '.event_item', receiver_id=event.receiver_id, event_id=event.id,\n        _external=True\n    )})\n    return response, code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error_handler(f):\n    @wraps(f)\n    def inner(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except ReceiverDoesNotExist:\n            return jsonify(\n                status=404,\n                description='Receiver does not exists.'\n            ), 404\n        except InvalidPayload as e:\n            return jsonify(\n                status=415,\n                description='Receiver does not support the'\n                            ' content-type \"%s\".' % e.args[0]\n            ), 415\n        except WebhooksError:\n            return jsonify(\n                status=500,\n                description='Internal server error'\n            ), 500\n    return inner", "response": "Return a json payload and appropriate status code on expection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_event(receiver_id, event_id):\n        event = Event.query.filter_by(\n            receiver_id=receiver_id, id=event_id\n        ).first_or_404()\n\n        try:\n            user_id = request.oauth.access_token.user_id\n        except AttributeError:\n            user_id = current_user.get_id()\n\n        if event.user_id != int(user_id):\n            abort(401)\n\n        return event", "response": "Find event and check access rights."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving trailing and leading backslashes from string", "response": "def _stripslashes(s):\n  '''Removes trailing and leading backslashes from string'''\n  r = re.sub(r\"\\\\(n|r)\", \"\\n\", s)\n  r = re.sub(r\"\\\\\", \"\", r)\n  return r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget git config user name", "response": "def _get_config_name():\n  '''Get git config user name'''\n  p = subprocess.Popen('git config --get user.name', shell=True,\n                       stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n  output = p.stdout.readlines()\n  return _stripslashes(output[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_licences():\n  licenses = _LICENSES\n\n  for license in licenses:\n    print(\"{license_name} [{license_code}]\".format(\n          license_name=licenses[license], license_code=license))", "response": "Lists all the licenses on command line"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the body for a license based on a license code", "response": "def _get_license_description(license_code):\n  \"\"\" Gets the body for a license based on a license code \"\"\"\n  req = requests.get(\"{base_url}/licenses/{license_code}\".format(\n      base_url=BASE_URL, license_code=license_code), headers=_HEADERS)\n\n  if req.status_code == requests.codes.ok:\n    s = req.json()[\"body\"]\n    search_curly = re.search(r'\\{(.*)\\}', s)\n    search_square = re.search(r'\\[(.*)\\]', s)\n    license = \"\"\n    replace_string = '{year} {name}'.format(year=date.today().year,\n                                            name=_get_config_name())\n\n    if search_curly:\n      license = re.sub(r'\\{(.+)\\}', replace_string, s)\n    elif search_square:\n      license = re.sub(r'\\[(.+)\\]', replace_string, s)\n    else:\n      license = s\n\n    return license\n  else:\n    print(Fore.RED + 'No such license. Please check again.'),\n    print(Style.RESET_ALL),\n    sys.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the license summary and permitted forbidden and required", "response": "def get_license_summary(license_code):\n  \"\"\" Gets the license summary and permitted, forbidden and required\n  behaviour \"\"\"\n  try:\n    abs_file = os.path.join(_ROOT, \"summary.json\")\n    with open(abs_file, 'r') as f:\n      summary_license = json.loads(f.read())[license_code]\n\n    # prints summary\n    print(Fore.YELLOW + 'SUMMARY')\n    print(Style.RESET_ALL),\n    print(summary_license['summary'])\n\n    # prints source for summary\n    print(Style.BRIGHT + 'Source:'),\n    print(Style.RESET_ALL),\n    print(Fore.BLUE + summary_license['source'])\n    print(Style.RESET_ALL)\n\n    # prints cans\n    print(Fore.GREEN + 'CAN')\n    print(Style.RESET_ALL),\n    for rule in summary_license['can']:\n      print(rule)\n    print('')\n\n    # prints cannot\n    print(Fore.RED + 'CANNOT')\n    print(Style.RESET_ALL),\n    for rule in summary_license['cannot']:\n      print(rule)\n    print('')\n\n    # prints musts\n    print(Fore.BLUE + 'MUST')\n    print(Style.RESET_ALL),\n    for rule in summary_license['must']:\n      print(rule)\n    print('')\n\n  except KeyError:\n    print(Fore.RED + 'No such license. Please check again.'),\n    print(Style.RESET_ALL),"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving license to LICENSE. txt file", "response": "def save_license(license_code):\n  \"\"\" Grab license, save to LICENSE/LICENSE.txt file \"\"\"\n  desc = _get_license_description(license_code)\n  fname = \"LICENSE\"\n  if sys.platform == \"win32\":\n    fname += \".txt\"  # Windows and file exts\n  with open(os.path.join(os.getcwd(), fname), \"w\") as afile:\n    afile.write(desc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n  ''' harvey helps you manage and add license from the command line '''\n  arguments = docopt(__doc__, version=__version__)\n\n  if arguments['ls'] or arguments['list']:\n    _get_licences()\n  elif arguments['--tldr'] and arguments['<NAME>']:\n    get_license_summary(arguments['<NAME>'].lower())\n  elif arguments['--export'] and arguments['<NAME>']:\n    save_license(arguments['<NAME>'].lower())\n  elif arguments['<NAME>']:\n    print(_get_license_description(arguments['<NAME>'].lower()))\n  else:\n    print(__doc__)", "response": "Harvey helps you manage and add license from the command line"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, user_id):\n        path = '/'.join(['person', user_id])\n        return self.rachio.get(path)", "response": "Retrieve the information for a person entity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef copy_template(self, name=None):\n        ret = Table(self.table_name)\n        ret._indexes.update(dict((k, v.copy_template()) for k, v in self._indexes.items()))\n        ret(name)\n        return ret", "response": "Create a copy of the current table with copies of all\n           index definitions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(self, name=None):\n        ret = self.copy_template().insert_many(self.obs)(name)\n        return ret", "response": "Create a full copy of the current table including table contents\n           and index definitions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new index on a given attribute.", "response": "def create_index(self, attr, unique=False, accept_none=False):\n        \"\"\"Create a new index on a given attribute.\n           If C{unique} is True and records are found in the table with duplicate\n           attribute values, the index is deleted and C{KeyError} is raised.\n\n           If the table already has an index on the given attribute, then\n           ValueError is raised.\n           @param attr: the attribute to be used for indexed access and joins\n           @type attr: string\n           @param unique: flag indicating whether the indexed field values are \n               expected to be unique across table entries\n           @type unique: boolean\n           @param accept_none: flag indicating whether None is an acceptable\n               unique key value for this attribute (always True for non-unique\n               indexes, default=False for unique indexes)\n           @type accept_none: boolean\n        \"\"\"\n        if attr in self._indexes:\n            raise ValueError('index %r already defined for table' % attr)\n\n        if unique:\n            self._indexes[attr] = _UniqueObjIndex(attr, accept_none)\n            self._uniqueIndexes = [ind for ind in self._indexes.values() if ind.is_unique]\n        else:\n            self._indexes[attr] = _ObjIndex(attr)\n            accept_none = True\n        ind = self._indexes[attr]\n        try:\n            for obj in self.obs:\n                obval = getattr(obj, attr, None)\n                if obval is not None or accept_none:\n                    ind[obval] = obj\n                else:\n                    raise KeyError(\"None is not an allowed key\")\n            return self\n                    \n        except KeyError:\n            del self._indexes[attr]\n            self._uniqueIndexes = [ind for ind in self._indexes.values() if ind.is_unique]\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes an index from the Table.", "response": "def delete_index(self, attr):\n        \"\"\"Deletes an index from the Table.  Can be used to drop and rebuild an index,\n           or to convert a non-unique index to a unique index, or vice versa.\n           @param attr: name of an indexed attribute\n           @type attr: string\n        \"\"\"\n        if attr in self._indexes:\n            del self._indexes[attr]\n            self._uniqueIndexes = [ind for ind in self._indexes.values() if ind.is_unique]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_many(self, it):\n        unique_indexes = self._uniqueIndexes  # [ind for ind in self._indexes.values() if ind.is_unique]\n        NO_SUCH_ATTR = object()\n        new_objs = list(it)\n        if unique_indexes:\n            for ind in unique_indexes:\n                ind_attr = ind.attr\n                new_keys = dict((getattr(obj, ind_attr, NO_SUCH_ATTR), obj) for obj in new_objs)\n                if not ind.accept_none and (None in new_keys or NO_SUCH_ATTR in new_keys):\n                    raise KeyError(\"unique key cannot be None or blank for index %s\" % ind_attr, \n                                    [ob for ob in new_objs if getattr(ob, ind_attr, NO_SUCH_ATTR) is None])\n                if len(new_keys) < len(new_objs):\n                    raise KeyError(\"given sequence contains duplicate keys for index %s\" % ind_attr)\n                for key in new_keys:\n                    if key in ind:\n                        obj = new_keys[key]\n                        raise KeyError(\"duplicate unique key value '%s' for index %s\" % (getattr(obj, ind_attr), ind_attr), \n                                       new_keys[key])\n                    \n        for obj in new_objs:\n            self.obs.append(obj)\n            for attr, ind in self._indexes.items():\n                obval = getattr(obj, attr)\n                ind[obval] = obj\n\n        return self", "response": "Inserts a collection of objects into the table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove a collection of objects from the table.", "response": "def remove_many(self, it):\n        \"\"\"Removes a collection of objects from the table.\"\"\"\n        # find indicies of objects in iterable\n        to_be_deleted = list(it)\n        del_indices = []\n        for i, ob in enumerate(self.obs):\n            try:\n                tbd_index = to_be_deleted.index(ob)\n            except ValueError:\n                continue\n            else:\n                del_indices.append(i)\n                to_be_deleted.pop(tbd_index)\n\n            # quit early if we have found them all\n            if not to_be_deleted:\n                break\n\n        for i in sorted(del_indices, reverse=True):\n            self.pop(i)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing to sort keys by most selective key first", "response": "def _query_attr_sort_fn(self, attr_val):\n        \"\"\"Used to order where keys by most selective key first\"\"\"\n        attr, v = attr_val\n        if attr in self._indexes:\n            idx = self._indexes[attr]\n            if v in idx:\n                return len(idx[v])\n            else:\n                return 0\n        else:\n            return 1e9"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new Table containing the matching objects from the table based on the given query criteria.", "response": "def where(self, wherefn=None, **kwargs):\n        \"\"\"\n        Retrieves matching objects from the table, based on given\n        named parameters.  If multiple named parameters are given, then\n        only objects that satisfy all of the query criteria will be returned.\n        \n        @param wherefn: a method or lambda that returns a boolean result, as in::\n           \n           lambda ob : ob.unitprice > 10\n           \n        @type wherefn: callable(object) returning boolean\n\n        @param kwargs: attributes for selecting records, given as additional \n          named arguments of the form C{attrname=\"attrvalue\"}.\n\n        @return: a new Table containing the matching objects\n        \"\"\"\n        if kwargs:\n            # order query criteria in ascending order of number of matching items\n            # for each individual given attribute; this will minimize the number \n            # of filtering records that each subsequent attribute will have to\n            # handle\n            kwargs = list(kwargs.items())\n            if len(kwargs) > 1 and len(self) > 100:\n                kwargs = sorted(kwargs, key=self._query_attr_sort_fn)\n                \n            ret = self\n            NO_SUCH_ATTR = object()\n            for k, v in kwargs:\n                newret = ret.copy_template()\n                if k in ret._indexes:\n                    newret.insert_many(ret._indexes[k][v])\n                else:\n                    newret.insert_many(r for r in ret.obs if getattr(r, k, NO_SUCH_ATTR) == v)\n                ret = newret\n                if not ret:\n                    break\n        else:\n            ret = self.clone()\n\n        if ret and wherefn is not None:\n            newret = ret.copy_template()\n            newret.insert_many(filter(wherefn, ret.obs))\n            ret = newret\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete matching objects from the table based on given named parameters.", "response": "def delete(self, **kwargs):\n        \"\"\"Deletes matching objects from the table, based on given\n           named parameters.  If multiple named parameters are given, then\n           only objects that satisfy all of the query criteria will be removed.\n           @param kwargs: attributes for selecting records, given as additional \n              named arguments of the form C{attrname=\"attrvalue\"}.\n           @return: the number of objects removed from the table\n        \"\"\"\n        if not kwargs:\n            return 0\n        \n        affected = self.where(**kwargs)\n        self.remove_many(affected)\n        return len(affected)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort the object in place using given fields as sort key.", "response": "def sort(self, key, reverse=False):\n        \"\"\"Sort Table in place, using given fields as sort key.\n           @param key: if this is a string, it is a comma-separated list of field names,\n              optionally followed by 'desc' to indicate descending sort instead of the \n              default ascending sort; if a list or tuple, it is a list or tuple of field names\n              or field names with ' desc' appended; if it is a function, then it is the \n              function to be used as the sort key function\n           @param reverse: (default=False) set to True if results should be in reverse order\n           @type reverse: bool\n           @return: self\n        \"\"\"\n        if isinstance(key, (basestring, list, tuple)):\n            if isinstance(key, basestring):\n                attrdefs = [s.strip() for s in key.split(',')]\n                attr_orders = [(a.split()+['asc', ])[:2] for a in attrdefs]\n            else:\n                # attr definitions were already resolved to a sequence by the caller\n                if isinstance(key[0], basestring):\n                    attr_orders = [(a.split()+['asc', ])[:2] for a in key]\n                else:\n                    attr_orders = key\n            attrs = [attr for attr, order in attr_orders]\n\n            # special optimization if all orders are ascending or descending\n            if all(order == 'asc' for attr, order in attr_orders):\n                self.obs.sort(key=attrgetter(*attrs), reverse=reverse)\n            elif all(order == 'desc' for attr, order in attr_orders):\n                self.obs.sort(key=attrgetter(*attrs), reverse=not reverse)\n            else:\n                # mix of ascending and descending sorts, have to do succession of sorts\n                # leftmost attr is the most primary sort key, so reverse attr_orders to do\n                # succession of sorts from right to left\n                do_all(self.obs.sort(key=attrgetter(attr), reverse=(order == \"desc\"))\n                       for attr, order in reversed(attr_orders))\n        else:\n            # sorting given a sort key function\n            keyfn = key\n            self.obs.sort(key=keyfn, reverse=reverse)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new table containing a subset of attributes with optionally added fields computed from each rec in the original table.", "response": "def select(self, fields, **exprs):\n        \"\"\"\n        Create a new table containing a subset of attributes, with optionally \n        newly-added fields computed from each rec in the original table.\n\n        @param fields: list of strings, or single space-delimited string, listing attribute name to be included in the\n        output\n        @type fields: list, or space-delimited string\n        @param exprs: one or more named callable arguments, to compute additional fields using the given function\n        @type exprs: C{name=callable}, callable takes the record as an argument, and returns the new attribute value\n        If a string is passed as a callable, this string will be used using string formatting, given the record\n        as a source of interpolation values.  For instance, C{fullName = '%(lastName)s, %(firstName)s'}\n        \n        \"\"\"\n        fields = self._parse_fields_string(fields)\n\n        def _make_string_callable(expr):\n            if isinstance(expr, basestring):\n                return lambda r: expr % r\n            else:\n                return expr\n\n        exprs = dict((k, _make_string_callable(v)) for k, v in exprs.items())\n            \n        raw_tuples = []\n        for ob in self.obs:\n            attrvalues = tuple(getattr(ob, fieldname, None) for fieldname in fields)\n            if exprs:\n                attrvalues += tuple(expr(ob) for expr in exprs.values())\n            raw_tuples.append(attrvalues)\n        \n        all_names = tuple(fields) + tuple(exprs.keys())\n        ret = Table()\n        ret._indexes.update(dict((k, v.copy_template()) for k, v in self._indexes.items() if k in all_names))\n        return ret().insert_many(DataObject(**dict(zip(all_names, outtuple))) for outtuple in raw_tuples)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef formatted_table(self, *fields, **exprs):\n        # select_exprs = {}\n        # for f in fields:\n        #     select_exprs[f] = lambda r : str(getattr,f,None)\n        fields = set(fields)\n        select_exprs = ODict((f, lambda r, f=f: str(getattr, f, None)) for f in fields)\n\n        for ename, expr in exprs.items():\n            if isinstance(expr, basestring):\n                if re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', expr):\n                    select_exprs[ename] = lambda r: str(getattr(r, expr, None))\n                else:\n                    if \"{}\" in expr or \"{0}\" or \"{0:\" in expr:\n                        select_exprs[ename] = lambda r: expr.format(r)\n                    else:\n                        select_exprs[ename] = lambda r: expr % getattr(r, ename, \"None\")\n        \n        return self.select(**select_exprs)", "response": "Create a new table with all string formatted attribute values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef join(self, other, attrlist=None, auto_create_indexes=True, **kwargs):\n        if not kwargs:\n            raise TypeError(\"must specify at least one join attribute as a named argument\")\n        thiscol, othercol = next(iter(kwargs.items()))\n\n        retname = (\"(%s:%s^%s:%s)\" % (self.table_name, thiscol, other.table_name, othercol))\n        # make sure both tables contain records to join - if not, just return empty list\n        if not (self.obs and other.obs):\n            return Table(retname)\n        \n        if isinstance(attrlist, basestring):\n            attrlist = re.split(r'[,\\s]+', attrlist)\n            \n        # expand attrlist to full (table, name, alias) tuples\n        thisnames = set(_object_attrnames(self.obs[0]))\n        othernames = set(_object_attrnames(other.obs[0]))\n        fullcols = []\n        if attrlist is not None:\n            for col in attrlist:\n                if isinstance(col, tuple):\n                    # assume col contains at least (table, colname), fill in alias if missing \n                    # to be same as colname\n                    fullcols.append((col + (col[1],))[:3])\n                else:\n                    if col in thisnames:\n                        fullcols.append((self, col, col))\n                    elif col in othernames:\n                        fullcols.append((other, col, col))\n                    else:\n                        raise ValueError(\"join attribute not found: \" + col)\n        else:\n            fullcols = [(self, n, n) for n in thisnames]\n            fullcols += [(other, n, n) for n in othernames]\n\n        thiscols = list(filter(lambda o: o[0] is self, fullcols))\n        othercols = list(filter(lambda o: o[0] is other, fullcols))\n\n        if auto_create_indexes:\n            if thiscol not in self._indexes:\n                self.create_index(thiscol)\n            if othercol not in other._indexes:\n                other.create_index(othercol)\n\n        if thiscol in self._indexes:\n            thiscolindex = self._indexes[thiscol]\n        else:\n            raise ValueError(\"indexed attribute required for join: \"+thiscol)\n        if othercol in other._indexes:\n            othercolindex = other._indexes[othercol]\n        else:\n            raise ValueError(\"indexed attribute required for join: \"+othercol)\n\n        # use table with fewer keys to drive join\n        if len(thiscolindex) < len(othercolindex):\n            shortindex, longindex = (thiscolindex, othercolindex)\n            swap = False\n        else:\n            shortindex, longindex = (othercolindex, thiscolindex)\n            swap = True\n            \n        # find matching rows\n        matchingrows = list((longindex[key], rows) if swap else (rows, longindex[key])\n                            for key, rows in shortindex.items())\n\n        joinrows = []\n        for thisrows, otherrows in matchingrows:\n            for trow, orow in product(thisrows, otherrows):\n                retobj = DataObject()\n                do_all(setattr(retobj, a, getattr(trow, c)) for _, c, a in thiscols)\n                do_all(setattr(retobj, a, getattr(orow, c)) for _, c, a in othercols if not hasattr(retobj, a))\n                joinrows.append(retobj)\n\n        ret = Table(retname)\n        for tbl, collist in zip([self, other], [thiscols, othercols]):\n            for _, c, a in collist:\n                if c in tbl._indexes:\n                    if a not in ret._indexes:\n                        ret.create_index(a)  # no unique indexes in join results\n        ret.insert_many(joinrows)\n        return ret", "response": "Joins two tables and returns a new table containing the joined data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a JoinTerm in preparation for joining with another table.", "response": "def join_on(self, attr):\n        \"\"\"Creates a JoinTerm in preparation for joining with another table, to \n           indicate what attribute should be used in the join.  Only indexed attributes\n           may be used in a join.\n           @param attr: attribute name to join from this table (may be different\n               from the attribute name in the table being joined to)\n           @type attr: string\n           @returns: L{JoinTerm}\"\"\"\n        if attr not in self._indexes:\n            raise ValueError(\"can only join on indexed attributes\")\n        return JoinTerm(self, attr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npivots the data using the given attributes returning a PivotTable.", "response": "def pivot(self, attrlist):\n        \"\"\"Pivots the data using the given attributes, returning a L{PivotTable}.\n            @param attrlist: list of attributes to be used to construct the pivot table\n            @type attrlist: list of strings, or string of space-delimited attribute names\n        \"\"\"\n        if isinstance(attrlist, basestring):\n            attrlist = attrlist.split()\n        if all(a in self._indexes for a in attrlist):\n            return PivotTable(self, [], attrlist)\n        else:\n            raise ValueError(\"pivot can only be called using indexed attributes\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef csv_import(self, csv_source, encoding='utf-8', transforms=None, row_class=DataObject, **kwargs):\n        reader_args = dict((k, v) for k, v in kwargs.items() if k not in ['encoding',\n                                                                          'csv_source',\n                                                                          'transforms',\n                                                                          'row_class'])\n        reader = lambda src: csv.DictReader(src, **reader_args)\n        return self._import(csv_source, encoding, transforms, reader=reader, row_class=row_class)", "response": "Imports the contents of a CSV - formatted file into this table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tsv_import(self, xsv_source, encoding=\"UTF-8\", transforms=None, row_class=DataObject, **kwargs):\n        return self._xsv_import(xsv_source, encoding, transforms=transforms, delimiter=\"\\t\", row_class=row_class, **kwargs)", "response": "Imports the contents of a tab - separated data file into this table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef csv_export(self, csv_dest, fieldnames=None, encoding=\"UTF-8\"):\n        close_on_exit = False\n        if isinstance(csv_dest, basestring):\n            if PY_3:\n                csv_dest = open(csv_dest, 'w', newline='', encoding=encoding)\n            else:\n                csv_dest = open(csv_dest, 'wb')\n            close_on_exit = True\n        try:\n            if fieldnames is None:\n                fieldnames = list(_object_attrnames(self.obs[0]))\n            if isinstance(fieldnames, basestring):\n                fieldnames = fieldnames.split()\n\n            csv_dest.write(','.join(fieldnames) + NL)\n            csvout = csv.DictWriter(csv_dest, fieldnames, extrasaction='ignore', lineterminator=NL)\n            if hasattr(self.obs[0], \"__dict__\"):\n                csvout.writerows(o.__dict__ for o in self.obs)\n            else:\n                do_all(csvout.writerow(ODict(starmap(lambda obj, fld: (fld, getattr(obj, fld)),\n                                                     zip(repeat(o), fieldnames)))) for o in self.obs)\n        finally:\n            if close_on_exit:\n                csv_dest.close()", "response": "Exports the contents of the table to a CSV - formatted file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting the contents of a JSON data file into this table.", "response": "def json_import(self, source, encoding=\"UTF-8\", transforms=None, row_class=DataObject):\n        \"\"\"Imports the contents of a JSON data file into this table.\n           @param source: JSON data file - if a string is given, the file with that name will be\n               opened, read, and closed; if a file object is given, then that object\n               will be read as-is, and left for the caller to be closed.\n           @type source: string or file\n           @param transforms: dict of functions by attribute name; if given, each\n               attribute will be transformed using the corresponding transform; if there is no\n               matching transform, the attribute will be read as a string (default); the\n               transform function can also be defined as a (function, default-value) tuple; if\n               there is an Exception raised by the transform function, then the attribute will\n               be set to the given default value\n           @type transforms: dict (optional)\n        \"\"\"\n        class _JsonFileReader(object):\n            def __init__(self, src):\n                self.source = src\n            def __iter__(self):\n                current = ''\n                for line in self.source:\n                    if current:\n                        current += ' '\n                    current += line\n                    try:\n                        yield json.loads(current)\n                        current = ''\n                    except Exception:\n                        pass\n        return self._import(source, encoding, transforms=transforms, reader=_JsonFileReader, row_class=row_class)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef json_export(self, dest, fieldnames=None, encoding=\"UTF-8\"):\n        close_on_exit = False\n        if isinstance(dest, basestring):\n            if PY_3:\n                dest = open(dest, 'w', encoding=encoding)\n            else:\n                dest = open(dest, 'w')\n            close_on_exit = True\n        try:\n            if isinstance(fieldnames, basestring):\n                fieldnames = fieldnames.split()\n\n            if fieldnames is None:\n                do_all(dest.write(_to_json(o)+'\\n') for o in self.obs)\n            else:\n                do_all(dest.write(json.dumps(ODict((f, getattr(o, f)) for f in fieldnames))+'\\n') for o in self.obs)\n        finally:\n            if close_on_exit:\n                dest.close()", "response": "Exports the contents of the table to a JSON - formatted file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_field(self, attrname, fn, default=None):\n        # for rec in self:\n        def _add_field_to_rec(rec_, fn_=fn, default_=default):\n            try:\n                val = fn_(rec_)\n            except Exception:\n                val = default_\n            if isinstance(rec_, DataObject):\n                rec_.__dict__[attrname] = val\n            else:\n                setattr(rec_, attrname, val)\n        try:\n            do_all(_add_field_to_rec(r) for r in self)\n        except AttributeError:\n            raise AttributeError(\"cannot add/modify attribute {!r} in table records\".format(attrname))\n        return self", "response": "Adds a new attribute to each object in table or replaces an existing attribute in each record with a computed value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef groupby(self, keyexpr, **outexprs):\n        if isinstance(keyexpr, basestring):\n            keyattrs = keyexpr.split()\n            keyfn = lambda o: tuple(getattr(o, k) for k in keyattrs)\n\n        elif isinstance(keyexpr, tuple):\n            keyattrs = (keyexpr[0],)\n            keyfn = keyexpr[1]\n\n        else:\n            raise TypeError(\"keyexpr must be string or tuple\")\n\n        groupedobs = defaultdict(list)\n        do_all(groupedobs[keyfn(ob)].append(ob) for ob in self.obs)\n\n        tbl = Table()\n        do_all(tbl.create_index(k, unique=(len(keyattrs) == 1)) for k in keyattrs)\n        for key, recs in sorted(groupedobs.items()):\n            groupobj = DataObject(**dict(zip(keyattrs, key)))\n            do_all(setattr(groupobj, subkey, expr(recs)) for subkey, expr in outexprs.items())\n            tbl.insert(groupobj)\n        return tbl", "response": "simple prototype of group by with support for expressions in the group - by clause \n           and outputs\n          "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new table of objects containing no duplicate values.", "response": "def unique(self, key=None):\n        \"\"\"\n        Create a new table of objects,containing no duplicate values.\n\n        @param key: (default=None) optional callable for computing a representative unique key for each\n        object in the table. If None, then a key will be composed as a tuple of all the values in the object.\n        @type key: callable, takes the record as an argument, and returns the key value or tuple to be used\n        to represent uniqueness.\n        \"\"\"\n        if isinstance(key, basestring):\n            key = lambda r, attr=key: getattr(r, attr, None)\n        ret = self.copy_template()\n        seen = set()\n        for ob in self:\n            if key is None:\n                try:\n                    ob_dict = vars(ob)\n                except TypeError:\n                    ob_dict = dict((k, getattr(ob, k)) for k in _object_attrnames(ob))\n                reckey = tuple(sorted(ob_dict.items()))\n            else:\n                reckey = key(ob)\n            if reckey not in seen:\n                seen.add(reckey)\n                ret.insert(ob)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(self):\n        unique_indexes = set(self._uniqueIndexes)\n        return {\n            'len': len(self),\n            'name': self.table_name,\n            'fields': list(_object_attrnames(self[0])) if self else [],\n            'indexes': [(iname, self._indexes[iname] in unique_indexes) for iname in self._indexes],\n        }", "response": "Quick method to list informative table information and statistics\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_fields_string(self, field_names):\n        if isinstance(field_names, basestring):\n            field_names = field_names.split()\n        if not self.obs:\n            return field_names\n\n        suppress_names = [nm[1:] for nm in field_names if nm.startswith('-')]\n        field_names = [nm for nm in field_names if not nm.startswith('-')]\n        if not field_names:\n            field_names = ['*']\n        if '*' in field_names:\n            star_fields = [name for name in _object_attrnames(self[0]) if name not in field_names]\n            fn_iter = iter(field_names)\n            field_names = list(takewhile(lambda x: x != '*', fn_iter)) + star_fields + list(fn_iter)\n        field_names = [nm for nm in field_names if nm not in suppress_names]\n        return field_names", "response": "Convert raw string or list of names to actual column names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noutput the table as a rudimentary HTML table.", "response": "def as_html(self, fields='*'):\n        \"\"\"\n        Output the table as a rudimentary HTML table.\n        @param fields: fields in the table to be shown in the table\n                       - listing '*' as a field will add all unnamed fields\n                       - starting a field name with '-' will suppress that name\n        @type fields: list of strings or a single space-delimited string\n        @return: string of generated HTML representing the selected table row attributes\n        \"\"\"\n        fields = self._parse_fields_string(fields)\n        def td_value(v):\n            return '<td><div align=\"{}\">{}</div></td>'.format(('left','right')[isinstance(v, (int, float))], str(v))\n        def row_to_tr(r):\n            return \"<tr>\" + \"\".join(td_value(getattr(r, fld)) for fld in fields) + \"</tr>\\n\"\n        ret = \"\"\n        ret += \"<table>\\n\"\n        ret += \"<tr>\" + \"\".join(map('<th><div align=\"center\">{}</div></th>'.format, fields)) + \"</tr>\\n\"\n        ret += \"\".join(map(row_to_tr, self))\n        ret += \"</table>\"\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump(self, out=sys.stdout, row_fn=repr, limit=-1, indent=0):\n        NL = '\\n'\n        if indent:\n            out.write(\"  \"*indent + self.pivot_key_str())\n        else:\n            out.write(\"Pivot: %s\" % ','.join(self._pivot_attrs))\n        out.write(NL)\n        if self.has_subtables():\n            do_all(sub.dump(out, row_fn, limit, indent+1) for sub in self.subtables if sub)\n        else:\n            if limit >= 0:\n                showslice = slice(0, limit)\n            else:\n                showslice = slice(None, None)\n            do_all(out.write(\"  \"*(indent+1) + row_fn(r) + NL) for r in self.obs[showslice])\n        out.flush()", "response": "Dump out the contents of this table in a nested listing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping out the summary counts of entries in this pivot table as a tabular listing.", "response": "def dump_counts(self, out=sys.stdout, count_fn=len, colwidth=10):\n        \"\"\"Dump out the summary counts of entries in this pivot table as a tabular listing.\n           @param out: output stream to write to\n           @param count_fn: (default=len) function for computing value for each pivot cell\n           @param colwidth: (default=10)\n        \"\"\"\n        if len(self._pivot_attrs) == 1:\n            out.write(\"Pivot: %s\\n\" % ','.join(self._pivot_attrs))\n            maxkeylen = max(len(str(k)) for k in self.keys())\n            maxvallen = colwidth\n            keytally = {}\n            for k, sub in self.items():\n                sub_v = count_fn(sub)\n                maxvallen = max(maxvallen, len(str(sub_v)))\n                keytally[k] = sub_v\n            for k, sub in self.items():\n                out.write(\"%-*.*s \" % (maxkeylen, maxkeylen, k))\n                out.write(\"%*s\\n\" % (maxvallen, keytally[k]))\n        elif len(self._pivot_attrs) == 2:\n            out.write(\"Pivot: %s\\n\" % ','.join(self._pivot_attrs))\n            maxkeylen = max(max(len(str(k)) for k in self.keys()), 5)\n            maxvallen = max(max(len(str(k)) for k in self.subtables[0].keys()), colwidth)\n            keytally = dict((k, 0) for k in self.subtables[0].keys())\n            out.write(\"%*s \" % (maxkeylen, ''))\n            out.write(' '.join(\"%*.*s\" % (maxvallen, maxvallen, k) for k in self.subtables[0].keys()))\n            out.write(' %*s\\n' % (maxvallen, 'Total'))\n            for k, sub in self.items():\n                out.write(\"%-*.*s \" % (maxkeylen, maxkeylen, k))\n                for kk, ssub in sub.items():\n                    ssub_v = count_fn(ssub)\n                    out.write(\"%*d \" % (maxvallen, ssub_v))\n                    keytally[kk] += ssub_v\n                    maxvallen = max(maxvallen, len(str(ssub_v)))\n                sub_v = count_fn(sub)\n                maxvallen = max(maxvallen, len(str(sub_v)))\n                out.write(\"%*d\\n\" % (maxvallen, sub_v))\n            out.write('%-*.*s ' % (maxkeylen, maxkeylen, \"Total\"))\n            out.write(' '.join(\"%*d\" % (maxvallen, tally) for k, tally in sorted(keytally.items())))\n            out.write(\" %*d\\n\" % (maxvallen, sum(tally for k, tally in keytally.items())))\n        else:\n            raise ValueError(\"can only dump summary counts for 1 or 2-attribute pivots\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndumps out the summary counts of this pivot table as a Table.", "response": "def as_table(self, fn=None, col=None, col_label=None):\n        \"\"\"Dump out the summary counts of this pivot table as a Table.\n        \"\"\"\n        if col_label is None:\n            col_label = col\n        if fn is None:\n            fn = len\n            if col_label is None:\n                col_label = 'count'\n        ret = Table()\n        # topattr = self._pivot_attrs[0]\n        do_all(ret.create_index(attr) for attr in self._pivot_attrs)\n        if len(self._pivot_attrs) == 1:\n            for sub in self.subtables:\n                subattr, subval = sub._attr_path[-1]\n                attrdict = {subattr: subval}\n                if col is None or fn is len:\n                    attrdict[col_label] = fn(sub)\n                else:\n                    attrdict[col_label] = fn(s[col] for s in sub)\n                ret.insert(DataObject(**attrdict))\n        elif len(self._pivot_attrs) == 2:\n            for sub in self.subtables:\n                for ssub in sub.subtables:\n                    attrdict = dict(ssub._attr_path)\n                    if col is None or fn is len:\n                        attrdict[col_label] = fn(ssub)\n                    else:\n                        attrdict[col_label] = fn(s[col] for s in ssub)\n                    ret.insert(DataObject(**attrdict))\n        elif len(self._pivot_attrs) == 3:\n            for sub in self.subtables:\n                for ssub in sub.subtables:\n                    for sssub in ssub.subtables:\n                        attrdict = dict(sssub._attr_path)\n                        if col is None or fn is len:\n                            attrdict[col_label] = fn(sssub)\n                        else:\n                            attrdict[col_label] = fn(s[col] for s in sssub)\n                        ret.insert(DataObject(**attrdict))\n        else:\n            raise ValueError(\"can only dump summary counts for 1 or 2-attribute pivots\")\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _python3_record_factory(*args, **kwargs):\n    record = _python_record_factory(*args, **kwargs)\n    _update_record(record)\n    return record", "response": "Python 3 approach to custom logging using logging. getLogger."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the log record with the values needed by LOG_FORMAT", "response": "def _update_record(record):\n    \"\"\"Collates values needed by LOG_FORMAT\n\n    This adds additional information to the log record to implement the logging standard.\n\n    :return: A log record augmented with the values required by LOG_FORMAT:\n     * springtime: LogRecord.asctime, but with a `.` instead of a `,` as the millisecond separator\n     * levelname_spring: specifically, \"WARN\" instead of \"WARNING\"\n     * process_id\n     * thread_name\n     * logger_name\n     * tracing_information: if B3 values have not been collected this will be an empty string\n    \"\"\"\n\n    # Standard fields\n    dt = datetime.fromtimestamp(record.created)\n    # Truncate microseconds to milliseconds\n    record.springtime = str(dt)[:-3]\n    record.levelname_spring = \"WARN\" if record.levelname == \"WARNING\" else record.levelname\n    record.process_id = str(os.getpid())\n    record.thread_name = (current_thread().getName())[:15]\n    record.logger_name = record.name[:40]\n    record.tracing_information = \"\"\n\n    # Optional distributed tracing information\n    tracing_information = _tracing_information()\n    if tracing_information:\n        record.tracing_information = \"[\" + \",\".join(tracing_information) + \"] \""}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _tracing_information():\n\n    # We'll collate trace information if the B3 headers have been collected:\n    values = b3.values()\n    if values[b3.b3_trace_id]:\n        # Trace information would normally be sent to Zipkin if either of sampled or debug (\"flags\") is set to 1\n        # However we're not currently using Zipkin, so it's always false\n        # exported = \"true\" if values[b3.b3_sampled] == '1' or values[b3.b3_flags] == '1' else \"false\"\n\n        return [\n            current_app.name if current_app.name else \" - \",\n            values[b3.b3_trace_id],\n            values[b3.b3_span_id],\n            \"false\",\n        ]", "response": "Gets the B3 distributed tracing information if available."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef upgrade():\n    def json_column(name, **kwargs):\n        \"\"\"Return JSON column.\"\"\"\n        return sa.Column(\n            name,\n            sqlalchemy_utils.types.JSONType().with_variant(\n                postgresql.JSON(none_as_null=True), 'postgresql',\n            ),\n            **kwargs\n        )\n\n    op.create_table(\n        'webhooks_events',\n        sa.Column('created', sa.DateTime(), nullable=False),\n        sa.Column('updated', sa.DateTime(), nullable=False),\n        sa.Column(\n            'id', sqlalchemy_utils.types.uuid.UUIDType(), nullable=False),\n        sa.Column('receiver_id', sa.String(length=255), nullable=False),\n        sa.Column('user_id', sa.Integer(), nullable=True),\n        json_column('payload', nullable=True),\n        json_column('payload_headers', nullable=True),\n        json_column('response', nullable=True),\n        json_column('response_headers', nullable=True),\n        sa.Column('response_code', sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint(['user_id'], [u'accounts_user.id'], ),\n        sa.PrimaryKeyConstraint('id')\n    )\n    op.create_index(\n        op.f('ix_webhooks_events_receiver_id'),\n        'webhooks_events',\n        ['receiver_id'],\n        unique=False\n    )", "response": "Upgrade database to version 1. 0."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _authenticate(self):\n        data = {'username': self.username,\n                'password': self.password}\n        url = '{base}/client/login'.format(base=self.base_url)\n        response = self._session.get(url, params=data)\n        print(response.text)\n        data = response.json()\n        if not data.get('success'):\n            raise InvalidCredentials(data.get('reason', None))\n        self._populate_info(data)", "response": "Authenticates to the api and sets up client information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _logout(self, reset=True):\n        url = '{base}/client/auth/logout'.format(base=self.base_url)\n        response = self._session.get(url, params=self._parameters)\n        if response.ok:\n            if reset:\n                self._reset()\n            return True\n        else:\n            return False", "response": "Log out of the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _state(self):\n        state = {}\n        required_keys = ('deviceStatusInfo',\n                         'gasUsage',\n                         'powerUsage',\n                         'thermostatInfo',\n                         'thermostatStates')\n        try:\n            for _ in range(self._state_retries):\n                state.update(self._get_data('/client/auth/retrieveToonState'))\n        except TypeError:\n            self._logger.exception('Could not get answer from service.')\n        message = ('Updating internal state with retrieved '\n                   'state:{state}').format(state=state)\n        self._logger.debug(message)\n        self._state_.update(state)\n        if not all([key in self._state_.keys() for key in required_keys]):\n            raise IncompleteResponse(state)\n        return self._state_", "response": "The internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef smokedetectors(self):\n        return [SmokeDetector(smokedetector.get('devUuid'),\n                              smokedetector.get('name'),\n                              smokedetector.get('lastConnectedChange'),\n                              smokedetector.get('connected'),\n                              smokedetector.get('batteryLevel'),\n                              smokedetector.get('type'))\n                for smokedetector in self._state.get('smokeDetectors',\n                                                     {}).get('device', [])]", "response": "Returns a list of smokedetectors modeled as named tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a smokedetector object by its name", "response": "def get_smokedetector_by_name(self, name):\n        \"\"\"Retrieves a smokedetector object by its name\n\n        :param name: The name of the smokedetector to return\n        :return: A smokedetector object\n        \"\"\"\n        return next((smokedetector for smokedetector in self.smokedetectors\n                     if smokedetector.name.lower() == name.lower()), None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of lights in the system", "response": "def lights(self):\n        \"\"\":return: A list of light objects\"\"\"\n        return [Light(self, light.get('name'))\n                for light in self._state.get('deviceStatusInfo',\n                                             {}).get('device', [])\n                if light.get('rgbColor')]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving a light object by its name", "response": "def get_light_by_name(self, name):\n        \"\"\"Retrieves a light object by its name\n\n        :param name: The name of the light to return\n        :return: A light object\n        \"\"\"\n        return next((light for light in self.lights\n                     if light.name.lower() == name.lower()), None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef smartplugs(self):\n        return [SmartPlug(self, plug.get('name'))\n                for plug in self._state.get('deviceStatusInfo',\n                                            {}).get('device', [])\n                if plug.get('networkHealthState')]", "response": "A list of smartplug objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_smartplug_by_name(self, name):\n        return next((plug for plug in self.smartplugs\n                     if plug.name.lower() == name.lower()), None)", "response": "Retrieves a smartplug object by its name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef thermostat_states(self):\n        return [ThermostatState(STATES[state.get('id')],\n                                state.get('id'),\n                                state.get('tempValue'),\n                                state.get('dhw'))\n                for state in self._state['thermostatStates']['state']]", "response": "Returns a list of thermostatstate objects modeled as named tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_thermostat_state_by_name(self, name):\n        self._validate_thermostat_state_name(name)\n        return next((state for state in self.thermostat_states\n                     if state.name.lower() == name.lower()), None)", "response": "Retrieves a thermostat state object by its assigned name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_thermostat_state_by_id(self, id_):\n        return next((state for state in self.thermostat_states\n                     if state.id == id_), None)", "response": "Retrieves a thermostat state object by its id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef thermostat_state(self, name):\n        self._validate_thermostat_state_name(name)\n        id_ = next((key for key in STATES.keys()\n                    if STATES[key].lower() == name.lower()), None)\n        data = copy.copy(self._parameters)\n        data.update({'state': 2,\n                     'temperatureState': id_})\n        response = self._get_data('/client/auth/schemeState', data)\n        self._logger.debug('Response received {}'.format(response))\n        self._clear_cache()", "response": "Changes the thermostat state to the one passed as an argument as name\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef thermostat(self, temperature):\n        target = int(temperature * 100)\n        data = copy.copy(self._parameters)\n        data.update({'value': target})\n        response = self._get_data('/client/auth/setPoint', data)\n        self._logger.debug('Response received {}'.format(response))\n        self._clear_cache()", "response": "Sets the thermostat of the current system. Requires a float."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rec_update(self, other, **third):\n        try:\n            iterator = other.iteritems()\n        except AttributeError:\n            iterator = other\n        self.iter_rec_update(iterator)\n        self.iter_rec_update(third.iteritems())", "response": "Recursively update the dictionary with the contents of other and\n        third like dict. update does - but don t overwrite sub - dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef euler_tour_dfs(G, source=None):\n    if source is None:\n        # produce edges for all components\n        nodes = G\n    else:\n        # produce edges for components with source\n        nodes = [source]\n    yielder = []\n    visited = set()\n    for start in nodes:\n        if start in visited:\n            continue\n        visited.add(start)\n        stack = [(start, iter(G[start]))]\n        while stack:\n            parent, children = stack[-1]\n            try:\n                child = next(children)\n                if child not in visited:\n                    # yielder += [[parent, child]]\n                    yielder += [parent]\n                    visited.add(child)\n                    stack.append((child, iter(G[child])))\n            except StopIteration:\n                if stack:\n                    last = stack[-1]\n                    yielder += [last[0]]\n                stack.pop()\n    return yielder", "response": "adaptation of networkx dfs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef comparison():\n    n = 12\n    a, b = 9, 20\n    num = 3\n\n    import utool\n    for timer in utool.Timerit(num, 'old bst version (PY)'):\n        g = nx.balanced_tree(2, n)\n        self = TestETT.from_tree(g, version='bst', fast=False)\n        with timer:\n            self.delete_edge_bst_version(a, b, bstjoin=False)\n\n    import utool\n    for timer in utool.Timerit(num, 'new bst version (PY) (with join)'):\n        g = nx.balanced_tree(2, n)\n        self = TestETT.from_tree(g, version='bst', fast=False)\n        with timer:\n            self.delete_edge_bst_version(a, b, bstjoin=True)\n\n    import utool\n    for timer in utool.Timerit(num, 'old bst version (C)'):\n        g = nx.balanced_tree(2, n)\n        self = TestETT.from_tree(g, version='bst', fast=True)\n        with timer:\n            self.delete_edge_bst_version(a, b, bstjoin=False)\n\n    import utool\n    for timer in utool.Timerit(num, 'new bst version (C) (with join)'):\n        g = nx.balanced_tree(2, n)\n        self = TestETT.from_tree(g, version='bst', fast=True)\n        with timer:\n            self.delete_edge_bst_version(a, b, bstjoin=True)\n\n    import utool\n    for timer in utool.Timerit(num, 'list version'):\n        g = nx.balanced_tree(2, n)\n        self = TestETT.from_tree(g, version='list')\n        with timer:\n            self.delete_edge_list_version(a, b)\n    pass", "response": "r Compare two sets of objects in a tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncutting edge a and b into two parts because this is a tree", "response": "def cut(self, a, b, bstjoin=False):\n        \"\"\"\n        cuts edge (a, b) into two parts because this is a tree\n\n        a, b = (2, 5)\n        print(self.first_lookup[a] > self.first_lookup[b])\n        tree = self.tour_tree\n        list(tree.item_slice(k1, k2))\n        \"\"\"\n        if self.first_lookup[a] > self.last_lookup[b]:\n            a, b = b, a\n\n        o_a1 = self.first_lookup[a]\n        o_a2 = self.last_lookup[a]\n        o_b1 = self.first_lookup[b]\n        o_b2 = self.last_lookup[b]\n        assert o_a1 < o_b1\n        # assert o_b1 < o_b2\n        assert o_b2 < o_a2\n\n        # splice out the inside contiguous range inplace\n        inside, outside = self.tour_tree.splice_inplace(o_b1, o_b2 + 1)\n        # Remove unneeded values\n        outside = outside.splice_inplace(o_b1, o_a2 + 1)[1]\n\n        other = EulerTourTree()\n        other.tour_tree = inside\n        # We can reuse these pointers without any modification\n        other.first_lookup = self.first_lookup\n        other.first_lookup = self.last_lookup\n        # Should make an O(n) cleanup step at some point\n        return other"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reroot(self, s):\n        # Splice out the first part of the sequence ending with the occurrence before os\n        # remove its first occurrence (or),\n        o_s1 = self.first_lookup[s]\n        splice1 = self.tour[1:o_s1]\n        rest = self.tour[o_s1 + 1:]\n        new_tour = [s] + rest + splice1 + [s]\n        new_tree = TestETT.from_tour(new_tour, fast=self.fast)\n        return new_tree", "response": "Return a new tree from the current tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_edge(self, u, v):\n        # ubin = self.find_root(u)\n        ru = self.find_root(u)\n        rv = self.find_root(v)\n        ru = self.reroot(ru, u)\n        rv = self.reroot(rv, v)\n\n        ubin.set_child(vbin)\n\n        pass", "response": "add an edge between two nodes u and v"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an edge between nodes u and v.", "response": "def add_edge(self, u, v):\n        \"\"\"\n        O(log(n))\n        \"\"\"\n        # print('add_edge u, v = %r, %r' % (u, v,))\n        if self.graph.has_edge(u, v):\n            return\n        for node in (u, v):\n            if not self.graph.has_node(node):\n                self.graph.add_node(node)\n            for Fi in self.forests:\n                Fi.add_node(node)\n        # First set the level of (u, v) to 0\n        self.level[(u, v)] = 0\n        # update the adjacency lists of u and v\n        self.graph.add_edge(u, v)\n        # If u and v are in separate trees in F_0, add e to F_0\n        ru = self.forests[0].find_root(u)\n        rv = self.forests[0].find_root(v)\n        if ru is not rv:\n            # If they are in different connected compoments merge compoments\n            self.forests[0].add_edge(u, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_edge(self, u, v):\n        # Remove (u, v) from represented graph\n        print('Dynamically removing uv=(%r, %r)' % (u, v))\n        self.graph.remove_edge(u, v)\n        e = (u, v)\n        # Remove edge e = (u, v) from all graphs.\n        if not self.forests[0].has_edge(u, v):\n            # If (u, v) is a non-tree edge, simply delete it.\n            # Nothing else to do.\n            return\n        # If (u, v) is a tree edge we delete it and search for a replacement.\n        # Delete from all higher levels\n        for i in reversed(range(0, self.level[e] + 1)):\n            self.forests[i].remove_edge(u, v)\n\n        # Determine if another edge that connects u and v exists.\n        # (This must be an edge r, level[r] <= level[e])\n        # (Find max possible level[r] <= level[e])\n        for i in reversed(range(0, self.level[e] + 1)):\n            # Tu != Tw b/c (u, v) was just deleted from all forests\n            Tu = self.forests[i].subtree(u)\n            print('Tu = %r' % (list(Tu.nodes()),))\n            Tv = self.forests[i].subtree(v)\n            print('Tv = %r' % (list(Tv.nodes()),))\n            # Relabel so len(Tu) <= len(Tv)\n            # This ensures len(Tu) < 2 ** (floor(log(n)) - i)\n            if len(Tu) > len(Tv):\n                Tu, Tv = Tv, Tu\n                # Note len(Tu) <= 2 * (len(Tu) + len(Tv) + 1)\n            # We can afford to push all of Tu's edges to the next level and\n            # still preserve invariant 1.\n            seen_ = set([])\n            for x in Tu.nodes():\n                # Visit all edges INCIDENT (in real graph) to nodes in Tu.\n                # This lets us find non-tree edges to make a tree edge\n                seen_.add(x)\n                for y in self.graph.neighbors(x):\n                    if y in seen_:\n                        continue\n                    # print('Check replacement edge xy=(%r, %r)' % (x, y))\n                    if y in Tv:\n                        print('* Found replacement xy=(%r, %r)' % (x, y))\n                        # edge (x, y) is a replacement edge.\n                        # add (x, y) to prev forests F[0:i+1]\n                        # This is the only place edges are added to forets of\n                        # higher levels.\n                        if len(self.forests) == i + 1:\n                            self.forests.append(DummyEulerTourForest(self.graph.nodes()))\n                        for j in range(0, i + 2):\n                            print('* Add replacment to F[j=%r]' % (j,))\n                            # Need euler tree augmentation for outgoing level edges\n                            self.forests[j].add_edge(x, y)\n                        return\n                    else:\n                        print('* Charging xy=(%r, %r)' % (x, y))\n                        # charge --- add (x, y) to next level\n                        # this pays for our search in an amortized sense\n                        # (ie, the next search at this level wont consider this)\n                        if len(self.forests) == i + 1:\n                            self.forests.append(DummyEulerTourForest(self.graph.nodes()))\n                        if self.forests[i].has_edge(x, y):\n                            self.forests[i + 1].add_edge(x, y)\n                        #     # assert False, 'we got it, should add it?'\n                        self.level[(x, y)] = i + 1", "response": "Remove an edge from the nontree and return the F\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if vertices u and v are connected.", "response": "def is_connected(self, u, v):\n        \"\"\"\n        Check if vertices u and v are connected.\n        Query top level forest F[0] to see if u and v are in the same tree.\n        This can be done by checking F_{log(n)} if Findroot(u) = Findroot(v).\n        This costs O(log(n) / log(log(n))) using B-tree based Euler-Tour trees.\n        but this trades off with a O(log^2(n)/log(log(n))) update\n        This is O(log(n)) otherwise\n        \"\"\"\n        ru = self.forests[0].find_root(u)\n        rv = self.forests[0].find_root(v)\n        return ru == rv"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_text_to_varname(text):\n    varname_pattern = '[^a-zA-Z0-9_]'\n    varname = text\n    # Convert spaces to underscores\n    varname = varname.replace(' ', '_')\n    # Remove all other non-varname chars\n    varname = re.sub(varname_pattern, '', varname)\n    # Make sure there are not leading numbers\n    if re.match('^[0-9]', varname):\n        varname = '_' + varname\n    assert re.match(REGEX_VARNAME, varname), 'invalid varname=%r' % (varname,)\n    return varname", "response": "r Convert text to a valid variablename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extend_regex2(regexpr, reflags=0):\n    regexpr = extend_regex(regexpr)\n    IGNORE_CASE_PREF = '\\\\c'\n    if regexpr.startswith(IGNORE_CASE_PREF):\n        # hack for vim-like ignore case\n        regexpr = regexpr[len(IGNORE_CASE_PREF):]\n        reflags = reflags | re.IGNORECASE\n    return regexpr, reflags", "response": "extend_regex2 - extend regexpr to include flags"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a named regex group that can be used to match a field in a backref.", "response": "def named_field(key, regex, vim=False):\n    \"\"\"\n    Creates a named regex group that can be referend via a backref.\n    If key is None the backref is referenced by number.\n\n    References:\n        https://docs.python.org/2/library/re.html#regular-expression-syntax\n    \"\"\"\n    if key is None:\n        #return regex\n        return r'(%s)' % (regex,)\n    if vim:\n        return r'\\(%s\\)' % (regex)\n    else:\n        return r'(?P<%s>%s)' % (key, regex)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef regex_replace(regex, repl, text):\n    return re.sub(regex, repl, text, **RE_KWARGS)", "response": "r Replace pattern with this\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a regex for the named fields of a set of named patterns", "response": "def named_field_regex(keypat_tups):\n    \"\"\"\n    named_field_regex\n\n    Args:\n        keypat_tups (list): tuples of (name, pattern) or a string for an unnamed\n        pattern\n\n    Returns:\n        str: regex\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_regex import *  # NOQA\n        >>> keypat_tups = [\n        ...     ('name',  r'G\\d+'),  # species and 2 numbers\n        ...     ('under', r'_'),     # 2 more numbers\n        ...     ('id',    r'\\d+'),   # 2 more numbers\n        ...     ( None,   r'\\.'),\n        ...     ('ext',   r'\\w+'),\n        ... ]\n        >>> regex = named_field_regex(keypat_tups)\n        >>> result = (regex)\n        >>> print(result)\n        (?P<name>G\\d+)(?P<under>_)(?P<id>\\d+)(\\.)(?P<ext>\\w+)\n    \"\"\"\n    # Allow for unnamed patterns\n    keypat_tups_ = [(None, tup) if isinstance(tup, six.string_types) else tup\n                    for tup in keypat_tups]\n    named_fields = [named_field(key, pat) for key, pat in keypat_tups_]\n    regex = ''.join(named_fields)\n    return regex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef named_field_repl(field_list):\n    # Allow for unnamed patterns\n    bref_field_list = [\n        backref_field(key[0]) if isinstance(key, tuple) else key\n        for key in field_list\n    ]\n    repl = ''.join(bref_field_list)\n    return repl", "response": "r Replaces all named keywords with unnamed patterns"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the docstring of the function.", "response": "def parse_docblock(func_code):\n    \"\"\"\n    #TODO: Finish me\n\n    References:\n        http://pyparsing.wikispaces.com/share/view/1264103\n        http://code.activestate.com/recipes/576704-python-code-minifier/\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import utool as ut\n        >>> import inspect\n        >>> func_code = inspect.getsource(ut.modify_quoted_strs)\n        >>> func_code =\n    \"\"\"\n    import pyparsing\n    doublequote_comment = pyparsing.QuotedString(quoteChar='\"\"\"', escChar='\\\\', multiline=True)\n    singlequote_comment = pyparsing.QuotedString(quoteChar='\\'\\'\\'', escChar='\\\\', multiline=True)\n    docblock_parser  = doublequote_comment | singlequote_comment\n    docblock_parser.parseString(func_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the python syntax of the current language.", "response": "def parse_python_syntax(text):\n    \"\"\"\n    step1: split lines\n\n    step2: parse enclosure pairity for each line to find unended lines\n\n    for each unending line, is there a valid merge line?\n    (a line that could snytatically finish this runnon statement?\n    If no then error. Else try to join the two lines.\n\n    step3: perform context_sensitive_edit\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_regex import *   # NOQA\n        >>> import utool\n        >>> from os.path import normpath\n        >>> text = utool.read_from(utool.util_regex.__file__)\n    \"\"\"\n    import utool as ut\n    def find_all(a_str, sub):\n        start = 0\n        while True:\n            start = a_str.find(sub, start)\n            if start == -1:\n                return\n            yield start\n            start += len(sub)  # use start += 1 to find overlapping matches\n    line_list_ = [line + '\\n' for line in text.splitlines()]  # NOQA\n    import re  # NOQA\n    line_list = [line[0:line.find('#')] for line in line_list_]\n    open_tokens  = ['\\'\\'\\'', '\"\"\"', '\\'', '\"', '(', '[']   # ,  '#']\n    close_tokens = ['\\'\\'\\'', '\"\"\"', '\\'', '\"', ')', ']']   # , '\\n']\n    def find_token_pos(line, token):\n        return list(find_all(line, token))\n    open_tokenxs  = [[find_token_pos(line, token) for line in line_list] for token in open_tokens]\n    close_tokenxs = [[find_token_pos(line, token) for line in line_list] for token in close_tokens]\n    print(open_tokenxs)\n    print(close_tokenxs)\n    print(sum(ut.flatten(ut.flatten(open_tokenxs))))\n    print(sum(ut.flatten(ut.flatten(close_tokenxs))))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding peptide table has no input file (though it has a lookup), which is why we set it to outfile name so the infile fetching and outfile creating wont error.", "response": "def parse_input(self, **kwargs):\n        \"\"\"Build peptide table has no input file (though it has a lookup),\n        which is why we set it to outfile name so the infile fetching\n        and outfile creating wont error.\"\"\"\n        super().parse_input(**kwargs)\n        self.fn = os.path.join(os.getcwd(), 'built_peptide_table.txt')\n        if self.genecentric:\n            self.lookuptype = 'peptidegenecentrictable'\n        elif self.noncentric:\n            self.genecentric = 'plain'\n            self.lookuptype = 'peptidetableplain'"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_feature_generator(self):\n        self.features = preparation.build_peptidetable(self.lookup,\n                                                       self.headerfields,\n                                                       self.isobaric,\n                                                       self.precursor,\n                                                       self.fdr, self.pep,\n                                                       self.genecentric)", "response": "Generates proteins with quant from the lookup table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear(prompt=True, cache=None):\n    cache = cache or config.cache()\n    if prompt:\n        answer = input(\n            'Clear library cache files in %s/? (yN) ' % cache)\n        if not answer.startswith('y'):\n            return False\n    shutil.rmtree(cache, ignore_errors=True)\n    return True", "response": "Clear loady s cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Library from a git path.", "response": "def create(gitpath, cache=None):\n    \"\"\"\n    Create a Library from a git path.\n\n    \"\"\"\n    if gitpath.startswith(config.LIBRARY_PREFIX):\n        path = gitpath[len(config.LIBRARY_PREFIX):]\n        return Library(*path.split('/'), cache=cache)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pull(self):\n        git.Repo(self.path).remote().pull(self.branch)", "response": "Pull the git repo from its origin."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_existens_of_staging_tag_in_remote_repo():\n        staging_tag = Git.create_git_version_tag(APISettings.GIT_STAGING_PRE_TAG)\n\n        command_git = 'git ls-remote -t'\n        command_awk = 'awk \\'{print $2}\\''\n        command_cut_1 = 'cut -d \\'/\\' -f 3'\n        command_cut_2 = 'cut -d \\'^\\' -f 1'\n        command_sort = 'sort -b -t . -k 1,1nr -k 2,2nr -k 3,3r -k 4,4r -k 5,5r'\n        command_uniq = 'uniq'\n\n        command = command_git + ' | ' + command_awk + ' | ' + command_cut_1 + ' | ' + \\\n                  command_cut_2 + ' | ' + command_sort + ' | ' + command_uniq\n\n        list_of_tags = str(check_output(command, shell=True))\n\n        if staging_tag in list_of_tags:\n            return True\n        return False", "response": "This method will check if the given tag exists as a staging tag in the remote repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds files to staging.", "response": "def __git_add(args=''):\n        \"\"\"\n        Add files to staging.\n        The function call will return 0 if the command success.\n        \"\"\"\n        command = ['git', 'add', '.']\n        Shell.msg('Adding files...')\n\n        if APISettings.DEBUG:\n            Git.__debug(command, True)\n\n        for key in args:\n            command.append(key)\n        if not call(command):\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __git_commit(git_tag):\n        Shell.msg('Commit changes.')\n        if APISettings.DEBUG:\n            Shell.debug('Execute \"git commit\" in dry mode.')\n            if not call(['git', 'commit', '-m', '\\'' + git_tag + '\\'', '--dry-run']):\n                pass\n            return True\n\n        if not call(['git', 'commit', '-m', '\\'' + git_tag + '\\'']):\n            return True\n        return False", "response": "Commit the changes to branch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate new tag. The function call will return 0 if the command success.", "response": "def __git_tag(git_tag):\n        \"\"\"\n        Create new tag.\n        The function call will return 0 if the command success.\n        \"\"\"\n        command = ['git', 'tag', '-a', git_tag, '-m', '\\'' + git_tag + '\\'']\n        Shell.msg('Create tag from version ' + git_tag)\n\n        if APISettings.DEBUG:\n            Git.__debug(command, False)\n\n        if not call(command):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npushing all tags. The function call will return 0 if the command success.", "response": "def __git_tag_push():\n        \"\"\"\n        Push all tags.\n        The function call will return 0 if the command success.\n        \"\"\"\n        command = ['git', 'push', 'origin', '--tags']\n        Shell.msg('Pushing tags...')\n\n        if APISettings.DEBUG:\n            Git.__debug(command, True)\n\n        if not call(command):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tag(self, deploy_tag=''):\n        if APISettings.SECURE_TAGGING and deploy_tag == APISettings.GIT_ACTIVATE_PRE_TAG:\n            if self.check_existens_of_staging_tag_in_remote_repo():\n                pass\n            else:\n                Shell.fail('SECURE TAGGING is TRUE! That means, before you are able to create a production tag, ' \\\n                           'you need to deploy the software on a staging envirnment.')\n                return False\n        if self.__git_tag(self.create_git_version_tag(deploy_tag)):\n            return True\n        return False", "response": "Function is public. This function is public."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning is public. Tag delete.", "response": "def tag_delete(self, tag):\n        \"\"\"\n        Function is public.\n        Push tags.\n        :return:\n        \"\"\"\n        if tag:\n            if self.__git_tag_delete(tag):\n                return True\n            return False\n\n        if self.__git_tag_delete(self.get_latest_tag()):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the input data into smaller batches.", "response": "def split_into_batches(input_list, batch_size, batch_storage_dir, checkpoint=False):\n    \"\"\"\n    Break the input data into smaller batches, optionally saving each one to disk.\n\n    Args:\n        input_list: An input object that has a list-like interface (indexing and slicing).\n        batch_size: The maximum number of input items in each batch.\n        batch_storage_dir: The directory to save the checkpoints to.\n        checkpoint: Whether to save each batch to a file.\n\n    Returns:\n        A list of batch objects with the following structure:\n        {'index', 'data', 'input_filename', 'result_filename'}\n    \"\"\"\n\n    if checkpoint and not os.path.exists(batch_storage_dir):\n        os.mkdir(batch_storage_dir)\n\n    batches = [\n        {\n            'index': batch_index,\n            'data': input_list[start_index:start_index + batch_size],\n            'input_filename': os.path.join(batch_storage_dir, 'batch-{:05d}-input.pickle'.format(batch_index)),\n            'result_filename': os.path.join(batch_storage_dir, 'batch-{:05d}-output.pickle'.format(batch_index)),\n        }\n        for batch_index, start_index in enumerate(range(0, len(input_list), batch_size))\n    ]\n\n    if checkpoint:\n        for batch in batches:\n            save(batch['data'], batch['input_filename'])\n\n    return batches"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess items in a list in parallel (optionally, one smaller batch at a time). Args: input_list: An input object that has a list-like interface (indexing and slicing). mapper: A function to apply to each item of the input list. project: An instance of pygoose project. n_jobs: The number of parallel processing jobs. -1 will use the number of CPUs on the system. batch_size: The maximum number of input items in each batch. -1 will store all data as a single batch. checkpoint: Whether to save each batch and its corresponding output to disk. cleanup: Whether to remove the batch checkpoints from the disk after all batches are processed. **kwargs: Additional keyword arguments to joblib.Parallel. Returns: A list representing the combined output from the mapper function called on all input items.", "response": "def map_embarrassingly_parallel(input_list, mapper, project, n_jobs=-1, batch_size=-1,\n                                checkpoint=False, cleanup=True, **kwargs):\n    \"\"\"\n    Process items in a list in parallel (optionally, one smaller batch at a time).\n\n    Args:\n        input_list: An input object that has a list-like interface (indexing and slicing).\n        mapper: A function to apply to each item of the input list.\n        project: An instance of pygoose project.\n        n_jobs: The number of parallel processing jobs. -1 will use the number of CPUs on the system.\n        batch_size: The maximum number of input items in each batch. -1 will store all data as a single batch.\n        checkpoint: Whether to save each batch and its corresponding output to disk.\n        cleanup: Whether to remove the batch checkpoints from the disk after all batches are processed.\n        **kwargs: Additional keyword arguments to joblib.Parallel.\n\n    Returns:\n        A list representing the combined output from the mapper function called on all input items.\n    \"\"\"\n\n    if batch_size < 0:\n        batch_size = len(input_list)\n\n    # Partition the data.\n    job_id = _create_job_id()\n    print('Creating job ID:', job_id)\n\n    batch_storage_dir = os.path.join(project.temp_dir, job_id)\n    batches = split_into_batches(input_list, batch_size, batch_storage_dir, checkpoint)\n\n    # The results will be collected here.\n    # TODO: collecting lists like this may be memory inefficient. Perhaps we could use another callback function.\n    combined_results = []\n\n    # Process data one batch at a time.\n    for batch in batches:\n        description = 'Batch {}/{}'.format(batch['index'] + 1, len(batches))\n\n        # Process each item in the batch in parallel.\n        batch_result = Parallel(n_jobs=n_jobs, **kwargs)(\n            delayed(mapper)(input_item)\n            for input_item in progressbar(\n                batch['data'],\n                desc=description,\n                total=len(batch['data']),\n                file=sys.stdout,\n            )\n        )\n        if checkpoint:\n            save(batch_result, batch['result_filename'])\n\n        combined_results.extend(batch_result)\n\n    # Remove the temporary files.\n    if checkpoint and cleanup:\n        shutil.rmtree(batch_storage_dir)\n\n    return combined_results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmaps a list of items into a single list of items in a single thread.", "response": "def map_batch_parallel(input_list, batch_size, item_mapper=None, batch_mapper=None, flatten=True, n_jobs=-1, **kwargs):\n    \"\"\"\n    Split the data into batches and process each batch in its own thread.\n\n    Args:\n        input_list: An input object that has a list-like interface (indexing and slicing).\n        item_mapper: (optional) A function to apply to each item in the batch.\n        batch_mapper: (optional) A function to apply to each batch. Either item_mapper or batch_mapper must be set.\n        flatten: Whether to unwrap individual batch results or keep them grouped by batch.\n        n_jobs: The number of parallel processing jobs. -1 will use the number of CPUs on the system.\n        batch_size: The maximum number of input items in each batch. -1 will store all data as a single batch.\n        **kwargs: Additional keyword arguments to joblib.Parallel.\n\n    Returns:\n        A list representing the combined output from the mapper function called on all input items of each batch.\n    \"\"\"\n\n    # We must specify either how to process each batch or how to process each item.\n    if item_mapper is None and batch_mapper is None:\n        raise ValueError('You should specify either batch_mapper or item_mapper.')\n\n    if batch_mapper is None:\n        batch_mapper = _default_batch_mapper\n\n    batches = split_into_batches(input_list, batch_size, batch_storage_dir='')\n    all_batch_results = Parallel(n_jobs=n_jobs, **kwargs)(\n        delayed(batch_mapper)(batch['data'], item_mapper)\n        for batch in progressbar(\n            batches,\n            desc='Batches',\n            total=len(batches),\n            file=sys.stdout,\n        )\n    )\n\n    # Unwrap the individual batch results if necessary.\n    if flatten:\n        final_result = []\n        for batch_result in all_batch_results:\n            final_result.extend(batch_result)\n    else:\n        final_result = all_batch_results\n\n    return final_result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_cfg(ast_func):\n    cfg_func = cfg.Function()\n    for ast_var in ast_func.input_variable_list:\n        cfg_var = cfg_func.get_variable(ast_var.name)\n        cfg_func.add_input_variable(cfg_var)\n    for ast_var in ast_func.output_variable_list:\n        cfg_var = cfg_func.get_variable(ast_var.name)\n        cfg_func.add_output_variable(cfg_var)\n    bb_start  = cfg.BasicBlock()\n    cfg_func.add_basic_block(bb_start)\n    for stmt in ast_func.body:\n        bb_temp = bb_start\n        bb_temp = process_cfg(stmt, bb_temp, cfg_func)\n    cfg_func.clean_up()\n    cfg_func.add_summary(ast_func.summary)\n    return cfg_func", "response": "This function traverses the AST and returns the corresponding CFG"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef overrideable_partial(func, *args, **default_kwargs):\n    import functools\n    @functools.wraps(func)\n    def partial_wrapper(*given_args, **given_kwargs):\n        kwargs = default_kwargs.copy()\n        kwargs.update(given_kwargs)\n        return func(*(args + given_args), **kwargs)\n    return partial_wrapper", "response": "like partial but given kwargs can be overrideden at calltime"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_nonconflicting_string(base_fmtstr, conflict_set, offset=0):\n    # Infinite loop until we find a non-conflict\n    conflict_set_ = set(conflict_set)\n    for count in it.count(offset):\n        base_str = base_fmtstr % count\n        if base_str not in conflict_set_:\n            return base_str", "response": "Get a new string that doesn t conflict with something that already exists in the set"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nonconflicting_path_old(base_fmtstr, dpath, offset=0):\n    import utool as ut\n    from os.path import basename\n\n    pattern = '*'\n    dname_list = ut.glob(dpath, pattern, recursive=False,\n                               with_files=True, with_dirs=True)\n    conflict_set = set([basename(dname) for dname in dname_list])\n\n    newname = ut.get_nonconflicting_string(base_fmtstr, conflict_set,\n                                           offset=offset)\n    newpath = join(dpath, newname)\n    return newpath", "response": "r Returns the path to the non - conflicting file in the given base_fmtstr."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to wait for input and return the next available keyboard key.", "response": "def input_timeout(msg='Waiting for input...', timeout=30):\n    \"\"\"\n    FIXME: Function does not work quite right yet.\n\n    Args:\n        msg (str):\n        timeout (int):\n\n    Returns:\n        ?: ans\n\n    References:\n        http://stackoverflow.com/questions/1335507/keyboard-input-with-timeout-in-python\n        http://home.wlu.edu/~levys/software/kbhit.py\n        http://stackoverflow.com/questions/3471461/raw-input-and-timeout/3911560#3911560\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> msg = 'Waiting for input...'\n        >>> timeout = 30\n        >>> ans = input_timeout(msg, timeout)\n        >>> print(ans)\n    \"\"\"\n    import sys\n    import select\n    import time\n    ans = None\n    print('You have %d seconds to answer!' % timeout)\n    print(msg)\n    if sys.platform.startswith('win32'):\n        import msvcrt\n        start_time = time.time()\n        instr = ''\n        while True:\n            if msvcrt.kbhit():\n                chr_ = msvcrt.getche()\n                if ord(chr_) == 13:  # enter_key\n                    # Accept input\n                    ans = instr\n                    break\n                elif ord(chr_) >= 32:  # space_char\n                    # Append to input\n                    instr += chr_\n            ellapsed = time.time() - start_time\n            if ellapsed > timeout:\n                ans = None\n        print('')  # needed to move to next line\n    else:\n        rlist, o, e = select.select([sys.stdin], [], [], timeout)\n        if rlist:\n            ans = sys.stdin.readline().strip()\n    return ans"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef timeit_compare(stmt_list, setup='', iterations=100000, verbose=True,\n                   strict=False, assertsame=True):\n    \"\"\"\n    Compares several statments by timing them and also\n    checks that they have the same return value\n\n    Args:\n        stmt_list (list) : list of statments to compare\n        setup (str) :\n        iterations (int) :\n        verbose (bool) :\n        strict (bool) :\n\n    Returns:\n        tuple (bool, list, list) : (passed, time_list, result_list)\n            passed (bool): True if all results are the same\n            time_list (list): list of times for each statment\n            result_list (list): list of results values for each statment\n\n    CommandLine:\n        python -m utool.util_dev --exec-timeit_compare\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import utool as ut\n        >>> setup = ut.codeblock(\n                '''\n                import numpy as np\n                rng = np.random.RandomState(0)\n                invVR_mats = rng.rand(1000, 3, 3).astype(np.float64)\n                ''')\n        >>> stmt1 = 'invVR_mats[:, 0:2, 2].T'\n        >>> stmt2 = 'invVR_mats.T[2, 0:2]'\n        >>> iterations = 1000\n        >>> verbose = True\n        >>> stmt_list = [stmt1, stmt2]\n        >>> ut.timeit_compare(stmt_list, setup=setup, iterations=iterations, verbose=verbose)\n    \"\"\"\n    import timeit\n    import utool as ut\n\n    stmt_list = [s for s in stmt_list if not s.startswith('#')]\n\n    for stmtx in range(len(stmt_list)):\n        # Hacky way of removing assignment and just getting statement\n        # We have to make sure it is ok when using it for kwargs\n        stmt = stmt_list[stmtx]\n        eqpos = stmt.find('=')\n        lparen_pos = stmt.find('(')\n        if eqpos > 0 and (lparen_pos == -1 or lparen_pos > eqpos):\n            stmt = '='.join(stmt.split('=')[1:])\n            stmt_list[stmtx] = stmt\n\n    if verbose:\n        print('+----------------')\n        print('| TIMEIT COMPARE')\n        print('+----------------')\n        print('| iterations = %d' % (iterations,))\n        print('| Input:')\n        #print('|     +------------')\n        print('|     | num | stmt')\n        for count, stmt in enumerate(stmt_list):\n            print('|     | %3d | %r' % (count, stmt))\n        print('...')\n        sys.stdout.flush()\n        #print('+     L________________')\n\n    if assertsame:\n        result_list = [_testit(stmt, setup) for stmt in stmt_list]\n    else:\n        result_list = None\n    time_list   = [timeit.timeit(stmt, setup=setup, number=iterations)\n                   for stmt in stmt_list]\n\n    def numpy_diff_tests(result_list):\n        print('Testing numpy arrays')\n        shape_list = [a.shape for a in result_list]\n        print('shape_list = %r' % (shape_list,))\n        sum_list = [a.sum() for a in result_list]\n        diff_list = [np.abs(a - b) for a, b in ut.itertwo(result_list)]\n        print('diff stats')\n        for diffs in diff_list:\n            print(ut.repr4(ut.get_stats(diffs, precision=2, use_median=True)))\n        print('diff_list = %r' % (diff_list,))\n        print('sum_list = %r' % (sum_list,))\n        print('passed_list = %r' % (passed_list,))\n\n    if assertsame:\n        if ut.list_type(result_list) is np.ndarray:\n            passed_list = [np.allclose(*tup) for tup in ut.itertwo(result_list)]\n            passed = all(passed_list)\n            is_numpy = True\n        else:\n            passed = ut.allsame(result_list, strict=False)\n            is_numpy = False\n    else:\n        passed = True\n    if verbose:\n        print('| Output:')\n        if not passed and assertsame:\n            print('|    * FAILED: results differ between some statements')\n            if is_numpy:\n                numpy_diff_tests(result_list)\n            print('| Results:')\n            for result in result_list:\n                for count, result in enumerate(result_list):\n                    print('<Result %d>' % count)\n                    print(result)\n                    #print(ut.truncate_str(repr(result)))\n                    print('</Result %d>' % count)\n            if strict:\n                raise AssertionError('Results are not valid')\n        else:\n            if assertsame:\n                print('|    * PASSED: each statement produced the same result')\n            else:\n                print('|    * PASSED: each statement did not error')\n            passed = True\n        #print('|    +-----------------------------------')\n        print('|    | num | total time | per loop | stmt')\n        for count, tup in enumerate(zip(stmt_list, time_list)):\n            stmt, time = tup\n            print('|    | %3d | %10s | %8s | %s' %\n                  (count, ut.seconds_str(time),\n                   ut.seconds_str(time / iterations), stmt))\n        #print('|    L___________________________________')\n        if verbose:\n            print('L_________________')\n        return (passed, time_list, result_list)", "response": "This function is used to compare several statments by timing them and then checking that they have the same value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef memory_dump():\n    import cPickle\n    dump = open(\"memory.pickle\", 'w')\n    for obj in gc.get_objects():\n        i = id(obj)\n        size = sys.getsizeof(obj, 0)\n        #    referrers = [id(o) for o in gc.get_referrers(obj) if hasattr(o, '__class__')]\n        referents = [id(o) for o in gc.get_referents(obj) if hasattr(o, '__class__')]\n        if hasattr(obj, '__class__'):\n            cls = str(obj.__class__)\n            cPickle.dump({'id': i, 'class': cls, 'size': size, 'referents': referents}, dump)", "response": "Dump the memory of the objects in the\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef are_you_sure(msg=''):\n    print(msg)\n    from utool import util_arg\n    from utool import util_str\n    override = util_arg.get_argflag(('--yes', '--y', '-y'))\n    if override:\n        print('accepting based on command line flag')\n        return True\n    valid_ans = ['yes', 'y']\n    valid_prompt = util_str.conj_phrase(valid_ans, 'or')\n    ans = input('Are you sure?\\n Enter %s to accept\\n' % valid_prompt)\n    return ans.lower() in valid_ans", "response": "r Prompts user to accept or checks command line for - y\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grace_period(msg='', seconds=10):\n    import time\n    print(msg)\n    override = util_arg.get_argflag(('--yes', '--y', '-y'))\n    print('starting grace period')\n    if override:\n        print('ending based on command line flag')\n        return True\n    for count in reversed(range(1, seconds + 1)):\n        time.sleep(1)\n        print('%d' % (count,))\n    print('%d' % (0,))\n    print('grace period is over')\n    return True", "response": "Gives user a window to stop a process before it happens"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delayed_retry_gen(delay_schedule=[.1, 1, 10], msg=None, timeout=None, raise_=True):\n    import utool as ut\n    import time\n    if not ut.isiterable(delay_schedule):\n        delay_schedule = [delay_schedule]\n    tt = ut.tic()\n\n    # First attempt is immediate\n    yield 0\n\n    for count in it.count(0):\n        #print('count = %r' % (count,))\n        if timeout is not None and ut.toc(tt) > timeout:\n            if raise_:\n                raise Exception('Retry loop timed out')\n            else:\n                raise StopIteration('Retry loop timed out')\n        index = min(count, len(delay_schedule) - 1)\n        delay = delay_schedule[index]\n        time.sleep(delay)\n        yield count + 1", "response": "generator function for delayed retry loop"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_stats(list_, axis=None, use_nan=False, use_sum=False, use_median=False,\n              size=False):\n    \"\"\"\n    Args:\n        list_ (listlike): values to get statistics of\n        axis (int): if `list_` is ndarray then this specifies the axis\n\n    Returns:\n        OrderedDict: stats: dictionary of common numpy statistics\n            (min, max, mean, std, nMin, nMax, shape)\n\n    SeeAlso:\n        get_stats_str\n\n    CommandLine:\n        python -m utool.util_dev --test-get_stats\n        python -m utool.util_dev --test-get_stats:1\n\n    Examples0:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import numpy as np\n        >>> import utool\n        >>> axis = 0\n        >>> np.random.seed(0)\n        >>> list_ = np.random.rand(10, 2).astype(np.float32)\n        >>> stats = get_stats(list_, axis, use_nan=False)\n        >>> result = str(utool.repr4(stats, nl=1, precision=4, with_dtype=True))\n        >>> print(result)\n        {\n            'mean': np.array([0.5206, 0.6425], dtype=np.float32),\n            'std': np.array([0.2854, 0.2517], dtype=np.float32),\n            'max': np.array([0.9637, 0.9256], dtype=np.float32),\n            'min': np.array([0.0202, 0.0871], dtype=np.float32),\n            'nMin': np.array([1, 1], dtype=np.int32),\n            'nMax': np.array([1, 1], dtype=np.int32),\n            'shape': (10, 2),\n        }\n\n    Examples1:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import numpy as np\n        >>> import utool\n        >>> axis = 0\n        >>> rng = np.random.RandomState(0)\n        >>> list_ = rng.randint(0, 42, size=100).astype(np.float32)\n        >>> list_[4] = np.nan\n        >>> stats = get_stats(list_, axis, use_nan=True)\n        >>> result = str(utool.repr2(stats, precision=1, strkeys=True))\n        >>> print(result)\n        {mean: 20.0, std: 13.2, max: 41.0, min: 0.0, nMin: 7, nMax: 3, shape: (100,), num_nan: 1}\n    \"\"\"\n    datacast = np.float32\n    # Assure input is in numpy format\n    if isinstance(list_, np.ndarray):\n        nparr = list_\n    elif isinstance(list_, list):\n        nparr = np.array(list_)\n    else:\n        nparr = np.array(list(list_))\n    # Check to make sure stats are feasible\n    if len(nparr) == 0:\n        stats = OrderedDict([('empty_list', True)])\n        if size:\n            stats['size'] = 0\n    else:\n        if use_nan:\n            min_val = np.nanmin(nparr, axis=axis)\n            max_val = np.nanmax(nparr, axis=axis)\n            mean_ = np.nanmean(nparr, axis=axis)\n            std_  = np.nanstd(nparr, axis=axis)\n        else:\n            min_val = nparr.min(axis=axis)\n            max_val = nparr.max(axis=axis)\n            mean_ = nparr.mean(axis=axis)\n            std_  = nparr.std(axis=axis)\n        # number of entries with min/max val\n        nMin = np.sum(nparr == min_val, axis=axis)\n        nMax = np.sum(nparr == max_val, axis=axis)\n        stats = OrderedDict([\n            ('mean',  datacast(mean_)),\n            ('std',   datacast(std_)),\n            ('max',   (max_val)),\n            ('min',   (min_val)),\n            ('nMin',  np.int32(nMin)),\n            ('nMax',  np.int32(nMax)),\n        ])\n        if size:\n            stats['size'] = nparr.size\n        else:\n            stats['shape'] = nparr.shape\n        if use_median:\n            stats['med'] = np.nanmedian(nparr)\n        if use_nan:\n            stats['num_nan'] = np.isnan(nparr).sum()\n        if use_sum:\n            sumfunc = np.nansum if use_nan else np.sum\n            stats['sum'] = sumfunc(nparr, axis=axis)\n    return stats", "response": "Get statistics of a set of items in a single level of the tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a string version of get_stats_str", "response": "def get_stats_str(list_=None, newlines=False, keys=None, exclude_keys=[], lbl=None,\n                  precision=None, axis=0, stat_dict=None, use_nan=False,\n                  align=False, use_median=False, **kwargs):\n    \"\"\"\n    Returns the string version of get_stats\n\n    DEPRICATE in favor of ut.repr3(ut.get_stats(...))\n\n    if keys is not None then it only displays chosen keys\n    excluded keys are always removed\n\n\n    CommandLine:\n        python -m utool.util_dev --test-get_stats_str\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> list_ = [1, 2, 3, 4, 5]\n        >>> newlines = False\n        >>> keys = None\n        >>> exclude_keys = []\n        >>> lbl = None\n        >>> precision = 2\n        >>> stat_str = get_stats_str(list_, newlines, keys, exclude_keys, lbl, precision)\n        >>> result = str(stat_str)\n        >>> print(result)\n        {'mean': 3, 'std': 1.41, 'max': 5, 'min': 1, 'nMin': 1, 'nMax': 1, 'shape': (5,)}\n\n    SeeAlso:\n        repr2\n        get_stats\n    \"\"\"\n    from utool.util_str import repr4\n    import utool as ut\n    # Get stats dict\n    if stat_dict is None:\n        stat_dict = get_stats(list_, axis=axis, use_nan=use_nan, use_median=use_median)\n    else:\n        stat_dict = stat_dict.copy()\n    # Keep only included keys if specified\n    if keys is not None:\n        for key in list(six.iterkeys(stat_dict)):\n            if key not in keys:\n                del stat_dict[key]\n    # Remove excluded keys\n    for key in exclude_keys:\n        if key in stat_dict:\n            del stat_dict[key]\n    # apply precision\n    statstr_dict = stat_dict.copy()\n    #precisionless_types =  (bool,) + six.string_types\n    if precision is not None:\n        assert ut.is_int(precision), 'precision must be an integer'\n        float_fmtstr = '%.' + str(precision) + 'f'\n        for key in list(six.iterkeys(statstr_dict)):\n            val = statstr_dict[key]\n            isfloat = ut.is_float(val)\n            if not isfloat and isinstance(val, list):\n                type_list = list(map(type, val))\n                if len(type_list) > 0 and ut.allsame(type_list):\n                    if ut.is_float(val[0]):\n                        isfloat = True\n                        val = np.array(val)\n            if isfloat:\n                if isinstance(val, np.ndarray):\n                    strval = str([float_fmtstr % v for v in val]).replace('\\'', '').lstrip('u')\n                    #np.array_str((val), precision=precision)\n                else:\n                    strval = float_fmtstr % val\n                if not strval.startswith('0'):\n                    strval = strval.rstrip('0')\n                    strval = strval.rstrip('.')\n                statstr_dict[key] = strval\n            else:\n                if isinstance(val, np.ndarray):\n                    strval = repr(val.tolist())\n                else:\n                    strval = str(val)\n                statstr_dict[key] = strval\n\n    # format the dictionary string\n    stat_str  = repr4(statstr_dict, strvals=True, newlines=newlines)\n    # add a label if requested\n    if lbl is True:\n        lbl = ut.get_varname_from_stack(list_, N=1)  # fancy\n    if lbl is not None:\n        stat_str = 'stats_' + lbl + ' = ' + stat_str\n    if align:\n        stat_str = ut.align(stat_str, ':')\n    return stat_str"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_call_graph(func, *args, **kwargs):\n    from pycallgraph import PyCallGraph\n    from pycallgraph.output import GraphvizOutput\n    with PyCallGraph(output=GraphvizOutput):\n        func(*args, **kwargs)", "response": "profile with pycallgraph\n\n    Example:\n        pycallgraph graphviz -- ./mypythonscript.py\n\n    References:\n        http://pycallgraph.slowchop.com/en/master/"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_object_graph(obj, fpath='sample_graph.png'):\n    import objgraph\n    objgraph.show_most_common_types()\n    #print(objgraph.by_type('ndarray'))\n    #objgraph.find_backref_chain(\n    #     random.choice(objgraph.by_type('ndarray')),\n    #     objgraph.is_proper_module)\n    objgraph.show_refs([obj], filename='ref_graph.png')\n    objgraph.show_backrefs([obj], filename='backref_graph.png')", "response": "Make a memoryprofile with objgraph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the size of the object in a single page.", "response": "def get_object_nbytes(obj, fallback_type=None, follow_pointers=False, exclude_modules=True, listoverhead=False):\n    \"\"\"\n    CommandLine:\n        python -m utool.util_dev --test-get_object_nbytes\n        python -m utool.util_dev --test-get_object_nbytes:1\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import numpy as np\n        >>> import utool as ut\n        >>> obj = [np.empty(1, dtype=np.uint8) for _ in range(8)]\n        >>> nBytes = ut.get_object_nbytes(obj)\n        >>> result = ('nBytes = %s' % (nBytes,))\n        >>> print(result)\n        nBytes = 8\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # UNSTABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import ibeis\n        >>> import utool as ut\n        >>> species = ibeis.const.TEST_SPECIES.ZEB_PLAIN\n        >>> ibs = ibeis.opendb(defaultdb='testdb1')\n        >>> qaids = ibs.get_valid_aids(species=species)\n        >>> daids = ibs.get_valid_aids(species=species)\n        >>> qreq_ = ibs.new_query_request(qaids, daids, verbose=True)\n        >>> nBytes = ut.get_object_nbytes(qreq_)\n        >>> result = (ut.byte_str2(nBytes))\n        >>> print('result = %r' % (result,))\n\n    Ignore:\n        import sys\n        sizedict = {key: sys.getsizeof(key()) for key in [dict, list, set, tuple, int, float]}\n        ut.print_dict(sizedict)\n        sizedict = {\n            <type 'tuple'>: 56,\n            <type 'set'>: 232,\n            <type 'list'>: 72,\n            <type 'float'>: 24,\n            <type 'int'>: 24,\n            <type 'dict'>: 280,\n        }\n    \"\"\"\n    import utool as ut\n    import types\n    seen = set([])\n    def _object_nbytes(obj):\n        object_id = id(obj)\n        if object_id in seen:\n            return 0\n        if (obj is None or isinstance(obj, (int, bool, float))):\n            return sys.getsizeof(obj)\n        elif isinstance(obj, six.string_types):\n            return sys.getsizeof(obj)\n        seen.add(object_id)\n\n        if listoverhead:\n            totalsize = sys.getsizeof(obj)\n        else:\n            totalsize = 0\n        try:\n            if isinstance(obj, (ut.DynStruct, ut.Pref)):\n                # dont deal with dynstruct shenanigans\n                return\n            elif exclude_modules and isinstance(obj, types.ModuleType):\n                return 0\n            elif isinstance(obj, np.ndarray):\n                if not obj.flags['OWNDATA']:\n                    # somebody else owns the data, returned size may be smaller than sobj.nbytes\n                    # because sys.getsizeof will return the container size.\n                    # if owndata is true sys.getsizeof returns the actual size\n                    if follow_pointers:\n                        totalsize += obj.nbytes\n                    pass\n                # TODO: check if a view or is memmapped\n                # Does sys.getsizeof do the right thing ?\n                totalsize = obj.nbytes\n            elif (isinstance(obj, (tuple, list, set, frozenset))):\n                for item in obj:\n                    totalsize += _object_nbytes(item)\n            elif isinstance(obj, dict):\n                try:\n                    for key, val in six.iteritems(obj):\n                        totalsize += _object_nbytes(key)\n                        totalsize += _object_nbytes(val)\n                except RuntimeError as dictex:\n                    ut.printex(dictex, 'RuntimeError in parsing dict nbytes',\n                               keys=['key', (type, 'obj')], iswarning=True)\n                    raise\n            elif isinstance(obj, object) and hasattr(obj, '__dict__'):\n                if hasattr(obj, 'used_memory') and not isinstance(obj, type):\n                    # hack for flann objects\n                    totalsize += obj.used_memory()\n                totalsize += _object_nbytes(obj.__dict__)\n                return totalsize\n            elif isinstance(obj, type):\n                # use zero for class definitions\n                return 0\n            elif isinstance(obj, np.int32):\n                return obj.nbytes\n            else:\n                print('Unknown type %r for parsing size' % (type(obj),))\n                return 0\n        #except TypeError as ex:\n        except Exception as ex:\n            ut.printex(ex, 'may be an error in _object_nbytes',\n                       keys=[(type, 'obj')], iswarning=True, tb=True)\n            pass\n            #import utool as ut\n            #print('obj = %r' % (obj,))\n            #ut.printex(ex)\n            #ut.embed()\n            #raise RuntimeError(str(ex))  # from ex\n        return totalsize\n    return _object_nbytes(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_at_least_n_items_valid(flag_list, n):\n    flag_list = np.array(flag_list)\n    num_valid = flag_list.sum()\n    # Find how many places we need to make true\n    num_extra = min(len(flag_list) - num_valid, n - num_valid)\n    # make_at_least_n_items_valid\n    # Add in some extra daids to show if there are not enough\n    for index in range(len(flag_list)):\n        if num_extra <= 0:\n            break\n        if not flag_list[index]:\n            flag_list[index] = True\n            num_extra -= 1\n    return flag_list", "response": "This function tries to make at least n items True in flag_list and returns a list of booleans that are True in flag_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple of unique items in the order of the items in the item1_list and item2_list", "response": "def inverable_unique_two_lists(item1_list, item2_list):\n    \"\"\"\n    item1_list = aid1_list\n    item2_list = aid2_list\n    \"\"\"\n    import utool as ut\n    unique_list1, inverse1 = np.unique(item1_list, return_inverse=True)\n    unique_list2, inverse2 = np.unique(item2_list, return_inverse=True)\n\n    flat_stacked, cumsum = ut.invertible_flatten2((unique_list1, unique_list2))\n    flat_unique, inverse3 = np.unique(flat_stacked, return_inverse=True)\n    reconstruct_tup = (inverse3, cumsum, inverse2, inverse1)\n    return flat_unique, reconstruct_tup"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inverable_group_multi_list(item_lists):\n    #unique_list1, inverse1 = np.unique(item1_list, return_index=True, return_inverse=True)\n    import vtool as vt\n    import utool as ut\n    # Find uniques and groups in each individual list\n    unique_lists = []\n    groupx_lists = []\n    for item_list in item_lists:\n        unique_items, groupxs = vt.group_indices(item_list)\n        unique_lists.append(unique_items)\n        groupx_lists.append(groupxs)\n    # Merge all indexes into a signle long list\n    groups_stacked = ut.flatten(groupx_lists)\n    flat_stacked, cumsum = ut.invertible_flatten2(unique_lists)\n    # Find uniques in those lists\n    flat_unique, stack_groups = vt.group_indices(np.array(flat_stacked))\n    # Get a list of corresonding group indicies from each input list\n    flat_groupx_multilist = [ut.take(groups_stacked, groupx) for groupx in stack_groups]\n    # flat_unique corresponds with the aids (hence chips) the flag_groupxs\n    # multilist is a list where each item is a tuple who's nth item indexes\n    # into the nth input list. Ie (1, 0) is a list of indexes into the 1st chip\n    # the 0th keypoint list\n    return flat_unique, flat_groupx_multilist", "response": "This function takes a list of item lists and returns a list of unique items and group indices for each item in the list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autopep8_diff(fpath):\n    import utool as ut\n    args = ('autopep8', fpath, '--diff')\n    res = ut.cmd(args, verbose=False)\n    out, err, ret = res\n    ut.print_difftext(out)", "response": "r Autopep8 diff of a single object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_submodules_from_dpath(dpath, only_packages=False, recursive=True):\n    import utool as ut\n    submod_dpaths = [d for d in ut.ls_dirs(dpath) if ut.is_module_dir(d) ]\n    if only_packages:\n        submod_fpaths = submod_dpaths\n    else:\n        submod_fpaths = ut.ls_modulefiles(dpath)\n    if recursive and len(submod_dpaths) > 0:\n        recusive_results = [get_submodules_from_dpath(d, only_packages)\n                            for d in submod_dpaths]\n        submod_fpaths.extend(ut.flatten(recusive_results))\n    return submod_fpaths", "response": "r Get all submodules of a directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute methods and attribute calls on a list of objects of the same type Bundles a list of object of the same type into a single object. The new object contains the same functions as each original object but applies them to each element of the list independantly when called. CommandLine: python -m utool.util_dev instancelist Example: >>> # ENABLE_DOCTEST >>> from utool.util_dev import * # NOQA >>> import utool as ut >>> obj_list = ['hi', 'bye', 'foo'] >>> self = ut.instancelist(obj_list, check=False) >>> print(self) >>> print(self.upper()) >>> print(self.isalpha())", "response": "def instancelist(obj_list, check=False, shared_attrs=None):\n    \"\"\"\n    Executes methods and attribute calls on a list of objects of the same type\n\n    Bundles a list of object of the same type into a single object.\n    The new object contains the same functions as each original object\n    but applies them to each element of the list independantly when called.\n\n    CommandLine:\n        python -m utool.util_dev instancelist\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import utool as ut\n        >>> obj_list = ['hi', 'bye', 'foo']\n        >>> self = ut.instancelist(obj_list, check=False)\n        >>> print(self)\n        >>> print(self.upper())\n        >>> print(self.isalpha())\n    \"\"\"\n    class InstanceList_(object):\n        def __init__(self, obj_list, shared_attrs=None):\n            self._obj_list = []\n            self._shared_public_attrs = []\n            self._example_type = None\n\n            if len(obj_list) > 0:\n                import utool as ut\n                self._obj_list = obj_list\n\n                example_obj = obj_list[0]\n                example_type = type(example_obj)\n                self._example_type = example_type\n\n                if shared_attrs is None:\n                    if check:\n                        attrsgen = [set(dir(obj)) for obj in obj_list]\n                        shared_attrs = list(reduce(set.intersection, attrsgen))\n                    else:\n                        shared_attrs = dir(example_obj)\n\n                #allowed = ['__getitem__']  # TODO, put in metaclass\n                allowed = []\n                self._shared_public_attrs = [\n                    a for a in shared_attrs\n                    if a in allowed or not a.startswith('_')\n                ]\n\n                for attrname in self._shared_public_attrs:\n                    attrtype = getattr(example_type, attrname, None)\n                    if attrtype is not None and isinstance(attrtype, property):\n                        # need to do this as metaclass\n                        setattr(InstanceList_, attrname,\n                                property(self._define_prop(attrname)))\n                    else:\n                        func = self._define_func(attrname)\n                        ut.inject_func_as_method(self, func, attrname)\n\n        def __nice__(self):\n            if self._example_type is None:\n                typename = 'object'\n            else:\n                typename = self._example_type.__name__\n            return 'of %d %s(s)' % (len(self._obj_list), typename)\n\n        def __repr__(self):\n            classname = self.__class__.__name__\n            devnice = self.__nice__()\n            return '<%s(%s) at %s>' % (classname, devnice, hex(id(self)))\n\n        def __str__(self):\n            classname = self.__class__.__name__\n            devnice = self.__nice__()\n            return '<%s(%s)>' % (classname, devnice)\n\n        def __getitem__(self, key):\n            # TODO, put in metaclass\n            return self._map_method('__getitem__', key)\n\n        def _define_func(self, attrname):\n            import utool as ut\n            def _wrapper(self, *args, **kwargs):\n                return self._map_method(attrname, *args, **kwargs)\n            ut.set_funcname(_wrapper, attrname)\n            return _wrapper\n\n        def _map_method(self, attrname, *args, **kwargs):\n            mapped_vals = [getattr(obj, attrname)(*args, **kwargs)\n                           for obj in self._obj_list]\n            return mapped_vals\n\n        def _define_prop(self, attrname):\n            import utool as ut\n            def _getter(self):\n                return self._map_property(attrname)\n            ut.set_funcname(_getter, 'get_' + attrname)\n            return _getter\n\n        def _map_property(self, attrname):\n            mapped_vals = [getattr(obj, attrname) for obj in self._obj_list]\n            return mapped_vals\n    return InstanceList_(obj_list, shared_attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes an error where reload causes super(X, self) to raise an exception The problem is that reloading causes X to point to the wrong version of the class. This function fixes the problem by searching and returning the correct version of the class. See example for proper usage. USE `ut.super2` INSTEAD Args: this_class (class): class passed into super self (instance): instance passed into super DisableExample: >>> # DISABLE_DOCTEST >>> import utool as ut >>> class Parent(object): >>> def __init__(self): >>> self.parent_attr = 'bar' >>> # >>> class Foo(Parent): >>> def __init__(self): >>> # Dont do this, it will error if you reload >>> # super(Foo, self).__init__() >>> # Do this instead >>> _Foo = ut.super2(Foo, self) >>> super(_Foo, self).__init__() >>> self = Foo() >>> assert self.parent_attr == 'bar'", "response": "def fix_super_reload(this_class, self):\n    \"\"\"\n    Fixes an error where reload causes super(X, self) to raise an exception\n\n    The problem is that reloading causes X to point to the wrong version of the\n    class.  This function fixes the problem by searching and returning the\n    correct version of the class. See example for proper usage.\n\n    USE `ut.super2` INSTEAD\n\n    Args:\n        this_class (class): class passed into super\n        self (instance): instance passed into super\n\n    DisableExample:\n        >>> # DISABLE_DOCTEST\n        >>> import utool as ut\n        >>> class Parent(object):\n        >>>     def __init__(self):\n        >>>         self.parent_attr = 'bar'\n        >>> #\n        >>> class Foo(Parent):\n        >>>     def __init__(self):\n        >>>         # Dont do this, it will error if you reload\n        >>>         # super(Foo, self).__init__()\n        >>>         # Do this instead\n        >>>         _Foo = ut.super2(Foo, self)\n        >>>         super(_Foo, self).__init__()\n        >>> self = Foo()\n        >>> assert self.parent_attr == 'bar'\n    \"\"\"\n\n    if isinstance(self, this_class):\n        # Case where everything is ok\n        this_class_now = this_class\n    else:\n        # Case where we need to search for the right class\n        def find_parent_class(leaf_class, target_name):\n            target_class = None\n            from collections import deque\n            queue = deque()\n            queue.append(leaf_class)\n            seen_ = set([])\n            while len(queue) > 0:\n                related_class = queue.pop()\n                if related_class.__name__ != target_name:\n                    for base in related_class.__bases__:\n                        if base not in seen_:\n                            queue.append(base)\n                            seen_.add(base)\n                else:\n                    target_class = related_class\n                    break\n            return target_class\n        # Find new version of class\n        leaf_class = self.__class__\n        target_name = this_class.__name__\n        target_class = find_parent_class(leaf_class, target_name)\n\n        this_class_now = target_class\n        #print('id(this_class)     = %r' % (id(this_class),))\n        #print('id(this_class_now) = %r' % (id(this_class_now),))\n    assert isinstance(self, this_class_now), (\n        'Failed to fix %r, %r, %r' % (self, this_class, this_class_now))\n    return this_class_now"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _heappush_max(heap, item):\n    heap.append(item)\n    heapq._siftdown_max(heap, 0, len(heap) - 1)", "response": "Add item to heap and update heapq."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_ans(self, ans_):\n        ans = ans_.strip(' ')\n        def chack_if_answer_was(valid_keys):\n            return any([ans == key or ans.startswith(key + ' ')\n                        for key in valid_keys])\n        # Custom interactions\n        for func, tup in self.actions.items():\n            valid_keys = tup[0]\n            if chack_if_answer_was(valid_keys):\n                func()", "response": "Preforms an actionm based on a user answer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_ans(iiter, ans_):\n        ans = ans_.strip(' ')\n        def parse_str_value(ans):\n            return ' '.join(ans.split(' ')[1:])\n        def chack_if_answer_was(valid_keys):\n            return any([ans == key or ans.startswith(key + ' ') for key in valid_keys])\n        # Handle standard actions\n        if ans in iiter.action_keys['quit']:\n            raise StopIteration()\n        elif ans in iiter.action_keys['prev']:\n            iiter.index -= 1\n        elif ans in iiter.action_keys['next']:\n            iiter.index += 1\n        elif ans in iiter.action_keys['reload']:\n            iiter.index += 0\n        elif chack_if_answer_was(iiter.action_keys['index']):\n            try:\n                iiter.index = int(parse_str_value(ans))\n            except ValueError:\n                print('Unknown ans=%r' % (ans,))\n        elif chack_if_answer_was(iiter.action_keys['set']):\n            try:\n                iiter.iterable[iiter.index] = eval(parse_str_value(ans))\n            except ValueError:\n                print('Unknown ans=%r' % (ans,))\n        elif ans in iiter.action_keys['ipy']:\n            return 'IPython'\n        else:\n            # Custom interactions\n            for func, tup in zip(iiter.custom_funcs, iiter.custom_actions):\n                key = tup[0]\n                if chack_if_answer_was(iiter.action_keys[key]):\n                    value  = parse_str_value(ans)\n                    # cal custom function\n                    print('Calling custom action func')\n                    import utool as ut\n                    argspec = ut.get_func_argspec(func)\n                    if len(argspec.args) == 3:\n                        # Forgot why I had custom functions take args in the first place\n                        func(iiter, key, value)\n                    else:\n                        func()\n                    # Custom funcs dont cause iteration\n                    return False\n            print('Unknown ans=%r' % (ans,))\n            return False\n        return True", "response": "Handle an answer from an actionm based on a user answer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef take_column(self, keys, *extra_keys):\n        import utool as ut\n        keys = ut.ensure_iterable(keys) + list(extra_keys)\n        key_to_list = ut.dict_subset(self._key_to_list, keys)\n        newself = self.__class__(key_to_list, self._meta.copy())\n        return newself", "response": "Takes a subset of columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef take(self, idxs):\n        import utool as ut\n        if False:\n            key_to_list = ut.odict([\n                (key, ut.take(val, idxs))\n                for key, val in six.iteritems(self._key_to_list)\n            ])\n        else:\n            import numpy as np\n            key_to_list = ut.odict([\n                (key, ut.take(val, idxs))\n                if not isinstance(val, np.ndarray)\n                else val.take(idxs, axis=0)\n                for key, val in six.iteritems(self._key_to_list)\n            ])\n        newself = self.__class__(key_to_list, self._meta.copy())\n        return newself", "response": "Takes a subset of rows"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(self, idxs):\n        import utool as ut\n        keep_idxs = ut.index_complement(idxs, len(self))\n        return self.take(keep_idxs)", "response": "Returns a copy with idxs removed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike map column but applies values inplace", "response": "def cast_column(self, keys, func):\n        \"\"\" like map column but applies values inplace \"\"\"\n        import utool as ut\n        for key in ut.ensure_iterable(keys):\n            self[key] = [func(v) for v in self[key]]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the func to each element in the specified columns.", "response": "def map_column(self, keys, func):\n        \"\"\"\n        Args:\n            keys (list or str): the column name(s) to apply the `func` to\n            func (callable): applied to each element in the specified columns\n        \"\"\"\n        return [[func(v) for v in self[key]] for key in keys]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_rows(self, key, merge_scalars=True):\n        import utool as ut\n        unique_labels, groupxs = self.group_indicies(key)\n        single_xs = [xs for xs in groupxs if len(xs) == 1]\n        multi_xs = [xs for xs in groupxs if len(xs) > 1]\n        singles = self.take(ut.flatten(single_xs))\n        multis = [self.take(idxs) for idxs in multi_xs]\n\n        merged_groups = []\n        for group in multis:\n            newgroup = {}\n            for key_ in group.keys():\n                val = group[key_]\n                if key_ == key:\n                    # key_ was garuenteed unique\n                    val_ = val[0]\n                elif hasattr(val[0].__class__, 'union'):\n                    # HACK\n                    # Sets are unioned\n                    val_ = ut.oset.union(*val)\n                elif isinstance(val[0], (ut.oset,)):\n                    # Sets are unioned\n                    val_ = ut.oset.union(*val)\n                elif isinstance(val[0], (set)):\n                    # Sets are unioned\n                    val_ = set.union(*val)\n                elif isinstance(val[0], (tuple, list)):\n                    # Lists are merged together\n                    val_ = ut.flatten(val)\n                    #val_ = ut.unique(ut.flatten(val))\n                else:\n                    if ut.allsame(val):\n                        # Merge items that are the same\n                        val_ = val[0]\n                    else:\n                        if merge_scalars:\n                            # If mergeing scalars is ok, then\n                            # Values become lists if they are different\n                            val_ = val\n                        else:\n                            if True:\n                                # If there is only one non-none value then use that.\n                                other_vals = ut.filter_Nones(val)\n                                if len(other_vals) == 1:\n                                    val_ = val[0]\n                                else:\n                                    raise ValueError(\n                                        'tried to merge a scalar in %r, val=%r' % (\n                                            key_, val))\n                            else:\n                                # If merging scalars is not ok, then\n                                # we must raise an error\n                                raise ValueError(\n                                    'tried to merge a scalar in %r, val=%r' % (\n                                        key_, val))\n                newgroup[key_] = [val_]\n            merged_groups.append(ut.ColumnLists(newgroup))\n        merged_multi = self.__class__.flatten(merged_groups)\n        merged = singles + merged_multi\n        return merged", "response": "Merges all rows in the table into one table."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npeeks at the next item in the queue.", "response": "def peek(self):\n        \"\"\"\n        Peek at the next item in the queue\n        \"\"\"\n        # Ammortized O(1)\n        _heap = self._heap\n        _dict = self._dict\n        val, key = _heap[0]\n        # Remove items marked for lazy deletion as they are encountered\n        while key not in _dict or _dict[key] != val:\n            self._heappop(_heap)\n            val, key = _heap[0]\n        return key, val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pop(self, key=util_const.NoParam, default=util_const.NoParam):\n        # Dictionary pop if key is specified\n        if key is not util_const.NoParam:\n            if default is util_const.NoParam:\n                return (key, self._dict.pop(key))\n            else:\n                return (key, self._dict.pop(key, default))\n        # Otherwise do a heap pop\n        try:\n            # Ammortized O(1)\n            _heap = self._heap\n            _dict = self._dict\n            val, key = self._heappop(_heap)\n            # Remove items marked for lazy deletion as they are encountered\n            while key not in _dict or _dict[key] != val:\n                val, key = self._heappop(_heap)\n        except IndexError:\n            if len(_heap) == 0:\n                raise IndexError('queue is empty')\n            else:\n                raise\n        del _dict[key]\n        return key, val", "response": "Pop the next item off the queue and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes the fromimport function in the given module.", "response": "def __execute_fromimport(module, modname, import_tuples, verbose=False):\n    \"\"\" Module From Imports \"\"\"\n    if verbose:\n        print('[UTIL_IMPORT] EXECUTING %d FROM IMPORT TUPLES' % (len(import_tuples),))\n    from_imports = __get_from_imports(import_tuples)\n    for name, fromlist in from_imports:\n        full_modname = '.'.join((modname, name))\n        tmp = __import__(full_modname, globals(), locals(), fromlist=fromlist, level=0)\n        for attrname in fromlist:\n            setattr(module, attrname, getattr(tmp, attrname))\n    return from_imports"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_from_imports(import_tuples):\n    from_imports = [(tup[0], tup[1]) for tup in import_tuples\n                    if tup[1] is not None and len(tup[1]) > 0]\n    return from_imports", "response": "Returns import names and fromlist\n    import_tuples are specified as\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _initstr(modname, imports, from_imports, inject_execstr, withheader=True):\n    header         = _make_module_header() if withheader else ''\n    import_str     = _make_imports_str(imports, modname)\n    fromimport_str = _make_fromimport_str(from_imports, modname)\n    initstr = '\\n'.join([str_ for str_ in [\n        header,\n        import_str,\n        fromimport_str,\n        inject_execstr,\n    ] if len(str_) > 0])\n    return initstr", "response": "Returns the string to be used in the init string of a new module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_initstr(modname, import_tuples, verbose=False):\n    imports = [tup[0] for tup in import_tuples]\n    from_imports = __get_from_imports(import_tuples)\n    inject_execstr = _inject_execstr(modname, import_tuples)\n    return _initstr(modname, imports, from_imports, inject_execstr)", "response": "Creates the string representation of a module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninfers the import_tuples from a module_path", "response": "def make_import_tuples(module_path, exclude_modnames=[]):\n    \"\"\" Infer the import_tuples from a module_path \"\"\"\n    from utool import util_path\n    kwargs = dict(private=False, full=False)\n    module_list  = util_path.ls_modulefiles(module_path, noext=True, **kwargs)\n    package_list = util_path.ls_moduledirs(module_path, **kwargs)\n    exclude_set = set(exclude_modnames)\n    module_import_tuples = [(modname, None) for modname in module_list\n                            if modname not in exclude_set]\n    package_import_tuples = [(modname, None, True)  for modname in package_list\n                            if modname not in exclude_set]\n    import_tuples = (module_import_tuples + package_import_tuples)\n    return import_tuples"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_resource_dir():\n    #resource_prefix = '~'\n    if WIN32:\n        dpath_ = '~/AppData/Roaming'\n    elif LINUX:\n        dpath_ = '~/.config'\n    elif DARWIN:\n        dpath_  = '~/Library/Application Support'\n    else:\n        raise AssertionError('unknown os')\n    dpath = normpath(expanduser(dpath_))\n    return dpath", "response": "Returns a directory which should be writable for any application\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _insert_defaults(self):\n        merged = merge_defaults(self.structure, self)\n        self.update(merged)", "response": "Inserts default values from StructuredDictMixin. structure\n        to self."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload data from file.", "response": "def load_data(fpath, **kwargs):\n    \"\"\" More generic interface to load data \"\"\"\n    ext = splitext(fpath)[1]\n    if ext in ['.pickle', '.cPkl', '.pkl']:\n        return load_cPkl(fpath, **kwargs)\n    elif ext in ['.json']:\n        return load_json(fpath, **kwargs)\n    elif ext in ['.hdf5']:\n        return load_hdf5(fpath, **kwargs)\n    elif ext in ['.txt']:\n        return load_text(fpath, **kwargs)\n    elif HAS_NUMPY and ext in ['.npz', '.npy']:\n        return load_numpy(fpath, **kwargs)\n    else:\n        assert False, 'unknown ext=%r for fpath=%r' % (ext, fpath)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_data(fpath, data, **kwargs):\n    ext = splitext(fpath)[1]\n    if ext in ['.pickle', '.cPkl', '.pkl']:\n        return save_cPkl(fpath, data, **kwargs)\n    elif ext in ['.json']:\n        return save_json(fpath, data, **kwargs)\n    elif ext in ['.hdf5']:\n        return save_hdf5(fpath, data, **kwargs)\n    elif ext in ['.txt']:\n        return save_text(fpath, **kwargs)\n    elif HAS_NUMPY and ext in ['.npz', '.npy']:\n        return save_numpy(fpath, data, **kwargs)\n    else:\n        assert False, 'unknown ext=%r for fpath=%r' % (ext, fpath)", "response": "More generic interface to write data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites text to a file. Automatically encodes text as utf8.", "response": "def write_to(fpath, to_write, aslines=False, verbose=None,\n             onlyifdiff=False, mode='w', n=None):\n    \"\"\" Writes text to a file. Automatically encodes text as utf8.\n\n    Args:\n        fpath (str): file path\n        to_write (str): text to write (must be unicode text)\n        aslines (bool): if True to_write is assumed to be a list of lines\n        verbose (bool): verbosity flag\n        onlyifdiff (bool): only writes if needed!\n                checks hash of to_write vs the hash of the contents of fpath\n        mode (unicode): (default = u'w')\n        n (int):  (default = 2)\n\n    CommandLine:\n        python -m utool.util_io --exec-write_to --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_io import *  # NOQA\n        >>> import utool as ut\n        >>> fpath = ut.unixjoin(ut.get_app_resource_dir('utool'), 'testwrite.txt')\n        >>> ut.delete(fpath)\n        >>> to_write = 'utf-8 symbols \u0394, \u0419, \u05e7, \u0645, \u0e57, \u3042, \u53f6, \u8449, and \ub9d0.'\n        >>> aslines = False\n        >>> verbose = True\n        >>> onlyifdiff = False\n        >>> mode = u'w'\n        >>> n = 2\n        >>> write_to(fpath, to_write, aslines, verbose, onlyifdiff, mode, n)\n        >>> read_ = ut.read_from(fpath)\n        >>> print('read_    = ' + read_)\n        >>> print('to_write = ' + to_write)\n        >>> assert read_ == to_write\n    \"\"\"\n    if onlyifdiff:\n        import utool as ut\n        if ut.hashstr(read_from(fpath)) == ut.hashstr(to_write):\n            print('[util_io] * no difference')\n            return\n    verbose = _rectify_verb_write(verbose)\n    if verbose:\n        # n = None if verbose > 1 else 2\n        # print('[util_io] * Writing to text file: %r ' % util_path.tail(fpath, n=n))\n        print('[util_io] * Writing to text file: {}'.format(fpath))\n\n    backup = False and exists(fpath)\n    if backup:\n        util_path.copy(fpath, fpath + '.backup')\n\n    if not isinstance(fpath, six.string_types):\n        # Assuming a file object with a name attribute\n        # Should just read from the file\n        fpath = fpath.name\n\n    with open(fpath, mode) as file_:\n        if aslines:\n            file_.writelines(to_write)\n        else:\n            # Ensure python2 writes in bytes\n            if six.PY2:\n                if isinstance(to_write, unicode):  # NOQA\n                    to_write = to_write.encode('utf8')\n            try:\n                file_.write(to_write)\n            except UnicodeEncodeError as ex:\n                start = max(ex.args[2] - 10, 0)\n                end = ex.args[3] + 10\n                context = to_write[start:end]\n                print(repr(context))\n                print(context)\n                from utool import util_dbg\n                util_dbg.printex(ex, keys=[(type, 'to_write')])\n                file_.close()\n                if backup:\n                    # restore\n                    util_path.copy(fpath + '.backup', fpath)\n                # import utool\n                # utool.embed()\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_from(fpath, verbose=None, aslines=False, strict=True, n=None, errors='replace'):\n    if n is None:\n        n = __READ_TAIL_N__\n    verbose = _rectify_verb_read(verbose)\n    if verbose:\n        print('[util_io] * Reading text file: %r ' % util_path.tail(fpath, n=n))\n    try:\n        if not util_path.checkpath(fpath, verbose=verbose, n=n):\n            raise IOError('[io] * FILE DOES NOT EXIST!')\n        #with open(fpath, 'r') as file_:\n        with open(fpath, 'rb') as file_:\n            if aslines:\n                #text = file_.readlines()\n                if six.PY2:\n                    # python2 writes in bytes, so read as bytes then convert to\n                    # utf8\n                    text = [line.decode('utf8', errors=errors)\n                            for line in file_.readlines()]\n                else:\n                    text = [line.decode('utf8', errors=errors)\n                            for line in file_.readlines()]\n                    #text = file_.readlines()\n            else:\n                # text = file_.read()\n                if six.PY2:\n                    text = file_.read().decode('utf8', errors=errors)\n                else:\n                    #text = file_.read()\n                    text = file_.read().decode('utf8', errors=errors)\n        return text\n    except IOError as ex:\n        from utool import util_dbg\n        if verbose or strict:\n            util_dbg.printex(ex, ' * Error reading fpath=%r' %\n                             util_path.tail(fpath, n=n), '[io]')\n        if strict:\n            raise", "response": "r Reads text from a file. Automatically returns utf8."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_cPkl(fpath, data, verbose=None, n=None):\n    verbose = _rectify_verb_write(verbose)\n    if verbose:\n        print('[util_io] * save_cPkl(%r, data)' % (util_path.tail(fpath, n=n),))\n    with open(fpath, 'wb') as file_:\n        # Use protocol 2 to support python2 and 3\n        pickle.dump(data, file_, protocol=2)", "response": "Save data to a pickled file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_cPkl(fpath, verbose=None, n=None):\n    verbose = _rectify_verb_read(verbose)\n    if verbose:\n        print('[util_io] * load_cPkl(%r)' % (util_path.tail(fpath, n=n),))\n    try:\n        with open(fpath, 'rb') as file_:\n            data = pickle.load(file_)\n    except UnicodeDecodeError:\n        if six.PY3:\n            # try to open python2 pickle\n            with open(fpath, 'rb') as file_:\n                data = pickle.load(file_, encoding='latin1')\n        else:\n            raise\n    except ValueError as ex:\n        if six.PY2:\n            if ex.message == 'unsupported pickle protocol: 4':\n                raise ValueError(\n                    'unsupported Python3 pickle protocol 4 '\n                    'in Python2 for fpath=%r' % (fpath,))\n            else:\n                raise\n        else:\n            raise\n    return data", "response": "Load a pickled file with optional verbosity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a C ++ file from Python 2 and Python 3.", "response": "def _python2_load_cpkl(fpath):\n    \"\"\"\n    References:\n        https://stackoverflow.com/questions/41720952/unpickle-sklearn-tree-descisiontreeregressor-in-python-2-from-python3\n    \"\"\"\n    from lib2to3.fixes.fix_imports import MAPPING\n    import sys\n    import pickle\n\n    # MAPPING maps Python 2 names to Python 3 names. We want this in reverse.\n    REVERSE_MAPPING = {}\n    for key, val in MAPPING.items():\n        REVERSE_MAPPING[val] = key\n\n    # We can override the Unpickler and loads\n    class Python_3_Unpickler(pickle.Unpickler):\n        \"\"\"Class for pickling objects from Python 3\"\"\"\n        def find_class(self, module, name):\n            if module in REVERSE_MAPPING:\n                module = REVERSE_MAPPING[module]\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n            return klass\n\n    def load(fpath):\n        with open(fpath, 'rb') as file_:\n            data = Python_3_Unpickler(file_).load()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_pytables(fpath, data, verbose=False):\n    import tables\n    #from os.path import basename\n    #fname = basename(fpath)\n    #shape = data.shape\n    #dtype = data.dtype\n    #file_ = tables.open_file(fpath)\n    verbose = _rectify_verb_write(verbose)\n    if verbose:\n        print('[util_io] * save_pytables(%r, data)' % (util_path.tail(fpath),))\n    with tables.open_file(fpath, 'w') as file_:\n        atom = tables.Atom.from_dtype(data.dtype)\n        filters = tables.Filters(complib='blosc', complevel=5)\n        dset = file_.createCArray(file_.root, 'data', atom, data.shape, filters=filters)\n        # save w/o compressive filter\n        #dset = file_.createCArray(file_.root, 'all_data', atom, all_data.shape)\n        dset[:] = data", "response": "Save a numpy array to a HDF5 file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a local port is open", "response": "def is_local_port_open(port):\n    \"\"\"\n    Args:\n        port (int):\n\n    Returns:\n        bool:\n\n    References:\n        http://stackoverflow.com/questions/7436801/identifying-listening-ports-using-python\n\n    CommandLine:\n        python -m utool.util_web is_local_port_open --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_web import *  # NOQA\n        >>> port = 32183\n        >>> assert is_local_port_open(80) is False, 'port 80 should always be closed'\n        >>> assert is_local_port_open(port) is True, 'maybe this port is actually used?'\n    \"\"\"\n    import socket\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    result = s.connect_ex(('127.0.0.1', port))\n    s.close()\n    return result != 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start_simple_webserver(domain=None, port=5832):\n    import tornado.ioloop\n    import tornado.web\n    import tornado.httpserver\n    import tornado.wsgi\n    import flask\n    app = flask.Flask('__simple__')\n    @app.route('/', methods=['GET', 'POST', 'DELETE', 'PUT'])\n    def echo_args(*args, **kwargs):\n        from flask import request\n        print('Simple server was pinged')\n        print('args = %r' % (args,))\n        print('kwargs = %r' % (kwargs,))\n        print('request.args = %r' % (request.args,))\n        print('request.form = %r' % (request.form,))\n        return ''\n    if domain is None:\n        domain = get_localhost()\n    app.server_domain = domain\n    app.server_port = port\n    app.server_url = 'http://%s:%s' % (app.server_domain, app.server_port)\n    print('app.server_url = %s' % (app.server_url,))\n    http_server = tornado.httpserver.HTTPServer(\n        tornado.wsgi.WSGIContainer(app))\n    http_server.listen(app.server_port)\n    tornado.ioloop.IOLoop.instance().start()", "response": "r Start a simple webserver that echos its arguments"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders a temporary html string", "response": "def render_html(html_str):\n    \"\"\"\n    makes a temporary html rendering\n    \"\"\"\n    import utool as ut\n    from os.path import abspath\n    import webbrowser\n\n    try:\n        html_str = html_str.decode('utf8')\n    except Exception:\n        pass\n\n    html_dpath = ut.ensure_app_resource_dir('utool', 'temp_html')\n    fpath = abspath(ut.unixjoin(html_dpath, 'temp.html'))\n    url = 'file://' + fpath\n    ut.writeto(fpath, html_str)\n    webbrowser.open(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publish(func):\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):  # outgoing\n        payload = func(self, *args, **kwargs)\n        payload.pop('self', None)\n        self._publish(func.__name__, payload)\n        return None\n\n    wrapper.is_publish = True\n\n    return wrapper", "response": "publish the return value of this function as a message from this endpoint\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xsubscribe(func=None, strategy='DESIGNATION'):\n    if func is None:\n        return partial(xsubscribe, strategy=strategy)\n    else:\n        wrapper = _get_subscribe_decorator(func)\n        wrapper.is_xsubscribe = True\n        wrapper.strategy = strategy\n        return wrapper", "response": "Decorator to subscribe to events from a specific endpoint of a service."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse to request an api call from a specific endpoint", "response": "def request(func=None, timeout=600):\n    \"\"\"\n    use to request an api call from a specific endpoint\n    \"\"\"\n    if func is None:\n        return partial(request, timeout=timeout)\n\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        params = func(self, *args, **kwargs)\n        self = params.pop('self', None)\n        entity = params.pop('entity', None)\n        app_name = params.pop('app_name', None)\n        request_id = unique_hex()\n        params['request_id'] = request_id\n        future = self._send_request(app_name, endpoint=func.__name__, entity=entity, params=params, timeout=timeout)\n        return future\n\n    wrapper.is_request = True\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide a request/response api receives any requests here and return value is the response all functions must have the following signature - request_id - entity (partition/routing key) followed by kwargs", "response": "def api(func=None, timeout=API_TIMEOUT):  # incoming\n    \"\"\"\n    provide a request/response api\n    receives any requests here and return value is the response\n    all functions must have the following signature\n        - request_id\n        - entity (partition/routing key)\n        followed by kwargs\n    \"\"\"\n    if func is None:\n        return partial(api, timeout=timeout)\n    else:\n        wrapper = _get_api_decorator(func=func, timeout=timeout)\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserializing the given instance of Problem into the response.", "response": "def serialize_problem(req, resp, problem):\n    \"\"\"Serialize the given instance of Problem.\"\"\"\n    preferred = req.client_prefers(\n        ('application/json', 'application/problem+json')\n    )\n    if preferred is None:\n        preferred = 'application/json'\n\n    resp.data = problem.to_json().encode('utf-8')\n    resp.content_type = preferred\n    resp.append_header('Vary', 'Accept')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ir(cfg_func):\n    ir_func = ir.Function()\n    ir_var_list = []\n    cfg_var_list = []\n    ir_bb_label_list = []\n    for cfg_var in cfg_func.variable_list:\n        ir_var = ir.Variable(cfg_var.name)\n        ir_var_list.append(ir_var)\n        cfg_var_list.append(cfg_var)\n    label = 0\n    for cfg_bb in cfg_func.basic_block_list:\n        ir_bb_label_list.append(label)\n        for cfg_instr in cfg_bb.instruction_list:\n            if isinstance(cfg_instr, cfg.ArithInstruction):\n                ir_instr = ir.ArithInstruction(ir_func)\n                ir_lhs = get_ir_numeric(cfg_instr.lhs, cfg_var_list, ir_var_list)\n                ir_rhs_1 = get_ir_numeric(cfg_instr.rhs_1, cfg_var_list, ir_var_list)\n                ir_rhs_2 = get_ir_numeric(cfg_instr.rhs_2, cfg_var_list, ir_var_list)\n                ir_op = ir.Operation(cfg_instr.op.name)\n                ir_instr.update(ir_lhs, ir_rhs_1, ir_rhs_2, ir_op)\n            elif isinstance(cfg_instr, cfg.CmpInstruction):\n                ir_instr = ir.CmpInstruction(ir_func)\n                ir_lhs = get_ir_numeric(cfg_instr.lhs, cfg_var_list, ir_var_list)\n                ir_rhs_1 = get_ir_numeric(cfg_instr.rhs_1, cfg_var_list, ir_var_list)\n                ir_rhs_2 = get_ir_numeric(cfg_instr.rhs_2, cfg_var_list, ir_var_list)\n                ir_op = ir.Operation(cfg_instr.op.name)\n                ir_instr.update(ir_lhs, ir_rhs_1, ir_rhs_2, ir_op)\n            elif isinstance(cfg_instr, cfg.EqInstruction):\n                ir_instr = ir.EqInstruction(ir_func)\n                ir_lhs = get_ir_numeric(cfg_instr.lhs, cfg_var_list, ir_var_list)\n                ir_rhs = get_ir_numeric(cfg_instr.rhs, cfg_var_list, ir_var_list)\n                ir_instr.update(ir_lhs, ir_rhs)\n            ir_func.add_instruction_by_label(label, ir_instr)\n            label += 1\n        #at end of BB, add branch statements\n        if cfg_bb.number_of_children is 1:\n            ir_instr = ir.UncondnJumpInstruction(ir_func)\n            ir_func.add_instruction_by_label(label, ir_instr)\n        elif cfg_bb.number_of_children is 2:\n            if isinstance(cfg_bb.condition_instr, cfg.CmpInstruction):\n                ir_instr = ir.CmpInstruction(ir_func)\n                ir_lhs = get_ir_numeric(cfg_bb.condition_instr.lhs, cfg_var_list, ir_var_list)\n                ir_rhs_1 = get_ir_numeric(cfg_bb.condition_instr.rhs_1, cfg_var_list, ir_var_list)\n                ir_rhs_2 = get_ir_numeric(cfg_bb.condition_instr.rhs_2, cfg_var_list, ir_var_list)\n                ir_op = ir.Operation(cfg_bb.condition_instr.op.name)\n                ir_instr.update(ir_lhs, ir_rhs_1, ir_rhs_2, ir_op)\n                ir_func.add_instruction_by_label(label, ir_instr)\n                label += 1\n            ir_instr = ir.CondnJumpInstruction(ir_func)\n            ir_condn_var = get_ir_numeric(cfg_bb.condition, cfg_var_list, ir_var_list)\n            ir_instr.update(ir_condn_var, 0, 0)\n            ir_func.add_instruction_by_label(label, ir_instr)\n        else:\n            ir_instr = ir.ReturnInstruction(ir_func)\n            ir_func.add_instruction_by_label(label, ir_instr)\n        label += 1\n\n    k = 0\n    for cfg_bb in cfg_func.basic_block_list:\n        if cfg_bb.number_of_children is 1:\n            this_label = ir_bb_label_list[k] + len(cfg_bb.instruction_list)\n            assert(isinstance(ir_func.instr_list[this_label], ir.UncondnJumpInstruction))\n            next_label = ir_bb_label_list[cfg_bb.child.identity]\n            ir_func.instr_list[this_label].next_instr_label = next_label\n        elif cfg_bb.number_of_children is 2:\n            this_label = ir_bb_label_list[k] + len(cfg_bb.instruction_list) \n            if isinstance(cfg_bb.condition_instr, cfg.CmpInstruction):\n                this_label += 1\n            assert(isinstance(ir_func.instr_list[this_label], ir.CondnJumpInstruction))\n            next_true_label = ir_bb_label_list[cfg_bb.child_true.identity]\n            next_false_label = ir_bb_label_list[cfg_bb.child_false.identity]\n            ir_func.instr_list[this_label].instr_true_label = next_true_label\n            ir_func.instr_list[this_label].instr_false_label = next_false_label\n        k += 1\n    ir_input_variables = []\n    for cfg_var in cfg_func.input_variable_list:\n        ir_var = get_ir_numeric(cfg_var, cfg_var_list, ir_var_list)\n        ir_input_variables.append(ir_var)\n\n    ir_output_variables = []\n    for cfg_var in cfg_func.output_variable_list:\n        ir_var = get_ir_numeric(cfg_var, cfg_var_list, ir_var_list)\n        ir_output_variables.append(ir_var)\n    ir_func.set_input_variables(ir_input_variables)\n    ir_func.set_output_variables(ir_output_variables)\n    ir_func.add_summary(cfg_func.summary)\n    return ir_func", "response": "Converts the given CFG function into IR entities\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling function for create_featuredata_map", "response": "def add_psms_to_proteindata(proteindata, p_acc, pool, psmdata):\n    \"\"\"Fill function for create_featuredata_map\"\"\"\n    seq, psm_id = psmdata[2], psmdata[3]\n    try:\n        proteindata[p_acc]['pools'][pool]['psms'].add(psm_id)\n    except KeyError:\n        emptyinfo = {'psms': set(), 'peptides': set(), 'unipeps': 0}\n        try:\n            proteindata[p_acc]['pools'][pool] = emptyinfo\n        except KeyError:\n            proteindata[p_acc].update({'pools': {pool: emptyinfo}})\n        proteindata[p_acc]['pools'][pool]['psms'].add(psm_id)\n    proteindata[p_acc]['pools'][pool]['peptides'].add(seq)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a dictionary of protein data containing PSMs peptides description and coverage. Loops through the protein data and fills the outputted map.", "response": "def create_featuredata_map(pgdb, psm_fill_fun, pgene_fill_fun, genecentric=False, \n        count_fun=None, pool_to_output=False, get_uniques=False, is_peptides=False):\n    \"\"\"Creates dict of protein data containing PSMs, peptides, proteins in\n    protein group, unique peptides, description and coverage. Loops through\n    PSM/protein matches and uses a passed fill_fun function to actually\n    fill the outputted map.\n    \"\"\"\n    if genecentric:\n        pgcontentmap = None\n    else:\n        pgcontentmap = get_proteingroup_content(pgdb)\n    proteindata = {}\n    for pgenedata in pgdb.get_protein_gene_symbol_for_map():\n        if is_peptides:\n            # include sequence (index 1 of record) to fill func\n            pgene_fill_fun(proteindata, pgenedata[1], pgenedata[0], \n                    pgenedata, genecentric, pgcontentmap)\n        else:\n            pgene_fill_fun(proteindata, pgenedata[0], \n                    pgenedata, genecentric, pgcontentmap)\n    protein_psms_data = pgdb.get_proteins_psms_for_map()\n    psmdata = next(protein_psms_data)\n    last_prot, last_pool = psmdata[0], psmdata[1]\n    psm_fill_fun(proteindata, last_prot, last_pool, psmdata)\n    for psmdata in protein_psms_data:\n        p_acc, samplepool = psmdata[0], psmdata[1]\n        if pool_to_output and samplepool != pool_to_output:\n            continue\n        if samplepool != last_pool or p_acc != last_prot:\n            if count_fun is not None:\n                count_fun(proteindata, last_prot, last_pool)\n            last_pool, last_prot = samplepool, p_acc\n        psm_fill_fun(proteindata, p_acc, samplepool, psmdata)\n    if count_fun is not None:\n        count_fun(proteindata, last_prot, last_pool)\n    \n    if get_uniques:\n        for no_uni, pool, pacc in pgdb.get_unique_peptide_nrs():\n            proteindata[pacc]['pools'][pool]['unipeps'] = no_uni\n    return proteindata"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toxml(self):\n\n        return '<Dimension name=\"{0}\"'.format(self.name) +\\\n          (' m = \"{0}\"'.format(self.m) if self.m != 0 else '') +\\\n          (' l = \"{0}\"'.format(self.l) if self.l != 0 else '') +\\\n          (' t = \"{0}\"'.format(self.t) if self.t != 0 else '') +\\\n          (' i = \"{0}\"'.format(self.i) if self.i != 0 else '') +\\\n          (' k = \"{0}\"'.format(self.k) if self.k != 0 else '') +\\\n          (' n = \"{0}\"'.format(self.n) if self.n != 0 else '') +\\\n          (' j = \"{0}\"'.format(self.j) if self.j != 0 else '') +\\\n          '/>'", "response": "Returns a LEMS XML string representation of the Dimension object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef toxml(self):\n\n        # Probably name should be removed altogether until its usage is decided, see\n        # https://github.com/LEMS/LEMS/issues/4\n        #  '''(' name = \"{0}\"'.format(self.name) if self.name else '') +\\'''\n\n        return '<Unit' +\\\n          (' symbol = \"{0}\"'.format(self.symbol) if self.symbol else '') +\\\n          (' dimension = \"{0}\"'.format(self.dimension) if self.dimension else '') +\\\n          (' power = \"{0}\"'.format(self.power) if self.power else '') +\\\n          (' scale = \"{0}\"'.format(self.scale) if self.scale else '') +\\\n          (' offset = \"{0}\"'.format(self.offset) if self.offset else '') +\\\n          (' description = \"{0}\"'.format(self.description) if self.description else '') +\\\n          '/>'", "response": "Returns a LEMS XML string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_traceback(with_colors=True):\n    #traceback.print_tb()\n    import traceback\n    stack = traceback.extract_stack()\n    stack_lines = traceback.format_list(stack)\n    tbtext = ''.join(stack_lines)\n    if with_colors:\n        try:\n            from pygments import highlight\n            from pygments.lexers import get_lexer_by_name\n            from pygments.formatters import TerminalFormatter\n            lexer = get_lexer_by_name('pytb', stripall=True)\n            formatter = TerminalFormatter(bg='dark')\n            formatted_text = highlight(tbtext, lexer, formatter)\n            print(formatted_text)\n        except Exception:\n            print(tbtext)\n    else:\n        print(tbtext)", "response": "Print the traceback to the screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks syntax and validity of a variable name", "response": "def is_valid_varname(varname):\n    \"\"\" Checks syntax and validity of a variable name \"\"\"\n    if not isinstance(varname, six.string_types):\n        return False\n    match_obj = re.match(varname_regex, varname)\n    valid_syntax = match_obj is not None\n    valid_name = not keyword.iskeyword(varname)\n    isvalid = valid_syntax and valid_name\n    return isvalid"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the executable string that declares variables using keys and values from dict_", "response": "def execstr_dict(dict_, local_name=None, exclude_list=None, explicit=False):\n    \"\"\"\n    returns execable python code that declares variables using keys and values\n\n    execstr_dict\n\n    Args:\n        dict_ (dict):\n        local_name (str): optional: local name of dictionary. Specifying this\n            is much safer\n        exclude_list (list):\n\n    Returns:\n        str: execstr --- the executable string that will put keys from dict\n            into local vars\n\n    CommandLine:\n        python -m utool.util_dbg --test-execstr_dict\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # UNSTABLE_DOCTEST\n        >>> from utool.util_dbg import *  # NOQA\n        >>> my_dictionary = {'a': True, 'b': False}\n        >>> execstr = execstr_dict(my_dictionary)\n        >>> exec(execstr)\n        >>> assert 'a' in vars() and 'b' in vars(), 'execstr failed'\n        >>> assert b is False and a is True, 'execstr failed'\n        >>> result = execstr\n        >>> print(result)\n        a = my_dictionary['a']\n        b = my_dictionary['b']\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dbg import *  # NOQA\n        >>> import utool as ut\n        >>> my_dictionary = {'a': True, 'b': False}\n        >>> execstr = execstr_dict(my_dictionary)\n        >>> locals_ = locals()\n        >>> exec(execstr, locals_)\n        >>> a, b = ut.dict_take(locals_, ['a', 'b'])\n        >>> assert 'a' in locals_ and 'b' in locals_, 'execstr failed'\n        >>> assert b is False and a is True, 'execstr failed'\n        >>> result = execstr\n        >>> print(result)\n        a = my_dictionary['a']\n        b = my_dictionary['b']\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dbg import *  # NOQA\n        >>> import utool as ut\n        >>> my_dictionary = {'a': True, 'b': False}\n        >>> execstr = execstr_dict(my_dictionary, explicit=True)\n        >>> result = execstr\n        >>> print(result)\n        a = True\n        b = False\n    \"\"\"\n    import utool as ut\n    if explicit:\n        expr_list = []\n        for (key, val) in sorted(dict_.items()):\n            assert isinstance(key, six.string_types), 'keys must be strings'\n            expr_list.append('%s = %s' % (key, ut.repr2(val),))\n        execstr = '\\n'.join(expr_list)\n        return execstr\n    else:\n        if local_name is None:\n            # Magic way of getting the local name of dict_\n            local_name = get_varname_from_locals(dict_, get_parent_frame().f_locals)\n        try:\n            if exclude_list is None:\n                exclude_list = []\n            assert isinstance(exclude_list, list)\n            exclude_list.append(local_name)\n            expr_list = []\n            assert isinstance(dict_, dict), 'incorrect type type(dict_)=%r, dict_=%r' % (type(dict), dict_)\n            for (key, val) in sorted(dict_.items()):\n                assert isinstance(key, six.string_types), 'keys must be strings'\n                if not is_valid_varname(key):\n                    continue\n                if not any((fnmatch.fnmatch(key, pat) for pat in exclude_list)):\n                    expr = '%s = %s[%s]' % (key, local_name, ut.repr2(key))\n                    expr_list.append(expr)\n            execstr = '\\n'.join(expr_list)\n            return execstr\n        except Exception as ex:\n            locals_ = locals()\n            ut.printex(ex, key_list=['locals_'])\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhacks adds current locals to globals.", "response": "def fix_embed_globals():\n    \"\"\"\n    HACK adds current locals() to globals().\n    Can be dangerous.\n    \"\"\"\n    frame = get_stack_frame(N=1)\n    frame.f_globals.update(frame.f_locals)\n    frame.f_globals['_did_embed_fix'] = True\n    \"\"\"\n    def fix_embed_globals(N=0):\n        import inspect\n        # Get the parent frame\n        frame_cur = inspect.currentframe()\n        for _ix in range(N + 1):\n            # always skip the frame of this function\n            frame_next = frame_cur.f_back\n            if frame_next is None:\n                break\n            frame_cur = frame_next\n        # Add locals to parent globals\n        frame = frame_cur\n        frame.f_globals.update(frame.f_locals)\n        frame.f_globals['_did_embed_fix'] = True\n    \"\"\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _wip_embed(parent_locals=None, parent_globals=None, exec_lines=None,\n               remove_pyqt_hook=True, N=0):\n    \"\"\"\n    Starts interactive session. Similar to keyboard command in matlab.\n    Wrapper around IPython.embed\n\n    Notes:\n        #https://github.com/ipython/ipython/wiki/Cookbook%3a-Updating-code-for-use-with-IPython-0.11-and-later\n\n        import IPython\n        x = 3\n        IPython.embed()\n        c = IPython.Config()\n        c.InteractiveShellApp.exec_lines = [\n            '%pylab qt4',\n            \"print 'System Ready!'\",\n        ]\n\n        def foo():\n            return x + 3\n\n        a = 3\n        def bar():\n            return a + 3\n        bar()\n        #NameError: global name 'a' is not defined\n\n\n        from IPython.terminal.ipapp import TerminalIPythonApp\n        x = 3\n        app = TerminalIPythonApp.instance()\n        app.initialize(argv=[]) # argv=[] instructs IPython to ignore sys.argv\n        app.start()\n\n\n    Args:\n        parent_locals (None):\n        parent_globals (None):\n        exec_lines (None):\n        remove_pyqt_hook (bool):\n        N (int):\n\n    CommandLine:\n        python -m utool.util_dbg --test-embed\n\n    References:\n       http://stackoverflow.com/questions/27911570/can-you-specify-a-command-to-run-after-you-embed-into-ipython/27914204#27914204\n       http://stackoverflow.com/questions/15167200/how-do-i-embed-an-ipython-interpreter-into-an-application-running-in-an-ipython\n\n    Notes:\n        Use cases I want to achieve\n\n        1) Simply stop execution and embed in an IPython terminal session\n        2) Like case 1, but execute a specific set of command (eg '%gui qt')\n           AFTER IPython has started\n        3) Embed and pause GUI execution (this is just case 1)\n        3) Embed and let GUI execution continue while embeded. (basically just need case 2)\n\n    TODO:\n        try:\n            get_ipython\n        except NameError:\n            banner=exit_msg=''\n        else:\n            banner = '*** Nested interpreter ***'\n            exit_msg = '*** Back in main IPython ***'\n\n        # First import the embed function\n        from IPython.frontend.terminal.embed import InteractiveShellEmbed\n        # Now create the IPython shell instance. Put ipshell() anywhere in your code\n        # where you want it to open.\n        ipshell = InteractiveShellEmbed(banner1=banner, exit_msg=exit_msg)\n        #Then use ipshell() whenever you want to be dropped into an IPython shell. This\n        #will allow you to embed (and even nest) IPython interpreters in your code and\n        #inspect objects or the state of the program.\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_dbg import *  # NOQA\n        >>> # build test data\n        >>> parent_locals = None\n        >>> parent_globals = None\n        >>> exec_lines = None\n        >>> remove_pyqt_hook = True\n        >>> N = 0\n        >>> # execute function\n        >>> result = embed(parent_locals, parent_globals, exec_lines, remove_pyqt_hook, N)\n        >>> # verify results\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    from functools import partial\n    import IPython\n\n    if parent_globals is None:\n        parent_globals = get_parent_frame(N=N).f_globals\n    if parent_locals is None:\n        parent_locals = get_parent_frame(N=N).f_locals\n\n    stackdepth = N  # NOQA\n    getframe = partial(ut.get_parent_frame, N=N)  # NOQA\n\n    exec(execstr_dict(parent_globals, 'parent_globals'))\n    exec(execstr_dict(parent_locals,  'parent_locals'))\n    print('')\n    print('================')\n    print(ut.bubbletext('EMBEDDING'))\n    print('================')\n    print('[util] embedding')\n    try:\n        if remove_pyqt_hook:\n            try:\n                import guitool\n                guitool.remove_pyqt_input_hook()\n            except (ImportError, ValueError, AttributeError) as ex:\n                #print(ex)\n                printex(ex, iswarning=True)\n                pass\n            # make qt not loop forever (I had qflag loop forever with this off)\n    except ImportError as ex:\n        print(ex)\n    user_ns = globals()\n    user_ns = globals().copy()\n    user_ns.update(locals())\n    if parent_globals is not None:\n        user_ns.update(parent_globals)\n    if parent_locals is not None:\n        user_ns.update(parent_locals)\n    orig_argv = sys.argv  # NOQA\n    print('About to start_ipython')\n    config = IPython.Config()\n    exec_lines_ = [\n        '%pylab qt4',\n        'print(\"Entered IPYTHON via utool\")',\n        'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=11).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=10).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=9).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=8).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=7).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=6).f_code.co_name,))',\n        #'print(\"Entry Point: %r\" % (ut.get_parent_frame(N=5).f_code.co_name,))',\n        #execstr_dict(parent_locals)\n    ] + ut.ensure_str_list(exec_lines if exec_lines is not None else [])\n    config.InteractiveShellApp.exec_lines = exec_lines_\n    print('Exec Lines: ')\n    print(ut.indentjoin(exec_lines_, '\\n    >>> '))\n    IPython.start_ipython(config=config, argv=[], user_ns=user_ns)\n    # Exit python immediately if specifed\n    if user_ns.get('qqq', False) or vars.get('qqq', False) or user_ns.get('EXIT_NOW', False):\n        print('[utool.embed] EXIT_NOW or qqq specified')\n        sys.exit(1)", "response": "This function is used to start an interactive session in matlab."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef embed(parent_locals=None, parent_globals=None, exec_lines=None,\n          remove_pyqt_hook=True, N=0):\n    \"\"\"\n    Starts interactive session. Similar to keyboard command in matlab.\n    Wrapper around IPython.embed\n\n    \"\"\"\n    import utool as ut\n    from functools import partial\n    import IPython\n\n    if parent_globals is None:\n        parent_globals = get_parent_frame(N=N).f_globals\n    if parent_locals is None:\n        parent_locals = get_parent_frame(N=N).f_locals\n\n    stackdepth = N  # NOQA\n    getframe = partial(ut.get_parent_frame, N=N)  # NOQA\n\n    # exec(execstr_dict(parent_globals, 'parent_globals'))\n    # exec(execstr_dict(parent_locals,  'parent_locals'))\n    print('')\n    print('================')\n    print(ut.bubbletext('EMBEDDING'))\n    print('================')\n    print('[util] embedding')\n    try:\n        if remove_pyqt_hook:\n            try:\n                import guitool\n                guitool.remove_pyqt_input_hook()\n            except (ImportError, ValueError, AttributeError) as ex:\n                #print(ex)\n                printex(ex, iswarning=True)\n                pass\n            # make qt not loop forever (I had qflag loop forever with this off)\n    except ImportError as ex:\n        print(ex)\n\n    #from IPython.config.loader import Config\n    # cfg = Config()\n    #config_dict = {}\n    #if exec_lines is not None:\n    #    config_dict['exec_lines'] = exec_lines\n    #IPython.embed(**config_dict)\n    print('[util]  Get stack location with: ')\n    print('[util] ut.get_parent_frame(N=8).f_code.co_name')\n    print('[util] set EXIT_NOW or qqq to True(ish) to hard exit on unembed')\n    #print('set iup to True to draw plottool stuff')\n    print('[util] call %pylab qt4 to get plottool stuff working')\n    once = True\n    # Allow user to set iup and redo the loop\n    while once or vars().get('iup', False):\n        if not once:\n            # SUPER HACKY WAY OF GETTING FIGURES ON THE SCREEN BETWEEN UPDATES\n            #vars()['iup'] = False\n            # ALL YOU NEED TO DO IS %pylab qt4\n            print('re-emebeding')\n            #import plottool as pt\n            #pt.update()\n            #(pt.present())\n            for _ in range(100):\n                time.sleep(.01)\n\n        once = False\n        #vars().get('iup', False):\n        print('[util] calling IPython.embed()')\n        \"\"\"\n        Notes:\n            /usr/local/lib/python2.7/dist-packages/IPython/terminal/embed.py\n            IPython.terminal.embed.InteractiveShellEmbed\n\n            # instance comes from  IPython.config.configurable.SingletonConfigurable.instance\n        \"\"\"\n        #c = IPython.Config()\n        #c.InteractiveShellApp.exec_lines = [\n        #    '%pylab qt4',\n        #    '%gui qt4',\n        #    \"print 'System Ready!'\",\n        #]\n        #IPython.embed(config=c)\n        parent_ns = parent_globals.copy()\n        parent_ns.update(parent_locals)\n        locals().update(parent_ns)\n        try:\n            IPython.embed()\n        except RuntimeError as ex:\n            ut.printex(ex, 'Failed to open ipython')\n        #config = IPython.terminal.ipapp.load_default_config()\n        #config.InteractiveShellEmbed = config.TerminalInteractiveShell\n        #module = sys.modules[parent_globals['__name__']]\n        #config['module'] = module\n        #config['module'] = module\n        #embed2(stack_depth=N + 2 + 1)\n        #IPython.embed(config=config)\n        #IPython.embed(config=config)\n        #IPython.embed(module=module)\n        # Exit python immediately if specifed\n        if vars().get('EXIT_NOW', False) or vars().get('qqq', False):\n            print('[utool.embed] EXIT_NOW specified')\n            sys.exit(1)", "response": "Wrapper around IPython. embed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef embed2(**kwargs):\n    config = kwargs.get('config')\n    header = kwargs.pop('header', u'')\n    stack_depth = kwargs.pop('stack_depth', 2)\n    compile_flags = kwargs.pop('compile_flags', None)\n    import IPython\n    from IPython.core.interactiveshell import InteractiveShell\n    from IPython.terminal.embed import InteractiveShellEmbed\n    if config is None:\n        config = IPython.terminal.ipapp.load_default_config()\n        config.InteractiveShellEmbed = config.TerminalInteractiveShell\n        kwargs['config'] = config\n    #save ps1/ps2 if defined\n    ps1 = None\n    ps2 = None\n    try:\n        ps1 = sys.ps1\n        ps2 = sys.ps2\n    except AttributeError:\n        pass\n    #save previous instance\n    saved_shell_instance = InteractiveShell._instance\n    if saved_shell_instance is not None:\n        cls = type(saved_shell_instance)\n        cls.clear_instance()\n    shell = InteractiveShellEmbed.instance(**kwargs)\n    shell(header=header, stack_depth=stack_depth, compile_flags=compile_flags)\n    InteractiveShellEmbed.clear_instance()\n    #restore previous instance\n    if saved_shell_instance is not None:\n        cls = type(saved_shell_instance)\n        cls.clear_instance()\n        for subclass in cls._walk_mro():\n            subclass._instance = saved_shell_instance\n    if ps1 is not None:\n        sys.ps1 = ps1\n        sys.ps2 = ps2", "response": "Embeds a single terminal into another terminal."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if code is executed in the jupyter notebook.", "response": "def in_jupyter_notebook():\n    \"\"\"\n    http://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n    \"\"\"\n    try:\n        cfg = get_ipython().config\n        #print('cfg = %s' % (ut.repr4(cfg),))\n        #x = cfg['IPKernelApp']['parent_appname']\n        # might not work if using jupyter-console\n        if cfg['IPKernelApp']['connection_file'].count('jupyter'):\n            return True\n        else:\n            return False\n    except (AttributeError, NameError):\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch the stack for a local varable somewhere in the stack and returns its value.", "response": "def search_stack_for_localvar(varname):\n    \"\"\"\n    Finds a local varable somewhere in the stack and returns the value\n\n    Args:\n        varname (str): variable name\n\n    Returns:\n        None if varname is not found else its value\n    \"\"\"\n    curr_frame = inspect.currentframe()\n    print(' * Searching parent frames for: ' + six.text_type(varname))\n    frame_no = 0\n    while curr_frame.f_back is not None:\n        if varname in curr_frame.f_locals.keys():\n            print(' * Found in frame: ' + six.text_type(frame_no))\n            return curr_frame.f_locals[varname]\n        frame_no += 1\n        curr_frame = curr_frame.f_back\n    print('... Found nothing in all ' + six.text_type(frame_no) + ' frames.')\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsearching the stack for a variable in the current context and returns the value.", "response": "def search_stack_for_var(varname, verbose=util_arg.NOT_QUIET):\n    \"\"\"\n    Finds a varable (local or global) somewhere in the stack and returns the value\n\n    Args:\n        varname (str): variable name\n\n    Returns:\n        None if varname is not found else its value\n    \"\"\"\n    curr_frame = inspect.currentframe()\n    if verbose:\n        print(' * Searching parent frames for: ' + six.text_type(varname))\n    frame_no = 0\n    while curr_frame.f_back is not None:\n        if varname in curr_frame.f_locals.keys():\n            if verbose:\n                print(' * Found local in frame: ' + six.text_type(frame_no))\n            return curr_frame.f_locals[varname]\n        if varname in curr_frame.f_globals.keys():\n            if verbose:\n                print(' * Found global in frame: ' + six.text_type(frame_no))\n            return curr_frame.f_globals[varname]\n        frame_no += 1\n        curr_frame = curr_frame.f_back\n    if verbose:\n        print('... Found nothing in all ' + six.text_type(frame_no) + ' frames.')\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the frame of the current stack.", "response": "def get_stack_frame(N=0, strict=True):\n    \"\"\"\n    Args:\n        N (int): N=0 means the frame you called this function in.\n                 N=1 is the parent frame.\n        strict (bool): (default = True)\n    \"\"\"\n    frame_cur = inspect.currentframe()\n    for _ix in range(N + 1):\n        # always skip the frame of this function\n        frame_next = frame_cur.f_back\n        if frame_next is None:\n            if strict:\n                raise AssertionError('Frame level %r is root' % _ix)\n            else:\n                break\n        frame_cur = frame_next\n    return frame_cur"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint an exception to the log", "response": "def printex(ex, msg='[!?] Caught exception', prefix=None, key_list=[],\n            locals_=None, iswarning=False, tb=TB, pad_stdout=True, N=0,\n            use_stdout=False, reraise=False, msg_=None, keys=None,\n            colored=None):\n    \"\"\"\n    Prints (and/or logs) an exception with relevant info\n\n    Args:\n        ex (Exception): exception to print\n        msg (str): a message to display to the user\n        keys (None): a list of strings denoting variables or expressions of interest\n        iswarning (bool): prints as a warning rather than an error if True (defaults to False)\n        tb (bool): if True prints the traceback in the error message\n        pad_stdout (bool): separate the error message from the rest of stdout with newlines\n        prefix (None):\n        locals_ (None):\n        N (int):\n        use_stdout (bool):\n        reraise (bool):\n        msg_ (None):\n        key_list (list): DEPRICATED use keys\n\n    Returns:\n        None\n    \"\"\"\n    import utool as ut\n    if isinstance(ex, MemoryError):\n        ut.print_resource_usage()\n    if keys is not None:\n        # shorthand for key_list\n        key_list = keys\n    # Get error prefix and local info\n    if prefix is None:\n        prefix = get_caller_prefix(aserror=True, N=N)\n    if locals_ is None:\n        locals_ = get_parent_frame(N=N).f_locals\n    # build exception message\n    if msg is True:\n        key_list = get_parent_frame().f_locals\n        msg = msg_\n    exstr = formatex(ex, msg, prefix, key_list, locals_, iswarning, tb=tb, colored=colored)\n    # get requested print function\n    if use_stdout:\n        def print_func(*args):\n            msg = ', '.join(list(map(six.text_type, args)))\n            sys.stdout.write(msg + '\\n')\n            sys.stdout.flush()\n    else:\n        print_func = ut.partial(ut.colorprint, color='yellow' if iswarning else 'red')\n        # print_func = print\n    if pad_stdout:\n        print_func('\\n+------\\n')\n    # print the execption\n    print_func(exstr)\n    if pad_stdout:\n        print_func('\\nL______\\n')\n    # If you dont know where an error is coming from raise-all\n    if (reraise and not iswarning) or RAISE_ALL:\n        sys.stdout.flush()\n        sys.stderr.flush()\n        raise ex\n    if ut.get_argflag('--exit-on-error'):\n        print('WARNING: dont use this flag. Some errors are meant to be caught')\n        ut.print_traceback()\n        print('REQUESTED EXIT ON ERROR')\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the varname which is val in the parent namespace", "response": "def get_varname_from_locals(val, locals_, default='varname-not-found',\n                            strict=False, cmpfunc_=operator.is_):\n    \"\"\" Finds the string name which has where locals_[name] is val\n\n    Check the varname is in the parent namespace\n    This will only work with objects not primatives\n\n    Args:\n        val (): some value\n        locals_ (dict): local dictionary to search\n        default (str):\n        strict (bool):\n\n    Returns:\n        str: the varname which is Val (if it exists)\n\n    \"\"\"\n    if val is None or isinstance(val, (int, float, bool)):\n        # Cannot work on primative types\n        return default\n    try:\n        for count, val_ in enumerate(six.itervalues(locals_)):\n            if cmpfunc_(val, val_):\n                index_ = count\n        varname = six.text_type(list(locals_.keys())[index_])\n    except NameError:\n        varname = default\n        if strict:\n            raise\n    return varname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a variable value from locals.", "response": "def get_varval_from_locals(key, locals_, strict=False):\n    \"\"\"\n    Returns a variable value from locals.\n    Different from locals()['varname'] because\n    get_varval_from_locals('varname.attribute', locals())\n    is allowed\n    \"\"\"\n    assert isinstance(key, six.string_types), 'must have parsed key into a string already'\n    if key not in locals_:\n        dotpos = key.find('.')\n        if dotpos > -1:\n            key_ = key[:dotpos]\n            attrstr_ = key[dotpos:]\n            try:\n                baseval = locals_[key_]  # NOQA\n                val = eval('baseval' + attrstr_)\n            except Exception as ex:\n                if strict:\n                    raise\n                val = ex\n        else:\n            raise AssertionError('%s = NameError' % (key))\n    else:\n        val = locals_[key]\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __send_rdy(self, connection, command):\n\n        if self.__consumer.original_rdy is None:\n            node_count = self.__consumer.get_node_count_for_topic(\n                            connection.context.topic)\n\n            self.__logger_rdy.debug(\"Calculating RDY: max_in_flight=(%d) \"\n                                    \"node_count=(%d)\", \n                                    self.__consumer.max_in_flight, node_count)\n\n            if self.__consumer.max_in_flight >= node_count:\n                # Calculate the RDY based on the max_in_flight and total number \n                # of servers. We always round up, or else we'd run the risk of \n                # not facilitating some servers.\n                rdy_this = int(math.ceil(\n                                        float(self.__consumer.max_in_flight) /\n                                        float(node_count)))\n\n                self.__logger_rdy.debug(\"Assigning RDY based on max_in_flight \"\n                                        \"(%d) and node count (%d) (optimal): \"\n                                        \"(%d)\", \n                                        self.__consumer.max_in_flight, \n                                        node_count, rdy_this)\n            else:\n                # We have two possible scenarios:\n                # (1) The client is starting up, and the total RDY count is \n                #     already accounted for.\n                # (2) The client is already started, and another connection has\n                #     a (0) RDY count.\n                #\n                # In the case of (1), we'll take an RDY of (0). In the case of\n                # (2) We'll send an RDY of (1) on their behalf, before we \n                # assume a (0) for ourself.\n\n                # Look for existing connections that have a (0) RDY (which \n                # would've only been set to (0) intentionally).\n\n                self.__logger_rdy.debug(\"(max_in_flight > nodes). Doing RDY \"\n                                        \"election.\")\n\n                sleeping_connections = [\n                    c \\\n                    for (c, info) \\\n                    in self.__consumer.connection_context.items() \\\n                    if info['rdy_count'] == 0]\n\n                self.__logger_rdy.debug(\"Current sleeping_connections: %s\", \n                                        sleeping_connections)\n\n                if sleeping_connections:\n                    elected_connection = random.choice(sleeping_connections)\n                    self.__logger_rdy.debug(\"Sending RDY of (1) on: [%s]\", \n                                            elected_connection)\n\n                    command_elected = nsq.command.Command(elected_connection)\n                    command_elected.rdy(1)\n                else:\n                    self.__logger.debug(\"No sleeping connections. We got the \"\n                                        \"short stick: [%s]\", connection)\n\n                rdy_this = 0\n        else:\n            try:\n                rdy_this = self.__consumer.original_rdy(\n                            connection.node, \n                            self.__consumer.connection_count, \n                            self.__consumer)\n\n                self.__logger_rdy.debug(\"Using RDY from callback: (%d)\", \n                                        rdy_this)\n            except TypeError:\n                rdy_this = self.__consumer.original_rdy\n                self.__logger_rdy.debug(\"Using static RDY: (%d)\", rdy_this)\n\n        # Make sure that the aggregate set of RDY counts doesn't exceed the \n        # max. This constrains the previous value, above.\n        rdy_this = min(rdy_this + \\\n                        self.__get_total_rdy_count(), \n                       self.__consumer.max_in_flight)\n\n        # Make sure we don't exceed the maximum specified by the server. This \n        # only works because we're running greenlets, not threads. At any given \n        # time, only one greenlet is running, and we can make sure to \n        # distribute the remainder of (max_in_flight / nodes) across a subset \n        # of the nodes (they don't all have to have an even slice of \n        # max_in_flight).\n\n        server_features = self.__consumer.identify.server_features\n        max_rdy_count = server_features['max_rdy_count']\n        rdy_this = min(max_rdy_count, rdy_this)\n\n        self.__logger_rdy.debug(\"Final RDY (max_in_flight=(%d) \"\n                                \"max_rdy_count=(%d)): (%d)\", \n                                self.__consumer.max_in_flight, max_rdy_count, \n                                rdy_this)\n\n        if rdy_this > 0:\n            command.rdy(rdy_this)\n        else:\n            self.__logger_rdy.info(\"This connection will go to sleep (not \"\n                                   \"enough RDY to go around).\")\n\n        return rdy_this", "response": "Send the RDY command to the specified master."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef switch_psm_to_peptable_fields(oldheader):\n    return {old: new for old, new in zip([mzidtsvdata.HEADER_PEPTIDE,\n                                          mzidtsvdata.HEADER_PROTEIN,\n                                          mzidtsvdata.HEADER_PEPTIDE_Q,\n                                          mzidtsvdata.HEADER_PEPTIDE_PEP],\n                                         [peptabledata.HEADER_PEPTIDE,\n                                          peptabledata.HEADER_PROTEINS,\n                                          peptabledata.HEADER_QVAL,\n                                          peptabledata.HEADER_PEP])}", "response": "Returns a dict map with old to new header fields"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_header(headerfields, oldheader, group_by_field):\n    fieldtypes = ['peptidefdr', 'peptidepep', 'nopsms', 'proteindata',\n                  'precursorquant', 'isoquant']\n    return generate_general_header(headerfields, fieldtypes,\n                                   peptabledata.HEADER_PEPTIDE, oldheader,\n                                   group_by_field)", "response": "Generates a header as a list ready to write to TSV file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_peptable_headerfields(headertypes, lookup=False, poolnames=False):\n    field_defs = {'isoquant': get_isoquant_fields,\n                  'precursorquant': get_precursorquant_fields,\n                  'peptidefdr': get_peptidefdr_fields,\n                  'peptidepep': get_peptidepep_fields,\n                  'proteindata': get_proteininfo_fields,\n                  }\n    return generate_headerfields(headertypes, field_defs, poolnames, lookup)", "response": "Returns a generator object that can be used to generate headerfields objects for a given list of headype"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn header fields for protein group information.", "response": "def get_proteininfo_fields(poolnames=False, genecentric=False):\n    \"\"\"Returns header fields for protein (group) information.\"\"\"\n    allfields = OrderedDict()\n    basefields = [peptabledata.HEADER_PROTEINS,\n                  peptabledata.HEADER_GENES,\n                  peptabledata.HEADER_ASSOCIATED,\n                  peptabledata.HEADER_DESCRIPTIONS,\n                  peptabledata.HEADER_COVERAGES,\n                  peptabledata.HEADER_NO_CONTENTPROTEINS,\n                  ]\n    for field in basefields:\n        allfields[field] = False\n    allfields[peptabledata.HEADER_NO_PSM] = poolnames\n    return allfields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_argval(argstr, type_=None, default=None):\n    arg_after = default\n    if type_ is bool:\n        arg_after = False if default is None else default\n    try:\n        # New for loop way (accounts for =)\n        for argx, item in enumerate(sys.argv):\n            if item == argstr:\n                if argx < len(sys.argv):\n                    if type_ is bool:\n                        arg_after = True\n                    else:\n                        arg_after = _try_cast(sys.argv[argx + 1], type_)\n            if item.startswith(argstr + '='):\n                val_after = ''.join(item.split('=')[1:])\n                arg_after = _try_cast(val_after, type_)\n    except Exception:\n        pass\n    return arg_after", "response": "r Returns a value of an argument specified on the command line after some flag\nInsights"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the argument instruction to the list of instructions of this basic block.", "response": "def add_instruction (self, instr):\n        \"\"\"\n        Adds the argument instruction in the list of instructions of this basic block.\n\n        Also updates the variable lists (used_variables, defined_variables)\n        \"\"\"\n        assert(isinstance(instr, Instruction))\n        self.instruction_list.append(instr)\n        if instr.lhs not in self.defined_variables:\n            if isinstance(instr.lhs, Variable):\n                self.defined_variables.append(instr.lhs)\n        if isinstance(instr, EqInstruction):\n            if isinstance(instr.rhs, Variable):\n                if instr.rhs not in self.used_variables:\n                    self.used_variables.append(instr.rhs)\n        else:\n            if isinstance(instr.rhs_1, Variable):\n                if instr.rhs_1 not in self.used_variables:\n                    self.used_variables.append(instr.rhs_1)\n            if isinstance(instr.rhs_2, Variable):\n                if instr.rhs_2 not in self.used_variables:\n                    self.used_variables.append(instr.rhs_2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the condition which decides how the basic block exits.", "response": "def set_condition(self, condition, condition_instr=None):\n        \"\"\"\n        Defines the condition which decides how the basic block exits\n\n        :param condition:\n        :type condition:\n\n        :param condition_instr: If the 'condition' argument is a Variable, then\\\n                condition_instr is None, else, condition_instr should be\\\n                of type CmpInstruction\n        :type condition_instr: CmpInstruction\n        \"\"\"\n        assert(isinstance(condition, Numeric))\n        if condition_instr is not None:\n            assert(isinstance(condition_instr, CmpInstruction))\n        self.condition = condition\n        self.condition_instr = condition_instr\n        if condition_instr is not None:\n            if condition_instr.lhs not in self.defined_variables:\n                if isinstance(condition_instr.lhs, Variable):\n                    self.defined_variables.append(condition_instr.lhs)\n            if isinstance(condition_instr.rhs_1, Variable):\n                if condition_instr.rhs_1 not in self.used_variables:\n                    self.used_variables.append(condition_instr.rhs_1)\n            if isinstance(condition_instr.rhs_2, Variable):\n                if condition_instr.rhs_2 not in self.used_variables:\n                    self.used_variables.append(condition_instr.rhs_2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the given basic block in the function", "response": "def add_basic_block(self, basic_block):\n        \"\"\"Adds the given basic block in the function\"\"\"\n        assert(isinstance(basic_block, BasicBlock))\n        self.basic_block_list.append(basic_block)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_variable(self, var_name):\n        assert(isinstance(var_name, str))\n        if isinstance(var_name, str):\n            for var in self.variable_list:\n                if var.name == var_name:\n                    return var\n            new_var = Variable(var_name)\n            self.variable_list.append(new_var)\n            return new_var", "response": "Returns a variable object with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the argument variable as one of the input variable", "response": "def add_input_variable(self, var):\n        \"\"\"Adds the argument variable as one of the input variable\"\"\"\n        assert(isinstance(var, Variable))\n        self.input_variable_list.append(var)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding the argument variable as one of the output variable list", "response": "def add_output_variable(self, var):\n        \"\"\"Adds the argument variable as one of the output variable\"\"\"\n        assert(isinstance(var, Variable))\n        self.output_variable_list.append(var)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenize(self):\n\n        self.token_list = []\n        ps = self.parse_string.strip()\n\n        i = 0\n        last_token = None\n\n        while i < len(ps) and ps[i].isspace():\n            i += 1\n\n        while i < len(ps):\n            token = ''\n\n            if ps[i].isalpha():\n                while i < len(ps) and (ps[i].isalnum() or ps[i] == '_'):\n                    token += ps[i]\n                    i += 1\n            elif ps[i].isdigit():\n                while i < len(ps) and (ps[i].isdigit() or\n                                       ps[i] == '.' or\n                                       ps[i] == 'e' or\n                                       ps[i] == 'E' or\n                                       (ps[i] == '+' and (ps[i-1] == 'e' or ps[i-1] == 'E')) or\n                                       (ps[i] == '-' and (ps[i-1] == 'e' or ps[i-1] == 'E'))):\n                    token += ps[i]\n                    i += 1\n            elif ps[i] == '.':\n                if ps[i+1].isdigit():\n                    while i < len(ps) and (ps[i].isdigit() or ps[i] == '.'):\n                        token += ps[i]\n                        i += 1\n                else:\n                    while i < len(ps) and (ps[i].isalpha() or ps[i] == '.'):\n                        token += ps[i]\n                        i += 1\n            else:\n                token += ps[i]\n                i += 1\n\n            if token == '-' and \\\n               (last_token == None or last_token == '(' or self.is_op(last_token)):\n                token = '~'\n\n            self.token_list += [token]\n            last_token = token\n\n            while i < len(ps) and ps[i].isspace():\n                i += 1", "response": "Tokenizes the string stored in the parser object into a list of tokens."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_token_list_rec(self, min_precedence):\n\n        exit_loop = False\n\n        ExprParser.depth = ExprParser.depth + 1\n        if self.debug: print('>>>>> Depth: %i'% ExprParser.depth)\n\n        precedence = min_precedence\n\n        while self.token_list:\n            token = self.token_list[0]\n            la = self.token_list[1] if len(self.token_list) > 1 else None\n\n            if self.debug: print('0> %s'% self.token_list)\n            if self.debug: print('1> Token: %s, next: %s, op stack: %s, val stack: %s, node stack: %s'% (token, la, self.op_stack, self.val_stack, self.node_stack))\n\n            self.token_list = self.token_list[1:]\n            \n            close_bracket = False\n            \n            if token == '(':\n                np = ExprParser('')\n                np.token_list = self.token_list\n\n                nexp = np.parse2()\n\n                self.node_stack.push(nexp)\n                self.val_stack.push('$')\n\n                self.token_list = np.token_list\n                if self.debug: print('>>> Tokens left: %s'%self.token_list)\n                close_bracket = True\n            elif token == ')':\n                break\n            elif self.is_func(token):\n                self.op_stack.push(token)\n            elif self.is_op(token):\n                stack_top = self.op_stack.top()\n                if self.debug: print('OP Token: %s (prior: %i), top: %s (prior: %i)'% (token, self.priority(token), stack_top, self.priority(stack_top)))\n                if self.priority(token) < self.priority(stack_top):\n                    if self.debug: print('  Priority of %s is less than %s'%(token, stack_top))\n                    self.node_stack.push(self.cleanup_stacks())\n                    self.val_stack.push('$')\n                else:\n                    if self.debug: print('  Priority of %s is greater than %s'%(token, stack_top))\n                \n\n                self.op_stack.push(token)\n            else:\n                if self.debug: print('Not a bracket func or op...')\n                if la == '(':\n                    raise Exception(\"Error parsing expression: %s\\nToken: %s is placed like a function but is not recognised!\\nKnown functions: %s\"%(self.parse_string, token, known_functions))\n                stack_top = self.op_stack.top()\n                if stack_top == '$':\n                    if self.debug: print(\"option a\")\n                    self.node_stack.push(ValueNode(token))\n                    self.val_stack.push('$')\n                else:\n                    if (self.is_op(la) and\n                        self.priority(stack_top) < self.priority(la)):\n                        if self.debug: print(\"option b\")\n\n                        self.node_stack.push(ValueNode(token))\n                        self.val_stack.push('$')\n                    else:\n                        if self.debug: print(\"option c, nodes: %s\"% self.node_stack)\n                        op = self.op_stack.pop()\n\n                        right = ValueNode(token)\n                        op_node = self.make_op_node(op,right)\n\n                        self.node_stack.push(op_node)\n                        self.val_stack.push('$')\n                        \n            if close_bracket:\n                stack_top = self.op_stack.top()\n                if self.debug: print(\"+ Closing bracket, op stack: %s, node stack: %s la: %s\"%(self.op_stack, self.node_stack, la))\n                if self.debug: print('>>> Tokens left: %s'%self.token_list)\n                \n                if stack_top == '$':\n                    if self.debug: print(\"+ option a\")\n                    '''\n                    self.node_stack.push(ValueNode(token))\n                    self.val_stack.push('$')'''\n                else:\n                    la = self.token_list[0] if len(self.token_list) > 1 else None\n                    if (self.is_op(la) and self.priority(stack_top) < self.priority(la)):\n                        if self.debug: print(\"+ option b\")\n                        #self.node_stack.push(ValueNode(token))\n                        #self.val_stack.push('$')\n                    else:\n                        if self.debug: print(\"+ option c, nodes: %s\"% self.node_stack)\n                        if self.debug: print('35> op stack: %s, val stack: %s, node stack: %s'% ( self.op_stack, self.val_stack, self.node_stack))\n                        right = self.node_stack.pop()\n                        op = self.op_stack.pop()\n                        op_node = self.make_op_node(stack_top,right)\n                        if self.debug: print(\"Made op node: %s, right: %s\"%(op_node, right))\n\n                        self.node_stack.push(op_node)\n                        self.val_stack.push('$')\n                        if self.debug: print('36> op stack: %s, val stack: %s, node stack: %s'% ( self.op_stack, self.val_stack, self.node_stack))\n                        \n            \n\n            if self.debug: print('2> Token: %s, next: %s, op stack: %s, val stack: %s, node stack: %s'% (token, la, self.op_stack, self.val_stack, self.node_stack))\n            if self.debug: print('')\n\n        if self.debug: print('3> op stack: %s, val stack: %s, node stack: %s'% ( self.op_stack, self.val_stack, self.node_stack))\n        ret = self.cleanup_stacks()\n\n        if self.debug: print('4> op stack: %s, val stack: %s, node stack: %s'% ( self.op_stack, self.val_stack, self.node_stack))\n        if self.debug: print('<<<<< Depth: %s, returning: %s'% (ExprParser.depth, ret))\n        ExprParser.depth = ExprParser.depth - 1\n        if self.debug: print('')\n        return ret", "response": "Parses a tokenized arithmetic expression into a parse tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self):\n        #print(\"Parsing: %s\"%self.parse_string)\n        self.tokenize()\n        if self.debug: print(\"Tokens found: %s\"%self.token_list)\n        \n        try:\n            parse_tree = self.parse2()\n        except Exception as e:\n                raise e\n        return parse_tree", "response": "Tokenizes and parses an arithmetic expression into a parse tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting keys into a table which assigns an ID", "response": "def insert_keys(self, keys):\n        \"\"\"Insert keys into a table which assigns an ID\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        keys_len = len(keys)\n        query = 'INSERT IGNORE INTO gauged_keys (namespace, `key`) VALUES '\n        execute = self.cursor.execute\n        while start < keys_len:\n            rows = keys[start:start+bulk_insert]\n            params = [param for params in rows for param in params]\n            insert = '(%s,%s),' * (len(rows) - 1) + '(%s,%s)'\n            execute(query + insert, params)\n            start += bulk_insert"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replace_blocks(self, blocks):\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        row = '(%s,%s,%s,%s,%s)'\n        query = 'REPLACE INTO gauged_data (namespace, offset, `key`, ' \\\n            'data, flags) VALUES '\n        execute = self.cursor.execute\n        to_buffer = self.to_buffer\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = []\n            for namespace, offset, key, data, flags in rows:\n                params.extend((namespace, offset, key, to_buffer(data), flags))\n            insert = (row + ',') * (len(rows) - 1) + row\n            execute(query + insert, params)\n            start += bulk_insert", "response": "Replace multiple blocks. blocks must be a list of tuples where\n            each tuple consists of namespace offset key data and flags."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the current writer position", "response": "def get_writer_position(self, name):\n        \"\"\"Get the current writer position\"\"\"\n        cursor = self.cursor\n        cursor.execute('SELECT timestamp FROM gauged_writer_history '\n                       'WHERE id = %s', (name,))\n        result = cursor.fetchone()\n        return result[0] if result else 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_namespaces(self):\n        cursor = self.cursor\n        cursor.execute('SELECT DISTINCT namespace FROM gauged_statistics')\n        return [namespace for namespace, in cursor]", "response": "Get a list of namespaces"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_namespace(self, namespace):\n        params = (namespace, )\n        execute = self.cursor.execute\n        execute('DELETE FROM gauged_data WHERE namespace = %s', params)\n        execute('DELETE FROM gauged_statistics WHERE namespace = %s', params)\n        execute('DELETE FROM gauged_keys WHERE namespace = %s', params)\n        self.remove_cache(namespace)", "response": "Remove all data associated with the current namespace"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all cached values for the specified namespace optionally specifying a key", "response": "def remove_cache(self, namespace, key=None):\n        \"\"\"Remove all cached values for the specified namespace,\n        optionally specifying a key\"\"\"\n        if key is None:\n            self.cursor.execute('DELETE FROM gauged_cache '\n                                'WHERE namespace = %s', (namespace,))\n        else:\n            self.cursor.execute('DELETE FROM gauged_cache '\n                                'WHERE namespace = %s and `key` = %s',\n                                (namespace, key))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_namespace_statistics(self, namespace, offset, data_points,\n                                 byte_count):\n        \"\"\"Update namespace statistics for the period identified by\n        offset\"\"\"\n        self.cursor.execute(\n            'INSERT INTO gauged_statistics VALUES (%s,%s,%s,%s) ON DUPLICATE '\n            'KEY UPDATE data_points = data_points + VALUES(data_points),'\n            'byte_count = byte_count + VALUES(byte_count)',\n            (namespace, offset, data_points, byte_count))", "response": "Update the gauged_statistics table for the period identified by namespace and offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the number of data points for the period between start_offset and end_offset ( inclusive", "response": "def get_namespace_statistics(self, namespace, start_offset, end_offset):\n        \"\"\"Get namespace statistics for the period between start_offset and\n        end_offset (inclusive)\"\"\"\n        cursor = self.cursor\n        cursor.execute('SELECT SUM(data_points), SUM(byte_count) '\n                       'FROM gauged_statistics WHERE namespace = %s '\n                       'AND offset BETWEEN %s AND %s',\n                       (namespace, start_offset, end_offset))\n        return [long(count or 0) for count in cursor.fetchone()]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_schema(self):\n        cursor = self.cursor\n        execute = cursor.execute\n        execute('SHOW TABLES')\n        tables = {table for table, in cursor}\n        if 'gauged_data' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_data (\n                namespace INT(11) UNSIGNED NOT NULL,\n                offset INT(11) UNSIGNED NOT NULL,\n                `key` BIGINT(15) UNSIGNED NOT NULL,\n                data MEDIUMBLOB NOT NULL,\n                flags INT(11) UNSIGNED NOT NULL,\n                PRIMARY KEY (offset, namespace, `key`))\"\"\")\n        if 'gauged_keys' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_keys (\n                id BIGINT(15) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT,\n                namespace INT(11) UNSIGNED NOT NULL,\n                `key` VARCHAR(255) BINARY NOT NULL,\n                UNIQUE KEY (namespace, `key`))\"\"\")\n        if 'gauged_writer_history' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_writer_history (\n                id VARCHAR(255) NOT NULL PRIMARY KEY,\n                timestamp BIGINT(15) UNSIGNED NOT NULL)\"\"\")\n        if 'gauged_cache' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_cache (\n                namespace INT(11) UNSIGNED NOT NULL,\n                `key` BIGINT(15) UNSIGNED NOT NULL,\n                hash BINARY(20) NOT NULL,\n                length BIGINT(15) UNSIGNED NOT NULL,\n                start BIGINT(15) UNSIGNED NOT NULL,\n                value FLOAT(11),\n                PRIMARY KEY (namespace, hash, length, start))\"\"\")\n        if 'gauged_statistics' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_statistics (\n                namespace INT(11) UNSIGNED NOT NULL,\n                offset INT(11) UNSIGNED NOT NULL,\n                data_points INT(11) UNSIGNED NOT NULL,\n                byte_count INT(11) UNSIGNED NOT NULL,\n                PRIMARY KEY (namespace, offset))\"\"\")\n        if 'gauged_metadata' not in tables:\n            execute(\"\"\"CREATE TABLE gauged_metadata (\n                `key` VARCHAR(255) NOT NULL PRIMARY KEY,\n                value VARCHAR(255) NOT NULL)\"\"\")\n        self.db.commit()", "response": "Create all necessary tables and tables in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_schema(self):\n        execute = self.cursor.execute\n        execute('TRUNCATE TABLE gauged_data')\n        execute('TRUNCATE TABLE gauged_keys')\n        execute('TRUNCATE TABLE gauged_writer_history')\n        execute('TRUNCATE TABLE gauged_cache')\n        execute('TRUNCATE TABLE gauged_statistics')\n        self.db.commit()", "response": "Clear all gauged data and all keys and statistics"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndropping all tables and tables in the database.", "response": "def drop_schema(self):\n        \"\"\"Drop all gauged tables\"\"\"\n        execute = self.cursor.execute\n        execute('DROP TABLE IF EXISTS gauged_data')\n        execute('DROP TABLE IF EXISTS gauged_keys')\n        execute('DROP TABLE IF EXISTS gauged_writer_history')\n        execute('DROP TABLE IF EXISTS gauged_cache')\n        execute('DROP TABLE IF EXISTS gauged_statistics')\n        execute('DROP TABLE IF EXISTS gauged_metadata')\n        self.db.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef opt_key(spec):\n    if isinstance(spec, text_types):\n        spec = Equals(spec)\n    return optional(spec)", "response": "Returns a validator which allows the value to be missing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef one_of(choices, first_is_default=False, as_rules=False):\n    assert choices\n\n    if as_rules:\n        None    # for coverage\n    else:\n        choices = [Equals(x) for x in choices]\n\n    return Any(choices, first_is_default=first_is_default)", "response": "A wrapper for Any."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges https:// in .git/config files to git@ and makes appropriate changes to colons and slashses", "response": "def permit_gitrepo(config_fpath, writeback=False):\n    \"\"\"\n    Changes https:// in .git/config files to git@ and makes\n    appropriate changes to colons and slashses\n    \"\"\"\n    # Define search replace patterns\n    username_regex = utool.named_field('username', utool.REGEX_VARNAME)\n    username_repl = utool.backref_field('username')\n    regexpat = r'https://github.com/' + username_regex + '/'\n    replpat = r'git@github.com:' + username_repl + '/'\n    # Read and replace\n    lines = utool.read_from(config_fpath, aslines=True)\n    newlines = utool.regex_replace_lines(lines, regexpat, replpat)\n    # Writeback or print\n    if not WRITEBACK:\n        print(''.join(newlines))\n    else:\n        utool.write_to(config_fpath, newlines, aslines=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef quantum_random():\n    import quantumrandom\n    data16 = quantumrandom.uint16(array_length=2)\n    assert data16.flags['C_CONTIGUOUS']\n    data32 = data16.view(np.dtype('uint32'))[0]\n    return data32", "response": "returns a 32 bit unsigned integer quantum random number"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a NumPy RandomState object to a state that can be used by Python s Random.", "response": "def _npstate_to_pystate(npstate):\n    \"\"\"\n    Convert state of a NumPy RandomState object to a state\n    that can be used by Python's Random.\n\n    References:\n        https://stackoverflow.com/questions/44313620/converting-randomstate\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_numpy import *  # NOQA\n        >>> from utool.util_numpy import _npstate_to_pystate\n        >>> py_rng = random.Random(0)\n        >>> np_rng = np.random.RandomState(seed=0)\n        >>> npstate = np_rng.get_state()\n        >>> pystate = _npstate_to_pystate(npstate)\n        >>> py_rng.setstate(pystate)\n        >>> assert np_rng.rand() == py_rng.random()\n    \"\"\"\n    PY_VERSION = 3\n    version, keys, pos, has_gauss, cached_gaussian_ = npstate\n    keys_pos = tuple(map(int, keys)) + (int(pos),)\n    cached_gaussian_ = cached_gaussian_ if has_gauss else None\n    pystate = (PY_VERSION, keys_pos, cached_gaussian_)\n    return pystate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pystate_to_npstate(pystate):\n    NP_VERSION = 'MT19937'\n    version, keys_pos_, cached_gaussian_ = pystate\n    keys, pos = keys_pos_[:-1], keys_pos_[-1]\n    keys = np.array(keys, dtype=np.uint32)\n    has_gauss = cached_gaussian_ is not None\n    cached_gaussian = cached_gaussian_ if has_gauss else 0.0\n    npstate = (NP_VERSION, keys, pos, has_gauss, cached_gaussian)\n    return npstate", "response": "Convert a Python Random object to state usable by NumPy RandomState."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the random number generator is correct.", "response": "def ensure_rng(rng, impl='numpy'):\n    \"\"\"\n    Returns a random number generator\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_numpy import *  # NOQA\n        >>> import utool as ut\n        >>> import numpy as np\n        >>> num = 4\n        >>> print('--- Python as PYTHON ---')\n        >>> py_rng = random.Random(0)\n        >>> pp_nums = [py_rng.random() for _ in range(num)]\n        >>> print(pp_nums)\n        >>> print('--- Numpy as PYTHON ---')\n        >>> np_rng = ut.ensure_rng(random.Random(0), impl='numpy')\n        >>> np_nums = [np_rng.rand() for _ in range(num)]\n        >>> print(np_nums)\n        >>> print('--- Numpy as NUMPY---')\n        >>> np_rng = np.random.RandomState(seed=0)\n        >>> nn_nums = [np_rng.rand() for _ in range(num)]\n        >>> print(nn_nums)\n        >>> print('--- Python as NUMPY---')\n        >>> py_rng = ut.ensure_rng(np.random.RandomState(seed=0), impl='python')\n        >>> pn_nums = [py_rng.random() for _ in range(num)]\n        >>> print(pn_nums)\n        >>> assert np_nums == pp_nums\n        >>> assert pn_nums == nn_nums\n    \"\"\"\n    if impl == 'numpy':\n        if rng is None:\n            rng = np.random\n        elif isinstance(rng, int):\n            rng = np.random.RandomState(seed=rng)\n        elif isinstance(rng, random.Random):\n            # Convert python to numpy random state\n            py_rng = rng\n            pystate = py_rng.getstate()\n            npstate = _pystate_to_npstate(pystate)\n            rng = np_rng = np.random.RandomState(seed=0)\n            np_rng.set_state(npstate)\n    elif impl == 'python':\n        if rng is None:\n            rng = random\n        elif isinstance(rng, int):\n            rng = random.Random(rng)\n        elif isinstance(rng, np.random.RandomState):\n            # Convert numpy to python random state\n            np_rng = rng\n            npstate = np_rng.get_state()\n            pystate = _npstate_to_pystate(npstate)\n            rng = py_rng = random.Random(0)\n            py_rng.setstate(pystate)\n    else:\n        raise KeyError('unknown rng impl={}'.format(impl))\n    return rng"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef random_indexes(max_index, subset_size=None, seed=None, rng=None):\n    subst_ = np.arange(0, max_index)\n    rng = ensure_rng(seed if rng is None else rng)\n    rng.shuffle(subst_)\n    if subset_size is None:\n        subst = subst_\n    else:\n        subst = subst_[0:min(subset_size, max_index)]\n    return subst", "response": "random unrepeated indicies\n\n    Args:\n        max_index (?):\n        subset_size (None): (default = None)\n        seed (None): (default = None)\n        rng (RandomState):  random number generator(default = None)\n\n    Returns:\n        ?: subst\n\n    CommandLine:\n        python -m utool.util_numpy --exec-random_indexes\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_numpy import *  # NOQA\n        >>> max_index = 10\n        >>> subset_size = None\n        >>> seed = None\n        >>> rng = np.random.RandomState(0)\n        >>> subst = random_indexes(max_index, subset_size, seed, rng)\n        >>> result = ('subst = %s' % (str(subst),))\n        >>> print(result)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning n evenly spaced indexes.", "response": "def spaced_indexes(len_, n, trunc=False):\n    \"\"\"\n    Returns n evenly spaced indexes.\n    Returns as many as possible if trunc is true\n    \"\"\"\n\n    if n is None:\n        return np.arange(len_)\n    all_indexes = np.arange(len_)\n    if trunc:\n        n = min(len_, n)\n    if n == 0:\n        return np.empty(0)\n    stride = len_ // n\n    try:\n        indexes = all_indexes[0:-1:stride]\n    except ValueError:\n        raise ValueError('cannot slice list of len_=%r into n=%r parts' % (len_, n))\n    return indexes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nintersecting - function that returns the intersection of two numpy arrays", "response": "def intersect2d(A, B):\n    \"\"\"\n    intersect2d\n\n    intersect rows of 2d numpy arrays\n\n    DEPRICATE: use intersect2d in vtool instead\n\n    Args:\n        A (ndarray[ndim=2]):\n        B (ndarray[ndim=2]):\n\n    Returns:\n        tuple: (C, Ax, Bx)\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_numpy import *  # NOQA\n        >>> import utool as ut\n        >>> A = np.array([[1, 2, 3], [1, 1, 1]])\n        >>> B = np.array([[1, 2, 3], [1, 2, 14]])\n        >>> (C, Ax, Bx) = ut.intersect2d(A, B)\n        >>> result = str((C, Ax, Bx))\n        >>> print(result)\n        (array([[1, 2, 3]]), array([0]), array([0]))\n    \"\"\"\n    Cset  =  set(tuple(x) for x in A).intersection(set(tuple(x) for x in B))\n    Ax = np.array([x for x, item in enumerate(A) if tuple(item) in Cset], dtype=np.int)\n    Bx = np.array([x for x, item in enumerate(B) if tuple(item) in Cset], dtype=np.int)\n    C = np.array(tuple(Cset))\n    return C, Ax, Bx"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef deterministic_shuffle(list_, seed=0, rng=None):\n    rng = ensure_rng(seed if rng is None else rng)\n    rng.shuffle(list_)\n    return list_", "response": "r Samples a list of the n - tuple of items in the order of the items in the order they appear."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of random samples from the list", "response": "def random_sample(list_, nSample, strict=False, rng=None, seed=None):\n    \"\"\"\n    Grabs data randomly\n\n    Args:\n        list_ (list):\n        nSample (?):\n        strict (bool): (default = False)\n        rng (module):  random number generator(default = numpy.random)\n        seed (None): (default = None)\n\n    Returns:\n        list: sample_list\n\n    CommandLine:\n        python -m utool.util_numpy --exec-random_sample\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_numpy import *  # NOQA\n        >>> list_ = np.arange(10)\n        >>> nSample = 4\n        >>> strict = False\n        >>> rng = np.random.RandomState(0)\n        >>> seed = None\n        >>> sample_list = random_sample(list_, nSample, strict, rng, seed)\n        >>> result = ('sample_list = %s' % (str(sample_list),))\n        >>> print(result)\n    \"\"\"\n    rng = ensure_rng(seed if rng is None else rng)\n    if isinstance(list_, list):\n        list2_ = list_[:]\n    else:\n        list2_ = np.copy(list_)\n    if len(list2_) == 0 and not strict:\n        return list2_\n    rng.shuffle(list2_)\n    if nSample is None and strict is False:\n        return list2_\n    if not strict:\n        nSample = min(max(0, nSample), len(list2_))\n    sample_list = list2_[:nSample]\n    return sample_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef deterministic_sample(list_, nSample, seed=0, rng=None, strict=False):\n    rng = ensure_rng(seed if rng is None else rng)\n    sample_list = random_sample(list_, nSample, strict=strict, rng=rng)\n    return sample_list", "response": "Grabs data randomly but in a repeatable way"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning n evenly spaced items", "response": "def spaced_items(list_, n, **kwargs):\n    \"\"\" Returns n evenly spaced items \"\"\"\n    indexes = spaced_indexes(len(list_), n, **kwargs)\n    items = list_[indexes]\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsampling the domain of the nSamp items.", "response": "def sample_domain(min_, max_, nSamp, mode='linear'):\n    \"\"\"\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> import utool\n        >>> min_ = 10\n        >>> max_ = 1000\n        >>> nSamp  = 7\n        >>> result = utool.sample_domain(min_, max_, nSamp)\n        >>> print(result)\n        [10, 151, 293, 434, 576, 717, 859]\n    \"\"\"\n    if mode == 'linear':\n        samples_ = np.rint(np.linspace(min_, max_, nSamp + 1)).astype(np.int64)\n    elif mode == 'log':\n        base = 2\n        logmin = np.log2(min_) / np.log2(base)\n        logmax = np.log2(max_) / np.log2(base)\n        samples_ = np.rint(np.logspace(logmin, logmax, nSamp + 1, base=base)).astype(np.int64)\n    else:\n        raise NotImplementedError(mode)\n    sample = [index for index in samples_ if index < max_]\n    return sample"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of servers that can serve the given topic.", "response": "def get_servers(self, topic):\n        \"\"\"We're assuming that the static list of servers can serve the given \n        topic, since we have to preexisting knowledge about them.\n        \"\"\"\n\n        return (nsq.node.ServerNode(sh) for sh in self.__server_hosts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _molfile(stream):\n    yield MolfileStart()\n    yield HeaderBlock(stream.popleft().strip(), stream.popleft().strip(), stream.popleft().strip())\n    # yield from _ctab(stream)\n    for token in _ctab(stream):\n        yield token\n    yield MolfileEnd()", "response": "Process ``Molfile``.\n    \n    :param stream: Queue containing lines of text.\n    :type stream: :py:class:`collections.deque`\n    :return: Tuples of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing ``Ctab``. :param stream: Queue containing lines of text. :type stream: :py:class:`collections.deque` :return: Tuples of data.", "response": "def _ctab(stream):\n    \"\"\"Process ``Ctab``.\n    \n    :param stream: Queue containing lines of text.\n    :type stream: :py:class:`collections.deque`\n    :return: Tuples of data.\n    \"\"\"\n    yield CtabBlockStart()\n    counts_line = stream.popleft()\n    counts_line_values = [counts_line[i:i + 3].strip() for i in range(0, len(counts_line) - 6, 3)] + \\\n                         [counts_line[-6:len(counts_line)].strip()]\n    ctab_counts_line = CtabCountsLine(*counts_line_values)\n    yield ctab_counts_line\n\n    number_of_atoms = ctab_counts_line.number_of_atoms\n    number_of_bonds = ctab_counts_line.number_of_bonds\n\n    # yield from _ctab_atom_bond_block(number_of_lines=number_of_atoms, block_type=CtabAtomBlockLine, stream=stream)\n    for token in _ctab_atom_bond_block(number_of_lines=number_of_atoms, block_type=CtabAtomBlockLine, stream=stream):\n        yield token\n\n    # yield from _ctab_atom_bond_block(number_of_lines=number_of_bonds, block_type=CtabBondBlockLine, stream=stream)\n    for token in _ctab_atom_bond_block(number_of_lines=number_of_bonds, block_type=CtabBondBlockLine, stream=stream):\n        yield token\n\n    # yield from _ctab_property_block(stream=stream)\n    for token in _ctab_property_block(stream=stream):\n        yield token\n\n    yield CtabBlockEnd()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing atom and bond blocks of Ctab.", "response": "def _ctab_atom_bond_block(number_of_lines, block_type, stream):\n    \"\"\"Process atom and bond blocks of ``Ctab``.\n\n    :param number_of_lines: Number of lines to process from stream.\n    :param block_type: :py:class:`collections.namedtuple` to use for data processing.\n    :type block_type: :class:`~ctfile.tokenizer.CtabAtomBlockLine` or :class:`~ctfile.tokenizer.CtabBondBlockLine`\n    :param stream: Queue containing lines of text.\n    :type stream: :py:class:`collections.deque`\n    :return: Tuples of data.\n    :rtype: :class:`~ctfile.tokenizer.CtabAtomBlockLine` or :class:`~ctfile.tokenizer.CtabBondBlockLine`\n    \"\"\"\n    for _ in range(int(number_of_lines)):\n        line = stream.popleft()\n        yield block_type(*line.split())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess properties block of Ctab.", "response": "def _ctab_property_block(stream):\n    \"\"\"Process properties block of ``Ctab``.\n\n    :param stream: Queue containing lines of text.\n    :type stream: :py:class:`collections.deque`\n    :return: Tuples of data.\n    :rtype: :class:`~ctfile.tokenizer.CtabPropertiesBlockLine`\n    \"\"\"\n    line = stream.popleft()\n    while line != 'M  END':\n        name = line.split()[1]\n        yield CtabPropertiesBlockLine(name, line)\n        line = stream.popleft()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _data_block(stream):\n    while len(stream) > 0:\n        line = stream.popleft()\n\n        if line.startswith('>'):\n            yield DataHeader(line[1:].strip())\n        else:\n            data_item = line.strip()\n            if data_item:\n                yield DataItem(line)\n            else:\n                continue", "response": "Process data block of CTfile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_features(self):\n        self.scores = {}\n        for t_or_d, feats in zip(['target', 'decoy'], [self.target,\n                                                       self.decoy]):\n            self.scores[t_or_d] = {}\n            self.scores[t_or_d]['scores'] = self.score_get_fun(\n                feats, self.featuretype, self.prepare_percolator_output)\n            self.scores[t_or_d]['fn'] = '{}_qvality_input.txt'.format(t_or_d)\n            writers.write_qvality_input(self.scores[t_or_d]['scores'],\n                                        self.scores[t_or_d]['fn'])", "response": "Creates scorefiles for target and decoy distributions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_text(fname, text, repo_dpath='.', force=None, locals_={}, chmod=None):\n    import utool as ut\n    ut.colorprint('Ensuring fname=%r' % (fname), 'yellow')\n\n    # if not fname.endswith('__init__.py'):\n    #     # HACK\n    #     return\n\n    if force is None and ut.get_argflag('--force-%s' % (fname,)):\n        force = True\n    text_ = ut.remove_codeblock_syntax_sentinals(text)\n    fmtkw = locals_.copy()\n    fmtkw['fname'] = fname\n    text_ = text_.format(**fmtkw) + '\\n'\n\n    fpath = join(repo_dpath, fname)\n    ut.dump_autogen_code(fpath, text_)", "response": "Ensures that the given text is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_repo():\n    print('\\n [setup_repo]!')\n    # import os\n    from functools import partial\n    import utool as ut\n    # import os\n    code_dpath  = ut.truepath(ut.get_argval('--code-dir', default='~/code'))\n    _code_dpath = ut.unexpanduser(code_dpath)\n    repo_fname = (ut.get_argval(('--repo', '--repo-name'), type_=str))\n    repo_dpath = join(code_dpath, repo_fname)\n    modname = ut.get_argval('--modname', default=repo_fname)\n    ut.ensuredir(repo_dpath, verbose=True)\n    _regencmd = 'python -m utool --tf setup_repo --repo={repo_fname} --codedir={_code_dpath} --modname={modname}'\n    flake8_noqacmd = 'flake8' + ':noqa'\n    regencmd = _regencmd.format(**locals())\n    with ut.ChdirContext(repo_dpath):\n        # os.chdir(repo_fname)\n        locals_ = locals()\n        force = True\n\n        _ensure_text = partial(ensure_text, repo_dpath='.', force=None, locals_=locals_)\n\n        _ensure_text(\n            fname='todo.md',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                # {modname} TODO File\n\n                * Add TODOS!\n                # ENDBLOCK\n                ''')\n        )\n\n        _ensure_text(\n            fname='README.md',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                # {modname} README FILE\n                # ENDBLOCK\n                ''')\n        )\n\n        _ensure_text(\n            fname='setup.py',\n            chmod='+x',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                #!/usr/bin/env python\n                \"\"\"\n                Initially Generated By:\n                    {regencmd} --force-{fname}\n                \"\"\"\n                from __future__ import absolute_import, division, print_function, unicode_literals\n                from setuptools import setup\n                try:\n                    from utool import util_setup\n                except ImportError:\n                    print('ERROR: setup requires utool')\n                    raise\n\n                INSTALL_REQUIRES = [\n                    #'cython >= 0.21.1',\n                    #'numpy >= 1.9.0',\n                    #'scipy >= 0.16.0',\n                ]\n\n                CLUTTER_PATTERNS = [\n                    # Patterns removed by python setup.py clean\n                ]\n\n                if __name__ == '__main__':\n                    kwargs = util_setup.setuptools_setup(\n                        setup_fpath=__file__,\n                        name='{modname}',\n                        packages=util_setup.find_packages(),\n                        version=util_setup.parse_package_for_version('{modname}'),\n                        license=util_setup.read_license('LICENSE'),\n                        long_description=util_setup.parse_readme('README.md'),\n                        ext_modules=util_setup.find_ext_modules(),\n                        cmdclass=util_setup.get_cmdclass(),\n                        #description='description of module',\n                        #url='https://github.com/<username>/{repo_fname}.git',\n                        #author='<author>',\n                        #author_email='<author_email>',\n                        keywords='',\n                        install_requires=INSTALL_REQUIRES,\n                        clutter_patterns=CLUTTER_PATTERNS,\n                        #package_data={{'build': ut.get_dynamic_lib_globstrs()}},\n                        #build_command=lambda: ut.std_build_command(dirname(__file__)),\n                        classifiers=[],\n                    )\n                    setup(**kwargs)\n                # ENDBLOCK\n                '''\n            )\n        )\n\n        _ensure_text(\n            fname='.gitignore',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                *.py[cod]\n\n                # C extensions\n                *.so\n                # Packages\n                *.egg\n                *.egg-info\n                dist\n                build\n                eggs\n                parts\n                bin\n                var\n                sdist\n                develop-eggs\n                .installed.cfg\n                lib\n                lib64\n                __pycache__\n\n                # Installer logs\n                pip-log.txt\n\n                # Print Logs\n                logs\n\n                # Unit test / coverage reports\n                .coverage\n                .tox\n                nosetests.xml\n\n                # Translations\n                *.mo\n\n                # Mr Developer\n                .mr.developer.cfg\n                .project\n                .pydevproject\n                .DS_Store\n                *.dump.txt\n                *.sqlite3\n\n                # profiler\n                *.lprof\n                *.prof\n\n                *.flann\n                *.npz\n\n                # utool output\n                _timeings.txt\n                failed.txt\n\n                *.orig\n                _doc\n                timeings.txt\n                failed_doctests.txt\n                # ENDBLOCK\n                '''\n            )\n        )\n\n        _ensure_text(\n            fname=join(repo_dpath, modname, '__init__.py'),\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                # -*- coding: utf-8 -*-\n                # {flake8_noqacmd}\n                \"\"\"\n                Initially Generated By:\n                    {regencmd}\n                \"\"\"\n                from __future__ import absolute_import, division, print_function, unicode_literals\n                import sys\n                __version__ = '0.0.0'\n\n                IMPORT_TUPLES = [\n                    # ('<modname>', None),\n                ]\n                __DYNAMIC__ = '--nodyn' not in sys.argv\n\n                \"\"\"\n                python -c \"import {modname}\" --dump-{modname}-init\n                python -c \"import {modname}\" --update-{modname}-init\n                \"\"\"\n\n                DOELSE = False\n                if __DYNAMIC__:\n                    # Dynamically import listed util libraries and their members.\n                    from utool._internal import util_importer\n                    ignore_endswith = []\n                    import_execstr = util_importer.dynamic_import(\n                        __name__, IMPORT_TUPLES, ignore_endswith=ignore_endswith)\n                    exec(import_execstr)\n                    DOELSE = False\n                else:\n                    DOELSE = True\n                if DOELSE:\n                    # <AUTOGEN_INIT>\n                    pass\n                    # </AUTOGEN_INIT>\n                # ENDBLOCK\n                '''\n            )\n        )\n\n        _ensure_text(\n            fname=join(repo_dpath, modname, '__main__.py'),\n            chmod='+x',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                #!/usr/bin/env python\n                # -*- coding: utf-8 -*-\n                \"\"\"\n                Initially Generated By:\n                    {regencmd}\n                \"\"\"\n                from __future__ import absolute_import, division, print_function, unicode_literals\n\n\n                def {modname}_main():\n                    ignore_prefix = []\n                    ignore_suffix = []\n                    import utool as ut\n                    ut.main_function_tester('{modname}', ignore_prefix, ignore_suffix)\n\n                if __name__ == '__main__':\n                    \"\"\"\n                    Usage:\n                        python -m {modname} <funcname>\n                    \"\"\"\n                    print('Running {modname} main')\n                    {modname}_main()\n                # ENDBLOCK\n                '''\n            )\n        )\n\n        _ensure_text(\n            fname='run_tests.py',\n            chmod='+x',\n            text=ut.codeblock(\n                r'''\n                # STARTBLOCK\n                #!/usr/bin/env python\n                \"\"\"\n                Initially Generated By:\n                    {regencmd} --force-{fname}\n                \"\"\"\n                from __future__ import absolute_import, division, print_function\n                import sys\n                import utool as ut\n\n\n                def run_tests():\n                    # Build module list and run tests\n                    import sys\n                    ut.change_term_title('RUN {modname} TESTS')\n                    exclude_doctests_fnames = set([\n                    ])\n                    exclude_dirs = [\n                        '_broken', 'old', 'tests', 'timeits',\n                        '_scripts', '_timeits', '_doc', 'notebook',\n                    ]\n                    dpath_list = ['{modname}']\n                    doctest_modname_list = ut.find_doctestable_modnames(\n                        dpath_list, exclude_doctests_fnames, exclude_dirs)\n\n                    coverage = ut.get_argflag(('--coverage', '--cov',))\n                    if coverage:\n                        import coverage\n                        cov = coverage.Coverage(source=doctest_modname_list)\n                        cov.start()\n                        print('Starting coverage')\n\n                        exclude_lines = [\n                            'pragma: no cover',\n                            'def __repr__',\n                            'if self.debug:',\n                            'if settings.DEBUG',\n                            'raise AssertionError',\n                            'raise NotImplementedError',\n                            'if 0:',\n                            'if ut.VERBOSE',\n                            'if _debug:',\n                            'if __name__ == .__main__.:',\n                            'print(.*)',\n                        ]\n                        for line in exclude_lines:\n                            cov.exclude(line)\n\n                    for modname in doctest_modname_list:\n                        exec('import ' + modname, globals())\n                    module_list = [sys.modules[name] for name in doctest_modname_list]\n\n                    nPass, nTotal, failed_cmd_list = ut.doctest_module_list(module_list)\n\n                    if coverage:\n                        print('Stoping coverage')\n                        cov.stop()\n                        print('Saving coverage')\n                        cov.save()\n                        print('Generating coverage html report')\n                        cov.html_report()\n\n                    if nPass != nTotal:\n                        return 1\n                    else:\n                        return 0\n\n                if __name__ == '__main__':\n                    import multiprocessing\n                    multiprocessing.freeze_support()\n                    retcode = run_tests()\n                    sys.exit(retcode)\n                # ENDBLOCK\n                '''\n            )\n        )\n\n    ut.ensuredir(join(repo_dpath, modname), verbose=True)", "response": "r Sets up the default structure for a new repository"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grep_projects(tofind_list, user_profile=None, verbose=True, new=False,\n                  **kwargs):\n    r\"\"\"\n    Greps the projects defined in the current UserProfile\n\n    Args:\n        tofind_list (list):\n        user_profile (None): (default = None)\n\n    Kwargs:\n        user_profile\n\n    CommandLine:\n        python -m utool --tf grep_projects grep_projects\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_project import *  # NOQA\n        >>> import utool as ut\n        >>> import sys\n        >>> tofind_list = ut.get_argval('--find', type_=list,\n        >>>                             default=[sys.argv[-1]])\n        >>> grep_projects(tofind_list)\n    \"\"\"\n    import utool as ut\n    user_profile = ensure_user_profile(user_profile)\n    print('user_profile = {!r}'.format(user_profile))\n\n    kwargs = kwargs.copy()\n    colored = kwargs.pop('colored', True)\n\n    grepkw = {}\n    grepkw['greater_exclude_dirs'] = user_profile.project_exclude_dirs\n    grepkw['exclude_dirs'] = user_profile.project_exclude_dirs\n    grepkw['dpath_list'] = user_profile.project_dpaths\n    grepkw['include_patterns'] = user_profile.project_include_patterns\n    grepkw['exclude_patterns'] = user_profile.project_exclude_patterns\n    grepkw.update(kwargs)\n\n    msg_list1 = []\n    msg_list2 = []\n\n    print_ = msg_list1.append\n    print_('Greping Projects')\n    print_('tofind_list = %s' % (ut.repr4(tofind_list, nl=True),))\n    #print_('grepkw = %s' % ut.repr4(grepkw, nl=True))\n    if verbose:\n        print('\\n'.join(msg_list1))\n    #with ut.Timer('greping', verbose=True):\n    grep_result = ut.grep(tofind_list, **grepkw)\n    found_fpath_list, found_lines_list, found_lxs_list = grep_result\n\n    # HACK, duplicate behavior. TODO: write grep print result function\n    reflags = grepkw.get('reflags', 0)\n    _exprs_flags = [ut.extend_regex2(expr, reflags)\n                    for expr in tofind_list]\n    extended_regex_list = ut.take_column(_exprs_flags, 0)\n    reflags_list = ut.take_column(_exprs_flags, 1)\n    # HACK\n    # pat = ut.util_regex.regex_or(extended_regex_list)\n    reflags = reflags_list[0]\n\n    # from utool import util_regex\n    resultstr = ut.make_grep_resultstr(grep_result, extended_regex_list,\n                                       reflags, colored=colored)\n    msg_list2.append(resultstr)\n    print_ = msg_list2.append\n    #for fpath, lines, lxs in zip(found_fpath_list, found_lines_list,\n    #                             found_lxs_list):\n    #    print_('----------------------')\n    #    print_('found %d line(s) in %r: ' % (len(lines), fpath))\n    #    name = split(fpath)[1]\n    #    max_line = len(lines)\n    #    ndigits = str(len(str(max_line)))\n    #    for (lx, line) in zip(lxs, lines):\n    #        line = line.replace('\\n', '')\n    #        print_(('%s : %' + ndigits + 'd |%s') % (name, lx, line))\n    # iter_ = zip(found_fpath_list, found_lines_list, found_lxs_list)\n    # for fpath, lines, lxs in iter_:\n    #     print_('----------------------')\n    #     print_('found %d line(s) in %r: ' % (len(lines), fpath))\n    #     name = split(fpath)[1]\n    #     max_line = len(lines)\n    #     ndigits = str(len(str(max_line)))\n    #     for (lx, line) in zip(lxs, lines):\n    #         line = line.replace('\\n', '')\n    #         colored_line = ut.highlight_regex(\n    #             line.rstrip('\\n'), pat, reflags=reflags)\n    #         print_(('%s : %' + ndigits + 'd |%s') % (name, lx, colored_line))\n\n    print_('====================')\n    print_('found_fpath_list = ' + ut.repr4(found_fpath_list))\n    print_('')\n    #print_('gvim -o ' + ' '.join(found_fpath_list))\n    if verbose:\n        print('\\n'.join(msg_list2))\n    msg_list = msg_list1 + msg_list2\n\n    if new:\n        return GrepResult(found_fpath_list, found_lines_list, found_lxs_list,\n                          extended_regex_list, reflags)\n    else:\n        return msg_list", "response": "r Greps the projects defined in the current UserProfile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buffered_generator(source_gen, buffer_size=2, use_multiprocessing=False):\n    if buffer_size < 2:\n        raise RuntimeError(\"Minimal buffer_ size is 2!\")\n\n    if use_multiprocessing:\n        print('WARNING seems to freeze if passed in a generator')\n        #assert False, 'dont use this buffered multiprocessing'\n        if False:\n            pool = multiprocessing.Pool(processes=get_default_numprocs(),\n                                        initializer=init_worker,\n                                        maxtasksperchild=None)\n            Process = pool.Process\n        else:\n            Process = multiprocessing.Process\n        _Queue = multiprocessing.Queue\n        target = _buffered_generation_process\n    else:\n        _Queue = queue.Queue\n        Process = KillableThread\n        target = _buffered_generation_thread\n\n    # the effective buffer_ size is one less, because the generation process\n    # will generate one extra element and block until there is room in the\n    # buffer_.\n    buffer_ = _Queue(maxsize=buffer_size - 1)\n\n    # previously None was used as a sentinal, which fails when source_gen\n    # genrates None need to make object that it will not be generated by the\n    # process. A reasonable hack is to use the StopIteration exception instead\n    sentinal = StopIteration\n\n    process = Process(\n        target=target,\n        args=(iter(source_gen), buffer_, sentinal)\n    )\n    #if not use_multiprocessing:\n    process.daemon = True\n\n    process.start()\n\n    while True:\n        #output = buffer_.get(timeout=1.0)\n        output = buffer_.get()\n        if output is sentinal:\n            raise StopIteration\n        yield output", "response": "r A slow generator that runs a slow source generator in a separate process."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the buffered generator", "response": "def _buffered_generation_process(source_gen, buffer_, sentinal):\n    \"\"\" helper for buffered_generator \"\"\"\n    for data in source_gen:\n        buffer_.put(data, block=True)\n    # sentinel: signal the end of the iterator\n    buffer_.put(sentinal)\n    # unfortunately this does not suffice as a signal: if buffer_.get() was\n    # called and subsequently the buffer_ is closed, it will block forever.\n    buffer_.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nspawns a background process in the background.", "response": "def spawn_background_process(func, *args, **kwargs):\n    \"\"\"\n    Run a function in the background\n    (like rebuilding some costly data structure)\n\n    References:\n        http://stackoverflow.com/questions/2046603/is-it-possible-to-run-function-in-a-subprocess-without-threading-or-writing-a-se\n        http://stackoverflow.com/questions/1196074/starting-a-background-process-in-python\n        http://stackoverflow.com/questions/15063963/python-is-thread-still-running\n\n    Args:\n        func (function):\n\n    CommandLine:\n        python -m utool.util_parallel --test-spawn_background_process\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # SLOW_DOCTEST\n        >>> from utool.util_parallel import *  # NOQA\n        >>> import utool as ut\n        >>> import time\n        >>> from os.path import join\n        >>> # build test data\n        >>> fname = 'test_bgfunc_output.txt'\n        >>> dpath = ut.get_app_resource_dir('utool')\n        >>> ut.ensuredir(dpath)\n        >>> fpath = join(dpath, fname)\n        >>> # ensure file is not around\n        >>> sleep_time = 1\n        >>> ut.delete(fpath)\n        >>> assert not ut.checkpath(fpath, verbose=True)\n        >>> def backgrond_func(fpath, sleep_time):\n        ...     import utool as ut\n        ...     import time\n        ...     print('[BG] Background Process has started')\n        ...     time.sleep(sleep_time)\n        ...     print('[BG] Background Process is writing')\n        ...     ut.write_to(fpath, 'background process')\n        ...     print('[BG] Background Process has finished')\n        ...     #raise AssertionError('test exception')\n        >>> # execute function\n        >>> func = backgrond_func\n        >>> args = (fpath, sleep_time)\n        >>> kwargs = {}\n        >>> print('[FG] Spawning process')\n        >>> threadid = ut.spawn_background_process(func, *args, **kwargs)\n        >>> assert threadid.is_alive() is True, 'thread should be active'\n        >>> print('[FG] Spawned process. threadid=%r' % (threadid,))\n        >>> # background process should not have finished yet\n        >>> assert not ut.checkpath(fpath, verbose=True)\n        >>> print('[FG] Waiting to check')\n        >>> time.sleep(sleep_time + .1)\n        >>> print('[FG] Finished waiting')\n        >>> # Now the file should be there\n        >>> assert ut.checkpath(fpath, verbose=True)\n        >>> assert threadid.is_alive() is False, 'process should have died'\n    \"\"\"\n    import utool as ut\n    func_name = ut.get_funcname(func)\n    name = 'mp.Progress-' + func_name\n    #proc_obj = multiprocessing.Process(target=func, name=name, args=args, kwargs=kwargs)\n    proc_obj = KillableProcess(target=func, name=name, args=args, kwargs=kwargs)\n    #proc_obj.daemon = True\n    #proc_obj.isAlive = proc_obj.is_alive\n    proc_obj.start()\n    return proc_obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the nearest point in pts to x y", "response": "def nearest_point(x, y, pts, mode='random'):\n    \"\"\" finds the nearest point(s) in pts to (x, y)\n    FIXME VIZ FEATROW\n    \"\"\"\n    dists = (pts.T[0] - x) ** 2 + (pts.T[1] - y) ** 2\n    fx = dists.argmin()\n    mindist = dists[fx]\n    other_fx = np.where(mindist == dists)[0]\n    if len(other_fx) > 0:\n        if mode == 'random':\n            np.random.shuffle(other_fx)\n            fx = other_fx[0]\n        if mode == 'all':\n            fx = other_fx\n        if mode == 'first':\n            fx = fx\n    return fx, mindist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_new_mimetype_association(ext, mime_name, exe_fpath=None, dry=True):\n    import utool as ut\n    terminal = True\n\n    mime_codeblock = ut.codeblock(\n        '''\n        <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n        <mime-info xmlns=\"http://www.freedesktop.org/standards/shared-mime-info\">\n            <mime-type type=\"application/x-{mime_name}\">\n                <glob-deleteall/>\n                <glob pattern=\"*{ext}\"/>\n            </mime-type>\n        </mime-info>\n        '''\n    ).format(**locals())\n\n    prefix = ut.truepath('~/.local/share')\n    mime_dpath = join(prefix, 'mime/packages')\n    mime_fpath = join(mime_dpath, 'application-x-{mime_name}.xml'.format(**locals()))\n\n    print(mime_codeblock)\n    print('---')\n    print(mime_fpath)\n    print('L___')\n\n    if exe_fpath is not None:\n        exe_fname_noext = splitext(basename(exe_fpath))[0]\n        app_name = exe_fname_noext.replace('_', '-')\n        nice_name = ' '.join(\n            [word[0].upper() + word[1:].lower()\n             for word in app_name.replace('-', ' ').split(' ')]\n        )\n        app_codeblock = ut.codeblock(\n            '''\n            [Desktop Entry]\n            Name={nice_name}\n            Exec={exe_fpath}\n            MimeType=application/x-{mime_name}\n            Terminal={terminal}\n            Type=Application\n            Categories=Utility;Application;\n            Comment=Custom App\n            '''\n        ).format(**locals())\n        app_dpath = join(prefix, 'applications')\n        app_fpath = join(app_dpath, '{app_name}.desktop'.format(**locals()))\n\n        print(app_codeblock)\n        print('---')\n        print(app_fpath)\n        print('L___')\n\n    # WRITE FILES\n    if not dry:\n        ut.ensuredir(mime_dpath)\n        ut.ensuredir(app_dpath)\n\n        ut.writeto(mime_fpath, mime_codeblock, verbose=ut.NOT_QUIET, n=None)\n        if exe_fpath is not None:\n            ut.writeto(app_fpath, app_codeblock, verbose=ut.NOT_QUIET, n=None)\n\n        # UPDATE BACKENDS\n\n        #ut.cmd('update-mime-database /usr/share/mime')\n        #~/.local/share/applications/mimeapps.list\n        print(ut.codeblock(\n            '''\n            Run these commands:\n            update-desktop-database ~/.local/share/applications\n            update-mime-database ~/.local/share/mime\n            '''\n        ))\n        if exe_fpath is not None:\n            ut.cmd('update-desktop-database ~/.local/share/applications')\n        ut.cmd('update-mime-database ~/.local/share/mime')\n    else:\n        print('dry_run')", "response": "This function will add a new mimetype association to the internal manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_application_icon(exe_fpath, dry=True, props={}):\n    import utool as ut\n    exe_fname_noext = splitext(basename(exe_fpath))[0]\n    app_name = exe_fname_noext.replace('_', '-')\n    nice_name = ' '.join(\n        [word[0].upper() + word[1:].lower()\n         for word in app_name.replace('-', ' ').split(' ')]\n    )\n    lines = [\n        '[Desktop Entry]',\n        'Name={nice_name}',\n        'Exec={exe_fpath}',\n    ]\n\n    if 'mime_name' in props:\n        lines += ['MimeType=application/x-{mime_name}']\n\n    if 'icon' in props:\n        lines += ['Icon={icon}']\n\n    if props.get('path'):\n        lines += ['Path={path}']\n\n    # if props.get('comment'):\n    #     lines += ['Path={comment}']\n\n    lines += [\n        'Terminal={terminal}',\n        'Type=Application',\n        'Categories=Utility;Application;',\n        'Comment=Custom App',\n    ]\n    fmtdict = locals()\n    fmtdict.update(props)\n\n    prefix = ut.truepath('~/.local/share')\n    app_codeblock = '\\n'.join(lines).format(**fmtdict)\n    app_dpath = join(prefix, 'applications')\n    app_fpath = join(app_dpath, '{app_name}.desktop'.format(**locals()))\n\n    print(app_codeblock)\n    print('---')\n    print(app_fpath)\n    print('L___')\n\n    if not dry:\n        ut.writeto(app_fpath, app_codeblock, verbose=ut.NOT_QUIET, n=None)\n        ut.cmd('update-desktop-database ~/.local/share/applications')", "response": "A function that creates a new application icon"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove a window to a new bounding box", "response": "def move_window(win_key, bbox):\n        \"\"\"\n        CommandLine:\n            # List windows\n            wmctrl -l\n            # List desktops\n            wmctrl -d\n\n            # Window info\n            xwininfo -id 60817412\n\n            python -m utool.util_ubuntu XCtrl.move_window joncrall 0+1920,680,400,600,400\n            python -m utool.util_ubuntu XCtrl.move_window joncrall [0,0,1000,1000]\n            python -m utool.util_ubuntu XCtrl.move_window GVIM special2\n            python -m utool.util_ubuntu XCtrl.move_window joncrall special2\n            python -m utool.util_ubuntu XCtrl.move_window x-terminal-emulator.X-terminal-emulator [0,0,1000,1000]\n\n        # >>> import utool as ut\n        # >>> from utool import util_ubuntu\n        # >>> orig_window = []\n        # >>> X = util_ubuntu.XCtrl\n        win_key =  'x-terminal-emulator.X-terminal-emulator'\n        win_id = X.findall_window_ids(key)[0]\n\n        python -m utool.util_ubuntu XCtrl.findall_window_ids gvim --src\n\n        \"\"\"\n        import utool as ut\n        import plottool as pt  # NOQA\n        import plottool.screeninfo as screeninfo\n        monitor_infos = {\n            i + 1: screeninfo.get_resolution_info(i)\n            for i in range(2)\n        }\n        # TODO: cut out borders\n        # TODO: fix screeninfo monitor offsets\n        # TODO: dynamic num screens\n        def rel_to_abs_bbox(m, x, y, w, h):\n            \"\"\" monitor_num, relative x, y, w, h \"\"\"\n            minfo = monitor_infos[m]\n            # print('minfo(%d) = %s' % (m, ut.repr3(minfo),))\n            mx, my = minfo['off_x'], minfo['off_y']\n            mw, mh = minfo['pixels_w'], minfo['pixels_h']\n            # Transform to the absolution position\n            abs_x = (x * mw) + mx\n            abs_y = (y * mh) + my\n            abs_w = (w * mw)\n            abs_h = (h * mh)\n            abs_bbox = [abs_x, abs_y, abs_w, abs_h]\n            abs_bbox = ','.join(map(str, map(int, abs_bbox)))\n            return abs_bbox\n\n        if win_key.startswith('joncrall') and bbox == 'special2':\n            # Specify the relative position\n            abs_bbox = rel_to_abs_bbox(m=2,\n                                       x=0.0, y=0.7,\n                                       w=1.0, h=0.3)\n        elif win_key.startswith('GVIM') and bbox == 'special2':\n            # Specify the relative position\n            abs_bbox = rel_to_abs_bbox(m=2,\n                                       x=0.0, y=0.0,\n                                       w=1.0, h=0.7)\n        else:\n            abs_bbox = ','.join(map(str, eval(bbox)))\n\n        print('MOVING: win_key = %r' % (win_key,))\n        print('TO: abs_bbox = %r' % (abs_bbox,))\n        # abs_bbox.replace('[', '').replace(']', '')\n        # get = lambda cmd: ut.cmd2(' '.join([\"/bin/bash\", \"-c\", cmd]))['out']  # NOQA\n        win_id = XCtrl.find_window_id(win_key, error='raise')\n        print('MOVING: win_id = %r' % (win_id,))\n        fmtdict = locals()\n        cmd_list = [\n            (\"wmctrl -ir {win_id} -b remove,maximized_horz\".format(**fmtdict)),\n            (\"wmctrl -ir {win_id} -b remove,maximized_vert\".format(**fmtdict)),\n            (\"wmctrl -ir {win_id} -e 0,{abs_bbox}\".format(**fmtdict)),\n        ]\n        print('\\n'.join(cmd_list))\n        for cmd in cmd_list:\n            ut.cmd2(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of window IDs that match a pattern", "response": "def findall_window_ids(pattern):\n        \"\"\"\n        CommandLine:\n            wmctrl  -l\n            python -m utool.util_ubuntu XCtrl.findall_window_ids gvim --src\n            python -m utool.util_ubuntu XCtrl.findall_window_ids gvim --src\n            python -m utool.util_ubuntu XCtrl.findall_window_ids joncrall --src\n\n        xprop -id\n\n        wmctrl -l | awk '{print $1}' | xprop -id\n\n        0x00a00007 | grep \"WM_CLASS(STRING)\"\n\n        \"\"\"\n        import utool as ut\n        cmdkw = dict(verbose=False, quiet=True, silence=True)\n        command = \"wmctrl -lx | grep '%s' | awk '{print $1}'\" % (pattern,)\n        # print(command)\n        winid_list = ut.cmd(command, **cmdkw)[0].strip().split('\\n')\n        winid_list = [h for h in winid_list if h]\n        winid_list = [int(h, 16) for h in winid_list]\n        return winid_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_window_ids(winid_list, order='mru'):\n        import utool as ut\n        winid_order = XCtrl.sorted_window_ids(order)\n        sorted_win_ids = ut.isect(winid_order, winid_list)\n        return sorted_win_ids", "response": "Sorts the list of window ids by most recently used"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nkill old window with pattern.", "response": "def killold(pattern, num=4):\n        \"\"\"\n        Leaves no more than `num` instances of a program alive.  Ordering is\n        determined by most recent usage.\n\n        CommandLine:\n            python -m utool.util_ubuntu XCtrl.killold gvim 2\n\n        >>> import utool as ut\n        >>> from utool import util_ubuntu\n        >>> XCtrl = util_ubuntu.XCtrl\n        >>> pattern = 'gvim'\n        >>> num = 2\n\n        \"\"\"\n        import utool as ut\n        cmdkw = dict(verbose=False, quiet=True, silence=True)\n        num = int(num)\n        winid_list = XCtrl.findall_window_ids(pattern)\n        winid_list = XCtrl.sort_window_ids(winid_list, 'mru')[num:]\n        output_lines = ut.cmd(\n            \"\"\"wmctrl -lxp | awk '{print $1 \" \" $3}'\"\"\",\n            **cmdkw)[0].strip().split('\\n')\n        output_fields = [line.split(' ') for line in output_lines]\n        output_fields = [(int(wid, 16), int(pid)) for wid, pid in output_fields]\n        pid_list = [pid for wid, pid in output_fields if wid in winid_list]\n        import psutil\n        for pid in pid_list:\n            proc = psutil.Process(pid=pid)\n            proc.kill()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sorted_window_ids(order='mru'):\n        import utool as ut\n        if order in ['mru', 'lru']:\n            cmdkw = dict(verbose=False, quiet=True, silence=True)\n            winid_order_str = ut.cmd(\n                'xprop -root | grep \"^_NET_CLIENT_LIST_STACKING\"', **cmdkw)[0]\n            winid_order = winid_order_str.split('#')[1].strip().split(', ')[::-1]\n            winid_order = [int(h, 16) for h in winid_order]\n            if order == 'lru':\n                winid_order = winid_order[::-1]\n        else:\n            raise NotImplementedError(order)\n        return winid_order", "response": "Return a list of window ids ordered by criteria"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_window_id(pattern, method='mru', error='raise'):\n        import utool as ut\n        winid_candidates = XCtrl.findall_window_ids(pattern)\n        if len(winid_candidates) == 0:\n            if error == 'raise':\n                available_windows = ut.cmd2('wmctrl -l')['out']\n                msg = 'No window matches pattern=%r' % (pattern,)\n                msg += '\\navailable windows are:\\n%s' % (available_windows,)\n                print(msg)\n                raise Exception(msg)\n            win_id = None\n        elif len(winid_candidates) == 1:\n            win_id = winid_candidates[0]\n        else:\n            # print('Multiple (%d) windows matches pattern=%r' % (\n            #     len(winid_list), pattern,))\n            # Find most recently used window with the focus name.\n            win_id = XCtrl.sort_window_ids(winid_candidates, method)[0]\n        return win_id", "response": "Find the most recently used window with the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies a text to a terminal script", "response": "def copy_gvim_to_terminal_script(text, return_to_win=\"1\", verbose=0, sleeptime=.02):\n        \"\"\"\n        import utool.util_ubuntu\n        utool.util_ubuntu.XCtrl.copy_gvim_to_terminal_script('print(\"hi\")', verbose=1)\n        python -m utool.util_ubuntu XCtrl.copy_gvim_to_terminal_script \"echo hi\" 1 1\n\n        If this doesn't work make sure pyperclip is installed and set to xsel\n\n        print('foobar')\n        echo hi\n        \"\"\"\n        # Prepare to send text to xdotool\n        import utool as ut\n        import utool.util_ubuntu\n        ut.copy_text_to_clipboard(text)\n\n        if verbose:\n            print('text = %r' % (text,))\n            print(ut.get_clipboard())\n\n        import re\n        terminal_pattern = r'\\|'.join([\n            'terminal',\n            re.escape('terminator.Terminator'),  # gtk3 terminator\n            re.escape('x-terminal-emulator.X-terminal-emulator'),  # gtk2 terminator\n        ])\n\n        # Build xdtool script\n        doscript = [\n            ('remember_window_id', 'ACTIVE_WIN'),\n            # ('focus', 'x-terminal-emulator.X-terminal-emulator'),\n            ('focus', terminal_pattern),\n            ('key', 'ctrl+shift+v'),\n            ('key', 'KP_Enter'),\n        ]\n        if '\\n' in text:\n            # Press enter twice for multiline texts\n            doscript += [\n                ('key', 'KP_Enter'),\n            ]\n\n        if return_to_win == \"1\":\n            doscript += [\n                ('focus_id', '$ACTIVE_WIN'),\n            ]\n        # execute script\n        # verbose = 1\n        utool.util_ubuntu.XCtrl.do(*doscript, sleeptime=sleeptime, verbose=verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef focus_window(winhandle, path=None, name=None, sleeptime=.01):\n        import utool as ut\n        import time\n        print('focus: ' + winhandle)\n        args = ['wmctrl', '-xa', winhandle]\n        ut.cmd(*args, verbose=False, quiet=True)\n        time.sleep(sleeptime)", "response": "sudo apt-get install xautomation\n        apt-get install autokey-gtk\n\n        wmctrl -xa gnome-terminal.Gnome-terminal\n        wmctrl -xl"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the chmod flags of the files in the setup. py directory.", "response": "def setup_chmod(setup_fpath, setup_dir, chmod_patterns):\n    \"\"\" Gives files matching pattern the same chmod flags as setup.py \"\"\"\n    #st_mode = os.stat(setup_fpath).st_mode\n    st_mode = 33277\n    for pattern in chmod_patterns:\n        for fpath in util_path.glob(setup_dir, pattern, recursive=True):\n            print('[setup] chmod fpath=%r' % fpath)\n            os.chmod(fpath, st_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nassert that setup. py is in repository root", "response": "def assert_in_setup_repo(setup_fpath, name=''):\n    \"\"\" pass in __file__ from setup.py \"\"\"\n    setup_dir, setup_fname = split(setup_fpath)\n    cwd = os.getcwd()\n    #repo_dname = split(setup_dir)[1]\n    #print('cwd       = %r' % (cwd))\n    #print('repo_dname = %r' % repo_dname)\n    #print('setup_dir = %r' % (setup_dir))\n    #print('setup_fname = %r' % (setup_fname))\n    try:\n        assert setup_fname == 'setup.py', 'name is not setup.py'\n        #assert name == '' or repo_dname == name,\n        ('name=%r' % name)\n        assert cwd == setup_dir, 'cwd is not setup_dir'\n        assert exists(setup_dir), 'setup dir does not exist'\n        assert exists(join(setup_dir, 'setup.py')), 'setup.py does not exist'\n    except AssertionError as ex:\n        printex(ex, 'ERROR!: setup.py must be run from repository root')\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_packages(recursive=True, maxdepth=None):\n    import utool\n    if utool.VERBOSE:\n        print('[util_setup] find_packages(recursive=%r, maxdepth=%r)' % (recursive, maxdepth))\n    from os.path import relpath\n    cwd = os.getcwd()\n    init_files = utool.glob(cwd, '__init__.py', recursive=recursive, maxdepth=maxdepth)\n    package_paths = list(map(dirname, init_files))\n    package_relpaths = [relpath(path, cwd) for path in package_paths]\n\n    packages = []\n    for path in package_relpaths:\n        base = utool.dirsplit(path)[0]\n        if exists(join(base, '__init__.py')):\n            package = path.replace('/', '.').replace('\\\\', '.')\n            packages.append(package)\n    return packages", "response": "Find all packages in the current directory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef autogen_sphinx_apidoc():\n    # TODO: assert sphinx-apidoc exe is found\n    # TODO: make find_exe work?\n    import utool as ut\n\n    def build_sphinx_apidoc_cmdstr():\n        print('')\n        print('if this fails try: sudo pip install sphinx')\n        print('')\n        apidoc = 'sphinx-apidoc'\n        if ut.WIN32:\n            winprefix = 'C:/Python27/Scripts/'\n            sphinx_apidoc_exe = winprefix + apidoc + '.exe'\n        else:\n            sphinx_apidoc_exe = apidoc\n        apidoc_argfmt_list = [\n            sphinx_apidoc_exe,\n            '--force',\n            '--full',\n            '--maxdepth=\"{maxdepth}\"',\n            '--doc-author=\"{author}\"',\n            '--doc-version=\"{doc_version}\"',\n            '--doc-release=\"{doc_release}\"',\n            '--output-dir=\"_doc\"',\n            #'--separate',  # Put documentation for each module on its own page\n            '--private',  # Include \"_private\" modules\n            '{pkgdir}',\n        ]\n        outputdir = '_doc'\n        author = ut.parse_author()\n        packages = ut.find_packages(maxdepth=1)\n        assert len(packages) != 0, 'directory must contain at least one package'\n        if len(packages) > 1:\n            assert len(packages) == 1,\\\n                ('FIXME I dont know what to do with more than one root package: %r'\n                 % (packages,))\n        pkgdir = packages[0]\n        version = ut.parse_package_for_version(pkgdir)\n        modpath = dirname(ut.truepath(pkgdir))\n\n        apidoc_fmtdict = {\n            'author': author,\n            'maxdepth': '8',\n            'pkgdir': pkgdir,\n            'doc_version': version,\n            'doc_release': version,\n            'outputdir': outputdir,\n        }\n        ut.assert_exists('setup.py')\n        ut.ensuredir('_doc')\n        apidoc_fmtstr = ' '.join(apidoc_argfmt_list)\n        apidoc_cmdstr = apidoc_fmtstr.format(**apidoc_fmtdict)\n        print('[util_setup] autogenerate sphinx docs for %r' % (pkgdir,))\n        if ut.VERBOSE:\n            print(ut.repr4(apidoc_fmtdict))\n        return apidoc_cmdstr, modpath, outputdir\n\n    def build_conf_replstr():\n        #\n        # Make custom edits to conf.py\n        # FIXME:\n        #ext_search_text = ut.unindent(\n        #    r'''\n        #    extensions = [\n        #    [^\\]]*\n        #    ]\n        #    ''')\n        ext_search_text = r'extensions = \\[[^/]*\\]'\n        # TODO: http://sphinx-doc.org/ext/math.html#module-sphinx.ext.pngmath\n        #'sphinx.ext.mathjax',\n        exclude_modules = []\n        ext_repl_text = ut.codeblock(\n            '''\n            MOCK_MODULES = {exclude_modules}\n            if len(MOCK_MODULES) > 0:\n                import mock\n                for mod_name in MOCK_MODULES:\n                    sys.modules[mod_name] = mock.Mock()\n\n            extensions = [\n                'sphinx.ext.autodoc',\n                'sphinx.ext.viewcode',\n                # For LaTeX\n                'sphinx.ext.pngmath',\n                # For Google Sytle Docstrs\n                # https://pypi.python.org/pypi/sphinxcontrib-napoleon\n                'sphinxcontrib.napoleon',\n                #'sphinx.ext.napoleon',\n            ]\n            '''\n        ).format(exclude_modules=str(exclude_modules))\n        #theme_search = 'html_theme = \\'default\\''\n        theme_search = 'html_theme = \\'[a-zA-Z_1-3]*\\''\n        theme_repl = ut.codeblock(\n            '''\n            import sphinx_rtd_theme\n            html_theme = \"sphinx_rtd_theme\"\n            html_theme_path = [sphinx_rtd_theme.get_html_theme_path()]\n            ''')\n        head_text = ut.codeblock(\n            '''\n            from sphinx.ext.autodoc import between\n            import sphinx_rtd_theme\n            import sys\n            import os\n\n            # Dont parse IBEIS args\n            os.environ['IBIES_PARSE_ARGS'] = 'OFF'\n            os.environ['UTOOL_AUTOGEN_SPHINX_RUNNING'] = 'ON'\n\n            sys.path.append('{modpath}')\n            sys.path.append(sys.path.insert(0, os.path.abspath(\"../\")))\n\n            autosummary_generate = True\n\n            modindex_common_prefix = ['_']\n            '''\n        ).format(modpath=ut.truepath(modpath))\n        tail_text = ut.codeblock(\n            '''\n            def setup(app):\n                # Register a sphinx.ext.autodoc.between listener to ignore everything\n                # between lines that contain the word IGNORE\n                app.connect('autodoc-process-docstring', between('^.*IGNORE.*$', exclude=True))\n                return app\n            '''\n        )\n        return (ext_search_text, ext_repl_text, theme_search, theme_repl, head_text, tail_text)\n\n    apidoc_cmdstr, modpath, outputdir = build_sphinx_apidoc_cmdstr()\n    ext_search_text, ext_repl_text, theme_search, theme_repl, head_text, tail_text = build_conf_replstr()\n\n    dry = ut.get_argflag('--dry')\n\n    if not dry:\n        # Execute sphinx-apidoc\n        ut.cmd(apidoc_cmdstr, shell=True)\n        # sphinx-apidoc outputs conf.py to <outputdir>, add custom commands\n        #\n        # Change dir to <outputdir>\n        print('chdir' + outputdir)\n        os.chdir(outputdir)\n        conf_fname = 'conf.py'\n        conf_text = ut.read_from(conf_fname)\n        conf_text = conf_text.replace('import sys', 'import sys  # NOQA')\n        conf_text = conf_text.replace('import os', 'import os  # NOQA')\n        conf_text = ut.regex_replace(theme_search, theme_repl, conf_text)\n        conf_text = ut.regex_replace(ext_search_text, ext_repl_text, conf_text)\n        conf_text = head_text + '\\n' + conf_text + tail_text\n        ut.write_to(conf_fname, conf_text)\n        # Make the documentation\n        #if ut.LINUX:\n        #    ut.cmd('make html', shell=True)\n        #if ut.WIN32:\n        #raw_input('waiting')\n        if not ut.get_argflag('--nomake'):\n            ut.cmd('make', 'html', shell=True)\n    else:\n        print(apidoc_cmdstr)\n        print('cd ' + outputdir)\n        print('manual edits of conf.py')\n        print('make html')", "response": "A simple version of sphinx - apidoc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for a variable named __version__ in name's __init__.py file and returns the value. This function parses the source text. It does not load the module.", "response": "def parse_package_for_version(name):\n    \"\"\"\n    Searches for a variable named __version__ in name's __init__.py file and\n    returns the value.  This function parses the source text. It does not load\n    the module.\n    \"\"\"\n    from utool import util_regex\n    init_fpath = join(name, '__init__.py')\n    version_errmsg = textwrap.dedent(\n        '''\n        You must include a __version__ variable\n        in %s\\'s __init__.py file.\n        Try something like:\n        __version__ = '1.0.0.dev1' ''' % (name,))\n    if not exists(init_fpath):\n        raise AssertionError(version_errmsg)\n    val_regex = util_regex.named_field('version', '[0-9a-zA-Z.]+')\n    regexstr = '__version__ *= *[\\'\"]' + val_regex\n    def parse_version(line):\n        # Helper\n        line = line.replace(' ', '').replace('\\t', '')\n        match_dict = util_regex.regex_parse(regexstr, line)\n        if match_dict is not None:\n            return match_dict['version']\n    # Find the version  in the text of the source\n    #version = 'UNKNOWN_VERSION'\n    with open(init_fpath, 'r') as file_:\n        for line in file_.readlines():\n            if line.startswith('__version__'):\n                version = parse_version(line)\n                if version is not None:\n                    return version\n    raise AssertionError(version_errmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __infer_setup_kwargs(module, kwargs):\n    # Get project name from the module\n    #if 'name' not in kwargs:\n    #    kwargs['name'] = module.__name__\n    #else:\n    #    raise AssertionError('must specify module name!')\n    name = kwargs['name']\n    # Our projects depend on utool\n    #if kwargs['name'] != 'utool':\n    #    install_requires = kwargs.get('install_requires', [])\n    #    if 'utool' not in install_requires:\n    #        install_requires.append('utool')\n    #    kwargs['install_requires'] = install_requires\n\n    packages = kwargs.get('packages', [])\n    if name not in packages:\n        packages.append(name)\n        kwargs['packages'] = packages\n\n    if 'version' not in kwargs:\n        version = parse_package_for_version(name)\n        kwargs['version'] = version\n\n    # Parse version\n    #if 'version' not in kwargs:\n    #    if module is None:\n    #        version_errmsg = 'You must include a version (preferably one that matches the __version__ variable in your modules init file'\n    #        raise AssertionError(version_errmsg)\n    #    else:\n    # Parse license\n    if 'license' not in kwargs:\n        try:\n            kwargs['license'] = read_license('LICENSE')\n        except IOError:\n            pass\n    # Parse readme\n    if 'long_description' not in kwargs:\n        kwargs['long_description'] = parse_readme()", "response": "Infer kwargs based on standard info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setuptools_setup(setup_fpath=None, module=None, **kwargs):\n    # TODO: Learn this better\n    # https://docs.python.org/3.1/distutils/apiref.html\n    # https://pythonhosted.org/an_example_pypi_project/setuptools.html\n    # https://docs.python.org/2/distutils/setupscript.html https://docs.python.org/2/distutils/setupscript.html\n    # Useful documentation: http://bashelton.com/2009/04/setuptools-tutorial/#setup.py-package_dir\n    \"\"\"\n    Arguments which can be passed to setuptools::\n\n        ============       =====            ===========\n        Install-Data       Value            Description\n        ------------       -----            -----------\n        *packages          strlist          a list of packages modules to be distributed\n        py_modules         strlist          a list of singlefile modules to be distributed\n        scripts            strlist          a list of standalone scripts to build and install\n        *install_requires  list             e.g: ['distribute == 0.7.3', 'numpy', 'matplotlib']\n        data_files         strlist          a list of data files to install\n        zip_safe           bool             install efficiently installed as a zipped module?\n        namespace_packages list             packages without meaningful __init__.py's\n        package_dir        dict             keys are packagenames ('' is the root)\n        package_data       dict             keys are foldernames, values are a list of globstrs\n        *entry_pionts      dict             installs a script {'console_scripts': ['script_name_to_install = entry_module:entry_function']}\n\n        ============       =====            ===========\n        Meta-Data          Value            Description\n        ------------       -----            -----------\n        name               short string     ('name of the package')\n        version            short string     ('version of this release')\n        author             short string     ('package authors name')\n        author_email       email address    ('email address of the package author')\n        maintainer         short string     ('package maintainers name')\n        maintainer_email   email address    ('email address of the package maintainer')\n        url                URL              ('home page for the package')\n        description        short string     ('short, summary description of the package')\n        long_description   long string      ('longer description of the package')\n        download_url       URL              ('location where the package may be downloaded')\n        classifiers        list of strings  ('a list of classifiers')\n        platforms          list of strings  ('a list of platforms')\n        license            short string     ('license for the package')\n    \"\"\"\n    from utool.util_inject import inject_colored_exceptions\n    inject_colored_exceptions()  # Fluffly, but nice\n    if VERBOSE:\n        print(util_str.repr4(kwargs))\n    __infer_setup_kwargs(module, kwargs)\n    presetup_commands(setup_fpath, kwargs)\n    if VERBOSE:\n        print(util_str.repr4(kwargs))\n    return kwargs", "response": "This function is called by setuptools to create a new setuptools - specific version of the package."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tuple of elements that are replaced with values from an alias dict suppressing empty values.", "response": "def _replaced(__values, **__replacements):\n    \"\"\"\n    Replace elements in iterable with values from an alias dict, suppressing empty values.\n\n    Used to consistently enhance how certain fields are displayed in list and detail pages.\n    \"\"\"\n    return tuple(o for o in (__replacements.get(name, name) for name in __values) if o)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_admin_route_name(model_or_instance):\n    model = model_or_instance if isinstance(model_or_instance, type) else type(model_or_instance)\n    return 'admin:{meta.app_label}_{meta.model_name}'.format(meta=model._meta)", "response": "Returns the base name of the admin route for a model or model instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild a filter URL to an admin changelist of all objects with similar field values.", "response": "def _build_admin_filter_url(model, filters):\n    \"\"\"Build a filter URL to an admin changelist of all objects with similar field values.\"\"\"\n    url = reverse(_get_admin_route_name(model) + '_changelist')\n    parts = urlsplit(url)\n    query = parse_qs(parts.query)\n    query.update(filters)\n    parts_with_filter = parts._replace(query=urlencode(query))\n    return urlunsplit(parts_with_filter)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_admin_link_to_similar(primary_field, *fields, name=None):\n    fields = (primary_field,) + fields\n    url_template = '<a href=\"{url}\">{name_or_value}</a>'\n\n    def field_link(self, obj):\n        value = getattr(obj, primary_field, None)\n        name_or_value = name or value\n        filters = {field_name: getattr(obj, field_name) for field_name in fields}\n        url = _build_admin_filter_url(obj, filters)\n        return format_html(url_template, **locals()) if url else value\n    field_link.allow_tags = True\n    field_link.short_description = primary_field.replace('_', ' ').capitalize()\n    field_link.admin_order_field = primary_field\n    field_link.__name__ = field_link.__name__.replace('field', primary_field)\n\n    return field_link", "response": "Create a function that links to a changelist of all objects with similar field values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to re - apply a failed trigger log action.", "response": "def _retry_failed_log(failed_trigger_log):\n    \"\"\"\n    Try to re-apply a failed trigger log action.\n\n    Makes sure the argument trigger log is in a FAILED state and acquires a row lock on it.\n\n    Returns:\n          True if the operation succeeded\n\n    \"\"\"\n    model = type(failed_trigger_log)\n    try:\n        failed_trigger_log = (\n            model.objects\n            .select_for_update()\n            .get(\n                id=failed_trigger_log.id,\n                state=TRIGGER_LOG_STATE['FAILED'],\n            )\n        )\n    except model.DoesNotExist:\n        return False\n    failed_trigger_log.redo()\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ignore_failed_logs_action(self, request, queryset):\n        count = _ignore_failed_logs(queryset)\n        self.message_user(\n            request,\n            _('{count} failed trigger logs marked as ignored.').format(count=count),\n        )", "response": "Set FAILED trigger logs in queryset to IGNORED."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to re - apply FAILED trigger log actions in the queryset.", "response": "def retry_failed_logs_action(self, request, queryset):\n        \"\"\"Try to re-apply FAILED trigger log actions in the queryset.\"\"\"\n        count = 0\n        for trigger_log in queryset:\n            retried = _retry_failed_log(trigger_log)\n            if retried:\n                count += 1\n        self.message_user(\n            request,\n            _('{count} failed trigger logs retried.').format(count=count),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread PSMs from file stores them to a database backend in chunked PSMs.", "response": "def create_psm_lookup(fn, fastafn, mapfn, header, pgdb, unroll=False,\n                      specfncol=None, decoy=False,\n                      fastadelim=None, genefield=None):\n    \"\"\"Reads PSMs from file, stores them to a database backend in chunked PSMs.\n    \"\"\"\n    proteins = store_proteins_descriptions(pgdb, fastafn, fn, mapfn, header,\n                                           decoy, fastadelim, genefield)\n    mzmlmap = pgdb.get_mzmlfile_map()\n    sequences = {}\n    for psm in tsvreader.generate_tsv_psms(fn, header):\n        seq = tsvreader.get_psm_sequence(psm, unroll)\n        sequences[seq] = 1\n    pgdb.store_pepseqs(((seq,) for seq in sequences))\n    pepseqmap = pgdb.get_peptide_seq_map()\n    psms = []\n    for row, psm in enumerate(tsvreader.generate_tsv_psms(fn, header)):\n        specfn, psm_id, scan, seq, score = tsvreader.get_psm(psm, unroll,\n                                                             specfncol)\n        if len(psms) % DB_STORE_CHUNK == 0:\n            pgdb.store_psms(psms)\n            psms = []\n        psms.append({'rownr': row,\n                     'psm_id': psm_id,\n                     'seq': pepseqmap[seq],\n                     'score': score,\n                     'specfn': mzmlmap[specfn],\n                     'scannr': scan,\n                     'spec_id': '{}_{}'.format(mzmlmap[specfn], scan),\n                     })\n    pgdb.store_psms(psms)\n    pgdb.index_psms()\n    store_psm_protein_relations(fn, header, pgdb, proteins)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread PSMs from file extracts their proteins and peptides and passes them to a database backend in chunks.", "response": "def store_psm_protein_relations(fn, header, pgdb, proteins):\n    \"\"\"Reads PSMs from file, extracts their proteins and peptides and passes\n    them to a database backend in chunks.\n    \"\"\"\n    # TODO do we need an OrderedDict or is regular dict enough?\n    # Sorting for psm_id useful?\n    allpsms = OrderedDict()\n    last_id, psmids_to_store = None, set()\n    store_soon = False\n    for psm in tsvreader.generate_tsv_psms(fn, header):\n        psm_id, prots = tsvreader.get_pepproteins(psm)\n        prots = [x for x in prots if x in proteins]\n        try:\n            # In case the PSMs are presented unrolled\n            allpsms[psm_id].extend(prots)\n        except KeyError:\n            allpsms[psm_id] = prots\n        if len(psmids_to_store) % DB_STORE_CHUNK == 0:\n            store_soon = True\n        if store_soon and last_id != psm_id:\n            pgdb.store_peptides_proteins(allpsms, psmids_to_store)\n            store_soon = False\n            psmids_to_store = set()\n        psmids_to_store.add(psm_id)\n        last_id = psm_id\n    if len(psmids_to_store) > 0:\n        pgdb.store_peptides_proteins(allpsms, psmids_to_store)\n    pgdb.index_protein_peptides()\n    return allpsms"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    buff = ''\n    for line in fileinput.input():\n        buff += line\n\n    parser = jbossparser.JbossParser()\n    result = parser.parse(buff)\n    print(json.dumps(result))", "response": "Reads stdin jboss output writes json on output"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndo the actual work of indent_func", "response": "def _indent_decor(lbl):\n    \"\"\"\n    does the actual work of indent_func\n    \"\"\"\n    def closure_indent(func):\n        if util_arg.TRACE:\n            @ignores_exc_tb(outer_wrapper=False)\n            #@wraps(func)\n            def wrp_indent(*args, **kwargs):\n                with util_print.Indenter(lbl):\n                    print('    ...trace[in]')\n                    ret = func(*args, **kwargs)\n                    print('    ...trace[out]')\n                    return ret\n        else:\n            @ignores_exc_tb(outer_wrapper=False)\n            #@wraps(func)\n            def wrp_indent(*args, **kwargs):\n                with util_print.Indenter(lbl):\n                    ret = func(*args, **kwargs)\n                    return ret\n        wrp_indent_ = ignores_exc_tb(wrp_indent)\n        wrp_indent_ = preserve_sig(wrp_indent, func)\n        return wrp_indent_\n    return closure_indent"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindent a function in the tree tree.", "response": "def indent_func(input_):\n    \"\"\"\n    Takes either no arguments or an alias label\n    \"\"\"\n    if isinstance(input_, six.string_types):\n        # A label was specified\n        lbl = input_\n        return _indent_decor(lbl)\n    elif isinstance(input_, (bool, tuple)):\n        # Allow individually turning of of this decorator\n        func = input_\n        return func\n    else:\n        # Use the function name as the label\n        func = input_\n        lbl = '[' + meta_util_six.get_funcname(func) + ']'\n        return _indent_decor(lbl)(func)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntracing a function in an XML style block.", "response": "def tracefunc_xml(func):\n    \"\"\"\n    Causes output of function to be printed in an XML style block\n    \"\"\"\n    funcname = meta_util_six.get_funcname(func)\n    def wrp_tracefunc2(*args, **kwargs):\n        verbose = kwargs.get('verbose', True)\n        if verbose:\n            print('<%s>' % (funcname,))\n        with util_print.Indenter('    '):\n            ret = func(*args, **kwargs)\n        if verbose:\n            print('</%s>' % (funcname,))\n        return ret\n    wrp_tracefunc2_ = ignores_exc_tb(wrp_tracefunc2)\n    wrp_tracefunc2_ = preserve_sig(wrp_tracefunc2_, func)\n    return wrp_tracefunc2_"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef accepts_scalar_input(func):\n    #@on_exception_report_input\n    @ignores_exc_tb(outer_wrapper=False)\n    #@wraps(func)\n    def wrp_asi(self, input_, *args, **kwargs):\n        #if HAVE_PANDAS:\n        #    if isinstance(input_, (pd.DataFrame, pd.Series)):\n        #        input_ = input_.values\n        if util_iter.isiterable(input_):\n            # If input is already iterable do default behavior\n            return func(self, input_, *args, **kwargs)\n        else:\n            # If input is scalar, wrap input, execute, and unpack result\n            #ret = func(self, (input_,), *args, **kwargs)\n            ret = func(self, [input_], *args, **kwargs)\n            if ret is not None:\n                return ret[0]\n    wrp_asi = preserve_sig(wrp_asi, func)\n    return wrp_asi", "response": "Decorator for classical entry point accepts_scalar_input2\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accepts_scalar_input2(argx_list=[0], outer_wrapper=True):\n    assert isinstance(argx_list, (list, tuple)), (\n        'accepts_scalar_input2 must be called with argument positions')\n\n    def closure_asi2(func):\n        #@on_exception_report_input\n        @ignores_exc_tb(outer_wrapper=False)\n        def wrp_asi2(self, *args, **kwargs):\n            # Hack in case wrapping a function with varargs\n            argx_list_ = [argx for argx in argx_list if argx < len(args)]\n            __assert_param_consistency(args, argx_list_)\n            if all([util_iter.isiterable(args[ix]) for ix in argx_list_]):\n                # If input is already iterable do default behavior\n                return func(self, *args, **kwargs)\n            else:\n                # If input is scalar, wrap input, execute, and unpack result\n                args_wrapped = [(arg,) if ix in argx_list_ else arg\n                                for ix, arg in enumerate(args)]\n                ret = func(self, *args_wrapped, **kwargs)\n                if ret is not None:\n                    return ret[0]\n        if outer_wrapper:\n            wrp_asi2 = on_exception_report_input(preserve_sig(wrp_asi2, func))\n        return wrp_asi2\n    return closure_asi2", "response": "r Decorator for classical accepts_scalar_input2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndebugging function for accepts_scalar_input2 checks to make sure all the iterable inputs are of the same length", "response": "def __assert_param_consistency(args, argx_list_):\n    \"\"\"\n    debugging function for accepts_scalar_input2\n    checks to make sure all the iterable inputs are of the same length\n    \"\"\"\n    if util_arg.NO_ASSERTS:\n        return\n    if len(argx_list_) == 0:\n        return True\n    argx_flags = [util_iter.isiterable(args[argx]) for argx in argx_list_]\n    try:\n        assert all([argx_flags[0] == flag for flag in argx_flags]), (\n            'invalid mixing of iterable and scalar inputs')\n    except AssertionError as ex:\n        print('!!! ASSERTION ERROR IN UTIL_DECOR !!!')\n        for argx in argx_list_:\n            print('[util_decor] args[%d] = %r' % (argx, args[argx]))\n        raise ex"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accepts_numpy(func):\n    #@ignores_exc_tb\n    #@wraps(func)\n    def wrp_accepts_numpy(self, input_, *args, **kwargs):\n        if not (util_type.HAVE_NUMPY and isinstance(input_, np.ndarray)):\n            # If the input is not numpy, just call the function\n            return func(self, input_, *args, **kwargs)\n        else:\n            # TODO: use a variant of util_list.unflat_unique_rowid_map\n            # If the input is a numpy array, and return the output with the same\n            # shape as the input\n            if UNIQUE_NUMPY:\n                # Remove redundant input (because we are passing it to SQL)\n                input_list, inverse_unique = np.unique(input_, return_inverse=True)\n            else:\n                input_list = input_.flatten()\n            # Call the function in list format\n            # TODO: is this necessary?\n            input_list = input_list.tolist()\n            output_list = func(self, input_list, *args, **kwargs)\n            # Put the output back into numpy\n            if UNIQUE_NUMPY:\n                # Reconstruct redundant queries\n                output_arr = np.array(output_list)[inverse_unique]\n                output_shape = tuple(list(input_.shape) + list(output_arr.shape[1:]))\n                return np.array(output_arr).reshape(output_shape)\n            else:\n                return np.array(output_list).reshape(input_.shape)\n    wrp_accepts_numpy = preserve_sig(wrp_accepts_numpy, func)\n    return wrp_accepts_numpy", "response": "Allows the first input to be a numpy array and get result in numpy form"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef memoize(func):\n    cache = func._util_decor_memoize_cache = {}\n    # @functools.wraps(func)\n    def memoizer(*args, **kwargs):\n        key = str(args) + str(kwargs)\n        if key not in cache:\n            cache[key] = func(*args, **kwargs)\n        return cache[key]\n    memoizer = preserve_sig(memoizer, func)\n    memoizer.cache = cache\n    return memoizer", "response": "A decorator that memoizes a live python function"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lazyfunc(func):\n    closuremem_ = [{}]\n    def wrapper(*args, **kwargs):\n        mem = closuremem_[0]\n        key = (repr(args), repr(kwargs))\n        try:\n            return mem[key]\n        except KeyError:\n            mem[key] = func(*args, **kwargs)\n        return mem[key]\n    return wrapper", "response": "Returns a memcached version of a function\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a function that applies a docstr to the current functio.", "response": "def apply_docstr(docstr_func):\n    \"\"\"\n    Changes docstr of one functio to that of another\n    \"\"\"\n    def docstr_applier(func):\n        #docstr = meta_util_six.get_funcdoc(docstr_func)\n        #meta_util_six.set_funcdoc(func, docstr)\n        if isinstance(docstr_func, six.string_types):\n            olddoc = meta_util_six.get_funcdoc(func)\n            if olddoc is None:\n                olddoc = ''\n            newdoc = olddoc + docstr_func\n            meta_util_six.set_funcdoc(func, newdoc)\n            return func\n        else:\n            preserved_func = preserve_sig(func, docstr_func)\n            return preserved_func\n    return docstr_applier"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preserve_sig(wrapper, orig_func, force=False):\n    #if True:\n    #    import functools\n    #    return functools.wraps(orig_func)(wrapper)\n    from utool._internal import meta_util_six\n    from utool import util_str\n    from utool import util_inspect\n\n    if wrapper is orig_func:\n        # nothing to do\n        return orig_func\n    orig_docstr = meta_util_six.get_funcdoc(orig_func)\n    orig_docstr = '' if orig_docstr is None else orig_docstr\n    orig_argspec = util_inspect.get_func_argspec(orig_func)\n    wrap_name = meta_util_six.get_funccode(wrapper).co_name\n    orig_name = meta_util_six.get_funcname(orig_func)\n\n    # At the very least preserve info in a dictionary\n    _utinfo = {}\n    _utinfo['orig_func'] = orig_func\n    _utinfo['wrap_name'] = wrap_name\n    _utinfo['orig_name'] = orig_name\n    _utinfo['orig_argspec'] = orig_argspec\n\n    if hasattr(wrapper, '_utinfo'):\n        parent_wrapper_utinfo = wrapper._utinfo\n        _utinfo['parent_wrapper_utinfo'] = parent_wrapper_utinfo\n    if hasattr(orig_func, '_utinfo'):\n        parent_orig_utinfo = orig_func._utinfo\n        _utinfo['parent_orig_utinfo'] = parent_orig_utinfo\n\n    # environment variable is set if you are building documentation\n    # preserve sig if building docs\n    building_docs = os.environ.get('UTOOL_AUTOGEN_SPHINX_RUNNING', 'OFF') == 'ON'\n\n    if force or SIG_PRESERVE or building_docs:\n        # PRESERVES ALL SIGNATURES WITH EXECS\n        src_fmt = r'''\n        def _wrp_preserve{defsig}:\n            \"\"\" {orig_docstr} \"\"\"\n            try:\n                return wrapper{callsig}\n            except Exception as ex:\n                import utool as ut\n                msg = ('Failure in signature preserving wrapper:\\n')\n                ut.printex(ex, msg)\n                raise\n        '''\n        # Put wrapped function into a scope\n        globals_ =  {'wrapper': wrapper}\n        locals_ = {}\n        # argspec is :ArgSpec(args=['bar', 'baz'], varargs=None, keywords=None,\n        # defaults=(True,))\n        # get orig functions argspec\n        # get functions signature\n        # Get function call signature (no defaults)\n        # Define an exec function\n        argspec = inspect.getargspec(orig_func)\n        (args, varargs, varkw, defaults) = argspec\n        defsig = inspect.formatargspec(*argspec)\n        callsig = inspect.formatargspec(*argspec[0:3])\n        # TODO:\n        # ut.func_defsig\n        # ut.func_callsig\n        src_fmtdict = dict(defsig=defsig, callsig=callsig, orig_docstr=orig_docstr)\n        src = textwrap.dedent(src_fmt).format(**src_fmtdict)\n        # Define the new function on the fly\n        # (I wish there was a non exec / eval way to do this)\n        #print(src)\n        code = compile(src, '<string>', 'exec')\n        six.exec_(code, globals_, locals_)\n        #six.exec_(src, globals_, locals_)\n        # Use functools.update_wapper to complete preservation\n        _wrp_preserve = functools.update_wrapper(locals_['_wrp_preserve'], orig_func)\n        # Keep debug info\n        _utinfo['src'] = src\n        # Set an internal sig variable that we may use\n        #_wrp_preserve.__sig__ = defsig\n    else:\n        # PRESERVES SOME SIGNATURES NO EXEC\n        # signature preservation is turned off. just preserve the name.\n        # Does not use any exec or eval statments.\n        _wrp_preserve = functools.update_wrapper(wrapper, orig_func)\n        # Just do something to preserve signature\n\n    DEBUG_WRAPPED_DOCSTRING = False\n    if DEBUG_WRAPPED_DOCSTRING:\n        new_docstr_fmtstr = util_str.codeblock(\n            '''\n            Wrapped function {wrap_name}({orig_name})\n\n            orig_argspec = {orig_argspec}\n\n            orig_docstr = {orig_docstr}\n            '''\n        )\n    else:\n        new_docstr_fmtstr = util_str.codeblock(\n            '''\n            {orig_docstr}\n            '''\n        )\n    new_docstr = new_docstr_fmtstr.format(\n        wrap_name=wrap_name, orig_name=orig_name, orig_docstr=orig_docstr,\n        orig_argspec=orig_argspec)\n    meta_util_six.set_funcdoc(_wrp_preserve, new_docstr)\n    _wrp_preserve._utinfo = _utinfo\n    return _wrp_preserve", "response": "Decorator to preserve the signature of a function in python 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_moments(m_a, m_a2, m_a3, m_a4, n_a, m_b, m_b2, m_b3, m_b4, n_b):\r\n    '''\r\n    Merge moments of two samples A and B.\r\n    parameters are \r\n    m_a, ..., m_a4 = first through fourth moment of sample A\r\n    n_a = size of sample A\r\n    m_b, ..., m_b4 = first through fourth moment of sample B\r\n    n_b = size of sample B\r\n    '''\r\n    delta = m_b - m_a\r\n    delta_2 = delta * delta\r\n    delta_3 = delta * delta_2\r\n    delta_4 = delta * delta_3\r\n    n_x = n_a + n_b\r\n    m_x = m_a + delta * n_b / n_x\r\n    m_x2 = m_a2 + m_b2 + delta_2 * n_a * n_b / n_x\r\n    m_x3 = m_a3 + m_b3 + delta_3 * n_a * n_b * (n_a - n_b) + 3 * delta * (n_a * m_2b - n_b * m_2a) / n_x\r\n    m_x4 = (m_a4 + m_b4 + delta_4 * (n_a * n_b * (n_a * n_a - n_a * n_b + n_b * n_b)) / (n_x ** 3) +\r\n            6 * delta_2 * (n_a * n_a * m_b2 + n_b * n_b * m_a2) / (n_x ** 2) +\r\n            4 * delta * (n_a * m_b3 - n_b * m_a3) / n_x )\r\n    return m_x, m_x2, m_x3, m_x4, n_x", "response": "Merge moments of two samples A and B."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lag_avgs(self):\r\n        '''\r\n        same data as expo_avgs, but with keys as the average age\r\n        of the data -- assuming evenly spaced data points -- rather\r\n        than decay rates\r\n        '''\r\n        if not self.interval:\r\n            return\r\n        interval = self.interval.mean\r\n        return dict([(interval/alpha, val) \r\n            for alpha, val in self.get_expo_avgs().items()])", "response": "Returns a dictionary of the lagged expo avgs with the average age as the average age of the data points and the decay rates as the decay rates."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters that a transition has taken place.", "response": "def _transition(self, nxt, cur=None, since=None):\r\n        '''\r\n        Register that a transition has taken place.\r\n        nxt is an identifier for the state being entered.\r\n        cur is an identifier for the state being left.\r\n        since is the time at which the previous state was entered.\r\n        '''\r\n        self.transition_intervals[(cur, nxt)].tick()\r\n        if since:\r\n            self.state_durations[cur].end(since)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _commit(self, ref):\r\n        'commit a walkers data after it is collected'\r\n        path_times = self._weakref_path_map[ref]\r\n        path_times.append(nanotime())\r\n        del self._weakref_path_map[ref]\r\n        path = tuple(path_times[1::2])\r\n        times = path_times[::2]\r\n        if path not in self.path_stats:\r\n            # tuple to save a tiny bit of memory\r\n            self.path_stats[path] = tuple([\r\n                Duration(interval=False) for i in range(len(path))])\r\n        path_stats = self.path_stats[path]\r\n        for i in range(1, len(times)):\r\n            path_stats[i - 1]._stats.add(times[i] - times[i - 1])", "response": "commit a walkers data after it is collected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pformat(self, prefix=()):\r\n        '''\r\n        Makes a pretty ASCII format of the data, suitable for\r\n        displaying in a console or saving to a text file.\r\n        Returns a list of lines.\r\n        '''\r\n        nan = float(\"nan\")\r\n\r\n        def sformat(segment, stat):\r\n            FMT = \"n={0}, mean={1}, p50/95={2}/{3}, max={4}\"\r\n            line_segs = [segment]\r\n            for s in [stat]:\r\n                p = s.get_percentiles()\r\n                p50, p95 = p.get(0.50, nan), p.get(0.95, nan)\r\n                line_segs.append(FMT.format(s.n, s.mean, p50, p95, s.max))\r\n            return '{0}: {1}'.format(*line_segs)\r\n\r\n        lines = []\r\n        for path in sorted(self.path_stats.keys()):\r\n            lines.append('=====================')\r\n            for seg, stat in zip(path, self.path_stats[path]):\r\n                lines.append(sformat(seg, stat))\r\n        return lines", "response": "Returns a list of lines suitable for printing to a text file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef walk_dict(data):\n    assert hasattr(data, '__getitem__')\n    for key, value in data.items():\n        if isinstance(value, dict):\n            yield (key,), None\n            for keys, value in walk_dict(value):\n                path = (key,) + keys\n                yield path, value\n        else:\n            yield (key,), value", "response": "Recursively walks a dictionary and yields pairs of keys value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeploying the docs to GitHub Pages", "response": "def deploy_docs():\n    \"\"\"\n    Based on https://gist.github.com/domenic/ec8b0fc8ab45f39403dd\n    \"\"\"\n    run('rm -rf ./site/')\n    build_docs()\n    with util.cd('./site/'):\n        run('git init')\n        run('echo \".*pyc\" > .gitignore')\n        run('git config user.name \"Travis CI\"')\n        run('git config user.email \"%s\"' % os.environ['EMAIL'])\n        run('git add .')\n        run('git commit -m \"Deploy to GitHub Pages\"')\n        run(\n            'git push --force --quiet \"https://{GH_TOKEN}@{GH_REF}\" '\n            'master:gh-pages > /dev/null 2>&1'.format(\n                GH_TOKEN=os.environ['GH_TOKEN'],\n                GH_REF=os.environ['GH_REF'],\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating tuples of specfile and quant element for general formats", "response": "def specfn_quant_generator(specfiles, quantfiles, tag, ignore_tags):\n    \"\"\"Generates tuples of specfile and quant element for general formats\"\"\"\n    for specfn, qfn in zip(specfiles, quantfiles):\n        for quant_el in basereader.generate_xmltags(qfn, tag, ignore_tags):\n            yield os.path.basename(specfn), quant_el"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_feature_info(feature):\n    dimensions = feature.findall('position')\n    for dim in dimensions:\n        if dim.attrib['dim'] == '0':\n            rt = dim.text\n        elif dim.attrib['dim'] == '1':\n            mz = dim.text\n    return {'rt': float(rt), 'mz': float(mz),\n            'charge': int(feature.find('charge').text),\n            'intensity': float(feature.find('intensity').text),\n            }", "response": "Returns a dict with feature information"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_quantmap(consfile):\n    quantmap = {}\n    maplist = basereader.get_element(consfile, 'mapList')\n    for mapitem in maplist:\n        quantmap[mapitem.attrib['label']] = mapitem.attrib['id']\n    return quantmap", "response": "Returns a dictionary of isobaric quant channel names as keys channel numbers\n    values from consensus XML."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(cls, host_name: str = '', service_name: str = '', service_version='',\n                  http_host: str = '127.0.0.1', http_port: int = 8000,\n                  tcp_host: str = '127.0.0.1', tcp_port: int = 8001, ssl_context=None,\n                  registry_host: str = \"0.0.0.0\", registry_port: int = 4500,\n                  pubsub_host: str = \"0.0.0.0\", pubsub_port: int = 6379, ronin: bool = False):\n        \"\"\" A convenience method for providing registry and pubsub(redis) endpoints\n\n        :param host_name: Used for process name\n        :param registry_host: IP Address for trellio-registry; default = 0.0.0.0\n        :param registry_port: Port for trellio-registry; default = 4500\n        :param pubsub_host: IP Address for pubsub component, usually redis; default = 0.0.0.0\n        :param pubsub_port: Port for pubsub component; default= 6379\n        :return: None\n        \"\"\"\n        Host.host_name = host_name\n        Host.service_name = service_name\n        Host.service_version = str(service_version)\n        Host.http_host = http_host\n        Host.http_port = http_port\n        Host.tcp_host = tcp_host\n        Host.tcp_port = tcp_port\n        Host.registry_host = registry_host\n        Host.registry_port = registry_port\n        Host.pubsub_host = pubsub_host\n        Host.pubsub_port = pubsub_port\n        Host.ssl_context = ssl_context\n        Host.ronin = ronin", "response": "Configure the trellio - registry and pubsub endpoints."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef attach_service(cls, service):\n        if isinstance(service, HTTPService):\n            cls._http_service = service\n        elif isinstance(service, TCPService):\n            cls._tcp_service = service\n        else:\n            cls._logger.error('Invalid argument attached as service')\n        cls._set_bus(service)", "response": "Allows you to attach one TCP or HTTP service to the current instance of this class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attach_http_service(cls, http_service: HTTPService):\n        if cls._http_service is None:\n            cls._http_service = http_service\n            cls._set_bus(http_service)\n        else:\n            warnings.warn('HTTP service is already attached')", "response": "Attaches a service for hosting\n            to the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attach_tcp_service(cls, tcp_service: TCPService):\n        if cls._tcp_service is None:\n            cls._tcp_service = tcp_service\n            cls._set_bus(tcp_service)\n        else:\n            warnings.warn('TCP service is already attached')", "response": "Attaches a service for hosting\n            to the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(cls):\n        if cls._tcp_service or cls._http_service or cls._http_views or cls._tcp_views:\n            cls._set_host_id()\n            cls._setup_logging()\n            cls._set_process_name()\n            cls._set_signal_handlers()\n            cls._start_pubsub()\n            cls._start_server()\n        else:\n            cls._logger.error('No services to host')", "response": "Starts serving attached services and sets up logging and process name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_maps(m, base):\n    \n    for k in base.keys():\n        if k not in m:\n            m[k] = base[k]", "response": "Merge in undefined map entries from given map."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_lists(l, base):\n    \n    for i in base:\n        if i not in l:\n            l.append(i)", "response": "Merge in undefined list entries from given list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfeeding with a psms generator this returns the 3 PSMs with the highest precursor intensities or area of the top - level precursor intensities or whatever is given in the HEADER_PRECURSOR_QUANT.", "response": "def generate_top_psms(psms, protcol):\n    \"\"\"Fed with a psms generator, this returns the 3 PSMs with\n    the highest precursor intensities (or areas, or whatever is\n    given in the HEADER_PRECURSOR_QUANT\"\"\"\n    top_ms1_psms = {}\n    for psm in psms:\n        protacc = psm[protcol]\n        precursor_amount = psm[mzidtsvdata.HEADER_PRECURSOR_QUANT]\n        if ';' in protacc or precursor_amount == 'NA':\n            continue\n        precursor_amount = float(precursor_amount)\n        psm_seq = psm[mzidtsvdata.HEADER_PEPTIDE]\n        try:\n            peptide_area = top_ms1_psms[protacc][psm_seq]\n        except KeyError:\n            try:\n                top_ms1_psms[protacc][psm_seq] = precursor_amount\n            except KeyError:\n                top_ms1_psms[protacc] = {psm_seq: precursor_amount}\n        else:\n            if precursor_amount > peptide_area:\n                top_ms1_psms[protacc][psm_seq] = precursor_amount\n    return top_ms1_psms"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the sum of the top 3 precursor quant values to a protein table", "response": "def add_ms1_quant_from_top3_mzidtsv(proteins, psms, headerfields, protcol):\n    \"\"\"Collects PSMs with the highes precursor quant values,\n    adds sum of the top 3 of these to a protein table\"\"\"\n    if not protcol:\n        protcol = mzidtsvdata.HEADER_MASTER_PROT\n    top_ms1_psms = generate_top_psms(psms, protcol)\n    for protein in proteins:\n        prot_acc = protein[prottabledata.HEADER_PROTEIN]\n        prec_area = calculate_protein_precursor_quant(top_ms1_psms, prot_acc)\n        outprotein = {k: v for k, v in protein.items()}\n        outprotein[headerfields['precursorquant'][\n            prottabledata.HEADER_AREA][None]] = str(prec_area)\n        yield outprotein"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toc(tt, return_msg=False, write_msg=True, verbose=None):\n    if verbose is not None:\n        write_msg = verbose\n    (msg, start_time) = tt\n    ellapsed = (default_timer() - start_time)\n    if (not return_msg) and write_msg and msg is not None:\n        sys.stdout.write('...toc(%.4fs, ' % ellapsed + '\"' + str(msg) + '\"' + ')\\n')\n    if return_msg:\n        return msg\n    else:\n        return ellapsed", "response": "toc - Print the time elapsed since the last time"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef determine_timestamp_format(datetime_str, warn=True):\n    import re\n    # try to determine the format\n    clean_datetime_str = datetime_str.replace('\\x00', ' ').strip(';').strip()\n    if len(clean_datetime_str) == 25 and 'T' in clean_datetime_str:\n        # Delete last colon from ISO 8601 format\n        # clean_datetime_str = clean_datetime_str[:-3] + clean_datetime_str[-2:]\n        if True or six.PY2:\n            if warn:\n                print('WARNING: Python 2.7 does not support %z directive '\n                      'in strptime, ignoring timezone in parsing: ' +\n                      clean_datetime_str)\n            clean_datetime_str = clean_datetime_str[:-6]\n\n    year_regex  = r'(\\d\\d)?\\d\\d'\n    month_regex = '[0-1]?[0-9]'\n    day_regex   = '[0-3]?[0-9]'\n\n    time_regex = r'[0-6]?[0-9]:[0-6]?[0-9]:[0-6]?[0-9]'\n\n    #odd_time_regex = r'[0-6]?[0-9]:[0-6]?[0-9]:[0-6 ]?[0-9]'\n\n    date_regex1 = '/'.join([year_regex, month_regex, day_regex])\n    date_regex2 = ':'.join([year_regex, month_regex, day_regex])\n    date_regex3 = '-'.join([year_regex, month_regex, day_regex])\n    datetime_regex1 = date_regex1 + ' ' + time_regex\n    datetime_regex2 = date_regex2 + ' ' + time_regex\n    datetime_regex3 = date_regex3 + 'T' + time_regex  # + r'\\+[0-2]?[0-9]?[0-6]?[0-9]'\n    datetime_regex4 = time_regex + ' ' + date_regex2 + ' 1'\n\n    timefmt = None\n\n    if re.match(datetime_regex1, clean_datetime_str):\n        timefmt = '%Y/%m/%d %H:%M:%S'\n    elif re.match(datetime_regex2, clean_datetime_str):\n        timefmt = '%Y:%m:%d %H:%M:%S'\n    elif re.match(datetime_regex3, clean_datetime_str):\n        # timefmt = '%Y-%m-%dT%H:%M:%S%z'\n        timefmt = '%Y-%m-%dT%H:%M:%S'\n    elif re.match(datetime_regex4, clean_datetime_str):\n        # timefmt = '%Y-%m-%dT%H:%M:%S%z'\n        timefmt = '%H:%M:%S %Y:%m:%d 1'\n    # Just dont accept this bad format\n    #elif re.match(datetime_regex3, clean_datetime_str):\n    #    timefmt = '%Y:%m:%d %H:%M: %S'\n    else:\n        if isinstance(clean_datetime_str, six.string_types):\n            if len(clean_datetime_str.strip()) == 0:\n                return None\n            elif len(clean_datetime_str.strip(':/ ')) == 0:\n                return None\n            elif clean_datetime_str.find('No EXIF Data') == 0:\n                return None\n            elif clean_datetime_str.find('Invalid') == 0:\n                return None\n            elif clean_datetime_str == '0000:00:00 00:00:00':\n                return None\n            elif [ ord(_) >= 128 for _ in clean_datetime_str ].count(True) > 1:\n                return None\n        #return -1\n        #import utool as ut\n        #ut.embed()\n        msg = 'Unknown format: datetime_str=%r' % (datetime_str,)\n        print(msg)\n        return None\n        #raise NotImplementedError(msg)\n    return timefmt", "response": "r This function will determine the format of the datetime_str in the ISO 8601 format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exiftime_to_unixtime(datetime_str, timestamp_format=None, strict=None):\n    invalid_value = -1\n    if isinstance(datetime_str, int):\n        if datetime_str == -1:\n            return invalid_value\n    elif datetime_str is None:\n        return None\n\n    # TODO: use parse_timestamp to reduce duplicate code\n    #try:\n    #    dt = parse_timestamp(datetime_str)\n    #except TypeError:\n    #    #if datetime_str is None:\n    #        #return -1\n    #    return -1\n    #except ValueError as ex:\n    #    if strict is None:\n    #        ...\n\n    if not isinstance(datetime_str, six.string_types):\n        raise NotImplementedError('Unknown format: datetime_str=%r' % (datetime_str,))\n    # Normal format, or non-standard year first data\n    if timestamp_format is None:\n        timefmt = determine_timestamp_format(datetime_str)\n        if timefmt is None:\n            return invalid_value\n    elif timestamp_format == 2:\n        timefmt = '%m/%d/%Y %H:%M:%S'\n    elif timestamp_format == 1:\n        timefmt = '%Y:%m:%d %H:%M:%S'\n    else:\n        assert isinstance(timestamp_format, six.string_types)\n        timefmt = timestamp_format\n        #raise AssertionError('unknown timestamp_format=%r' % (timestamp_format,))\n    try:\n        if len(datetime_str) == 20 and '\\x00' in datetime_str:\n            datetime_str_ = datetime_str.replace('\\x00', ' ').strip(';').strip()\n        elif len(datetime_str) > 19:\n            datetime_str_ = datetime_str[:19].strip(';').strip()\n        else:\n            datetime_str_ = datetime_str\n        #try:\n        dt = datetime.datetime.strptime(datetime_str_, timefmt)\n        #except ValueError as ex:\n        #    import utool as ut\n        #    ut.printex(ex, iswarning=True)\n        #    return invalid_value\n        return calendar.timegm(dt.timetuple())\n        #return time.mktime(dt.timetuple())\n    except TypeError:\n        #if datetime_str is None:\n            #return -1\n        return -1\n    except ValueError as ex:\n        if strict is None:\n            strict = util_arg.SUPER_STRICT\n        #strict = False\n        #from utool.util_arg import STRICT\n        if isinstance(datetime_str, six.string_types):\n            if len(datetime_str_.strip()) == 0:\n                return invalid_value\n            if datetime_str_.find('No EXIF Data') == 0:\n                return invalid_value\n            if datetime_str_.find('Invalid') == 0:\n                return invalid_value\n            if datetime_str_ == '0000:00:00 00:00:00':\n                return invalid_value\n        print('<!!! ValueError !!!>')\n        print('[util_time] Caught Error: ' + repr(ex))\n        print('[util_time] type(datetime_str)  = %r' % type(datetime_str))\n        print('[util_time] repr(datetime_str)  = %r' % datetime_str)\n        print('[util_time]     (datetime_str)  = %s' % datetime_str)\n        print('[util_time]  len(datetime_str)  = %d' % len(datetime_str))\n        print('[util_time] repr(datetime_str_) = %r' % datetime_str_)\n        print('[util_time]  len(datetime_str_) = %d' % len(datetime_str_))\n        print('</!!! ValueError !!!>')\n\n        debug = True\n        if debug:\n            def find_offending_part(datetime_str_, timefmt, splitchar=' '):\n                import utool as ut\n                parts_list = datetime_str_.split(splitchar)\n                fmt_list = timefmt.split(splitchar)\n                if len(parts_list) == 1:\n                    return\n                for part, fmt in zip(parts_list, fmt_list):\n                    print('Trying:')\n                    with ut.Indenter('  '):\n                        try:\n                            print('fmt = %r' % (fmt,))\n                            print('part = %r' % (part,))\n                            datetime.datetime.strptime(part, fmt)\n                        except ValueError:\n                            find_offending_part(part, fmt, '/')\n                            print('Failed')\n                        else:\n                            print('Passed')\n            find_offending_part(datetime_str_, timefmt)\n\n        #import utool as ut\n        #ut.embed()\n        if strict:\n            raise\n        else:\n            print('Supressed ValueError')\n            return invalid_value", "response": "r Converts a datetime string to unixtime"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a Unix timestamp to a datetimestr.", "response": "def unixtime_to_datetimestr(unixtime, timefmt='%Y/%m/%d %H:%M:%S', isutc=True):\n    \"\"\"\n    TODO: ranme to datetimestr\n    \"\"\"\n    try:\n        if unixtime == -1:\n            return 'NA'\n        if unixtime is None:\n            return None\n        if isutc:\n            return datetime.datetime.utcfromtimestamp(unixtime).strftime(timefmt)\n        else:\n            return datetime.datetime.fromtimestamp(unixtime).strftime(timefmt)\n    except ValueError:\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string representation of a datetime. timedelta object", "response": "def get_timedelta_str(timedelta, exclude_zeros=False):\n    \"\"\"\n    get_timedelta_str\n\n    Returns:\n        str: timedelta_str, formated time string\n\n    References:\n        http://stackoverflow.com/questions/8906926/formatting-python-timedelta-objects\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_time import *  # NOQA\n        >>> timedelta = get_unix_timedelta(10)\n        >>> timedelta_str = get_timedelta_str(timedelta)\n        >>> result = (timedelta_str)\n        >>> print(result)\n        10 seconds\n    \"\"\"\n    if timedelta == datetime.timedelta(0):\n        return '0 seconds'\n    days = timedelta.days\n    hours, rem = divmod(timedelta.seconds, 3600)\n    minutes, seconds = divmod(rem, 60)\n    fmtstr_list = []\n    fmtdict = {}\n\n    def append_cases(unit, fmtlbl):\n        if not exclude_zeros or unit != 0:\n            if unit == 1:\n                fmtstr_list.append('{%s} %s' % (fmtlbl, fmtlbl))\n            else:\n                fmtstr_list.append('{%s} %ss' % (fmtlbl, fmtlbl))\n            fmtdict[fmtlbl] = unit\n\n    if abs(days) > 0:\n        append_cases(days, 'day')\n    if len(fmtstr_list) > 0 or abs(hours) > 0:\n        append_cases(hours, 'hour')\n    if len(fmtstr_list) > 0 or abs(minutes) > 0:\n        append_cases(minutes, 'minute')\n    if len(fmtstr_list) > 0 or abs(seconds) > 0:\n        append_cases(seconds, 'second')\n    fmtstr = ' '.join(fmtstr_list)\n    timedelta_str = fmtstr.format(**fmtdict)\n    return timedelta_str"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the timedelta_str of a set of posix times", "response": "def get_posix_timedelta_str(posixtime, year=False, approx=True):\n    \"\"\"\n    get_timedelta_str\n\n    TODO: rectify this function with get_unix_timedelta_str (unix_timedelta_str probably has better implementation)\n\n    Returns:\n        str: timedelta_str, formated time string\n\n    CommandLine:\n        python -m utool.util_time --test-get_posix_timedelta_str\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_time import *  # NOQA\n        >>> import utool as ut\n        >>> posixtime_list = [-13, 10.2, 10.2 ** 2, 10.2 ** 3, 10.2 ** 4, 10.2 ** 5, 10.2 ** 8, 60 * 60 * 60 * 24 * 7]\n        >>> posixtime = posixtime_list[-1]\n        >>> timedelta_str = [get_posix_timedelta_str(posixtime) for posixtime in posixtime_list]\n        >>> result = ut.repr2(timedelta_str)\n        >>> print(result)\n        ['-00:00:13', '00:00:10', '00:01:44', '00:17:41', '03:00:24', '1 day', '193 weeks', '60 weeks']\n\n    Timeit::\n        import datetime\n        # Seems like like timedelta is just faster. must be because it is builtin\n        %timeit get_posix_timedelta_str(posixtime)\n        %timeit str(datetime.timedelta(seconds=posixtime))\n\n    \"\"\"\n    import numpy as np\n    if np.isnan(posixtime):\n        return 'NaN'\n    sign, posixtime_ = (1, posixtime) if posixtime >= 0 else (-1, -posixtime)\n    seconds_, subseconds = divmod(posixtime_, 1)\n    minutes_, seconds    = divmod(int(seconds_), 60)\n    hours_, minutes      = divmod(minutes_, 60)\n    days_, hours         = divmod(hours_, 24)\n    weeks_, days         = divmod(days_, 7)\n    if year:\n        years, weeks = divmod(weeks_, 52)  # not accurate\n    else:\n        years = 0\n        weeks = weeks_\n    timedelta_parts = []\n    if subseconds > 0:\n        timedelta_parts += [('%.2f' % (subseconds,))[1:]]\n    timedelta_parts += [':'.join(['%02d' % _ for _ in (hours, minutes, seconds)])]\n    import utool as ut\n    if days > 0:\n        timedelta_parts += ['%d %s ' % (days, ut.pluralize('day', days))]\n    if weeks > 0:\n        timedelta_parts += ['%d %s ' % (weeks, ut.pluralize('week', weeks))]\n    if years > 0:\n        timedelta_parts += ['%d %s ' % (years, ut.pluralize('year', years))]\n    if sign == -1:\n        timedelta_parts += ['-']\n    else:\n        timedelta_parts += ['']\n    if approx is not False:\n        if approx is True:\n            approx = 1\n        timedelta_str = ''.join(timedelta_parts[::-1][0:(approx + 1)]).strip()\n    else:\n        timedelta_str = ''.join(timedelta_parts[::-1])\n    return timedelta_str"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a date to a datetime object.", "response": "def date_to_datetime(date, fraction=0.0):\n    \"\"\"\n    fraction is how much through the day you are. 0=start of the day, 1=end of the day.\n    \"\"\"\n    day_seconds = (60 * 60 * 24) - 1\n    total_seconds = int(day_seconds * fraction)\n    delta = datetime.timedelta(seconds=total_seconds)\n    time = datetime.time()\n    dt = datetime.datetime.combine(date, time) + delta\n    return dt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nuse the EC2 API to get a list of all machines", "response": "def ec2_instances():\n    \"Use the EC2 API to get a list of all machines\"\n    region = boto.ec2.get_region(REGION)\n    reservations = region.connect().get_all_instances()\n    instances = []\n    for reservation in reservations:\n        instances += reservation.instances\n    return instances"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters list of machines matching an expression", "response": "def instances(exp=\".*\"):\n    \"Filter list of machines matching an expression\"\n    expression = re.compile(exp)\n    instances = []\n    for node in ec2_instances():\n        if node.tags and ip(node):\n            try:\n                if expression.match(node.tags.get(\"Name\")):\n                    instances.append(node)\n            except TypeError:\n                pass\n    return instances"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the fabric environment for the specifed node", "response": "def use(node):\n    \"Set the fabric environment for the specifed node\"\n    try:\n        role = node.tags.get(\"Name\").split('-')[1]\n        env.roledefs[role] += [ip(node)]\n    except IndexError:\n        pass\n    env.nodes += [node]\n    env.hosts += [ip(node)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_alias_map(regex_map, tag_vocab):\n    import utool as ut\n    import re\n    alias_map = ut.odict([])\n    for pats, new_tag in reversed(regex_map):\n        pats = ut.ensure_iterable(pats)\n        for pat in pats:\n            flags = [re.match(pat, t) for t in tag_vocab]\n            for old_tag in ut.compress(tag_vocab, flags):\n                alias_map[old_tag] = new_tag\n    identity_map = ut.take_column(regex_map, 1)\n    for tag in ut.filter_Nones(identity_map):\n        alias_map[tag] = tag\n    return alias_map", "response": "Constructs explicit mapping of items in regex_map."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef alias_tags(tags_list, alias_map):\n    def _alias_dict(tags):\n        tags_ = [alias_map.get(t, t) for t in tags]\n        return list(set([t for t in tags_ if t is not None]))\n    tags_list_ = [_alias_dict(tags) for tags in tags_list]\n    return tags_list_", "response": "This function updates the tags of a node in a list of tags with the ones from the original tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filterflags_general_tags(tags_list, has_any=None, has_all=None,\n                             has_none=None, min_num=None, max_num=None,\n                             any_startswith=None, any_endswith=None,\n                             in_any=None, any_match=None, none_match=None,\n                             logic='and', ignore_case=True):\n    r\"\"\"\n    maybe integrate into utool? Seems pretty general\n\n    Args:\n        tags_list (list):\n        has_any (None): (default = None)\n        has_all (None): (default = None)\n        min_num (None): (default = None)\n        max_num (None): (default = None)\n\n    Notes:\n        in_any should probably be ni_any\n\n    TODO: make this function more natural\n\n    CommandLine:\n        python -m utool.util_tags --exec-filterflags_general_tags\n        python -m utool.util_tags --exec-filterflags_general_tags:0  --helpx\n        python -m utool.util_tags --exec-filterflags_general_tags:0\n        python -m utool.util_tags --exec-filterflags_general_tags:0  --none_match n\n        python -m utool.util_tags --exec-filterflags_general_tags:0  --has_none=n,o\n        python -m utool.util_tags --exec-filterflags_general_tags:1\n        python -m utool.util_tags --exec-filterflags_general_tags:2\n\n    Example0:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_tags import *  # NOQA\n        >>> import utool as ut\n        >>> tags_list = [['v'], [], ['P'], ['P', 'o'], ['n', 'o'], [], ['n', 'N'], ['e', 'i', 'p', 'b', 'n'], ['q', 'v'], ['n'], ['n'], ['N']]\n        >>> kwargs = ut.argparse_dict(ut.get_kwdefaults2(filterflags_general_tags), type_hint=list)\n        >>> print('kwargs = %r' % (kwargs,))\n        >>> flags = filterflags_general_tags(tags_list, **kwargs)\n        >>> print(flags)\n        >>> result = ut.compress(tags_list, flags)\n        >>> print('result = %r' % (result,))\n\n    Example1:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_tags import *  # NOQA\n        >>> import utool as ut\n        >>> tags_list = [['v'], [], ['P'], ['P'], ['n', 'o'], [], ['n', 'N'], ['e', 'i', 'p', 'b', 'n'], ['n'], ['n'], ['N']]\n        >>> has_all = 'n'\n        >>> min_num = 1\n        >>> flags = filterflags_general_tags(tags_list, has_all=has_all, min_num=min_num)\n        >>> result = ut.compress(tags_list, flags)\n        >>> print('result = %r' % (result,))\n\n    Example2:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_tags import *  # NOQA\n        >>> import utool as ut\n        >>> tags_list = [['vn'], ['vn', 'no'], ['P'], ['P'], ['n', 'o'], [], ['n', 'N'], ['e', 'i', 'p', 'b', 'n'], ['n'], ['n', 'nP'], ['NP']]\n        >>> kwargs = {\n        >>>     'any_endswith': 'n',\n        >>>     'any_match': None,\n        >>>     'any_startswith': 'n',\n        >>>     'has_all': None,\n        >>>     'has_any': None,\n        >>>     'has_none': None,\n        >>>     'max_num': 3,\n        >>>     'min_num': 1,\n        >>>     'none_match': ['P'],\n        >>> }\n        >>> flags = filterflags_general_tags(tags_list, **kwargs)\n        >>> filtered = ut.compress(tags_list, flags)\n        >>> result = ('result = %s' % (ut.repr2(filtered),))\n        result = [['vn', 'no'], ['n', 'o'], ['n', 'N'], ['n'], ['n', 'nP']]\n    \"\"\"\n    import numpy as np\n    import utool as ut\n\n    def _fix_tags(tags):\n        if ignore_case:\n            return set([]) if tags is None else {six.text_type(t.lower()) for t in tags}\n        else:\n            return set([]) if tags is None else {six.text_type() for t in tags}\n\n    if logic is None:\n        logic = 'and'\n\n    logic_func = {\n        'and': np.logical_and,\n        'or': np.logical_or,\n    }[logic]\n\n    default_func = {\n        'and': np.ones,\n        'or': np.zeros,\n    }[logic]\n\n    tags_list_ = [_fix_tags(tags_) for tags_ in tags_list]\n    flags = default_func(len(tags_list_), dtype=np.bool)\n\n    if min_num is not None:\n        flags_ = [len(tags_) >= min_num for tags_ in tags_list_]\n        logic_func(flags, flags_, out=flags)\n\n    if max_num is not None:\n        flags_ = [len(tags_) <= max_num for tags_ in tags_list_]\n        logic_func(flags, flags_, out=flags)\n\n    if has_any is not None:\n        has_any = _fix_tags(set(ut.ensure_iterable(has_any)))\n        flags_ = [len(has_any.intersection(tags_)) > 0 for tags_ in tags_list_]\n        logic_func(flags, flags_, out=flags)\n\n    if has_none is not None:\n        has_none = _fix_tags(set(ut.ensure_iterable(has_none)))\n        flags_ = [len(has_none.intersection(tags_)) == 0 for tags_ in tags_list_]\n        logic_func(flags, flags_, out=flags)\n\n    if has_all is not None:\n        has_all = _fix_tags(set(ut.ensure_iterable(has_all)))\n        flags_ = [len(has_all.intersection(tags_)) == len(has_all) for tags_ in tags_list_]\n        logic_func(flags, flags_, out=flags)\n\n    def _test_item(tags_, fields, op, compare):\n        t_flags = [any([compare(t, f) for f in fields]) for t in tags_]\n        num_passed = sum(t_flags)\n        flag = op(num_passed, 0)\n        return flag\n\n    def _flag_tags(tags_list, fields, op, compare):\n        flags = [_test_item(tags_, fields, op, compare) for tags_ in tags_list_]\n        return flags\n\n    def _exec_filter(flags, tags_list, fields, op, compare):\n        if fields is not None:\n            fields = ut.ensure_iterable(fields)\n            if ignore_case:\n                fields = [f.lower() for f in fields]\n            flags_ = _flag_tags(tags_list, fields, op, compare)\n            logic_func(flags, flags_, out=flags)\n        return flags\n\n    flags = _exec_filter(\n        flags, tags_list, any_startswith,\n        operator.gt, six.text_type.startswith)\n\n    flags = _exec_filter(\n        flags, tags_list, in_any,\n        operator.gt, operator.contains)\n\n    flags = _exec_filter(\n        flags, tags_list, any_endswith,\n        operator.gt, six.text_type.endswith)\n\n    flags = _exec_filter(\n        flags, tags_list, any_match,\n        operator.gt, lambda t, f: re.match(f, t))\n\n    flags = _exec_filter(\n        flags, tags_list, none_match,\n        operator.eq, lambda t, f: re.match(f, t))\n    return flags", "response": "r Filter the tags in a list of tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup(self):\n        self.client = self._get_client()\n        sg = self._create_isolation_security_group()\n        if self.exists is not True:\n            acl = self._create_network_acl()\n            self._add_network_acl_entries(acl)\n            self._add_security_group_rule(sg)\n        self._add_security_group_to_instance(sg)\n\n        \"\"\"Conditions that can not be dry_run\"\"\"\n        if self.dry_run is not False:\n            self._add_security_group_rule(sg)\n            self._add_security_group_to_instance(sg)", "response": "Conditions that can not be dry_run"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef text_dict_write(fpath, dict_):\n    #dict_ = text_dict_read(fpath)\n    #dict_[key] = val\n    dict_text2 = util_str.repr4(dict_, strvals=False)\n    if VERBOSE:\n        print('[cache] ' + str(dict_text2))\n    util_io.write_to(fpath, dict_text2)", "response": "Write a dictionary to a file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _args2_fpath(dpath, fname, cfgstr, ext):\n    if len(ext) > 0 and ext[0] != '.':\n        raise ValueError('Please be explicit and use a dot in ext')\n    max_len = 128\n    # should hashlen be larger?\n    cfgstr_hashlen = 16\n    prefix = fname\n    fname_cfgstr = consensed_cfgstr(prefix, cfgstr, max_len=max_len,\n                                    cfgstr_hashlen=cfgstr_hashlen)\n    fpath = join(dpath, fname_cfgstr + ext)\n    fpath = normpath(fpath)\n    return fpath", "response": "r This function is used to test the length of the file in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving data to a file in a directory.", "response": "def save_cache(dpath, fname, cfgstr, data, ext='.cPkl', verbose=None):\n    \"\"\"\n    Saves data using util_io, but smartly constructs a filename\n    \"\"\"\n    fpath = _args2_fpath(dpath, fname, cfgstr, ext)\n    util_io.save_data(fpath, data, verbose=verbose)\n    return fpath"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_cache(dpath, fname, cfgstr, ext='.cPkl', verbose=None, enabled=True):\n    if verbose is None:\n        verbose = VERBOSE_CACHE\n    if not USE_CACHE or not enabled:\n        if verbose > 1:\n            print('[util_cache] ... cache disabled: dpath=%s cfgstr=%r' %\n                    (basename(dpath), cfgstr,))\n        raise IOError(3, 'Cache Loading Is Disabled')\n    fpath = _args2_fpath(dpath, fname, cfgstr, ext)\n    if not exists(fpath):\n        if verbose > 0:\n            print('[util_cache] ... cache does not exist: dpath=%r fname=%r cfgstr=%r' % (\n                basename(dpath), fname, cfgstr,))\n        raise IOError(2, 'No such file or directory: %r' % (fpath,))\n    else:\n        if verbose > 2:\n            print('[util_cache] ... cache exists: dpath=%r fname=%r cfgstr=%r' % (\n                basename(dpath), fname, cfgstr,))\n        import utool as ut\n        nbytes = ut.get_file_nBytes(fpath)\n        big_verbose = (nbytes > 1E6 and verbose > 2) or verbose > 2\n        if big_verbose:\n            print('[util_cache] About to read file of size %s' % (ut.byte_str2(nbytes),))\n    try:\n        with ut.Timer(fpath, verbose=big_verbose and verbose > 3):\n            data = util_io.load_data(fpath, verbose=verbose > 2)\n    except (EOFError, IOError, ImportError) as ex:\n        print('CORRUPTED? fpath = %s' % (fpath,))\n        if verbose > 1:\n            print('[util_cache] ... cache miss dpath=%s cfgstr=%r' % (\n                basename(dpath), cfgstr,))\n        raise IOError(str(ex))\n    except Exception:\n        print('CORRUPTED? fpath = %s' % (fpath,))\n        raise\n    else:\n        if verbose > 2:\n            print('[util_cache] ... cache hit')\n    return data", "response": "Loads a single file from a cache file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntries to load a single file from a cache.", "response": "def tryload_cache(dpath, fname, cfgstr, verbose=None):\n    \"\"\"\n    returns None if cache cannot be loaded\n    \"\"\"\n    try:\n        return load_cache(dpath, fname, cfgstr, verbose=verbose)\n    except IOError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a list of similar cached datas. Returns a list of similar cached datas. Returns a list of ismiss_list.", "response": "def tryload_cache_list(dpath, fname, cfgstr_list, verbose=False):\n    \"\"\"\n    loads a list of similar cached datas. Returns flags that needs to be computed\n    \"\"\"\n    data_list = [tryload_cache(dpath, fname, cfgstr, verbose) for cfgstr in cfgstr_list]\n    ismiss_list = [data is None for data in data_list]\n    return data_list, ismiss_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to load data from a list of CFG strings and compute it if it can t give a compute function.", "response": "def tryload_cache_list_with_compute(use_cache, dpath, fname, cfgstr_list,\n                                    compute_fn, *args):\n    \"\"\"\n    tries to load data, but computes it if it can't give a compute function\n    \"\"\"\n    # Load precomputed values\n    if use_cache is False:\n        data_list = [None] * len(cfgstr_list)\n        ismiss_list = [True] * len(cfgstr_list)\n        # Don't load or save, just compute\n        data_list = compute_fn(ismiss_list, *args)\n        return data_list\n    else:\n        data_list, ismiss_list = tryload_cache_list(dpath, fname, cfgstr_list,\n                                                    verbose=False)\n    num_total = len(cfgstr_list)\n    if any(ismiss_list):\n        # Compute missing values\n        newdata_list = compute_fn(ismiss_list, *args)\n        newcfgstr_list = util_list.compress(cfgstr_list, ismiss_list)\n        index_list = util_list.list_where(ismiss_list)\n        print('[cache] %d/%d cache hits for %s in %s' % (num_total -\n                                                         len(index_list),\n                                                         num_total, fname,\n                                                         util_path.tail(dpath)))\n        # Cache write\n        for newcfgstr, newdata in zip(newcfgstr_list, newdata_list):\n            save_cache(dpath, fname, newcfgstr, newdata, verbose=False)\n        # Populate missing result\n        for index, newdata in zip(index_list, newdata_list):\n            data_list[index] = newdata\n    else:\n        print('[cache] %d/%d cache hits for %s in %s' % (num_total, num_total,\n                                                         fname,\n                                                         util_path.tail(dpath)))\n    return data_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_utool_json_encoder(allow_pickle=False):\n    import utool as ut\n    PYOBJECT_TAG = '__PYTHON_OBJECT__'\n    UUID_TAG = '__UUID__'\n    SLICE_TAG = '__SLICE__'\n\n    def decode_pickle(text):\n        obj = pickle.loads(codecs.decode(text.encode(), 'base64'))\n        return obj\n\n    def encode_pickle(obj):\n        try:\n            # Use protocol 2 to support both python2.7 and python3\n            COMPATIBLE_PROTOCOL = 2\n            pickle_bytes = pickle.dumps(obj, protocol=COMPATIBLE_PROTOCOL)\n        except Exception:\n            raise\n        text = codecs.encode(pickle_bytes, 'base64').decode()\n        return text\n\n    type_to_tag = collections.OrderedDict([\n        (slice, SLICE_TAG),\n        (uuid.UUID, UUID_TAG),\n        (object, PYOBJECT_TAG),\n    ])\n\n    tag_to_type = {tag: type_ for type_, tag in type_to_tag.items()}\n\n    def slice_part(c):\n        return '' if c is None else str(c)\n\n    def encode_slice(s):\n        parts = [slice_part(s.start), slice_part(s.stop), slice_part(s.step)]\n        return ':'.join(parts)\n\n    def decode_slice(x):\n        return ut.smart_cast(x, slice)\n\n    encoders = {\n        UUID_TAG: str,\n        SLICE_TAG: encode_slice,\n        PYOBJECT_TAG: encode_pickle,\n    }\n\n    decoders = {\n        UUID_TAG: uuid.UUID,\n        SLICE_TAG: decode_slice,\n        PYOBJECT_TAG: decode_pickle,\n    }\n\n    if not allow_pickle:\n        del encoders[PYOBJECT_TAG]\n        del decoders[PYOBJECT_TAG]\n        type_ = tag_to_type[PYOBJECT_TAG]\n        del tag_to_type[PYOBJECT_TAG]\n        del type_to_tag[type_]\n\n    class UtoolJSONEncoder(json.JSONEncoder):\n        def default(self, obj):\n            if isinstance(obj, util_type.NUMPY_TYPE_TUPLE):\n                return obj.tolist()\n            elif six.PY3 and isinstance(obj, bytes):\n                return obj.decode('utf-8')\n            elif isinstance(obj, (set, frozenset)):\n                return list(obj)\n                # return json.JSONEncoder.default(self, list(obj))\n                # return [json.JSONEncoder.default(o) for o in obj]\n            elif isinstance(obj, util_type.PRIMATIVE_TYPES):\n                return json.JSONEncoder.default(self, obj)\n            elif hasattr(obj, '__getstate__'):\n                return obj.__getstate__()\n            else:\n                for type_, tag in type_to_tag.items():\n                    if isinstance(obj, type_):\n                        #print('----')\n                        #print('encoder obj = %r' % (obj,))\n                        #print('encoder type_ = %r' % (type_,))\n                        func = encoders[tag]\n                        text = func(obj)\n                        return {tag: text}\n                raise TypeError('Invalid serialization type=%r' % (type(obj)))\n\n        @classmethod\n        def _json_object_hook(cls, value, verbose=False, **kwargs):\n            if len(value) == 1:\n                tag, text = list(value.items())[0]\n                if tag in decoders:\n                    #print('----')\n                    #print('decoder tag = %r' % (tag,))\n                    func = decoders[tag]\n                    obj = func(text)\n                    #print('decoder obj = %r' % (obj,))\n                    return obj\n            else:\n                return value\n            return value\n    return UtoolJSONEncoder", "response": "Make a json encoder for the current utool object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_json(val, allow_pickle=False, pretty=False):\n    UtoolJSONEncoder = make_utool_json_encoder(allow_pickle)\n    json_kw = {}\n    json_kw['cls'] = UtoolJSONEncoder\n    if pretty:\n        json_kw['indent'] = 4\n        json_kw['separators'] = (',', ': ')\n    json_str = json.dumps(val, **json_kw)\n    return json_str", "response": "r Converts a python object to a JSON string using the utool convention"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(json_str, allow_pickle=False):\n    if six.PY3:\n        if isinstance(json_str, bytes):\n            json_str = json_str.decode('utf-8')\n    UtoolJSONEncoder = make_utool_json_encoder(allow_pickle)\n    object_hook = UtoolJSONEncoder._json_object_hook\n    val = json.loads(json_str, object_hook=object_hook)\n    return val", "response": "Returns a utool object specified in the utool convention"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_func_result_cachekey(func_, args_=tuple(), kwargs_={}):\n    import utool as ut\n    # Rectify partials and whatnot\n    true_args = args_\n    true_kwargs = kwargs_\n    true_func = func_\n    if isinstance(func_, partial):\n        true_func = func_.func\n        if func_.args is not None:\n            true_args = tuple(list(func_.args) + list(args_))\n        if func_.keywords is not None:\n            true_kwargs.update(func_.keywords)\n\n    if ut.is_method(true_func):\n        method = true_func\n        true_func = method.im_func\n        self = method.im_self\n        true_args = tuple([self] + list(true_args))\n\n    # Build up cachekey\n    funcname = ut.get_funcname(true_func)\n    kwdefaults = ut.get_kwdefaults(true_func, parse_source=False)\n    #kwdefaults = ut.get_kwdefaults(true_func, parse_source=True)\n    argnames   = ut.get_argnames(true_func)\n    key_argx = None\n    key_kwds = None\n    func = true_func  # NOQA\n    args = true_args  # NOQA\n    kwargs = true_kwargs  # NOQA\n    args_key = ut.get_cfgstr_from_args(true_func, true_args, true_kwargs,\n                                       key_argx, key_kwds, kwdefaults,\n                                       argnames)\n    cachekey = funcname + '(' + args_key + ')'\n    return cachekey", "response": "This function returns the cachekey for the function result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cachestr_repr(val):\n    try:\n        memview = memoryview(val)\n        return memview.tobytes()\n    except Exception:\n        try:\n            return to_json(val)\n        except Exception:\n            # SUPER HACK\n            if repr(val.__class__) == \"<class 'ibeis.control.IBEISControl.IBEISController'>\":\n                return val.get_dbname()", "response": "Return a string representation of an object as a cache string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_global_cache_dir(appname='default', ensure=False):\n    if appname is None or  appname == 'default':\n        appname = get_default_appname()\n    global_cache_dir = util_cplat.get_app_resource_dir(appname,\n                                                       meta_util_constants.global_cache_dname)\n    if ensure:\n        util_path.ensuredir(global_cache_dir)\n    return global_cache_dir", "response": "Returns the directory where the application cache is stored."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the filepath to the global shelf", "response": "def get_global_shelf_fpath(appname='default', ensure=False):\n    \"\"\" Returns the filepath to the global shelf \"\"\"\n    global_cache_dir = get_global_cache_dir(appname, ensure=ensure)\n    shelf_fpath = join(global_cache_dir, meta_util_constants.global_cache_fname)\n    return shelf_fpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a value to a safe place in each operating system s cache files.", "response": "def global_cache_write(key, val, appname='default'):\n    \"\"\" Writes cache files to a safe place in each operating system \"\"\"\n    with GlobalShelfContext(appname) as shelf:\n        shelf[key] = val"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread and deletes the global cache files to a safe place in each operating system", "response": "def delete_global_cache(appname='default'):\n    \"\"\" Reads cache files to a safe place in each operating system \"\"\"\n    #close_global_shelf(appname)\n    shelf_fpath = get_global_shelf_fpath(appname)\n    util_path.remove_file(shelf_fpath, verbose=True, dryrun=False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a LRU cache of the given size", "response": "def get_lru_cache(max_size=5):\n    \"\"\"\n    Args:\n        max_size (int):\n\n    References:\n        https://github.com/amitdev/lru-dict\n\n    CommandLine:\n        python -m utool.util_cache --test-get_lru_cache\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # UNSTABLE_DOCTEST\n        >>> from utool.util_cache import *  # NOQA\n        >>> import utool as ut  # NOQA\n        >>> max_size = 5\n        >>> # execute function\n        >>> cache_obj = get_lru_cache(max_size)\n        >>> cache_obj[1] = 1\n        >>> cache_obj[2] = 2\n        >>> cache_obj[3] = 3\n        >>> cache_obj[4] = 4\n        >>> cache_obj[5] = 5\n        >>> cache_obj[6] = 6\n        >>> # verify results\n        >>> result = ut.repr2(dict(cache_obj), nl=False)\n        >>> print(result)\n        {2: 2, 3: 3, 4: 4, 5: 5, 6: 6}\n    \"\"\"\n    USE_C_LRU = False\n    if USE_C_LRU:\n        import lru\n        cache_obj = lru.LRU(max_size)\n    else:\n        cache_obj = LRUDict(max_size)\n    return cache_obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef time_different_diskstores():\n    import utool as ut\n    import simplejson as json\n    shelf_path = 'test.shelf'\n    json_path = 'test.json'\n    cpkl_path = 'test.pkl'\n    size = 1000\n    dict_ = {str(key): str(uuid.uuid4()) for key in range(size)}\n    ut.delete(cpkl_path)\n    ut.delete(json_path)\n    ut.delete(shelf_path)\n\n    def shelf_write_test():\n        with ut.shelf_open(shelf_path) as shelf_dict:\n            shelf_dict.update(dict_)\n\n    def shelf_read_test():\n        with ut.shelf_open(shelf_path) as shelf_dict:\n            test = {key: val for key, val in six.iteritems(shelf_dict)}\n        assert len(test) > 0\n\n    def json_write_test():\n        with open(json_path, 'wb') as outfile:\n            json.dump(dict_, outfile)\n\n    def cPickle_write_test():\n        with open(cpkl_path, 'wb') as outfile:\n            pickle.dump(dict_, outfile)\n\n    def cPickle_read_test():\n        with open(cpkl_path, 'rb') as outfile:\n            test = {key: val for key, val in six.iteritems(pickle.load(outfile))}\n        assert len(test) > 0\n\n    def cPickle_read_test2():\n        with open(cpkl_path, 'rb') as outfile:\n            test = pickle.load(outfile)\n        assert len(test) > 0\n\n    shelf_write_test()\n    shelf_read_test()\n    #json_write_test()\n    #json_read_test()\n    cPickle_write_test()\n    cPickle_read_test()\n    cPickle_read_test2()", "response": "%timeit shelf_write_test()    # 15.1 ms per loop\n    %timeit cPickle_write_test()  # 1.26 ms per loop\n\n    %timeit shelf_read_test()     # 8.77 ms per loop\n    %timeit cPickle_read_test()   # 2.4 ms per loop\n    %timeit cPickle_read_test2()  # 2.35 ms per loop\n\n    %timeit json_read_test()\n    %timeit json_write_test()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning data with different cfgstr values that were previously computed with this cacher.", "response": "def existing_versions(self):\n        \"\"\"\n        Returns data with different cfgstr values that were previously computed\n        with this cacher.\n        \"\"\"\n        import glob\n        pattern = self.fname + '_*' + self.ext\n        for fname in glob.glob1(self.dpath, pattern):\n            fpath = join(self.dpath, fname)\n            yield fpath"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking load but returns None if the load fails", "response": "def tryload(self, cfgstr=None):\n        \"\"\"\n        Like load, but returns None if the load fails\n        \"\"\"\n        if cfgstr is None:\n            cfgstr = self.cfgstr\n        if cfgstr is None:\n            import warnings\n            warnings.warn('No cfgstr given in Cacher constructor or call')\n            cfgstr = ''\n        # assert cfgstr is not None, (\n        #     'must specify cfgstr in constructor or call')\n        if not self.enabled:\n            if self.verbose > 0:\n                print('[cache] ... %s Cacher disabled' % (self.fname))\n            return None\n        try:\n            if self.verbose > 1:\n                print('[cache] tryload fname=%s' % (self.fname,))\n                # if self.verbose > 2:\n                #     print('[cache] cfgstr=%r' % (cfgstr,))\n            return self.load(cfgstr)\n        except IOError:\n            if self.verbose > 0:\n                print('[cache] ... %s Cacher miss' % (self.fname))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the full path of the cache file.", "response": "def get_fpath(self, cachedir=None, cfgstr=None, ext=None):\n        \"\"\"\n        Ignore:\n            fname = _fname\n            cfgstr = _cfgstr\n        \"\"\"\n        _dpath = self.get_cachedir(cachedir)\n        _fname = self.get_prefix()\n        _cfgstr = self.get_cfgstr() if cfgstr is None else cfgstr\n        _ext =   self.ext if ext is None else ext\n        fpath = _args2_fpath(_dpath, _fname, _cfgstr, _ext)\n        return fpath"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, cachedir=None, cfgstr=None, verbose=True or VERBOSE or util_arg.VERBOSE):\n        fpath = self.get_fpath(cachedir, cfgstr=cfgstr)\n        if verbose:\n            print('[Cachable] cache delete: %r' % (basename(fpath),))\n        os.remove(fpath)", "response": "Delete the cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(self, cachedir=None, cfgstr=None, verbose=VERBOSE, quiet=QUIET,\n             ignore_keys=None):\n        \"\"\"\n        saves query result to directory\n        \"\"\"\n        fpath = self.get_fpath(cachedir, cfgstr=cfgstr)\n        if verbose:\n            print('[Cachable] cache save: %r' % (basename(fpath),))\n\n        if hasattr(self, '__getstate__'):\n            statedict = self.__getstate__()\n        else:\n            statedict = self.__dict__\n\n        if ignore_keys is None:\n            save_dict = statedict\n        else:\n            save_dict = {key: val\n                         for (key, val) in six.iteritems(statedict)\n                         if key not in ignore_keys}\n\n        util_io.save_data(fpath, save_dict)\n        return fpath", "response": "Saves the current state of the object to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries and load from a partially specified configuration string", "response": "def fuzzyload(self, cachedir=None, partial_cfgstr='', **kwargs):\n        \"\"\"\n        Try and load from a partially specified configuration string\n        \"\"\"\n        valid_targets = self.glob_valid_targets(cachedir, partial_cfgstr)\n        if len(valid_targets) != 1:\n            import utool as ut\n            msg = 'need to further specify target. valid_targets=%s' % (ut.repr3(valid_targets,))\n            raise ValueError(msg)\n        fpath = valid_targets[0]\n        self.load(fpath=fpath, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading the result from the given database and returns the object.", "response": "def load(self, cachedir=None, cfgstr=None, fpath=None, verbose=None,\n             quiet=QUIET, ignore_keys=None):\n        \"\"\"\n        Loads the result from the given database\n        \"\"\"\n        if verbose is None:\n            verbose = getattr(self, 'verbose', VERBOSE)\n        if fpath is None:\n            fpath = self.get_fpath(cachedir, cfgstr=cfgstr)\n        if verbose:\n            print('[Cachable] cache tryload: %r' % (basename(fpath),))\n        try:\n            self._unsafe_load(fpath, ignore_keys)\n            if verbose:\n                print('... self cache hit: %r' % (basename(fpath),))\n        except ValueError as ex:\n            import utool as ut\n            msg = '[!Cachable] Cachable(%s) is likely corrupt' % (self.get_cfgstr())\n            print('CORRUPT fpath = %s' % (fpath,))\n            ut.printex(ex, msg, iswarning=True)\n            raise\n        #except BadZipFile as ex:\n        except zipfile.error as ex:\n            import utool as ut\n            msg = '[!Cachable] Cachable(%s) has bad zipfile' % (self.get_cfgstr())\n            print('CORRUPT fpath = %s' % (fpath,))\n            ut.printex(ex, msg, iswarning=True)\n            raise\n            #if exists(fpath):\n            #    #print('[Cachable] Removing corrupted file: %r' % fpath)\n            #    #os.remove(fpath)\n            #    raise hsexcept.HotsNeedsRecomputeError(msg)\n            #else:\n            #    raise Exception(msg)\n        except IOError as ex:\n            import utool as ut\n            if not exists(fpath):\n                msg = '... self cache miss: %r' % (basename(fpath),)\n                if verbose:\n                    print(msg)\n                raise\n            print('CORRUPT fpath = %s' % (fpath,))\n            msg = '[!Cachable] Cachable(%s) is corrupt' % (self.get_cfgstr())\n            ut.printex(ex, msg, iswarning=True)\n            raise\n        except Exception as ex:\n            import utool as ut\n            ut.printex(ex, 'unknown exception while loading query result')\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_ext(fpath, ext, replace=False):\n    import utool as ut\n    dpath, fname = split(fpath)\n    fname_ = fname.lstrip('.')\n    n_leading_dots = len(fname) - len(fname_)\n    fname_, ext_ = splitext(fname_)\n    valid_exts = list(ut.ensure_iterable(ext))\n    if ext_ not in valid_exts:\n        if replace:\n            fname = fname_.split('.')[0] + valid_exts[0]\n        else:\n            fname = fname_ + ext_ + valid_exts[0]\n    fpath = join(dpath, ('.' * n_leading_dots) + fname)\n    return fpath", "response": "r This function will ensure that the file name or path is in the specified extensions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef truepath_relative(path, otherpath=None):\n    if otherpath is None:\n        otherpath = os.getcwd()\n    otherpath = truepath(otherpath)\n    path_ = normpath(relpath(path, otherpath))\n    return path_", "response": "Normalizes and returns absolute path with so specs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the last n directories in a file.", "response": "def tail(fpath, n=2, trailing=True):\n    \"\"\" Alias for path_ndir_split \"\"\"\n    return path_ndir_split(fpath, n=n, trailing=trailing)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unexpanduser(path):\n    homedir = expanduser('~')\n    if path.startswith(homedir):\n        path = '~' + path[len(homedir):]\n    return path", "response": "r Removes home directory from path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_file(fpath, verbose=None, ignore_errors=True, dryrun=False,\n                quiet=QUIET):\n    \"\"\" Removes a file \"\"\"\n    if verbose is None:\n        verbose = not quiet\n    if dryrun:\n        if verbose:\n            print('[util_path] Dryrem %r' % fpath)\n        return\n    else:\n        try:\n            os.remove(fpath)\n            if verbose:\n                print('[util_path] Removed %r' % fpath)\n        except OSError:\n            print('[util_path.remove_file] Misrem %r' % fpath)\n            #warnings.warn('OSError: %s,\\n Could not delete %s' % (str(e), fpath))\n            if not ignore_errors:\n                raise\n            return False\n    return True", "response": "Removes a file from the nunjucks tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef augpath(path, augsuf='', augext='', augpref='', augdir=None, newext=None,\n            newfname=None, ensure=False, prefix=None, suffix=None):\n    \"\"\"\n    augments end of path before the extension.\n\n    augpath\n\n    Args:\n        path (str):\n        augsuf (str): augment filename before extension\n\n    Returns:\n        str: newpath\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> path = 'somefile.txt'\n        >>> augsuf = '_aug'\n        >>> newpath = augpath(path, augsuf)\n        >>> result = str(newpath)\n        >>> print(result)\n        somefile_aug.txt\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> path = 'somefile.txt'\n        >>> augsuf = '_aug2'\n        >>> newext = '.bak'\n        >>> augdir = 'backup'\n        >>> newpath = augpath(path, augsuf, newext=newext, augdir=augdir)\n        >>> result = str(newpath)\n        >>> print(result)\n        backup/somefile_aug2.bak\n    \"\"\"\n    if prefix is not None:\n        augpref = prefix\n    if suffix is not None:\n        augsuf = suffix\n    # Breakup path\n    dpath, fname = split(path)\n    fname_noext, ext = splitext(fname)\n    if newfname is not None:\n        fname_noext = newfname\n    # Augment ext\n    if newext is None:\n        newext = ext\n    # Augment fname\n    new_fname = ''.join((augpref, fname_noext, augsuf, newext, augext))\n    # Augment dpath\n    if augdir is not None:\n        new_dpath = join(dpath, augdir)\n        if ensure:\n            # create new dir if needebe\n            ensuredir(new_dpath)\n    else:\n        new_dpath = dpath\n    # Recombine into new path\n    newpath = join(new_dpath, new_fname)\n    return newpath", "response": "Augments end of path before the extension."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_files_in_dir(dpath, fname_pattern_list='*', recursive=False,\n                        verbose=VERBOSE, dryrun=False, ignore_errors=False):\n    \"\"\" Removes files matching a pattern from a directory \"\"\"\n    if isinstance(fname_pattern_list, six.string_types):\n        fname_pattern_list = [fname_pattern_list]\n    if verbose > 2:\n        print('[util_path] Removing files:')\n        print('  * from dpath = %r ' % dpath)\n        print('  * with patterns = %r' % fname_pattern_list)\n        print('  * recursive = %r' % recursive)\n    num_removed, num_matched = (0, 0)\n    if not exists(dpath):\n        msg = ('!!! dir = %r does not exist!' % dpath)\n        if verbose:\n            print(msg)\n        warnings.warn(msg, category=UserWarning)\n    for root, dname_list, fname_list in os.walk(dpath):\n        for fname_pattern in fname_pattern_list:\n            for fname in fnmatch.filter(fname_list, fname_pattern):\n                num_matched += 1\n                num_removed += remove_file(join(root, fname),\n                                           ignore_errors=ignore_errors,\n                                           dryrun=dryrun,\n                                           verbose=verbose > 5)\n        if not recursive:\n            break\n    if verbose > 0:\n        print('[util_path] ... Removed %d/%d files' % (num_removed, num_matched))\n    return True", "response": "Removes files matching a pattern from a directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(path, dryrun=False, recursive=True, verbose=None, print_exists=True,\n           ignore_errors=True):\n    \"\"\" Removes a file, directory, or symlink \"\"\"\n    if verbose is None:\n        verbose = VERBOSE\n        if not QUIET:\n            verbose = 1\n    if verbose > 0:\n        print('[util_path] Deleting path=%r' % path)\n    exists_flag = exists(path)\n    link_flag = islink(path)\n    if not exists_flag and not link_flag:\n        if print_exists and verbose:\n            print('..does not exist!')\n        flag = False\n    else:\n        rmargs = dict(verbose=verbose > 1, ignore_errors=ignore_errors,\n                      dryrun=dryrun)\n        if islink(path):\n            os.unlink(path)\n            flag = True\n        elif isdir(path):\n            # First remove everything in the directory\n            flag = remove_files_in_dir(path, recursive=recursive, **rmargs)\n            # Then remove the directory itself\n            flag = flag and remove_dirs(path, **rmargs)\n        elif isfile(path):\n            flag = remove_file(path, **rmargs)\n        else:\n            raise ValueError('Unknown type of path=%r' % (path,))\n        if verbose > 0:\n            print('[util_path] Finished deleting path=%r' % path)\n    return flag", "response": "Removes a file directory or symlink from the tree tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_existing_fpaths(fpath_list, verbose=VERBOSE, quiet=QUIET,\n                           strict=False, print_caller=PRINT_CALLER,\n                           lbl='files'):\n    \"\"\" checks existance before removing. then tries to remove exisint paths \"\"\"\n    import utool as ut\n    if print_caller:\n        print(util_dbg.get_caller_name(range(1, 4)) + ' called remove_existing_fpaths')\n    fpath_list_ = ut.filter_Nones(fpath_list)\n    exists_list = list(map(exists, fpath_list_))\n    if verbose:\n        n_total = len(fpath_list)\n        n_valid = len(fpath_list_)\n        n_exist = sum(exists_list)\n        print('[util_path.remove_existing_fpaths] request delete of %d %s' % (\n            n_total, lbl))\n        if n_valid != n_total:\n            print(('[util_path.remove_existing_fpaths] '\n                   'trying to delete %d/%d non None %s ') %\n                  (n_valid, n_total, lbl))\n        print(('[util_path.remove_existing_fpaths] '\n               ' %d/%d exist and need to be deleted')\n              % (n_exist, n_valid))\n    existing_fpath_list = ut.compress(fpath_list_, exists_list)\n    return remove_fpaths(existing_fpath_list, verbose=verbose, quiet=quiet,\n                            strict=strict, print_caller=False, lbl=lbl)", "response": "remove existing file paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves multiple file paths in a single tree tree", "response": "def remove_fpaths(fpaths, verbose=VERBOSE, quiet=QUIET, strict=False,\n                  print_caller=PRINT_CALLER, lbl='files'):\n    \"\"\"\n    Removes multiple file paths\n    \"\"\"\n    import utool as ut\n    if print_caller:\n        print(util_dbg.get_caller_name(range(1, 4)) + ' called remove_fpaths')\n    n_total = len(fpaths)\n    _verbose = (not quiet and n_total > 0) or VERYVERBOSE\n    if _verbose:\n        print('[util_path.remove_fpaths] try removing %d %s' % (n_total, lbl))\n    n_removed = 0\n    prog = ut.ProgIter(fpaths, label='removing files', enabled=verbose)\n    _iter = iter(prog)\n    # Try to be fast at first\n    try:\n        for fpath in _iter:\n            os.remove(fpath)\n            n_removed += 1\n    except OSError as ex:\n        # Buf if we fail put a try in the inner loop\n        if VERYVERBOSE:\n            print('WARNING: Could not remove fpath = %r' % (fpath,))\n        if strict:\n            util_dbg.printex(ex, 'Could not remove fpath = %r' % (fpath,),\n                             iswarning=False)\n            raise\n        for fpath in _iter:\n            try:\n                os.remove(fpath)\n                n_removed += 1\n            except OSError as ex:\n                if VERYVERBOSE:\n                    print('WARNING: Could not remove fpath = %r' % (fpath,))\n    if _verbose:\n        print('[util_path.remove_fpaths] ... removed %d / %d %s' % (\n            n_removed, n_total, lbl))\n    return n_removed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_path_type(path_):\n    path_type = ''\n    if isfile(path_):\n        path_type += 'file'\n    if isdir(path_):\n        path_type += 'directory'\n    if islink(path_):\n        path_type += 'link'\n    if ismount(path_):\n        path_type += 'mount'\n    return path_type", "response": "r Returns if a path is a file directory link or mount"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkpath(path_, verbose=VERYVERBOSE, n=None, info=VERYVERBOSE):\n    assert isinstance(path_, six.string_types), (\n        'path_=%r is not a string. type(path_) = %r' % (path_, type(path_)))\n    path_ = normpath(path_)\n    if sys.platform.startswith('win32'):\n        # convert back to windows style path if using unix style\n        if path_.startswith('\\\\'):\n            dirs = path_.split('\\\\')\n            if len(dirs) > 1 and len(dirs[0]) == 0 and len(dirs[1]) == 1:\n                dirs[1] = dirs[1].upper() + ':'\n                path_ = '\\\\'.join(dirs[1:])\n    does_exist = exists(path_)\n    if verbose:\n        #print_('[utool] checkpath(%r)' % (path_))\n        pretty_path = path_ndir_split(path_, n)\n        caller_name = util_dbg.get_caller_name(allow_genexpr=False)\n        print('[%s] checkpath(%r)' % (caller_name, pretty_path))\n        if does_exist:\n            path_type = get_path_type(path_)\n            #path_type = 'file' if isfile(path_) else 'directory'\n            print('[%s] ...(%s) exists' % (caller_name, path_type,))\n        else:\n            print('[%s] ... does not exist' % (caller_name))\n    if not does_exist and info:\n        #print('[util_path]  ! Does not exist')\n        _longest_path = longest_existing_path(path_)\n        _longest_path_type = get_path_type(_longest_path)\n        print('[util_path] ... The longest existing path is: %r' % _longest_path)\n        print('[util_path] ... and has type %r' % (_longest_path_type,))\n    return does_exist", "response": "r Tests if a path exists on the filesystem show only the top n directories"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nensuring that path_ is a valid directory.", "response": "def ensurepath(path_, verbose=None):\n    \"\"\" DEPRICATE - alias - use ensuredir instead \"\"\"\n    if verbose is None:\n        verbose = VERYVERBOSE\n    return ensuredir(path_, verbose=verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies files from one list to another.", "response": "def copy_files_to(src_fpath_list, dst_dpath=None, dst_fpath_list=None,\n                  overwrite=False, verbose=True, veryverbose=False):\n    \"\"\"\n    parallel copier\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *\n        >>> import utool as ut\n        >>> overwrite = False\n        >>> veryverbose = False\n        >>> verbose = True\n        >>> src_fpath_list = [ut.grab_test_imgpath(key)\n        >>>                   for key in ut.get_valid_test_imgkeys()]\n        >>> dst_dpath = ut.get_app_resource_dir('utool', 'filecopy_tests')\n        >>> copy_files_to(src_fpath_list, dst_dpath, overwrite=overwrite,\n        >>>               verbose=verbose)\n    \"\"\"\n    from utool import util_list\n    from utool import util_parallel\n\n    if verbose:\n        print('[util_path] +--- COPYING FILES ---')\n        print('[util_path]  * len(src_fpath_list) = %r' % (len(src_fpath_list)))\n        print('[util_path]  * dst_dpath = %r' % (dst_dpath,))\n\n    if dst_fpath_list is None:\n        ensuredir(dst_dpath, verbose=veryverbose)\n        dst_fpath_list = [join(dst_dpath, basename(fpath))\n                          for fpath in src_fpath_list]\n    else:\n        assert dst_dpath is None, 'dst_dpath was specified but overrided'\n        assert len(dst_fpath_list) == len(src_fpath_list), 'bad correspondence'\n\n    exists_list = list(map(exists, dst_fpath_list))\n    if verbose:\n        print('[util_path]  * %d files already exist dst_dpath' % (\n            sum(exists_list),))\n    if not overwrite:\n        notexists_list = util_list.not_list(exists_list)\n        dst_fpath_list_ = util_list.compress(dst_fpath_list, notexists_list)\n        src_fpath_list_ = util_list.compress(src_fpath_list, notexists_list)\n    else:\n        dst_fpath_list_ = dst_fpath_list\n        src_fpath_list_ = src_fpath_list\n\n    args_list = zip(src_fpath_list_, dst_fpath_list_)\n    _gen = util_parallel.generate2(_copy_worker, args_list,\n                                   ntasks=len(src_fpath_list_))\n    success_list = list(_gen)\n\n    #success_list = copy_list(src_fpath_list_, dst_fpath_list_)\n    if verbose:\n        print('[util_path]  * Copied %d / %d' % (sum(success_list),\n                                                 len(src_fpath_list)))\n        print('[util_path] L___ DONE COPYING FILES ___')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy_single(src, dst, overwrite=True, verbose=True, deeplink=True,\n                dryrun=False):\n    r\"\"\"\n    Args:\n        src (str): file or directory to copy\n        dst (str): directory or new file to copy to\n\n    Copies src file or folder to dst.\n\n    If src is a folder this copy is recursive.\n    \"\"\"\n    try:\n        if exists(src):\n            if not isdir(src) and isdir(dst):\n                # copying file to directory\n                dst = join(dst, basename(src))\n            if exists(dst):\n                if overwrite:\n                    prefix = 'C+O'\n                    if verbose:\n                        print('[util_path] [Copying + Overwrite]:')\n                else:\n                    prefix = 'Skip'\n                    if verbose:\n                        print('[%s] ->%s' % (prefix, dst))\n                    return\n            else:\n                prefix = 'C'\n                if verbose:\n                    if dryrun:\n                        print('[util_path] [DryRun]: ')\n                    else:\n                        print('[util_path] [Copying]: ')\n            if verbose:\n                print('[%s] | %s' % (prefix, src))\n                print('[%s] ->%s' % (prefix, dst))\n            if not dryrun:\n                if not deeplink and islink(src):\n                    linkto = os.readlink(src)\n                    symlink(linkto, dst)\n                elif isdir(src):\n                    print('isdir')\n                    shutil.copytree(src, dst)\n                else:\n                    shutil.copy2(src, dst)\n        else:\n            prefix = 'Miss'\n            if verbose:\n                print('[util_path] [Cannot Copy]: ')\n                print('[%s] src=%s does not exist!' % (prefix, src))\n                print('[%s] dst=%s' % (prefix, dst))\n    except Exception as ex:\n        from utool import util_dbg\n        util_dbg.printex(ex, 'Error copying single', keys=['src', 'dst'])\n        raise", "response": "r Copy a single file or folder to another file or folder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy_list(src_list, dst_list, lbl='Copying',\n              ioerr_ok=False, sherro_ok=False, oserror_ok=False):\n    \"\"\" Copies all data and stat info \"\"\"\n    # Feb - 6 - 2014 Copy function\n    task_iter = zip(src_list, dst_list)\n    def docopy(src, dst):\n        try:\n            shutil.copy2(src, dst)\n        except OSError:\n            if ioerr_ok:\n                return False\n            raise\n        except shutil.Error:\n            if sherro_ok:\n                return False\n            raise\n        except IOError:\n            if ioerr_ok:\n                return False\n            raise\n        return True\n    progiter = util_progress.ProgIter(task_iter, adjust=True, lbl=lbl)\n    success_list = [docopy(src, dst) for (src, dst) in progiter]\n    return success_list", "response": "Copy a list of files to another list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iglob(dpath, pattern=None, recursive=False, with_files=True, with_dirs=True,\n          maxdepth=None, exclude_dirs=[], fullpath=True, **kwargs):\n    r\"\"\"\n    Iteratively globs directory for pattern\n\n    FIXME:\n        This function has a speed issue\n\n    Args:\n        dpath (str):  directory path\n        pattern (str):\n        recursive (bool): (default = False)\n        with_files (bool): (default = True)\n        with_dirs (bool): (default = True)\n        maxdepth (None): (default = None)\n        exclude_dirs (list): (default = [])\n\n    Yields:\n        path\n\n    References:\n        http://stackoverflow.com/questions/19859840/excluding-dirs-in-os-walk\n    \"\"\"\n    from utool import util_iter\n    if kwargs.get('verbose', False):  # log what i'm going to do\n        print('[util_path] glob(dpath=%r)' % truepath(dpath,))\n\n    debug = False\n    if pattern is None:\n        # separate extract pattern from dpath\n        if debug:\n            print('[iglob] parsing dpath = %r' % (dpath,))\n        dpath_ = dpath\n        dpath = longest_existing_path(dpath_)\n        pattern = relpath(dpath_, dpath)\n    else:\n        # hack check for pattern\n        GLOB_PATS = ['*', '?']\n        for _ in GLOB_PATS:\n            assert dpath.find(_) == -1, (\n                'warning: pattern _=%r in dpath, but a pattern was specified' %\n                (_,))\n    if isinstance(pattern, list):\n        # overload pattern with list\n        pattern_list  = pattern\n        subiters = (\n            iglob(dpath, pattern=pattern, recursive=recursive,\n                  with_files=with_files, with_dirs=with_dirs,\n                  maxdepth=maxdepth, exclude_dirs=exclude_dirs,\n                  fullpath=fullpath, **kwargs)\n            for pattern in pattern_list\n        )\n        for item in util_iter.iflatten(subiters):\n            yield item\n        raise StopIteration\n    if kwargs.get('verbose', False):\n        print('[iglob] pattern = %r' % (pattern,))\n        print('[iglob] dpath = %r' % (dpath,))\n    n_files = 0\n    n_dirs  = 0\n    current_depth = 0\n    dpath_ = truepath(dpath)\n    posx1 = len(dpath_) + len(os.path.sep)\n    #exclude_dirs_rel = [relpath(dpath_, dir_) for dir_ in exclude_dirs]\n    #exclude_dirs_rel = [relpath(dpath_, dir_) for dir_ in exclude_dirs]\n    #print('\\n\\n\\n')\n    #import utool as ut\n    #print('exclude_dirs = %s' % (ut.repr4(exclude_dirs),))\n    for root, dirs, files in os.walk(dpath_, topdown=True):\n        # Modifying dirs in-place will prune the subsequent files and\n        # directories visitied by os.walk\n        # References:\n        #     http://stackoverflow.com/questions/19859840/excluding-directories-in-os-walk\n        rel_root = relpath(root, dpath_)\n        rel_root2 = relpath(root, dirname(dpath_))\n        #print('rel_root = %r' % (rel_root,))\n        #if len(dirs) > 0:\n        #    print('dirs = %s' % (ut.repr4([join(rel_root, d) for d in dirs]),))\n        if len(exclude_dirs) > 0:\n            dirs[:] = [d for d in dirs if normpath(join(rel_root, d)) not in exclude_dirs]\n            # hack\n            dirs[:] = [d for d in dirs if normpath(join(rel_root2, d)) not in exclude_dirs]\n            # check abs path as well\n            dirs[:] = [d for d in dirs if normpath(join(root, d)) not in exclude_dirs]\n\n        # yeild data\n        # print it only if you want\n        if maxdepth is not None:\n            current_depth = root[posx1:].count(os.path.sep)\n            if maxdepth <= current_depth:\n                continue\n        #print('-----------')\n        #print(current_depth)\n        #print(root)\n        #print('==')\n        #print(dirs)\n        #print('-----------')\n        if with_files:\n            for fname in fnmatch.filter(files, pattern):\n                n_files += 1\n                fpath = join(root, fname)\n                if fullpath:\n                    yield fpath\n                else:\n                    yield relpath(fpath, dpath_)\n\n        if with_dirs:\n            for dname in fnmatch.filter(dirs, pattern):\n                dpath = join(root, dname)\n                n_dirs += 1\n                if fullpath:\n                    yield dpath\n                else:\n                    yield relpath(dpath, dpath_)\n        if not recursive:\n            break\n    if kwargs.get('verbose', False):  # log what i've done\n        n_total = n_dirs + n_files\n        print('[util_path] iglob Found: %d' % (n_total))", "response": "r globs the directory tree for the specified pattern"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef num_images_in_dir(path):\n    num_imgs = 0\n    for root, dirs, files in os.walk(path):\n        for fname in files:\n            if fpath_has_imgext(fname):\n                num_imgs += 1\n    return num_imgs", "response": "returns the number of images in a directory"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn true if the filename has any of the given extensions", "response": "def fpath_has_ext(fname, exts, case_sensitive=False):\n    \"\"\" returns true if the filename has any of the given extensions \"\"\"\n    fname_ = fname.lower() if not case_sensitive else fname\n    if case_sensitive:\n        ext_pats = ['*' + ext for ext in exts]\n    else:\n        ext_pats = ['*' + ext.lower() for ext in exts]\n    return any([fnmatch.fnmatch(fname_, pat) for pat in ext_pats])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dirsplit(path):\n    #return path.split(os.sep)\n    parts = []\n    remain = path\n    part = True\n    #while True:\n    while part != '' and remain != '':\n        remain, part = split(remain)\n        parts.append(part)\n    parts = [p for p in parts if p != '']\n    if remain != '':\n        parts.append(remain)\n    parts = parts[::-1]\n    return parts", "response": "r Dirsplit for the base directory"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_modpath(modname, prefer_pkg=False, prefer_main=False):\n    import importlib\n    if isinstance(modname, six.string_types):\n        module = importlib.import_module(modname)\n    else:\n        module = modname  # Hack\n    modpath = module.__file__.replace('.pyc', '.py')\n    initname = '__init__.py'\n    mainname = '__main__.py'\n    if prefer_pkg:\n        if modpath.endswith(initname) or modpath.endswith(mainname):\n            modpath = dirname(modpath)\n            # modpath = modpath[:-len(initname)]\n    if prefer_main:\n        if modpath.endswith(initname):\n            main_modpath = modpath[:-len(initname)] + mainname\n            if exists(main_modpath):\n                modpath = main_modpath\n    #modname = modname.replace('.__init__', '').strip()\n    #module_dir = get_module_dir(module)\n    return modpath", "response": "r Returns the path to the module containing the n - grams in the module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_crossplat_path(path, winroot='C:'):\n    cplat_path = path.replace('\\\\', '/')\n    if cplat_path == winroot:\n        cplat_path += '/'\n    return cplat_path", "response": "r Ensure that the path is in the cross - platform directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn path to module relative to the package root", "response": "def get_relative_modpath(module_fpath):\n    \"\"\"\n    Returns path to module relative to the package root\n\n    Args:\n        module_fpath (str): module filepath\n\n    Returns:\n        str: modname\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> module_fpath = ut.util_path.__file__\n        >>> rel_modpath = ut.get_relative_modpath(module_fpath)\n        >>> rel_modpath = rel_modpath.replace('.pyc', '.py')  # allow pyc or py\n        >>> result = ensure_crossplat_path(rel_modpath)\n        >>> print(result)\n        utool/util_path.py\n    \"\"\"\n    modsubdir_list = get_module_subdir_list(module_fpath)\n    _, ext = splitext(module_fpath)\n    rel_modpath = join(*modsubdir_list) + ext\n    rel_modpath = ensure_crossplat_path(rel_modpath)\n    return rel_modpath"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn importable name from file path", "response": "def get_modname_from_modpath(module_fpath):\n    \"\"\"\n    returns importable name from file path\n\n    get_modname_from_modpath\n\n    Args:\n        module_fpath (str): module filepath\n\n    Returns:\n        str: modname\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> module_fpath = ut.util_path.__file__\n        >>> modname = ut.get_modname_from_modpath(module_fpath)\n        >>> result = modname\n        >>> print(result)\n        utool.util_path\n    \"\"\"\n    modsubdir_list = get_module_subdir_list(module_fpath)\n    modname = '.'.join(modsubdir_list)\n    modname = modname.replace('.__init__', '').strip()\n    modname = modname.replace('.__main__', '').strip()\n    return modname"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_module_subdir_list(module_fpath):\n    module_fpath = truepath(module_fpath)\n    dpath, fname_ext = split(module_fpath)\n    fname, ext = splitext(fname_ext)\n    full_dpath = dpath\n    dpath = full_dpath\n    _modsubdir_list = [fname]\n    while is_module_dir(dpath):\n        dpath, dname = split(dpath)\n        _modsubdir_list.append(dname)\n    modsubdir_list = _modsubdir_list[::-1]\n    return modsubdir_list", "response": "Function to get a list of all sub - directories of a module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ls(path, pattern='*'):\n    path_iter = glob(path, pattern, recursive=False)\n    return sorted(list(path_iter))", "response": "like unix ls - lists all files and dirs in path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist all python modules in path", "response": "def ls_moduledirs(path, private=True, full=True):\n    \"\"\" lists all dirs which are python modules in path \"\"\"\n    dir_list = ls_dirs(path)\n    module_dir_iter = filter(is_module_dir, dir_list)\n    if not private:\n        module_dir_iter = filterfalse(is_private_module, module_dir_iter)\n    if not full:\n        module_dir_iter = map(basename, module_dir_iter)\n    return list(module_dir_iter)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assertpath(path_, msg='', **kwargs):\n    if NO_ASSERTS:\n        return\n    if path_ is None:\n        raise AssertionError('path is None! %s' % (path_, msg))\n    if path_ == '':\n        raise AssertionError('path=%r is the empty string! %s' % (path_, msg))\n    if not checkpath(path_, **kwargs):\n        raise AssertionError('path=%r does not exist! %s' % (path_, msg))", "response": "Assert that a path exists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sed(regexpr, repl, force=False, recursive=False, dpath_list=None,\n        fpath_list=None, verbose=None, include_patterns=None,\n        exclude_patterns=[]):\n    \"\"\"\n    Python implementation of sed. NOT FINISHED\n\n    searches and replaces text in files\n\n    Args:\n        regexpr (str): regx patterns to find\n        repl (str): text to replace\n        force (bool):\n        recursive (bool):\n        dpath_list (list): directories to search (defaults to cwd)\n    \"\"\"\n    #_grep(r, [repl], dpath_list=dpath_list, recursive=recursive)\n    if include_patterns is None:\n        include_patterns = ['*.py', '*.pyx', '*.pxi', '*.cxx', '*.cpp', '*.hxx', '*.hpp', '*.c', '*.h', '*.html', '*.tex']\n    if dpath_list is None:\n        dpath_list = [os.getcwd()]\n    if verbose is None:\n        verbose = ut.NOT_QUIET\n    if fpath_list is None:\n        greater_exclude_dirs = get_standard_exclude_dnames()\n        exclude_dirs = []\n        fpath_generator = matching_fpaths(\n            dpath_list, include_patterns, exclude_dirs,\n            greater_exclude_dirs=greater_exclude_dirs,\n            recursive=recursive, exclude_patterns=exclude_patterns)\n    else:\n        fpath_generator = fpath_list\n    if verbose:\n        print('sed-ing %r' % (dpath_list,))\n        print(' * regular expression : %r' % (regexpr,))\n        print(' * replacement        : %r' % (repl,))\n        print(' * include_patterns   : %r' % (include_patterns,))\n        print(' * recursive: %r' % (recursive,))\n        print(' * force: %r' % (force,))\n        from utool import util_str\n        print(' * fpath_list: %s' % (util_str.repr3(fpath_list),))\n    regexpr = extend_regex(regexpr)\n    #if '\\x08' in regexpr:\n    #    print('Remember \\\\x08 != \\\\b')\n    #    print('subsituting for you for you')\n    #    regexpr = regexpr.replace('\\x08', '\\\\b')\n    #    print(' * regular expression : %r' % (regexpr,))\n\n    # Walk through each directory recursively\n    num_changed = 0\n    num_files_checked = 0\n    fpaths_changed = []\n    for fpath in fpath_generator:\n        num_files_checked += 1\n        changed_lines = sedfile(fpath, regexpr, repl, force, verbose=verbose)\n        if changed_lines is not None:\n            fpaths_changed.append(fpath)\n            num_changed += len(changed_lines)\n    import utool as ut\n    print('num_files_checked = %r' % (num_files_checked,))\n    print('fpaths_changed = %s' % (ut.repr3(sorted(fpaths_changed)),))\n    print('total lines changed = %r' % (num_changed,))", "response": "This function returns a new version of the sed function that searches for text in files in the specified directories and replaces them with text in files in the specified directories."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sedfile(fpath, regexpr, repl, force=False, verbose=True, veryverbose=False):\n    # TODO: move to util_edit\n    path, name = split(fpath)\n    new_file_lines = []\n\n    if veryverbose:\n        print('[sedfile] fpath=%r' % fpath)\n        print('[sedfile] regexpr=%r' % regexpr)\n        print('[sedfile] repl=%r' % repl)\n        print('[sedfile] force=%r' % force)\n\n    import utool as ut\n    file_lines = ut.readfrom(fpath, aslines=True, verbose=False)\n    # with open(fpath, 'r') as file:\n    #     import utool\n    #     with utool.embed_on_exception_context:\n    #         file_lines = file.readlines()\n    # Search each line for the desired regexpr\n    new_file_lines = [re.sub(regexpr, repl, line) for line in file_lines]\n\n    changed_lines = [(newline, line)\n                     for newline, line in zip(new_file_lines, file_lines)\n                     if  newline != line]\n    n_changed = len(changed_lines)\n    if n_changed > 0:\n        rel_fpath = relpath(fpath, os.getcwd())\n        print(' * %s changed %d lines in %r ' %\n              (['(dry-run)', '(real-run)'][force], n_changed, rel_fpath))\n        print(' * --------------------')\n        import utool as ut\n        new_file_lines = ut.lmap(ut.ensure_unicode, new_file_lines)\n        new_file = ''.join(new_file_lines)\n        #print(new_file.replace('\\n','\\n))\n        if verbose:\n            if True:\n                import utool as ut\n                old_file = ut.ensure_unicode(\n                    ''.join(ut.lmap(ut.ensure_unicode, file_lines)))\n                ut.print_difftext(old_file, new_file)\n            else:\n                changed_new, changed_old = zip(*changed_lines)\n                prefixold = ' * old (%d, %r):  \\n | ' % (n_changed, name)\n                prefixnew = ' * new (%d, %r):  \\n | ' % (n_changed, name)\n                print(prefixold + (' | '.join(changed_old)).strip('\\n'))\n                print(' * ____________________')\n                print(prefixnew + (' | '.join(changed_new)).strip('\\n'))\n                print(' * --------------------')\n                print(' * =====================================================')\n        # Write back to file\n        if force:\n            print(' ! WRITING CHANGES')\n            ut.writeto(fpath, new_file)\n            # with open(fpath, 'w') as file:\n            #     file.write(new_file.encode('utf8'))\n        else:\n            print(' dry run')\n        return changed_lines\n    #elif verbose:\n    #    print('Nothing changed')\n    return None", "response": "A function that executes sed on a specific file and returns a list of changed lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef greplines(lines, regexpr_list, reflags=0):\n    found_lines = []\n    found_lxs = []\n    # Ensure a list\n    islist = isinstance(regexpr_list, (list, tuple))\n    islist2 = isinstance(reflags, (list, tuple))\n    regexpr_list_ = regexpr_list if islist else [regexpr_list]\n    reflags_list = reflags if islist2 else [reflags] * len(regexpr_list_)\n    re_list = [re.compile(pat, flags=_flags)\n               for pat, _flags in  zip(regexpr_list_, reflags_list)]\n    #print('regexpr_list_ = %r' % (regexpr_list_,))\n    #print('re_list = %r' % (re_list,))\n\n    import numpy as np\n    #import utool as ut\n    #cumsum = ut.cumsum(map(len, lines))\n    cumsum = np.cumsum(list(map(len, lines)))\n    text = ''.join(lines)\n\n    # Search each line for each pattern\n    for re_ in re_list:\n        # FIXME: multiline mode doesnt work\n        for match_object in re_.finditer(text):\n            lxs = np.where(match_object.start() < cumsum)[0][0:1]\n            if len(lxs) == 1:\n                lx = lxs[0]\n                if lx > 0:\n                    line_start = cumsum[lx - 1]\n                else:\n                    line_start = 0\n                line_end = cumsum[lx]\n                line = text[line_start:line_end]\n                found_lines.append(line)\n                found_lxs.append(lx)\n    return found_lines, found_lxs", "response": "greps a specific file and returns a list of lines and a list of lists of lists of lists of regexps"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef grep(regex_list, recursive=True, dpath_list=None, include_patterns=None,\n         exclude_dirs=[], greater_exclude_dirs=None, inverse=False,\n         exclude_patterns=[], verbose=VERBOSE, fpath_list=None, reflags=0,\n         cache=None):\n    r\"\"\"\n    greps for patterns\n    Python implementation of grep. NOT FINISHED\n\n    Args:\n        regex_list (str or list): one or more patterns to find\n        recursive (bool):\n        dpath_list (list): directories to search (defaults to cwd)\n        include_patterns (list) : defaults to standard file extensions\n\n    Returns:\n        (list, list, list): (found_fpaths, found_lines_list, found_lxs_list)\n\n    CommandLine:\n        python -m utool.util_path --test-grep\n        utprof.py -m utool.util_path --exec-grep\n        utprof.py utool/util_path.py --exec-grep\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> #dpath_list = [ut.truepath('~/code/utool/utool')]\n        >>> dpath_list = [ut.truepath(dirname(ut.__file__))]\n        >>> include_patterns = ['*.py']\n        >>> exclude_dirs = []\n        >>> regex_list = ['grepfile']\n        >>> verbose = True\n        >>> recursive = True\n        >>> result = ut.grep(regex_list, recursive, dpath_list, include_patterns,\n        >>>                  exclude_dirs)\n        >>> (found_fpath_list, found_lines_list, found_lxs_list) = result\n        >>> assert 'util_path.py' in list(map(basename, found_fpath_list))\n    \"\"\"\n    from utool import util_regex\n    # from utool import util_str\n    from utool import util_list\n    if include_patterns is None:\n        include_patterns =  ['*']\n        # include_patterns = get_standard_include_patterns()\n    if greater_exclude_dirs is None:\n        greater_exclude_dirs = []\n        # greater_exclude_dirs =  get_standard_exclude_dnames()\n    # ensure list input\n    if isinstance(include_patterns, six.string_types):\n        include_patterns = [include_patterns]\n    if dpath_list is None:\n        dpath_list = [os.getcwd()]\n    if verbose:\n        recursive_stat_str = ['flat', 'recursive'][recursive]\n        print('[util_path] Greping (%s) %r for %r' % (recursive_stat_str,\n                                                      dpath_list, regex_list))\n        print('[util_path] regex_list = %s' % (regex_list))\n    if isinstance(regex_list, six.string_types):\n        regex_list = [regex_list]\n    found_fpath_list = []\n    found_lines_list = []\n    found_lxs_list = []\n    # Walk through each directory recursively\n    if fpath_list is None:\n        fpath_generator = matching_fpaths(\n            dpath_list=dpath_list, include_patterns=include_patterns,\n            exclude_dirs=exclude_dirs,\n            greater_exclude_dirs=greater_exclude_dirs,\n            exclude_patterns=exclude_patterns, recursive=recursive)\n    else:\n        fpath_generator = fpath_list\n    #     from utool import util_regex\n    #     extended_regex_list, reflags = util_regex.extend_regex3(regex_list, reflags)\n    #     if verbose:\n    #         print('extended_regex_list = %r' % (extended_regex_list,))\n    #         print('reflags = %r' % (reflags,))\n    _exprs_flags = [util_regex.extend_regex2(expr, reflags)\n                    for expr in regex_list]\n    extended_regex_list = util_list.take_column(_exprs_flags, 0)\n    reflags_list = util_list.take_column(_exprs_flags, 1)\n    # HACK\n    reflags = reflags_list[0]\n\n    # For each matching filepath\n    for fpath in fpath_generator:\n        # For each search pattern\n        found_lines, found_lxs = grepfile(fpath, extended_regex_list,\n                                          reflags_list, cache=cache)\n        if inverse:\n            if len(found_lines) == 0:\n                # Append files that the pattern was not found in\n                found_fpath_list.append(fpath)\n                found_lines_list.append([])\n                found_lxs_list.append([])\n        elif len(found_lines) > 0:\n            found_fpath_list.append(fpath)  # regular matching\n            found_lines_list.append(found_lines)\n            found_lxs_list.append(found_lxs)\n\n    grep_result = (found_fpath_list, found_lines_list, found_lxs_list)\n    if verbose:\n        print('==========')\n        print('==========')\n        print('[util_path] found matches in %d files' %\n              len(found_fpath_list))\n        print(make_grep_resultstr(grep_result, extended_regex_list, reflags))\n        # print('[util_path] found matches in %d files' % len(found_fpath_list))\n\n        # pat = util_regex.regex_or(extended_regex_list)\n\n        # for fpath, found, lxs in zip(found_fpath_list, found_lines_list,\n        #                              found_lxs_list):\n        #     if len(found) > 0:\n        #         print('----------------------')\n        #         print('Found %d line(s) in %r: ' % (len(found), fpath))\n        #         name = split(fpath)[1]\n        #         max_line = len(lxs)\n        #         ndigits = str(len(str(max_line)))\n        #         fmt_str = '%s : %' + ndigits + 'd |%s'\n        #         for (lx, line) in zip(lxs, found):\n        #             # hack\n        #             colored_line = util_str.highlight_regex(\n        #                 line.rstrip('\\n'), pat, reflags=reflags)\n        #             print(fmt_str % (name, lx, colored_line))\n\n        #print('[util_path] found matches in %d files' % len(found_fpath_list))\n\n    return grep_result", "response": "r Grep for patterns\n    Python implementation of grep. NOT FINISHED"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the short path name of a given long path.", "response": "def get_win32_short_path_name(long_name):\n    \"\"\"\n    Gets the short path name of a given long path.\n\n    References:\n        http://stackoverflow.com/a/23598461/200291\n        http://stackoverflow.com/questions/23598289/get-win-short-fname-python\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut  # NOQA\n        >>> # build test data\n        >>> #long_name = unicode(normpath(ut.get_resource_dir()))\n        >>> long_name = unicode(r'C:/Program Files (x86)')\n        >>> #long_name = unicode(r'C:/Python27')\n        #unicode(normpath(ut.get_resource_dir()))\n        >>> # execute function\n        >>> result = get_win32_short_path_name(long_name)\n        >>> # verify results\n        >>> print(result)\n        C:/PROGRA~2\n    \"\"\"\n    import ctypes\n    from ctypes import wintypes\n    _GetShortPathNameW = ctypes.windll.kernel32.GetShortPathNameW\n    _GetShortPathNameW.argtypes = [wintypes.LPCWSTR, wintypes.LPWSTR, wintypes.DWORD]\n    _GetShortPathNameW.restype = wintypes.DWORD\n    output_buf_size = 0\n    while True:\n        output_buf = ctypes.create_unicode_buffer(output_buf_size)\n        needed = _GetShortPathNameW(long_name, output_buf, output_buf_size)\n        if output_buf_size >= needed:\n            short_name = output_buf.value\n            break\n        else:\n            output_buf_size = needed\n    return short_name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef existing_subpath(root_path, valid_subpaths, tiebreaker='first',\n                     verbose=VERYVERBOSE):\n    \"\"\"\n    Returns join(root_path, subpath) where subpath in valid_subpath ane\n    exists(subpath)\n    \"\"\"\n    # Find the oxford_style groundtruth directory\n    for subpath in valid_subpaths:\n        path  = join(root_path, subpath)\n        if checkpath(path, verbose=verbose):\n            if tiebreaker == 'first':\n                return path\n    raise AssertionError('none of the following subpaths exist: %r' %\n                         (valid_subpaths,))", "response": "Returns the path of the ane archive that exists in the root_path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef search_in_dirs(fname, search_dpaths=[], shortcircuit=True,\n                   return_tried=False, strict=False):\n    \"\"\"\n    search_in_dirs\n\n    Args:\n        fname (str):  file name\n        search_dpaths (list):\n        shortcircuit (bool):\n        return_tried (bool): return tried paths\n        strict (bool): (default = False)\n\n    Returns:\n        fpath: None\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import utool as ut\n        >>> fname = 'Inno Setup 5\\ISCC.exe'\n        >>> search_dpaths = ut.get_install_dirs()\n        >>> shortcircuit = True\n        >>> fpath = ut.search_in_dirs(fname, search_dpaths, shortcircuit)\n        >>> print(fpath)\n    \"\"\"\n    fpath_list = []\n    tried_list = []\n    for dpath in search_dpaths:\n        fpath = join(dpath, fname)\n        if return_tried:\n            tried_list.append(fpath)\n        if exists(fpath):\n            if shortcircuit:\n                if return_tried:\n                    return fpath, tried_list\n                return fpath\n            else:\n                fpath_list.append(fpath)\n    if strict and len(fpath_list) == 0:\n        msg = ('Cannot find: fname=%r\\n'  % (fname,))\n        if return_tried:\n            msg += 'Tried: \\n    ' + '\\n    '.join(tried_list)\n        raise Exception(msg)\n\n    if shortcircuit:\n        if return_tried:\n            return None, tried_list\n        return None\n    else:\n        if return_tried:\n            return fpath_list, tried_list\n        return fpath_list", "response": "Search in a list of directories"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for a library in a tree.", "response": "def find_lib_fpath(libname, root_dir, recurse_down=True, verbose=False, debug=False):\n    \"\"\" Search for the library \"\"\"\n\n    def get_lib_fname_list(libname):\n        \"\"\"\n        input <libname>: library name (e.g. 'hesaff', not 'libhesaff')\n        returns <libnames>: list of plausible library file names\n        \"\"\"\n        if sys.platform.startswith('win32'):\n            libnames = ['lib' + libname + '.dll', libname + '.dll']\n        elif sys.platform.startswith('darwin'):\n            libnames = ['lib' + libname + '.dylib']\n        elif sys.platform.startswith('linux'):\n            libnames = ['lib' + libname + '.so']\n        else:\n            raise Exception('Unknown operating system: %s' % sys.platform)\n        return libnames\n\n    def get_lib_dpath_list(root_dir):\n        \"\"\"\n        input <root_dir>: deepest directory to look for a library (dll, so, dylib)\n        returns <libnames>: list of plausible directories to look.\n        \"\"\"\n        'returns possible lib locations'\n        get_lib_dpath_list = [root_dir,\n                              join(root_dir, 'lib'),\n                              join(root_dir, 'build'),\n                              join(root_dir, 'build', 'lib')]\n        return get_lib_dpath_list\n\n    lib_fname_list = get_lib_fname_list(libname)\n    tried_fpaths = []\n    while root_dir is not None:\n        for lib_fname in lib_fname_list:\n            for lib_dpath in get_lib_dpath_list(root_dir):\n                lib_fpath = normpath(join(lib_dpath, lib_fname))\n                if exists(lib_fpath):\n                    if verbose:\n                        print('\\n[c] Checked: '.join(tried_fpaths))\n                    if debug:\n                        print('using: %r' % lib_fpath)\n                    return lib_fpath\n                else:\n                    # Remember which candiate library fpaths did not exist\n                    tried_fpaths.append(lib_fpath)\n            _new_root = dirname(root_dir)\n            if _new_root == root_dir:\n                root_dir = None\n                break\n            else:\n                root_dir = _new_root\n        if not recurse_down:\n            break\n\n    msg = ('\\n[C!] load_clib(libname=%r root_dir=%r, recurse_down=%r, verbose=%r)' %\n           (libname, root_dir, recurse_down, verbose) +\n           '\\n[c!] Cannot FIND dynamic library')\n    print(msg)\n    print('\\n[c!] Checked: '.join(tried_fpaths))\n    raise ImportError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ancestor_paths(start=None, limit={}):\n    import utool as ut\n    limit = ut.ensure_iterable(limit)\n    limit = {expanduser(p) for p in limit}.union(set(limit))\n    if start is None:\n        start = os.getcwd()\n    path = start\n    prev = None\n    while path != prev and prev not in limit:\n        yield path\n        prev = path\n        path = dirname(path)", "response": "Yields all paths above you\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches for existing paths that meed a requirement Args: candidate_path_list (list): list of paths to check. If candidate_name_list is specified this is the dpath list instead candidate_name_list (list): specifies several names to check (default = None) priority_paths (None): specifies paths to check first. Ignore candidate_name_list (default = None) required_subpaths (list): specified required directory structure (default = []) verbose (bool): verbosity flag(default = True) Returns: str: return_path CommandLine: python -m utool.util_path --test-search_candidate_paths Example: >>> # DISABLE_DOCTEST >>> from utool.util_path import * # NOQA >>> candidate_path_list = [ut.truepath('~/RPI/code/utool'), >>> ut.truepath('~/code/utool')] >>> candidate_name_list = None >>> required_subpaths = [] >>> verbose = True >>> priority_paths = None >>> return_path = search_candidate_paths(candidate_path_list, >>> candidate_name_list, >>> priority_paths, required_subpaths, >>> verbose) >>> result = ('return_path = %s' % (str(return_path),)) >>> print(result)", "response": "def search_candidate_paths(candidate_path_list, candidate_name_list=None,\n                           priority_paths=None, required_subpaths=[],\n                           verbose=None):\n    \"\"\"\n    searches for existing paths that meed a requirement\n\n    Args:\n        candidate_path_list (list): list of paths to check. If\n            candidate_name_list is specified this is the dpath list instead\n        candidate_name_list (list): specifies several names to check\n            (default = None)\n        priority_paths (None): specifies paths to check first.\n            Ignore candidate_name_list (default = None)\n        required_subpaths (list): specified required directory structure\n            (default = [])\n        verbose (bool):  verbosity flag(default = True)\n\n    Returns:\n        str: return_path\n\n    CommandLine:\n        python -m utool.util_path --test-search_candidate_paths\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> candidate_path_list = [ut.truepath('~/RPI/code/utool'),\n        >>>                        ut.truepath('~/code/utool')]\n        >>> candidate_name_list = None\n        >>> required_subpaths = []\n        >>> verbose = True\n        >>> priority_paths = None\n        >>> return_path = search_candidate_paths(candidate_path_list,\n        >>>                                      candidate_name_list,\n        >>>                                      priority_paths, required_subpaths,\n        >>>                                      verbose)\n        >>> result = ('return_path = %s' % (str(return_path),))\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    if verbose is None:\n        verbose = 0 if QUIET else 1\n\n    if verbose >= 1:\n        print('[search_candidate_paths] Searching for candidate paths')\n\n    if candidate_name_list is not None:\n        candidate_path_list_ = [join(dpath, fname) for dpath, fname in\n                                itertools.product(candidate_path_list,\n                                                  candidate_name_list)]\n    else:\n        candidate_path_list_ = candidate_path_list\n\n    if priority_paths is not None:\n        candidate_path_list_ = priority_paths + candidate_path_list_\n\n    return_path = None\n    for path in candidate_path_list_:\n        if path is not None and exists(path):\n            if verbose >= 2:\n                print('[search_candidate_paths] Found candidate directory %r' % (path,))\n                print('[search_candidate_paths] ... checking for approprate structure')\n            # tomcat directory exists. Make sure it also contains a webapps dir\n            subpath_list = [join(path, subpath) for subpath in required_subpaths]\n            if all(ut.checkpath(path_, verbose=verbose) for path_ in subpath_list):\n                return_path = path\n                if verbose >= 2:\n                    print('[search_candidate_paths] Found acceptable path')\n                return return_path\n                break\n    if verbose >= 1:\n        print('[search_candidate_paths] Failed to find acceptable path')\n    return return_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef win_shortcut(source, link_name):\n    if True:\n        import ctypes\n        kdll = ctypes.windll.LoadLibrary(\"kernel32.dll\")\n        code = 1 if isdir(source) else 0\n        kdll.CreateSymbolicLinkA(source, link_name, code)\n    else:\n        import ctypes\n        csl = ctypes.windll.kernel32.CreateSymbolicLinkW\n        csl.argtypes = (ctypes.c_wchar_p, ctypes.c_wchar_p, ctypes.c_uint32)\n        csl.restype = ctypes.c_ubyte\n        flags = 1 if isdir(source) else 0\n        retval = csl(link_name, source, flags)\n        if retval == 0:\n            #warn_msg = '[util_path] Unable to create symbolic link on windows.'\n            #print(warn_msg)\n            #warnings.warn(warn_msg, category=UserWarning)\n            if checkpath(link_name):\n                return True\n            raise ctypes.WinError()", "response": "Create a Windows shortcut on a given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to create a symbolic link. TODO: Can this be fixed on windows? Args: path (str): path to real file or directory link_path (str): path to desired location for symlink overwrite (bool): overwrite existing symlinks (default = False) on_error (str): strategy for dealing with errors. raise or ignore verbose (int): verbosity level (default=2) Returns: str: link path CommandLine: python -m utool.util_path symlink Example: >>> # ENABLE_DOCTEST >>> from utool.util_path import * # NOQA >>> import utool as ut >>> dpath = ut.get_app_resource_dir('utool') >>> real_path = join(dpath, 'real_file.txt') >>> link_path = join(dpath, 'link_file.txt') >>> ut.emap(ut.delete, [real_path, link_path], verbose=0) >>> ut.writeto(real_path, 'foo') >>> result = symlink(real_path, link_path) >>> assert ut.readfrom(result) == 'foo' >>> ut.emap(ut.delete, [real_path, link_path], verbose=0) Example: >>> # ENABLE_DOCTEST >>> from utool.util_path import * # NOQA >>> import utool as ut >>> real_dpath = ut.get_app_resource_dir('utool', 'real_dpath') >>> link_dpath = ut.augpath(real_dpath, newfname='link_dpath') >>> real_path = join(real_dpath, 'afile.txt') >>> link_path = join(link_dpath, 'afile.txt') >>> ut.emap(ut.delete, [real_path, link_path], verbose=0) >>> ut.ensuredir(real_dpath) >>> ut.writeto(real_path, 'foo') >>> result = symlink(real_dpath, link_dpath) >>> assert ut.readfrom(link_path) == 'foo' >>> ut.delete(link_dpath, verbose=0) >>> assert ut.checkpath(real_path) >>> ut.delete(real_dpath, verbose=0) >>> assert not ut.checkpath(real_path)", "response": "def symlink(real_path, link_path, overwrite=False, on_error='raise',\n            verbose=2):\n    \"\"\"\n    Attempt to create a symbolic link.\n\n    TODO:\n        Can this be fixed on windows?\n\n    Args:\n        path (str): path to real file or directory\n        link_path (str): path to desired location for symlink\n        overwrite (bool): overwrite existing symlinks (default = False)\n        on_error (str): strategy for dealing with errors.\n            raise or ignore\n        verbose (int):  verbosity level (default=2)\n\n    Returns:\n        str: link path\n\n    CommandLine:\n        python -m utool.util_path symlink\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> dpath = ut.get_app_resource_dir('utool')\n        >>> real_path = join(dpath, 'real_file.txt')\n        >>> link_path = join(dpath, 'link_file.txt')\n        >>> ut.emap(ut.delete, [real_path, link_path], verbose=0)\n        >>> ut.writeto(real_path, 'foo')\n        >>> result = symlink(real_path, link_path)\n        >>> assert ut.readfrom(result) == 'foo'\n        >>> ut.emap(ut.delete, [real_path, link_path], verbose=0)\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> real_dpath = ut.get_app_resource_dir('utool', 'real_dpath')\n        >>> link_dpath = ut.augpath(real_dpath, newfname='link_dpath')\n        >>> real_path = join(real_dpath, 'afile.txt')\n        >>> link_path = join(link_dpath, 'afile.txt')\n        >>> ut.emap(ut.delete, [real_path, link_path], verbose=0)\n        >>> ut.ensuredir(real_dpath)\n        >>> ut.writeto(real_path, 'foo')\n        >>> result = symlink(real_dpath, link_dpath)\n        >>> assert ut.readfrom(link_path) == 'foo'\n        >>> ut.delete(link_dpath, verbose=0)\n        >>> assert ut.checkpath(real_path)\n        >>> ut.delete(real_dpath, verbose=0)\n        >>> assert not ut.checkpath(real_path)\n    \"\"\"\n    path = normpath(real_path)\n    link = normpath(link_path)\n    if verbose:\n        print('[util_path] Creating symlink: path={} link={}'.format(path, link))\n    if os.path.islink(link):\n        if verbose:\n            print('[util_path] symlink already exists')\n        os_readlink = getattr(os, \"readlink\", None)\n        if callable(os_readlink):\n            if os_readlink(link) == path:\n                if verbose > 1:\n                    print('[path] ... and points to the right place')\n                return link\n        else:\n            print('[util_path] Warning, symlinks are not implemented on windows')\n        if verbose > 1:\n            print('[util_path] ... but it points somewhere else')\n        if overwrite:\n            delete(link, verbose > 1)\n        elif on_error == 'ignore':\n            return False\n    try:\n        os_symlink = getattr(os, \"symlink\", None)\n        if callable(os_symlink):\n            os_symlink(path, link)\n        else:\n            win_shortcut(path, link)\n    except Exception as ex:\n        import utool as ut\n        checkpath(link, verbose=True)\n        checkpath(path, verbose=True)\n        do_raise = (on_error == 'raise')\n        ut.printex(ex, '[util_path] error making symlink',\n                   iswarning=not do_raise)\n        if do_raise:\n            raise\n    return link"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all broken links in a directory Args: dpath (str): directory path Returns: int: num removed References: http://stackoverflow.com/questions/20794/find-broken-symlinks-with-python CommandLine: python -m utool remove_broken_links:0 Example: >>> # DISABLE_DOCTEST >>> # SCRIPT >>> from utool.util_path import * # NOQA >>> remove_broken_links('.') Example: >>> # ENABLE_DOCTEST >>> from utool.util_path import * # NOQA >>> import utool as ut >>> dpath = ut.ensure_app_resource_dir('utool', 'path_tests') >>> ut.delete(dpath) >>> test_dpath = ut.ensuredir(join(dpath, 'testdpath')) >>> test_fpath = ut.ensurefile(join(dpath, 'testfpath.txt')) >>> flink1 = ut.symlink(test_fpath, join(dpath, 'flink1')) >>> dlink1 = ut.symlink(test_fpath, join(dpath, 'dlink1')) >>> assert len(ut.ls(dpath)) == 4 >>> ut.delete(test_fpath) >>> assert len(ut.ls(dpath)) == 3 >>> remove_broken_links(dpath) >>> ut.delete(test_dpath) >>> remove_broken_links(dpath) >>> assert len(ut.ls(dpath)) == 0", "response": "def remove_broken_links(dpath, verbose=True):\n    \"\"\"\n    Removes all broken links in a directory\n\n    Args:\n        dpath (str):  directory path\n\n    Returns:\n        int: num removed\n\n    References:\n        http://stackoverflow.com/questions/20794/find-broken-symlinks-with-python\n\n    CommandLine:\n        python -m utool remove_broken_links:0\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # SCRIPT\n        >>> from utool.util_path import *  # NOQA\n        >>> remove_broken_links('.')\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> dpath = ut.ensure_app_resource_dir('utool', 'path_tests')\n        >>> ut.delete(dpath)\n        >>> test_dpath = ut.ensuredir(join(dpath, 'testdpath'))\n        >>> test_fpath = ut.ensurefile(join(dpath, 'testfpath.txt'))\n        >>> flink1 = ut.symlink(test_fpath, join(dpath, 'flink1'))\n        >>> dlink1 = ut.symlink(test_fpath, join(dpath, 'dlink1'))\n        >>> assert len(ut.ls(dpath)) == 4\n        >>> ut.delete(test_fpath)\n        >>> assert len(ut.ls(dpath)) == 3\n        >>> remove_broken_links(dpath)\n        >>> ut.delete(test_dpath)\n        >>> remove_broken_links(dpath)\n        >>> assert len(ut.ls(dpath)) == 0\n    \"\"\"\n    fname_list = [join(dpath, fname) for fname in os.listdir(dpath)]\n    broken_links = list(filterfalse(exists, filter(islink, fname_list)))\n    num_broken = len(broken_links)\n    if verbose:\n        if verbose > 1 or num_broken > 0:\n            print('[util_path] Removing %d broken links in %r' % (num_broken, dpath,))\n    for link in broken_links:\n        os.unlink(link)\n    return num_broken"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef non_existing_path(path_, dpath=None, offset=0, suffix=None,\n                            force_fmt=False):\n    r\"\"\"\n    Searches for and finds a path garuenteed to not exist.\n\n    Args:\n        path_ (str):  path string. If may include a \"%\" formatstr.\n        dpath (str):  directory path(default = None)\n        offset (int): (default = 0)\n        suffix (None): (default = None)\n\n    Returns:\n        str: path_ - path string\n\n    CommandLine:\n        python -m utool.util_path non_existing_path\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> base = ut.ensure_app_resource_dir('utool', 'tmp')\n        >>> ut.touch(base + '/tmp.txt')\n        >>> ut.touch(base + '/tmp0.txt')\n        >>> ut.delete(base + '/tmp1.txt')\n        >>> path_ = base + '/tmp.txt'\n        >>> newpath = ut.non_existing_path(path_)\n        >>> assert basename(newpath) == 'tmp1.txt'\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> import utool as ut\n        >>> base = ut.ensure_app_resource_dir('utool', 'tmp')\n        >>> ut.ensurepath(base + '/dir_old')\n        >>> ut.ensurepath(base + '/dir_old0')\n        >>> ut.ensurepath(base + '/dir_old1')\n        >>> ut.delete(base + '/dir_old2')\n        >>> path_ = base + '/dir'\n        >>> suffix = '_old'\n        >>> newpath = ut.non_existing_path(path_, suffix=suffix)\n        >>> ut.assert_eq(basename(newpath), 'dir_old2')\n    \"\"\"\n    import utool as ut\n    from os.path import basename, dirname\n\n    if dpath is None:\n        dpath = dirname(path_)\n    base_fmtstr = basename(path_)\n    if suffix is not None:\n        base_fmtstr = ut.augpath(base_fmtstr, suffix)\n\n    if '%' not in base_fmtstr:\n        if not force_fmt:\n            # If we have don't have to format,\n            # then try to use the first choice\n            first_choice = join(dpath, base_fmtstr)\n            if not exists(first_choice):\n                return first_choice\n        # otherwise we ensure we can format and we continue\n        base_fmtstr = ut.augpath(base_fmtstr, '%d')\n\n    dname_list = ut.glob(dpath, pattern='*', recursive=False, with_files=True,\n                         with_dirs=True)\n    conflict_set = set(basename(dname) for dname in dname_list)\n\n    newname = ut.get_nonconflicting_string(base_fmtstr, conflict_set,\n                                           offset=offset)\n    newpath = join(dpath, newname)\n    return newpath", "response": "r Searches for and finds a path garuenteed to not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an sqlite lookup table of scannrs with quant data.", "response": "def create_isobaric_quant_lookup(quantdb, specfn_consensus_els, channelmap):\n    \"\"\"Creates an sqlite lookup table of scannrs with quant data.\n\n    spectra - an iterable of tupled (filename, spectra)\n    consensus_els - a iterable with consensusElements\"\"\"\n    # store quantchannels in lookup and generate a db_id vs channel map\n    channels_store = ((name,) for name, c_id\n                      in sorted(channelmap.items(), key=lambda x: x[1]))\n    quantdb.store_channelmap(channels_store)\n    channelmap_dbid = {channelmap[ch_name]: ch_id for ch_id, ch_name in\n                       quantdb.get_channelmap()}\n    quants = []\n    mzmlmap = quantdb.get_mzmlfile_map()\n    for specfn, consensus_el in specfn_consensus_els:\n        rt = openmsreader.get_consxml_rt(consensus_el)\n        rt = round(float(Decimal(rt) / 60), 12)\n        qdata = get_quant_data(consensus_el)\n        spectra_id = quantdb.get_spectra_id(mzmlmap[specfn],\n                                            retention_time=rt)\n        for channel_no in sorted(qdata.keys()):\n            quants.append((spectra_id, channelmap_dbid[channel_no],\n                           qdata[channel_no]))\n            if len(quants) == DB_STORE_CHUNK:\n                quantdb.store_isobaric_quants(quants)\n    quantdb.store_isobaric_quants(quants)\n    quantdb.index_isobaric_quants()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_precursor_quant_lookup(quantdb, mzmlfn_feats, quanttype,\n                                  rttol, mztol, mztoltype):\n    \"\"\"Fills quant sqlite with precursor quant from:\n        features - generator of xml features from openms\n    \"\"\"\n    featparsermap = {'kronik': kronik_featparser,\n                     'openms': openms_featparser,\n                     }\n    features = []\n    mzmlmap = quantdb.get_mzmlfile_map()\n    for specfn, feat_element in mzmlfn_feats:\n        feat = featparsermap[quanttype](feat_element)\n        features.append((mzmlmap[specfn], feat['rt'], feat['mz'],\n                         feat['charge'], feat['intensity'])\n                        )\n        if len(features) == DB_STORE_CHUNK:\n            quantdb.store_ms1_quants(features)\n            features = []\n    quantdb.store_ms1_quants(features)\n    quantdb.index_precursor_quants()\n    align_quants_psms(quantdb, rttol, mztol, mztoltype)", "response": "Fills quant sqlite with precursor quant from mzmlfn_feats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_precursors_from_window(quantdb, minmz):\n    featmap = {}\n    mz = False\n    features = quantdb.get_precursor_quant_window(FEATURE_ALIGN_WINDOW_AMOUNT,\n                                                  minmz)\n    for feat_id, fn_id, charge, mz, rt in features:\n        try:\n            featmap[fn_id][charge].append((mz, rt, feat_id))\n        except KeyError:\n            try:\n                featmap[fn_id][charge] = [(mz, rt, feat_id)]\n            except KeyError:\n                featmap[fn_id] = {charge: [(mz, rt, feat_id)]}\n    return featmap, mz", "response": "Returns a dict of a specified amount of features from the ms1 quant database and the highest mz of those features"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_quant_data(cons_el):\n    quant_out = {}\n    for reporter in cons_el.findall('.//element'):\n        quant_out[reporter.attrib['map']] = reporter.attrib['it']\n    return quant_out", "response": "Gets quant data from consensusXML element"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_plat_specifier():\n    import setuptools  # NOQA\n    import distutils\n    plat_name = distutils.util.get_platform()\n    plat_specifier = \".%s-%s\" % (plat_name, sys.version[0:3])\n    if hasattr(sys, 'gettotalrefcount'):\n        plat_specifier += '-pydebug'\n    return plat_specifier", "response": "Returns the platform specifier used by distutils\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_system_python_library():\n    import os\n    import utool as ut\n    from os.path import basename, realpath\n    pyname = basename(realpath(sys.executable))\n    ld_library_path = os.environ['LD_LIBRARY_PATH']\n    libdirs = [x for x in ld_library_path.split(os.pathsep) if x] + ['/usr/lib']\n    libfiles = ut.flatten([ut.glob(d, '*' + ut.get_lib_ext(), recursive=True) for d in libdirs])\n    python_libs = [realpath(f) for f in libfiles if 'lib' + pyname in basename(f)]\n    python_libs = ut.unique_ordered(python_libs)\n    assert len(python_libs) == 1, str(python_libs)\n    return python_libs[0]", "response": "Get the path to the system python library."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds an executable file to the current directory.", "response": "def chmod_add_executable(fpath, group=True, user=True):\n    \"\"\"\n    References:\n        http://stackoverflow.com/questions/15607903/python-module-os-chmodfile-664-does-not-change-the-permission-to-rw-rw-r-bu\n        http://www.tutorialspoint.com/python/os_chmod.htm\n        https://en.wikipedia.org/wiki/Chmod\n    \"\"\"\n    import stat\n    orig_mode = os.stat(fpath).st_mode\n    new_mode = orig_mode\n    if group:\n        new_mode |= stat.S_IXGRP\n    if user:\n        # new_mode |= stat.S_IXUSR | stat.S_IEXEC\n        new_mode |= stat.S_IXGRP | stat.S_IEXEC\n    os.chmod(fpath, new_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_disk_space(start_path='.'):\n    total_size = 0\n    for root, dname_list, fname_list in os.walk(start_path):\n        for fname in fname_list:\n            fpath = os.path.join(root, fname)\n            try:\n                total_size += os.path.getsize(fpath)\n            except OSError:\n                pass\n    return total_size", "response": "Returns the total size of the disk space of the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef python_executable(check=True, short=False):\n    if not check:\n        python_exe = 'python'\n    else:\n        from os.path import isdir\n        python_exe_long = unixpath(sys.executable)\n        python_exe = python_exe_long\n        if short:\n            python_exe_short = basename(python_exe_long)\n            found = search_env_paths(python_exe_short, key_list=['PATH'],\n                                     verbose=False)\n            found = [f for f in found if not isdir(f)]\n            if len(found) > 0:\n                if found[0] == python_exe_long:\n                    # Safe to use the short name in this env\n                    python_exe = python_exe_short\n    return python_exe", "response": "r Returns the name of the python executable that is available in the environment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_dynlib_dependencies(lib_path):\n    if LINUX:\n        ldd_fpath = '/usr/bin/ldd'\n        depend_out, depend_err, ret = cmd(ldd_fpath, lib_path, verbose=False)\n    elif DARWIN:\n        otool_fpath = '/opt/local/bin/otool'\n        depend_out, depend_err, ret = cmd(otool_fpath, '-L', lib_path, verbose=False)\n    elif WIN32:\n        depend_out, depend_err, ret = cmd('objdump', '-p', lib_path, verbose=False)\n        #fnmatch.filter(depend_out.split('\\n'), '*DLL*')\n        relevant_lines = [line for line in depend_out.splitlines() if 'DLL Name:' in line]\n        depend_out = '\\n'.join(relevant_lines)\n    assert ret == 0, 'bad dependency check'\n    return depend_out", "response": "Returns the dynamic library dependencies for the current platform."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_dynlib_exports(lib_path):\n    if LINUX:\n        '''\n        nm_fpath = '/usr/bin/nm'\n        exportssout, err, ret = cmd(nm_fpath, '-D', lib_path, '|', 'c++filt', verbose=False)\n        lines = exportssout.split('\\n')\n        #lines = [line[19:] for line in line]\n        others = []\n        info = []\n        for line in lines:\n            if line == '':\n                continue\n            line = ut.remove_doublspaces(line)\n            words = line.split(' ')\n            if len(words) > 2:\n                # address, type_, rest\n                rest = ' '.join(words[2:])\n                info.append((rest, words[0], words[1]))\n            else:\n                others.append(line)\n\n        # remove duplicate address spaces\n        info = ut.unique_ordered(info)\n        # remove stdlib\n        info = [line for line in info if 'std::' not in line[0]]\n        info = [line for line in info if not line[0].startswith('typeinfo')]\n        info = [line for line in info if not line[0].startswith('vtable')]\n        info = [line for line in info if 'flann' in line[0]]\n        info = [line for line in info if 'flann_' in line[0]]\n\n        info2 = []\n        for rest, loc, type_ in info:\n            parts = rest.split(' ')\n            rettype = parts[0]\n            rest2 = ' '.join(parts[1:])\n            if not rest2.startswith('__'):\n                info2.append((rettype, rest2, type_))\n                #info2.append((rettype, rest2, type_, loc))\n\n        len([line for line in info if 'flann' in line[0]])\n\n        len([(line.split(' ')[0], line.split(' ')[1], ' '.join(line.split(' ')[2:])) for line in lines])\n        len([line for line in lines if line.startswith('flann::')])\n        len([line for line in lines if 'flann_' in line])\n        len([line for line in lines if not line.endswith(')') and 'flann_' in line])\n        # HACK: FIND A CORRECT PARSING\n        return info2\n        '''\n    elif DARWIN:\n        otool_fpath = '/opt/local/bin/otool'\n        exportssout, err, ret = cmd(otool_fpath, '-L', lib_path, verbose=False)\n        #TODO\n    elif WIN32:\n        exportssout, err, ret = cmd('objdump', '-p', lib_path, verbose=False)\n        #TODO\n        #fnmatch.filter(depend_out.split('\\n'), '*DLL*')\n        #relevant_lines = [line for line in depend_out.splitlines() if 'DLL Name:' in line]\n        #depend_out = '\\n'.join(relevant_lines)\n    assert ret == 0, 'bad dependency check'\n    return exportssout", "response": "Returns the names of the functions that are exported by the dynamic library."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef startfile(fpath, detatch=True, quote=False, verbose=False, quiet=True):\n    print('[cplat] startfile(%r)' % fpath)\n    fpath = normpath(fpath)\n    # print('[cplat] fpath=%s' % fpath)\n    if not exists(fpath):\n        raise Exception('Cannot start nonexistant file: %r' % fpath)\n    #if quote:\n    #    fpath = '\"%s\"' % (fpath,)\n    if not WIN32:\n        fpath = pipes.quote(fpath)\n    if LINUX:\n        #out, err, ret = cmd(['xdg-open', fpath], detatch=True)\n        outtup = cmd(('xdg-open', fpath), detatch=detatch, verbose=verbose, quiet=quiet)\n        #outtup = cmd('xdg-open', fpath, detatch=detatch)\n    elif DARWIN:\n        outtup = cmd(('open', fpath), detatch=detatch, verbose=verbose, quiet=quiet)\n    elif WIN32:\n        os.startfile(fpath)\n    else:\n        raise RuntimeError('Unknown Platform')\n    if outtup is not None:\n        out, err, ret = outtup\n        if not ret:\n            raise Exception(out + ' -- ' + err)\n    pass", "response": "Starts a new file in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun gvim. Can also accept a module or class or function.", "response": "def editfile(fpath):\n    \"\"\" Runs gvim. Can also accept a module / class / function \"\"\"\n    if not isinstance(fpath, six.string_types):\n        from six import types\n        print('Rectify to module fpath = %r' % (fpath,))\n        if isinstance(fpath, types.ModuleType):\n            fpath = fpath.__file__\n        else:\n            fpath =  sys.modules[fpath.__module__].__file__\n        fpath_py = fpath.replace('.pyc', '.py')\n        if exists(fpath_py):\n            fpath = fpath_py\n\n    print('[cplat] startfile(%r)' % fpath)\n    if not exists(fpath):\n        raise Exception('Cannot start nonexistant file: %r' % fpath)\n    if LINUX:\n        out, err, ret = cmd(geteditor(), fpath, detatch=True)\n        if not ret:\n            raise Exception(out + ' -- ' + err)\n    elif DARWIN:\n        out, err, ret = cmd(geteditor(), fpath, detatch=True)\n        if not ret:\n            raise Exception(out + ' -- ' + err)\n    else:\n        out, err, ret = cmd(geteditor(), fpath, detatch=True)\n        if not ret:\n            raise Exception(out + ' -- ' + err)\n        #os.startfile(fpath)\n    pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_directory(dname=None, fname=None, verbose=True):\n    from utool.util_arg import STRICT\n    from utool.util_path import checkpath\n    # from utool.util_str import SINGLE_QUOTE, DOUBLE_QUOTE\n\n    if HAVE_PATHLIB and isinstance(dname, pathlib.Path):\n        dname = str(dname)\n\n    if verbose:\n        print('[cplat] view_directory(%r) ' % dname)\n    dname = os.getcwd() if dname is None else dname\n    open_prog = {\n        'win32': 'explorer.exe',\n        'linux': 'nautilus',\n        'darwin': 'open'\n    }[OS_TYPE]\n    dname = normpath(dname)\n    if STRICT:\n        assert checkpath(dname, verbose=verbose), 'directory doesnt exit'\n    if fname is not None and OS_TYPE == 'linux':\n        arg = join(dname, fname)\n    else:\n        arg = dname\n    # if ' ' in dname and not dname.startswith((SINGLE_QUOTE, DOUBLE_QUOTE)):\n    #     # Ensure quotations\n    #     dname = '\"%s\"' % dname\n    # if not WIN32:\n    #     arg = dname\n    #     # arg = subprocess.list2cmdline([dname])\n    #     # arg = pipes.quote(dname)\n    # else:\n    #     arg = dname\n    # spawn and detatch process\n    args = (open_prog, arg)\n    print(subprocess.list2cmdline(args))\n    subprocess.Popen(args)", "response": "View a directory in the operating system file browser."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a directory which should be writable for any application", "response": "def platform_cache_dir():\n    \"\"\"\n    Returns a directory which should be writable for any application\n    This should be used for temporary deletable data.\n    \"\"\"\n    if WIN32:  # nocover\n        dpath_ = '~/AppData/Local'\n    elif LINUX:  # nocover\n        dpath_ = '~/.cache'\n    elif DARWIN:  # nocover\n        dpath_  = '~/Library/Caches'\n    else:  # nocover\n        raise NotImplementedError('Unknown Platform  %r' % (sys.platform,))\n    dpath = normpath(expanduser(dpath_))\n    return dpath"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ensure_app_cache_dir(appname, *args):\n    import utool as ut\n    dpath = get_app_cache_dir(appname, *args)\n    ut.ensuredir(dpath)\n    return dpath", "response": "This function creates a directory for the app cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __parse_cmd_args(args, sudo, shell):\n    # Case where tuple is passed in as only argument\n    if isinstance(args, tuple) and len(args) == 1 and isinstance(args[0], tuple):\n        args = args[0]\n\n    if shell:\n        # When shell is True, ensure args is a string\n        if isinstance(args, six.string_types):\n            pass\n        elif  isinstance(args, (list, tuple)) and len(args) > 1:\n            args = ' '.join(args)\n        elif isinstance(args, (list, tuple)) and len(args) == 1:\n            if isinstance(args[0], (tuple, list)):\n                args = ' '.join(args)\n            elif isinstance(args[0], six.string_types):\n                args = args[0]\n    else:\n        # When shell is False, ensure args is a tuple\n        if isinstance(args, six.string_types):\n            args = shlex.split(args, posix=not WIN32)\n        elif isinstance(args, (list, tuple)):\n            if len(args) > 1:\n                args = tuple(args)\n            elif len(args) == 1:\n                if isinstance(args[0], (tuple, list)):\n                    args = tuple(args[0])\n                elif isinstance(args[0], six.string_types):\n                    args = shlex.split(args[0], posix=not WIN32)\n    if sudo is True:\n        if not WIN32:\n            if shell:\n                args = 'sudo ' + args\n            else:\n                args = tuple(['sudo']) + tuple(args)\n            #if isinstance(args, six.string_types):\n            #    args = shlex.split(args)\n            #args = ['sudo'] + args\n            ## using sudo means we need to use a single string I believe\n            #args = ' '.join(args)\n        else:\n            # TODO: strip out sudos\n            pass\n    # HACK FOR WINDOWS AGAIN\n    # makes  this command work:\n    # python -c \"import utool as ut; ut.cmd('build\\\\hesaffexe.exe ' + ut.grab_test_imgpath('star.png'))\"\n    # and this should still work\n    # python -c \"import utool as ut; ut.cmd('build\\\\hesaffexe.exe', ut.grab_test_imgpath('star.png'))\"\n    if WIN32:\n        if len(args) == 1 and isinstance(args[0], six.string_types):\n            args = shlex.split(args[0], posix=not WIN32)\n    return args", "response": "Parse command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cmd(*args, **kwargs):\n    try:\n        # Parse the keyword arguments\n        verbose, detatch, shell, sudo, pad_stdout = __parse_cmd_kwargs(kwargs)\n        quiet = kwargs.pop('quiet', False)\n        silence = kwargs.pop('silence', False)\n        if pad_stdout:\n            sys.stdout.flush()\n            print('\\n+--------')\n        args = __parse_cmd_args(args, sudo, shell)\n        # Print what you are about to do\n        if not quiet:\n            print('[ut.cmd] RUNNING: %r' % (args,))\n        # Open a subprocess with a pipe\n        if kwargs.get('dryrun', False):\n            print('[ut.cmd] Exiting because dryrun=True')\n            return None, None, None\n        proc = subprocess.Popen(args, stdout=subprocess.PIPE,\n                                stderr=subprocess.STDOUT, shell=shell,\n                                universal_newlines=True\n                                # universal_newlines=False\n                                )\n        hack_use_stdout = True\n        if detatch:\n            if not quiet:\n                print('[ut.cmd] PROCESS DETATCHING. No stdoutput can be reported...')\n            # There is no immediate confirmation as to whether or not the script\n            # finished. It might still be running for all you know\n            return None, None, proc\n        else:\n            if verbose and not detatch:\n                if not quiet:\n                    print('[ut.cmd] RUNNING WITH VERBOSE OUTPUT')\n                logged_out = []\n                for line in _run_process(proc):\n                    #line_ = line if six.PY2 else line.decode('utf-8')\n                    line_ = line if six.PY2 else line\n                    if len(line_) > 0:\n                        if not silence:\n                            if hack_use_stdout:\n                                sys.stdout.write(line_)\n                                sys.stdout.flush()\n                            else:\n                                # TODO make this play nicely with loggers\n                                print_(line_)\n                        logged_out.append(line)\n                try:\n                    from utool import util_str  # NOQA\n                    # logged_out = util_str.ensure_unicode_strlist(logged_out)\n                    out = '\\n'.join(logged_out)\n                except UnicodeDecodeError:\n                    from utool import util_str  # NOQA\n                    logged_out = util_str.ensure_unicode_strlist(logged_out)\n                    out = '\\n'.join(logged_out)\n                    # print('logged_out = %r' % (logged_out,))\n                    # raise\n                (out_, err) = proc.communicate()\n                #print('[ut.cmd] out: %s' % (out,))\n                if not quiet:\n                    try:\n                        print('[ut.cmd] stdout: %s' % (out_,))\n                        print('[ut.cmd] stderr: %s' % (err,))\n                    except UnicodeDecodeError:\n                        from utool import util_str  # NOQA\n                        print('[ut.cmd] stdout: %s' % (util_str.ensure_unicode(out_),))\n                        print('[ut.cmd] stderr: %s' % (util_str.ensure_unicode(err),))\n\n            else:\n                # Surpress output\n                #print('[ut.cmd] RUNNING WITH SUPRESSED OUTPUT')\n                (out, err) = proc.communicate()\n            # Make sure process if finished\n            ret = proc.wait()\n            if not quiet:\n                print('[ut.cmd] PROCESS FINISHED')\n            if pad_stdout:\n                print('L________\\n')\n            return out, err, ret\n    except Exception as ex:\n        import utool as ut\n        #if isinstance(args, tuple):\n        #    print(ut.truepath(args[0]))\n        #elif isinstance(args, six.string_types):\n        #    print(ut.unixpath(args))\n        ut.printex(ex, 'Exception running ut.cmd',\n                   keys=['verbose', 'detatch', 'shell', 'sudo', 'pad_stdout'],\n                   tb=True)", "response": "r A really roundabout way to issue a system call to get a new version of the current version of the current version of the current version of the version of the version of the version of the version of the version of the version of the version of the version"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmd2(command, shell=False, detatch=False, verbose=False, verbout=None):\n    import shlex\n    if isinstance(command, (list, tuple)):\n        raise ValueError('command tuple not supported yet')\n    args = shlex.split(command, posix=not WIN32)\n    if verbose is True:\n        verbose = 2\n    if verbout is None:\n        verbout = verbose >= 1\n    if verbose >= 2:\n        print('+=== START CMD2 ===')\n        print('Command:')\n        print(command)\n        if verbout:\n            print('----')\n            print('Stdout:')\n    proc = subprocess.Popen(args, stdout=subprocess.PIPE,\n                            stderr=subprocess.STDOUT, shell=shell,\n                            universal_newlines=True)\n    if detatch:\n        info = {'proc': proc}\n    else:\n        write_fn = sys.stdout.write\n        flush_fn = sys.stdout.flush\n        logged_out = []\n        for line in _run_process(proc):\n            #line_ = line if six.PY2 else line.decode('utf-8')\n            line_ = line if six.PY2 else line\n            if len(line_) > 0:\n                if verbout:\n                    write_fn(line_)\n                    flush_fn()\n                logged_out.append(line)\n        try:\n            from utool import util_str  # NOQA\n            # out = '\\n'.join(logged_out)\n            out = ''.join(logged_out)\n        except UnicodeDecodeError:\n            from utool import util_str  # NOQA\n            logged_out = util_str.ensure_unicode_strlist(logged_out)\n            # out = '\\n'.join(logged_out)\n            out = ''.join(logged_out)\n            # print('logged_out = %r' % (logged_out,))\n            # raise\n        (out_, err) = proc.communicate()\n        ret = proc.wait()\n        info = {\n            'out': out,\n            'err': err,\n            'ret': ret,\n        }\n    if verbose >= 2:\n        print('L___ END CMD2 ___')\n    return info", "response": "Runs a command and returns the status of the command"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_flops():\n    from sys import stdout\n    from re import compile\n\n    filename = \"linpack.out\"\n    fpnum = r'\\d+\\.\\d+E[+-]\\d\\d'\n    fpnum_1 = fpnum + r' +'\n    pattern = compile(r'^ *' + fpnum_1 + fpnum_1 + fpnum_1 + r'(' + fpnum + r') +' + fpnum_1 + fpnum + r' *\\n$')\n    speeds = [0.0, 1.0e75, 0.0]\n\n    file = open(filename)\n    count = 0\n    while file :\n        line = file.readline()\n        if not line :\n            break\n        if pattern.match(line) :\n            count = count + 1\n            x = float(pattern.sub(r'\\1', line))\n            if x < 1.0 :\n                print(count)\n            speeds[0] = speeds[0] + x\n            speeds[1] = min(speeds[1], x)\n            speeds[2] = max(speeds[2], x)\n    file.close()\n    if count != 0 :\n        speeds[0] = speeds[0] / count\n\n    stdout.write(\"%6.1f MFlops (%d from %.1f to %.1f)\\n\" % (speeds[0], count, speeds[1], speeds[2]))", "response": "Get the total fan from the last one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python_dynlib():\n    import sysconfig\n    cfgvars = sysconfig.get_config_vars()\n    dynlib = os.path.join(cfgvars['LIBDIR'], cfgvars['MULTIARCH'], cfgvars['LDLIBRARY'])\n    if not exists(dynlib):\n        dynlib = os.path.join(cfgvars['LIBDIR'], cfgvars['LDLIBRARY'])\n    assert exists(dynlib)\n    return dynlib", "response": "get_python_dynlib returns the path to the Python dynamic library"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search_env_paths(fname, key_list=None, verbose=None):\n    import utool as ut\n    # from os.path import join\n    if key_list is None:\n        key_list = [key for key in os.environ if key.find('PATH') > -1]\n        print('key_list = %r' % (key_list,))\n\n    found = ut.ddict(list)\n\n    for key in key_list:\n        dpath_list = os.environ[key].split(os.pathsep)\n        for dpath in dpath_list:\n            #if verbose:\n            #    print('dpath = %r' % (dpath,))\n            # testname = join(dpath, fname)\n            matches = ut.glob(dpath, fname)\n            found[key].extend(matches)\n            #import fnmatch\n            #import utool\n            #utool.embed()\n            #if ut.checkpath(testname, verbose=False):\n            #    if verbose:\n            #        print('Found in key=%r' % (key,))\n            #        ut.checkpath(testname, verbose=True, info=True)\n            #    found += [testname]\n    return dict(found)", "response": "r Searches your PATH to see if fname exists in the environment"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef change_term_title(title):\n    if True:\n        # Disabled\n        return\n    if not WIN32:\n        #print(\"CHANGE TERM TITLE to %r\" % (title,))\n        if title:\n            #os.environ['PS1'] = os.environ['PS1'] + '''\"\\e]2;\\\"''' + title + '''\\\"\\a\"'''\n            cmd_str = r'''echo -en \"\\033]0;''' + title + '''\\a\"'''\n            os.system(cmd_str)", "response": "Change the terminal title for identifying debugging tasks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_keyboard_input(text=None, key_list=None):\n    #key_mapping = {\n    #    'enter':\n    #}\n    if WIN32:\n        #raise NotImplementedError()\n        #import win32api\n        #import win32gui\n        #import win32con\n        #hwnd = win32gui.GetForegroundWindow()\n        #print('entering text into %r' % (win32gui.GetWindowText(hwnd ),))\n        #win32con.VK_RETURN\n\n        #def callback(hwnd, hwnds):\n            #if win32gui.IsWindowVisible(hwnd) and win32gui.IsWindowEnabled(hwnd):\n                #hwnds[win32gui.GetClassName(hwnd)] = hwnd\n            #return True\n        #hwnds = {}\n        #win32gui.EnumChildWindows(hwnd, callback, hwnds)\n\n        #for ord_char in map(ord, text):\n            #win32api.SendMessage(hwnd, win32con.WM_CHAR, ord_char, 0)\n        from utool._internal import win32_send_keys\n        pause = float(.05)\n        text = 'paste'\n        keys = text\n        kw = dict(with_spaces=False, with_tabs=True, with_newlines=False)\n        win32_send_keys.SendKeys(keys, pause=pause, turn_off_numlock=True, **kw)\n        #win32_send_keys\n        #import time\n        #keys_ = win32_send_keys.parse_keys(keys, **kw)\n        #for k in keys_:\n        #    k.Run()\n        #    time.sleep(pause)\n\n    else:\n        if key_list is None:\n            char_map = {\n                '%': 'shift+5'\n            }\n            key_list = [char_map.get(char, char) for char in text]\n        xdotool_args = ['xdotool', 'key'] + key_list\n        #, 'shift+5', 'p', 'a', 's', 't', 'e', 'enter']\n        cmd = ' '.join(xdotool_args)\n        print('Running: cmd=%r' % (cmd,))\n        print('+---')\n        print(cmd)\n        print('L___')\n        os.system(cmd)", "response": "Send keyboard input to the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npastes the current page of the current page.", "response": "def ipython_paste(*args, **kwargs):\n    \"\"\" pastes for me FIXME: make something like this work on unix and windows\"\"\"\n    if WIN32:\n        pass\n    else:\n        winhandle = 'joncrall@Hyrule'\n        args1 = ['wmctrl', '-a', winhandle]\n        args2 = ['xdotool', 'key', 'shift+5', 'p', 'a', 's', 't', 'e', 'enter']\n        os.system(' '.join(args1))\n        os.system(' '.join(args2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_system_users():\n    import utool as ut\n    text = ut.read_from('/etc/passwd')\n    userinfo_text_list = text.splitlines()\n    userinfo_list = [uitext.split(':') for uitext in userinfo_text_list]\n    #print(ut.repr4(sorted(userinfo_list)))\n    bash_users = [tup for tup in userinfo_list if tup[-1] == '/bin/bash']\n    print(ut.repr4(sorted(bash_users)))", "response": "r Prints the system users on the system\n    On unix looks for bash users in / etc. passwd and print them on the system\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_installed_debian(pkgname):\n    import utool as ut\n    #pkgname = 'espeak'\n    #pkgname = 'sudo'\n    #ut.cmd('hash ' + pkgname + ' 2>/dev/null')\n    tup = ut.cmd('hash ' + pkgname + ' 2>/dev/null', quiet=True, pad_stdout=False)\n    out, err, ret = tup\n    is_installed = (ret == 0)\n    return is_installed", "response": "Check if a given package is installed in Debian"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunload a module and all its children.", "response": "def unload_module(modname):\n    \"\"\"\n    WARNING POTENTIALLY DANGEROUS AND MAY NOT WORK\n\n    References:\n        http://stackoverflow.com/questions/437589/how-do-i-unload-reload-a-python-module\n\n    CommandLine:\n        python -m utool.util_cplat --test-unload_module\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import sys, gc  # NOQA\n        >>> import pyhesaff\n        >>> import utool as ut\n        >>> modname = 'pyhesaff'\n        >>> print('%s refcount=%r' % (modname, sys.getrefcount(pyhesaff),))\n        >>> #referrer_list = gc.get_referrers(sys.modules[modname])\n        >>> #print('referrer_list = %s' % (ut.repr4(referrer_list),))\n        >>> ut.unload_module(modname)\n        >>> assert pyhesaff is None\n\n    \"\"\"\n    import sys\n    import gc\n    if modname in sys.modules:\n        referrer_list = gc.get_referrers(sys.modules[modname])\n        #module = sys.modules[modname]\n        for referer in referrer_list:\n            if referer is not sys.modules:\n                referer[modname] = None\n            #del referer[modname]\n        #sys.modules[modname] = module\n        #del module\n        refcount = sys.getrefcount(sys.modules[modname])\n        print('%s refcount=%r' % (modname, refcount))\n        del sys.modules[modname]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef base_add_isoquant_data(features, quantfeatures, acc_col, quantacc_col,\n                           quantfields):\n    \"\"\"Generic function that takes a peptide or protein table and adds\n    quant data from ANOTHER such table.\"\"\"\n    quant_map = get_quantmap(quantfeatures, quantacc_col, quantfields)\n    for feature in features:\n        feat_acc = feature[acc_col]\n        outfeat = {k: v for k, v in feature.items()}\n        try:\n            outfeat.update(quant_map[feat_acc])\n        except KeyError:\n            outfeat.update({field: 'NA' for field in quantfields})\n        yield outfeat", "response": "Generic function that takes a peptide or protein table and adds the quant data from ANOTHER such table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_quantmap(features, acc_col, quantfields):\n    qmap = {}\n    for feature in features:\n        feat_acc = feature.pop(acc_col)\n        qmap[feat_acc] = {qf: feature[qf] for qf in quantfields}\n    return qmap", "response": "Extracts the information based on the quantfields list input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_argv_cfg(argname, default=[''], named_defaults_dict=None,\n                   valid_keys=None, alias_keys=None):\n    \"\"\"\n    simple configs\n\n    Args:\n        argname (str):\n        default (list): (default = [])\n        named_defaults_dict (dict): (default = None)\n        valid_keys (None): (default = None)\n\n    Returns:\n        list: cfg_list\n\n    CommandLine:\n        python -m utool.util_gridsearch --exec-parse_argv_cfg --filt :foo=bar\n        python -m utool.util_gridsearch --exec-parse_argv_cfg\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> argname = '--filt'\n        >>> cfg_list = parse_argv_cfg(argname)\n        >>> result = ('cfg_list = %s' % (six.text_type(cfg_list),))\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    if ut.in_jupyter_notebook():\n        # dont parse argv in ipython notebook\n        cfgstr_list = default\n    else:\n        cfgstr_list = ut.get_argval(argname, type_=list, default=default)\n    if cfgstr_list is None:\n        return None\n    cfg_combos_list = parse_cfgstr_list2(cfgstr_list,\n                                         named_defaults_dict=named_defaults_dict,\n                                         valid_keys=valid_keys,\n                                         alias_keys=alias_keys,\n                                         strict=False)\n    cfg_list = ut.flatten(cfg_combos_list)\n    return cfg_list", "response": "Parse the argv - cfg list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_nonvaried_cfg_lbls(cfg_list, default_cfg=None, mainkey='_cfgname'):\n    try:\n        cfgname_list = [cfg[mainkey] for cfg in cfg_list]\n    except KeyError:\n        cfgname_list = [''] * len(cfg_list)\n    nonvaried_cfg = partition_varied_cfg_list(cfg_list, default_cfg)[0]\n    cfglbl_list = [get_cfg_lbl(nonvaried_cfg, name) for name in cfgname_list]\n    return cfglbl_list", "response": "r Returns a list of non - variied cfg names"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_cfg_lbl(cfg, name=None, nonlbl_keys=INTERNAL_CFGKEYS, key_order=None,\n                with_name=True, default_cfg=None, sep=''):\n    r\"\"\"\n    Formats a flat configuration dict into a short string label. This is useful\n    for re-creating command line strings.\n\n    Args:\n        cfg (dict):\n        name (str): (default = None)\n        nonlbl_keys (list): (default = INTERNAL_CFGKEYS)\n\n    Returns:\n        str: cfg_lbl\n\n    CommandLine:\n        python -m utool.util_gridsearch get_cfg_lbl\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> cfg = {'_cfgname': 'test', 'var1': 'val1', 'var2': 'val2'}\n        >>> name = None\n        >>> nonlbl_keys = ['_cfgstr', '_cfgname', '_cfgtype', '_cfgindex']\n        >>> cfg_lbl = get_cfg_lbl(cfg, name, nonlbl_keys)\n        >>> result = ('cfg_lbl = %s' % (six.text_type(cfg_lbl),))\n        >>> print(result)\n        cfg_lbl = test:var1=val1,var2=val2\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> cfg = {'var1': 'val1', 'var2': 'val2'}\n        >>> default_cfg = {'var2': 'val1', 'var1': 'val1'}\n        >>> name = None\n        >>> cfg_lbl = get_cfg_lbl(cfg, name, default_cfg=default_cfg)\n        >>> result = ('cfg_lbl = %s' % (six.text_type(cfg_lbl),))\n        >>> print(result)\n        cfg_lbl = :var2=val2\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> cfg = {'_cfgname': 'test:K=[1,2,3]', 'K': '1'}\n        >>> name = None\n        >>> nonlbl_keys = ['_cfgstr', '_cfgname', '_cfgtype', '_cfgindex']\n        >>> cfg_lbl = get_cfg_lbl(cfg, name, nonlbl_keys)\n        >>> result = ('cfg_lbl = %s' % (six.text_type(cfg_lbl),))\n        >>> print(result)\n        cfg_lbl = test:K=1\n    \"\"\"\n    import utool as ut\n    if name is None:\n        name = cfg.get('_cfgname', '')\n\n    if default_cfg is not None:\n        # Remove defaulted labels\n        cfg = ut.partition_varied_cfg_list([cfg], default_cfg)[1][0]\n\n    # remove keys that should not belong to the label\n    _clean_cfg = ut.delete_keys(cfg.copy(), nonlbl_keys)\n    _lbl = ut.repr4(_clean_cfg, explicit=True, nl=False, strvals=True,\n                       key_order=key_order, itemsep=sep)\n    # _search = ['dict(', ')', ' ']\n    _search = ['dict(', ')']\n    _repl = [''] * len(_search)\n    _lbl = ut.multi_replace(_lbl, _search, _repl).rstrip(',')\n    if not with_name:\n        return _lbl\n    if NAMEVARSEP in name:\n        # hack for when name contains a little bit of the _lbl\n        # VERY HACKY TO PARSE OUT PARTS OF THE GIVEN NAME.\n        hacked_name, _cfgstr, _ = parse_cfgstr_name_options(name)\n        _cfgstr_options_list = re.split(\n            r',\\s*' + ut.negative_lookahead(r'[^\\[\\]]*\\]'), _cfgstr)\n        #cfgstr_options_list = cfgopt_strs.split(',')\n        _cfg_options = ut.parse_cfgstr_list(\n            _cfgstr_options_list, smartcast=False, oldmode=False)\n        #\n        ut.delete_keys(_cfg_options, cfg.keys())\n        _preflbl = ut.repr4(_cfg_options, explicit=True, nl=False, strvals=True)\n        _preflbl = ut.multi_replace(_preflbl, _search, _repl).rstrip(',')\n        hacked_name += NAMEVARSEP + _preflbl\n        ###\n        cfg_lbl = hacked_name + _lbl\n    else:\n        cfg_lbl = name + NAMEVARSEP + _lbl\n    return cfg_lbl", "response": "r Returns a short string label for a flat configuration dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_cfgstr3(string, debug=None):\n    import utool as ut  # NOQA\n    import pyparsing as pp\n\n    if debug is None:\n        debug_ = ut.VERBOSE\n    else:\n        debug_ = debug\n\n    def as_tagged(parent, doctag=None, namedItemsOnly=False):\n        \"\"\"Returns the parse results as XML. Tags are created for tokens and lists that have defined results names.\"\"\"\n        namedItems = dict((v[1], k) for (k, vlist) in parent._ParseResults__tokdict.items()\n                          for v in vlist)\n        # collapse out indents if formatting is not desired\n        parentTag = None\n        if doctag is not None:\n            parentTag = doctag\n        else:\n            if parent._ParseResults__name:\n                parentTag = parent._ParseResults__name\n        if not parentTag:\n            if namedItemsOnly:\n                return \"\"\n            else:\n                parentTag = \"ITEM\"\n        out = []\n        for i, res in enumerate(parent._ParseResults__toklist):\n            if isinstance(res, pp.ParseResults):\n                if i in namedItems:\n                    child = as_tagged(\n                        res, namedItems[i], namedItemsOnly and doctag is None)\n                else:\n                    child = as_tagged(\n                        res, None, namedItemsOnly and doctag is None)\n                out.append(child)\n            else:\n                # individual token, see if there is a name for it\n                resTag = None\n                if i in namedItems:\n                    resTag = namedItems[i]\n                if not resTag:\n                    if namedItemsOnly:\n                        continue\n                    else:\n                        resTag = \"ITEM\"\n                child = (resTag, pp._ustr(res))\n                out += [child]\n        return (parentTag, out)\n\n    def combine_nested(opener, closer, content, name=None):\n        \"\"\"\n        opener, closer, content = '(', ')', nest_body\n        \"\"\"\n        import utool as ut  # NOQA\n        ret1 = pp.Forward()\n        _NEST = ut.identity\n        #_NEST = pp.Suppress\n        opener_ = _NEST(opener)\n        closer_ = _NEST(closer)\n        # ret1 <<= pp.Group(opener_ + pp.ZeroOrMore(content) + closer_)\n        ret2 = ret1 << pp.Group(opener_ + pp.ZeroOrMore(content) + closer_)\n        if ret2 is None:\n            ret2 = ret1\n        else:\n            pass\n            #raise AssertionError('Weird pyparsing behavior. Comment this line if encountered. pp.__version__ = %r' % (pp.__version__,))\n        if name is None:\n            ret3 = ret2\n        else:\n            ret3 = ret2.setResultsName(name)\n        assert ret3 is not None, 'cannot have a None return'\n        return ret3\n\n    # Current Best Grammar\n    STRING = (pp.quotedString.copy()).setResultsName('quotedstring')\n    NUM    = pp.Word(pp.nums).setResultsName('num')\n    NAME   = pp.Regex('[a-zA-Z_][a-zA-Z_0-9]*')\n    atom   = (NAME | NUM | STRING).setResultsName('atom')\n    key    = pp.Word(pp.alphanums + '_').setResultsName('key')  # identifier\n\n    nest_body = pp.Forward().setResultsName('nest_body')\n    nestedParens   = combine_nested('(', ')', content=nest_body, name='paren')\n    nestedBrackets = combine_nested('[', ']', content=nest_body, name='brak')\n    nestedCurlies  = combine_nested('{', '}', content=nest_body, name='curl')\n\n    nest_stmt = pp.Combine((nestedParens | nestedBrackets | nestedCurlies).setResultsName('sequence'))\n\n    val = (atom | nest_stmt)\n\n    # Nest body cannot have assignments in it\n    #COMMA = pp.Suppress(',')\n    COMMA = ','\n    nest_body << val + pp.ZeroOrMore(COMMA + val)\n\n    assign = pp.Group(key + pp.Suppress('=') + (val)).setResultsName('assign')\n    #item = (assign | val).setResultsName('item')\n    item = (assign | val)\n    # OMG THIS LINE CAUSES NON-DETERMENISTIC RESULTS IN PYTHON3\n    #item = (assign | val).setResultsName('item')\n\n    # Assignments only allowed at outer level\n    assign_body = item + pp.ZeroOrMore(pp.Suppress(',') + item)\n\n    if len(string) > 0:\n        tokens = assign_body.parseString(string)\n        if debug_:\n            print('string = %r' % (string,))\n            print('tokens List: ' + ut.repr3(tokens.asList()))\n            print('tokens XML: ' + tokens.asXML())\n        parsed_blocks = as_tagged(tokens)[1]\n        if debug_:\n            print('PARSED_BLOCKS = ' + ut.repr3(parsed_blocks, nl=1))\n    else:\n        parsed_blocks = []\n\n    from utool import util_type  # NOQA\n    #from collections import OrderedDict\n    #cfgdict = OrderedDict()\n    cfgdict = {}\n    for item in parsed_blocks:\n        if item[0] == 'assign':\n            keyval_pair = item[1]\n            keytup, valtup = keyval_pair\n            key = keytup[1]\n            val = util_type.smart_cast2(valtup[1])\n            #val = valtup[1]\n        elif item[0] == 'atom':\n            key = item[1]\n            val = True\n        else:\n            key = None\n            val = None\n        if key is not None:\n            cfgdict[key] = val\n    if debug_:\n        print('[TOKENS] CFGDICT ' + ut.repr3(cfgdict, nl=1))\n        #x = list(map(type, cfgdict.keys()))\n        #print(x)\n        #print(list(cfgdict.keys()))\n        #if len(x) > 0 and str(x[0]).find('Word') > -1:\n        #    import utool\n        #    utool.embed()\n    return cfgdict", "response": "r Parses a CF - GSTR string and returns a dictionary of the nested structure that is represented by the nested structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef customize_base_cfg(cfgname, cfgopt_strs, base_cfg, cfgtype,\n                       alias_keys=None, valid_keys=None, offset=0,\n                       strict=True):\n    \"\"\"\n    Args:\n        cfgname (str): config name\n        cfgopt_strs (str): mini-language defining key variations\n        base_cfg (dict): specifies the default cfg to customize\n        cfgtype (?):\n        alias_keys (None): (default = None)\n        valid_keys (None): if base_cfg is not specied, this defines the valid\n            keys (default = None)\n        offset (int): (default = 0)\n        strict (bool): (default = True)\n\n    Returns:\n        list: cfg_combo - list of config dicts defining customized configs\n            based on cfgopt_strs. customized configs always are given an\n            _cfgindex, _cfgstr, and _cfgname key.\n\n    CommandLine:\n        python -m utool.util_gridsearch --test-customize_base_cfg:0\n\n\n    Ignore:\n        >>> cfgname = 'default'\n        >>> cfgopt_strs = 'dsize=1000,per_name=[1,2]'\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> cfgname = 'name'\n        >>> cfgopt_strs = 'b=[1,2]'\n        >>> base_cfg = {}\n        >>> alias_keys = None\n        >>> cfgtype = None\n        >>> offset = 0\n        >>> valid_keys = None\n        >>> strict = False\n        >>> cfg_combo = customize_base_cfg(cfgname, cfgopt_strs, base_cfg, cfgtype,\n        >>>                                alias_keys, valid_keys, offset, strict)\n        >>> result = ('cfg_combo = %s' % (ut.repr2(cfg_combo, nl=1),))\n        >>> print(result)\n        cfg_combo = [\n            {'_cfgindex': 0, '_cfgname': 'name', '_cfgstr': 'name:b=[1,2]', '_cfgtype': None, 'b': 1},\n            {'_cfgindex': 1, '_cfgname': 'name', '_cfgstr': 'name:b=[1,2]', '_cfgtype': None, 'b': 2},\n        ]\n    \"\"\"\n    import utool as ut\n    cfg = base_cfg.copy()\n    # Parse config options without expansion\n    cfg_options = noexpand_parse_cfgstrs(cfgopt_strs, alias_keys)\n    # Ensure that nothing bad is being updated\n    if strict:\n        parsed_keys = cfg_options.keys()\n        if valid_keys is not None:\n            ut.assert_all_in(parsed_keys, valid_keys,\n                             'keys specified not in valid set')\n        else:\n            ut.assert_all_in(parsed_keys, cfg.keys(),\n                             'keys specified not in default options')\n    # Finalize configuration dict\n    cfg.update(cfg_options)\n    cfg['_cfgtype'] = cfgtype\n    cfg['_cfgname'] = cfgname\n    # Perform expansion\n    cfg_combo = ut.all_dict_combinations(cfg)\n    #if len(cfg_combo) > 1:\n    for combox, cfg_ in enumerate(cfg_combo, start=offset):\n        cfg_['_cfgindex'] = combox\n    for cfg_ in cfg_combo:\n        if len(cfgopt_strs) > 0:\n            cfg_['_cfgstr'] = cfg_['_cfgname'] + NAMEVARSEP + cfgopt_strs\n        else:\n            cfg_['_cfgstr'] = cfg_['_cfgname']\n    return cfg_combo", "response": "This function is used to customize the base config for a key variation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_cfgstr_name_options(cfgstr):\n    import utool as ut\n    cfgname_regex = ut.named_field('cfgname', r'[^\\[:]*')  # name is optional\n    subx_regex = r'\\[' + ut.named_field('subx', r'[^\\]]*') + r'\\]'\n    cfgopt_regex = re.escape(NAMEVARSEP) + ut.named_field('cfgopt', '.*')\n    regex_str = cfgname_regex + ('(%s)?' % (subx_regex,)) + ('(%s)?' % (cfgopt_regex,))\n    match = re.match(regex_str, cfgstr)\n    assert match is not None, 'parsing of cfgstr failed'\n    groupdict = match.groupdict()\n    cfgname = groupdict['cfgname']\n    if cfgname == '':\n        cfgname = 'default'\n    cfgopt_strs = groupdict.get('cfgopt', None)\n    subx_str = groupdict.get('subx', None)\n    if cfgopt_strs is None:\n        cfgopt_strs = ''\n    subx = ut.fuzzy_subset(subx_str)\n    return cfgname, cfgopt_strs, subx", "response": "r Parses the name options of a node in a tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grid_search_generator(grid_basis=[], *args, **kwargs):\n    grid_basis_ = grid_basis + list(args) + list(kwargs.items())\n    grid_basis_dict = OrderedDict(grid_basis_)\n    grid_point_iter = util_dict.iter_all_dict_combinations_ordered(grid_basis_dict)\n    for grid_point in grid_point_iter:\n        yield grid_point", "response": "r This function will iterate over individual configuration points in a defined basis set and print out the results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconstraining configurations and removes duplicates", "response": "def constrain_cfgdict_list(cfgdict_list_, constraint_func):\n    \"\"\" constrains configurations and removes duplicates \"\"\"\n    cfgdict_list = []\n    for cfg_ in cfgdict_list_:\n        cfg = cfg_.copy()\n        if constraint_func(cfg) is not False and len(cfg) > 0:\n            if cfg not in cfgdict_list:\n                cfgdict_list.append(cfg)\n    return cfgdict_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_cfglbls(cfgdict_list, varied_dict):\n    import textwrap\n    wrapper = textwrap.TextWrapper(width=50)\n    cfglbl_list =  []\n    for cfgdict_ in cfgdict_list:\n        cfgdict = cfgdict_.copy()\n        for key in six.iterkeys(cfgdict_):\n            try:\n                vals = varied_dict[key]\n                # Dont print label if not varied\n                if len(vals) == 1:\n                    del cfgdict[key]\n                else:\n                    # Dont print label if it is None (irrelevant)\n                    if cfgdict[key] is None:\n                        del cfgdict[key]\n            except KeyError:\n                # Don't print keys not in varydict\n                del cfgdict[key]\n        cfglbl = six.text_type(cfgdict)\n        search_repl_list = [('\\'', ''), ('}', ''),\n                            ('{', ''), (': ', '=')]\n        for search, repl in search_repl_list:\n            cfglbl = cfglbl.replace(search, repl)\n        #cfglbl = str(cfgdict).replace('\\'', '').replace('}', '').replace('{', '').replace(': ', '=')\n        cfglbl = ('\\n'.join(wrapper.wrap(cfglbl)))\n        cfglbl_list.append(cfglbl)\n    return cfglbl_list", "response": "Create a list of text in labels that mater from the cfgdict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning to interact with gridsearch results", "response": "def interact_gridsearch_result_images(show_result_func, cfgdict_list,\n                                      cfglbl_list, cfgresult_list,\n                                      score_list=None, fnum=None, figtitle='',\n                                      unpack=False, max_plots=25, verbose=True,\n                                      precision=3, scorelbl='score',\n                                      onclick_func=None):\n    \"\"\" helper function for visualizing results of gridsearch \"\"\"\n    assert callable(show_result_func), 'NEED FUNCTION GOT: %r' % (show_result_func,)\n\n    import utool as ut\n    import plottool as pt\n    from plottool import plot_helpers as ph\n    from plottool import interact_helpers as ih\n    if verbose:\n        print('Plotting gridsearch results figtitle=%r' % (figtitle,))\n    if score_list is None:\n        score_list = [None] * len(cfgdict_list)\n    else:\n        # sort by score if available\n        sortx_list = ut.list_argsort(score_list, reverse=True)\n        score_list = ut.take(score_list, sortx_list)\n        cfgdict_list = ut.take(cfgdict_list, sortx_list)\n        cfglbl_list = ut.take(cfglbl_list, sortx_list)\n        cfgresult_list = ut.take(cfgresult_list, sortx_list)\n    # Dont show too many results only the top few\n    score_list = ut.listclip(score_list, max_plots)\n\n    # Show the config results\n    fig = pt.figure(fnum=fnum)\n    # Get plots for each of the resutls\n    nRows, nCols = pt.get_square_row_cols(len(score_list), fix=True)\n    next_pnum = pt.make_pnum_nextgen(nRows, nCols)\n    for cfgdict, cfglbl, cfgresult, score in zip(cfgdict_list, cfglbl_list,\n                                                  cfgresult_list,\n                                                  score_list):\n        if score is not None:\n            cfglbl += '\\n' + scorelbl + '=' + ut.repr2(score, precision=precision)\n        pnum = next_pnum()\n        try:\n            if unpack:\n                show_result_func(*cfgresult, fnum=fnum, pnum=pnum)\n            else:\n                show_result_func(cfgresult, fnum=fnum, pnum=pnum)\n        except Exception as ex:\n            if isinstance(cfgresult, tuple):\n                #print(ut.repr4(cfgresult))\n                print(ut.depth_profile(cfgresult))\n                print(ut.list_type_profile(cfgresult))\n            ut.printex(ex, 'error showing', keys=['cfgresult', 'fnum', 'pnum'])\n            raise\n        #pt.imshow(255 * cfgresult, fnum=fnum, pnum=next_pnum(), title=cfglbl)\n        ax = pt.gca()\n        pt.set_title(cfglbl, ax=ax)  # , size)\n        ph.set_plotdat(ax, 'cfgdict', cfgdict)\n        ph.set_plotdat(ax, 'cfglbl', cfglbl)\n        ph.set_plotdat(ax, 'cfgresult', cfgresult)\n    # Define clicked callback\n    def on_clicked(event):\n        print('\\n[pt] clicked gridsearch axes')\n        if event is None or event.xdata is None or event.inaxes is None:\n            print('out of axes')\n            pass\n        else:\n            ax = event.inaxes\n            plotdat_dict = ph.get_plotdat_dict(ax)\n            print(ut.repr4(plotdat_dict))\n            cfglbl = ph.get_plotdat(ax, 'cfglbl', None)\n            cfgdict = ph.get_plotdat(ax, 'cfgdict', {})\n            cfgresult = ph.get_plotdat(ax, 'cfgresult', {})\n            infostr_list = [\n                ('cfglbl = %s' % (cfglbl,)),\n                '',\n                ('cfgdict = ' + ut.repr4(cfgdict, sorted_=True)),\n            ]\n            # Call a user defined function if given\n            if onclick_func is not None:\n                if unpack:\n                    onclick_func(*cfgresult)\n                else:\n                    onclick_func(cfgresult)\n            infostr = ut.msgblock('CLICKED', '\\n'.join(infostr_list))\n            print(infostr)\n    # Connect callbacks\n    ih.connect_callback(fig, 'button_press_event', on_clicked)\n    pt.set_figtitle(figtitle)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntime a series of functions on a series of inputs args_list is a list should vary the input sizes can also be a func that take a count param items in args_list list or returned by the func should be a tuple so it can be unpacked CommandLine: python -m ibeis.annotmatch_funcs --exec-get_annotmatch_rowids_from_aid2 --show python -m ibeis.annotmatch_funcs --exec-get_annotmatch_rowids_from_aid:1 --show Args: func_list (list): args_list (list): niters (None): (default = None) Returns: dict: time_result CommandLine: python -m utool.util_gridsearch --exec-gridsearch_timer --show Example: >>> # DISABLE_DOCTEST >>> from utool.util_gridsearch import * # NOQA >>> import utool as ut >>> func_list = [ut.fibonacci_recursive, ut.fibonacci_iterative] >>> args_list = list(range(1, 35)) >>> niters = None >>> searchkw = {} >>> time_result = gridsearch_timer(func_list, args_list, niters, **searchkw) >>> result = ('time_result = %s' % (six.text_type(time_result),)) >>> print(result) >>> time_result['plot_timings']() >>> ut.show_if_requested()", "response": "def gridsearch_timer(func_list, args_list, niters=None, **searchkw):\n    \"\"\"\n    Times a series of functions on a series of inputs\n\n    args_list is a list should vary the input sizes\n    can also be a func that take a count param\n\n    items in args_list list or returned by the func should be a tuple so it can be\n    unpacked\n\n    CommandLine:\n        python -m ibeis.annotmatch_funcs --exec-get_annotmatch_rowids_from_aid2 --show\n        python -m ibeis.annotmatch_funcs --exec-get_annotmatch_rowids_from_aid:1 --show\n\n    Args:\n        func_list (list):\n        args_list (list):\n        niters (None): (default = None)\n\n    Returns:\n        dict: time_result\n\n    CommandLine:\n        python -m utool.util_gridsearch --exec-gridsearch_timer --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_gridsearch import *  # NOQA\n        >>> import utool as ut\n        >>> func_list = [ut.fibonacci_recursive, ut.fibonacci_iterative]\n        >>> args_list = list(range(1, 35))\n        >>> niters = None\n        >>> searchkw = {}\n        >>> time_result = gridsearch_timer(func_list, args_list, niters, **searchkw)\n        >>> result = ('time_result = %s' % (six.text_type(time_result),))\n        >>> print(result)\n        >>> time_result['plot_timings']()\n        >>> ut.show_if_requested()\n    \"\"\"\n    import utool as ut\n    timings = ut.ddict(list)\n    if niters is None:\n        niters = len(args_list)\n\n    if ut.is_funclike(args_list):\n        get_args = args_list\n    else:\n        get_args = args_list.__getitem__\n\n    #func_labels = searchkw.get('func_labels', list(range(len(func_list))))\n    func_labels = searchkw.get('func_labels', [ut.get_funcname(func) for func in func_list])\n    use_cache = searchkw.get('use_cache', not ut.get_argflag(('--nocache', '--nocache-time')))\n    assert_eq = searchkw.get('assert_eq', True)\n\n    count_list = list(range(niters))\n    xlabel_list = []\n\n    cache = ut.ShelfCacher('timeings.shelf', enabled=use_cache)\n\n    for count in ut.ProgressIter(count_list, lbl='Testing Timeings'):\n        args_ = get_args(count)\n        xlabel_list.append(args_)\n        if True:\n            # HACK\n            # There is an unhandled corner case that will fail if the function expects a tuple.\n            if not isinstance(args_, tuple):\n                args_ = (args_,)\n        assert isinstance(args_, tuple), 'args_ should be a tuple so it can be unpacked'\n        ret_list = []\n        for func_ in func_list:\n            try:\n                kwargs_ = {}\n                func_cachekey = ut.get_func_result_cachekey(func_, args_, kwargs_)\n                ellapsed = cache.load(func_cachekey)\n            except ut.CacheMissException:\n                with ut.Timer(verbose=False) as t:\n                    ret = func_(*args_)\n                    ret_list.append(ret)\n                ellapsed = t.ellapsed\n                cache.save(func_cachekey, ellapsed)\n            timings[func_].append(ellapsed)\n        if assert_eq:\n            # Hacky, not guarenteed to work if cache is one\n            ut.assert_all_eq(list(map(ut.cachestr_repr, ret_list)))\n\n    cache.close()\n\n    count_to_xtick = searchkw.get('count_to_xtick', lambda x, y: x)\n    xtick_list = [count_to_xtick(count, get_args(count)) for count in count_list]\n\n    def plot_timings():\n        import plottool as pt\n        ydata_list = ut.dict_take(timings, func_list)\n        xdata = xtick_list\n\n        ylabel = 'seconds'\n        xlabel = 'input size'\n\n        pt.multi_plot(\n            xdata, ydata_list, label_list=func_labels,\n            ylabel=ylabel, xlabel=xlabel,\n            **searchkw\n        )\n    time_result = {\n        'plot_timings': plot_timings,\n        'timings': timings,\n    }\n    return time_result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mapping(version=1, exported_at=None, app_name=None):\n    if exported_at is None:\n        exported_at = timezone.now()\n    app_name = app_name or settings.HEROKU_CONNECT_APP_NAME\n    return {\n        'version': version,\n        'connection': {\n            'organization_id': settings.HEROKU_CONNECT_ORGANIZATION_ID,\n            'app_name': app_name,\n            'exported_at': exported_at.isoformat(),\n        },\n        'mappings': [\n            model.get_heroku_connect_mapping()\n            for model in get_heroku_connect_models()\n        ]\n    }", "response": "Returns a Heroku Connect mapping for the entire project."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_heroku_connect_models():\n    from django.apps import apps\n    apps.check_models_ready()\n    from heroku_connect.db.models import HerokuConnectModel\n\n    return (\n        model\n        for models in apps.all_models.values()\n        for model in models.values()\n        if issubclass(model, HerokuConnectModel)\n        and not model._meta.managed\n    )", "response": "Returns all registered Heroku Connect Models."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a connected model for a given table name.", "response": "def get_connected_model_for_table_name(table_name):\n    \"\"\"Return a connected model's table name (which read and written to by Heroku Connect).\"\"\"\n    for connected_model in get_heroku_connect_models():\n        if connected_model.get_heroku_connect_table_name() == table_name:\n            return connected_model\n    raise LookupError('No connected model found for table %r' % (table_name,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate Heroku Connect schema.", "response": "def create_heroku_connect_schema(using=DEFAULT_DB_ALIAS):\n    \"\"\"\n    Create Heroku Connect schema.\n\n    Note:\n        This function is only meant to be used for local development.\n        In a production environment the schema will be created by\n        Heroku Connect.\n\n    Args:\n        using (str): Alias for database connection.\n\n    Returns:\n        bool: ``True`` if the schema was created, ``False`` if the\n            schema already exists.\n\n    \"\"\"\n    connection = connections[using]\n\n    with connection.cursor() as cursor:\n        cursor.execute(_SCHEMA_EXISTS_QUERY, [settings.HEROKU_CONNECT_SCHEMA])\n        schema_exists = cursor.fetchone()[0]\n        if schema_exists:\n            return False\n\n        cursor.execute(\"CREATE SCHEMA %s;\", [AsIs(settings.HEROKU_CONNECT_SCHEMA)])\n\n    with connection.schema_editor() as editor:\n        for model in get_heroku_connect_models():\n            editor.create_model(model)\n\n        # Needs PostgreSQL and database superuser privileges (which is the case on Heroku):\n        editor.execute('CREATE EXTENSION IF NOT EXISTS \"hstore\";')\n\n        from heroku_connect.models import (TriggerLog, TriggerLogArchive)\n        for cls in [TriggerLog, TriggerLogArchive]:\n            editor.create_model(cls)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_connections(app):\n    payload = {'app': app}\n    url = os.path.join(settings.HEROKU_CONNECT_API_ENDPOINT, 'connections')\n    response = requests.get(url, params=payload, headers=_get_authorization_headers())\n    response.raise_for_status()\n    return response.json()['results']", "response": "Returns all Heroku Connect connections setup with the given application."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_connection(connection_id, deep=False):\n    url = os.path.join(settings.HEROKU_CONNECT_API_ENDPOINT, 'connections', connection_id)\n    payload = {'deep': deep}\n    response = requests.get(url, params=payload, headers=_get_authorization_headers())\n    response.raise_for_status()\n    return response.json()", "response": "Get Heroku Connection information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport Heroku Connect mapping for given connection.", "response": "def import_mapping(connection_id, mapping):\n    \"\"\"\n    Import Heroku Connection mapping for given connection.\n\n    Args:\n        connection_id (str): Heroku Connection connection ID.\n        mapping (dict): Heroku Connect mapping.\n\n    Raises:\n        requests.HTTPError: If an error occurs uploading the mapping.\n        ValueError: If the mapping is not JSON serializable.\n\n    \"\"\"\n    url = os.path.join(settings.HEROKU_CONNECT_API_ENDPOINT,\n                       'connections', connection_id, 'actions', 'import')\n\n    response = requests.post(\n        url=url,\n        json=mapping,\n        headers=_get_authorization_headers()\n    )\n    response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlinks the connection to your Heroku user account.", "response": "def link_connection_to_account(app):\n    \"\"\"\n    Link the connection to your Heroku user account.\n\n    https://devcenter.heroku.com/articles/heroku-connect-api#step-3-link-the-connection-to-your-heroku-user-account\n    \"\"\"\n    url = os.path.join(settings.HEROKU_CONNECT_API_ENDPOINT, 'users', 'me', 'apps', app, 'auth')\n    response = requests.post(\n        url=url,\n        headers=_get_authorization_headers()\n    )\n    response.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches a base element for subelement by name then takes the cvParams of that subelement and returns the values as a list for the paramnames that match.", "response": "def fetch_cvparams_values_from_subel(base, subelname, paramnames, ns):\n    \"\"\"Searches a base element for subelement by name, then takes the\n    cvParams of that subelement and returns the values as a list\n    for the paramnames that match. Value order in list equals input\n    paramnames order.\"\"\"\n    sub_el = basereader.find_element_xpath(base, subelname, ns)\n    cvparams = get_all_cvparams(sub_el, ns)\n    output = []\n    for param in paramnames:\n        output.append(fetch_cvparam_value_by_name(cvparams, param))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the issue s body in plain text.", "response": "def format_body(self, description, sys_info=None, traceback=None):\n        \"\"\"\n        Formats the body in plain text. (add a series of '-' under each section\n            title).\n\n        :param description: Description of the issue, written by the user.\n        :param sys_info: Optional system information string\n        :param log: Optional application log\n        :param traceback: Optional traceback.\n        \"\"\"\n        name = 'Description'\n        delim = '-' * 40\n        body = BODY_ITEM_TEMPLATE % {\n            'name': name, 'value': description, 'delim': delim\n        }\n        if traceback:\n            name = 'Traceback'\n            traceback = '\\n'.join(traceback.splitlines()[-NB_LINES_MAX:])\n            body += BODY_ITEM_TEMPLATE % {\n                'name': name, 'value': traceback, 'delim': delim\n            }\n        if sys_info:\n            name = 'System information'\n            body += BODY_ITEM_TEMPLATE % {\n                'name': name, 'value': sys_info, 'delim': delim\n            }\n        return body"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_tables(self, tables):\n        cursor = self.get_cursor()\n        for table in tables:\n            columns = mslookup_tables[table]\n            try:\n                cursor.execute('CREATE TABLE {0}({1})'.format(\n                    table, ', '.join(columns)))\n            except sqlite3.OperationalError as error:\n                print(error)\n                print('Warning: Table {} already exists in database, will '\n                      'add to existing tables instead of creating '\n                      'new.'.format(table))\n            else:\n                self.conn.commit()", "response": "Creates database tables in sqlite lookup db"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(self, fn):\n        self.conn = sqlite3.connect(fn)\n        cur = self.get_cursor()\n        cur.execute('PRAGMA page_size=4096')\n        cur.execute('PRAGMA FOREIGN_KEYS=ON')\n        cur.execute('PRAGMA cache_size=10000')\n        cur.execute('PRAGMA journal_mode=MEMORY')", "response": "SQLite connect method initialize db"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef index_column(self, index_name, table, column):\n        cursor = self.get_cursor()\n        try:\n            cursor.execute(\n                'CREATE INDEX {0} on {1}({2})'.format(index_name, table, column))\n        except sqlite3.OperationalError as error:\n            print(error)\n            print('Skipping index creation and assuming it exists already')\n        else:\n            self.conn.commit()", "response": "Called by interfaces to index specific column in table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sql_select(self, columns, table, distinct=False):\n        sql = 'SELECT {0} {1} FROM {2}'\n        dist = {True: 'DISTINCT', False: ''}[distinct]\n        return sql.format(dist, ', '.join(columns), table)", "response": "Creates and returns an SQL SELECT statement"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes SQL and returns cursor for it", "response": "def execute_sql(self, sql):\n        \"\"\"Executes SQL and returns cursor for it\"\"\"\n        cursor = self.get_cursor()\n        cursor.execute(sql)\n        return cursor"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn dict of mzmlfilenames and their db ids", "response": "def get_mzmlfile_map(self):\n        \"\"\"Returns dict of mzmlfilenames and their db ids\"\"\"\n        cursor = self.get_cursor()\n        cursor.execute('SELECT mzmlfile_id, mzmlfilename FROM mzmlfiles')\n        return {fn: fnid for fnid, fn in cursor.fetchall()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_spectra_id(self, fn_id, retention_time=None, scan_nr=None):\n        cursor = self.get_cursor()\n        sql = 'SELECT spectra_id FROM mzml WHERE mzmlfile_id=? '\n        values = [fn_id]\n        if retention_time is not None:\n            sql = '{0} AND retention_time=?'.format(sql)\n            values.append(retention_time)\n        if scan_nr is not None:\n            sql = '{0} AND scan_nr=?'.format(sql)\n            values.append(scan_nr)\n        cursor.execute(sql, tuple(values))\n        return cursor.fetchone()[0]", "response": "Returns the spectra id for the given file ID and retention time"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmonkeys patch to pandas to highlight the maximum value in specified columns", "response": "def to_string_monkey(df, highlight_cols=None, latex=False):\n    \"\"\"  monkey patch to pandas to highlight the maximum value in specified\n    cols of a row\n\n    Example:\n        >>> from utool.experimental.pandas_highlight import *\n        >>> import pandas as pd\n        >>> df = pd.DataFrame(\n        >>>     np.array([[ 0.9,         0.86886931,  0.86842073,  0.9       ],\n        >>>               [ 0.34196218,  0.34289191,  0.34206377,  0.34252863],\n        >>>               [ 0.34827074,  0.34827074,  0.34827074,  0.34827074],\n        >>>               [ 0.76979453,  0.77214855,  0.77547518,  0.38850962]]),\n        >>>     columns=['sum(fgweights)', 'sum(weighted_ratio)', 'len(matches)', 'score_lnbnn_1vM'],\n        >>>     index=['match_state(match-v-rest)', 'match_state(nomatch-v-rest)', 'match_state(notcomp-v-rest)', 'photobomb_state']\n        >>> )\n        >>> highlight_cols = 'all'\n        >>> print(to_string_monkey(df, highlight_cols))\n        >>> print(to_string_monkey(df, highlight_cols, latex=True))\n\n    ut.editfile(pd.io.formats.printing.adjoin)\n    \"\"\"\n    try:\n        import pandas as pd\n        import utool as ut\n        import numpy as np\n        import six\n        if isinstance(highlight_cols, six.string_types) and highlight_cols == 'all':\n            highlight_cols = np.arange(len(df.columns))\n        # kwds = dict(buf=None, columns=None, col_space=None, header=True,\n        #             index=True, na_rep='NaN', formatters=None,\n        #             float_format=None, sparsify=None, index_names=True,\n        #             justify=None, line_width=None, max_rows=None,\n        #             max_cols=None, show_dimensions=False)\n        # self = pd.formats.format.DataFrameFormatter(df, **kwds)\n        try:\n            self = pd.formats.format.DataFrameFormatter(df)\n        except AttributeError:\n            self = pd.io.formats.format.DataFrameFormatter(df)\n\n        self.highlight_cols = highlight_cols\n\n        def monkey(self):\n            return monkey_to_str_columns(self, latex=latex)\n\n        ut.inject_func_as_method(self, monkey, '_to_str_columns', override=True, force=True)\n\n        def strip_ansi(text):\n            import re\n            ansi_escape = re.compile(r'\\x1b[^m]*m')\n            return ansi_escape.sub('', text)\n\n        def justify_ansi(self, texts, max_len, mode='right'):\n            if mode == 'left':\n                return [x.ljust(max_len + (len(x) - len(strip_ansi(x)))) for x in texts]\n            elif mode == 'center':\n                return [x.center(max_len + (len(x) - len(strip_ansi(x)))) for x in texts]\n            else:\n                return [x.rjust(max_len + (len(x) - len(strip_ansi(x)))) for x in texts]\n        ut.inject_func_as_method(self.adj, justify_ansi, 'justify', override=True, force=True)\n\n        def strlen_ansii(self, text):\n            return pd.compat.strlen(strip_ansi(text), encoding=self.encoding)\n        ut.inject_func_as_method(self.adj, strlen_ansii, 'len', override=True, force=True)\n\n        if False:\n            strlen = ut.partial(strlen_ansii, self.adj)  # NOQA\n            justfunc = ut.partial(justify_ansi, self.adj)  # NOQA\n            # Essentially what to_string does\n            strcols = monkey_to_str_columns(self)\n            # texts = strcols[2]\n            space = 1\n            lists = strcols\n            str_ = self.adj.adjoin(space, *lists)\n            print(str_)\n            print(strip_ansi(str_))\n        self.to_string()\n        result = self.buf.getvalue()\n        # hack because adjoin is not working correctly with injected strlen\n        result = '\\n'.join([x.rstrip() for x in result.split('\\n')])\n        return result\n    except Exception as ex:\n        ut.printex('pandas monkey-patch is broken: {}'.format(str(ex)),\n                   tb=True, iswarning=True)\n        return str(df)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating given schema from pythonic syntax to a validator.", "response": "def translate(value):\n    \"\"\"\n    Translates given schema from \"pythonic\" syntax to a validator.\n\n    Usage::\n\n        >>> translate(str)\n        IsA(str)\n\n        >>> translate('hello')\n        IsA(str, default='hello')\n\n    \"\"\"\n    if isinstance(value, BaseValidator):\n        return value\n\n    if value is None:\n        return Anything()\n\n    if isinstance(value, type):\n        return IsA(value)\n\n    if type(value) in compat.func_types:\n        real_value = value()\n        return IsA(type(real_value), default=real_value)\n\n    if isinstance(value, list):\n        if value == []:\n            # no inner spec, just an empty list as the default value\n            return IsA(list)\n        elif len(value) == 1:\n            # the only item as spec for each item of the collection\n            return ListOf(translate(value[0]))\n        else:\n            raise StructureSpecificationError(\n                'Expected a list containing exactly 1 item; '\n                'got {cnt}: {spec}'.format(cnt=len(value), spec=value))\n\n    if isinstance(value, dict):\n        if not value:\n            return IsA(dict)\n        items = []\n        for k, v in value.items():\n            if isinstance(k, BaseValidator):\n                k_validator = k\n            else:\n                k_validator = translate(k)\n                default = k_validator.get_default_for(None)\n                if default is not None:\n                    k_validator = Equals(default)\n            v_validator = translate(v)\n            items.append((k_validator, v_validator))\n        return DictOf(items)\n\n    return IsA(type(value), default=value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _merge(self, value):\n        if not value:\n            return []\n\n        if value is not None and not isinstance(value, list):\n            # bogus value; will not pass validation but should be preserved\n            return value\n\n        item_spec = self._nested_validator\n        return [x if x is None else item_spec.get_default_for(x) for x in value]", "response": "Merge the items in value into a list based on self. _nested_validator. get_default_for."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge(self, value):\n\n        if value is not None and not isinstance(value, dict):\n            # bogus value; will not pass validation but should be preserved\n            return value\n\n        if not self._pairs:\n            return {}\n\n        collected = {}\n#        collected.update(value)\n        for k_validator, v_validator in self._pairs:\n            k_default = k_validator.get_default_for(None)\n            if k_default is None:\n                continue\n\n            # even None is ok\n            if value:\n                v_for_this_k = value.get(k_default)\n            else:\n                v_for_this_k = None\n            v_default = v_validator.get_default_for(v_for_this_k)\n            collected.update({k_default: v_default})\n\n        if value:\n            for k, v in value.items():\n                if k not in collected:\n                    collected[k] = v\n\n        return collected", "response": "Merge the value with each value recursively"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_code(code):\n    \"Handle a key or sequence of keys in braces\"\n\n    code_keys = []\n    # it is a known code (e.g. {DOWN}, {ENTER}, etc)\n    if code in CODES:\n        code_keys.append(VirtualKeyAction(CODES[code]))\n\n    # it is an escaped modifier e.g. {%}, {^}, {+}\n    elif len(code) == 1:\n        code_keys.append(KeyAction(code))\n\n    # it is a repetition or a pause  {DOWN 5}, {PAUSE 1.3}\n    elif ' ' in code:\n        to_repeat, count = code.rsplit(None, 1)\n        if to_repeat == \"PAUSE\":\n            try:\n                pause_time = float(count)\n            except ValueError:\n                raise KeySequenceError('invalid pause time %s'% count)\n            code_keys.append(PauseAction(pause_time))\n\n        else:\n            try:\n                count = int(count)\n            except ValueError:\n                raise KeySequenceError(\n                    'invalid repetition count %s'% count)\n\n            # If the value in to_repeat is a VK e.g. DOWN\n            # we need to add the code repeated\n            if to_repeat in CODES:\n                code_keys.extend(\n                    [VirtualKeyAction(CODES[to_repeat])] * count)\n            # otherwise parse the keys and we get back a KeyAction\n            else:\n                to_repeat = parse_keys(to_repeat)\n                if isinstance(to_repeat, list):\n                    keys = to_repeat * count\n                else:\n                    keys = [to_repeat] * count\n                code_keys.extend(keys)\n    else:\n        raise RuntimeError(\"Unknown code: %s\"% code)\n\n    return code_keys", "response": "Handle a key or sequence of keys in braces"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_keys(string,\n                with_spaces = False,\n                with_tabs = False,\n                with_newlines = False,\n                modifiers = None):\n    \"Return the parsed keys\"\n\n    keys = []\n    if not modifiers:\n        modifiers = []\n    index = 0\n    while index < len(string):\n\n        c = string[index]\n        index += 1\n        # check if one of CTRL, SHIFT, ALT has been pressed\n        if c in MODIFIERS.keys():\n            modifier = MODIFIERS[c]\n            # remember that we are currently modified\n            modifiers.append(modifier)\n            # hold down the modifier key\n            keys.append(VirtualKeyAction(modifier, up = False))\n            if DEBUG:\n                print(\"MODS+\", modifiers)\n            continue\n\n        # Apply modifiers over a bunch of characters (not just one!)\n        elif c == \"(\":\n            # find the end of the bracketed text\n            end_pos = string.find(\")\", index)\n            if end_pos == -1:\n                raise KeySequenceError('`)` not found')\n            keys.extend(\n                parse_keys(string[index:end_pos], modifiers = modifiers))\n            index = end_pos + 1\n\n        # Escape or named key\n        elif c == \"{\":\n            # We start searching from index + 1 to account for the case {}}\n            end_pos = string.find(\"}\", index + 1)\n            if end_pos == -1:\n                raise KeySequenceError('`}` not found')\n\n            code = string[index:end_pos]\n            index = end_pos + 1\n            keys.extend(handle_code(code))\n\n        # unmatched \")\"\n        elif c == ')':\n            raise KeySequenceError('`)` should be preceeded by `(`')\n\n        # unmatched \"}\"\n        elif c == '}':\n            raise KeySequenceError('`}` should be preceeded by `{`')\n\n        # so it is a normal character\n        else:\n            # don't output white space unless flags to output have been set\n            if (c == ' ' and not with_spaces or\n                c == '\\t' and not with_tabs or\n                c == '\\n' and not with_newlines):\n                continue\n\n            # output nuewline\n            if c in ('~', '\\n'):\n                keys.append(VirtualKeyAction(CODES[\"ENTER\"]))\n\n            # safest are the virtual keys - so if our key is a virtual key\n            # use a VirtualKeyAction\n            #if ord(c) in CODE_NAMES:\n            #    keys.append(VirtualKeyAction(ord(c)))\n\n            elif modifiers:\n                keys.append(EscapedKeyAction(c))\n\n            else:\n                keys.append(KeyAction(c))\n\n        # as we have handled the text - release the modifiers\n        while modifiers:\n            if DEBUG:\n                print(\"MODS-\", modifiers)\n            keys.append(VirtualKeyAction(modifiers.pop(), down = False))\n\n    # just in case there were any modifiers left pressed - release them\n    while modifiers:\n        keys.append(VirtualKeyAction(modifiers.pop(), down = False))\n\n    return keys", "response": "Return the parsed keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the keys and type them", "response": "def SendKeys(keys,\n             pause=0.05,\n             with_spaces=False,\n             with_tabs=False,\n             with_newlines=False,\n             turn_off_numlock=True):\n    \"Parse the keys and type them\"\n    keys = parse_keys(keys, with_spaces, with_tabs, with_newlines)\n\n    for k in keys:\n        k.Run()\n        time.sleep(pause)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    \"Send some test strings\"\n\n    actions = \"\"\"\n        {LWIN}\n        {PAUSE .25}\n        r\n        {PAUSE .25}\n        Notepad.exe{ENTER}\n        {PAUSE 1}\n        Hello{SPACE}World!\n        {PAUSE 1}\n        %{F4}\n        {PAUSE .25}\n        n\n        \"\"\"\n    SendKeys(actions, pause = .1)\n\n    keys = parse_keys(actions)\n    for k in keys:\n        print(k)\n        k.Run()\n        time.sleep(.1)\n\n    test_strings = [\n        \"\\n\"\n        \"(aa)some text\\n\",\n        \"(a)some{ }text\\n\",\n        \"(b)some{{}text\\n\",\n        \"(c)some{+}text\\n\",\n        \"(d)so%me{ab 4}text\",\n        \"(e)so%me{LEFT 4}text\",\n        \"(f)so%me{ENTER 4}text\",\n        \"(g)so%me{^aa 4}text\",\n        \"(h)some +(asdf)text\",\n        \"(i)some %^+(asdf)text\",\n        \"(j)some %^+a text+\",\n        \"(k)some %^+a tex+{&}\",\n        \"(l)some %^+a tex+(dsf)\",\n        \"\",\n        ]\n\n    for s in test_strings:\n        print(repr(s))\n        keys = parse_keys(s, with_newlines = True)\n        print(keys)\n\n        for k in keys:\n            k.Run()\n            time.sleep(.1)\n        print()", "response": "Send some test strings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetInput(self):\n        \"Build the INPUT structure for the action\"\n        actions = 1\n        # if both up and down\n        if self.up and self.down:\n            actions = 2\n\n        inputs = (INPUT * actions)()\n\n        vk, scan, flags = self._get_key_info()\n\n        for inp in inputs:\n            inp.type = INPUT_KEYBOARD\n\n            inp._.ki.wVk = vk\n            inp._.ki.wScan = scan\n            inp._.ki.dwFlags |= flags\n\n        # if we are releasing - then let it up\n        if self.up:\n            inputs[-1]._.ki.dwFlags |= KEYEVENTF_KEYUP\n\n        return inputs", "response": "Build the INPUT structure for the action"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string that will show whether the string is up or down", "response": "def _get_down_up_string(self):\n        \"\"\"Return a string that will show whether the string is up or down\n\n        return 'down' if the key is a press only\n        return 'up' if the key is up only\n        return '' if the key is up & down (as default)\n        \"\"\"\n        down_up = \"\"\n        if not (self.down and self.up):\n            if self.down:\n                down_up = \"down\"\n            elif self.up:\n                down_up = \"up\"\n        return down_up"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a description of the key", "response": "def key_description(self):\n        \"Return a description of the key\"\n        vk, scan, flags = self._get_key_info()\n        desc = ''\n        if vk:\n            if vk in CODE_NAMES:\n                desc = CODE_NAMES[vk]\n            else:\n                desc = \"VK %d\"% vk\n        else:\n            desc = \"%s\"% self.key\n\n        return desc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_key_info(self):\n        \"Virtual keys have extended flag set\"\n\n        # copied more or less verbatim from\n        # http://www.pinvoke.net/default.aspx/user32.sendinput\n        if (\n            (self.key >= 33 and self.key <= 46) or\n            (self.key >= 91 and self.key <= 93) ):\n            flags = KEYEVENTF_EXTENDEDKEY;\n        else:\n            flags = 0\n        # This works for %{F4} - ALT + F4\n        #return self.key, 0, 0\n\n        # this works for Tic Tac Toe i.e. +{RIGHT} SHIFT + RIGHT\n        return self.key, MapVirtualKey(self.key, 0), flags", "response": "Virtual keys have extended flag set"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the vk and scan code for this key.", "response": "def _get_key_info(self):\n        \"\"\"EscapedKeyAction doesn't send it as Unicode and the vk and\n        scan code are generated differently\"\"\"\n        vkey_scan = LoByte(VkKeyScan(self.key))\n\n        return (vkey_scan, MapVirtualKey(vkey_scan, 0), 0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup(self):\n        self.template = self._generate_inline_policy()\n        if self.dry_run is not True:\n            self.client = self._get_client()\n            username = self._get_username_for_key()\n            policy_document = self._generate_inline_policy()\n            self._attach_inline_policy(username, policy_document)\n            pass", "response": "Method runs the plugin attaching policies to the user in question"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning all the policy names for a given user", "response": "def _get_policies(self):\n        \"\"\"Returns all the policy names for a given user\"\"\"\n        username = self._get_username_for_key()\n        policies = self.client.list_user_policies(\n            UserName=username\n        )\n        return policies"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_username_for_key(self):\n        response = self.client.get_access_key_last_used(\n            AccessKeyId=self.compromised_resource['access_key_id']\n        )\n        username = response['UserName']\n        return username", "response": "Find the user for a given access key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender a policy from a jinja template", "response": "def _generate_inline_policy(self):\n        \"\"\"Renders a policy from a jinja template\"\"\"\n        template_name = self._locate_file('deny-sts-before-time.json.j2')\n        template_file = open(template_name)\n        template_contents = template_file.read()\n        template_file.close()\n        jinja_template = Template(template_contents)\n        policy_document = jinja_template.render(\n            before_date=self._get_date()\n        )\n        return policy_document"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _attach_inline_policy(self, username, policy_document):\n        response = self.client.put_user_policy(\n            UserName=username,\n            PolicyName=\"threatresponse-temporal-key-revocation\",\n            PolicyDocument=policy_document\n        )\n        logger.info(\n            'An inline policy has been attached for'\n            ' {u} revoking sts tokens.'.format(u=username)\n        )\n        return response", "response": "Attaches the inline policy to the user"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _locate_file(self, pattern, root=os.path.dirname('revokests_key.py')):\n\n        for path, dirs, files in os.walk(os.path.abspath(root)):\n            for filename in fnmatch.filter(files, pattern):\n                return os.path.join(path, filename)", "response": "Locate all files matching pattern in and below\n\n        supplied root directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate tuples of spectra filename and corresponding output features from kronik", "response": "def mzmlfn_kronikfeature_generator(mzmlfns, kronikfns):\n    \"\"\"Generates tuples of spectra filename and corresponding output\n    features from kronik\"\"\"\n    for mzmlfn, kronikfn in zip(mzmlfns, kronikfns):\n        for quant_el in generate_kronik_feats(kronikfn):\n            yield os.path.basename(mzmlfn), quant_el"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_split_tsv_lines(fn, header):\n    for line in generate_tsv_psms_line(fn):\n        yield {x: y for (x, y) in zip(header, line.strip().split('\\t'))}", "response": "Returns a generator that yields a list of dicts with header - keys and psm statistic values"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_proteins_from_psm(line):\n    proteins = line[mzidtsvdata.HEADER_PROTEIN].split(';')\n    outproteins = []\n    for protein in proteins:\n        prepost_protein = re.sub('\\(pre=.*post=.*\\)', '', protein).strip()\n        outproteins.append(prepost_protein)\n    return outproteins", "response": "From a line return list of proteins reported by Mzid2TSV. When unrolled\n    lines are given this returns the single protein from the line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_psm(line, unroll=False, specfncol=None):\n    if specfncol is None:\n        specfncol = mzidtsvdata.HEADER_SPECFILE\n    specfn = line[specfncol]\n    psm_id = get_psm_id(line)\n    scan = line[mzidtsvdata.HEADER_SCANNR]\n    peptideseq = get_psm_sequence(line, unroll)\n    score = line[mzidtsvdata.HEADER_MSGFSCORE]\n    return specfn, psm_id, scan, peptideseq, score", "response": "Returns from a PSM line and other information about the PSM."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns from a PSM line peptide sequence and other information about the PSM.", "response": "def get_pepproteins(line):\n    \"\"\"Returns from a PSM line peptide sequence,\n    and other information about the PSM.\n    Return values:\n        psm_id          -   str\n        proteins        -   list of str\n    \"\"\"\n    psm_id = get_psm_id(line)\n    proteins = get_proteins_from_psm(line)\n    return psm_id, proteins"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naugmenting sys. argv with the command line arguments.", "response": "def aug_sysargv(cmdstr):\n    \"\"\" DEBUG FUNC modify argv to look like you ran a command \"\"\"\n    import shlex\n    argv = shlex.split(cmdstr)\n    sys.argv.extend(argv)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_module_verbosity_flags(*labels):\n    verbose_prefix_list = ['--verbose-', '--verb', '--verb-']\n    veryverbose_prefix_list = ['--veryverbose-', '--veryverb', '--veryverb-']\n    verbose_flags = tuple(\n        [prefix + lbl for prefix, lbl in\n         itertools.product(verbose_prefix_list, labels)])\n    veryverbose_flags = tuple(\n        [prefix + lbl for prefix, lbl in\n         itertools.product(veryverbose_prefix_list, labels)])\n    veryverbose_module = get_argflag(veryverbose_flags) or VERYVERBOSE\n    verbose_module = (get_argflag(verbose_flags) or veryverbose_module or VERBOSE)\n    if veryverbose_module:\n        verbose_module = 2\n    return verbose_module, veryverbose_module", "response": "checks for standard flags for enableing module specific verbosity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autogen_argparse_block(extra_args=[]):\n    #import utool as ut  # NOQA\n    #__REGISTERED_ARGS__\n    # TODO FINISHME\n\n    grouped_args = []\n    # Group similar a args\n    for argtup in __REGISTERED_ARGS__:\n        argstr_list, type_, default, help_ = argtup\n        argstr_set = set(argstr_list)\n        # <MULTIKEY_SETATTR>\n        # hack in multikey setattr n**2 yuck\n        found = False\n        for index, (keyset, vals) in enumerate(grouped_args):\n            if len(keyset.intersection(argstr_set)) > 0:\n                # update\n                keyset.update(argstr_set)\n                vals.append(argtup)\n                found = True\n                break\n        if not found:\n            new_keyset = argstr_set\n            new_vals = [argtup]\n            grouped_args.append((new_keyset, new_vals))\n        # </MULTIKEY_SETATTR>\n    # DEBUG\n    multi_groups = []\n    for keyset, vals in grouped_args:\n        if len(vals) > 1:\n            multi_groups.append(vals)\n    if len(multi_groups) > 0:\n        import utool as ut\n        print('Following arg was specified multiple times')\n        print(ut.repr4(multi_groups, newlines=2))", "response": "A function that automatically sets up the autocleaned - argument - set for each entry in the config file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the value of a flag or a corresponding noflag", "response": "def get_argflag(argstr_, default=False, help_='', return_specified=None,\n                need_prefix=True, return_was_specified=False, argv=None,\n                debug=None,\n                **kwargs):\n    \"\"\"\n    Checks if the commandline has a flag or a corresponding noflag\n\n    Args:\n        argstr_ (str, list, or tuple): the flag to look for\n        default (bool): dont use this (default = False)\n        help_ (str): a help string (default = '')\n        return_specified (bool): returns if flag was specified or not (default = False)\n\n    Returns:\n        tuple: (parsed_val, was_specified)\n\n    TODO:\n        depricate return_was_specified\n\n    CommandLine:\n        python -m utool.util_arg --exec-get_argflag --noface --exec-mode\n        python -m utool.util_arg --exec-get_argflag --foo --exec-mode\n        python -m utool.util_arg --exec-get_argflag --no-foo --exec-mode\n        python -m utool.util_arg --exec-get_argflag --foo=True --exec-mode\n        python -m utool.util_arg --exec-get_argflag --foo=False  --exec-mode\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_arg import *  # NOQA\n        >>> argstr_ = '--foo'\n        >>> default = False\n        >>> help_ = ''\n        >>> return_specified = True\n        >>> (parsed_val, was_specified) = get_argflag(argstr_, default, help_, return_specified)\n        >>> result = ('(parsed_val, was_specified) = %s' % (str((parsed_val, was_specified)),))\n        >>> print(result)\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n    assert isinstance(default, bool), 'default must be boolean'\n    argstr_list = meta_util_iter.ensure_iterable(argstr_)\n    #if VERYVERBOSE:\n    #    print('[util_arg] checking argstr_list=%r' % (argstr_list,))\n    # arg registration\n    _register_arg(argstr_list, bool, default, help_)\n    parsed_val = default\n    was_specified = False\n\n    if debug is None:\n        debug = DEBUG\n\n    # Check environment variables for default as well as argv\n    import os\n    #\"\"\"\n    #set UTOOL_NOCNN=True\n    #export UTOOL_NOCNN True\n    #\"\"\"\n    #argv_orig = argv[:]\n    # HACK: make this not happen very time you loop\n    for key, val in os.environ.items():\n        key = key.upper()\n        sentinal = 'UTOOL_'\n        if key.startswith(sentinal):\n            flag = '--' + key[len(sentinal):].lower().replace('_', '-')\n            if val.upper() in ['TRUE', 'ON']:\n                pass\n            elif val.upper() in ['FALSE', 'OFF']:\n                continue\n            else:\n                continue\n                #flag += '=False'\n            new_argv = [flag]\n            argv = argv[:] + new_argv\n            if debug:\n                print('ENV SPECIFIED COMMAND LINE')\n                print('argv.extend(new_argv=%r)' % (new_argv,))\n\n    for argstr in argstr_list:\n        #if VERYVERBOSE:\n        #    print('[util_arg]   * checking argstr=%r' % (argstr,))\n        if not (argstr.find('--') == 0 or (argstr.find('-') == 0 and len(argstr) == 2)):\n            raise AssertionError('Invalid argstr: %r' % (argstr,))\n        if not need_prefix:\n            noprefix = argstr.replace('--', '')\n            if noprefix in argv:\n                parsed_val = True\n                was_specified = True\n                break\n        #if argstr.find('--no') == 0:\n            #argstr = argstr.replace('--no', '--')\n        noarg = argstr.replace('--', '--no')\n        if argstr in argv:\n            parsed_val = True\n            was_specified = True\n            #if VERYVERBOSE:\n            #    print('[util_arg]   * ...WAS_SPECIFIED. AND PARSED')\n            break\n        elif noarg in argv:\n            parsed_val = False\n            was_specified = True\n            #if VERYVERBOSE:\n            #    print('[util_arg]   * ...WAS_SPECIFIED. AND NOT PARSED')\n            break\n        elif argstr + '=True' in argv:\n            parsed_val = True\n            was_specified = True\n            break\n        elif argstr + '=False' in argv:\n            parsed_val = False\n            was_specified = True\n            break\n\n    if return_specified is None:\n        return_specified = return_was_specified\n\n    if return_specified:\n        return parsed_val, was_specified\n    else:\n        return parsed_val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_argval(argstr_, type_=None, default=None, help_=None, smartcast=True,\n               return_specified=None, argv=None, verbose=None,\n               debug=None, return_was_specified=False, pos=None):\n    r\"\"\"\n    Returns a value of an argument specified on the command line after some flag\n\n    Args:\n        argstr_ (str or tuple): string or tuple of strings denoting the command line values to parse\n        type_ (None): type of the variable to parse (default = None)\n        default (None): (default = None)\n        help_ (None): help for this argument (not fully integrated) (default = None)\n        smartcast (bool): tries to be smart about casting the parsed strings (default = True)\n        return_specified (bool): (default = False)\n        argv (None): override sys.argv with custom command line vector (default = None)\n        pos (int): if specified the argument can also be found in position `pos` of the command line varargs\n\n    TODO:\n        depricate return_was_specified\n\n    CommandLine:\n        python -m utool.util_arg --test-get_argval\n        python -m utool.util_arg --exec-get_argval:0\n        python -m utool.util_arg --exec-get_argval:1\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval('--quest')]])\" --quest=\"holy grail\"\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval('--quest')]])\" --quest=\"42\"\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval('--quest')]])\" --quest=42\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval('--quest')]])\" --quest 42\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval('--quest', float)]])\" --quest 42\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval(('--nAssign'), int)]])\" --nAssign 42\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval(('--test'), str)]])\" --test\n        python -c \"import utool; print([(type(x), x) for x in [utool.get_argval(('--test'), str)]])\" --test \"foobar is good\" --youbar ok\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_arg import *  # NOQA\n        >>> import utool as ut\n        >>> import sys\n        >>> argv = ['--spam', 'eggs', '--quest=holy grail', '--ans=42', '--the-val=1,2,3']\n        >>> # specify a list of args and kwargs to get_argval\n        >>> argstr_kwargs_list = [\n        >>>     ('--spam',                    dict(type_=str, default=None, argv=argv)),\n        >>>     ('--quest',                   dict(type_=str, default=None, argv=argv)),\n        >>>     (('--ans', '--foo'),          dict(type_=int, default=None, argv=argv)),\n        >>>     (('--not-there', '--absent'), dict(argv=argv)),\n        >>>     ('--the_val',                 dict(type_=list, argv=argv)),\n        >>>     ('--the-val',                 dict(type_=list, argv=argv)),\n        >>> ]\n        >>> # Execute the command with for each of the test cases\n        >>> res_list = []\n        >>> argstr_list = ut.get_list_column(argstr_kwargs_list, 0)\n        >>> for argstr_, kwargs in argstr_kwargs_list:\n        >>>     res = get_argval(argstr_, **kwargs)\n        >>>     res_list.append(res)\n        >>> result = ut.repr2(ut.odict(zip(argstr_list, res_list)), nl=1)\n        >>> result = result.replace('u\\'', '\\'')  # hack\n        >>> print(result)\n        {\n            '--spam': 'eggs',\n            '--quest': 'holy grail',\n            ('--ans', '--foo'): 42,\n            ('--not-there', '--absent'): None,\n            '--the_val': [1, 2, 3],\n            '--the-val': [1, 2, 3],\n        }\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_arg import *  # NOQA\n        >>> import utool as ut\n        >>> import sys\n        >>> argv = ['--slice1', '::', '--slice2=4:', '--slice3=::4', '--slice4', '[1,2,3,4]', '--slice5=3']\n        >>> # specify a list of args and kwargs to get_argval\n        >>> argstr_kwargs_list = [\n        >>>     ('--slice1',            dict(type_='fuzzy_subset', default=None, argv=argv)),\n        >>>     ('--slice2',            dict(type_='fuzzy_subset', default=None, argv=argv)),\n        >>>     ('--slice3',            dict(type_='fuzzy_subset', default=None, argv=argv)),\n        >>>     ('--slice4',            dict(type_='fuzzy_subset', default=None, argv=argv)),\n        >>>     ('--slice5',            dict(type_='fuzzy_subset', default=None, argv=argv)),\n        >>> ]\n        >>> # Execute the command with for each of the test cases\n        >>> res_list = []\n        >>> argstr_list = ut.get_list_column(argstr_kwargs_list, 0)\n        >>> list1 = [1, 3, 5, 7, 9]\n        >>> import numpy as np\n        >>> list2 = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 1]])\n        >>> for argstr_, kwargs in argstr_kwargs_list:\n        >>>     res = get_argval(argstr_, **kwargs)\n        >>>     print('---')\n        >>>     print('res = %r' % (res,))\n        >>>     print('list1[%r=%r] = %r' % (argstr_, res, ut.take(list1, res),))\n        >>>     print('list2[%r=%r] = %r' % (argstr_, res, list2[res].tolist(),))\n        >>>     res_list.append(res)\n        >>> result = ut.repr4(ut.odict(zip(argstr_list, res_list)))\n        >>> result = result.replace('u\\'', '\\'')  # hack\n        >>> print(result)\n\n    \"\"\"\n    if verbose is None:\n        pass\n        # verbose = VERYVERBOSE\n\n    if debug is None:\n        debug = DEBUG\n        # debug = VERYVERBOSE\n\n    if argv is None:\n        argv = sys.argv\n\n    #verbose = 1\n\n    if verbose:\n        print('[get_argval] Searching Commandline for argstr_=%r' % (argstr_,))\n        #print('[get_argval]  * type_ = %r' % (type_,))\n        #print('[get_argval]  * default = %r' % (default,))\n        #print('[get_argval]  * help_ = %r' % (help_,))\n        #print('[get_argval]  * smartcast = %r' % (smartcast,))\n\n    if return_specified is None:\n        return_specified = return_was_specified\n\n    #print(argstr_)\n    was_specified = False\n    arg_after = default\n    if type_ is bool:\n        arg_after = False if default is None else default\n    try:\n        # New for loop way (accounts for =)\n        argstr_list = meta_util_iter.ensure_iterable(argstr_)\n        # arg registration\n        _register_arg(argstr_list, type_, default, help_)\n\n        # expand out hypens\n        EXPAND_HYPENS = True\n        if EXPAND_HYPENS:\n            argstr_list2 = []\n            seen_ = set([])\n            for argstr in argstr_list:\n                if argstr not in seen_:\n                    argstr_list2.append(argstr)\n                    seen_.add(argstr)\n                if argstr.startswith('--'):\n                    num = 2\n                elif argstr.startswith('-'):\n                    num = 1\n                else:\n                    continue\n                argstr2_0 = argstr[0:num] + argstr[num:].replace('_', '-')\n                argstr2_1 = argstr[0:num] + argstr[num:].replace('-', '_')\n                if argstr2_0 not  in seen_:\n                    argstr_list2.append(argstr2_0)\n                    seen_.add(argstr2_0)\n                if argstr2_1 not  in seen_:\n                    argstr_list2.append(argstr2_1)\n                    seen_.add(argstr2_1)\n            argstr_list = argstr_list2\n\n        # Check environment variables for default as well as argv\n        import os\n        \"\"\"\n        set UTOOL_NOCNN=True\n        export UTOOL_NOCNN True\n        \"\"\"\n        #argv_orig = argv[:]\n        for key, val in os.environ.items():\n            key = key.upper()\n            sentinal = 'UTOOL_'\n            if key.startswith(sentinal):\n                key = '--' + key[len(sentinal):].lower()\n                new_argv = [key, val]\n                if val.upper() in ['TRUE', 'FALSE', 'ON', 'OFF']:\n                    # handled by get_argflag\n                    continue\n                argv = argv[:] + new_argv\n                if debug:\n                    print('argv.extend(new_argv=%r)' % (new_argv,))\n\n        for argx, item in enumerate(argv):\n            for argstr in argstr_list:\n                if item == argstr:\n                    if type_ is bool:\n                        if debug:\n                            print('[get_argval] ... argstr=%r' % (argstr,))\n                            print('[get_argval] ... Found bool argx=%r' % (argx,))\n                        arg_after = True\n                        was_specified = True\n                        break\n                    if argx < len(argv):\n                        if type_ is list:\n                            # HACK FOR LIST. TODO INTEGRATE\n                            if debug:\n                                print('[get_argval] ... argstr=%r' % (argstr,))\n                                print('[get_argval] ... Found noequal list argx=%r' % (argx,))\n                            arg_after = parse_arglist_hack(argx, argv=argv)\n                            if debug:\n                                print('[get_argval] ... arg_after=%r' % (arg_after,))\n                                print('argv=%r' % (argv,))\n                            if smartcast:\n                                arg_after = list(map(util_type.smart_cast2, arg_after))\n                                if debug:\n                                    print('[get_argval] ... smartcast arg_after=%r' % (arg_after,))\n                        else:\n                            if debug:\n                                print('[get_argval] ... argstr=%r' % (argstr,))\n                                print('[get_argval] ... Found type_=%r argx=%r' % (type_, argx,))\n                            arg_after = argv[argx + 1]\n                            if type_ is not None:\n                                arg_after = util_type.try_cast(arg_after, type_)\n                            elif smartcast:\n                                arg_after = util_type.smart_cast2(arg_after)\n                        if was_specified:\n                            print('WARNING: argstr=%r already specified' % (argstr,))\n                        was_specified = True\n                        break\n                elif item.startswith(argstr + '='):\n                    val_after = ''.join(item.split('=')[1:])\n                    if type_ is list:\n                        # HACK FOR LIST. TODO INTEGRATE\n                        if verbose:\n                            print('[get_argval] ... Found equal list')\n                        val_after_ = val_after.rstrip(']').lstrip('[')\n                        if True:\n                            # Hacker way to be less hacky about parsing lists\n                            from utool import util_gridsearch\n                            blocks = util_gridsearch.parse_nestings(val_after_)\n                            sentinal = '##COM&&'\n                            changed = [(block[0], block[1].replace(',', sentinal))\n                                       if block[0] == 'nonNested' else block\n                                       for block in blocks]\n                            val_after2 = util_gridsearch.recombine_nestings(changed)\n                            arg_after = val_after2.split(sentinal)\n                        else:\n                            arg_after = val_after_.split(',')\n                        if smartcast:\n                            arg_after = list(map(util_type.smart_cast2, arg_after))\n                    else:\n                        if type_ is None:\n                            if smartcast:\n                                arg_after = util_type.smart_cast2(val_after)\n                            else:\n                                arg_after = val_after\n                        else:\n                            arg_after = util_type.try_cast(val_after, type_)\n                            if not isinstance(type_, six.string_types) and issubclass(type_, six.string_types):\n                                if arg_after == 'None':\n                                    # hack\n                                    arg_after = None\n                    if was_specified:\n                        print('WARNING: argstr=%r already specified' % (argstr,))\n                    was_specified = True\n                    break\n    except Exception as ex:\n        import utool as ut\n        ut.printex(ex, 'problem in arg_val', keys=['type_'])\n        if ut.SUPER_STRICT:\n            raise\n        pass\n    if not was_specified and pos is not None:\n        varargs = get_cmdline_varargs(argv)\n        if len(varargs) > pos:\n            arg_after = varargs[pos]\n            assert type_ is not list, 'list not handled yet'\n            if type_ is not None:\n                arg_after = util_type.try_cast(arg_after, type_)\n            elif smartcast:\n                arg_after = util_type.smart_cast2(arg_after)\n\n    if verbose:\n        print('[get_argval] ... Parsed arg_after=%r, was_specified=%r' % (arg_after, was_specified))\n    if return_specified:\n        return arg_after, was_specified\n    else:\n        return arg_after", "response": "r Returns a value of an argument specified on the command line after some flag"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cfgstr_list(cfgstr_list, smartcast=True, oldmode=True):\n    cfgdict = {}\n    for item in cfgstr_list:\n        if item == '':\n            continue\n        if oldmode:\n            keyval_tup = item.replace('=', ':').split(':')\n            assert len(keyval_tup) == 2, '[!] Invalid cfgitem=%r' % (item,)\n            key, val = keyval_tup\n        else:\n            keyval_tup = item.split('=')\n            if len(keyval_tup) == 1:\n                # single specifications are interpeted as booleans\n                key = keyval_tup[0]\n                val = True\n            else:\n                assert len(keyval_tup) >= 2, '[!] Invalid cfgitem=%r' % (item,)\n                key, val = keyval_tup[0], '='.join(keyval_tup[1:])\n        if smartcast:\n            val = util_type.smart_cast2(val)\n        cfgdict[key] = val\n    return cfgdict", "response": "r Parses a list of items in the format var1 = val1 var2 = val2 var3 = val3 var4 = val1 var5 = val2 var6 = val1 var4 = val1 var5 = val2 var6 = val3 var6 = val1 var4 = val1 var5 = val2 var6 = val3"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_arg_dict(argv=None, prefix_list=['--'], type_hints={}):\n    if argv is None:\n        argv = sys.argv\n    arg_dict = {}\n\n    def startswith_prefix(arg):\n        return any([arg.startswith(prefix) for prefix in prefix_list])\n\n    def argx_has_value(argv, argx):\n        # Check if has a value\n        if argv[argx].find('=') > -1:\n            return True\n        if argx + 1 < len(argv) and not startswith_prefix(argv[argx + 1]):\n            return True\n        return False\n\n    def get_arg_value(argv, argx, argname):\n        if argv[argx].find('=') > -1:\n            return '='.join(argv[argx].split('=')[1:])\n        else:\n            type_ = type_hints.get(argname, None)\n            if type_ is None:\n                return argv[argx + 1]\n            else:\n                return parse_arglist_hack(argx, argv=argv)\n\n    for argx in range(len(argv)):\n        arg = argv[argx]\n        for prefix in prefix_list:\n            if arg.startswith(prefix):\n                argname = arg[len(prefix):]\n                if argx_has_value(argv, argx):\n                    if arg.find('=') > -1:\n                        argname = arg[len(prefix):arg.find('=')]\n                    argvalue = get_arg_value(argv, argx, argname)\n                    arg_dict[argname] = argvalue\n                else:\n                    arg_dict[argname] = True\n                break\n    return arg_dict", "response": "r Get the dictionary of arguments for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef autogen_argparse2(dpath_list):\n    import utool as ut\n    import parse\n    include_patterns = ['*.py']\n    regex_list = ['get_argflag', 'get_argval']\n    recursive = True\n    result = ut.grep(regex_list, recursive, dpath_list, include_patterns, verbose=True)\n    (found_filestr_list, found_lines_list, found_lxs_list) = result\n    # TODO: Check to see if in a comment block\n    flagtups_list = []\n    for found_lines in found_lines_list:\n        flagtups = []\n        for line in found_lines:\n            line_ = ut.regex_replace('#.*', '', line)\n\n            argval_parse_list = [\n                '\\'{flag}\\' in sys.argv',\n                'get_argval({flagtup}, type={type}, default={default})',\n                'get_argval({flagtup}, {type}, default={default})',\n                'get_argval({flagtup}, {type}, {default})',\n                'get_argval({flagtup})',\n            ]\n            argflag_parse_list = [\n                'get_argflag({flagtup})',\n            ]\n            def parse_pattern_list(parse_list, line):\n                #result_list = []\n                result = None\n                for pattern in parse_list:\n                    result = parse.parse('{_prefix}' + pattern, line_)\n                    if result is not None:\n                        break\n                        #if len(result_list) > 1:\n                        #    print('warning')\n                        #result_list.append(result)\n                return result\n            val_result  = parse_pattern_list(argval_parse_list, line)\n            flag_result = parse_pattern_list(argflag_parse_list, line)\n            if flag_result is None and val_result is None:\n                print('warning1')\n            elif flag_result is not None and val_result is not None:\n                print('warning2')\n            else:\n                result = flag_result if val_result is None else val_result\n                flagtups.append(result['flagtup'])\n        flagtups_list.append(flagtups)\n    return flagtups_list", "response": "r Automatically creates a new object of the same class and all of its attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef argv_flag_dec(*argin, **kwargs):\n    kwargs = kwargs.copy()\n    kwargs['default'] = kwargs.get('default', False)\n    from utool import util_decor\n    @util_decor.ignores_exc_tb(outer_wrapper=False)\n    def wrap_argv_flag_dec(func):\n        return __argv_flag_dec(func, **kwargs)\n\n    assert len(argin) < 2, 'specify 0 or 1 args'\n\n    if len(argin) == 1 and util_type.is_funclike(argin[0]):\n        func = argin[0]\n        return wrap_argv_flag_dec(func)\n    else:\n        return wrap_argv_flag_dec", "response": "Decorator for sys. argv\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __argv_flag_dec(func, default=False, quiet=QUIET, indent=False):\n    from utool import util_decor\n    flagname = meta_util_six.get_funcname(func)\n    if flagname.find('no') == 0:\n        flagname = flagname[2:]\n\n    flags = (\n        '--' + flagname.replace('_', '-'),\n        '--' + flagname,\n    )\n\n    @util_decor.ignores_exc_tb(outer_wrapper=False)\n    def GaurdWrapper(*args, **kwargs):\n        from utool import util_print\n        # FIXME: the --print-all is a hack\n        default_ = kwargs.pop('default', default)\n        alias_flags = kwargs.pop('alias_flags', [])\n        is_flagged = (get_argflag(flags, default_) or\n                      get_argflag('--print-all') or\n                      any([get_argflag(_) for _ in alias_flags]))\n        if flagname in kwargs:\n            is_flagged = kwargs.pop(flagname)\n        if is_flagged:\n            func_label = flags[0].replace('--', '').replace('print-', '')\n            # print('')\n            print('\\n+ --- ' + func_label + ' ___')\n            use_indent = indent is not False\n            if indent is True:\n                indent_ = '[%s]' % func_label\n            else:\n                indent_ = indent\n            with util_print.Indenter(indent_, enabled=use_indent):\n                ret = func(*args, **kwargs)\n            print('L ___ ' + func_label + '___\\n')\n            return ret\n        else:\n            PRINT_DISABLED_FLAGDEC = not get_argflag(\n                '--noinform', help_='does not print disabled flag decorators')\n            if not quiet and PRINT_DISABLED_FLAGDEC:\n                #print('\\n~~~ %s ~~~' % flag)\n                print('~~~ %s ~~~' % flags[0])\n    meta_util_six.set_funcname(GaurdWrapper, meta_util_six.get_funcname(func))\n    return GaurdWrapper", "response": "Decorator for controlling if a function gets called based on command line flags"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef argparse_dict(default_dict_, lbl=None, verbose=None,\n                  only_specified=False, force_keys={}, type_hint=None,\n                  alias_dict={}):\n    r\"\"\"\n    Gets values for a dict based on the command line\n\n    Args:\n        default_dict_ (?):\n        only_specified (bool): if True only returns keys that are specified on commandline. no defaults.\n\n    Returns:\n        dict_: dict_ -  a dictionary\n\n    CommandLine:\n        python -m utool.util_arg --test-argparse_dict\n        python -m utool.util_arg --test-argparse_dict --foo=3\n        python -m utool.util_arg --test-argparse_dict --flag1\n        python -m utool.util_arg --test-argparse_dict --flag2\n        python -m utool.util_arg --test-argparse_dict --noflag2\n        python -m utool.util_arg --test-argparse_dict --thresh=43\n        python -m utool.util_arg --test-argparse_dict --bins=-10\n        python -m utool.util_arg --test-argparse_dict --bins=-10 --only-specified --helpx\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_arg import *  # NOQA\n        >>> import utool as ut\n        >>> # build test data\n        >>> default_dict_ = {\n        ...    'bins': 8,\n        ...    'foo': None,\n        ...    'flag1': False,\n        ...    'flag2': True,\n        ...    'max': 0.2,\n        ...    'neg': -5,\n        ...    'thresh': -5.333,\n        ... }\n        >>> # execute function\n        >>> only_specified = ut.get_argflag('--only-specified')\n        >>> dict_ = argparse_dict(default_dict_, only_specified=only_specified)\n        >>> # verify results\n        >>> result = ut.repr4(dict_, sorted_=True)\n        >>> print(result)\n    \"\"\"\n    if verbose is None:\n        verbose = VERBOSE_ARGPARSE\n    def make_argstrs(key, prefix_list):\n        for prefix in prefix_list:\n            yield prefix + key\n            yield prefix + key.replace('-', '_')\n            yield prefix + key.replace('_', '-')\n\n    def get_dictkey_cmdline_val(key, default, type_hint):\n        # see if the user gave a commandline value for this dict key\n        defaulttype_ = None if default is None else type(default)\n        if type_hint is None:\n            type_ = defaulttype_\n        elif isinstance(type_hint, dict):\n            type_ = type_hint.get(key, defaulttype_)\n        elif isinstance(type_hint, type):\n            type_ = type_hint\n        else:\n            raise NotImplementedError('Unknown type of type_hint=%r' % (type_hint,))\n        was_specified = False\n        if isinstance(default, bool):\n            val = default\n            if default is True:\n                falsekeys = list(set(make_argstrs(key, ['--no', '--no-'])))\n                notval, was_specified = get_argflag(falsekeys, return_specified=True)\n                val = not notval\n                if not was_specified:\n                    truekeys = list(set(make_argstrs(key, ['--'])))\n                    val_, was_specified = get_argflag(truekeys, return_specified=True)\n                    if was_specified:\n                        val = val_\n            elif default is False:\n                truekeys = list(set(make_argstrs(key, ['--'])))\n                val, was_specified = get_argflag(truekeys, return_specified=True)\n        else:\n            argtup = list(set(make_argstrs(key, ['--'])))\n            #if key == 'species':\n            #    import utool as ut\n            #    ut.embed()\n            val, was_specified = get_argval(argtup, type_=type_,\n                                            default=default,\n                                            return_specified=True)\n        return val, was_specified\n\n    dict_  = {}\n    num_specified = 0\n    for key, default in six.iteritems(default_dict_):\n        val, was_specified = get_dictkey_cmdline_val(key, default, type_hint)\n        if not was_specified:\n            alias_keys = meta_util_iter.ensure_iterable(alias_dict.get(key, []))\n            for alias_key in alias_keys:\n                val, was_specified = get_dictkey_cmdline_val(alias_key, default,\n                                                             type_hint)\n                if was_specified:\n                    break\n        if VERBOSE_ARGPARSE:\n            if was_specified:\n                num_specified += 1\n                print('[argparse_dict] Specified key=%r, val=%r' % (key, val))\n        #if key == 'foo':\n        #    import utool as ut\n        #    ut.embed()\n        if not only_specified or was_specified or key in force_keys:\n            dict_[key] = val\n    if VERBOSE_ARGPARSE:\n        print('[argparse_dict] num_specified = %r' % (num_specified,))\n        print('[argparse_dict] force_keys = %r' % (force_keys,))\n    #dict_ = {key: get_dictkey_cmdline_val(key, default) for key, default in\n    #six.iteritems(default_dict_)}\n\n    if verbose:\n        for key in dict_:\n            if dict_[key] != default_dict_[key]:\n                print('[argparse_dict] GOT ARGUMENT: cfgdict[%r] = %r' % (key, dict_[key]))\n\n    do_helpx = get_argflag('--helpx',\n                           help_='Specifies that argparse_dict should print help and quit')\n\n    if get_argflag(('--help', '--help2')) or do_helpx:\n        import utool as ut\n        print('COMMAND LINE IS ACCEPTING THESE PARAMS WITH DEFAULTS:')\n        if lbl is not None:\n            print(lbl)\n        #print(ut.align(ut.repr4(dict_, sorted_=True), ':'))\n        print(ut.align(ut.repr4(default_dict_, sorted_=True), ':'))\n        if do_helpx:\n            sys.exit(1)\n    return dict_", "response": "r Runs argparse s dict command line"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning positional args specified directly after the scriptname and before any args starting with - on the commandline.", "response": "def get_cmdline_varargs(argv=None):\n    \"\"\"\n    Returns positional args specified directly after the scriptname\n    and before any args starting with '-' on the commandline.\n    \"\"\"\n    if argv is None:\n        argv = sys.argv\n    scriptname = argv[0]\n    if scriptname == '':\n        # python invoked by iteself\n        pos_start = 0\n        pos_end = 0\n    else:\n        pos_start = pos_end = 1\n        for idx in range(pos_start, len(argv)):\n            if argv[idx].startswith('-'):\n                pos_end = idx\n                break\n        else:\n            pos_end = len(argv)\n    cmdline_varargs = argv[pos_start:pos_end]\n    return cmdline_varargs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naliasing for get_argval Example: >>> # ENABLE_DOCTEST >>> import utool as ut >>> import sys >>> argv = ['--aids=[1,2,3]'] >>> value = ut.argval('--aids', default=[1, 2], argv=argv) >>> assert isinstance(value, list) >>> value2 = ut.argval('--aids', smartcast=False, argv=argv) >>> assert isinstance(value2, str) >>> value2 = ut.argval('--aids', smartcast=True, argv=argv) >>> assert isinstance(value2, list)", "response": "def argval(key, default=None, type=None, smartcast=True, return_exists=False,\n           argv=None):\n    \"\"\"\n    alias for get_argval\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> import utool as ut\n        >>> import sys\n        >>> argv = ['--aids=[1,2,3]']\n        >>> value = ut.argval('--aids', default=[1, 2], argv=argv)\n        >>> assert isinstance(value, list)\n        >>> value2 = ut.argval('--aids', smartcast=False, argv=argv)\n        >>> assert isinstance(value2, str)\n        >>> value2 = ut.argval('--aids', smartcast=True, argv=argv)\n        >>> assert isinstance(value2, list)\n    \"\"\"\n    defaultable_types = (tuple, list, int, float)\n    if type is None and isinstance(default, defaultable_types):\n        type = builtins.type(default)\n    return get_argval(key, type_=type, default=default,\n                      return_was_specified=return_exists, smartcast=smartcast,\n                      argv=argv)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef compose_functions(*func_list):\n    def apply_composition(f, g):\n        def compose(x):\n            return f(g(x))\n        return compose\n    composed_func = functools.reduce(apply_composition, func_list)\n    return composed_func", "response": "Compose a list of functions into one function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that the given string is unicode.", "response": "def ensure_unicode(str_):\n    \"\"\"\n    TODO:\n        rob gp \"isinstance\\\\(.*\\\\\\\\bstr\\\\\\\\b\\\\)\"\n    \"\"\"\n    if isinstance(str_, __STR__):\n        return str_\n    else:\n        try:\n            return __STR__(str_)\n        except UnicodeDecodeError:\n            if str_.startswith(codecs.BOM_UTF8):\n                # Can safely remove the utf8 marker\n                # http://stackoverflow.com/questions/12561063/python-extract-data-from-file\n                str_ = str_[len(codecs.BOM_UTF8):]\n            return str_.decode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_real_feature(df, feature_name, bins=50, figsize=(15, 15)):\n\n    ix_negative_target = df[df.target == 0].index\n    ix_positive_target = df[df.target == 1].index\n\n    plt.figure(figsize=figsize)\n\n    ax_overall_dist = plt.subplot2grid((3, 2), (0, 0), colspan=2)\n    ax_target_conditional_dist = plt.subplot2grid((3, 2), (1, 0), colspan=2)\n\n    ax_botplot = plt.subplot2grid((3, 2), (2, 0))\n    ax_violin_plot = plt.subplot2grid((3, 2), (2, 1))\n\n    ax_overall_dist.set_title('Distribution of {}'.format(feature_name), fontsize=16)\n    sns.distplot(\n        df[feature_name],\n        bins=50,\n        ax=ax_overall_dist\n    )\n\n    sns.distplot(\n        df.loc[ix_positive_target][feature_name],\n        bins=bins,\n        ax=ax_target_conditional_dist,\n        label='Positive Target'\n    )\n    sns.distplot(\n        df.loc[ix_negative_target][feature_name],\n        bins=bins,\n        ax=ax_target_conditional_dist,\n        label='Negative Target'\n    )\n    ax_target_conditional_dist.legend(loc='upper right', prop={'size': 14})\n\n    sns.boxplot(\n        y=feature_name,\n        x='target',\n        data=df,\n        ax=ax_botplot\n    )\n    sns.violinplot(\n        y=feature_name,\n        x='target',\n        data=df,\n        ax=ax_violin_plot\n    )\n\n    plt.show()", "response": "Plot the distribution of a real - valued feature conditioned by the target column."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_pair(df, feature_name_1, feature_name_2, kind='scatter', alpha=0.01, **kwargs):\n\n    plt.figure()\n    sns.jointplot(\n        feature_name_1,\n        feature_name_2,\n        df,\n        alpha=alpha,\n        kind=kind,\n        **kwargs\n    )\n    plt.show()", "response": "Plot a scatterplot of two features against one another."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a correlation heatmap between each feature pair.", "response": "def plot_feature_correlation_heatmap(df, features, font_size=9, figsize=(15, 15), save_filename=None):\n    \"\"\"\n    Plot a correlation heatmap between every feature pair.\n\n    Args:\n        df: Pandas dataframe containing the target column (named 'target').\n        features: The list of features to include in the correlation plot.\n        font_size: Font size for heatmap cells and axis labels.\n        figsize: The size of the plot.\n        save_filename: (Optional) The path of the file to save a high-res version of the plot to.\n    \"\"\"\n\n    features = features[:]\n    features += ['target']\n\n    mcorr = df[features].corr()\n    mask = np.zeros_like(mcorr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n\n    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n    fig = plt.figure(figsize=figsize)\n    heatmap = sns.heatmap(\n        mcorr,\n        mask=mask,\n        cmap=cmap,\n        square=True,\n        annot=True,\n        fmt='0.2f',\n        annot_kws={'size': font_size},\n    )\n\n    heatmap.tick_params(axis='both', which='major', labelsize=font_size)\n    heatmap.tick_params(axis='both', which='minor', labelsize=font_size)\n\n    heatmap.set_xticklabels(features, rotation=90)\n    heatmap.set_yticklabels(reversed(features))\n\n    plt.show()\n\n    if save_filename is not None:\n        fig.savefig(save_filename, dpi=300)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a scatterplot matrix for a list of features colored by target value.", "response": "def scatterplot_matrix(df, features, downsample_frac=None, figsize=(15, 15)):\n    \"\"\"\n    Plot a scatterplot matrix for a list of features, colored by target value.\n\n    Example: `scatterplot_matrix(X, X.columns.tolist(), downsample_frac=0.01)`\n\n    Args:\n        df: Pandas dataframe containing the target column (named 'target').\n        features: The list of features to include in the correlation plot.\n        downsample_frac: Dataframe downsampling rate (0.1 to include 10% of the dataset).\n        figsize: The size of the plot.\n    \"\"\"\n\n    if downsample_frac:\n        df = df.sample(frac=downsample_frac)\n\n    plt.figure(figsize=figsize)\n    sns.pairplot(df[features], hue='target')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_stmt_cmp_eq(p):\n    p[0] = ast.EqNode(ast.VariableNode(p[1]), ast.CmpExprNode(p[4], p[3], p[5]))", "response": "P Statement for comparison eq."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_stmt_ariarithh_eq(p):\n    p[0] = ast.EqNode(ast.VariableNode(p[1]), ast.ArithExprNode(p[4], p[3], p[5]))", "response": "P - statement ariarithh_eq"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_parser(self):\n\n        #self.token_list = None\n        #self.prev_token_lists = None\n\n        self.valid_children = dict()\n        self.valid_children['lems'] = ['component', 'componenttype',\n                                       'target', 'include',\n                                       'dimension', 'unit', 'assertion']\n                                       \n        #TODO: make this generic for any domain specific language based on LEMS\n        self.valid_children['neuroml'] = ['include', 'componenttype']\n                                       \n        self.valid_children['componenttype'] = ['dynamics',\n                                                'child', 'children',\n                                                'componentreference',\n                                                'exposure', 'eventport',\n                                                'fixed', 'link', 'parameter',\n                                                'property',\n                                                'indexparameter',\n                                                'path', 'requirement',\n                                                'componentrequirement',\n                                                'instancerequirement',\n                                                'simulation', 'structure',\n                                                'text', 'attachments',\n                                                'constant', 'derivedparameter']\n                                                \n        self.valid_children['dynamics'] = ['derivedvariable',\n                                           'conditionalderivedvariable',\n                                           'oncondition',\n                                           'onevent', 'onstart',\n                                           'statevariable', 'timederivative',\n                                           'kineticscheme', 'regime']\n                                           \n        self.valid_children['component'] = ['component']\n                                           \n        self.valid_children['conditionalderivedvariable'] = ['case']\n        \n        self.valid_children['regime'] = ['oncondition', 'onentry', 'timederivative']\n        self.valid_children['oncondition'] = ['eventout', 'stateassignment', 'transition']\n        self.valid_children['onentry'] = ['eventout', 'stateassignment', 'transition']\n        self.valid_children['onevent'] = ['eventout', 'stateassignment', 'transition']\n        self.valid_children['onstart'] = ['eventout', 'stateassignment', 'transition']\n        self.valid_children['structure'] = ['childinstance',\n                                            'eventconnection',\n                                            'foreach',\n                                            'multiinstantiate',\n                                            'with',\n                                            'tunnel']\n                                       \n        self.valid_children['foreach'] = ['foreach', 'eventconnection']     \n                                            \n        self.valid_children['simulation'] = ['record', 'eventrecord', 'run',\n                                             'datadisplay', 'datawriter', 'eventwriter']\n\n        self.tag_parse_table = dict()\n        #self.tag_parse_table['assertion'] = self.parse_assertion\n        self.tag_parse_table['attachments'] = self.parse_attachments\n        self.tag_parse_table['child'] = self.parse_child\n        self.tag_parse_table['childinstance'] = self.parse_child_instance\n        self.tag_parse_table['children'] = self.parse_children\n        self.tag_parse_table['component'] = self.parse_component\n        self.tag_parse_table['componentreference'] = self.parse_component_reference   \n        self.tag_parse_table['componentrequirement'] = self.parse_component_requirement\n        self.tag_parse_table['componenttype'] = self.parse_component_type\n        self.tag_parse_table['constant'] = self.parse_constant\n        self.tag_parse_table['datadisplay'] = self.parse_data_display\n        self.tag_parse_table['datawriter'] = self.parse_data_writer\n        self.tag_parse_table['eventwriter'] = self.parse_event_writer\n        self.tag_parse_table['derivedparameter'] = self.parse_derived_parameter\n        self.tag_parse_table['derivedvariable'] = self.parse_derived_variable\n        self.tag_parse_table['conditionalderivedvariable'] = self.parse_conditional_derived_variable\n        self.tag_parse_table['case'] = self.parse_case\n        self.tag_parse_table['dimension'] = self.parse_dimension\n        self.tag_parse_table['dynamics'] = self.parse_dynamics\n        self.tag_parse_table['eventconnection'] = self.parse_event_connection\n        self.tag_parse_table['eventout'] = self.parse_event_out\n        self.tag_parse_table['eventport'] = self.parse_event_port\n        self.tag_parse_table['exposure'] = self.parse_exposure\n        self.tag_parse_table['fixed'] = self.parse_fixed\n        self.tag_parse_table['foreach'] = self.parse_for_each\n        self.tag_parse_table['include'] = self.parse_include\n        self.tag_parse_table['indexparameter'] = self.parse_index_parameter\n        self.tag_parse_table['kineticscheme'] = self.parse_kinetic_scheme\n        self.tag_parse_table['link'] = self.parse_link\n        self.tag_parse_table['multiinstantiate'] = self.parse_multi_instantiate\n        self.tag_parse_table['oncondition'] = self.parse_on_condition\n        self.tag_parse_table['onentry'] = self.parse_on_entry\n        self.tag_parse_table['onevent'] = self.parse_on_event\n        self.tag_parse_table['onstart'] = self.parse_on_start\n        self.tag_parse_table['parameter'] = self.parse_parameter\n        self.tag_parse_table['property'] = self.parse_property\n        self.tag_parse_table['path'] = self.parse_path\n        self.tag_parse_table['record'] = self.parse_record\n        self.tag_parse_table['eventrecord'] = self.parse_event_record\n        self.tag_parse_table['regime'] = self.parse_regime\n        self.tag_parse_table['requirement'] = self.parse_requirement\n        self.tag_parse_table['instancerequirement'] = self.parse_instance_requirement     \n        self.tag_parse_table['run'] = self.parse_run\n        #self.tag_parse_table['show'] = self.parse_show\n        self.tag_parse_table['simulation'] = self.parse_simulation\n        self.tag_parse_table['stateassignment'] = self.parse_state_assignment\n        self.tag_parse_table['statevariable'] = self.parse_state_variable\n        self.tag_parse_table['structure'] = self.parse_structure\n        self.tag_parse_table['target'] = self.parse_target\n        self.tag_parse_table['text'] = self.parse_text\n        self.tag_parse_table['timederivative'] = self.parse_time_derivative\n        self.tag_parse_table['transition'] = self.parse_transition\n        self.tag_parse_table['tunnel'] = self.parse_tunnel\n        self.tag_parse_table['unit'] = self.parse_unit\n        self.tag_parse_table['with'] = self.parse_with\n\n        self.xml_node_stack = []\n\n        self.current_component_type = None\n        self.current_dynamics = None\n        self.current_regime = None\n        self.current_event_handler = None\n        self.current_structure = None\n        self.current_simulation = None\n        self.current_component = None\n        \n        def counter():\n            count = 1\n            while True:\n                yield count\n                count = count + 1\n\n        self.id_counter = counter()", "response": "Initializes the parser for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess child tags. @param node: Current node being parsed. @type node: xml.etree.Element @raise ParseError: Raised when an unexpected nested tag is found.", "response": "def process_nested_tags(self, node, tag = ''):\n        \"\"\"\n        Process child tags.\n\n        @param node: Current node being parsed.\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when an unexpected nested tag is found.\n        \"\"\"\n        ##print(\"---------Processing: %s, %s\"%(node.tag,tag))\n\n        if tag == '':\n            t = node.ltag\n        else:\n            t = tag.lower()\n        \n        for child in node.children:\n            self.xml_node_stack = [child] + self.xml_node_stack\n\n            ctagl = child.ltag\n\n            if ctagl in self.tag_parse_table and ctagl in self.valid_children[t]:\n                #print(\"Processing known type: %s\"%ctagl)\n                self.tag_parse_table[ctagl](child)\n            else:\n                #print(\"Processing unknown type: %s\"%ctagl)\n                self.parse_component_by_typename(child, child.tag)\n\n            self.xml_node_stack = self.xml_node_stack[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(self, xmltext):\n        \n        xml = LEMSXMLNode(xe.XML(xmltext))\n\n        if xml.ltag != 'lems' and xml.ltag != 'neuroml':\n            raise ParseError('<Lems> expected as root element (or even <neuroml>), found: {0}'.format(xml.ltag))\n        '''\n        if xml.ltag == 'lems':\n            if 'description' in xml.lattrib:\n                self.description = xml.lattrib['description']\n        '''\n\n        self.process_nested_tags(xml)", "response": "Parse a string containing LEMS XML formatted text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raise_error(self, message, *params, **key_params):\n        \n        s = 'Parser error in '\n\n        self.xml_node_stack.reverse()\n        if len(self.xml_node_stack) > 1:\n            node = self.xml_node_stack[0]\n            s += '<{0}'.format(node.tag)\n            if 'name' in node.lattrib:\n                s += ' name=\\\"{0}\\\"'.format(node.lattrib['name'])\n            if 'id' in node.lattrib:\n                s += ' id=\\\"{0}\\\"'.format(node.lattrib['id'])\n            s += '>'\n\n        for node in self.xml_node_stack[1:]:\n            s += '.<{0}'.format(node.tag)\n            if 'name' in node.lattrib:\n                s += ' name=\\\"{0}\\\"'.format(node.lattrib['name'])\n            if 'id' in node.lattrib:\n                s += ' id=\\\"{0}\\\"'.format(node.lattrib['id'])\n            s += '>'\n\n        s += ':\\n  ' + message\n\n        raise ParseError(s, *params, **key_params)\n\n        self.xml_node_stack.reverse()", "response": "Raise a parse error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_attachments(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Attachments> must specify a name.')\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            self.raise_error(\"Attachment '{0}' must specify a type.\",\n                             name)\n\n        description = node.lattrib.get('description', '')\n        self.current_component_type.add_attachments(Attachments(name, type_, description))", "response": "Parses the attachments element and adds them to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_child(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Child> must specify a name.')\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            self.raise_error(\"Child '{0}' must specify a type.\", name)\n\n        self.current_component_type.add_children(Children(name, type_, False))", "response": "Parses the child element of the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_child_instance(self, node):\n\n        if 'component' in node.lattrib:\n            component = node.lattrib['component']\n        else:\n            self.raise_error('<ChildInstance> must specify a component reference')\n\n        self.current_structure.add_child_instance(ChildInstance(component))", "response": "Parses the ChildInstance element and adds it to the current structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_children(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Children> must specify a name.')\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            self.raise_error(\"Children '{0}' must specify a type.\", name)\n\n        self.current_component_type.add_children(Children(name, type_, True))", "response": "Parses the children of the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_component_by_typename(self, node, type_):\n        #print('Parsing component {0} by typename {1}'.format(node, type_))\n        if 'id' in node.lattrib:\n            id_ = node.lattrib['id']\n        else:\n            #self.raise_error('Component must have an id')\n            id_ = node.tag #make_id()\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            type_ = node.tag\n\n        component = Component(id_, type_)\n\n        if self.current_component:\n            component.set_parent_id(self.current_component.id)\n            self.current_component.add_child(component)\n            \n        else:\n            self.model.add_component(component)\n\n        for key in node.attrib:\n            if key.lower() not in ['id', 'type']:\n                component.set_parameter(key, node.attrib[key])\n\n        old_component = self.current_component\n        self.current_component = component\n        self.process_nested_tags(node, 'component')\n        self.current_component = old_component", "response": "Parses the component by typename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the component element and stores the attributes in the current component object.", "response": "def parse_component(self, node):\n        \"\"\"\n        Parses <Component>\n\n        @param node: Node containing the <Component> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'id' in node.lattrib:\n            id_ = node.lattrib['id']\n        else:\n            #self.raise_error('Component must have an id')\n            id_ = make_id()\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n                self.raise_error(\"Component {0} must have a type.\",\n                                 id_)\n\n        component = Component(id_, type_)\n\n        if self.current_component:\n            component.set_parent_id(self.current_component.id)\n            self.current_component.add_child(component)\n        else:\n            self.model.add_component(component)\n                \n        for key in node.attrib:\n            if key.lower() not in ['id', 'type']:\n                component.set_parameter(key, node.attrib[key])\n\n        old_component = self.current_component\n        self.current_component = component\n        self.process_nested_tags(node)\n        self.current_component = old_component"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the component reference element and adds it to the current component type.", "response": "def parse_component_reference(self, node):\n        \"\"\"\n        Parses <ComponentReference>\n\n        @param node: Node containing the <ComponentTypeRef> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<ComponentReference> must specify a name for the ' +\n                             'reference.')\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            self.raise_error('<ComponentReference> must specify a type for the ' +\n                             'reference.')\n                             \n        if 'local' in node.lattrib:\n            local = node.lattrib['local']\n        else:\n            local = None\n\n        self.current_component_type.add_component_reference(ComponentReference(name, type_, local))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_component_type(self, node):\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<ComponentType> must specify a name')\n\n        if 'extends' in node.lattrib:\n            extends = node.lattrib['extends']\n        else:\n            extends = None\n\n        if 'description' in node.lattrib:\n            description = node.lattrib['description']\n        else:\n            description = ''\n\n        component_type = ComponentType(name, description, extends)\n        self.model.add_component_type(component_type)\n\n        self.current_component_type = component_type\n        self.process_nested_tags(node)\n        self.current_component_type = None", "response": "Parses the component type element and adds it to the current component type list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the Constant element and adds it to the current component type if it is not already there.", "response": "def parse_constant(self, node):\n        \"\"\"\n        Parses <Constant>\n\n        @param node: Node containing the <Constant> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Constant> must specify a name.')\n\n        dimension = node.lattrib.get('dimension', None)\n\n        try:\n            value = node.lattrib['value']\n        except:\n            self.raise_error(\"Constant '{0}' must have a value.\", name)\n\n        description = node.lattrib.get('description', '')\n\n        constant = Constant(name, value, dimension, description)\n\n        if self.current_component_type:\n            self.current_component_type.add_constant(constant)\n        else:\n            self.model.add_constant(constant)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the data display element and adds it to the current simulation.", "response": "def parse_data_display(self, node):\n        \"\"\"\n        Parses <DataDisplay>\n\n        @param node: Node containing the <DataDisplay> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'title' in node.lattrib:\n            title = node.lattrib['title']\n        else:\n            self.raise_error('<DataDisplay> must have a title.')\n\n        if 'dataregion' in node.lattrib:\n            data_region = node.lattrib['dataregion']\n        else:\n            data_region = None\n\n        self.current_simulation.add_data_display(DataDisplay(title, data_region))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the data writer element and adds it to the current simulation.", "response": "def parse_data_writer(self, node):\n        \"\"\"\n        Parses <DataWriter>\n\n        @param node: Node containing the <DataWriter> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'path' in node.lattrib:\n            path = node.lattrib['path']\n        else:\n            self.raise_error('<DataWriter> must specify a path.')\n\n        if 'filename' in node.lattrib:\n            file_path = node.lattrib['filename']\n        else:\n            self.raise_error(\"Data writer for '{0}' must specify a filename.\",\n                             path)\n\n        self.current_simulation.add_data_writer(DataWriter(path, file_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the event writer element and adds it to the simulation.", "response": "def parse_event_writer(self, node):\n        \"\"\"\n        Parses <EventWriter>\n\n        @param node: Node containing the <EventWriter> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'path' in node.lattrib:\n            path = node.lattrib['path']\n        else:\n            self.raise_error('<EventWriter> must specify a path.')\n\n        if 'filename' in node.lattrib:\n            file_path = node.lattrib['filename']\n        else:\n            self.raise_error(\"Event writer for '{0}' must specify a filename.\",\n                             path)\n                             \n        if 'format' in node.lattrib:\n            format = node.lattrib['format']\n        else:\n            self.raise_error(\"Event writer for '{0}' must specify a format.\",\n                             path)\n\n        self.current_simulation.add_event_writer(EventWriter(path, file_path, format))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_derived_parameter(self, node):\n\n        #if self.current_context.context_type != Context.COMPONENT_TYPE:\n        #    self.raise_error('Dynamics must be defined inside a ' +\n        #                     'component type')\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('A derived parameter must have a name')\n\n        if 'dimension' in node.lattrib:\n            dimension = node.lattrib['dimension']\n        else:\n            dimension = None\n\n        if 'value' in node.lattrib:\n            value = node.lattrib['value']\n        else:\n            value = None\n\n        if 'select' in node.lattrib:\n            select = node.lattrib['select']\n        else:\n            select = None\n\n        self.current_component_type.add_derived_parameter(DerivedParameter(name, value,\n                                                                    dimension, select))", "response": "Parses the derived parameter element and adds it to the current component type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the derived variable element and adds it to the current regime", "response": "def parse_derived_variable(self, node):\n        \"\"\"\n        Parses <DerivedVariable>\n\n        @param node: Node containing the <DerivedVariable> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when no name of specified for the derived variable.\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        elif 'exposure' in node.lattrib:\n            name = node.lattrib['exposure']\n        else:\n            self.raise_error('<DerivedVariable> must specify a name')\n\n        params = dict()\n        for attr_name in ['dimension', 'exposure', 'select', 'value', 'reduce', 'required']:\n            if attr_name in node.lattrib:\n                params[attr_name] = node.lattrib[attr_name]\n\n        self.current_regime.add_derived_variable(DerivedVariable(name, **params))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the conditional derived variable element and adds it to the registry", "response": "def parse_conditional_derived_variable(self, node):\n        \"\"\"\n        Parses <ConditionalDerivedVariable>\n\n        @param node: Node containing the <ConditionalDerivedVariable> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when no name or value is specified for the conditional derived variable.\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        elif 'exposure' in node.lattrib:\n            name = node.lattrib['exposure']\n        else:\n            self.raise_error('<ConditionalDerivedVariable> must specify a name')\n            \n        if 'exposure' in node.lattrib:\n            exposure = node.lattrib['exposure']\n        else:\n            exposure = None\n            \n        if 'dimension' in node.lattrib:\n            dimension = node.lattrib['dimension']\n        else:\n            dimension = None\n\n        conditional_derived_variable = ConditionalDerivedVariable(name, dimension, exposure)\n        \n        self.current_regime.add_conditional_derived_variable(conditional_derived_variable)\n        \n        self.current_conditional_derived_variable = conditional_derived_variable\n        \n        self.process_nested_tags(node)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the case element containing the node containing the element", "response": "def parse_case(self, node):\n        \"\"\"\n        Parses <Case>\n\n        @param node: Node containing the <Case> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: When no condition or value is specified\n        \"\"\"\n        \n        try:\n            condition = node.lattrib['condition']\n        except:\n            condition = None\n            \n        try:\n            value = node.lattrib['value']\n        except:\n            self.raise_error('<Case> must specify a value')\n\n        self.current_conditional_derived_variable.add_case(Case(condition, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_dimension(self, node):\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Dimension> must specify a name')\n\n        description = node.lattrib.get('description', '')\n\n        dim = dict()\n        for d in ['l', 'm', 't', 'i', 'k', 'c', 'n']:\n            dim[d] = int(node.lattrib.get(d, 0))\n\n        self.model.add_dimension(Dimension(name, description, **dim))", "response": "Parses the Dimension element and adds it to the internal dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the Dynamics object from the xml. etree. Element node.", "response": "def parse_dynamics(self, node):\n        \"\"\"\n        Parses <Dynamics>\n\n        @param node: Node containing the <Behaviour> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        self.current_dynamics = self.current_component_type.dynamics\n        self.current_regime = self.current_dynamics\n        self.process_nested_tags(node)\n        self.current_regime = None\n        self.current_dynamics = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_event_connection(self, node):\n\n        if 'from' in node.lattrib:\n            from_ = node.lattrib['from']\n        else:\n            self.raise_error('<EventConnection> must provide a source (from) component reference.')\n\n        if 'to' in node.lattrib:\n            to = node.lattrib['to']\n        else:\n            self.raise_error('<EventConnection> must provide a target (to) component reference.')\n\n        source_port = node.lattrib.get('sourceport', '')\n        target_port = node.lattrib.get('targetport', '')\n        receiver = node.lattrib.get('receiver', '')\n        receiver_container = node.lattrib.get('receivercontainer', '')\n\n        ec = EventConnection(from_, to, source_port, target_port, receiver, receiver_container)\n        self.current_structure.add_event_connection(ec)", "response": "Parses the EventConnection element and adds it to the event structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_event_out(self, node):\n\n        try:\n            port = node.lattrib['port']\n        except:\n            self.raise_error('<EventOut> must be specify a port.')\n\n        action = EventOut(port)\n\n        self.current_event_handler.add_action(action)", "response": "Parses the EventOut element and adds it to the event handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the EventPort element and adds it to the event_type.", "response": "def parse_event_port(self, node):\n        \"\"\"\n        Parses <EventPort>\n\n        @param node: Node containing the <EventPort> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error(('<EventPort> must specify a name.'))\n\n        if 'direction' in node.lattrib:\n            direction = node.lattrib['direction']\n        else:\n            self.raise_error(\"Event port '{0}' must specify a direction.\")\n\n        direction = direction.lower()\n        if direction != 'in' and direction != 'out':\n            self.raise_error(('Event port direction must be \\'in\\' '\n                              'or \\'out\\''))\n\n        description = node.lattrib.get('description', '')\n        \n        self.current_component_type.add_event_port(EventPort(name, direction, description))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the exposure element and adds it to the current component type.", "response": "def parse_exposure(self, node):\n        \"\"\"\n        Parses <Exposure>\n\n        @param node: Node containing the <Exposure> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when the exposure name is not\n        being defined in the context of a component type.\n        \"\"\"\n\n        if self.current_component_type == None:\n            self.raise_error('Exposures must be defined in a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Exposure> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Exposure '{0}' must specify a dimension\",\n                             name)\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_exposure(Exposure(name, dimension, description))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the fixed element and adds it to the current component type.", "response": "def parse_fixed(self, node):\n        \"\"\"\n        Parses <Fixed>\n\n        @param node: Node containing the <Fixed> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        try:\n            parameter = node.lattrib['parameter']\n        except:\n            self.raise_error('<Fixed> must specify a parameter to be fixed.')\n\n        try:\n            value = node.lattrib['value']\n        except:\n            self.raise_error(\"Fixed parameter '{0}'must specify a value.\", parameter)\n\n        description = node.lattrib.get('description', '')\n        \n        self.current_component_type.add_parameter(Fixed(parameter, value, description))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_for_each(self, node):\n        \n        if self.current_structure == None:\n            self.raise_error('<ForEach> can only be made within ' +\n                             'a structure definition')\n\n        if 'instances' in node.lattrib:\n            instances = node.lattrib['instances']\n        else:\n            self.raise_error('<ForEach> must specify a reference to target'\n                             'instances')\n\n        if 'as' in node.lattrib:\n            as_ = node.lattrib['as']\n        else:\n            self.raise_error('<ForEach> must specify a name for the '\n                             'enumerated target instances')\n\n        old_structure = self.current_structure\n        fe = ForEach(instances, as_)\n        self.current_structure.add_for_each(fe)\n        self.current_structure = fe\n        \n        self.process_nested_tags(node)\n\n        self.current_structure = old_structure", "response": "Parses the ForEach element and adds it to the current structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_include(self, node):\n        if not self.include_includes:\n            if self.model.debug: print(\"Ignoring included LEMS file: %s\"%node.lattrib['file'])\n        else:\n\n            #TODO: remove this hard coding for reading NeuroML includes...\n            if 'file' not in node.lattrib:\n                if 'href' in node.lattrib:\n                    self.model.include_file(node.lattrib['href'], self.include_dirs)\n                    return\n                else:\n                    self.raise_error('<Include> must specify the file to be included.')\n\n            self.model.include_file(node.lattrib['file'], self.include_dirs)", "response": "Parses the include element and returns the class ID of the include element."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the kinetic scheme element and adds it to the current regime.", "response": "def parse_kinetic_scheme(self, node):\n        \"\"\"\n        Parses <KineticScheme>\n\n        @param node: Node containing the <KineticScheme> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<KineticScheme> must specify a name.')\n\n        if 'nodes' in node.lattrib:\n            nodes = node.lattrib['nodes']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify nodes.\", name)\n\n        if 'statevariable' in node.lattrib:\n            state_variable = node.lattrib['statevariable']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify a state variable.\", name)\n\n        if 'edges' in node.lattrib:\n            edges = node.lattrib['edges']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify edges.\", name)\n\n        if 'edgesource' in node.lattrib:\n            edge_source = node.lattrib['edgesource']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify the edge source attribute.\", name)\n\n        if 'edgetarget' in node.lattrib:\n            edge_target = node.lattrib['edgetarget']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify the edge target attribute.\", name)\n\n        if 'forwardrate' in node.lattrib:\n            forward_rate = node.lattrib['forwardrate']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify the forward rate attribute.\", name)\n\n        if 'reverserate' in node.lattrib:\n            reverse_rate = node.lattrib['reverserate']\n        else:\n            self.raise_error(\"Kinetic scheme '{0}' must specify the reverse rate attribute\", name)\n\n        self.current_regime.add_kinetic_scheme(KineticScheme(name, nodes, state_variable,\n                                                             edges, edge_source, edge_target,\n                                                             forward_rate, reverse_rate))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the Link element and adds it to the current component type.", "response": "def parse_link(self, node):\n        \"\"\"\n        Parses <Link>\n\n        @param node: Node containing the <Link> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Link> must specify a name')\n\n        if 'type' in node.lattrib:\n            type_ = node.lattrib['type']\n        else:\n            self.raise_error(\"Link '{0}' must specify a type\", name)\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_link(Link(name, type_, description))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the MultiInstantiate element and adds it to the current structure.", "response": "def parse_multi_instantiate(self, node):\n        \"\"\"\n        Parses <MultiInstantiate>\n\n        @param node: Node containing the <MultiInstantiate> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'component' in node.lattrib:\n            component = node.lattrib['component']\n        else:\n            self.raise_error('<MultiInstantiate> must specify a component reference.')\n\n        if 'number' in node.lattrib:\n            number = node.lattrib['number']\n        else:\n            self.raise_error(\"Multi instantiation of '{0}' must specify a parameter specifying the number.\",\n                             component)\n\n        self.current_structure.add_multi_instantiate(MultiInstantiate(component, number))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the OnCondition element and sets the event handler and the event handler attribute.", "response": "def parse_on_condition(self, node):\n        \"\"\"\n        Parses <OnCondition>\n\n        @param node: Node containing the <OnCondition> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        try:\n            test = node.lattrib['test']\n        except:\n            self.raise_error('<OnCondition> must specify a test.')\n            \n        event_handler = OnCondition(test)\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the OnEntry element and sets the event handler to the event handler that is added to the registry.", "response": "def parse_on_entry(self, node):\n        \"\"\"\n        Parses <OnEntry>\n\n        @param node: Node containing the <OnEntry> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        event_handler = OnEntry()\n\n        self.current_event_handler = event_handler\n        self.current_regime.add_event_handler(event_handler)\n\n        self.process_nested_tags(node)\n\n        self.current_event_handler = None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_on_event(self, node):\n\n        try:\n            port = node.lattrib['port']\n        except:\n            self.raise_error('<OnEvent> must specify a port.')\n            \n        event_handler = OnEvent(port)\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "response": "Parses the OnEvent element and sets the event handler attribute."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_on_start(self, node):\n\n        event_handler = OnStart()\n\n        self.current_regime.add_event_handler(event_handler)\n\n        self.current_event_handler = event_handler\n        self.process_nested_tags(node)\n        self.current_event_handler = None", "response": "Parses the OnStart element and sets the event handler to the object that is passed to the event handler."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_parameter(self, node):\n\n        if self.current_component_type == None:\n            self.raise_error('Parameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Parameter> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Parameter '{0}' has no dimension\",\n                             name)\n\n        parameter = Parameter(name, dimension)\n\n        self.current_component_type.add_parameter(parameter)", "response": "Parses the parameter element and adds it to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_property(self, node):\n\n        if self.current_component_type == None:\n            self.raise_error('Property can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Property> must specify a name')\n\n        try:\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error(\"Property '{0}' has no dimension\",\n                             name)\n                             \n        default_value = node.lattrib.get('defaultvalue', None)\n        \n        property = Property(name, dimension, default_value=default_value)\n\n        self.current_component_type.add_property(property)", "response": "Parses the property from the xml. etree. Element node containing the property name and dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_index_parameter(self, node):\n\n        if self.current_component_type == None:\n            self.raise_error('IndexParameters can only be defined in ' +\n                             'a component type')\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<IndexParameter> must specify a name')\n\n\n        index_parameter = IndexParameter(name)\n\n        self.current_component_type.add_index_parameter(index_parameter)", "response": "Parses the index parameter element and adds it to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_tunnel(self, node):\n\n        try:\n            name = node.lattrib['name']\n        except:\n            self.raise_error('<Tunnel> must specify a name')\n        try:\n            end_a = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endA')\n        try:\n            end_b = node.lattrib['enda']\n        except:\n            self.raise_error('<Tunnel> must specify: endB')\n        try:\n            component_a = node.lattrib['componenta']\n        except:\n            self.raise_error('<Tunnel> must specify: componentA')\n        try:\n            component_b = node.lattrib['componentb']\n        except:\n            self.raise_error('<Tunnel> must specify: componentB')\n\n\n        tunnel = Tunnel(name, end_a, end_b, component_a, component_b)\n\n        self.current_structure.add_tunnel(tunnel)", "response": "Parses the Tunnel element and adds it to the current structure."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the path element and adds it to the current component type.", "response": "def parse_path(self, node):\n        \"\"\"\n        Parses <Path>\n\n        @param node: Node containing the <Path> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Path> must specify a name.')\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_path(Path(name, description))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the record element and adds it to the current simulation.", "response": "def parse_record(self, node):\n        \"\"\"\n        Parses <Record>\n\n        @param node: Node containing the <Record> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if self.current_simulation == None:\n            self.raise_error('<Record> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<Record> must specify a quantity.')\n\n        scale = node.lattrib.get('scale', None)\n        color  = node.lattrib.get('color', None)\n        id  = node.lattrib.get('id', None)\n\n        self.current_simulation.add_record(Record(quantity, scale, color, id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_event_record(self, node):\n\n        if self.current_simulation == None:\n            self.raise_error('<EventRecord> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<EventRecord> must specify a quantity.')\n\n        if 'eventport' in node.lattrib:\n            eventPort = node.lattrib['eventport']\n        else:\n            self.raise_error('<EventRecord> must specify an eventPort.')\n\n\n        self.current_simulation.add_event_record(EventRecord(quantity, eventPort))", "response": "Parses the event record element and adds it to the current simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_regime(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            name = ''\n\n        if 'initial' in node.lattrib:\n            initial = (node.lattrib['initial'].strip().lower() == 'true')\n        else:\n            initial = False\n\n        regime = Regime(name, self.current_dynamics, initial)\n        old_regime = self.current_regime\n        self.current_dynamics.add_regime(regime)\n        self.current_regime = regime\n\n        self.process_nested_tags(node)\n\n        self.current_regime = old_regime", "response": "Parses the Regime element and adds it to the registry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_requirement(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Requirement> must specify a name')\n\n        if 'dimension' in node.lattrib:\n            dimension = node.lattrib['dimension']\n        else:\n            self.raise_error(\"Requirement \\{0}' must specify a dimension.\", name)\n\n        self.current_component_type.add_requirement(Requirement(name, dimension))", "response": "Parses the requirement element and adds it to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the ComponentRequirement element and adds it to the current component type.", "response": "def parse_component_requirement(self, node):\n        \"\"\"\n        Parses <ComponentRequirement>\n\n        @param node: Node containing the <ComponentRequirement> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<ComponentRequirement> must specify a name')\n\n        self.current_component_type.add_component_requirement(ComponentRequirement(name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_instance_requirement(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<InstanceRequirement> must specify a name')\n\n        if 'type' in node.lattrib:\n            type = node.lattrib['type']\n        else:\n            self.raise_error(\"InstanceRequirement \\{0}' must specify a type.\", name)\n\n        self.current_component_type.add_instance_requirement(InstanceRequirement(name, type))", "response": "Parses the instance requirement element and adds it to the current component type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the run element and adds it to the current simulation", "response": "def parse_run(self, node):\n        \"\"\"\n        Parses <Run>\n\n        @param node: Node containing the <Run> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'component' in node.lattrib:\n            component = node.lattrib['component']\n        else:\n            self.raise_error('<Run> must specify a target component')\n\n        if 'variable' in node.lattrib:\n            variable = node.lattrib['variable']\n        else:\n            self.raise_error('<Run> must specify a state variable')\n\n        if 'increment' in node.lattrib:\n            increment = node.lattrib['increment']\n        else:\n            self.raise_error('<Run> must specify an increment for the ' +\n                             'state variable')\n\n        if 'total' in node.lattrib:\n            total = node.lattrib['total']\n        else:\n            self.raise_error('<Run> must specify a final value for the ' +\n                             'state variable')\n\n        self.current_simulation.add_run(Run(component, variable, increment, total))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_simulation(self, node):\n\n        self.current_simulation = self.current_component_type.simulation\n\n        self.process_nested_tags(node)\n\n        self.current_simulation = None", "response": "Parses the simulation element and sets the current_simulation attribute to None."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the state assignment element and adds it to the event handler.", "response": "def parse_state_assignment(self, node):\n        \"\"\"\n        Parses <StateAssignment>\n\n        @param node: Node containing the <StateAssignment> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'variable' in node.lattrib:\n            variable = node.lattrib['variable']\n        else:\n            self.raise_error('<StateAssignment> must specify a variable name')\n\n        if 'value' in node.lattrib:\n            value = node.lattrib['value']\n        else:\n            self.raise_error(\"State assignment for '{0}' must specify a value.\",\n                             variable)\n\n        action = StateAssignment(variable, value)\n\n        self.current_event_handler.add_action(action)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_state_variable(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<StateVariable> must specify a name')\n\n        if 'dimension' in node.lattrib:\n            dimension = node.lattrib['dimension']\n        else:\n            self.raise_error(\"State variable '{0}' must specify a dimension\", name)\n\n        if 'exposure' in node.lattrib:\n            exposure = node.lattrib['exposure']\n        else:\n            exposure = None\n\n        self.current_regime.add_state_variable(StateVariable(name, dimension, exposure))", "response": "Parses the state variable element and adds it to the current regime"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the structure node and sets the structure attribute to the object representation of the structure node.", "response": "def parse_structure(self, node):\n        \"\"\"\n        Parses <Structure>\n\n        @param node: Node containing the <Structure> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        self.current_structure = self.current_component_type.structure\n        self.process_nested_tags(node)\n        self.current_structure = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_text(self, node):\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            self.raise_error('<Text> must specify a name.')\n\n        description = node.lattrib.get('description', '')\n\n        self.current_component_type.add_text(Text(name, description))", "response": "Parses the text element of the current component."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_time_derivative(self, node):\n\n        if 'variable' in node.lattrib:\n            variable = node.lattrib['variable']\n        else:\n            self.raise_error('<TimeDerivative> must specify a variable.')\n\n        if 'value' in node.lattrib:\n            value = node.lattrib['value']\n        else:\n            self.raise_error(\"Time derivative for '{0}' must specify an expression.\",\n                             variable)\n\n        self.current_regime.add_time_derivative(TimeDerivative(variable, value))", "response": "Parses the time derivative element and adds it to the current regime."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the Transition element and adds it to the current event handler.", "response": "def parse_transition(self, node):\n        \"\"\"\n        Parses <Transition>\n\n        @param node: Node containing the <Transition> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'regime' in node.lattrib:\n            regime = node.lattrib['regime']\n        else:\n            self.raise_error('<Transition> mut specify a regime.')\n\n        action = Transition(regime)\n\n        self.current_event_handler.add_action(action)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the unit element and adds it to the model.", "response": "def parse_unit(self, node):\n        \"\"\"\n        Parses <Unit>\n\n        @param node: Node containing the <Unit> element\n        @type node: xml.etree.Element\n\n        @raise ParseError: When the name is not a string or the unit\n        specfications are incorrect.\n\n        @raise ModelError: When the unit refers to an undefined dimension.\n        \"\"\"\n\n        try:\n            symbol = node.lattrib['symbol']\n            dimension = node.lattrib['dimension']\n        except:\n            self.raise_error('Unit must have a symbol and dimension.')\n\n        if 'power' in node.lattrib:\n            power = int(node.lattrib['power'])\n        else:\n            power = 0\n\n        if 'name' in node.lattrib:\n            name = node.lattrib['name']\n        else:\n            name = ''\n            \n        if 'scale' in node.lattrib:\n            scale = float(node.lattrib['scale'])\n        else:\n            scale = 1.0\n            \n        if 'offset' in node.lattrib:\n            offset = float(node.lattrib['offset'])\n        else:\n            offset = 0.0\n\n        self.model.add_unit(Unit(name, symbol, dimension, power, scale, offset))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the with element and adds it to the current structure", "response": "def parse_with(self, node):\n        \"\"\"\n        Parses <With>\n\n        @param node: Node containing the <With> element\n        @type node: xml.etree.Element\n        \"\"\"\n\n        if 'instance' in node.lattrib:\n            instance = node.lattrib['instance']\n            list = None\n            index = None\n        elif 'list' in node.lattrib and 'index' in node.lattrib:\n            instance = None\n            list = node.lattrib['list']\n            index = node.lattrib['index']\n        else:\n            self.raise_error('<With> must specify EITHER instance OR list & index')\n\n        if 'as' in node.lattrib:\n            as_ = node.lattrib['as']\n        else:\n            self.raise_error('<With> must specify a name for the '\n                             'target instance')\n\n        self.current_structure.add_with(With(instance, as_, list, index))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate xmltags for multiple files.", "response": "def generate_tags_multiple_files(input_files, tag, ignore_tags, ns=None):\n    \"\"\"\n    Calls xmltag generator for multiple files.\n    \"\"\"\n    return itertools.chain.from_iterable([generate_xmltags(\n        fn, tag, ignore_tags, ns) for fn in input_files])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_tags_multiple_files_strings(input_files, ns, tag, ignore_tags):\n    for el in generate_tags_multiple_files(input_files, tag, ignore_tags, ns):\n        yield formatting.string_and_clear(el, ns)", "response": "Generates a stringified xml output of multiple elements with certain tag."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbasing generator for percolator xml psm, peptide, protein output, as well as for mzML, mzIdentML. ignore_tags are the ones that are cleared when met by parser.", "response": "def generate_xmltags(fn, returntag, ignore_tags, ns=None):\n    \"\"\"\n    Base generator for percolator xml psm, peptide, protein output,\n    as well as for mzML, mzIdentML.\n    ignore_tags are the ones that are cleared when met by parser.\n    \"\"\"\n    xmlns = create_namespace(ns)\n    ns_ignore = ['{0}{1}'.format(xmlns, x) for x in ignore_tags]\n    for ac, el in etree.iterparse(fn):\n        if el.tag == '{0}{1}'.format(xmlns, returntag):\n            yield el\n        elif el.tag in ns_ignore:\n            formatting.clear_el(el)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a component type to the model.", "response": "def add_component_type(self, component_type):\n        \"\"\"\n        Adds a component type to the model.\n\n        @param component_type: Component type to be added.\n        @type component_type: lems.model.fundamental.ComponentType\n        \"\"\"\n        name = component_type.name\n        \n        # To handle colons in names in LEMS\n        if ':' in name:\n            name = name.replace(':', '_')\n            component_type.name = name\n            \n        self.component_types[name] = component_type"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a typed child object to the internal list of the object.", "response": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the model.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Include):\n            self.add_include(child)\n        elif isinstance(child, Dimension):\n            self.add_dimension(child)\n        elif isinstance(child, Unit):\n            self.add_unit(child)\n        elif isinstance(child, ComponentType):\n            self.add_component_type(child)\n        elif isinstance(child, Component):\n            self.add_component(child)\n        elif isinstance(child, FatComponent):\n            self.add_fat_component(child)\n        elif isinstance(child, Constant):\n            self.add_constant(child)\n        else:\n            raise ModelError('Unsupported child element')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef include_file(self, path, include_dirs = []):\n        if self.include_includes:\n            if self.debug: print(\"------------------                   Including a file: %s\"%path)\n            inc_dirs = include_dirs if include_dirs else self.include_dirs\n\n            parser = LEMSFileParser(self, inc_dirs, self.include_includes)\n            if os.access(path, os.F_OK):\n                if not path in self.included_files:\n                    parser.parse(open(path).read()) \n                    self.included_files.append(path)\n                    return\n                else:\n                    if self.debug: print(\"Already included: %s\"%path)\n                    return\n            else:\n                for inc_dir in inc_dirs:\n                    new_path = (inc_dir + '/' + path)\n                    if os.access(new_path, os.F_OK):\n                        if not new_path in self.included_files:\n                            parser.parse(open(new_path).read())\n                            self.included_files.append(new_path)\n                            return\n                        else:\n                            if self.debug: print(\"Already included: %s\"%path)\n                            return\n            msg = 'Unable to open ' + path\n            if self.fail_on_missing_includes:\n                raise Exception(msg)\n            elif self.debug: \n                print(msg)", "response": "Include a file into the current model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports a model from a file.", "response": "def import_from_file(self, filepath):\n        \"\"\"\n        Import a model from a file.\n\n        @param filepath: File to be imported.\n        @type filepath: str\n        \"\"\"\n        \n        inc_dirs = self.include_directories[:]\n        inc_dirs.append(dirname(filepath))\n                        \n        parser = LEMSFileParser(self, inc_dirs, self.include_includes)\n        with open(filepath) as f:\n            parser.parse(f.read())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexport this model to a DOM.", "response": "def export_to_dom(self):\n        \"\"\"\n        Exports this model to a DOM.\n        \"\"\"\n        namespaces = 'xmlns=\"http://www.neuroml.org/lems/%s\" ' + \\\n                     'xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" ' + \\\n                     'xsi:schemaLocation=\"http://www.neuroml.org/lems/%s %s\"'\n\n        namespaces = namespaces%(self.target_lems_version,self.target_lems_version,self.schema_location)\n\n        xmlstr = '<Lems %s>'%namespaces\n\n        for include in self.includes:\n            xmlstr += include.toxml()\n\n        for target in self.targets:\n            xmlstr += '<Target component=\"{0}\"/>'.format(target)\n\n        for dimension in self.dimensions:\n            xmlstr += dimension.toxml()\n            \n        for unit in self.units:\n            xmlstr += unit.toxml()\n            \n        for constant in self.constants:\n            xmlstr += constant.toxml()\n            \n        for component_type in self.component_types:\n            xmlstr += component_type.toxml()\n            \n        for component in self.components:\n            xmlstr += component.toxml()\n            \n        xmlstr += '</Lems>'\n\n        xmldom = minidom.parseString(xmlstr)\n        return xmldom"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_to_file(self, filepath, level_prefix = '  '):\n        xmldom = self.export_to_dom()\n        xmlstr = xmldom.toprettyxml(level_prefix, '\\n',)\n\n\n        f = open(filepath, 'w')\n        f.write(xmlstr)\n        f.close()", "response": "Exports this model to a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(self):\n\n        model = self.copy()\n        \n        for ct in model.component_types:\n            model.resolve_component_type(ct)\n\n        for c in model.components:\n            if c.id not in model.fat_components:\n                model.add(model.fatten_component(c))\n\n        for c in ct.constants:\n            c2 = c.copy()\n            c2.numeric_value = model.get_numeric_value(c2.value, c2.dimension)\n            model.add(c2)\n            \n        return model", "response": "Resolves references in this model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresolving references in the specified component type.", "response": "def resolve_component_type(self, component_type):\n        \"\"\"\n        Resolves references in the specified component type.\n\n        @param component_type: Component type to be resolved.\n        @type component_type: lems.model.component.ComponentType\n        \"\"\"\n        \n        # Resolve component type from base types if present.\n        if component_type.extends:\n            try:\n                base_ct = self.component_types[component_type.extends]\n            except:\n                raise ModelError(\"Component type '{0}' trying to extend unknown component type '{1}'\",\n                                 component_type.name, component_type.extends)\n\n            self.resolve_component_type(base_ct)\n            self.merge_component_types(component_type, base_ct)\n            component_type.types = set.union(component_type.types, base_ct.types)\n            component_type.extends = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmerge various maps in the given base enumeration component type into a single base enumeration component type.", "response": "def merge_component_types(self, ct, base_ct):\n        \"\"\"\n        Merge various maps in the given component type from a base \n        component type.\n\n        @param ct: Component type to be resolved.\n        @type ct: lems.model.component.ComponentType\n\n        @param base_ct: Component type to be resolved.\n        @type base_ct: lems.model.component.ComponentType\n        \"\"\"\n\n        #merge_maps(ct.parameters, base_ct.parameters)\n        for parameter in base_ct.parameters:\n            if parameter.name in ct.parameters:\n                p = ct.parameters[parameter.name]\n                basep = base_ct.parameters[parameter.name]\n                if p.fixed:\n                    p.value = p.fixed_value\n                    p.dimension = basep.dimension\n            else:\n                ct.parameters[parameter.name] = base_ct.parameters[parameter.name]\n            \n        merge_maps(ct.properties, base_ct.properties)\n        \n        merge_maps(ct.derived_parameters, base_ct.derived_parameters)\n        merge_maps(ct.index_parameters, base_ct.index_parameters)\n        merge_maps(ct.constants, base_ct.constants)\n        merge_maps(ct.exposures, base_ct.exposures)\n        merge_maps(ct.requirements, base_ct.requirements)\n        merge_maps(ct.component_requirements, base_ct.component_requirements)\n        merge_maps(ct.instance_requirements, base_ct.instance_requirements)\n        merge_maps(ct.children, base_ct.children)\n        merge_maps(ct.texts, base_ct.texts)\n        merge_maps(ct.links, base_ct.links)\n        merge_maps(ct.paths, base_ct.paths)\n        merge_maps(ct.event_ports, base_ct.event_ports)\n        merge_maps(ct.component_references, base_ct.component_references)\n        merge_maps(ct.attachments, base_ct.attachments)\n\n        merge_maps(ct.dynamics.state_variables, base_ct.dynamics.state_variables)\n        merge_maps(ct.dynamics.derived_variables, base_ct.dynamics.derived_variables)\n        merge_maps(ct.dynamics.conditional_derived_variables, base_ct.dynamics.conditional_derived_variables)\n        merge_maps(ct.dynamics.time_derivatives, base_ct.dynamics.time_derivatives)\n        \n        #merge_lists(ct.dynamics.event_handlers, base_ct.dynamics.event_handlers)\n        \n        merge_maps(ct.dynamics.kinetic_schemes, base_ct.dynamics.kinetic_schemes)\n\n        merge_lists(ct.structure.event_connections, base_ct.structure.event_connections)\n        merge_lists(ct.structure.child_instances, base_ct.structure.child_instances)\n        merge_lists(ct.structure.multi_instantiates, base_ct.structure.multi_instantiates)\n\n        merge_maps(ct.simulation.runs, base_ct.simulation.runs)\n        merge_maps(ct.simulation.records, base_ct.simulation.records)\n        merge_maps(ct.simulation.event_records, base_ct.simulation.event_records)\n        merge_maps(ct.simulation.data_displays, base_ct.simulation.data_displays)\n        merge_maps(ct.simulation.data_writers, base_ct.simulation.data_writers)\n        merge_maps(ct.simulation.event_writers, base_ct.simulation.event_writers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfatten a component but resolving all references into the corresponding component type.", "response": "def fatten_component(self, c):\n        \"\"\"\n        Fatten a component but resolving all references into the corresponding component type.\n\n        @param c: Lean component to be fattened.\n        @type c: lems.model.component.Component\n\n        @return: Fattened component.\n        @rtype: lems.model.component.FatComponent\n        \"\"\"\n        if self.debug: print(\"Fattening %s\"%c.id) \n        try:\n            ct = self.component_types[c.type]\n        except:\n            raise ModelError(\"Unable to resolve type '{0}' for component '{1}'; existing: {2}\",\n                             c.type, c.id, self.component_types.keys())\n        \n        fc = FatComponent(c.id, c.type)\n        if c.parent_id: fc.set_parent_id(c.parent_id)\n\n        ### Resolve parameters\n        for parameter in ct.parameters:\n            if self.debug: print(\"Checking: %s\"%parameter)\n            if parameter.name in c.parameters:\n                p = parameter.copy()\n                p.value = c.parameters[parameter.name]\n                p.numeric_value = self.get_numeric_value(p.value, p.dimension)\n                fc.add_parameter(p)\n            elif parameter.fixed:\n                p = parameter.copy()\n                p.numeric_value = self.get_numeric_value(p.value, p.dimension)\n                fc.add_parameter(p)\n            else:\n                raise ModelError(\"Parameter '{0}' not initialized for component '{1}'\",\n                                 parameter.name, c.id)\n\n        ### Resolve properties\n        for property in ct.properties:\n            property2 = property.copy()\n            fc.add(property2)\n            \n        ### Resolve derived_parameters\n        for derived_parameter in ct.derived_parameters:\n            derived_parameter2 = derived_parameter.copy()\n            fc.add(derived_parameter2)\n            \n        ### Resolve derived_parameters\n        for index_parameter in ct.index_parameters:\n            raise ModelError(\"IndexParameter not yet implemented in PyLEMS!\")\n            index_parameter2 = index_parameter.copy()\n            fc.add(index_parameter2)\n            \n        ### Resolve constants\n        for constant in ct.constants:\n            constant2 = constant.copy()\n            constant2.numeric_value = self.get_numeric_value(constant2.value, constant2.dimension)\n            fc.add(constant2)\n\n        ### Resolve texts\n        for text in ct.texts:\n            t = text.copy()\n            t.value = c.parameters[text.name] if text.name in c.parameters else ''\n            fc.add(t)\n                \n        ### Resolve texts\n        for link in ct.links:\n            if link.name in c.parameters:\n                l = link.copy()\n                l.value = c.parameters[link.name]\n                fc.add(l)\n            else:\n                raise ModelError(\"Link parameter '{0}' not initialized for component '{1}'\",\n                                 link.name, c.id)\n                \n        ### Resolve paths\n        for path in ct.paths:\n            if path.name in c.parameters:\n                p = path.copy()\n                p.value = c.parameters[path.name]\n                fc.add(p)\n            else:\n                raise ModelError(\"Path parameter '{0}' not initialized for component '{1}'\",\n                                 path.name, c.id)\n\n        if len(ct.component_requirements)>0:\n            raise ModelError(\"ComponentRequirement not yet implemented in PyLEMS!\")\n        if len(ct.instance_requirements)>0:\n            raise ModelError(\"InstanceRequirement not yet implemented in PyLEMS!\")\n\n        ### Resolve component references.\n        for cref in ct.component_references:\n            if cref.local:\n                raise ModelError(\"Attribute local on ComponentReference not yet implemented in PyLEMS!\")\n            if cref.name in c.parameters:\n                cref2 = cref.copy()\n                cid = c.parameters[cref.name]\n\n                if cid not in self.fat_components:\n                    self.add(self.fatten_component(self.components[cid]))\n\n                cref2.referenced_component = self.fat_components[cid]\n                fc.add(cref2)\n            else:\n                raise ModelError(\"Component reference '{0}' not initialized for component '{1}'\",\n                                 cref.name, c.id)\n            \n        merge_maps(fc.exposures, ct.exposures)\n        merge_maps(fc.requirements, ct.requirements)\n        merge_maps(fc.component_requirements, ct.component_requirements)\n        merge_maps(fc.instance_requirements, ct.instance_requirements)\n        merge_maps(fc.children, ct.children)\n        merge_maps(fc.texts, ct.texts)\n        merge_maps(fc.links, ct.links)\n        merge_maps(fc.paths, ct.paths)\n        merge_maps(fc.event_ports, ct.event_ports)\n        merge_maps(fc.attachments, ct.attachments)\n\n        fc.dynamics = ct.dynamics.copy()\n        if len(fc.dynamics.regimes) != 0:\n            fc.dynamics.clear()\n               \n        self.resolve_structure(fc, ct)\n        self.resolve_simulation(fc, ct)\n\n        fc.types = ct.types\n\n        ### Resolve children\n        for child in c.children:\n            fc.add(self.fatten_component(child))\n\n        return fc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_parent_component(self, fc):\n        if self.debug: print(\"Looking for parent of %s (%s)\"%(fc.id, fc.parent_id)) \n        parent_comp = None\n        for comp in self.components.values():\n            if self.debug: print(\" - Checking \"+comp.id) \n            for child in comp.children:\n                if parent_comp == None:\n                    if child.id == fc.id and comp.id == fc.parent_id:\n                        if self.debug: print(\"1) It is \"+comp.id) \n                        parent_comp = comp\n                    else:\n                        for child2 in child.children:\n                            if self.debug: print(\"    - Checking child: %s, %s\"%(child.id,child2.id)) \n                            if parent_comp == None and child2.id == fc.id and child.id == fc.parent_id:\n                                if self.debug: print(\"2) It is \"+child.id) \n                                parent_comp = child\n                                break\n                            else:\n                                if self.debug: print(\"No...\"   )\n        return parent_comp", "response": "Get the parent component of a given component."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the numeric value for a parameter value specification.", "response": "def get_numeric_value(self, value_str, dimension = None):\n        \"\"\"\n        Get the numeric value for a parameter value specification.\n\n        @param value_str: Value string\n        @type value_str: str\n\n        @param dimension: Dimension of the value\n        @type dimension: str\n        \"\"\"\n\n        n = None\n        i = len(value_str)\n        while n is None:\n            try:\n                part = value_str[0:i]\n                nn = float(part)\n                n = nn\n                s = value_str[i:]\n            except ValueError:\n                i = i-1\n\n\n        number = n\n        sym = s\n\n        numeric_value = None\n\n        if sym == '':\n            numeric_value = number\n        else:\n            if sym in self.units:\n                unit = self.units[sym]\n                if dimension:\n                    if dimension != unit.dimension and dimension != '*':\n                        raise SimBuildError(\"Unit symbol '{0}' cannot \"\n                                            \"be used for dimension '{1}'\",\n                                            sym, dimension)\n                else:\n                    dimension = unit.dimension\n\n                numeric_value = (number * (10 ** unit.power) * unit.scale) + unit.offset\n            else:\n                raise SimBuildError(\"Unknown unit symbol '{0}'. Known: {1}\",\n                                    sym, self.units)\n                                    \n        #print(\"Have converted %s to value: %s, dimension %s\"%(value_str, numeric_value, dimension))                            \n        return numeric_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart the msstitch command line.", "response": "def start_msstitch(exec_drivers, sysargs):\n    \"\"\"Passed all drivers of executable, checks which command is passed to\n    the executable and then gets the options for a driver, parses them from\n    command line and runs the driver\"\"\"\n    parser = populate_parser(exec_drivers)\n    args = parser.parse_args(sysargs[1:])\n    args.func(**vars(args))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge dictionaries. Later keys overwrite.", "response": "def merged(*dicts, **kwargs):\n    \"\"\"\n    Merge dictionaries. Later keys overwrite.\n\n    .. code-block:: python\n\n        merged(dict(a=1), dict(b=2), c=3, d=1)\n\n    \"\"\"\n    if not dicts:\n        return Struct()\n    result = dict()\n    for d in dicts:\n        result.update(d)\n    result.update(kwargs)\n    struct_type = type(dicts[0])\n    return struct_type(**result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind ordering of derived parameters.", "response": "def order_derived_parameters(component):\n    \"\"\"\n    Finds ordering of derived_parameters.\n\n    @param component: Component containing derived parameters.\n    @type component: lems.model.component.Component\n\n    @return: Returns ordered list of derived parameters.\n    @rtype: list(string)\n\n    @raise SimBuildError: Raised when a proper ordering of derived\n    parameters could not be found.\n    \"\"\"\n\n    if len(component.derived_parameters) == 0:\n        return []\n    \n    ordering = []\n    dps = []\n    \n    for dp in component.derived_parameters:\n        dps.append(dp.name)\n            \n    maxcount = 5\n\n    count = maxcount\n\n    while count > 0 and dps != []:\n        count = count - 1\n\n        for dp1 in dps:\n            #exp_tree = regime.derived_variables[dv1].expression_tree\n            value = component.derived_parameters[dp1].value\n            found = False\n            for dp2 in dps:\n                if dp1 != dp2 and dp2 in value:\n                    found = True\n            if not found:\n                ordering.append(dp1)\n                del dps[dps.index(dp1)]\n                count = maxcount\n                break\n\n    if count == 0:\n        raise SimBuildError((\"Unable to find ordering for derived \"\n                             \"parameter in component '{0}'\").format(component))\n\n    #return ordering + dvsnoexp\n    return ordering"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all derived variables in a single virtual machine.", "response": "def order_derived_variables(regime):\n    \"\"\"\n    Finds ordering of derived_variables.\n\n    @param regime: Dynamics Regime containing derived variables.\n    @type regime: lems.model.dynamics.regime\n\n    @return: Returns ordered list of derived variables.\n    @rtype: list(string)\n\n    @raise SimBuildError: Raised when a proper ordering of derived\n    variables could not be found.\n    \"\"\"\n\n    ordering = []\n    dvs = []\n    dvsnoexp = []\n    maxcount = 5\n\n    for dv in regime.derived_variables:\n        if dv.expression_tree == None:\n            dvsnoexp.append(dv.name)\n        else:\n            dvs.append(dv.name)\n            \n    for dv in regime.conditional_derived_variables:\n        if len(dv.cases) == 0:\n            dvsnoexp.append(dv.name)\n        else:\n            dvs.append(dv.name)\n            \n\n    count = maxcount\n\n    while count > 0 and dvs != []:\n        count = count - 1\n\n        for dv1 in dvs:\n            if dv1 in regime.derived_variables:\n                dv = regime.derived_variables[dv1]\n            else:\n                dv = regime.conditional_derived_variables[dv1]\n                \n            found = False\n            if isinstance(dv, DerivedVariable):\n                exp_tree = dv.expression_tree\n                for dv2 in dvs:\n                    if dv1 != dv2 and is_var_in_exp_tree(dv2, exp_tree):\n                        found = True\n            else:\n                for case in dv.cases:\n                    for dv2 in dvs:\n                        if dv1 != dv2 and (is_var_in_exp_tree(dv2, case.condition_expression_tree) or\n                                           is_var_in_exp_tree(dv2, case.value_expression_tree)):\n                            found = True\n                            \n            if not found:\n                ordering.append(dv1)\n                del dvs[dvs.index(dv1)]\n                count = maxcount\n                break\n\n    if count == 0:\n        raise SimBuildError((\"Unable to find ordering for derived \"\n                             \"variables in regime '{0}'\").format(regime.name))\n\n    #return ordering + dvsnoexp\n    return dvsnoexp + ordering"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(self):\n\n        self.sim = Simulation()\n\n        for component_id in self.model.targets:\n            if component_id not in self.model.components:\n                raise SimBuildError(\"Unable to find target component '{0}'\",\n                                    component_id)\n            component = self.model.fat_components[component_id]\n\n            runnable = self.build_runnable(component)\n            self.sim.add_runnable(runnable)\n\n        return self.sim", "response": "Builds the simulation components from the model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding a runnable component from a component specification and add it to the simulation.", "response": "def build_runnable(self, component, parent = None, id_ = None):\n        \"\"\"\n        Build a runnable component from a component specification and add\n        it to the simulation.\n\n        @param component: Component specification\n        @type component: lems.model.component.FatComponent\n\n        @param parent: Parent runnable component.\n        @type parent: lems.sim.runnable.Runnable\n\n        @param id_: Optional id for therunnable. If it's not passed in,\n        the runnable will inherit the id of the component.\n\n        @raise SimBuildError: Raised when a component reference cannot be\n        resolved.\n        \"\"\"\n        if self.debug: print(\"++++++++ Calling build_runnable of %s with parent %s\"%(component, parent))\n\n        if id_ == None:\n            runnable = Runnable(component.id, component, parent)\n        else:\n            runnable = Runnable(id_, component, parent)\n\n        simulation = component.simulation\n\n        record_target_backup = self.current_record_target\n        data_output_backup = self.current_data_output\n\n        do = None\n        for d in simulation.data_displays:\n            do = d\n        if do == None:\n            for d in simulation.data_writers:\n                do = d\n\n        if do != None:\n            self.current_data_output = do\n\n        for parameter in component.parameters:\n            runnable.add_instance_variable(parameter.name, parameter.numeric_value)\n            \n        \n        for property in component.properties:\n            print(\"\\n\\n*****************************************************************\\n\\n\"+\n                  \"   Property element is not stable in PyLEMS yet, see https://github.com/LEMS/pylems/issues/16\\n\\n\"+\n                  \"   Used in: %s\\n\\n\"%property.toxml()+\n                  \"*****************************************************************\\n\\n\\n\")\n            runnable.add_instance_variable(property.name, property.default_value)\n\n        derived_parameter_code = []\n        \n        derived_parameter_ordering = order_derived_parameters(component)\n        \n        for dpn in derived_parameter_ordering:\n            derived_parameter = component.derived_parameters[dpn]\n            runnable.add_derived_variable(derived_parameter.name)\n            \n            expression = self.build_expression_from_tree(runnable,\n                                                        None,\n                                                        derived_parameter.expression_tree)\n            \n            derived_parameter_code += ['self.{0} = ({1})'.format(\n                        derived_parameter.name,\n                        expression)]\n            derived_parameter_code += ['self.{0}_shadow = ({1})'.format(\n                        derived_parameter.name,\n                        expression)]\n       \n        suffix = ''\n        runnable.add_method('update_derived_parameters' + suffix, ['self'],\n                            derived_parameter_code)\n            \n        for constant in component.constants:\n            runnable.add_instance_variable(constant.name, constant.numeric_value)\n\n        for text in component.texts:\n            runnable.add_text_variable(text.name, text.value)\n            \n        for link in component.links:\n            runnable.add_text_variable(link.name, link.value)\n\n        for ep in component.event_ports:\n            if ep.direction.lower() == 'in':\n                runnable.add_event_in_port(ep.name)\n            else:\n                runnable.add_event_out_port(ep.name)\n\n                \n        dynamics = component.dynamics\n        self.add_dynamics_1(component, runnable, dynamics, dynamics)\n\n        for regime in dynamics.regimes:\n            self.add_dynamics_1(component, runnable, regime, dynamics)\n            \n            if regime.initial:\n                runnable.current_regime = regime.name\n\n            rn = regime.name\n            if rn not in runnable.regimes:\n                runnable.add_regime(RunnableRegime(rn))\n            r = runnable.regimes[rn]\n            suffix = '_regime_' + rn\n\n            if runnable.__dict__.has_key('update_state_variables' + suffix): \n                  r.update_state_variables = runnable.__dict__['update_state_variables' + suffix]\n                  \n            if runnable.__dict__.has_key('update_derived_variables' + suffix): \n                r.update_derived_variables = runnable.__dict__['update_derived_variables' + suffix]\n            \n            if runnable.__dict__.has_key('run_startup_event_handlers' + suffix): \n                r.run_startup_event_handlers = runnable.__dict__['run_startup_event_handlers' + suffix]\n            \n            if runnable.__dict__.has_key('run_preprocessing_event_handlers' + suffix): \n                r.run_preprocessing_event_handlers = runnable.__dict__['run_preprocessing_event_handlers' + suffix]\n            \n            if runnable.__dict__.has_key('run_postprocessing_event_handlers' + suffix): \n                r.run_postprocessing_event_handlers = runnable.__dict__['run_postprocessing_event_handlers' + suffix]\n\n        self.process_simulation_specs(component, runnable, component.simulation)\n\n        for child in component.child_components:\n            child_runnable = self.build_runnable(child, runnable)\n            runnable.add_child(child.id, child_runnable)\n\n            for children in component.children:\n                #GG - These conditions need more debugging.\n                if children.type in child.types:\n                    runnable.add_child_typeref(children.type, child_runnable)\n                if children.multiple:\n                    if children.type in child.types:\n                        runnable.add_child_to_group(children.name, child_runnable)\n                else:\n                    if child_runnable.id == children.name:\n                        runnable.add_child_typeref(children.name, child_runnable)\n\n        for attachment in component.attachments:\n            runnable.make_attachment(attachment.type, attachment.name)\n\n        self.build_structure(component, runnable, component.structure)\n\n        dynamics = component.dynamics\n        self.add_dynamics_2(component, runnable,\n                            dynamics, dynamics)\n        for regime in dynamics.regimes:\n            self.add_dynamics_2(component, runnable, regime, dynamics)\n\n            if regime.name not in runnable.regimes:\n                runnable.add_regime(RunnableRegime(regime.name))\n            r = runnable.regimes[regime.name]\n            suffix = '_regime_' + regime.name\n\n            if runnable.__dict__.has_key('update_kinetic_scheme' + suffix): \n                r.update_kinetic_scheme = runnable.__dict__['update_kinetic_scheme' + suffix]\n\n        self.add_recording_behavior(component, runnable)\n\n        self.current_data_output = data_output_backup\n        self.current_record_target = record_target_backup\n\n        return runnable"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds event connections for a given component and structure.", "response": "def build_event_connections(self, component, runnable, structure):\n        \"\"\"\n        Adds event connections to a runnable component based on the structure\n        specifications in the component model.\n\n        @param component: Component model containing structure specifications.\n        @type component: lems.model.component.FatComponent\n\n        @param runnable: Runnable component to which structure is to be added.\n        @type runnable: lems.sim.runnable.Runnable\n\n        @param structure: The structure object to be used to add\n        structure code in the runnable component.\n        @type structure: lems.model.structure.Structure\n        \"\"\"\n        if self.debug: print(\"\\n++++++++ Calling build_event_connections of %s with runnable %s, parent %s\"%(component.id, runnable.id, runnable.parent))\n        # Process event connections\n        for ec in structure.event_connections:\n            if self.debug: print(ec.toxml())\n            source = runnable.parent.resolve_path(ec.from_)\n            target = runnable.parent.resolve_path(ec.to)\n            if ec.receiver:\n                receiver_template = self.build_runnable(ec.receiver,\n                                                            target)\n                                                            \n                #receiver = copy.deepcopy(receiver_template)\n                receiver = receiver_template.copy()\n                receiver.id = \"{0}__{1}__\".format(component.id,\n                                                  receiver_template.id)\n\n                if ec.receiver_container:\n                    target.add_attachment(receiver, ec.receiver_container)\n                target.add_child(receiver_template.id, receiver)\n                target = receiver\n            else:\n                source = runnable.resolve_path(ec.from_)\n                target = runnable.resolve_path(ec.to)\n\n            source_port = ec.source_port\n            target_port = ec.target_port\n\n            if not source_port:\n                if len(source.event_out_ports) == 1:\n                    source_port = source.event_out_ports[0]\n                else:\n                    raise SimBuildError((\"No source event port \"\n                                         \"uniquely identifiable\"\n                                         \" in '{0}'\").format(source.id))\n            if not target_port:\n                if len(target.event_in_ports) == 1:\n                    target_port = target.event_in_ports[0]\n                else:\n                    raise SimBuildError((\"No destination event port \"\n                                         \"uniquely identifiable \"\n                                         \"in '{0}'\").format(target))\n             \n            if self.debug: print(\"register_event_out_callback\\n   Source: %s, %s (port: %s) \\n   -> %s, %s (port: %s)\"%(source, id(source), source_port, target, id(target), target_port))\n            source.register_event_out_callback(\\\n                source_port, lambda: target.inc_event_in(target_port))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_structure(self, component, runnable, structure):\n        if self.debug: print(\"\\n++++++++ Calling build_structure of %s with runnable %s, parent %s\"%(component.id, runnable.id, runnable.parent))\n        \n        # Process single-child instantiations\n        for ch in structure.child_instances:\n            child_runnable = self.build_runnable(ch.referenced_component, runnable)\n            runnable.add_child(child_runnable.id, child_runnable)\n\n            runnable.add_child_typeref(ch.component, child_runnable)\n            \n        # Process multi-child instatiantions\n        for mi in structure.multi_instantiates:\n            template = self.build_runnable(mi.component,\n                                           runnable)\n\n            for i in range(mi.number):\n                #instance = copy.deepcopy(template)\n                instance = template.copy()\n                instance.id = \"{0}__{1}__{2}\".format(component.id,\n                                                     template.id,\n                                                     i)\n                runnable.array.append(instance)\n\n        # Process foreach statements\n        for fe in structure.for_eachs:\n            self.build_foreach(component, runnable, fe)\n        \n        self.build_event_connections(component, runnable, structure)", "response": "Builds the structure for a runnable component based on the structure specifications in the component model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_foreach(self, component, runnable, foreach, name_mappings = {}):\n        if self.debug: print(\"\\n++++++++ Calling build_foreach of %s with runnable %s, parent %s, name_mappings: %s\"%(component.id, runnable.id, runnable.parent, name_mappings))\n\n        target_array = runnable.resolve_path(foreach.instances)\n        \n        for target_runnable in target_array:\n            if self.debug: print(\"Applying contents of for_each to %s, as %s\"%(target_runnable.id, foreach.as_))\n            name_mappings[foreach.as_] = target_runnable\n\n            # Process foreach statements\n            for fe2 in foreach.for_eachs:\n                #print fe2.toxml()\n                target_array2 = runnable.resolve_path(fe2.instances)\n\n                for target_runnable2 in target_array2:\n                    name_mappings[fe2.as_] = target_runnable2\n                    self.build_foreach(component, runnable, fe2, name_mappings)\n\n            # Process event connections\n            for ec in foreach.event_connections:\n                source = name_mappings[ec.from_]\n                target = name_mappings[ec.to]\n\n                source_port = ec.source_port\n                target_port = ec.target_port\n\n                if not source_port:\n                    if len(source.event_out_ports) == 1:\n                        source_port = source.event_out_ports[0]\n                    else:\n                        raise SimBuildError((\"No source event port \"\n                                             \"uniquely identifiable\"\n                                             \" in '{0}'\").format(source.id))\n                if not target_port:\n                    if len(target.event_in_ports) == 1:\n                        target_port = target.event_in_ports[0]\n                    else:\n                        raise SimBuildError((\"No destination event port \"\n                                             \"uniquely identifiable \"\n                                             \"in '{0}'\").format(target))\n\n                if self.debug: print(\"register_event_out_callback\\n   Source: %s, %s (port: %s) \\n   -> %s, %s (port: %s)\"%(source, id(source), source_port, target, id(target), target_port))\n                source.register_event_out_callback(\\\n                    source_port, lambda: target.inc_event_in(target_port))", "response": "Iterate over ForEach constructs and process nested elements."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding dynamics to a runnable component based on the dynamics specifications in the component model. This method builds dynamics necessary for building child components. @param component: Component model containing dynamics specifications. @type component: lems.model.component.FatComponent @param runnable: Runnable component to which dynamics is to be added. @type runnable: lems.sim.runnable.Runnable @param regime: The dynamics regime to be used to generate dynamics code in the runnable component. @type regime: lems.model.dynamics.Regime @param dynamics: Shared dynamics specifications. @type dynamics: lems.model.dynamics.Regime @raise SimBuildError: Raised when a time derivative expression refers to an undefined variable. @raise SimBuildError: Raised when there are invalid time specifications for the <Run> statement. @raise SimBuildError: Raised when the component reference for <Run> cannot be resolved.", "response": "def add_dynamics_1(self, component, runnable, regime, dynamics):\n        \"\"\"\n        Adds dynamics to a runnable component based on the dynamics\n        specifications in the component model.\n\n        This method builds dynamics necessary for building child components.\n\n        @param component: Component model containing dynamics specifications.\n        @type component: lems.model.component.FatComponent\n\n        @param runnable: Runnable component to which dynamics is to be added.\n        @type runnable: lems.sim.runnable.Runnable\n\n        @param regime: The dynamics regime to be used to generate\n        dynamics code in the runnable component.\n        @type regime: lems.model.dynamics.Regime\n\n        @param dynamics: Shared dynamics specifications.\n        @type dynamics: lems.model.dynamics.Regime\n\n        @raise SimBuildError: Raised when a time derivative expression refers\n        to an undefined variable.\n\n        @raise SimBuildError: Raised when there are invalid time\n        specifications for the <Run> statement.\n\n        @raise SimBuildError: Raised when the component reference for <Run>\n        cannot be resolved.\n        \"\"\"\n\n        if isinstance(regime, Dynamics) or regime.name == '':\n            suffix = ''\n        else:\n            suffix = '_regime_' + regime.name\n\n        if isinstance(regime, Regime) and regime.initial:\n            runnable.new_regime = regime.name\n\n        # Process state variables\n        for sv in regime.state_variables:\n            runnable.add_instance_variable(sv.name, 0)\n\n        # Process time derivatives\n        time_step_code = []\n        for td in regime.time_derivatives:\n            if td.variable not in regime.state_variables and td.variable not in dynamics.state_variables:\n                raise SimBuildError(('Time derivative for undefined state '\n                                     'variable {0} in component {1}').format(td.variable, component.id))\n\n            exp = self.build_expression_from_tree(runnable,\n                                                  regime,\n                                                  td.expression_tree)\n            time_step_code += ['self.{0} += dt * ({1})'.format(td.variable,\n                                                               exp)]\n        runnable.add_method('update_state_variables' + suffix, ['self', 'dt'],\n                            time_step_code)\n\n        # Process derived variables\n        derived_variable_code = []\n        derived_variables_ordering = order_derived_variables(regime)\n        for dvn in derived_variables_ordering: #regime.derived_variables:\n            if dvn in dynamics.derived_variables:\n                dv = dynamics.derived_variables[dvn]\n                runnable.add_derived_variable(dv.name)\n                if dv.value:\n                    derived_variable_code += ['self.{0} = ({1})'.format(\n                        dv.name,\n                        self.build_expression_from_tree(runnable,\n                                                        regime,\n                                                        dv.expression_tree))]\n                elif dv.select:\n                    if dv.reduce:\n                        derived_variable_code += self.build_reduce_code(dv.name,\n                                                                        dv.select,\n                                                                        dv.reduce)\n                    else:\n                        derived_variable_code += ['self.{0} = (self.{1})'.format(\n                            dv.name,\n                            dv.select.replace('/', '.'))]\n                else:\n                    raise SimBuildError(('Inconsistent derived variable settings'\n                                         'for {0}').format(dvn))\n            elif dvn in dynamics.conditional_derived_variables:\n                dv = dynamics.conditional_derived_variables[dvn]\n                runnable.add_derived_variable(dv.name)\n                derived_variable_code += self.build_conditional_derived_var_code(runnable,\n                                                                                 regime,\n                                                                                 dv)\n            else:\n                raise SimBuildError(\"Unknown derived variable '{0}' in '{1}'\",\n                                     dvn, runnable.id)\n        runnable.add_method('update_derived_variables' + suffix, ['self'],\n                            derived_variable_code)\n\n        # Process event handlers\n        pre_event_handler_code = []\n        post_event_handler_code = []\n        startup_event_handler_code = []\n        on_entry_added = False\n        for eh in regime.event_handlers:\n            if isinstance(eh, OnStart):\n                startup_event_handler_code += self.build_event_handler(runnable,\n                                                                       regime,\n                                                                       eh)\n            elif isinstance(eh, OnCondition):\n                post_event_handler_code += self.build_event_handler(runnable,\n                                                                    regime,\n                                                                    eh)\n            else:\n                if isinstance(eh, OnEntry):\n                    on_entry_added = True\n                pre_event_handler_code += self.build_event_handler(runnable,\n                                                                   regime,\n                                                                   eh)\n        if isinstance(regime, Regime) and not on_entry_added:\n            pre_event_handler_code += self.build_event_handler(runnable, regime, OnEntry())\n            \n        runnable.add_method('run_startup_event_handlers' + suffix, ['self'],\n                            startup_event_handler_code)\n        runnable.add_method('run_preprocessing_event_handlers' + suffix, ['self'],\n                            pre_event_handler_code)\n        runnable.add_method('run_postprocessing_event_handlers' + suffix, ['self'],\n                            post_event_handler_code)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds dynamics to a runnable component based on the dynamics specifications in the component model. This method builds dynamics dependent on child components. @param component: Component model containing dynamics specifications. @type component: lems.model.component.FatComponent @param runnable: Runnable component to which dynamics is to be added. @type runnable: lems.sim.runnable.Runnable @param regime: The dynamics regime to be used to generate dynamics code in the runnable component. @type regime: lems.model.dynamics.Regime @param dynamics: Shared dynamics specifications. @type dynamics: lems.model.dynamics.Regime @raise SimBuildError: Raised when a time derivative expression refers to an undefined variable. @raise SimBuildError: Raised when there are invalid time specifications for the <Run> statement. @raise SimBuildError: Raised when the component reference for <Run> cannot be resolved.", "response": "def add_dynamics_2(self, component, runnable, regime, dynamics):\n        \"\"\"\n        Adds dynamics to a runnable component based on the dynamics\n        specifications in the component model.\n\n        This method builds dynamics dependent on child components.\n\n        @param component: Component model containing dynamics specifications.\n        @type component: lems.model.component.FatComponent\n\n        @param runnable: Runnable component to which dynamics is to be added.\n        @type runnable: lems.sim.runnable.Runnable\n\n        @param regime: The dynamics regime to be used to generate\n        dynamics code in the runnable component.\n        @type regime: lems.model.dynamics.Regime\n\n        @param dynamics: Shared dynamics specifications.\n        @type dynamics: lems.model.dynamics.Regime\n\n        @raise SimBuildError: Raised when a time derivative expression refers\n        to an undefined variable.\n\n        @raise SimBuildError: Raised when there are invalid time\n        specifications for the <Run> statement.\n\n        @raise SimBuildError: Raised when the component reference for <Run>\n        cannot be resolved.\n        \"\"\"\n\n        if isinstance(regime, Dynamics) or regime.name == '':\n            suffix = ''\n        else:\n            suffix = '_regime_' + regime.name\n\n        # Process kinetic schemes\n        ks_code = []\n        for ks in regime.kinetic_schemes:\n            \n            raise NotImplementedError(\"KineticScheme element is not stable in PyLEMS yet, see https://github.com/LEMS/pylems/issues/15\")\n        \n            try:\n                ###nodes = {node.id:node for node in runnable.__dict__[ks.nodes]}\n                nodes = {}\n                for node in runnable.__dict__[ks.nodes]:\n                    nodes[node.id] = node\n                edges = runnable.__dict__[ks.edges]\n\n                for edge in edges:\n                    from_ = edge.__dict__[ks.edge_source]\n                    to = edge.__dict__[ks.edge_target]\n\n                    ks_code += [('self.{0}.{2} += dt * (-self.{3}.{4} * self.{0}.{2}_shadow'\n                                 ' + self.{3}.{5} * self.{1}.{2}_shadow)').format(\n                        from_, to, ks.state_variable, edge.id,\n                        ks.forward_rate, ks.reverse_rate)]\n\n                    ks_code += [('self.{1}.{2} += dt * (self.{3}.{4} * self.{0}.{2}_shadow'\n                                 ' - self.{3}.{5} * self.{1}.{2}_shadow)').format(\n                        from_, to, ks.state_variable, edge.id,\n                        ks.forward_rate, ks.reverse_rate)]\n\n                ks_code += ['sum = 0']\n                for node in nodes:\n                    nodes[node].__dict__[ks.state_variable] = 1.0 / len(nodes)\n                    nodes[node].__dict__[ks.state_variable + '_shadow'] = 1.0 / len(nodes)\n                    ks_code += ['sum += self.{0}.{1}'.format(node, ks.state_variable)]\n\n                for node in nodes:\n                    ks_code += ['self.{0}.{1} /= sum'.format(node, ks.state_variable)]\n\n                for node in nodes:\n                    ks_code += [('self.{0}.{1}_shadow = '\n                                 'self.{0}.{1}').format(node,\n                                                        ks.state_variable)]\n\n            except Exception as e:\n                raise SimBuildError((\"Unable to construct kinetic scheme '{0}' \"\n                                     \"for component '{1}' - {2}\").format(ks.name,\n                                                                         component.id,\n                                                                         str(e)))\n\n        runnable.add_method('update_kinetic_scheme' + suffix, ['self', 'dt'],\n                            ks_code)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_simulation_specs(self, component, runnable, simulation):\n\n        # Process runs\n        for run in simulation.runs:\n            cid = run.component.id + '_' + component.id\n\n            target = self.build_runnable(run.component, runnable, cid)\n            self.sim.add_runnable(target)\n            self.current_record_target = target\n\n            target.configure_time(run.increment,\n                                  run.total)", "response": "Processes the simulation - related aspects of a runnable component based on the dynamics specifications in the component model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting NeuroML arithmetic or logical operator to python equivalents.", "response": "def convert_op(self, op):\n        \"\"\"\n        Converts NeuroML arithmetic/logical operators to python equivalents.\n\n        @param op: NeuroML operator\n        @type op: string\n\n\n        @return: Python operator\n        @rtype: string\n        \"\"\"\n\n        if op == '.gt.':\n            return '>'\n        elif op == '.ge.' or op == '.geq.':\n            return '>='\n        elif op == '.lt.':\n            return '<'\n        elif op == '.le.':\n            return '<='\n        elif op == '.eq.':\n            return '=='\n        elif op == '.neq.':   \n            return '!='\n        elif op == '.ne.':   # .neq. is preferred!\n            return '!='\n        elif op == '^':\n            return '**'\n        elif op == '.and.':\n            return 'and'\n        elif op == '.or.':\n            return 'or'\n        else:\n            return op"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert NeuroML arithmetic or logical functions to python equivalents.", "response": "def convert_func(self, func):\n        \"\"\"\n        Converts NeuroML arithmetic/logical functions to python equivalents.\n\n        @param func: NeuroML function\n        @type func: string\n\n\n        @return: Python operator\n        @rtype: string\n        \"\"\"\n\n        if func == 'ln':\n            return 'log'\n        elif func == 'random':\n            return 'random.uniform'\n        elif func == 'H':\n            def heaviside_step(x):\n                if x < 0: return 0\n                elif x > 0: return 1\n                else: return 0.5 \n            return 'heaviside_step'\n        else:\n            return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbuilds the event handler code.", "response": "def build_event_handler(self, runnable, regime, event_handler):\n        \"\"\"\n        Build event handler code.\n\n        @param event_handler: Event handler object\n        @type event_handler: lems.model.dynamics.EventHandler\n\n        @return: Generated event handler code.\n        @rtype: list(string)\n        \"\"\"\n\n        if isinstance(event_handler, OnCondition):\n            return self.build_on_condition(runnable, regime, event_handler)\n        elif isinstance(event_handler, OnEvent):\n            return self.build_on_event(runnable, regime, event_handler)\n        elif isinstance(event_handler, OnStart):\n            return self.build_on_start(runnable, regime, event_handler)\n        elif isinstance(event_handler, OnEntry):\n            return self.build_on_entry(runnable, regime, event_handler)\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_on_condition(self, runnable, regime, on_condition):\n\n        on_condition_code = []\n\n        on_condition_code += ['if {0}:'.format(\\\n            self.build_expression_from_tree(runnable,\n                                            regime,\n                                            on_condition.expression_tree))]\n\n        for action in on_condition.actions:\n            code = self.build_action(runnable, regime, action)\n            for line in code:\n                on_condition_code += ['    ' + line]\n\n        return on_condition_code", "response": "Build OnCondition event handler code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_on_event(self, runnable, regime, on_event):\n        on_event_code = []\n\n        if self.debug: on_event_code += ['print(\"Maybe handling something for %s (\"+str(id(self))+\")\")'%(runnable.id),\n                          'print(\"EICs (\"+str(id(self))+\"): \"+str(self.event_in_counters))']\n                          \n        on_event_code += ['count = self.event_in_counters[\\'{0}\\']'.\\\n                          format(on_event.port),\n                          'while count > 0:',\n                          '    print(\"  Handling event\")' if self.debug else '',\n                          '    count -= 1']\n        for action in on_event.actions:\n            code = self.build_action(runnable, regime, action)\n            for line in code:\n                on_event_code += ['    ' + line]\n\n        on_event_code += ['self.event_in_counters[\\'{0}\\'] = 0'.\\\n                          format(on_event.port),]\n\n        return on_event_code", "response": "Build OnEvent event handler code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding OnStart handler code.", "response": "def build_on_start(self, runnable, regime, on_start):\n        \"\"\"\n        Build OnStart start handler code.\n\n        @param on_start: OnStart start handler object\n        @type on_start: lems.model.dynamics.OnStart\n\n        @return: Generated OnStart code\n        @rtype: list(string)\n        \"\"\"\n\n        on_start_code = []\n\n        for action in on_start.actions:\n            code = self.build_action(runnable, regime, action)\n            for line in code:\n                on_start_code += [line]\n\n        return on_start_code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_on_entry(self, runnable, regime, on_entry):\n\n        on_entry_code = []\n\n        on_entry_code += ['if self.current_regime != self.last_regime:']\n        on_entry_code += ['    self.last_regime = self.current_regime']\n\n        for action in on_entry.actions:\n            code = self.build_action(runnable, regime, action)\n            for line in code:\n                on_entry_code += ['    ' + line]\n\n        return on_entry_code", "response": "Build OnEntry start handler code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding event handler action code.", "response": "def build_action(self, runnable, regime, action):\n        \"\"\"\n        Build event handler action code.\n\n        @param action: Event handler action object\n        @type action: lems.model.dynamics.Action\n\n        @return: Generated action code\n        @rtype: string\n        \"\"\"\n\n        if isinstance(action, StateAssignment):\n            return self.build_state_assignment(runnable, regime, action)\n        if isinstance(action, EventOut):\n            return self.build_event_out(action)\n        if isinstance(action, Transition):\n            return self.build_transition(action)\n        else:\n            return ['pass']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_state_assignment(self, runnable, regime, state_assignment):\n\n        return ['self.{0} = {1}'.format(\\\n            state_assignment.variable,\n            self.build_expression_from_tree(runnable,\n                                            regime,\n                                            state_assignment.expression_tree))]", "response": "Build state assignment code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_event_out(self, event_out):\n\n        event_out_code = ['if \"{0}\" in self.event_out_callbacks:'.format(event_out.port),\n                          '    for c in self.event_out_callbacks[\\'{0}\\']:'.format(event_out.port),\n                          '        c()']\n\n        return event_out_code", "response": "Build event out code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_reduce_code(self, result, select, reduce):\n\n        select = select.replace('/', '.')\n        select = select.replace(' ', '')\n        if reduce == 'add':\n            reduce_op = '+'\n            acc_start = 0\n        else:\n            reduce_op = '*'\n            acc_start = 1\n\n        #bits = select.split('[*]')\n        bits = re.split('\\[.*\\]', select)\n        seps = re.findall('\\[.*\\]', select)\n\n        code = ['self.{0} = {1}'.format(result, acc_start)]\n        code += ['self.{0}_shadow = {1}'.format(result, acc_start)]\n\n        code += ['try:']\n\n        if len(bits) == 1:\n            target = select\n            code += ['    self.{0} = self.{1}'.format(result, target)]\n            code += ['    self.{0}_shadow = self.{1}'.format(result, target)]\n        elif len(bits) == 2:\n            sep = seps[0][1:-1]\n\n            if sep == '*':\n                array = bits[0]\n                ref = bits[1]\n\n                code += ['    acc = {0}'.format(acc_start)]\n                code += ['    for o in self.{0}:'.format(array)]\n                code += ['        acc = acc {0} o{1}'.format(reduce_op, ref)]\n                code += ['    self.{0} = acc'.format(result)]\n                code += ['    self.{0}_shadow = acc'.format(result)]\n            else:\n                bits2 = sep.split('=')\n                if len(bits2) > 1:\n                    array = bits[0]\n                    ref = bits[1]\n\n                    code += ['    acc = {0}'.format(acc_start)]\n                    code += ['    for o in self.{0}:'.format(array)]\n                    code += ['        if o.{0} == {1}:'.format(bits2[0], bits2[1])]\n                    code += ['            acc = acc {0} o{1}'.format(reduce_op, ref)]\n                    code += ['    self.{0} = acc'.format(result)]\n                    code += ['    self.{0}_shadow = acc'.format(result)]\n                else:\n                    raise SimbuildError(\"Invalid reduce target - '{0}'\".format(select))\n        else:\n            raise SimbuildError(\"Invalid reduce target - '{0}'\".format(select))\n\n        code += ['except:']\n        code += ['    pass']\n\n        return code", "response": "Builds a reduce code for the selected target range."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_recording_behavior(self, component, runnable):\n\n        simulation = component.simulation\n\n        for rec in simulation.records:\n            rec.id = runnable.id\n            self.current_record_target.add_variable_recorder(self.current_data_output, rec)", "response": "Adds recording - related dynamics to a runnable component based on the dynamics specifications in the component model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the internal call graph for a given file", "response": "def get_internal_call_graph(fpath, with_doctests=False):\n    \"\"\"\n    CommandLine:\n        python -m utool.util_inspect get_internal_call_graph --show --modpath=~/code/ibeis/ibeis/init/main_helpers.py --show\n        python -m utool.util_inspect get_internal_call_graph --show --modpath=~/code/dtool/dtool/depcache_table.py --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_inspect import *  # NOQA\n        >>> import utool as ut\n        >>> fpath = ut.get_argval('--modpath', default='.')\n        >>> with_doctests = ut.get_argflag('--with_doctests')\n        >>> G = get_internal_call_graph(fpath, with_doctests)\n        >>> ut.quit_if_noshow()\n        >>> import plottool as pt\n        >>> pt.qt4ensure()\n        >>> pt.show_nx(G, fontsize=8, as_directed=False)\n        >>> z = pt.zoom_factory()\n        >>> p = pt.pan_factory()\n        >>> ut.show_if_requested()\n    \"\"\"\n    import utool as ut\n    fpath = ut.truepath(fpath)\n    sourcecode = ut.readfrom(fpath)\n    self = ut.BaronWraper(sourcecode)\n    G = self.internal_call_graph(with_doctests=with_doctests)\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks static member variables of a live object.", "response": "def check_static_member_vars(class_, fpath=None, only_init=True):\n    \"\"\"\n    class_ can either be live object or a classname\n\n    # fpath = ut.truepath('~/code/ibeis/ibeis/viz/viz_graph2.py')\n    # classname = 'AnnotGraphWidget'\n    \"\"\"\n    #import ast\n    #import astor\n    import utool as ut\n\n    if isinstance(class_, six.string_types):\n        classname = class_\n        if fpath is None:\n            raise Exception('must specify fpath')\n    else:\n        # We were given a live object\n        if not isinstance(class_, type):\n            # We were given the class instance not the class\n            class_instance = class_\n            class_ = class_instance.__class__\n        classname = class_.__name__\n        if fpath is None:\n            module = ut.get_module_from_class(class_)\n            fpath = ut.get_modpath(module)\n\n    sourcecode = ut.readfrom(fpath)\n\n    import redbaron\n    # Pares a FULL syntax tree that keeps blockcomments\n    baron = redbaron.RedBaron(sourcecode)\n\n    for node in baron:\n        if node.type == 'class' and node.name == classname:\n            classnode = node\n            break\n\n    def find_parent_method(node):\n        par = node.parent_find('def')\n        if par is not None and par.parent is not None:\n            if par.parent.type == 'class':\n                return par\n            else:\n                return find_parent_method(par)\n\n    # TODO: Find inherited attrs\n    #classnode.inherit_from\n    # inhertied_attrs = ['parent']\n    # inhertied_attrs = []\n\n    class_methods = []\n    for node in classnode:\n        if node.type == 'def':\n            if only_init:\n                if node.name == '__init__':\n                    class_methods.append(node)\n            else:\n                class_methods.append(node)\n\n    class_vars = []\n    self_vars = []\n    for method_node in class_methods:\n        self_var = method_node.arguments[0].dumps()\n        self_vars.append(self_var)\n        for assign in method_node.find_all('assignment'):\n            # method_node = find_parent_method(assign)\n            if assign.target.dumps().startswith(self_var + '.'):\n                class_vars.append(assign.target.value[1].dumps())\n    static_attrs = ut.unique(class_vars)\n    return static_attrs\n\n    # class_members = ut.unique(class_vars + class_methods + inhertied_attrs)\n    if False:\n        self_var = self_vars[0]\n\n        # Find everything that is used\n        complex_cases = []\n        simple_cases = []\n        all_self_ref = classnode.find_all(\n            'name_', value=re.compile('.*' + self_var + '\\\\.*'))\n        for x in all_self_ref:\n            if x.parent.type == 'def_argument':\n                continue\n            if x.parent.type == 'atomtrailers':\n                atom = x.parent\n                if ut.depth(atom.fst()) <= 3:\n                    simple_cases.append(atom)\n                else:\n                    complex_cases.append(atom)\n                #print(ut.depth(atom.value.data))\n                #print(atom.value)\n                #print(atom.dumps())\n                #if len(atom.dumps()) > 200:\n                #    break\n\n        accessed_attrs = []\n        for x in simple_cases:\n            if x.value[0].dumps() == self_var:\n                attr = x.value[1].dumps()\n                accessed_attrs.append(attr)\n        accessed_attrs = ut.unique(accessed_attrs)\n\n        ut.setdiff(accessed_attrs, class_vars)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all functions defined in a module", "response": "def get_funcnames_from_modpath(modpath, include_methods=True):\n    \"\"\"\n    Get all functions defined in module\n    \"\"\"\n    import utool as ut\n    if True:\n        import jedi\n        source = ut.read_from(modpath)\n        #script = jedi.Script(source=source, source_path=modpath, line=source.count('\\n') + 1)\n        definition_list = jedi.names(source)\n        funcname_list = [definition.name for definition in definition_list if definition.type == 'function']\n        if include_methods:\n            classdef_list = [definition for definition in definition_list if definition.type == 'class']\n            defined_methods = ut.flatten([definition.defined_names() for definition in classdef_list])\n            funcname_list += [method.name for method in defined_methods\n                              if method.type == 'function' and not method.name.startswith('_')]\n    else:\n        import redbaron\n        # Pares a FULL syntax tree that keeps blockcomments\n        sourcecode = ut.read_from(modpath)\n        baron = redbaron.RedBaron(sourcecode)\n        funcname_list = [node.name for node in baron.find_all('def', recursive=include_methods)\n                         if not node.name.startswith('_')]\n    return funcname_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_module_usage(modpath_patterns):\n    import utool as ut\n    #dpath = '~/code/ibeis/ibeis/algo/hots'\n    modpaths = ut.flatten([ut.glob_projects(pat) for pat in modpath_patterns])\n    modpaths = ut.unique(modpaths)\n    modnames = ut.lmap(ut.get_modname_from_modpath, modpaths)\n    print('Checking usage of modules: ' + ut.repr3(modpaths))\n\n    # Mark as True is module is always explicitly imported\n    restrict_to_importing_modpaths = False\n    cache = {}\n\n    def find_where_module_is_imported(modname):\n        \"\"\" finds where a module was explicitly imported. (in most scenareos) \"\"\"\n        # Find places where the module was imported\n        patterns = ut.possible_import_patterns(modname)\n        # do modname grep with all possible import patterns\n        grepres = ut.grep_projects(patterns, new=True, verbose=False, cache=cache)\n        return grepres.found_fpath_list\n\n    def find_function_callers(funcname, importing_modpaths):\n        \"\"\" searches for places where a function is used \"\"\"\n        pattern = '\\\\b' + funcname + '\\\\b',\n        # Search which module uses each public member\n        grepres = ut.grep_projects(\n            pattern, new=True, verbose=False, cache=cache,\n            fpath_list=importing_modpaths)\n        # Exclude places where function is defined or call is commented out\n        nohit_patterns = [\n            r'^\\s*def',\n            r'^\\s*#',\n            r'\\-\\-exec\\-',\n            r'\\-\\-test-',\n            r'^\\s*python -m ',\n            r'^\\s*python -m ibeis ',\n            r'^\\s*ibeis ',\n            r'\\-\\-test\\-[a-zA-z]*\\.',\n            r'\\-\\-exec\\-[a-zA-z]*\\.',\n        ]\n        nohit_patterns += [\n            r'^\\s*\\>\\>\\>',\n        ]\n        filter_pat = ut.regex_or(nohit_patterns)\n        # import copy\n        # grepres_ = copy.deepcopy(grepres)\n        grepres.inplace_filter_results(filter_pat)\n        grepres.found_modnames = ut.lmap(ut.get_modname_from_modpath,\n                                         grepres.found_fpath_list)\n        parent_numlines = ut.lmap(len, grepres.found_lines_list)\n\n        numcall_graph_ = dict(zip(grepres.found_modnames, parent_numlines))\n        # Remove self references\n        #ut.delete_keys(numcall_graph_, modnames)\n        return numcall_graph_, grepres\n\n    print('Find modules that use this the query modules')\n    # Note: only works for explicit imports\n    importing_modpaths_list = [find_where_module_is_imported(modname) for modname in modnames]\n    print('Find members of the query modules')\n    funcnames_list = [get_funcnames_from_modpath(modpath) for modpath in modpaths]\n\n    print('Building call graph')\n    cache = {}\n    func_numcall_graph = ut.ddict(dict)\n    grep_results = ut.ddict(dict)\n    # Extract public members from each module\n    exclude_self = ut.get_argflag('--exclude-self')\n    _iter = list(zip(modnames, modpaths, importing_modpaths_list, funcnames_list))\n    _iter = ut.ProgIter(_iter, lbl='Searching query module', bs=False)\n    for modname, modpath, importing_modpaths, funcname_list in _iter:\n        if not restrict_to_importing_modpaths:\n            importing_modpaths = None\n\n        # Search for each function in modpath\n        for funcname in ut.ProgIter(funcname_list, lbl='Searching funcs in query module'):\n            numcall_graph_, grepres = find_function_callers(funcname, importing_modpaths)\n            grep_results[modname][funcname] = grepres\n            if exclude_self:\n                if modname in numcall_graph_:\n                    del numcall_graph_[modname]\n            func_numcall_graph[modname][funcname] = numcall_graph_\n\n    # Sort by incidence cardinality\n    # func_numcall_graph = ut.odict([(key, ut.sort_dict(val, 'vals', len)) for key, val in func_numcall_graph.items()])\n    # Sort by weighted degree\n    func_numcall_graph = ut.odict([(key, ut.sort_dict(val, 'vals', lambda x: sum(x.values())))\n                                   for key, val in func_numcall_graph.items()])\n    # Print out grep results in order\n    print('PRINTING GREP RESULTS IN ORDER')\n    for modname, num_callgraph in func_numcall_graph.items():\n        print('\\n============\\n')\n        for funcname in num_callgraph.keys():\n            print('\\n============\\n')\n            with ut.Indenter('[%s]' % (funcname,)):\n                grepres = grep_results[modname][funcname]\n                print(grepres)\n                # print(func_numcall_graph[modname][funcname])\n    print('PRINTING NUMCALLGRAPH IN ORDER')\n    # Print out callgraph in order\n    print('func_numcall_graph = %s' % (ut.repr3(func_numcall_graph),))\n\n    # importance_dict = {}\n    # import copy\n    # func_call_graph2 = copy.deepcopy(func_numcall_graph)\n    # #ignore_modnames = []\n    # ignore_modnames = ['ibeis.algo.hots.multi_index', 'ibeis.algo.hots._neighbor_experiment']\n    # num_callers = ut.ddict(dict)\n    # for modname, modpath in list(zip(modnames, modpaths)):\n    #     subdict = func_call_graph2[modname]\n    #     for funcname in subdict.keys():\n    #         numcall_graph_ = subdict[funcname]\n    #         ut.delete_keys(numcall_graph_, modnames)\n    #         ut.delete_keys(numcall_graph_, ignore_modnames)\n    #         num_callers[modname][funcname] = sum(numcall_graph_.values())\n    #     print(ut.repr4(num_callers[modname], sorted_=True, key_order_metric='val'))\n\n    # # Check external usage\n    # unused_external = []\n    # grep_results2 = copy.deepcopy(grep_results)\n    # for modname, grepres_subdict in grep_results2.items():\n    #     for funcname, grepres_ in grepres_subdict.items():\n    #         idxs = ut.find_list_indexes(grepres_.found_modnames, modnames)\n    #         idxs += ut.find_list_indexes(grepres_.found_modnames, ignore_modnames)\n    #         idxs = list(ut.filter_Nones(idxs))\n    #         ut.delete_items_by_index(grepres_, idxs)\n    #         ut.delete_items_by_index(grepres_.found_modnames, idxs)\n    #         if len(grepres_) > 0:\n    #             print(grepres_.make_resultstr())\n    #         else:\n    #             unused_external += [funcname]\n\n    # print('internal grep')\n    # # Check internal usage\n    # unused_internal = []\n    # grep_results2 = copy.deepcopy(grep_results)\n    # for modname, grepres_subdict in grep_results2.items():\n    #     for funcname, grepres_ in grepres_subdict.items():\n    #         idxs = ut.filter_Nones(ut.find_list_indexes(grepres_.found_modnames, [modname]))\n    #         idxs_ = ut.index_complement(idxs, len(grepres_.found_modnames))\n    #         ut.delete_items_by_index(grepres_, idxs_)\n    #         ut.delete_items_by_index(grepres_.found_modnames, idxs_)\n    #         grepres_.hack_remove_pystuff()\n    #         #self = grepres_\n    #         if len(grepres_) > 0:\n    #             #print(modname)\n    #             #print(funcname)\n    #             #print(grepres_.extended_regex_list)\n    #             print(grepres_.make_resultstr())\n    #         else:\n    #             unused_internal += [funcname]\n\n    # # HACK: how to write ut.parfor\n    # # returns a 0 lenth iterator so the for loop is never run. Then uses code\n    # # introspection to determine the content of the for loop body executes code\n    # # using the values of the local variables in a parallel / distributed\n    # # context.\n\n    # for modname, modpath in zip(modnames, modpaths):\n    #     pattern = '\\\\b' + modname + '\\\\b',\n    #     grepres = ut.grep_projects(pattern, new=True, verbose=False, cache=cache)\n    #     parent_modnames = ut.lmap(ut.get_modname_from_modpath, grepres.found_fpath_list)\n    #     parent_numlines = ut.lmap(len, grepres.found_lines_list)\n    #     importance = dict(zip(parent_modnames, parent_numlines))\n    #     ut.delete_keys(importance, modnames)\n    #     importance_dict[modname] = importance\n\n    # print('importance_dict = %s' % (ut.repr3(importance_dict),))\n    # combo = reduce(ut.dict_union, importance_dict.values())\n    # print('combined %s' % (ut.repr3(combo),))\n    # print(ut.repr3(found_fpath_list))\n    pass", "response": "This function checks the usage of a set of modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all methods belonging to an object instance specified in by the __dir__", "response": "def get_object_methods(obj):\n    \"\"\"\n    Returns all methods belonging to an object instance specified in by the\n    __dir__ function\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_inspect import *  # NOQA\n        >>> import utool as ut\n        >>> obj = ut.NiceRepr()\n        >>> methods1 = ut.get_object_methods()\n        >>> ut.inject_func_as_method(obj, ut.get_object_methods)\n        >>> methods2 = ut.get_object_methods()\n        >>> assert ut.get_object_methods in methods2\n    \"\"\"\n    import utool as ut\n    attr_list = (getattr(obj, attrname) for attrname in dir(obj))\n    methods = [attr for attr in attr_list if ut.is_method(attr)]\n    return methods"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef help_members(obj, use_other=False):\n    import utool as ut\n    attrnames = dir(obj)\n    attr_list = [getattr(obj, attrname) for attrname in attrnames]\n    attr_types = ut.lmap(ut.type_str, map(type, attr_list))\n    unique_types, groupxs = ut.group_indices(attr_types)\n    type_to_items = ut.dzip(unique_types, ut.apply_grouping(attr_list, groupxs))\n    type_to_itemname = ut.dzip(unique_types, ut.apply_grouping(attrnames, groupxs))\n    #if memtypes is None:\n    #    memtypes = list(type_to_items.keys())\n    memtypes = ['instancemethod']  # , 'method-wrapper']\n    func_mems = ut.dict_subset(type_to_items, memtypes, [])\n\n    func_list = ut.flatten(func_mems.values())\n    defsig_list = []\n    num_unbound_args_list = []\n    num_args_list = []\n    for func in func_list:\n        #args = ut.get_func_argspec(func).args\n        argspec = ut.get_func_argspec(func)\n        args = argspec.args\n        unbound_args = get_unbound_args(argspec)\n        defsig = ut.func_defsig(func)\n        defsig_list.append(defsig)\n        num_unbound_args_list.append(len(unbound_args))\n        num_args_list.append(len(args))\n\n    group = ut.hierarchical_group_items(defsig_list, [num_unbound_args_list, num_args_list])\n    print(repr(obj))\n    print(ut.repr3(group, strvals=True))\n\n    if use_other:\n        other_mems = ut.delete_keys(type_to_items.copy(), memtypes)\n        other_mems_attrnames = ut.dict_subset(type_to_itemname, other_mems.keys())\n        named_other_attrs = ut.dict_union_combine(other_mems_attrnames, other_mems, lambda x, y: list(zip(x, y)))\n        print(ut.repr4(named_other_attrs, nl=2, strvals=True))", "response": "r Help for the members of a class or module."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_module_owned_functions(module):\n    import utool as ut\n    list_ = []\n    for key, val in ut.iter_module_doctestable(module):\n        belongs = False\n        if hasattr(val, '__module__'):\n            belongs = val.__module__ == module.__name__\n        elif hasattr(val, 'func_globals'):\n            belongs = val.func_globals['__name__'] == module.__name__\n        if belongs:\n            list_.append(val)\n    return list_", "response": "Returns a list of functions actually owned by the module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_defined_by_module(item, module, parent=None):\n    flag = False\n    if isinstance(item, types.ModuleType):\n        if not hasattr(item, '__file__'):\n            try:\n                # hack for cv2 and xfeatures2d\n                import utool as ut\n                name = ut.get_modname_from_modpath(module.__file__)\n                flag = name in str(item)\n            except:\n                flag = False\n        else:\n            item_modpath = os.path.realpath(dirname(item.__file__))\n            mod_fpath = module.__file__.replace('.pyc', '.py')\n            if not mod_fpath.endswith('__init__.py'):\n                flag = False\n            else:\n                modpath = os.path.realpath(dirname(mod_fpath))\n                modpath = modpath.replace('.pyc', '.py')\n                flag = item_modpath.startswith(modpath)\n    elif hasattr(item, '_utinfo'):\n        # Capture case where there is a utool wrapper\n        orig_func = item._utinfo['orig_func']\n        flag = is_defined_by_module(orig_func, module, parent)\n    else:\n        if isinstance(item, staticmethod):\n            # static methods are a wrapper around a function\n            item = item.__func__\n        try:\n            func_globals = meta_util_six.get_funcglobals(item)\n            func_module_name = func_globals['__name__']\n            if func_module_name == 'line_profiler':\n                valid_names = dir(module)\n                if parent is not None:\n                    valid_names += dir(parent)\n                if item.func_name in valid_names:\n                    # hack to prevent small names\n                    #if len(item.func_name) > 8:\n                    if len(item.func_name) > 6:\n                        flag = True\n            elif func_module_name == module.__name__:\n                flag = True\n        except  AttributeError:\n            if hasattr(item, '__module__'):\n                flag = item.__module__ == module.__name__\n    return flag", "response": "Check if item is directly defined by a module."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_class_funcnames(fname, blank_pats=['    #']):\n    with open(fname, 'r') as file_:\n        lines = file_.readlines()\n    funcname_list = []\n\n    #full_line_ = ''\n    for lx, line in enumerate(lines):\n        #full_line_ += line\n        if any([line.startswith(pat) for pat in blank_pats]):\n            funcname_list.append('')\n        if line.startswith('    def '):\n            def_x    = line.find('def')\n            rparen_x = line.find('(')\n            funcname = line[(def_x + 3):rparen_x]\n            #print(funcname)\n            funcname_list.append(funcname)\n    return funcname_list", "response": "Function list_class_funcnames - List all functions in a class file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninherit kwargs from base_func", "response": "def inherit_kwargs(inherit_func):\n    \"\"\"\n    TODO move to util_decor\n    inherit_func = inspect_pdfs\n    func = encoder.visualize.im_func\n    \"\"\"\n    import utool as ut\n    keys, is_arbitrary = ut.get_kwargs(inherit_func)\n    if is_arbitrary:\n        keys += ['**kwargs']\n    kwargs_append = '\\n'.join(keys)\n    #from six.moves import builtins\n    #builtins.print(kwargs_block)\n    def _wrp(func):\n        if func.__doc__ is None:\n            func.__doc__ = ''\n        # TODO append to kwargs block if it exists\n        kwargs_block = 'Kwargs:\\n' + ut.indent(kwargs_append)\n        func.__doc__ += kwargs_block\n        return func\n    return _wrp"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntests func for kwargs parseing", "response": "def dummy_func(arg1, arg2, arg3=None, arg4=[1, 2, 3], arg5={}, **kwargs):\n    \"\"\"\n    test func for kwargs parseing\n    \"\"\"\n    foo = kwargs.get('foo', None)\n    bar = kwargs.pop('bar', 4)\n    foo2 = kwargs['foo2']\n    foobar = str(foo) + str(bar) + str(foo2)\n    return foobar"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_kwdefaults(func, parse_source=False):\n    #import utool as ut\n    #with ut.embed_on_exception_context:\n    argspec = inspect.getargspec(func)\n    kwdefaults = {}\n    if argspec.args is None or argspec.defaults is None:\n        pass\n    else:\n        args = argspec.args\n        defaults = argspec.defaults\n        #kwdefaults = OrderedDict(zip(argspec.args[::-1], argspec.defaults[::-1]))\n        kwpos = len(args) - len(defaults)\n        kwdefaults = OrderedDict(zip(args[kwpos:], defaults))\n    if parse_source and argspec.keywords:\n        # TODO parse for kwargs.get/pop\n        keyword_defaults = parse_func_kwarg_keys(func, with_vals=True)\n        for key, val in keyword_defaults:\n            assert key not in kwdefaults, 'parsing error'\n            kwdefaults[key] = val\n    return kwdefaults", "response": "r Get keyword defaults for a function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_docstr(func_or_class):\n    import utool as ut\n    try:\n        docstr_ = func_or_class.func_doc\n    except AttributeError:\n        docstr_ = func_or_class.__doc__\n    if docstr_ is None:\n        docstr_ = ''\n    docstr = ut.unindent(docstr_)\n    return docstr", "response": "Get the docstring from a live object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_function_names(sourcecode, top_level=True, ignore_condition=1):\n    import ast\n    import utool as ut\n    func_names = []\n    if six.PY2:\n        sourcecode = ut.ensure_unicode(sourcecode)\n        encoded = sourcecode.encode('utf8')\n        pt = ast.parse(encoded)\n    else:\n        pt = ast.parse(sourcecode)\n\n    class FuncVisitor(ast.NodeVisitor):\n\n        def __init__(self):\n            super(FuncVisitor, self).__init__()\n            self.condition_names = None\n            self.condition_id = -9001\n            self.in_condition_chain = False\n\n        def visit_If(self, node):\n            if ignore_condition:\n                return\n            # if ignore_conditional:\n            #     return\n            # Ignore the main statement\n            # print('----')\n            # print('node.test = {!r}'.format(node.test))\n            # print('node.orelse = {!r}'.format(node.orelse))\n            if _node_is_main_if(node):\n                return\n\n            # if isinstance(node.orelse, ast.If):\n            #     # THIS IS AN ELIF\n            #     self.condition_id += 1\n            #     self.in_condition_chain = True\n            #     ast.NodeVisitor.generic_visit(self, node)\n            #     self.in_condition_chain = False\n            #     pass\n            # # TODO: where does else get parsed exactly?\n\n            # Reset the set of conditionals\n            # self.condition_id = 0\n            # self.condition_names = ut.ddict(list)\n\n            # self.in_condition_chain = True\n            ast.NodeVisitor.generic_visit(self, node)\n            # self.in_condition_chain = False\n\n            # if False:\n            #     # IF THIS WAS AN ELSE:\n            #     if self.condition_names is not None:\n            #         # anything defined in all conditions is kosher\n            #         from six.moves import reduce\n            #         common_names = reduce(set.intersection,\n            #                               map(set, self.condition_names.values()))\n            #         self.func_names.extend(common_names)\n            #         self.condition_names = None\n\n        def visit_FunctionDef(self, node):\n            # if self.in_condition_chain and self.condition_names is not None:\n            #     # dont immediately add things in conditions. Wait until we can\n            #     # ensure which definitions are common in all conditions.\n            #     self.condition_names[self.condition_id].append(node.name)\n            # else:\n            func_names.append(node.name)\n            if not top_level:\n                ast.NodeVisitor.generic_visit(self, node)\n\n        def visit_ClassDef(self, node):\n            if not top_level:\n                ast.NodeVisitor.generic_visit(self, node)\n    try:\n        FuncVisitor().visit(pt)\n    except Exception:\n        raise\n        pass\n    return func_names", "response": "A function that returns a list of function names in a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the project imports.", "response": "def parse_project_imports(dpath):\n    \"\"\"\n    dpath = ub.truepath('~/code/clab/clab')\n\n    Script:\n        >>> dpath = ut.get_argval('--dpath')\n        >>> parse_project_imports()\n\n    \"\"\"\n    import ubelt as ub\n    import glob\n    from os.path import join, exists\n    package_modules = set()\n    for fpath in glob.glob(join(dpath, '**/*.py'), recursive=True):\n        try:\n            sourcecode = ub.readfrom(fpath)\n            _, modules = ut.parse_import_names(sourcecode, False, fpath=fpath)\n            for mod in modules:\n                package_modules.add(mod.split('.')[0])  # just bases\n            if 'clab/live' in package_modules:\n                raise ValueError()\n                break\n        except SyntaxError:\n            print('encountered SyntaxError in fpath = {!r}'.format(fpath))\n    import warnings\n    import inspect\n    stdlibs = [dirname(warnings.__file__), dirname(inspect.__file__)]\n    def is_module_batteries_included(m):\n        if m in sys.builtin_module_names:\n            return True\n        for p in stdlibs:\n            if exists(join(p, m + '.py')):\n                return True\n            if exists(join(p, m)):\n                return True\n    used_modules = sorted([m for m in package_modules if not is_module_batteries_included(m)])\n    print('used_modules non-buildin modules = {}'.format(ub.repr2(used_modules)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_import_names(sourcecode, top_level=True, fpath=None, branch=False):\n    import ast\n    import_names = []\n    if six.PY2:\n        import utool as ut\n        sourcecode = ut.ensure_unicode(sourcecode)\n        encoded = sourcecode.encode('utf8')\n        pt = ast.parse(encoded)\n    else:\n        pt = ast.parse(sourcecode)\n\n    modules = []\n\n    class ImportVisitor(ast.NodeVisitor):\n\n        def _parse_alias_list(self, aliases):\n            for alias in aliases:\n                if alias.asname is not None:\n                    import_names.append(alias.asname)\n                else:\n                    if '.' not in alias.name:\n                        import_names.append(alias.name)\n\n        def visit_Import(self, node):\n            self._parse_alias_list(node.names)\n            self.generic_visit(node)\n\n            for alias in node.names:\n                modules.append(alias.name)\n\n        def visit_ImportFrom(self, node):\n            self._parse_alias_list(node.names)\n            self.generic_visit(node)\n\n            for alias in node.names:\n                prefix = ''\n                if node.level:\n                    if fpath is not None:\n                        from xdoctest import static_analysis as static\n                        modparts = static.split_modpath(os.path.abspath(fpath))[1].replace('\\\\', '/').split('/')\n                        parts = modparts[:-node.level]\n                        # parts = os.path.split(static.split_modpath(os.path.abspath(fpath))[1])[:-node.level]\n                        prefix = '.'.join(parts) + '.'\n                        # prefix = '.'.join(os.path.split(fpath)[-node.level:]) + '.'\n                    else:\n                        prefix = '.' * node.level\n                # modules.append(node.level * '.' + node.module + '.' + alias.name)\n                # modules.append(prefix + node.module + '.' + alias.name)\n                modules.append(prefix + node.module)\n\n        def visit_FunctionDef(self, node):\n            # Ignore modules imported in functions\n            if not top_level:\n                self.generic_visit(node)\n                # ast.NodeVisitor.generic_visit(self, node)\n\n        def visit_ClassDef(self, node):\n            if not top_level:\n                self.generic_visit(node)\n                # ast.NodeVisitor.generic_visit(self, node)\n\n        def visit_If(self, node):\n            if not branch:\n                # TODO: determine how to figure out if a name is in all branches\n                if not _node_is_main_if(node):\n                    # Ignore the main statement\n                    self.generic_visit(node)\n    try:\n        ImportVisitor().visit(pt)\n    except Exception:\n        pass\n    return import_names, modules", "response": "Parse a source code and return a list of all function names that are imported in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_python(code, reraise=True, ipy_magic_workaround=False):\n    import ast\n    try:\n        if ipy_magic_workaround:\n            code = '\\n'.join(['pass' if re.match(r'\\s*%[a-z]*', line) else line for line in code.split('\\n')])\n        ast.parse(code)\n    except SyntaxError:\n        if reraise:\n            import utool as ut\n            print('Syntax Error')\n            ut.print_python_code(code)\n            raise\n        return False\n    return True", "response": "Check if python code is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exec_func_src3(func, globals_, sentinal=None, verbose=False,\n                   start=None, stop=None):\n    \"\"\"\n    execs a func and returns requested local vars.\n\n    Does not modify globals unless update=True (or in IPython)\n\n    SeeAlso:\n        ut.execstr_funckw\n    \"\"\"\n    import utool as ut\n    sourcecode = ut.get_func_sourcecode(func, stripdef=True, stripret=True)\n    if sentinal is not None:\n        sourcecode = ut.replace_between_tags(sourcecode, '', sentinal)\n    if start is not None or stop is not None:\n        sourcecode = '\\n'.join(sourcecode.splitlines()[slice(start, stop)])\n    if verbose:\n        print(ut.color_text(sourcecode, 'python'))\n    six.exec_(sourcecode, globals_)", "response": "execs a function and returns requested local vars"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_func_sourcecode(func, stripdef=False, stripret=False,\n                        strip_docstr=False, strip_comments=False,\n                        remove_linenums=None):\n    \"\"\"\n    wrapper around inspect.getsource but takes into account utool decorators\n    strip flags are very hacky as of now\n\n    Args:\n        func (function):\n        stripdef (bool):\n        stripret (bool): (default = False)\n        strip_docstr (bool): (default = False)\n        strip_comments (bool): (default = False)\n        remove_linenums (None): (default = None)\n\n    CommandLine:\n        python -m utool.util_inspect --test-get_func_sourcecode\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_inspect import *  # NOQA\n        >>> # build test data\n        >>> func = get_func_sourcecode\n        >>> stripdef = True\n        >>> stripret = True\n        >>> sourcecode = get_func_sourcecode(func, stripdef)\n        >>> # verify results\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    #try:\n    inspect.linecache.clearcache()  # HACK: fix inspect bug\n    sourcefile = inspect.getsourcefile(func)\n    #except IOError:\n    #    sourcefile = None\n    if hasattr(func, '_utinfo'):\n        #if 'src' in func._utinfo:\n        #    sourcecode = func._utinfo['src']\n        #else:\n        func2 = func._utinfo['orig_func']\n        sourcecode = get_func_sourcecode(func2)\n    elif sourcefile is not None and (sourcefile != '<string>'):\n        try_limit = 2\n        for num_tries in range(try_limit):\n            try:\n                #print(func)\n                sourcecode = inspect.getsource(func)\n                if not isinstance(sourcecode, six.text_type):\n                    sourcecode = sourcecode.decode('utf-8')\n                #print(sourcecode)\n            except (IndexError, OSError, SyntaxError) as ex:\n                ut.printex(ex, 'Error getting source',\n                           keys=['sourcefile', 'func'])\n                if False:\n                    # VERY HACK: try to reload the module and get a redefined\n                    # version of the function\n                    import imp\n                    modname = get_func_modname(func)\n                    funcname = ut.get_funcname(func)\n                    module = sys.modules[modname]\n                    # TODO: ut.reload_module()\n                    module = imp.reload(module)\n                    func = module.__dict__[funcname]\n                else:\n                    # Fix inspect bug in python2.7\n                    inspect.linecache.clearcache()\n                if num_tries + 1 != try_limit:\n                    tries_left = try_limit - num_tries - 1\n                    print('Attempting %d more time(s)' % (tries_left))\n                else:\n                    raise\n    else:\n        sourcecode = None\n    #orig_source = sourcecode\n    #print(orig_source)\n    if stripdef:\n        # hacky\n        sourcecode = ut.unindent(sourcecode)\n        #sourcecode = ut.unindent(ut.regex_replace('def [^)]*\\\\):\\n', '', sourcecode))\n        #nodef_source = ut.regex_replace('def [^:]*\\\\):\\n', '', sourcecode)\n        regex_decor = '^@.' + ut.REGEX_NONGREEDY\n        regex_defline = '^def [^:]*\\\\):\\n'\n        patern = '(' + regex_decor + ')?' + regex_defline\n        nodef_source = ut.regex_replace(patern, '', sourcecode)\n        sourcecode = ut.unindent(nodef_source)\n        #print(sourcecode)\n        pass\n    if stripret:\n        r\"\"\" \\s is a whitespace char \"\"\"\n        return_ = ut.named_field('return', 'return .*$')\n        prereturn = ut.named_field('prereturn', r'^\\s*')\n        return_bref = ut.bref_field('return')\n        prereturn_bref = ut.bref_field('prereturn')\n        regex = prereturn + return_\n        repl = prereturn_bref + 'pass  # ' + return_bref\n        #import re\n        #print(re.search(regex, sourcecode, flags=re.MULTILINE ))\n        #print(re.search('return', sourcecode, flags=re.MULTILINE | re.DOTALL ))\n        #print(re.search(regex, sourcecode))\n        sourcecode_ = re.sub(regex, repl, sourcecode, flags=re.MULTILINE)\n        #print(sourcecode_)\n        sourcecode = sourcecode_\n        pass\n    if strip_docstr or strip_comments:\n        # pip install pyminifier\n        # References: http://code.activestate.com/recipes/576704/\n        #from pyminifier import minification, token_utils\n        def remove_docstrings_or_comments(source):\n            \"\"\"\n            TODO: commit clean version to pyminifier\n            \"\"\"\n            import tokenize\n            from six.moves import StringIO\n            io_obj = StringIO(source)\n            out = ''\n            prev_toktype = tokenize.INDENT\n            last_lineno = -1\n            last_col = 0\n            for tok in tokenize.generate_tokens(io_obj.readline):\n                token_type = tok[0]\n                token_string = tok[1]\n                start_line, start_col = tok[2]\n                end_line, end_col = tok[3]\n                if start_line > last_lineno:\n                    last_col = 0\n                if start_col > last_col:\n                    out += (' ' * (start_col - last_col))\n                # Remove comments:\n                if strip_comments and token_type == tokenize.COMMENT:\n                    pass\n                elif strip_docstr and token_type == tokenize.STRING:\n                    if prev_toktype != tokenize.INDENT:\n                        # This is likely a docstring; double-check we're not inside an operator:\n                        if prev_toktype != tokenize.NEWLINE:\n                            if start_col > 0:\n                                out += token_string\n                else:\n                    out += token_string\n                prev_toktype = token_type\n                last_col = end_col\n                last_lineno = end_line\n            return out\n        sourcecode = remove_docstrings_or_comments(sourcecode)\n        #sourcecode = minification.remove_comments_and_docstrings(sourcecode)\n        #tokens = token_utils.listified_tokenizer(sourcecode)\n        #minification.remove_comments(tokens)\n        #minification.remove_docstrings(tokens)\n        #token_utils.untokenize(tokens)\n\n    if remove_linenums is not None:\n        source_lines = sourcecode.strip('\\n').split('\\n')\n        ut.delete_items_by_index(source_lines, remove_linenums)\n        sourcecode = '\\n'.join(source_lines)\n    return sourcecode", "response": "wrapper around inspect. getsource but takes into account utool decorators"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the argspec of a function", "response": "def get_func_argspec(func):\n    \"\"\"\n    wrapper around inspect.getargspec but takes into account utool decorators\n    \"\"\"\n    if hasattr(func, '_utinfo'):\n        argspec = func._utinfo['orig_argspec']\n        return argspec\n    if isinstance(func, property):\n        func = func.fget\n    try:\n        argspec = inspect.getargspec(func)\n    except Exception:\n        argspec = inspect.getfullargspec(func)\n    return argspec"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_kwargs(func):\n    #if argspec.keywords is None:\n    import utool as ut\n    argspec = inspect.getargspec(func)\n    if argspec.defaults is not None:\n        num_args = len(argspec.args)\n        num_keys = len(argspec.defaults)\n        keys = ut.take(argspec.args, range(num_args - num_keys, num_args))\n    else:\n        keys = []\n    is_arbitrary = argspec.keywords is not None\n    RECURSIVE = False\n    if RECURSIVE and argspec.keywords is not None:\n        pass\n        # TODO: look inside function at the functions that the kwargs object is being\n        # passed to\n    return keys, is_arbitrary", "response": "Function to get keyword arguments for a single object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_attribute_chain(attrname, namespace):\n    #subdict = meta_util_six.get_funcglobals(root_func)\n    subtup = attrname.split('.')\n    subdict = namespace\n    for attr in subtup[:-1]:\n        subdict = subdict[attr].__dict__\n    leaf_name = subtup[-1]\n    leaf_attr = subdict[leaf_name]\n    return leaf_attr", "response": "lookup_attribute_chain - Lookup the attribute chain for a given attribute name in a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recursive_parse_kwargs(root_func, path_=None, verbose=None):\n    if verbose is None:\n        verbose = VERBOSE_INSPECT\n    if verbose:\n        print('[inspect] recursive parse kwargs root_func = %r ' % (root_func,))\n\n    import utool as ut\n    if path_ is None:\n        path_ = []\n    if root_func in path_:\n        if verbose:\n            print('[inspect] Encountered cycle. returning')\n        return []\n    path_.append(root_func)\n    spec = ut.get_func_argspec(root_func)\n    # ADD MORE\n    kwargs_list = []\n    found_explicit = list(ut.get_kwdefaults(root_func, parse_source=False).items())\n    if verbose:\n        print('[inspect] * Found explicit %r' % (found_explicit,))\n\n    #kwargs_list = [(kw,) for kw in  ut.get_kwargs(root_func)[0]]\n    sourcecode = ut.get_func_sourcecode(root_func, strip_docstr=True,\n                                        stripdef=True)\n    sourcecode1 = ut.get_func_sourcecode(root_func, strip_docstr=True,\n                                         stripdef=False)\n    found_implicit = ut.parse_kwarg_keys(sourcecode1, spec.keywords,\n                                         with_vals=True)\n    if verbose:\n        print('[inspect] * Found found_implicit %r' % (found_implicit,))\n    kwargs_list = found_explicit + found_implicit\n\n    def hack_lookup_mod_attrs(attr):\n        # HACKS TODO: have find_funcs_called_with_kwargs infer an attribute is a\n        # module / function / type. In the module case, we can import it and\n        # look it up.  Maybe args, or returns can help infer type.  Maybe just\n        # register some known varnames.  Maybe jedi has some better way to do\n        # this.\n        if attr == 'ut':\n            subdict = ut.__dict__\n        elif attr == 'pt':\n            import plottool as pt\n            subdict = pt.__dict__\n        else:\n            subdict = None\n        return subdict\n\n    def resolve_attr_subfunc(subfunc_name):\n        # look up attriute chain\n        #subdict = root_func.func_globals\n        subdict = meta_util_six.get_funcglobals(root_func)\n        subtup = subfunc_name.split('.')\n        try:\n            subdict = lookup_attribute_chain(subfunc_name, subdict)\n            if ut.is_func_or_method(subdict):\n                # Was subdict supposed to be named something else here?\n                subfunc = subdict\n                return subfunc\n        except (KeyError, TypeError):\n            for attr in subtup[:-1]:\n                try:\n                    subdict = subdict[attr].__dict__\n                except (KeyError, TypeError):\n                    # limited support for class lookup\n                    if ut.is_method(root_func) and spec.args[0] == attr:\n                        subdict = root_func.im_class.__dict__\n                    else:\n                        # FIXME TODO lookup_attribute_chain\n                        subdict = hack_lookup_mod_attrs(attr)\n                        if subdict is None:\n                            print('Unable to find attribute of attr=%r' % (attr,))\n                            if ut.SUPER_STRICT:\n                                raise\n        if subdict is not None:\n            attr_name = subtup[-1]\n            subfunc = subdict[attr_name]\n        else:\n            subfunc = None\n        return subfunc\n\n    def check_subfunc_name(subfunc_name):\n        if isinstance(subfunc_name, tuple) or '.' in subfunc_name:\n            subfunc = resolve_attr_subfunc(subfunc_name)\n        else:\n            # try to directly take func from globals\n            func_globals = meta_util_six.get_funcglobals(root_func)\n            try:\n                subfunc = func_globals[subfunc_name]\n            except KeyError:\n                print('Unable to find function definition subfunc_name=%r' %\n                      (subfunc_name,))\n                if ut.SUPER_STRICT:\n                    raise\n                subfunc = None\n        if subfunc is not None:\n            subkw_list = recursive_parse_kwargs(subfunc, path_, verbose=verbose)\n            new_subkw = subkw_list\n            # have_keys = set(ut.take_column(kwargs_list, 0))\n            # new_subkw = [item for item in subkw_list\n            #              if item[0] not in have_keys]\n        else:\n            new_subkw = []\n        return new_subkw\n\n    if spec.keywords is not None:\n        if verbose:\n            print('[inspect] Checking spec.keywords=%r' % (spec.keywords,))\n        subfunc_name_list = ut.find_funcs_called_with_kwargs(sourcecode, spec.keywords)\n        if verbose:\n            print('[inspect] Checking subfunc_name_list=%r' % (subfunc_name_list,))\n        for subfunc_name in subfunc_name_list:\n            try:\n                new_subkw = check_subfunc_name(subfunc_name)\n                if verbose:\n                    print('[inspect] * Found %r' % (new_subkw,))\n                kwargs_list.extend(new_subkw)\n            except TypeError:\n                print('warning: unable to recursivley parse type of : %r' % (subfunc_name,))\n    return kwargs_list", "response": "recursive function to parse kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting kwargs from a function", "response": "def get_func_kwargs(func, recursive=True):\n    \"\"\"\n    func = ibeis.run_experiment\n\n    SeeAlso:\n        argparse_funckw\n        recursive_parse_kwargs\n        parse_kwarg_keys\n        parse_func_kwarg_keys\n        get_func_kwargs\n    \"\"\"\n    import utool as ut\n    argspec = ut.get_func_argspec(func)\n    if argspec.defaults is None:\n        header_kw = {}\n    else:\n        header_kw = dict(zip(argspec.args[::-1], argspec.defaults[::-1]))\n    if argspec.keywords is not None:\n        header_kw.update(dict(ut.recursive_parse_kwargs(func)))\n    return header_kw"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nallows kwargs to be specified on the commandline from testfuncs Args: func (function): Kwargs: lbl, verbose, only_specified, force_keys, type_hint, alias_dict Returns: dict: funckw CommandLine: python -m utool.util_inspect argparse_funckw SeeAlso: exec_funckw recursive_parse_kwargs parse_kwarg_keys Example: >>> # ENABLE_DOCTEST >>> from utool.util_inspect import * # NOQA >>> import utool as ut >>> func = get_instance_attrnames >>> funckw = argparse_funckw(func) >>> result = ('funckw = %s' % (ut.repr3(funckw),)) >>> print(result) funckw = { 'default': True, 'with_methods': True, 'with_properties': True, }", "response": "def argparse_funckw(func, defaults={}, **kwargs):\n    \"\"\"\n    allows kwargs to be specified on the commandline from testfuncs\n\n    Args:\n        func (function):\n\n    Kwargs:\n        lbl, verbose, only_specified, force_keys, type_hint, alias_dict\n\n    Returns:\n        dict: funckw\n\n    CommandLine:\n        python -m utool.util_inspect argparse_funckw\n\n    SeeAlso:\n        exec_funckw\n        recursive_parse_kwargs\n        parse_kwarg_keys\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_inspect import *  # NOQA\n        >>> import utool as ut\n        >>> func = get_instance_attrnames\n        >>> funckw = argparse_funckw(func)\n        >>> result = ('funckw = %s' % (ut.repr3(funckw),))\n        >>> print(result)\n        funckw = {\n            'default': True,\n            'with_methods': True,\n            'with_properties': True,\n        }\n    \"\"\"\n    import utool as ut\n    funckw_ = ut.get_funckw(func, recursive=True)\n    funckw_.update(defaults)\n    funckw = ut.argparse_dict(funckw_, **kwargs)\n    return funckw"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef infer_function_info(func):\n    import utool as ut\n    import re\n\n    # TODO: allow a jedi argument\n    if False:\n        from jedi.evaluate import docstrings\n        script = func.script\n        argname_list = [p.name.value for p in func.params]\n        argtype_list = [docstrings.follow_param(script._evaluator, p) for p in func.params]\n\n    if isinstance(func, property):\n        func = func.fget\n    try:\n        doc_shortdesc = ''\n        doc_longdesc = ''\n\n        known_arginfo = ut.ddict(dict)\n\n        current_doc = inspect.getdoc(func)\n        docstr_blocks = ut.parse_docblocks_from_docstr(current_doc)\n        docblock_types = ut.take_column(docstr_blocks, 0)\n        docblock_types = [re.sub('Example[0-9]', 'Example', type_)\n                          for type_ in docblock_types]\n        docblock_dict = ut.group_items(docstr_blocks, docblock_types)\n\n        if '' in docblock_dict:\n            docheaders = docblock_dict['']\n            docheaders_lines = ut.take_column(docheaders, 1)\n            docheaders_order = ut.take_column(docheaders, 2)\n            docheaders_lines = ut.sortedby(docheaders_lines, docheaders_order)\n            doc_shortdesc = '\\n'.join(docheaders_lines)\n\n        if 'Args' in docblock_dict:\n            argblocks = docblock_dict['Args']\n            if len(argblocks) != 1:\n                print('Warning: should only be one args block')\n            else:\n                argblock = argblocks[0][1]\n\n                assert argblock.startswith('Args:\\n')\n                argsblock_ = argblock[len('Args:\\n'):]\n                arglines = re.split(r'^    \\b', argsblock_, flags=re.MULTILINE)\n                arglines = [line for line in arglines if len(line) > 0]\n\n                esc = re.escape\n\n                def escparen(pat):\n                    return esc('(')  + pat + esc(')')\n                argname = ut.named_field('argname', ut.REGEX_VARNAME)\n                argtype_ = ut.named_field('argtype', '.' + ut.REGEX_NONGREEDY)\n                argtype = escparen(argtype_)\n                argdesc = ut.named_field('argdesc', '.*')\n                WS = ut.REGEX_WHITESPACE\n                argpattern = (\n                    WS + argname + WS + argtype + WS + ':' + WS + argdesc)\n\n                for argline in arglines:\n                    m = re.match(argpattern, argline, flags=re.MULTILINE | re.DOTALL)\n                    try:\n                        groupdict_ = m.groupdict()\n                    except Exception as ex:\n                        print('---')\n                        print('argline = \\n%s' % (argline,))\n                        print('---')\n                        raise Exception('Unable to parse argline=%s' % (argline,))\n                    #print('groupdict_ = %s' % (ut.repr4(groupdict_),))\n                    argname = groupdict_['argname']\n                    known_arginfo[argname]['argdesc'] = groupdict_['argdesc'].rstrip('\\n')\n                    # TODO: record these in a file for future reference\n                    # and potential guessing\n                    if groupdict_['argtype'] != '?':\n                        known_arginfo[argname]['argtype'] = groupdict_['argtype']\n\n        is_class = isinstance(func, six.class_types)\n\n        needs_surround = current_doc is None or len(current_doc) == 0\n\n        if is_class:\n            argfunc = func.__init__\n        else:\n            argfunc = func\n        argspec = ut.get_func_argspec(argfunc)\n        (argname_list, varargs, varkw, defaults) = argspec\n\n        # See util_inspect\n        tup = ut.infer_arg_types_and_descriptions(argname_list, defaults)\n        argtype_list, argdesc_list, argdefault_list, hasdefault_list = tup\n        # Put in user parsed info\n        for index, argname in enumerate(argname_list):\n            if argname in known_arginfo:\n                arginfo = known_arginfo[argname]\n                if 'argdesc' in arginfo:\n                    argdesc_list[index] = arginfo['argdesc']\n                if 'argtype' in arginfo:\n                    argtype_list[index] = arginfo['argtype']\n\n        if not is_class:\n            # Move source down to base indentation, but remember original indentation\n            sourcecode = get_func_sourcecode(func)\n            #kwarg_keys = ut.parse_kwarg_keys(sourcecode)\n            kwarg_items = ut.recursive_parse_kwargs(func)\n            flags = ut.unique_flags(ut.take_column(kwarg_items, 0))\n            kwarg_items = ut.compress(kwarg_items, flags)\n            kwarg_keys = ut.take_column(kwarg_items, 0)\n            #kwarg_keys = ut.unique_ordered(kwarg_keys)\n            kwarg_keys = ut.setdiff_ordered(kwarg_keys, argname_list)\n        else:\n            sourcecode = None\n            kwarg_keys = []\n\n        if sourcecode is not None:\n            num_indent = ut.get_indentation(sourcecode)\n            sourcecode = ut.unindent(sourcecode)\n            returninfo = ut.parse_return_type(sourcecode)\n        else:\n            num_indent = 0\n            returninfo = None, None, None, ''\n        return_type, return_name, return_header, return_desc = returninfo\n\n        modname = func.__module__\n        funcname = ut.get_funcname(func)\n    except Exception as ex:\n        #print('dealing with infer function error')\n        #print('has utinfo? ' + str(hasattr(func, '_utinfo')))\n        #sourcefile = inspect.getsourcefile(func)  # NOQA\n        ut.printex(ex, 'Error Infering Function Info', keys=[\n            'func',\n            'sourcefile',\n            'sourcecode',\n            'argspec',\n        ], tb=True)\n        raise\n\n    class FunctionInfo(object):\n        def __init__(self):\n            pass\n    funcinfo = FunctionInfo()\n    funcinfo.needs_surround = needs_surround\n    funcinfo.argname_list = argname_list\n    funcinfo.argtype_list = argtype_list\n    funcinfo.argdesc_list = argdesc_list\n    funcinfo.argdefault_list = argdefault_list\n    funcinfo.hasdefault_list = hasdefault_list\n    funcinfo.kwarg_keys = kwarg_keys\n    # if new\n    funcinfo.va_name = varargs\n    funcinfo.kw_name = varkw\n    funcinfo.kw_keys = kwarg_keys\n    # else\n    funcinfo.varargs = varargs\n    funcinfo.varkw = varkw\n    # fi\n    funcinfo.defaults = defaults\n    funcinfo.num_indent = num_indent\n    funcinfo.return_type = return_type\n    funcinfo.return_name = return_name\n    funcinfo.return_header = return_header\n    funcinfo.return_desc = return_desc\n    funcinfo.modname = modname\n    funcinfo.funcname = funcname\n    funcinfo.doc_shortdesc = doc_shortdesc\n    funcinfo.doc_longdesc = doc_longdesc\n    funcinfo.ismethod = hasattr(func, 'im_class')\n    return funcinfo", "response": "r Infer information for a function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating iterator to write to new tsv. Contains input tsv lines plus quant data for these.", "response": "def get_psms(self):\n        \"\"\"Creates iterator to write to new tsv. Contains input tsv\n        lines plus quant data for these.\"\"\"\n        self.header = actions.create_header(self.oldheader, self.spectracol)\n        self.psms = actions.generate_psms_spectradata(self.lookup, self.fn,\n                                                      self.oldheader)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef roundrobin(*iterables):\n    raise NotImplementedError('not sure if this implementation is correct')\n    # http://stackoverflow.com/questions/11125212/interleaving-lists-in-python\n    #sentinel = object()\n    #return (x for x in chain(*zip_longest(fillvalue=sentinel, *iterables)) if x is not sentinel)\n    pending = len(iterables)\n    if six.PY2:\n        nexts = cycle(iter(it).next for it in iterables)\n    else:\n        nexts = cycle(iter(it).__next__ for it in iterables)\n    while pending:\n        try:\n            for next in nexts:\n                yield next()\n        except StopIteration:\n            pending -= 1\n            nexts = cycle(islice(nexts, pending))", "response": "Yields the elements of the given iterable in order."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_(num, n=8):\n    if num is None:\n        return 'None'\n    if util_type.is_float(num):\n        ret = ('%.' + str(n) + 'E') % num\n        exp_pos  = ret.find('E')\n        exp_part = ret[(exp_pos + 1):]\n        exp_part = exp_part.replace('+', '')\n        if exp_part.find('-') == 0:\n            exp_part = '-' + exp_part[1:].strip('0')\n        exp_part = exp_part.strip('0')\n        if len(exp_part) > 0:\n            exp_part = 'E' + exp_part\n        flt_part = ret[:exp_pos].strip('0').strip('.')\n        ret = flt_part + exp_part\n        return ret\n    return '%d' % num", "response": "Formats a number into a human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef normalize_list_of_dicts(value, default_key, default_value=UNDEFINED):\n    if value is None:\n        if default_value is UNDEFINED:\n            return []\n        value = default_value\n\n    if isinstance(value, dict):\n        return [value]\n\n    if isinstance(value, text_type):\n        return [{default_key: value}]\n\n    if isinstance(value, list):\n        if not all(isinstance(x, dict) for x in value):\n            def _fix(x):\n                return {default_key: x} if isinstance(x, text_type) else x\n            return list(map(_fix, value))\n\n    return value", "response": "Converts given value to a list of dictionaries as follows"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _qt_set_leaf_data(self, qvar):\n    if VERBOSE_PREF:\n        print('')\n        print('+--- [pref.qt_set_leaf_data]')\n        print('[pref.qt_set_leaf_data] qvar = %r' % qvar)\n        print('[pref.qt_set_leaf_data] _intern.name=%r' % self._intern.name)\n        print('[pref.qt_set_leaf_data] _intern.type_=%r' % self._intern.get_type())\n        print('[pref.qt_set_leaf_data] type(_intern.value)=%r' % type(self._intern.value))\n        print('[pref.qt_set_leaf_data] _intern.value=%r' % self._intern.value)\n        #print('[pref.qt_set_leaf_data] qvar.toString()=%s' % six.text_type(qvar.toString()))\n    if self._tree.parent is None:\n        raise Exception('[Pref.qtleaf] Cannot set root preference')\n    if self.qt_is_editable():\n        new_val = '[Pref.qtleaf] BadThingsHappenedInPref'\n        if self._intern.value == PrefNode:\n            raise Exception('[Pref.qtleaf] Qt can only change leafs')\n        elif self._intern.value is None:\n            # None could be a number of types\n            def cast_order(var, order=[bool, int, float, six.text_type]):\n                for type_ in order:\n                    try:\n                        ret = type_(var)\n                        return ret\n                    except Exception:\n                        continue\n            new_val = cast_order(six.text_type(qvar))\n        self._intern.get_type()\n        if isinstance(self._intern.value, bool):\n            #new_val = bool(qvar.toBool())\n            print('qvar = %r' % (qvar,))\n            new_val = util_type.smart_cast(qvar, bool)\n            #new_val = bool(eval(qvar, {}, {}))\n            print('new_val = %r' % (new_val,))\n        elif isinstance(self._intern.value, int):\n            #new_val = int(qvar.toInt()[0])\n            new_val = int(qvar)\n        # elif isinstance(self._intern.value, float):\n        elif self._intern.get_type() in util_type.VALID_FLOAT_TYPES:\n            #new_val = float(qvar.toDouble()[0])\n            new_val = float(qvar)\n        elif isinstance(self._intern.value, six.string_types):\n            #new_val = six.text_type(qvar.toString())\n            new_val = six.text_type(qvar)\n        elif isinstance(self._intern.value, PrefChoice):\n            #new_val = qvar.toString()\n            new_val = six.text_type(qvar)\n            if new_val.upper() == 'NONE':\n                new_val = None\n        else:\n            try:\n                #new_val = six.text_type(qvar.toString())\n                type_ = self._intern.get_type()\n                if type_ is not None:\n                    new_val = type_(six.text_type(qvar))\n                else:\n                    new_val = six.text_type(qvar)\n            except Exception:\n                raise NotImplementedError(\n                    ('[Pref.qtleaf] Unknown internal type. '\n                     'type(_intern.value) = %r, '\n                     '_intern.get_type() = %r, ')\n                    % type(self._intern.value), self._intern.get_type())\n        # Check for a set of None\n        if isinstance(new_val, six.string_types):\n            if new_val.lower() == 'none':\n                new_val = None\n            elif new_val.lower() == 'true':\n                new_val = True\n            elif new_val.lower() == 'false':\n                new_val = False\n        # save to disk after modifying data\n        if VERBOSE_PREF:\n            print('---')\n            print('[pref.qt_set_leaf_data] new_val=%r' % new_val)\n            print('[pref.qt_set_leaf_data] type(new_val)=%r' % type(new_val))\n            print('L____ [pref.qt_set_leaf_data]')\n        # TODO Add ability to set a callback function when certain\n        # preferences are changed.\n        return self._tree.parent.pref_update(self._intern.name, new_val)\n    return 'PrefNotEditable'", "response": "Sets the backend data for the leafs of the tree."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toggle(self, key):\n        val = self[key]\n        assert isinstance(val, bool), 'key[%r] = %r is not a bool' % (key, val)\n        self.pref_update(key, not val)", "response": "Toggles a boolean key"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_combo_val(self, new_val):\n        choice_obj = self._intern.value\n        assert isinstance(self._intern.value, PrefChoice), 'must be a choice'\n        return choice_obj.get_tuple()", "response": "Changes the value of the combo item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __new_attr(self, name, attr):\n        if isinstance(attr, Pref):\n            # Child attribute already has a Pref wrapping\n            if VERBOSE_PREF:\n                print('[pref.__new_attr]: %s.%s = %r' % (self._intern.name, name, attr.value()))\n            new_childx = len(self._tree.child_names)\n            # Children know about parents\n            attr._tree.parent = self     # Give child parent\n            attr._intern.name = name     # Give child name\n            if attr._intern.depeq is None:\n                attr._intern.depeq = self._intern.depeq  # Give child parent dependencies\n            if attr._intern.hidden:\n                self._tree.hidden_children.append(new_childx)\n                self._tree.hidden_children.sort()\n            # Used for QTIndexing\n            attr._intern.aschildx = new_childx\n            # Parents know about children\n            self._tree.child_names.append(name)  # Add child to tree\n            self._tree.child_list.append(attr)\n            self.__dict__[name] = attr.value()   # Add child value to dict\n        else:\n            # The child attribute is not wrapped. Wrap with Pref and readd.\n            pref_attr = Pref(default=attr)\n            self.__new_attr(name, pref_attr)", "response": "Add a new child attribute to the tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over the items of the object.", "response": "def iteritems(self):\n        \"\"\"\n        Wow this class is messed up. I had to overwrite items when\n        moving to python3, just because I haden't called it yet\n        \"\"\"\n        for (key, val) in six.iteritems(self.__dict__):\n            if key in self._printable_exclude:\n                continue\n            yield (key, val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts prefeters to a dictionary.", "response": "def to_dict(self, split_structs_bit=False):\n        \"\"\" Converts prefeters to a dictionary.\n        Children Pref can be optionally separated \"\"\"\n        pref_dict = {}\n        struct_dict = {}\n        for (key, val) in six.iteritems(self):\n            if split_structs_bit and isinstance(val, Pref):\n                struct_dict[key] = val\n                continue\n            pref_dict[key] = val\n        if split_structs_bit:\n            return (pref_dict, struct_dict)\n        return pref_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves prefs to disk in dict format", "response": "def save(self):\n        \"\"\" Saves prefs to disk in dict format \"\"\"\n        fpath = self.get_fpath()\n        if fpath in ['', None]:\n            if self._tree.parent is not None:\n                if VERBOSE_PREF:\n                    print('[pref.save] Can my parent save me?')  # ...to disk\n                return self._tree.parent.save()\n            if VERBOSE_PREF:\n                print('[pref.save] I cannot be saved. I have no parents.')\n            return False\n        with open(fpath, 'wb') as f:\n            print('[pref] Saving to ' + fpath)\n            pref_dict = self.to_dict()\n            pickle.dump(pref_dict, f, protocol=2)  # Use protocol 2 to support python2 and 3\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load(self):\n        if VERBOSE_PREF:\n            print('[pref.load()]')\n            #if not os.path.exists(self._intern.fpath):\n            #    msg = '[pref] fpath=%r does not exist' % (self._intern.fpath)\n            #    return msg\n        fpath = self.get_fpath()\n        try:\n            with open(fpath, 'rb') as f:\n                if VERBOSE_PREF:\n                    print('load: %r' % fpath)\n                pref_dict = pickle.load(f)\n        except EOFError as ex1:\n            util_dbg.printex(ex1, 'did not load pref fpath=%r correctly' % fpath, iswarning=True)\n            #warnings.warn(msg)\n            raise\n            #return msg\n        except ImportError as ex2:\n            util_dbg.printex(ex2, 'did not load pref fpath=%r correctly' % fpath, iswarning=True)\n            #warnings.warn(msg)\n            raise\n            #return msg\n        if not util_type.is_dict(pref_dict):\n            raise Exception('Preference file is corrupted')\n        self.add_dict(pref_dict)\n        return True", "response": "Load the pref dict stored on disk. Overwrites current values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full name of the user", "response": "def full_name(self):\n        \"\"\" returns name all the way up the tree \"\"\"\n        if self._tree.parent is None:\n            return self._intern.name\n        return self._tree.parent.full_name() + '.' + self._intern.name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pref_update(self, key, new_val):\n        print('Update and save pref from: %s=%r, to: %s=%r' %\n              (key, six.text_type(self[key]), key, six.text_type(new_val)))\n        self.__setattr__(key, new_val)\n        return self.save()", "response": "Updates a preference value and saves it to disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninjecting an instance of the class with all functions registered to the classkey.", "response": "def inject_instance(self, classkey=None, allow_override=False,\n                    verbose=VERBOSE_CLASS, strict=True):\n    \"\"\"\n    Injects an instance (self) of type (classkey)\n    with all functions registered to (classkey)\n\n    call this in the __init__ class function\n\n    Args:\n        self: the class instance\n        classkey: key for a class, preferably the class type itself, but it\n            doesnt have to be\n\n    SeeAlso:\n        make_class_method_decorator\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # DOCTEST_DISABLE\n        >>> utool.make_class_method_decorator(InvertedIndex)(smk_debug.invindex_dbgstr)\n        >>> utool.inject_instance(invindex)\n    \"\"\"\n    import utool as ut\n    if verbose:\n        print('[util_class] begin inject_instance')\n    try:\n        if classkey is None:\n            # Probably should depricate this block of code\n            # It tries to do too much\n            classkey = self.__class__\n            if classkey == 'ibeis.gui.models_and_views.IBEISTableView':\n                # HACK HACK HACK\n                # from guitool.__PYQT__ import QtGui  # NOQA\n                from guitool.__PYQT__ import QtWidgets  # NOQA\n                classkey = QtWidgets.QAbstractItemView\n            if len(__CLASSTYPE_ATTRIBUTES__[classkey]) == 0:\n                print('[utool] Warning: no classes of type %r are registered' % (classkey,))\n                print('[utool] type(self)=%r, self=%r' % (type(self), self)),\n                print('[utool] Checking to see if anybody else was registered...')\n                print('[utool] __CLASSTYPE_ATTRIBUTES__ = ' +\n                      ut.repr4(__CLASSTYPE_ATTRIBUTES__.keys()))\n                for classtype_, _ in six.iteritems(__CLASSTYPE_ATTRIBUTES__):\n                    isinstance(self, classtype_)\n                    classkey = classtype_\n                    print('[utool] Warning: using subclass=%r' % (classtype_,))\n                    break\n        func_list = __CLASSTYPE_ATTRIBUTES__[classkey]\n        if verbose:\n            print('[util_class] injecting %d methods\\n   with classkey=%r\\n   into %r'\n                  % (len(func_list), classkey, self,))\n        for func in func_list:\n            if VERBOSE_CLASS:\n                print('[util_class] * injecting %r' % (func,))\n            method_name = None\n            # Allow user to register tuples for aliases\n            if isinstance(func, tuple):\n                func, method_name = func\n            inject_func_as_method(self, func, method_name=method_name,\n                                  allow_override=allow_override, verbose=verbose)\n    except Exception as ex:\n        ut.printex(ex, 'ISSUE WHEN INJECTING %r' % (classkey,),\n                      iswarning=not strict)\n        if strict:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inject_all_external_modules(self, classname=None,\n                                allow_override='override+warn',\n                                strict=True):\n    \"\"\"\n    dynamically injects registered module methods into a class instance\n\n    FIXME: naming convention and use this in all places where this clas is used\n    \"\"\"\n    #import utool as ut\n    if classname is None:\n        classname = self.__class__.__name__\n    #import utool as ut\n    #ut.embed()\n\n    NEW = True\n    if NEW:\n        classkey_list = [key for key in __CLASSTYPE_ATTRIBUTES__\n                         if key[0] == classname]\n    else:\n        injected_modules = get_injected_modules(classname)\n        # the variable must be named CLASS_INJECT_KEY\n        # and only one class can be specified per module.\n        classkey_list = [module.CLASS_INJECT_KEY\n                         for module in injected_modules]\n\n    for classkey in classkey_list:\n        inject_instance(\n            self, classkey=classkey,\n            allow_override=allow_override, strict=False)\n\n    for classkey in classkey_list:\n        postinject_instance(\n            self, classkey=classkey)", "response": "injects all external modules into a class instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_injected_modules(classname):\n    modname_list = __CLASSNAME_CLASSKEY_REGISTER__[classname]\n\n    injected_modules = []\n    for modstr in modname_list:\n        parts = modstr.split('.')\n        pkgname = '.'.join(parts[:-1])\n        modname = parts[-1]\n        try:\n            exec('from %s import %s' % (pkgname, modname, ), globals(), locals())\n            module = eval(modname)\n            injected_modules.append(module)\n        except ImportError as ex:\n            ut.printex(ex, 'Cannot load package=%r, module=%r' % (pkgname, modname, ))\n    return injected_modules", "response": "r Returns a list of modules that are injected by the given class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autogen_explicit_injectable_metaclass(classname, regen_command=None,\n                                          conditional_imports=None):\n    r\"\"\"\n    Args:\n        classname (?):\n\n    Returns:\n        ?:\n\n    CommandLine:\n        python -m utool.util_class --exec-autogen_explicit_injectable_metaclass\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_class import *  # NOQA\n        >>> from utool.util_class import  __CLASSTYPE_ATTRIBUTES__  # NOQA\n        >>> import ibeis\n        >>> import ibeis.control.IBEISControl\n        >>> classname = ibeis.control.controller_inject.CONTROLLER_CLASSNAME\n        >>> result = autogen_explicit_injectable_metaclass(classname)\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    vals_list = []\n\n    def make_redirect(func):\n        # PRESERVES ALL SIGNATURES WITH EXECS\n        src_fmt = r'''\n        def {funcname}{defsig}:\n            \"\"\" {orig_docstr}\"\"\"\n            return {orig_funcname}{callsig}\n        '''\n        from utool._internal import meta_util_six\n        orig_docstr = meta_util_six.get_funcdoc(func)\n        funcname = meta_util_six.get_funcname(func)\n        orig_funcname = modname.split('.')[-1] + '.' + funcname\n        orig_docstr = '' if orig_docstr is None else orig_docstr\n        import textwrap\n        # Put wrapped function into a scope\n        import inspect\n        argspec = inspect.getargspec(func)\n        (args, varargs, varkw, defaults) = argspec\n        defsig = inspect.formatargspec(*argspec)\n        callsig = inspect.formatargspec(*argspec[0:3])\n        src_fmtdict = dict(funcname=funcname, orig_funcname=orig_funcname,\n                           defsig=defsig, callsig=callsig,\n                           orig_docstr=orig_docstr)\n        src = textwrap.dedent(src_fmt).format(**src_fmtdict)\n        return src\n\n    src_list = []\n\n    for classkey, vals in __CLASSTYPE_ATTRIBUTES__.items():\n        modname = classkey[1]\n        if classkey[0] == classname:\n            vals_list.append(vals)\n            for func in vals:\n                src = make_redirect(func)\n                src = ut.indent(src)\n                src = '\\n'.join([_.rstrip() for _ in src.split('\\n')])\n                src_list.append(src)\n\n    if regen_command is None:\n        regen_command = 'FIXME None given'\n\n    module_header = ut.codeblock(\n        \"\"\"\n        # -*- coding: utf-8 -*-\n        \"\"\" + ut.TRIPLE_DOUBLE_QUOTE + \"\"\"\n        Static file containing autogenerated functions for {classname}\n        Autogenerated on {autogen_time}\n\n        RegenCommand:\n            {regen_command}\n        \"\"\" + ut.TRIPLE_DOUBLE_QUOTE + \"\"\"\n\n        from __future__ import absolute_import, division, print_function\n        import utool as ut\n\n        \"\"\").format(\n            autogen_time=ut.get_timestamp(),\n            regen_command=regen_command,\n            classname=classname)\n\n    depends_module_block = autogen_import_list(classname, conditional_imports)\n    inject_statement_fmt = (\"print, rrr, profile = \"\n                            \"ut.inject2(__name__, '[autogen_explicit_inject_{classname}]')\")\n    inject_statement = inject_statement_fmt.format(classname=classname)\n\n    source_block_lines = [\n        module_header,\n        depends_module_block,\n        inject_statement,\n        '\\n',\n        'class ExplicitInject' + classname + '(object):',\n    ] + src_list\n    source_block = '\\n'.join(source_block_lines)\n\n    source_block = ut.autoformat_pep8(source_block, aggressive=2)\n    return source_block", "response": "r Automatically creates explicit injectable metaclass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_class_method_decorator(classkey, modname=None):\n    global __APP_MODNAME_REGISTER__\n    #if util_arg.VERBOSE or VERBOSE_CLASS:\n    if VERBOSE_CLASS:\n        print('[util_class] register via make_class_method_decorator classkey=%r, modname=%r'\n              % (classkey, modname))\n    if modname == '__main__':\n        # skips reinjects into main\n        print('WARNING: cannot register classkey=%r functions as __main__' % (classkey,))\n        return lambda func: func\n    # register that this module was injected into\n    if isinstance(classkey, tuple):\n        classname, _ = classkey\n        __CLASSNAME_CLASSKEY_REGISTER__[classname].append(modname)\n    elif isinstance(classkey, type):\n        classname = classkey.__name__\n        if modname is not None:\n            assert modname == classkey.__module__, (\n                'modname=%r does not agree with __module__=%r' % (\n                    modname, classkey.__module__))\n        modname = classkey.__module__\n        # Convert to new classkey format\n        classkey = (classname, modname)\n        __CLASSNAME_CLASSKEY_REGISTER__[classname].append(modname)\n    else:\n        print('Warning not using classkey for %r %r' % (classkey, modname))\n        raise AssertionError('classkey no longer supported. Use class_inject_key instead')\n    closure_decorate_class_method = functools.partial(decorate_class_method,\n                                                      classkey=classkey)\n    return closure_decorate_class_method", "response": "This function creates a decorator for injectable methods in a CheeseShop class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_class_postinject_decorator(classkey, modname=None):\n    if util_arg.VERBOSE or VERBOSE_CLASS:\n        print('[util_class] register class_postinject classkey=%r, modname=%r'\n              % (classkey, modname))\n    if modname == '__main__':\n        print('WARNING: cannot register class functions as __main__')\n        # skips reinjects into main\n        return lambda func: func\n    closure_decorate_postinject = functools.partial(decorate_postinject,\n                                                    classkey=classkey)\n    return closure_decorate_postinject", "response": "Decorator for injectable methods in the class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decorate_class_method(func, classkey=None, skipmain=False):\n    #import utool as ut\n    global __CLASSTYPE_ATTRIBUTES__\n    assert classkey is not None, 'must specify classkey'\n    #if not (skipmain and ut.get_caller_modname() == '__main__'):\n    __CLASSTYPE_ATTRIBUTES__[classkey].append(func)\n    return func", "response": "Will inject all decorated function as methods of classkey\n\n    classkey is some identifying string, tuple, or object\n\n    func can also be a tuple"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninject a function into an object as a method of the class instance.", "response": "def inject_func_as_method(self, func, method_name=None, class_=None,\n                          allow_override=False, allow_main=False,\n                          verbose=True, override=None, force=False):\n    \"\"\" Injects a function into an object as a method\n\n    Wraps func as a bound method of self. Then injects func into self\n    It is preferable to use make_class_method_decorator and inject_instance\n\n    Args:\n       self (object): class instance\n       func : some function whos first arugment is a class instance\n       method_name (str) : default=func.__name__, if specified renames the method\n       class_ (type) : if func is an unbound method of this class\n\n\n    References:\n        http://stackoverflow.com/questions/1015307/python-bind-an-unbound-method\n    \"\"\"\n    if override is not None:\n        # TODO depcirate allow_override\n        allow_override = override\n    if method_name is None:\n        method_name = get_funcname(func)\n    if force:\n        allow_override = True\n        allow_main = True\n    old_method = getattr(self, method_name, None)\n    # Bind function to the class instance\n    #new_method = types.MethodType(func, self, self.__class__)\n    new_method = func.__get__(self, self.__class__)\n    #new_method = profile(func.__get__(self, self.__class__))\n\n    if old_method is not None:\n        old_im_func = get_method_func(old_method)\n        new_im_func = get_method_func(new_method)\n        if not allow_main and old_im_func is not None and (\n                get_funcglobals(old_im_func)['__name__'] != '__main__' and\n                get_funcglobals(new_im_func)['__name__'] == '__main__'):\n            if True or VERBOSE_CLASS:\n                print('[util_class] skipping re-inject of %r from __main__' % method_name)\n            return\n        if old_method is new_method or old_im_func is new_im_func:\n            #if verbose and util_arg.NOT_QUIET:\n            #    print('WARNING: Skipping injecting the same function twice: %r' % new_method)\n                #print('WARNING: Injecting the same function twice: %r' % new_method)\n            return\n        elif allow_override is False:\n            raise AssertionError(\n                'Overrides are not allowed. Already have method_name=%r' %\n                (method_name))\n        elif allow_override == 'warn':\n            print(\n                'WARNING: Overrides are not allowed. Already have method_name=%r. Skipping' %\n                (method_name))\n            return\n        elif allow_override == 'override+warn':\n            #import utool as ut\n            #ut.embed()\n            print('WARNING: Overrides are allowed, but dangerous. method_name=%r.' %\n                  (method_name))\n            print('old_method = %r, im_func=%s' % (old_method, str(old_im_func)))\n            print('new_method = %r, im_func=%s' % (new_method, str(new_im_func)))\n            print(get_funcglobals(old_im_func)['__name__'])\n            print(get_funcglobals(new_im_func)['__name__'])\n        # TODO: does this actually decrement the refcount enough?\n        del old_method\n    setattr(self, method_name, new_method)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninjecting a function as a property of the current object.", "response": "def inject_func_as_property(self, func, method_name=None, class_=None):\n    \"\"\"\n    WARNING:\n        properties are more safely injected using metaclasses\n\n    References:\n        http://stackoverflow.com/questions/13850114/dynamically-adding-methods-with-or-without-metaclass-in-python\n    \"\"\"\n    if method_name is None:\n        method_name = get_funcname(func)\n    #new_method = func.__get__(self, self.__class__)\n    new_property = property(func)\n    setattr(self.__class__, method_name, new_property)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninject a function as an unbound method of the given class.", "response": "def inject_func_as_unbound_method(class_, func, method_name=None):\n    \"\"\" This is actually quite simple \"\"\"\n    if method_name is None:\n        method_name = get_funcname(func)\n    setattr(class_, method_name, func)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef makeForwardingMetaclass(forwarding_dest_getter, whitelist, base_class=object):\n    class ForwardingMetaclass(base_class.__class__):\n        def __init__(metaself, name, bases, dct):\n            # print('ForwardingMetaclass.__init__():\n            #  {forwarding_dest_getter: %r; whitelist: %r}' % (forwarding_dest_getter, whitelist))\n            super(ForwardingMetaclass, metaself).__init__(name, bases, dct)\n            old_getattr = metaself.__getattribute__\n            old_setattr = metaself.__setattr__\n            def new_getattr(self, item):\n                if item in whitelist:\n                    #dest = old_getattr(self, forwarding_dest_name)\n                    dest = forwarding_dest_getter(self)\n                    try:\n                        val = dest.__class__.__getattribute__(dest, item)\n                    except AttributeError:\n                        val = getattr(dest, item)\n                else:\n                    val = old_getattr(self, item)\n                return val\n            def new_setattr(self, name, val):\n                if name in whitelist:\n                    #dest = old_getattr(self, forwarding_dest_name)\n                    dest = forwarding_dest_getter(self)\n                    dest.__class__.__setattr__(dest, name, val)\n                else:\n                    old_setattr(self, name, val)\n            metaself.__getattribute__ = new_getattr\n            metaself.__setattr__ = new_setattr\n    return ForwardingMetaclass", "response": "Creates a metaclass that overrides __getattr__ and __setattr__ to forward some attribute references to a specified instance variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a reloading metaclass class that is a class that is a subclass of the base class.", "response": "def reloading_meta_metaclass_factory(BASE_TYPE=type):\n    \"\"\" hack for pyqt \"\"\"\n    class ReloadingMetaclass2(BASE_TYPE):\n        def __init__(metaself, name, bases, dct):\n            super(ReloadingMetaclass2, metaself).__init__(name, bases, dct)\n            #print('Making rrr for %r' % (name,))\n            metaself.rrr = reload_class\n    return ReloadingMetaclass2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reload_class(self, verbose=True, reload_module=True):\n    import utool as ut\n    verbose = verbose or VERBOSE_CLASS\n    classname = self.__class__.__name__\n    try:\n        modname = self.__class__.__module__\n        if verbose:\n            print('[class] reloading ' + classname + ' from ' + modname)\n        # --HACK--\n        if hasattr(self, '_on_reload'):\n            if verbose > 1:\n                print('[class] calling _on_reload for ' + classname)\n            self._on_reload()\n        elif verbose > 1:\n            print('[class] ' + classname + ' does not have an _on_reload function')\n\n        # Do for all inheriting classes\n        def find_base_clases(_class, find_base_clases=None):\n            class_list = []\n            for _baseclass in _class.__bases__:\n                parents = find_base_clases(_baseclass, find_base_clases)\n                class_list.extend(parents)\n            if _class is not object:\n                class_list.append(_class)\n            return class_list\n\n        head_class = self.__class__\n        # Determine if parents need reloading\n        class_list = find_base_clases(head_class, find_base_clases)\n        # HACK\n        ignore = {HashComparable2}\n        class_list = [_class for _class in class_list\n                      if _class not in ignore]\n        for _class in class_list:\n            if verbose:\n                print('[class] reloading parent ' + _class.__name__ +\n                      ' from ' + _class.__module__)\n            if _class.__module__ == '__main__':\n                # Attempt to find the module that is the main module\n                # This may be very hacky and potentially break\n                main_module_ = sys.modules[_class.__module__]\n                main_modname = ut.get_modname_from_modpath(main_module_.__file__)\n                module_ = sys.modules[main_modname]\n            else:\n                module_ = sys.modules[_class.__module__]\n            if hasattr(module_, 'rrr'):\n                if reload_module:\n                    module_.rrr(verbose=verbose)\n            else:\n                if reload_module:\n                    import imp\n                    if verbose:\n                        print('[class] reloading ' + _class.__module__ + ' with imp')\n                    try:\n                        imp.reload(module_)\n                    except (ImportError, AttributeError):\n                        print('[class] fallback reloading ' + _class.__module__ +\n                              ' with imp')\n                        # one last thing to try. probably used ut.import_module_from_fpath\n                        # when importing this module\n                        imp.load_source(module_.__name__, module_.__file__)\n            # Reset class attributes\n            _newclass = getattr(module_, _class.__name__)\n            reload_class_methods(self, _newclass, verbose=verbose)\n\n        # --HACK--\n        # TODO: handle injected definitions\n        if hasattr(self, '_initialize_self'):\n            if verbose > 1:\n                print('[class] calling _initialize_self for ' + classname)\n            self._initialize_self()\n        elif verbose > 1:\n            print('[class] ' + classname + ' does not have an _initialize_self function')\n    except Exception as ex:\n        ut.printex(ex, 'Error Reloading Class', keys=[\n            'modname', 'module', 'class_', 'class_list', 'self', ])\n        raise", "response": "Reloads the class in the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reload_class_methods(self, class_, verbose=True):\n    if verbose:\n        print('[util_class] Reloading self=%r as class_=%r' % (self, class_))\n    self.__class__ = class_\n    for key in dir(class_):\n        # Get unbound reloaded method\n        func = getattr(class_, key)\n        if isinstance(func, types.MethodType):\n            # inject it into the old instance\n            inject_func_as_method(self, func, class_=class_,\n                                  allow_override=True,\n                                  verbose=verbose)", "response": "rebinds all class methods to the same class instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of comparison methods", "response": "def get_comparison_methods():\n    \"\"\" makes methods for >, <, =, etc... \"\"\"\n    method_list = []\n    def _register(func):\n        method_list.append(func)\n        return func\n\n    # Comparison operators for sorting and uniqueness\n    @_register\n    def __lt__(self, other):\n        return compare_instance(op.lt, self, other)\n\n    @_register\n    def __le__(self, other):\n        return compare_instance(op.le, self, other)\n\n    @_register\n    def __eq__(self, other):\n        return compare_instance(op.eq, self, other)\n\n    @_register\n    def __ne__(self, other):\n        return compare_instance(op.ne, self, other)\n\n    @_register\n    def __gt__(self, other):\n        return compare_instance(op.gt, self, other)\n\n    @_register\n    def __ge__(self, other):\n        return compare_instance(op.ge, self, other)\n\n    return method_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_private_obfuscation(self):\n    classname = self.__class__.__name__\n    attrlist = [attr for attr in dir(self) if attr.startswith('_' + classname + '__')]\n    for attr in attrlist:\n        method = getattr(self, attr)\n        truename = attr.replace('_' + classname + '__', '__')\n        setattr(self, truename, method)", "response": "Removes the python obfuscation of class privates so they can be executed as\n   . Useful when playing with IPython."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_classname(class_, local=False):\n    if not local:\n        classname = class_.__module__ + '.' + class_.__name__\n    else:\n        classname = class_.__name__\n    return classname", "response": "r Get the classname of the object in the current directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_config():\n        db_type = os.environ['DB_TYPE']\n        db_host = os.environ['DB_HOST']\n        db_user = os.environ['DB_USER']\n        db_database = os.environ['DB_NAME']\n        db_password = os.environ['DB_PASSWORD']\n        db_prefix = os.environ['DB_PREFIX']\n\n        check = Migration.check_packages(db_type)\n\n        return check, {\n            db_type: {\n                'driver': db_type.strip(),\n                'host': db_host.strip(),\n                'database': db_database.strip(),\n                'user': db_user.strip(),\n                'password': db_password.strip(),\n                'prefix': db_prefix.strip()\n            }\n        }", "response": "Gets the config from the os. environ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the driver for the user defined host is available.", "response": "def check_packages(db_name):\n        \"\"\"\n        Check if the driver for the user defined host is available. If it is not available, download it using PIP\n        :param db_name:\n        :return:\n        \"\"\"\n        print('Checking for required Database Driver')\n\n        reqs = subprocess.check_output([sys.executable, '-m', 'pip', 'freeze'])\n        installed_packages = [r.decode().split('==')[0] for r in reqs.split()]\n\n        # print(installed_packages)\n\n        if db_name.lower() == 'mysql':\n            if 'PyMySQL' not in installed_packages:\n                print('Installing required Database Driver')\n                os.system('pip install pymysql')\n\n        if db_name.lower() == 'postgresql':\n            if 'psycopg2-binary' not in installed_packages:\n                print('Installing required Database Driver')\n                os.system('pip install psycopg2-binary')\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_peptidequant_lookup(fns, pqdb, poolnames, pepseq_colnr,\n                               ms1_qcolpattern=None, isobqcolpattern=None,\n                               psmnrpattern=None, fdrcolpattern=None,\n                               pepcolpattern=None):\n    \"\"\"Calls lower level function to create a peptide quant lookup\"\"\"\n    patterns = [ms1_qcolpattern, fdrcolpattern, pepcolpattern]\n    storefuns = [pqdb.store_precursor_quants, pqdb.store_fdr,\n                 pqdb.store_pep]\n    create_pep_protein_quant_lookup(fns, pqdb, poolnames, pepseq_colnr,\n                                    patterns, storefuns,\n                                    isobqcolpattern, psmnrpattern)", "response": "Creates a peptide quant lookup for a given pool."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a protein quant lookup for a given set of pool names.", "response": "def create_proteinquant_lookup(fns, pqdb, poolnames, protacc_colnr,\n                               ms1_qcolpattern=None, isobqcolpattern=None,\n                               psmnrpattern=None, probcolpattern=None,\n                               fdrcolpattern=None, pepcolpattern=None):\n    \"\"\"Calls lower level function to create a protein quant lookup\"\"\"\n    patterns = [ms1_qcolpattern, probcolpattern, fdrcolpattern, pepcolpattern]\n    storefuns = [pqdb.store_precursor_quants, pqdb.store_probability,\n                 pqdb.store_fdr, pqdb.store_pep]\n    create_pep_protein_quant_lookup(fns, pqdb, poolnames, protacc_colnr,\n                                    patterns, storefuns, isobqcolpattern,\n                                    psmnrpattern)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_pep_protein_quant_lookup(fns, pqdb, poolnames, featcolnr, patterns,\n                                    storefuns, isobqcolpattern=None,\n                                    psmnrpattern=None):\n    \"\"\"Does the work when creating peptide and protein quant lookups. This\n    loops through storing options and parses columns, passing on to the\n    storing functions\"\"\"\n    tablefn_map = create_tablefn_map(fns, pqdb, poolnames)\n    feat_map = pqdb.get_feature_map()\n    for pattern, storefun in zip(patterns, storefuns):\n        if pattern is None:\n            continue\n        colmap = get_colmap(fns, pattern, single_col=True)\n        if colmap:\n            store_single_col_data(fns, tablefn_map, feat_map,\n                                  storefun, featcolnr, colmap)\n    if isobqcolpattern is not None:\n        isocolmap = get_colmap(fns, isobqcolpattern, antipattern=psmnrpattern)\n    else:\n        return\n    if psmnrpattern is not None:\n        psmcolmap = get_colmap(fns, psmnrpattern)\n    else:\n        psmcolmap = False\n    create_isobaric_quant_lookup(fns, tablefn_map,\n                                 feat_map, pqdb,\n                                 featcolnr,\n                                 isocolmap, psmcolmap)", "response": "This function creates a peptide and protein quant lookup table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring protein table names in DB returns a map with their respective DB IDs", "response": "def create_tablefn_map(fns, pqdb, poolnames):\n    \"\"\"Stores protein/peptide table names in DB, returns a map with their\n    respective DB IDs\"\"\"\n    poolmap = {name: pid for (name, pid) in pqdb.get_all_poolnames()}\n    pqdb.store_table_files([(poolmap[pool], os.path.basename(fn))\n                            for fn, pool in zip(fns, poolnames)])\n    return pqdb.get_tablefn_map()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_colmap(fns, pattern, single_col=False, antipattern=False):\n    colmap = {}\n    for fn in fns:\n        header = tsvreader.get_tsv_header(fn)\n        basefn = os.path.basename(fn)\n        cols = tsvreader.get_cols_in_file(pattern, header, single_col)\n        if antipattern:\n            anticols = tsvreader.get_cols_in_file(antipattern, header,\n                                                  single_col)\n            cols = [col for col in cols if col not in anticols]\n        if cols:\n            colmap[basefn] = cols\n    return colmap", "response": "For a list of table files loops through headers and checks which column names match a passed pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef store_single_col_data(fns, prottable_id_map, pacc_map, pqdbmethod,\n                          protacc_colnr, colmap):\n    \"\"\"General method to store single column data from protein tables\n    in lookup\"\"\"\n    to_store = []\n    for fn, header, pquant in tsvreader.generate_tsv_pep_protein_quants(fns):\n        pacc_id = pacc_map[pquant[header[protacc_colnr]]]\n        pqdata = (pacc_id, prottable_id_map[fn], pquant[colmap[fn]])\n        to_store.append(pqdata)\n        if len(to_store) > 10000:\n            pqdbmethod(to_store)\n            to_store = []\n    pqdbmethod(to_store)", "response": "Store data from single column tables in lookup table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_isobaric_quant_lookup(fns, tablefn_map, featmap, pqdb,\n                                 featcolnr, allquantcols, psmcolmap):\n    \"\"\"Creates a lookup dict from peptide/protein quant input files and some\n    input parameters. This assumes the order of quant columns and\n    number-of-PSM columns is the same.\"\"\"\n    pqdb.store_quant_channels(\n            map_psmnrcol_to_quantcol(allquantcols, psmcolmap, tablefn_map),\n            psmcolmap)\n    quantmap = pqdb.get_quantchannel_map()\n    to_store = []\n    for fn, header, pquant in tsvreader.generate_tsv_pep_protein_quants(fns):\n        pqdata = get_isob_quant_data(pquant, header, tablefn_map[fn],\n                                     featmap, featcolnr, quantmap)\n        to_store.extend(pqdata)\n        if len(to_store) > 10000:\n            pqdb.store_isobaric_quants(to_store, psmcolmap)\n            to_store = []\n    pqdb.store_isobaric_quants(to_store, psmcolmap)", "response": "Creates a dictionary that contains the peptide - > protein quant data for each peptide - > protein quant."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a dict from a line of protein quant data into a set of tuples that can be stored in db", "response": "def get_isob_quant_data(pquant, header, fnid, featmap, featcol, qmap):\n    \"\"\"Turns a dict from a line of protein/peptide quant data into a set of\n    tuples that can be stored in db\"\"\"\n    feat_dbid = featmap[pquant[header[featcol]]]\n    for channel_name, (channel_id, psmfield) in qmap[fnid].items():\n        if psmfield is None:\n            yield (feat_dbid, channel_id, pquant[channel_name])\n        else:\n            yield (feat_dbid, channel_id, pquant[channel_name],\n                   pquant[psmfield])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef euler_tour(G, node=None, seen=None, visited=None):\n    if node is None:\n        node = next(G.nodes())\n    if visited is None:\n        assert nx.is_tree(G)\n        visited = []\n    if seen is None:\n        seen = set([])\n    visited.append(node)\n    for c in G.neighbors(node):\n        if c in seen:\n            continue\n        seen.add(c)\n        euler_tour(G, c, seen, visited)\n        visited.append(node)\n    return visited", "response": "euler tour for a tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreleases kids from a node", "response": "def avl_release_kids(node):\n    \"\"\"\n    splits a node from its kids maintaining parent pointers\n    \"\"\"\n    left, right = node.left, node.right\n    if left is not None:\n        # assert left.parent is node\n        left.parent = None\n    if right is not None:\n        # assert right.parent is node\n        right.parent = None\n    node.balance = 0\n    node.left = None\n    node.right = None\n    return node, left, right"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreleases the parent of a node.", "response": "def avl_release_parent(node):\n    \"\"\"\n    removes the parent of a child\n    \"\"\"\n    parent = node.parent\n    if parent is not None:\n        if parent.right is node:\n            parent.right = None\n        elif parent.left is node:\n            parent.left = None\n        else:\n            raise AssertionError('impossible state')\n        node.parent = None\n        parent.balance = max(height(parent.right), height(parent.left)) + 1\n    return node, parent"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avl_rotate_single(root, direction):\n    other_side = 1 - direction\n    save = root[other_side]\n    save.parent = root.parent\n    # root[other_side] = save[direction]\n    # save[direction] = root\n    root.set_child(other_side, save[direction])\n    save.set_child(direction, root)\n    rlh = height(root.left)\n    rrh = height(root.right)\n    slh = height(save[other_side])\n    root.balance = max(rlh, rrh) + 1\n    save.balance = max(slh, root.balance) + 1\n    return save", "response": "rotate a single node in the tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avl_rotate_double(root, direction):\n    other_side = 1 - direction\n    root[other_side] = avl_rotate_single(root[other_side], other_side)\n    return avl_rotate_single(root, direction)", "response": "rotate the tree root by the given direction"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef avl_join_dir_recursive(t1, t2, node, direction):\n    other_side = 1 - direction\n    if _DEBUG_JOIN_DIR:\n        print('--JOIN DIR (dir=%r) --' % (direction,))\n        ascii_tree(t1, 't1')\n        ascii_tree(t2, 't2')\n\n    if direction == 0:\n        large, small = t2, t1\n    elif direction == 1:\n        large, small = t1, t2\n    else:\n        assert False\n\n    # Follow the spine of the larger tree\n    spine = large[direction]\n    rest = large[other_side]\n    # k_, v_ = large.key, large.value\n\n    hsmall = height(small)\n    hspine = height(spine)\n    hrest = height(rest)\n\n    if _DEBUG_JOIN_DIR:\n        ascii_tree(spine, 'spine')\n        ascii_tree(rest, 'rest')\n        ascii_tree(small, 'small')\n\n    if hspine <= hsmall + 1:\n        t_ = avl_new_top(small, spine, node, direction)\n        if _DEBUG_JOIN_DIR:\n            print('JOIN DIR (BASE)')\n            ascii_tree(t_, 't_')\n        if height(t_) <= hrest + 1:\n            if _DEBUG_JOIN_DIR:\n                print('JOIN DIR (Case 1)')\n            return avl_new_top(t_, rest, large, direction)\n        else:\n            # Double rotation, but with a new node\n            if _DEBUG_JOIN_DIR:\n                print('JOIN DIR (Case 2)')\n            t_rotate = avl_rotate_single(t_, direction)\n            if _DEBUG_JOIN_DIR:\n                ascii_tree(t_rotate, 't_rotate')\n                EulerTourTree(root=t_rotate)._assert_nodes('t_rotate')\n            t_merge = avl_new_top(rest, t_rotate, large, other_side)\n            if _DEBUG_JOIN_DIR:\n                ascii_tree(t_merge, 't_merge')\n                EulerTourTree(root=t_merge)._assert_nodes('t_merge')\n            new_root = avl_rotate_single(t_merge, other_side)\n            if _DEBUG_JOIN_DIR:\n                ascii_tree(new_root, 'new_root')\n                EulerTourTree(root=new_root)._assert_nodes('new_root')\n            return new_root\n    else:\n        # Traverse down the spine in the appropriate direction\n        if _DEBUG_JOIN_DIR:\n            print('JOIN DIR (RECURSE)')\n        if direction == 0:\n            t_ = avl_join_dir_recursive(small, spine, node, direction)\n        elif direction == 1:\n            t_ = avl_join_dir_recursive(spine, t2, node, direction)\n        else:\n            raise AssertionError('invalid direction')\n        t__ = avl_new_top(t_, rest, large, direction)\n        if height(t_) <= hrest + 1:\n            if _DEBUG_JOIN_DIR:\n                print('JOIN DIR (Case 3)')\n            return t__\n        else:\n            if _DEBUG_JOIN_DIR:\n                print('JOIN DIR (Case 4)')\n            return avl_rotate_single(t__, other_side)\n    assert False, 'should never get here'", "response": "Recursive version of avl_join_dir"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef avl_join(t1, t2, node):\n    if DEBUG_JOIN:\n        print('-- JOIN node=%r' % (node,))\n\n    if t1 is None and t2 is None:\n        if DEBUG_JOIN:\n            print('Join Case 1')\n        top = node\n    elif t1 is None:\n        # FIXME keep track of count if possible\n        if DEBUG_JOIN:\n            print('Join Case 2')\n        top = avl_insert_dir(t2, node, 0)\n    elif t2 is None:\n        if DEBUG_JOIN:\n            print('Join Case 3')\n        top = avl_insert_dir(t1, node, 1)\n    else:\n        h1 = height(t1)\n        h2 = height(t2)\n        if h1 > h2 + 1:\n            if DEBUG_JOIN:\n                print('Join Case 4')\n            top = avl_join_dir_recursive(t1, t2, node, 1)\n            if DEBUG_JOIN:\n                ascii_tree(t1, 'top')\n        elif h2 > h1 + 1:\n            if DEBUG_JOIN:\n                print('Join Case 5')\n                ascii_tree(t1)\n                ascii_tree(t2)\n\n            top = avl_join_dir_recursive(t1, t2, node, 0)\n            if DEBUG_JOIN:\n                ascii_tree(top)\n        else:\n            if DEBUG_JOIN:\n                print('Join Case 6')\n            # Insert at the top of the tree\n            top = avl_new_top(t1, t2, node, 0)\n    return top", "response": "A function that joins two trees t1 and t2 with an intermediate key - value pair"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the new root and last node of the tree with the maximum element removed from the tree", "response": "def avl_split_last(root):\n    \"\"\"\n    Removes the maximum element from the tree\n\n    Returns:\n        tuple: new_root, last_node\n\n    O(log(n)) = O(height(root))\n    \"\"\"\n    if root is None:\n        raise IndexError('Empty tree has no maximum element')\n    root, left, right = avl_release_kids(root)\n    if right is None:\n        new_root, last_node = left, root\n    else:\n        new_right, last_node = avl_split_last(right)\n        new_root = avl_join(left, new_right, root)\n    return (new_root, last_node)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef avl_split_first(root):\n    if root is None:\n        raise IndexError('Empty tree has no maximum element')\n    root, left, right = avl_release_kids(root)\n    if left is None:\n        new_root, first_node = right, root\n    else:\n        new_left, first_node = avl_split_first(left)\n        new_root = avl_join(new_left, right, root)\n    return (new_root, first_node)", "response": "Returns the new root and first node of the tree with minimum element removed from the tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\njoins two trees without any intermediate key Returns: Node: new_root O(log(n) + log(m)) = O(r(t1) + r(t2)) For AVL-Trees the rank r(t1) = height(t1) - 1", "response": "def avl_join2(t1, t2):\n    \"\"\"\n    join two trees without any intermediate key\n\n    Returns:\n        Node: new_root\n\n    O(log(n) + log(m)) = O(r(t1) + r(t2))\n\n    For AVL-Trees the rank r(t1) = height(t1) - 1\n    \"\"\"\n    if t1 is None and t2 is None:\n        new_root = None\n    elif t2 is None:\n        new_root = t1\n    elif t1 is None:\n        new_root = t2\n    else:\n        new_left, last_node = avl_split_last(t1)\n\n        debug = 0\n\n        if debug:\n            EulerTourTree(root=new_left)._assert_nodes('new_left')\n            EulerTourTree(root=last_node)._assert_nodes('last_node')\n            EulerTourTree(root=t2)._assert_nodes('t2')\n\n            print('new_left')\n            EulerTourTree(root=new_left).print_tree()\n\n            print('last_node')\n            EulerTourTree(root=last_node).print_tree()\n\n            print('t2')\n            EulerTourTree(root=t2).print_tree()\n\n        new_root = avl_join(new_left, t2, last_node)\n\n        if debug:\n            print('new_root')\n            EulerTourTree(root=new_root).print_tree()\n            EulerTourTree(root=last_node)._assert_nodes('new_root')\n    return new_root"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef avl_new_top(t1, t2, top, direction=0):\n    top.parent = None\n    assert top.parent is None, str(top.parent.value)\n    top.set_child(direction, t1)\n    top.set_child(1 - direction, t2)\n    top.balance = max(height(t1), height(t2)) + 1\n    return top", "response": "Create a new top node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backtrace_root(node):\n    # Trace path to the root\n    rpath = []\n    prev = node\n    now = node.parent\n    while now is not None:\n        if now.left is prev:\n            rpath.append((now, 0))\n        elif now.right is prev:\n            rpath.append((now, 1))\n        else:\n            raise AssertionError('impossible state')\n        prev = now\n        now = now.parent\n    return rpath", "response": "Trace path to the root node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef avl_insert_dir(root, new_node, direction=1):\n    if root is None:\n        return new_node\n    assert new_node.parent is None, str((new_node, new_node.parent))\n    assert new_node.left is None\n    assert new_node.right is None\n    assert root.parent is None\n\n    node_stack = []  # node stack\n    # dir_stack = array('I')  # direction stack\n    done = False\n    top = 0\n    # Move all the way to the right/left in tree1\n    node = root\n    # search for an empty link, save path\n    while True:\n        # Always move to the right\n        # dir_stack.append(direction)\n        node_stack.append(node)\n        if node[direction] is None:\n            break\n        node = node[direction]\n    extreme_node = node\n\n    # Insert a new node at the bottom of the tree\n    extreme_node.set_child(direction, new_node)\n    new_root = root\n\n    # Walk back up the search path\n    # (which for joining orderless structures was always right)\n    other_side = 1 - direction\n    top = len(node_stack) - 1\n    while (top >= 0) and not done:\n        # direction = dir_stack[top]\n        # other_side = 1 - direction\n        top_node = node_stack[top]\n        left_height = height(top_node[direction])\n        right_height = height(top_node[other_side])\n\n        # Terminate or rebalance as necessary\n        if left_height - right_height == 0:\n            done = True\n        if left_height - right_height >= 2:\n            a = top_node[direction][direction]\n            b = top_node[direction][other_side]\n\n            # Determine which rotation is required\n            if height(a) >= height(b):\n                node_stack[top] = avl_rotate_single(top_node, other_side)\n            else:\n                node_stack[top] = avl_rotate_double(top_node, other_side)\n\n            # Fix parent\n            if top != 0:\n                # d_ = dir_stack[top - 1]\n                d_ = direction\n                node_stack[top - 1].set_child(d_, node_stack[top])\n            else:\n                new_root = node_stack[0]\n                new_root.parent = None\n            done = True\n\n        # Update balance factors\n        top_node = node_stack[top]\n        left_height = height(top_node[direction])\n        right_height = height(top_node[other_side])\n\n        top_node.balance = max(left_height, right_height) + 1\n        top -= 1\n    assert new_root.parent is None\n    return new_root", "response": "This function creates a new node in the tree and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reroot(self, first_node, last_node):\n        min_elem = self.min_elem()\n        if min_elem  == first_node.value:\n            print('Already rooted there')\n            return\n        # tour = list(self)\n        # print('tour     = %r' % (tour,))\n        # B is the part before R\n        # A is the part after R (with first element removed)\n        B, A, first_node = avl_split(self.root, first_node)\n        print('Splice out first part of sequence ending before os')\n        print('B = %r' % ([] if B is None else list(B),))\n        print('Remove its first occurrence or')\n        B, old_root = (B, B) if B is None else avl_split_first(B)\n        print('B = %r' % ([] if B is None else list(B),))\n        print('The rest of the sequence now begins with os')\n        A = avl_insert_dir(A, first_node, 0)\n        print('A = %r' % (list(A),))\n        print('Tack the first part onto the end')\n        EulerTourTree(root=A)._assert_nodes('A')\n        EulerTourTree(root=B)._assert_nodes('B')\n        C = avl_join2(A, B)\n        EulerTourTree(root=C)._assert_nodes('C')\n        print('C = %r' % (list(C),))\n        print('Add a new occurrence os to the end')\n        new_last = Node(value=last_node.value)\n        C = avl_insert_dir(C, new_last, 1)\n        print('C = %r' % (list(C),))\n\n        EulerTourTree(root=B)._assert_nodes()\n        EulerTourTree(root=A)._assert_nodes()\n        # EulerTourTree(root=first_node)._assert_nodes()\n\n        # EulerTourTree(root=B).print_tree()\n        # EulerTourTree(root=A).print_tree()\n        # EulerTourTree(root=first_node).print_tree()\n\n        # B = avl_insert_dir(B, new_last, 1)\n        # print('B = %r' % ([] if B is None else list(B),))\n        # print('A = %r' % (list(A),))\n\n        # EulerTourTree(root=A).print_tree()\n\n        # old_tour_parts = [S1, R, S2]\n        # old_tour = ut.flatten([list(p) for p in old_tour_parts if p])\n        # print('old_tour = %r' % (old_tour,))\n        # assert tour == old_tour\n        # new_tour_parts = [A, B]\n        # new_tour = ut.flatten([list(p) for p in new_tour_parts if p])\n        print('new_tour = %r' % (list(C)))\n        self.root = C\n\n        # TODO: fix lookups\n        self.last_lookup[new_last.value] = new_last\n\n        nodes = list(self._traverse_nodes())\n        new_first_lookup = {node.value: node for node in nodes[::-1]}\n        new_last_lookup = {node.value: node for node in nodes[::1]}\n\n        for key in new_last_lookup.keys():\n            old_last = self.last_lookup[key]\n            new_last = new_last_lookup[key]\n            if old_last is not new_last:\n                print('key=%r needs LAST_DICT update' % (key,))\n\n        for key in new_last_lookup.keys():\n            old_first = self.first_lookup[key]\n            new_first = new_first_lookup[key]\n            if old_first is not new_first:\n                print('key=%r needs FIRST_DICT update' % (key,))", "response": "This function will take the first node of the tree and the last node of the tree and return the new root node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntraverse the tree and yield nodes.", "response": "def _traverse_nodes(self):\n        \"\"\" Debugging function (exposes cython nodes as dummy nodes) \"\"\"\n        node = self.root\n        stack = []\n        while stack or node is not None:\n            if node is not None:\n                stack.append(node)\n                node = node.left\n            else:\n                node = stack.pop()\n                yield node\n                node = node.right"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a networkx representation of the binary search tree.", "response": "def to_networkx(self, labels=None, edge_labels=False):\n        \"\"\" Get a networkx representation of the binary search tree. \"\"\"\n        import networkx as nx\n        graph = nx.DiGraph()\n        for node in self._traverse_nodes():\n            u = node.key\n            graph.add_node(u)  # Minor redundancy\n            # Set node properties\n            graph.nodes[u]['value'] = node.value\n            if labels is not None:\n                label = ','.join([str(getattr(node, k)) for k in labels])\n                graph.nodes[u]['label'] = label\n            if node.left is not None:\n                v = node.left.key\n                graph.add_node(v)\n                graph.add_edge(u, v)\n                if edge_labels:\n                    graph.edge[u][v]['label'] = 'L'\n            if node.right is not None:\n                v = node.right.key\n                graph.add_node(v)\n                graph.add_edge(u, v)\n                if edge_labels:\n                    graph.edge[u][v]['label'] = 'R'\n        return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef repr_tree(self):\n        import utool as ut\n        import networkx as nx\n        repr_tree = nx.DiGraph()\n        for u, v in ut.itertwo(self.values()):\n            if not repr_tree.has_edge(v, u):\n                repr_tree.add_edge(u, v)\n        return repr_tree", "response": "reconstruct represented tree as a DiGraph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_proteins_psms_for_map(self):\n        fields = ['p.gene_acc', 'sets.set_name',\n                  'pep.sequence', 'psm.psm_id']\n        firstjoin = ('protein_psm', 'pp', 'protein_acc')\n        genetable = 'genes'\n        return self.get_unique_gene_psms(genetable, fields, firstjoin)", "response": "Gets the gene - PSM combinations for a given protein map."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike os. path. join but uses forward slashes on win32", "response": "def unixjoin(*args):\n    \"\"\"\n    Like os.path.join, but uses forward slashes on win32\n    \"\"\"\n    isabs_list = list(map(isabs, args))\n    if any(isabs_list):\n        poslist = [count for count, flag in enumerate(isabs_list) if flag]\n        pos = poslist[-1]\n        return '/'.join(args[pos:])\n    else:\n        return '/'.join(args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfeed it with a target and decoy score and the protein / gene id names and this will return the target and decoy type the winning ID and the score", "response": "def pick_target_decoy(tfeature, dfeature):\n    \"\"\"Feed it with a target and decoy score and the protein/gene/id names,\n    and this will return target/decoy type, the winning ID and the score\"\"\"\n    tscore, dscore = get_score(tfeature), get_score(dfeature)\n    if tscore == dscore:\n        # same score or both False\n        return False\n    elif False in [tscore, dscore]:\n        # return the non-False feature\n        return [v for k, v in {tscore: tfeature, dscore: dfeature}.items()\n                if k is not False][0]\n    elif tscore > dscore:\n        return tfeature\n    elif tscore < dscore:\n        return dfeature\n    else:\n        # in case uncaught edgecase occurs\n        print('WARNING, target score {} and decoy score {} could not be '\n              'compared'.format(tscore, dscore))\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nloop through peptides stores sequences mapped to PSM ids.", "response": "def create_merge_psm_map(peptides, ns):\n    \"\"\"Loops through peptides, stores sequences mapped to PSM ids.\"\"\"\n    psmmap = {}\n    for peptide in peptides:\n        seq = reader.get_peptide_seq(peptide, ns)\n        psm_ids = reader.get_psm_ids_from_peptide(peptide, ns)\n        for psm_id in psm_ids:\n            try:\n                psmmap[seq][psm_id.text] = 1\n            except KeyError:\n                psmmap[seq] = {psm_id.text: 2}\n    for seq, psm_id_dict in psmmap.items():\n        psmmap[seq] = [x for x in psm_id_dict]\n    return psmmap"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloops peptides from multiple files fetches PSMs from sequence : PSM map outputs correctly PSM mapped peptides", "response": "def merge_peptides(fns, ns):\n    \"\"\"Loops peptides from multiple files, fetches PSMs from\n    sequence:PSM map, outputs correctly PSM mapped peptides\"\"\"\n    peptides_to_map = reader.generate_peptides_multiple_fractions(fns, ns)\n    psmmap = create_merge_psm_map(peptides_to_map, ns)\n    peptides = reader.generate_peptides_multiple_fractions(fns, ns)\n    for peptide in peptides:\n        seq = reader.get_peptide_seq(peptide, ns)\n        psm_ids = reader.get_psm_ids_from_peptide(peptide, ns)\n        # remove current psm ids, repopulate with stored ones\n        psm_ids.clear()\n        for new_psm_id in psmmap[seq]:\n            etree.SubElement(psm_ids, 'psm_id').text = new_psm_id\n        yield formatting.string_and_clear(peptide, ns)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields a generator that iterates through proteins of each PSM or PETTAG element and yields a string of the PSM and PETTAG elements that do not match any of the headers.", "response": "def protein_header_split_generator(elements, headers, ns):\n    \"\"\"Loop through proteins of each PSM/peptide. If a protein does not\n    match any of headers, discard PSM/peptide immediately\"\"\"\n    for el in elements:\n        header_not_matching = False\n        for protein in el.findall('{%s}protein_id' % ns['xmlns']):\n            if not any((re.search(h, protein.text) for h in headers)):\n                header_not_matching = True\n                break\n        if header_not_matching:\n            formatting.clear_el(el)\n        else:\n            yield formatting.string_and_clear(el, ns)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_pool_b(\n    dsn=None,\n    *,\n    min_size=10,\n    max_size=10,\n    max_queries=50000,\n    max_inactive_connection_lifetime=300.0,\n    setup=None,\n    init=None,\n    loop=None,\n    connection_class=BuildPgConnection,\n    **connect_kwargs,\n):\n    \"\"\"\n    Create a connection pool.\n\n    Can be used either with an ``async with`` block:\n\n    Identical to ``asyncpg.create_pool`` except that both the pool and connection have the *_b varients of\n    ``execute``, ``fetch``, ``fetchval``, ``fetchrow`` etc\n\n    Arguments are exactly the same as ``asyncpg.create_pool``.\n    \"\"\"\n    return BuildPgPool(\n        dsn,\n        connection_class=connection_class,\n        min_size=min_size,\n        max_size=max_size,\n        max_queries=max_queries,\n        loop=loop,\n        setup=setup,\n        init=init,\n        max_inactive_connection_lifetime=max_inactive_connection_lifetime,\n        **connect_kwargs,\n    )", "response": "Create a connection pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy(self):\n        if self.debug: print(\"Coping.....\"+self.id)\n        r = Runnable(self.id, self.component, self.parent)\n        copies = dict()\n\n        # Copy simulation time parameters\n        r.time_step = self.time_step\n        r.time_completed = self.time_completed\n        r.time_total = self.time_total\n\n        # Plasticity and state stack (?)\n        r.plastic = self.plastic\n        r.state_stack = Stack()\n        \n        # Copy variables (GG - Faster using the add_* methods?)\n        for v in self.instance_variables:\n            r.instance_variables.append(v)\n            r.__dict__[v] = self.__dict__[v]\n            r.__dict__[v + '_shadow'] = self.__dict__[v + '_shadow']\n        \n        for v in self.derived_variables:\n            r.derived_variables.append(v)\n            r.__dict__[v] = self.__dict__[v]\n            r.__dict__[v + '_shadow'] = self.__dict__[v + '_shadow']\n        \n        # Copy array elements\n        for child in self.array:\n            child_copy = child.copy()\n            child_copy.parent = r\n            r.array.append(child_copy)\n            copies[child.uid] = child_copy\n            \n        # Copy attachment def\n        for att in self.attachments:\n            atn = self.attachments[att]\n            r.attachments[att] = atn\n            r.__dict__[atn] = []\n\n        # Copy children\n        for uid in self.uchildren:\n            child = self.uchildren[uid]\n            child_copy = child.copy()\n            child_copy.parent = r\n            copies[child.uid] = child_copy\n            \n            r.add_child(child_copy.id, child_copy)\n\n            # For typerefs\n            try:\n                idx = [k for k in self.__dict__ if self.__dict__[k] == child][0]\n                r.__dict__[idx] = child_copy\n            except:\n                pass\n\n            # For groups and attachments:\n            try:\n                idx = [k for k in self.__dict__ if child in self.__dict__[k]][0]\n                if idx not in r.__dict__:\n                    r.__dict__[idx] = []\n                r.__dict__[idx].append(child_copy)\n\n            except:\n                pass\n               \n        # Copy event ports\n        for port in self.event_in_ports:\n            r.event_in_ports.append(port)\n            r.event_in_counters[port] = 0\n\n        for port in self.event_out_ports:\n            r.event_out_ports.append(port)\n            r.event_out_callbacks[port] = self.event_out_callbacks[port]\n        \n        for ec in r.component.structure.event_connections:\n            if self.debug: print(\"--- Fixing event_connection: %s in %s\"%(ec.toxml(), id(r)))\n            \n            source = r.parent.resolve_path(ec.from_)\n            target = r.parent.resolve_path(ec.to)\n            \n            if ec.receiver:\n                # Will throw error...\n                receiver_template = self.build_runnable(ec.receiver, target)                                \n                #receiver = copy.deepcopy(receiver_template)\n                receiver = receiver_template.copy()\n                receiver.id = \"{0}__{1}__\".format(component.id,\n                                                  receiver_template.id)\n\n                if ec.receiver_container:\n                    target.add_attachment(receiver, ec.receiver_container)\n                target.add_child(receiver_template.id, receiver)\n                target = receiver\n            else:\n                source = r.resolve_path(ec.from_)\n                target = r.resolve_path(ec.to)\n\n            source_port = ec.source_port\n            target_port = ec.target_port\n\n            if not source_port:\n                if len(source.event_out_ports) == 1:\n                    source_port = source.event_out_ports[0]\n                else:\n                    raise SimBuildError((\"No source event port \"\n                                         \"uniquely identifiable\"\n                                         \" in '{0}'\").format(source.id))\n            if not target_port:\n                if len(target.event_in_ports) == 1:\n                    target_port = target.event_in_ports[0]\n                else:\n                    raise SimBuildError((\"No destination event port \"\n                                         \"uniquely identifiable \"\n                                         \"in '{0}'\").format(target))\n             \n            if self.debug: print(\"register_event_out_callback\\n   Source: %s, %s (port: %s) \\n   -> %s, %s (port: %s)\"%(source, id(source), source_port, target, id(target), target_port))\n            source.register_event_out_callback(\\\n                source_port, lambda: target.inc_event_in(target_port))\n            \n        \n            \n\n        # Copy methods\n        if getattr(self, \"update_kinetic_scheme\", None): r.update_kinetic_scheme = self.update_kinetic_scheme\n        if getattr(self, \"run_startup_event_handlers\", None): r.run_startup_event_handlers = self.run_startup_event_handlers\n        if getattr(self, \"run_preprocessing_event_handlers\", None): r.run_preprocessing_event_handlers = self.run_preprocessing_event_handlers\n        if getattr(self, \"run_postprocessing_event_handlers\", None): r.run_postprocessing_event_handlers = self.run_postprocessing_event_handlers\n        \n        if getattr(self, \"update_state_variables\", None): r.update_state_variables = self.update_state_variables\n        if getattr(self, \"update_derived_variables\", None): r.update_derived_variables = self.update_derived_variables\n        #r.update_shadow_variables = self.update_shadow_variables\n        if getattr(self, \"update_derived_parameters\", None): r.update_derived_parameters = self.update_derived_parameters\n\n        for rn in self.regimes:\n            r.add_regime(self.regimes[rn])\n        r.current_regime = self.current_regime\n\n        # Copy groups\n        for gn in self.groups:\n            g = self.__dict__[gn]\n            for c in g:\n                if c.uid in copies:\n                    r.add_child_to_group(gn, copies[c.uid])\n                else:\n                    c2 = c.copy()\n                    c2.parent = r\n                    copies[c.uid] = c2\n                    r.add_child_to_group(gn, c2)\n                    \n\n        # Copy remaining runnable references.\n        for k in self.__dict__:\n            if k == 'parent':\n                continue\n            c = self.__dict__[k]\n            if isinstance(c, Runnable):\n                if c.uid in copies:\n                    r.__dict__[k] = copies[c.uid]\n                else:\n                    c2 = c.copy()\n                    c2.parent = r\n                    copies[c.uid] = c2\n                    r.__dict__[k] = c2\n           \n        # Copy text fields\n        for k in self.__dict__:          \n            if not k in r.__dict__:\n                c = self.__dict__[k]\n                if self.debug: print(\"Adding remaining field: %s = %s\"%(k,c))\n                r.__dict__[k] = c\n                \n        if self.debug:\n            print('########################################')\n            keys = list(self.__dict__.keys())\n            keys.sort()\n            print(len(keys))\n            for k in keys:\n                print(k, self.__dict__[k])\n            print('----------------------------------------')\n            keys = list(r.__dict__.keys())\n            keys.sort()\n            print(len(keys))\n            for k in keys:\n                print(k, r.__dict__[k])\n            print('########################################')\n            print('')\n            print('')\n            print('')\n            print('')\n            \n        if self.debug: print(\"Finished coping...\"+self.id)\n        \n        return r", "response": "Make a copy of this runnable."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a runnable component to the list of runnable components in this simulation.", "response": "def add_runnable(self, runnable):\n        \"\"\"\n        Adds a runnable component to the list of runnable components in\n        this simulation.\n\n        @param runnable: A runnable component\n        @type runnable: lems.sim.runnable.Runnable\n        \"\"\"\n\n        if runnable.id in self.runnables:\n            raise SimError('Duplicate runnable component {0}'.format(runnable.id))\n\n        self.runnables[runnable.id] = runnable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef controller_creatr(filename):\n    if not check():\n        click.echo(Fore.RED + 'ERROR: Ensure you are in a bast app to run the create:controller command')\n        return\n\n    path = os.path.abspath('.') + '/controller'\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    # if os.path.isfile(path + )\n\n    file_name = str(filename + '.py')\n    if os.path.isfile(path+\"/\" + file_name):\n        click.echo(Fore.WHITE + Back.RED + \"ERROR: Controller file exists\")\n        return\n    controller_file = open(os.path.abspath('.') + '/controller/' + file_name, 'w+')\n    compose = \"from bast import Controller\\n\\nclass \" + filename + \"(Controller):\\n    pass\"\n    controller_file.write(compose)\n    controller_file.close()\n    click.echo(Fore.GREEN + \"Controller \" + filename + \" created successfully\")", "response": "Create a new controller file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef view_creatr(filename):\n    if not check():\n        click.echo(Fore.RED + 'ERROR: Ensure you are in a bast app to run the create:view command')\n        return\n\n    path = os.path.abspath('.') + '/public/templates'\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    filename_ = str(filename + \".html\").lower()\n    view_file = open(path + \"/\" + filename_, 'w+')\n    view_file.write(\"\")\n    view_file.close()\n    click.echo(Fore.GREEN + \"View file \" + filename_ + \"created in public/template folder\")", "response": "Create a new view file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_new(projectname):\n    git_url = \"https://github.com/moluwole/Bast_skeleton\"\n    path = os.path.abspath('.') + \"/\" + projectname\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    click.echo(Fore.GREEN + '    ___  ___   __________')\n    click.echo(Fore.GREEN + '  / _ )/ _ | / __/_  __/')\n    click.echo(Fore.GREEN + ' / _  / __ |_\\ \\  / /')\n    click.echo(Fore.GREEN + '/____/_/ |_/___/ /_/')\n\n    click.echo(Fore.GREEN + \"Creating Project at %s.... \" % path)\n    click.echo(Fore.GREEN + \"Pulling Project Skeleton from Repo\")\n    try:\n        Repo.clone_from(git_url, path)\n\n        click.echo(Fore.GREEN + \"Setting up project\")\n\n        shutil.rmtree(path + \"/.git\")\n\n        if not os.path.exists('/.env'):\n            shutil.copy(path + '/.env.example', path + '/.env')\n\n        env_file = path + \"/.env\"\n        if not os.path.isfile(env_file):\n            shutil.copy('.env.example', '.env')\n        call(['panther', 'generate:key', path])\n\n        click.echo(Fore.GREEN + \"New Bast Project created at  %s \" % path)\n    except Exception as e:\n        click.echo(Fore.RED + \"An error occurred creating a new project. Try Again.\\n Reason: {}\".format(e))", "response": "Create a new Bast project"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a migration file", "response": "def migration_creatr(migration_file, create, table):\n    \"\"\"Name of the migration file\"\"\"\n    if not check():\n        click.echo(Fore.RED + 'ERROR: Ensure you are in a bast app to run the create:migration command')\n        return\n\n    migration = CreateMigration()\n    if table is None:\n        table = snake_case(migration_file)\n    file = migration.create_file(snake_case(migration_file), table=table, create=create)\n    click.echo(Fore.GREEN + 'Migration file created at %s' % file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a message to the specified socket.", "response": "def send_msg(self, connection, data):\r\n        \"\"\"\r\n        Function to send messages\r\n        \r\n        Parameters\r\n        ----------\r\n        connection: socket or connection\r\n        data: data that can be serialized to json\r\n        \"\"\"\r\n        # serialize as JSON\r\n        msg = json.dumps(data)\r\n        # Prefix each message with a 4-byte length (network byte order)\r\n        msg = struct.pack('>I', len(msg)).decode() + msg\r\n        connection.sendall(msg.encode())\r\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recv_msg(self, connection):\r\n        # Read message length and unpack it into an integer\r\n        raw_msglen = self.__recvall(connection, 4, decode_json=False)\r\n        if not raw_msglen:\r\n            return None\r\n        msglen = struct.unpack('>I', raw_msglen)[0]\r\n        # Read the message data\r\n        return self.__recvall(connection, msglen)", "response": "Read a message from the socket and return it as a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, data):\r\n        answer = None\r\n        try:\r\n            logging.info(\"Client conntecting to {server}\".format(server=self.server_address))\r\n            if six.PY2:\r\n                sock = socket.socket(family=socket.AF_UNIX, type=socket.SOCK_STREAM)\r\n                answer = self.sending(sock, data)\r\n                sock.close()\r\n            else:\r\n                with socket.socket(family=socket.AF_UNIX, type=socket.SOCK_STREAM) as sock:\r\n                    answer = self.sending(sock, data)\r\n\r\n        except socket.error as e:\r\n            logging.error(\"Client cannot conntect to {server}: {msg}\".format(server=self.server_address, msg=e.strerror))\r\n            return None\r\n\r\n        return answer", "response": "Send date to server"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquit the server and join it.", "response": "def quit(self):\r\n        \"\"\"\r\n        Quit socket server\r\n        \"\"\"\r\n        logging.info(\"quiting sock server\")\r\n        if self.__quit is not None:\r\n            self.__quit.set()\r\n        self.join()\r\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\r\n        # Bind the socket to the port\r\n        logging.info(\"Server starts socket on {addr}\".format(addr=self.server_address))\r\n\r\n        # Create a UDS socket\r\n        if six.PY2:\r\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\r\n            self.listen(sock)\r\n            sock.close()\r\n        else:\r\n            with socket.socket(socket.AF_UNIX, socket.SOCK_STREAM) as sock:\r\n                self.listen(sock)\r\n        try:\r\n            os.unlink(self.server_address)\r\n        except OSError:\r\n            if os.path.exists(self.server_address):\r\n                raise\r\n        return", "response": "Loop for server. Executed via Sock_Server. start."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef si_round(val):\n    '''\n    round to a \"scientific notation\" tuple of (factor, exponent)\n    such that 1 < factor < 1000, and factor * 10 ** exponent == val\n    '''\n    if val < 0:\n        neg = True\n        val = -val\n    elif val == 0:\n        return 0, 0\n    else:\n        neg = False\n    exp = math.log(val) / math.log(1000)\n    if exp < 0:\n        exp = int(exp) - 1\n    else:\n        exp = int(exp)\n    val = val / 1000.0 ** exp\n    if neg:\n        val = -val\n    return val, 3 * exp", "response": "round to a scientific notation tuple of factor and exponent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_quantcols(pattern, oldheader, coltype):\n    if pattern is None:\n       return False\n    if coltype == 'precur':\n        return reader.get_cols_in_file(pattern, oldheader, single_col=True)", "response": "Searches for quantification columns using pattern and header list. Returns a single column for\n    precursor quant."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_peptide_quant(quantdata, quanttype):\n    parsefnx = {'precur': max}\n    quantfloats = []\n    for q in quantdata:\n        try:\n            quantfloats.append(float(q))\n        except(TypeError, ValueError):\n            pass\n    if not quantfloats:\n        return 'NA'\n    return str(parsefnx[quanttype](quantfloats))", "response": "Parses a list of quantdata and returns maxvalue from them. Strips NA"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef safe_str(value):\n    if sys.version_info < (3,0) and isinstance(value, unicode):\n        return value.encode('utf-8')\n    else:\n        return str(value)", "response": "Returns a string representation of the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef safe_unicode(value):\n    if sys.version_info < (3,0):\n        if isinstance(value, str):\n            return value.decode('utf-8')\n        else:\n            return unicode(value)\n    else:\n        return str(value)", "response": "Returns a unicode instance in Python 2. x or str in Python 3. x."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading csv in unicode", "response": "def read_csv(fpath):\n    \"\"\" reads csv in unicode \"\"\"\n    import csv\n    import utool as ut\n    #csvfile = open(fpath, 'rb')\n    with open(fpath, 'rb') as csvfile:\n        row_iter = csv.reader(csvfile, delimiter=str(','), quotechar=str('|'))\n        row_list = [ut.lmap(ut.ensure_unicode, row) for row in row_iter]\n    return row_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_csv_table(column_list=[], column_lbls=None, header='',\n                   column_type=None, row_lbls=None, transpose=False,\n                   precision=2, use_lbl_width=True, comma_repl='<com>',\n                   raw=False, new=False, standardize=False):\n    \"\"\"\n    Creates a csv table with aligned columns\n\n    make_csv_table\n\n    Args:\n        column_list (list):\n        column_lbls (None):\n        header (str):\n        column_type (None):\n        row_lbls (None):\n        transpose (bool):\n\n    Returns:\n        str: csv_text\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_csv import *  # NOQA\n        >>> column_list = [[1, 2, 3], ['A', 'B', 'C']]\n        >>> column_lbls = ['num', 'alpha']\n        >>> header = '# Test CSV'\n        >>> column_type = (int, str)\n        >>> row_lbls = None\n        >>> transpose = False\n        >>> csv_text = make_csv_table(column_list, column_lbls, header, column_type, row_lbls, transpose)\n        >>> result = csv_text\n        >>> print(result)\n        # Test CSV\n        # num_rows=3\n        #   num,  alpha\n              1,      A\n              2,      B\n              3,      C\n    \"\"\"\n    import utool as ut\n\n    assert comma_repl.find(',') == -1, 'comma_repl cannot contain a comma!'\n    if transpose:\n        column_lbls, row_lbls = row_lbls, column_lbls\n        column_list = list(map(list, zip(*column_list)))\n    if row_lbls is not None:\n        if isinstance(column_list, np.ndarray):\n            column_list = column_list.tolist()\n        if isinstance(row_lbls, np.ndarray):\n            row_lbls = row_lbls.tolist()\n        column_list = [row_lbls] + column_list\n        column_lbls = ['ROWLBL'] + list(map(six.text_type, column_lbls))\n        if column_type is not None:\n            column_type =  [six.text_type] + column_type\n    if len(column_list) == 0:\n        print('[csv] No columns')\n        return header\n    column_len = [len(col) for col in column_list]\n    num_data = column_len[0]\n    if num_data == 0:\n        #print('[csv.make_csv_table()] No data. (header=%r)' % (header,))\n        return header\n    if any([num_data != clen for clen in column_len]):\n        print('[csv] column_lbls = %r ' % (column_lbls,))\n        print('[csv] column_len = %r ' % (column_len,))\n        print('[csv] inconsistent column lengths')\n        return header\n\n    if column_type is None:\n        column_type = list(map(type, ut.get_list_column(column_list, 0)))\n        #column_type = [type(col[0]) for col in column_list]\n\n    csv_rows = []\n    if new:\n        csv_rows.append(header)\n    elif not raw:\n        csv_rows.append(header)\n        if not standardize:\n            csv_rows.append('# num_rows=%r' % num_data)\n\n    column_maxlen = []\n    column_str_list = []\n\n    if column_lbls is None:\n        column_lbls = [''] * len(column_list)\n\n    def _toint(c):\n        if c is None:\n            return 'None'\n        try:\n            if np.isnan(c):\n                return 'nan'\n        except TypeError as ex:\n            print('------')\n            print('[csv] TypeError %r ' % ex)\n            print('[csv] _toint(c) failed')\n            print('[csv] c = %r ' % c)\n            print('[csv] type(c) = %r ' % type(c))\n            print('------')\n            raise\n        return ('%d') % int(c)\n\n    import uuid\n    textable_types = [uuid.UUID, six.text_type]\n\n    try:\n        if standardize:\n            def csv_format(r):\n                text = ut.repr2(r, precision=precision)\n                #text = six.text_type(r)\n                # Check if needs escape\n                escape_chars = ['\"', ' ', ',']\n                if any([c in text for c in escape_chars]):\n                    # escape quotes with quotes\n                    text = text.replace('\"', '\"\"')\n                    # encapsulate with quotes\n                    text = '\"' + text + '\"'\n                return text\n            for col, lbl, coltype in zip(column_list, column_lbls, column_type):\n                col_str = [csv_format(r) for r in col]\n                column_str_list.append(col_str)\n                pass\n        else:\n            # Loop over every column\n            for col, lbl, coltype in zip(column_list, column_lbls, column_type):\n                # Loop over every row in the column (using list comprehension)\n                if coltype is list or util_type.is_list(coltype):\n                    col_str = [six.text_type(c).replace(',', ' ').replace('.', '<dot>')\n                               for c in col]\n                elif (coltype is float or\n                      util_type.is_float(coltype) or\n                      coltype == np.float32 or\n                      util_type.is_valid_floattype(coltype)):\n                    precision_fmtstr = '%.' + six.text_type(precision) + 'f'\n                    col_str = ['None' if r is None else precision_fmtstr % float(r)\n                               for r in col]\n                    #col_ = [r if r is None else float(r) for r in col]\n                    #col_str = [ut.repr2(r, precision=2) for r in col_]\n                elif coltype is int or util_type.is_int(coltype) or coltype == np.int64:\n                    col_str = [_toint(c) for c in (col)]\n                elif coltype in textable_types or util_type.is_str(coltype):\n                    col_str = [six.text_type(c).replace(',', comma_repl) for c in col]\n                else:\n                    print('[csv] is_unknown coltype=%r' % (coltype,))\n                    try:\n                        col_str = [six.text_type(c) for c in (col)]\n                    except UnicodeDecodeError:\n                        try:\n                            col_str = [ut.ensure_unicode(c) for c in (col)]\n                        except Exception:\n                            col_str = [repr(c) for c in (col)]\n                column_str_list.append(col_str)\n\n        for col_str, lbl in zip(column_str_list, column_lbls):\n            col_lens = [len(s) for s in (col_str)]\n            max_len  = max(col_lens)\n            if use_lbl_width:\n                # The column label counts towards the column width\n                max_len  = max(len(lbl), max_len)\n            column_maxlen.append(max_len)\n    except Exception as ex:\n        #ut.embed()\n        ut.printex(ex, keys=['col', 'lbl', 'coltype'])\n        raise\n\n    def _fmtfn(maxlen):\n        return  ''.join(['%', six.text_type(maxlen + 2), 's'])\n    fmtstr = ','.join([_fmtfn(maxlen) for maxlen in column_maxlen])\n    try:\n        if new:\n            csv_rows.append('# ' + fmtstr % tuple(column_lbls))\n        elif not raw:\n            csv_rows.append('# ' + fmtstr % tuple(column_lbls))\n            #csv_rows.append('# ' + fmtstr % column_lbls)\n    except Exception as ex:\n        #print(len(column_list))\n        #ut.embed()\n        ut.printex(ex, keys=['fmtstr', 'column_lbls'])\n        raise\n    for row in zip(*column_str_list):\n        csv_rows.append('  ' + fmtstr % row)\n\n    csv_text = '\\n'.join(csv_rows)\n    return csv_text", "response": "Function creates a csv table with aligned columns"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles a ping packet from registry_client only if the node_ids present in the ping payload are registered and the pong should be sent.", "response": "def _handle_ping(self, packet, protocol):\n        \"\"\" Responds to pings from registry_client only if the node_ids present in the ping payload are registered\n\n        :param packet: The 'ping' packet received\n        :param protocol: The protocol on which the pong should be sent\n        \"\"\"\n        if 'payload' in packet:\n            is_valid_node = True\n            node_ids = list(packet['payload'].values())\n            for node_id in node_ids:\n                if self._repository.get_node(node_id) is None:\n                    is_valid_node = False\n                    break\n            if is_valid_node:\n                self._pong(packet, protocol)\n        else:\n            self._pong(packet, protocol)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall splitter to split percolator output into target and decoy elements.", "response": "def set_features(self, filter_type):\n        \"\"\"Calls splitter to split percolator output into target/decoy\n        elements.\n        Writes two new xml files with features. Currently only psms and\n        peptides. Proteins not here, since one cannot do protein inference\n        before having merged and remapped multifraction data anyway.\n        \"\"\"\n        elements_to_split = {'psm': self.allpsms, 'peptide': self.allpeps}\n        self.features = self.splitfunc(elements_to_split, self.ns, filter_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges all psms and peptides", "response": "def set_features(self):\n        \"\"\"\"Merge all psms and peptides\"\"\"\n        allpsms_str = readers.generate_psms_multiple_fractions_strings(\n            self.mergefiles, self.ns)\n        allpeps = preparation.merge_peptides(self.mergefiles, self.ns)\n        self.features = {'psm': allpsms_str, 'peptide': allpeps}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef std_build_command(repo='.'):\n    import utool as ut\n    print('+**** stdbuild *******')\n    print('repo = %r' % (repo,))\n    if sys.platform.startswith('win32'):\n        # vtool --rebuild-sver didnt work with this line\n        #scriptname = './mingw_build.bat'\n        scriptname = 'mingw_build.bat'\n    else:\n        scriptname = './unix_build.sh'\n    if repo == '':\n        # default to cwd\n        repo = '.'\n    else:\n        os.chdir(repo)\n    ut.assert_exists(scriptname)\n    normbuild_flag = '--no-rmbuild'\n    if ut.get_argflag(normbuild_flag):\n        scriptname += ' ' + normbuild_flag\n    # Execute build\n    ut.cmd(scriptname)\n    #os.system(scriptname)\n    print('L**** stdbuild *******')", "response": "This function is used to build a standard build script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting until the Heroku Connect connection is no longer in the IMPORT_CONFIGURATION state.", "response": "def wait_for_import(self, connection_id, wait_interval):\n        \"\"\"\n        Wait until connection state is no longer ``IMPORT_CONFIGURATION``.\n\n        Args:\n            connection_id (str): Heroku Connect connection to monitor.\n            wait_interval (int): How frequently to poll in seconds.\n\n        Raises:\n            CommandError: If fetch connection information fails.\n\n        \"\"\"\n        self.stdout.write(self.style.NOTICE('Waiting for import'), ending='')\n        state = utils.ConnectionStates.IMPORT_CONFIGURATION\n        while state == utils.ConnectionStates.IMPORT_CONFIGURATION:\n            # before you get the first state, the API can be a bit behind\n            self.stdout.write(self.style.NOTICE('.'), ending='')\n            time.sleep(wait_interval)  # take a breath\n            try:\n                connection = utils.get_connection(connection_id)\n            except requests.HTTPError as e:\n                raise CommandError(\"Failed to fetch connection information.\") from e\n            else:\n                state = connection['state']\n        self.stdout.write(self.style.NOTICE(' Done!'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup(self):\n        if self.dry_run is not True:\n            self.client = self._get_client()\n            self._disable_access_key()", "response": "Method runs the plugin"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self):\n        try:\n            response = self.client.get_access_key_last_used(\n                AccessKeyId=self.access_key_id\n            )\n\n            username = response['UserName']\n            access_keys = self.client.list_access_keys(\n                UserName=username\n            )\n\n            for key in access_keys['AccessKeyMetadata']:\n                if \\\n                        (key['AccessKeyId'] == self.access_key_id)\\\n                        and (key['Status'] == 'Inactive'):\n                    return True\n\n            return False\n        except Exception as e:\n            logger.info(\n                \"Failed to validate key disable for \"\n                \"key {id} due to: {e}.\".format(\n                    e=e, id=self.access_key_id\n                )\n            )\n            return False", "response": "Returns whether this plugin does what it claims to have done"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ignores_exc_tb(*args, **kwargs):\n    outer_wrapper = kwargs.get('outer_wrapper', True)\n    def ignores_exc_tb_closure(func):\n        if not IGNORE_TRACEBACK:\n            # if the global enforces that we should not ignore anytracebacks\n            # then just return the original function without any modifcation\n            return func\n        from utool import util_decor\n        #@wraps(func)\n        def wrp_noexectb(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except Exception:\n                # Define function to reraise with python 2 syntax\n                #exc_type, exc_value, exc_traceback = sys.exc_info()\n                # Code to remove this decorator from traceback\n                # Remove two levels to remove this one as well\n                exc_type, exc_value, exc_traceback = sys.exc_info()\n                try:\n                    exc_traceback = exc_traceback.tb_next\n                    exc_traceback = exc_traceback.tb_next\n                    #exc_traceback = exc_traceback.tb_next\n                except Exception:\n                    print('too many reraise')\n                    pass\n                raise exc_type, exc_value, exc_traceback\n        if outer_wrapper:\n            wrp_noexectb = util_decor.preserve_sig(wrp_noexectb, func)\n        return wrp_noexectb\n    if len(args) == 1:\n        # called with one arg means its a function call\n        func = args[0]\n        return ignores_exc_tb_closure(func)\n    else:\n        # called with no args means kwargs as specified\n        return ignores_exc_tb_closure", "response": "This function is used to decorate a function that ignores any exception traceback that occurs in the current thread."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_master_proteins(psms, protcol):\n    master_proteins = {}\n    if not protcol:\n        protcol = mzidtsvdata.HEADER_MASTER_PROT\n    for psm in psms:\n        protacc = psm[protcol]\n        if ';' in protacc:\n            continue\n        master_proteins[protacc] = 1\n    if 'NA' in master_proteins:\n        master_proteins.pop('NA')\n    if '' in master_proteins:\n        master_proteins.pop('')\n    for protacc in master_proteins:\n        yield {prottabledata.HEADER_PROTEIN: protacc}", "response": "Fed with a psms generator this returns the master proteins present\n    in the PSM table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_percolator_output(self, fn):\n        ns = xml.get_namespace(fn)\n        static = readers.get_percolator_static_xml(fn, ns)\n        return ns, static", "response": "Returns namespace and static xml from percolator output file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef git_available(func):\n    def inner(*args):\n\n        os.chdir(APISettings.GIT_DIR)\n\n        if call(['git', 'rev-parse']) == 0:\n            return func(*args)\n\n        Shell.fail('There is no git repository!')\n        return exit(1)\n    return inner", "response": "Check if a git repository exists in the given folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a GPU id string to be used for CUDA_VISIBLE_DEVICES.", "response": "def _cuda_get_gpu_spec_string(gpu_ids=None):\n    \"\"\"\n    Build a GPU id string to be used for CUDA_VISIBLE_DEVICES.\n    \"\"\"\n\n    if gpu_ids is None:\n        return ''\n\n    if isinstance(gpu_ids, list):\n        return ','.join(str(gpu_id) for gpu_id in gpu_ids)\n\n    if isinstance(gpu_ids, int):\n        return str(gpu_ids)\n\n    return gpu_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefine argument and return types for the specified C function name", "response": "def prototype(self, name, argtypes, restype=None):\n        \"\"\"Define argument / return types for the specified C function\"\"\"\n        function = self.function(name)\n        function.argtypes = argtypes\n        if restype:\n            function.restype = restype"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the heartbeat interval in milliseconds between heartbeats.", "response": "def heartbeat_interval(self, heartbeat_interval_ms):\n        \"\"\"heartbeat_interval (nsqd 0.2.19+) milliseconds between heartbeats.\n\n        Valid range: 1000 <= heartbeat_interval <= configured_max (-1 disables \n        heartbeats)\n\n        --max-heartbeat-interval (nsqd flag) controls the max\n\n        Defaults to --client-timeout / 2\n        \"\"\"\n\n        assert issubclass(heartbeat_interval_ms.__class__, int)\n\n        return self.__push('heartbeat_interval', heartbeat_interval_ms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the output_buffer_size of the current entry", "response": "def output_buffer_size(self, output_buffer_size_b):\n        \"\"\"output_buffer_size (nsqd 0.2.21+) the size in bytes of the buffer \n        nsqd will use when writing to this client.\n\n        Valid range: 64 <= output_buffer_size <= configured_max (-1 disables \n        output buffering)\n\n        --max-output-buffer-size (nsqd flag) controls the max\n\n        Defaults to 16kb\n        \"\"\"\n\n        assert issubclass(output_buffer_size_b.__class__, int)\n\n        return self.__push('output_buffer_size', output_buffer_size_b)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the deflate level for this connection", "response": "def deflate_level(self, deflate_level):\n        \"\"\"deflate_level (nsqd 0.2.23+) configure the deflate compression level \n        for this connection.\n\n        --max-deflate-level (nsqd flag) configures the maximum allowed value\n\n        Valid range: 1 <= deflate_level <= configured_max\n\n        Higher values mean better compression but more CPU usage for nsqd.\n        \"\"\"\n\n        assert issubclass(deflate_level.__class__, int)\n\n        return self.__push('deflate_level', deflate_level)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the sample rate of the current session.", "response": "def sample_rate(self, sample_rate):\n        \"\"\"sample_rate (nsqd 0.2.25+) sample messages delivered over this \n        connection.\n\n        Valid range: 0 <= sample_rate <= 99 (0 disables sampling)\n\n        Defaults to 0\n        \"\"\"\n\n        assert issubclass(sample_rate.__class__, int)\n\n        return self.__push('sample_rate', sample_rate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef msg_timeout(self, msg_timeout_ms):\n\n        assert issubclass(msg_timeout_ms.__class__, int)\n\n        return self.__push('msg_timeout', msg_timeout_ms)", "response": "set the server - side message timeout \n        for messages delivered to this client"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating iterator to write to new tsv. Contains input tsv lines plus quant data for these.", "response": "def get_psms(self):\n        \"\"\"Creates iterator to write to new tsv. Contains input tsv\n        lines plus quant data for these.\"\"\"\n        self.header, isob_header = prep.get_full_and_isobaric_headers(\n            self.oldheader, self.lookup, self.isobaric, self.precursor)\n        self.psms = prep.generate_psms_quanted(self.lookup, self.fn,\n                                               isob_header, self.oldheader,\n                                               self.isobaric, self.precursor)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_error(self, status_code, **kwargs):\n        reason = self._reason\n\n        if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n            error = []\n            for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n                error.append(line)\n        else:\n            error = None\n        data = {'_traceback': error, 'message': reason, 'code': status_code}\n        content = self.render_exception(**data)\n        self.write(content)", "response": "Handles exceptions from the server and writes the error to the output stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering a template to view the current object", "response": "def view(self, template_name, kwargs=None):\n        \"\"\"\n        Used to render template to view\n\n        Sample usage\n        +++++++++++++\n        .. code:: python\n\n            from bast import Controller\n\n            class MyController(Controller):\n                def index(self):\n                    self.view('index.html')\n        \"\"\"\n        if kwargs is None:\n            kwargs = dict()\n\n        self.add_('session', self.session)\n\n        content = self.render_template(template_name, **kwargs)\n        self.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize(self, method, middleware, request_type):\n        self.method = method\n        self.middleware = middleware\n        self.request_type = request_type", "response": "Override initialize method from Tornado."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the key value pair of the arguments passed as a dict object", "response": "def only(self, arguments):\n        \"\"\"\n        returns the key, value pair of the arguments passed as a dict object\n\n        Sample Usage\n        ++++++++++++++\n        .. code:: python\n\n            from bast import Controller\n\n            class MyController(Controller):\n                def index(self):\n                    data = self.only(['username'])\n\n        Returns only the argument username and assigns it to the data variable.\n        \"\"\"\n        data = {}\n        if not isinstance(arguments, list):\n            arguments = list(arguments)\n\n        for i in arguments:\n            data[i] = self.get_argument(i)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all(self):\n        data = {}\n        args = self.request.arguments\n        for key, value in args.items():\n            data[key] = self.get_argument(key)\n\n        return data", "response": "Returns all the arguments passed with the request\n        Sample Usage Usage\n        ++++++++++++++\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the arguments passed to the route except that set by user", "response": "def except_(self, arguments):\n        \"\"\"\n        returns the arguments passed to the route except that set by user\n\n        Sample Usage\n        ++++++++++++++\n        .. code:: python\n\n            from bast import Controller\n\n            class MyController(Controller):\n                def index(self):\n                    data = self.except_(['arg_name'])\n\n        Returns a dictionary of all arguments except for that provided by as ``arg_name``\n        \"\"\"\n\n        if not isinstance(arguments, list):\n            arguments = list(arguments)\n\n        args = self.request.arguments\n        data = {}\n        for key, value in args.items():\n            if key not in arguments:\n                data[key] = self.get_argument(key)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef json(self, data):\n        self.write(json_.encode(data))\n        self.set_header('Content-type', 'application/json')", "response": "Writes the dictionary to the response and sets the Content - Type to application / json"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the given name. If default is provided it returns the last value. If strip is True the value is stripped from the argument.", "response": "def get_argument(self, name, default=None, strip=True):\n        \"\"\"\n        Returns the value of the argument with the given name.\n\n        If default is not provided, returns ``None``\n\n        If the argument appears in the url more than once, we return the last value.\n\n        The returned value is always unicode\n        \"\"\"\n        return self._get_argument(name, default, self.request.arguments, strip)[name]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of items in the specified database.", "response": "def find(cls, db, *args, **kwargs):\n        \"\"\"\n        Returns a :class:`MongoResultSet` object.\n\n        Example::\n\n            items = Item.find(db, {'title': u'Hello'})\n\n        .. note::\n\n           The arguments are those of pymongo collection's `find` method.\n           A frequent error is to pass query key/value pairs as keyword\n           arguments. This is **wrong**. In most cases you will want to pass\n           a dictionary (\"query spec\") as the first positional argument.\n\n        \"\"\"\n        cls._ensure_indexes(db)\n        docs = db[cls.collection].find(*args, **kwargs)\n        return MongoResultSet(docs, partial(cls.wrap_incoming, db=db))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_one(cls, db, *args, **kwargs):\n        data = db[cls.collection].find_one(*args, **kwargs)\n        if data:\n            return cls.wrap_incoming(data, db)\n        else:\n            return None", "response": "Returns an object that corresponds to given query or None."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, db):\n        assert self.collection\n\n        self._ensure_indexes(db)\n\n        # XXX self.structure belongs to StructuredDictMixin !!\n        outgoing = dict(dict_to_db(self, self.structure))\n\n        object_id = db[self.collection].save(outgoing)\n\n        if self.get('_id') is None:\n            self['_id'] = object_id\n        else:\n            pass\n\n        return object_id", "response": "Saves the object to given database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_id(self):\n        import warnings\n        warnings.warn('{0}.get_id() is deprecated, '\n                      'use {0}.id instead'.format(type(self).__name__),\n                      DeprecationWarning)\n        return self.get('_id')", "response": "Returns object id or None."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a DBRef object for this object or None.", "response": "def get_ref(self):\n        \"\"\" Returns a `DBRef` for this object or ``None``.\n        \"\"\"\n        _id = self.id\n        if _id is None:\n            return None\n        else:\n            return DBRef(self.collection, _id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, db):\n        assert self.collection\n        assert self.id\n\n        db[self.collection].remove(self.id)", "response": "Removes the object from given database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logx(supress_args=[], supress_all_args=False, supress_result=False, logger=logging.getLogger(),\n         debug_level=logging.DEBUG):\n    \"\"\"\n    logs parameters and result\n    takes arguments\n        supress_args - list of parameter names to supress\n        supress_all_args - boolean to supress all arguments\n        supress_result - boolean to supress result\n        receiver - custom logging function which takes a string as input; defaults to logging on stdout\n    \"\"\"\n\n    def decorator(fn):\n        def func(*args, **kwargs):\n            if not supress_all_args:\n                arg_string = \"\"\n                for i in range(0, len(args)):\n                    var_name = fn.__code__.co_varnames[i]\n                    if var_name != \"self\" and var_name not in supress_args:\n                        arg_string += var_name + \":\" + str(args[i]) + \",\"\n                arg_string = arg_string[0:len(arg_string) - 1]\n                string = (RED + BOLD + '>> ' + END + 'Calling {0}({1})'.format(fn.__name__, arg_string))\n                if len(kwargs):\n                    string = (\n                        RED + BOLD + '>> ' + END + 'Calling {0} with args {1} and kwargs {2}'.format(\n                            fn.__name__,\n                            arg_string, kwargs))\n                logger.log(debug_level, string)\n\n            wrapped_fn = fn\n            if not asyncio.iscoroutine(fn):\n                wrapped_fn = asyncio.coroutine(fn)\n            result = yield from wrapped_fn(*args, **kwargs)\n\n            if not supress_result:\n                string = BLUE + BOLD + '<< ' + END + 'Return {0} with result : {1}'.format(fn.__name__, result)\n                logger.log(debug_level, string)\n            return result\n\n        return func\n\n    return decorator", "response": "A function decorator that logs parameters and result of a function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting the time of the record in the specified datefmt.", "response": "def formatTime(self, record, datefmt=None):  # noqa\n        \"\"\"\n        Overrides formatTime method to use datetime module instead of time module\n        to display time in microseconds. Time module by default does not resolve\n        time to microseconds.\n        \"\"\"\n        if datefmt:\n            s = datetime.datetime.now().strftime(datefmt)\n        else:\n            t = datetime.datetime.now().strftime(self.default_time_format)\n            s = self.default_msec_format % (t, record.msecs)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef related_to(self, instance):\n        return self.filter(table_name=instance.table_name, record_id=instance.record_id)", "response": "Filter for all log objects of the same connected model as the given instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef capture_insert_from_model(cls, table_name, record_id, *, exclude_fields=()):\n        exclude_cols = ()\n        if exclude_fields:\n            model_cls = get_connected_model_for_table_name(table_name)\n            exclude_cols = cls._fieldnames_to_colnames(model_cls, exclude_fields)\n\n        raw_query = sql.SQL(\"\"\"\n            SELECT {schema}.hc_capture_insert_from_row(\n              hstore({schema}.{table_name}.*),\n              %(table_name)s,\n              ARRAY[{exclude_cols}]::text[]  -- cast to type expected by stored procedure\n            ) AS id\n            FROM {schema}.{table_name}\n            WHERE id = %(record_id)s\n        \"\"\").format(\n            schema=sql.Identifier(settings.HEROKU_CONNECT_SCHEMA),\n            table_name=sql.Identifier(table_name),\n            exclude_cols=sql.SQL(', ').join(sql.Identifier(col) for col in exclude_cols),\n        )\n        params = {'record_id': record_id, 'table_name': table_name}\n        result_qs = TriggerLog.objects.raw(raw_query, params)\n        return list(result_qs)", "response": "Create a fresh insert record from the current model state in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef capture_update_from_model(cls, table_name, record_id, *, update_fields=()):\n        include_cols = ()\n        if update_fields:\n            model_cls = get_connected_model_for_table_name(table_name)\n            include_cols = cls._fieldnames_to_colnames(model_cls, update_fields)\n        raw_query = sql.SQL(\"\"\"\n            SELECT {schema}.hc_capture_update_from_row(\n              hstore({schema}.{table_name}.*),\n              %(table_name)s,\n              ARRAY[{include_cols}]::text[]  -- cast to type expected by stored procedure\n            ) AS id\n            FROM {schema}.{table_name}\n            WHERE id = %(record_id)s\n        \"\"\").format(\n            schema=sql.Identifier(settings.HEROKU_CONNECT_SCHEMA),\n            table_name=sql.Identifier(table_name),\n            include_cols=sql.SQL(', ').join(sql.Identifier(col) for col in include_cols),\n        )\n        params = {'record_id': record_id, 'table_name': table_name}\n        result_qs = TriggerLog.objects.raw(raw_query, params)\n        return list(result_qs)", "response": "Create a fresh update record from the current model state in the database."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_model(self):\n        model_cls = get_connected_model_for_table_name(self.table_name)\n        return model_cls._default_manager.filter(id=self.record_id).first()", "response": "Fetch the instance of the connected model referenced by this log record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef related(self, *, exclude_self=False):\n        manager = type(self)._default_manager\n        queryset = manager.related_to(self)\n        if exclude_self:\n            queryset = queryset.exclude(id=self.id)\n        return queryset", "response": "Returns a QuerySet for all trigger log objects for the same connected model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies : meth :. TriggerLogAbstract. capture_insert_from_model for this log.", "response": "def capture_insert(self, *, exclude_fields=()):\n        \"\"\"Apply :meth:`.TriggerLogAbstract.capture_insert_from_model` for this log.\"\"\"\n        return self.capture_insert_from_model(self.table_name, self.record_id,\n                                              exclude_fields=exclude_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef capture_update(self, *, update_fields=()):\n        return self.capture_update_from_model(self.table_name, self.record_id,\n                                              update_fields=update_fields)", "response": "Apply : meth :. TriggerLogAbstract. capture_update_from_model for this log."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the names of columns referenced by the given model fields.", "response": "def _fieldnames_to_colnames(model_cls, fieldnames):\n        \"\"\"Get the names of columns referenced by the given model fields.\"\"\"\n        get_field = model_cls._meta.get_field\n        fields = map(get_field, fieldnames)\n        return {f.column for f in fields}"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nre-does the change recorded in this archived trigger log.", "response": "def redo(self):\n        \"\"\"\n        Re-sync the change recorded in this trigger log.\n\n        Creates a ``NEW`` live trigger log from the data in this archived trigger log and sets\n        the state of this archived instance to ``REQUEUED``.\n\n        .. seealso:: :meth:`.TriggerLog.redo`\n\n        Returns:\n            The :class:`.TriggerLog` instance that was created from the data of this archived log.\n\n        \"\"\"\n        trigger_log = self._to_live_trigger_log(state=TRIGGER_LOG_STATE['NEW'])\n        trigger_log.save(force_insert=True)  # make sure we get a fresh row\n        self.state = TRIGGER_LOG_STATE['REQUEUED']\n        self.save(update_fields=['state'])\n        return trigger_log"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _to_live_trigger_log(self, **kwargs):\n        field_names = (field.name for field in TriggerLogAbstract._meta.get_fields())\n        attributes = {name: getattr(self, name) for name in field_names}\n        del attributes['id']  # this is a completely new log, it should get its own id on save\n        attributes.update(kwargs)\n        return TriggerLog(**attributes)", "response": "Make a new non - archived trigger log with duplicate data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_isoquant_data(proteins, quantproteins, quantacc, quantfields):\n    for protein in base_add_isoquant_data(proteins, quantproteins,\n                                          prottabledata.HEADER_PROTEIN,\n                                          quantacc, quantfields):\n        yield protein", "response": "Runs through a protein table and adds quant data from ANOTHER protein\n    table that contains that data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_isoquant_data(peptides, quantpeptides, quantacc, quantfields):\n    for peptide in base_add_isoquant_data(peptides, quantpeptides, \n                                          peptabledata.HEADER_PEPTIDE,\n                                          quantacc, quantfields):\n        yield peptide", "response": "Runs through a peptide table and adds quant data from ANOTHER peptide \n    table that contains that data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map(self, fn):\n        return TimeSeries([(x, fn(y)) for x, y in self.points])", "response": "Run a map function across all y points in the series"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a protein table from a joined lookup table.", "response": "def build_proteintable(pqdb, headerfields, mergecutoff, isobaric=False,\n                       precursor=False, probability=False, fdr=False,\n                       pep=False, genecentric=False):\n    \"\"\"Fetches proteins and quants from joined lookup table, loops through\n    them and when all of a protein's quants have been collected, yields the\n    protein quant information.\"\"\"\n    pdmap = create_featuredata_map(pqdb, genecentric=genecentric,\n                                   psm_fill_fun=pinfo.add_psms_to_proteindata,\n                                   pgene_fill_fun=pinfo.add_protgene_to_protdata,\n                                   count_fun=pinfo.count_peps_psms,\n                                   get_uniques=True)\n    empty_return = lambda x, y, z: {}\n    iso_fun = {True: get_isobaric_quant, False: empty_return}[isobaric]\n    ms1_fun = {True: get_precursor_quant, False: empty_return}[precursor]\n    prob_fun = {True: get_prot_probability,\n                False: empty_return}[probability]\n    fdr_fun = {True: get_prot_fdr,\n               False: empty_return}[fdr]\n    pep_fun = {True: get_prot_pep,\n               False: empty_return}[pep]\n    pdata_fun = {True: get_protein_data_genecentric,\n                 False: get_protein_data}[genecentric is not False]\n    protein_sql, sqlfieldmap = pqdb.prepare_mergetable_sql(precursor, isobaric,\n                                                           probability, fdr,\n                                                           pep)\n    accession_field = prottabledata.ACCESSIONS[genecentric]\n    proteins = pqdb.get_merged_features(protein_sql)\n    protein = next(proteins)\n    outprotein = {accession_field: protein[sqlfieldmap['p_acc']]}\n    check_prot = {k: v for k, v in outprotein.items()}\n    if not mergecutoff or protein_pool_fdr_cutoff(protein, sqlfieldmap,\n                                                  mergecutoff):\n        fill_mergefeature(outprotein, iso_fun, ms1_fun, prob_fun, fdr_fun,\n                          pep_fun, pdata_fun, protein, sqlfieldmap,\n                          headerfields, pdmap, accession_field)\n    for protein in proteins:\n        if mergecutoff and not protein_pool_fdr_cutoff(protein, sqlfieldmap,\n                                                       mergecutoff):\n            continue\n        p_acc = protein[sqlfieldmap['p_acc']]\n        if p_acc != outprotein[accession_field]:\n            # check if protein has been filled, otherwise do not output\n            # sometimes proteins have NA in all fields\n            if outprotein != check_prot:\n                yield outprotein\n            outprotein = {accession_field: p_acc}\n            check_prot = {k: v for k, v in outprotein.items()}\n        fill_mergefeature(outprotein, iso_fun, ms1_fun, prob_fun, fdr_fun,\n                          pep_fun, pdata_fun, protein, sqlfieldmap,\n                          headerfields, pdmap, accession_field)\n    if outprotein != check_prot:\n        yield outprotein"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_feature_generator(self):\n        self.features = preparation.build_proteintable(self.lookup,\n                                                       self.headerfields,\n                                                       self.mergecutoff,\n                                                       self.isobaric,\n                                                       self.precursor,\n                                                       self.probability,\n                                                       self.fdr, self.pep,\n                                                       self.genecentric)", "response": "Generates proteins with quant from the lookup table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of protein accessions and a list of protein groups content from DB. Returns a list of str amounts.", "response": "def count_protein_group_hits(lineproteins, groups):\n    \"\"\"Takes a list of protein accessions and a list of protein groups\n    content from DB. Counts for each group in list how many proteins\n    are found in lineproteins. Returns list of str amounts.\n    \"\"\"\n    hits = []\n    for group in groups:\n        hits.append(0)\n        for protein in lineproteins:\n            if protein in group:\n                hits[-1] += 1\n    return [str(x) for x in hits]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_proteins_psms_for_map(self):\n        fields = ['p.assoc_id', 'sets.set_name',\n                  'pep.sequence', 'psm.psm_id']\n        firstjoin = ('protein_psm', 'pp', 'protein_acc')\n        genetable = self.table_map[self.datatype]['feattable']\n        return self.get_unique_gene_psms(genetable, fields, firstjoin)", "response": "Gets the gene - PSM combinations from the DB and filters out any PSM combinations that are not unique for the given protein."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the logging directory for a specific app", "response": "def get_logging_dir(appname='default'):\n    \"\"\"\n    The default log dir is in the system resource directory\n    But the utool global cache allows for the user to override\n    where the logs for a specific app should be stored.\n\n    Returns:\n        log_dir_realpath (str): real path to logging directory\n    \"\"\"\n    from utool._internal import meta_util_cache\n    from utool._internal import meta_util_cplat\n    from utool import util_cache\n    if appname is None or  appname == 'default':\n        appname = util_cache.get_default_appname()\n    resource_dpath = meta_util_cplat.get_resource_dir()\n    default = join(resource_dpath, appname, 'logs')\n    # Check global cache for a custom logging dir otherwise\n    # use the default.\n    log_dir = meta_util_cache.global_cache_read(logdir_cacheid,\n                                                appname=appname,\n                                                default=default)\n    log_dir_realpath = realpath(log_dir)\n    return log_dir_realpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the path to the log file for the current page.", "response": "def get_log_fpath(num='next', appname=None, log_dir=None):\n    \"\"\"\n    Returns:\n        log_fpath (str): path to log file\n    \"\"\"\n    if log_dir is None:\n        log_dir = get_logging_dir(appname=appname)\n    if not exists(log_dir):\n        os.makedirs(log_dir)\n    if appname is not None:\n        log_fname = appname + '_logs_%04d.out'\n    else:\n        log_fname = 'utool_logs_%04d.out'\n    if isinstance(num, six.string_types):\n        if num == 'next':\n            count = 0\n            log_fpath = join(log_dir, log_fname % count)\n            while exists(log_fpath):\n                log_fpath = join(log_dir, log_fname % count)\n                count += 1\n    else:\n        log_fpath = join(log_dir, log_fname % num)\n    return log_fpath"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_logging_handler(handler, format_='file'):\n    global __UTOOL_ROOT_LOGGER__\n    if __UTOOL_ROOT_LOGGER__ is None:\n        builtins.print('[WARNING] logger not started, cannot add handler')\n        return\n    # create formatter and add it to the handlers\n    #logformat = '%Y-%m-%d %H:%M:%S'\n    #logformat = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    timeformat = '%H:%M:%S'\n    if format_ == 'file':\n        logformat = '[%(asctime)s]%(message)s'\n    elif format_ == 'stdout':\n        logformat = '%(message)s'\n    else:\n        raise AssertionError('unknown logging format_: %r' % format_)\n    # Create formatter for handlers\n    formatter = logging.Formatter(logformat, timeformat)\n    handler.setLevel(logging.DEBUG)\n    handler.setFormatter(formatter)\n    __UTOOL_ROOT_LOGGER__.addHandler(handler)", "response": "Add a logging handler to the internal log list."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_logging():\n    global __UTOOL_ROOT_LOGGER__\n    global __UTOOL_PRINT__\n    global __UTOOL_WRITE__\n    global __UTOOL_FLUSH__\n    if __UTOOL_ROOT_LOGGER__ is not None:\n        # Flush remaining buffer\n        if VERBOSE or LOGGING_VERBOSE:\n            _utool_print()()('<__LOG_STOP__>')\n        _utool_flush()()\n        # Remove handlers\n        for h in __UTOOL_ROOT_LOGGER__.handlers[:]:\n            __UTOOL_ROOT_LOGGER__.removeHandler(h)\n        # Reset objects\n        __UTOOL_ROOT_LOGGER__ = None\n        __UTOOL_PRINT__    = None\n        __UTOOL_WRITE__    = None\n        __UTOOL_FLUSH__    = None", "response": "Restores utool print functions to python defaults"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit a record to the output stream.", "response": "def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s%s\"\n            if six.PY3 or not logging._unicode:  # if no unicode support...\n                stream.write(fs % (msg, self.terminator))\n            else:\n                try:\n                    if (isinstance(msg, unicode) and getattr(stream, 'encoding', None)):\n                        ufs = u'%s%s'\n                        try:\n                            stream.write(ufs % (msg, self.terminator))\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % (msg, self.terminator)).encode(stream.encoding))\n                    else:\n                        stream.write(fs % (msg, self.terminator))\n                except UnicodeError:\n                    stream.write(fs % (msg.encode(\"UTF-8\"), self.terminator.encode(\"UTF-8\")))\n            #self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_list_size(list_, size_):\n    lendiff = (size_) - len(list_)\n    if lendiff > 0:\n        extension = [None for _ in range(lendiff)]\n        list_.extend(extension)", "response": "Allocates more space if needbe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_list(text_list, pattern, flags=0):\n    import re\n    import utool as ut\n    match_list = [re.search(pattern, text, flags=flags) for text in text_list]\n    valid_index_list = [index for index, match in enumerate(match_list) if match is not None]\n    valid_match_list = ut.take(match_list, valid_index_list)\n    return valid_index_list, valid_match_list", "response": "A simple wrapper for search_list that returns a list of valid index and match lists"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing a string replace with a list of search and replacements", "response": "def multi_replace(instr, search_list=[], repl_list=None):\n    \"\"\"\n    Does a string replace with a list of search and replacements\n\n    TODO: rename\n    \"\"\"\n    repl_list = [''] * len(search_list) if repl_list is None else repl_list\n    for ser, repl in zip(search_list, repl_list):\n        instr = instr.replace(ser, repl)\n    return instr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invertible_flatten1(unflat_list):\n    nextnum = functools.partial(six.next, itertools.count(0))\n    # Build an unflat list of flat indexes\n    reverse_list = [[nextnum() for _ in tup] for tup in unflat_list]\n    flat_list = flatten(unflat_list)\n    return flat_list, reverse_list", "response": "r Flattens unflat_list but remember how to reconstruct the unflat_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrebuilding unflat list from invertible_flatten1 COOKIEList", "response": "def unflatten1(flat_list, reverse_list):\n    \"\"\" Rebuilds unflat list from invertible_flatten1\n\n    Args:\n        flat_list (list): the flattened list\n        reverse_list (list): the list which undoes flattenting\n\n    Returns:\n        unflat_list2: original nested list\n\n\n    SeeAlso:\n        invertible_flatten1\n        invertible_flatten2\n        unflatten2\n\n    \"\"\"\n    unflat_list2 = [[flat_list[index] for index in tup]\n                    for tup in reverse_list]\n    return unflat_list2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef invertible_total_flatten(unflat_list):\n    import utool as ut\n    next_list = unflat_list\n    scalar_flags = [not ut.isiterable(item) for item in next_list]\n    invert_stack = []\n    # print('unflat_list = %r' % (unflat_list,))\n    while not all(scalar_flags):\n        unflattenized = [[item] if flag else item\n                         for flag, item in zip(scalar_flags, next_list)]\n        flatter_list, invert_part = ut.invertible_flatten1(unflattenized)\n        # print('flatter_list = %r' % (flatter_list,))\n        for idx in ut.where(scalar_flags):\n            invert_part[idx] = invert_part[idx][0]\n        invert_stack.append(invert_part)\n        next_list = flatter_list\n        scalar_flags = [not ut.isiterable(item) for item in next_list]\n    # invert_part = [None] * len(scalar_flags)\n    # invert_stack.append(invert_part)\n    invert_levels = invert_stack[::-1]\n    flat_list = next_list\n    return flat_list, invert_levels", "response": "r Inverts a list of unflatized items into a single item and returns a tuple of the unflattened items and the order of the elements in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unflatten2(flat_list, cumlen_list):\n    unflat_list2 = [flat_list[low:high] for low, high in\n                    zip(itertools.chain([0], cumlen_list), cumlen_list)]\n    return unflat_list2", "response": "Rebuilds unflat list from invertible_flatten1\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unflat_unique_rowid_map(func, unflat_rowids, **kwargs):\n    import utool as ut\n    # First flatten the list, and remember the original dimensions\n    flat_rowids, reverse_list = ut.invertible_flatten2(unflat_rowids)\n    # Then make the input unique\n    flat_rowids_arr = np.array(flat_rowids)\n    unique_flat_rowids, inverse_unique = np.unique(flat_rowids_arr, return_inverse=True)\n    # Then preform the lookup / implicit mapping\n    unique_flat_vals = func(unique_flat_rowids, **kwargs)\n    # Then broadcast unique values back to original flat positions\n    flat_vals_ = np.array(unique_flat_vals)[inverse_unique]\n    #flat_vals_ = np.array(unique_flat_vals).take(inverse_unique, axis=0)\n    output_shape = tuple(list(flat_rowids_arr.shape) + list(flat_vals_.shape[1:]))\n    flat_vals = np.array(flat_vals_).reshape(output_shape)\n    # Then _unflatten the results to the original input dimensions\n    unflat_vals = ut.unflatten2(flat_vals, reverse_list)\n    return unflat_vals", "response": "This function is used to unflat the unique rowids of a tree structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nslice list and truncates if out of bounds", "response": "def safe_slice(list_, *args):\n    \"\"\" safe_slice(list_, [start], stop, [end], [step])\n        Slices list and truncates if out of bounds\n    \"\"\"\n    if len(args) == 3:\n        start = args[0]\n        stop  = args[1]\n        step  = args[2]\n    else:\n        step = 1\n        if len(args) == 2:\n            start = args[0]\n            stop  = args[1]\n        else:\n            start = 0\n            stop = args[0]\n    len_ = len(list_)\n    if stop > len_:\n        stop = len_\n    return list_[slice(start, stop, step)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef allsame(list_, strict=True):\n    if len(list_) == 0:\n        return True\n    first_item = list_[0]\n    return list_all_eq_to(list_, first_item, strict)", "response": "checks to see if list is equal everywhere\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_all_eq_to(list_, val, strict=True):\n    if util_type.HAVE_NUMPY and isinstance(val, np.ndarray):\n        return all([np.all(item == val) for item in list_])\n    try:\n        # FUTURE WARNING\n        # FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', category=FutureWarning)\n            flags = [item == val for item in list_]\n            return all([np.all(flag) if hasattr(flag, '__array__') else flag\n                        for flag in flags])\n        #return all([item == val for item in list_])\n    except ValueError:\n        if not strict:\n            return all([repr(item) == repr(val) for item in list_])\n        else:\n            raise", "response": "checks to see if all items in the list are equal to a value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlikes np. compress but for lists", "response": "def compress(item_list, flag_list):\n    \"\"\"\n    like np.compress but for lists\n\n    Returns items in item list where the corresponding item in flag list is\n    True\n\n    Args:\n        item_list (list): list of items to mask\n        flag_list (list): list of booleans used as a mask\n\n    Returns:\n        list : filtered_items - masked items\n    \"\"\"\n    assert len(item_list) == len(flag_list), (\n        'lists should correspond. len(item_list)=%r len(flag_list)=%r' %\n        (len(item_list), len(flag_list)))\n    filtered_items = list(util_iter.iter_compress(item_list, flag_list))\n    return filtered_items"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a list of items and returns a list of items with the given indexes.", "response": "def ziptake(items_list, indexes_list):\n    \"\"\"\n    SeeAlso:\n        vt.ziptake\n    \"\"\"\n    return [take(list_, index_list)\n            for list_, index_list in zip(items_list, indexes_list)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning items in item list where the corresponding item in flag_list is true", "response": "def filterfalse_items(item_list, flag_list):\n    \"\"\"\n    Returns items in item list where the corresponding item in flag list is true\n\n    Args:\n        item_list (list): list of items\n        flag_list (list): list of truthy values\n\n    Returns:\n        filtered_items : items where the corresponding flag was truthy\n\n    SeeAlso:\n        util_iter.ifilterfalse_items\n    \"\"\"\n    assert len(item_list) == len(flag_list)\n    filtered_items = list(util_iter.ifilterfalse_items(item_list, flag_list))\n    return filtered_items"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the union of lists.", "response": "def union(*lists, **kwargs):\n    \"\"\"\n    Ignore:\n        %timeit len(reduce(set.union, map(set, x)))\n        %timeit len(ut.union(*x))\n        %timeit len(ut.unique(ut.flatten(ut.lmap(np.unique, x))))\n        %timeit len(ut.unique(ut.flatten(x)))\n        %timeit len(ut.union(*x))\n        %timeit len(ut.list_union(*x))\n        %timeit len(set.union(*[set(list_) for list_ in lists]))\n        %timeit len(set.union(*(set(list_) for list_ in lists)))\n    \"\"\"\n    if kwargs.get('ordered', True):\n        return union_ordered(*lists)\n    else:\n        return list_union(*lists)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_subset_of_any(set_, other_sets):\n    set_ = set(set_)\n    other_sets = map(set, other_sets)\n    return any([set_.issubset(other_set) for other_set in other_sets])", "response": "Tests if set_ is a subset of any set in other_sets\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef priority_sort(list_, priority):\n    # remove requested priority items not in the list\n    priority_ = setintersect_ordered(priority, list_)\n    reordered_list = unique_ordered(priority_ + list_)\n    return reordered_list", "response": "r Sorts the items in a list by a given priority list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef priority_argsort(list_, priority):\n    reordered_list = priority_sort(list_, priority)\n    # FIXME: inefficient\n    sortx = [list_.index(item) for item in reordered_list]\n    return sortx", "response": "r Sorts the items in a list by a given priority list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flag_unique_items(list_):\n    len_ = len(list_)\n    item_to_index = dict(zip(reversed(list_), reversed(range(len_))))\n    flag_list = index_to_boolmask(item_to_index.values(), len_)\n    return flag_list", "response": "Returns a list of flags corresponding to the first time an item is seen\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of flags corresponding to the first time an item is seen", "response": "def iflag_unique_items(list_):\n    \"\"\"\n    Returns a list of flags corresponding to the first time an item is seen\n\n    Args:\n        list_ (list): list of items\n\n    Returns:\n        flag_iter\n    \"\"\"\n    seen = set()\n    def unseen(item):\n        if item in seen:\n            return False\n        seen.add(item)\n        return True\n    flag_iter = (unseen(item) for item in list_)\n    return flag_iter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unique_ordered(list_):\n    list_ = list(list_)\n    flag_list = flag_unique_items(list_)\n    unique_list = compress(list_, flag_list)\n    return unique_list", "response": "Returns unique items in list_ in the order they were seen."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list1 elements that are not in list2. preserves order of list1", "response": "def setdiff(list1, list2):\n    \"\"\"\n    returns list1 elements that are not in list2. preserves order of list1\n\n    Args:\n        list1 (list):\n        list2 (list):\n\n    Returns:\n        list: new_list\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> import utool as ut\n        >>> list1 = ['featweight_rowid', 'feature_rowid', 'config_rowid', 'featweight_forground_weight']\n        >>> list2 = [u'featweight_rowid']\n        >>> new_list = setdiff_ordered(list1, list2)\n        >>> result = ut.repr4(new_list, nl=False)\n        >>> print(result)\n        ['feature_rowid', 'config_rowid', 'featweight_forground_weight']\n    \"\"\"\n    set2 = set(list2)\n    return [item for item in list1 if item not in set2]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if list1 and list2 are not in list1", "response": "def isetdiff_flags(list1, list2):\n    \"\"\"\n    move to util_iter\n    \"\"\"\n    set2 = set(list2)\n    return (item not in set2 for item in list1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the list by the values of one list using key_list", "response": "def sortedby(item_list, key_list, reverse=False):\n    \"\"\" sorts ``item_list`` using key_list\n\n    Args:\n        list_ (list): list to sort\n        key_list (list): list to sort by\n        reverse (bool): sort order is descending (largest first)\n                        if reverse is True else acscending (smallest first)\n\n    Returns:\n        list : ``list_`` sorted by the values of another ``list``. defaults to\n        ascending order\n\n    SeeAlso:\n        sortedby2\n\n    Examples:\n        >>> # ENABLE_DOCTEST\n        >>> import utool\n        >>> list_    = [1, 2, 3, 4, 5]\n        >>> key_list = [2, 5, 3, 1, 5]\n        >>> result = utool.sortedby(list_, key_list, reverse=True)\n        >>> print(result)\n        [5, 2, 3, 1, 4]\n\n    \"\"\"\n    assert len(item_list) == len(key_list), (\n        'Expected same len. Got: %r != %r' % (len(item_list), len(key_list)))\n    sorted_list = [item for (key, item) in\n                   sorted(list(zip(key_list, item_list)), reverse=reverse)]\n    return sorted_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sortedby2(item_list, *args, **kwargs):\n    assert all([len(item_list) == len_ for len_ in map(len, args)])\n    reverse = kwargs.get('reverse', False)\n    key = operator.itemgetter(*range(1, len(args) + 1))\n    tup_list = list(zip(item_list, *args))\n    #print(tup_list)\n    try:\n        sorted_tups = sorted(tup_list, key=key, reverse=reverse)\n    except TypeError:\n        # Python 3 does not allow sorting mixed types\n        def keyfunc(tup):\n            return tuple(map(str, tup[1:]))\n        sorted_tups = sorted(tup_list, key=keyfunc, reverse=reverse)\n    sorted_list = [tup[0] for tup in sorted_tups]\n    return sorted_list", "response": "This function sorts the items of a list using key_list\nVIRTUAL_DOMAINS"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef argsort(*args, **kwargs):\n    if len(args) == 1 and isinstance(args[0], dict):\n        dict_ = args[0]\n        index_list = list(dict_.keys())\n        value_list = list(dict_.values())\n        return sortedby2(index_list, value_list)\n    else:\n        index_list = list(range(len(args[0])))\n        return sortedby2(index_list, *args, **kwargs)", "response": "Like np. argsort but for lists\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef argsort2(indexable, key=None, reverse=False):\n    # Create an iterator of value/key pairs\n    if isinstance(indexable, dict):\n        vk_iter = ((v, k) for k, v in indexable.items())\n    else:\n        vk_iter = ((v, k) for k, v in enumerate(indexable))\n    # Sort by values and extract the keys\n    if key is None:\n        indices = [k for v, k in sorted(vk_iter, reverse=reverse)]\n    else:\n        indices = [k for v, k in sorted(vk_iter, key=lambda vk: key(vk[0]),\n                                        reverse=reverse)]\n    return indices", "response": "Returns the indices that would sort a indexable object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef argmax(input_, multi=False):\n    if multi:\n        if isinstance(input_, dict):\n            keys = list(input_.keys())\n            values = list(input_.values())\n            return [keys[idx] for idx in argmax(values, multi=multi)]\n        else:\n            return where(equal([max(input_)], input_))\n    else:\n        if isinstance(input_, dict):\n            # its crazy, but this is faster\n            # max(input_.items(), key=operator.itemgetter(1))[0]\n            return list(input_.keys())[argmax(list(input_.values()))]\n        elif hasattr(input_, 'index'):\n            return input_.index(max(input_))\n        else:\n            return max(enumerate(input_), key=operator.itemgetter(1))[0]", "response": "Returns index of the item with the largest value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns index of the item with the smallest value.", "response": "def argmin(input_, key=None):\n    \"\"\"\n    Returns index / key of the item with the smallest value.\n\n    Args:\n        input_ (dict or list):\n\n    Note:\n        a[argmin(a, key=key)] == min(a, key=key)\n    \"\"\"\n    # if isinstance(input_, dict):\n    #     return list(input_.keys())[argmin(list(input_.values()))]\n    # elif hasattr(input_, 'index'):\n    #     return input_.index(min(input_))\n    # else:\n    #     return min(enumerate(input_), key=operator.itemgetter(1))[0]\n    if isinstance(input, dict):\n        return list(input.keys())[argmin(list(input.values()), key=key)]\n    else:\n        if key is None:\n            def _key(item):\n                return item[1]\n        else:\n            def _key(item):\n                return key(item[1])\n        return min(enumerate(input), key=_key)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index_complement(index_list, len_=None):\n    mask1 = index_to_boolmask(index_list, len_)\n    mask2 = not_list(mask1)\n    index_list_bar = list_where(mask2)\n    return index_list_bar", "response": "Returns the other indicies in a list of length len_."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns items in list_ not indexed by index_list", "response": "def take_complement(list_, index_list):\n    \"\"\" Returns items in ``list_`` not indexed by index_list \"\"\"\n    mask = not_list(index_to_boolmask(index_list, len(list_)))\n    return compress(list_, mask)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a subset of a list based on a list of indices", "response": "def take(list_, index_list):\n    \"\"\"\n    Selects a subset of a list based on a list of indices.\n    This is similar to np.take, but pure python.\n\n    Args:\n        list_ (list): some indexable object\n        index_list (list, slice, int): some indexing object\n\n    Returns:\n        list or scalar: subset of the list\n\n    CommandLine:\n        python -m utool.util_list --test-take\n\n    SeeAlso:\n        ut.dict_take\n        ut.dict_subset\n        ut.none_take\n        ut.compress\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> list_ = [0, 1, 2, 3]\n        >>> index_list = [2, 0]\n        >>> result = take(list_, index_list)\n        >>> print(result)\n        [2, 0]\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> list_ = [0, 1, 2, 3]\n        >>> index = 2\n        >>> result = take(list_, index)\n        >>> print(result)\n        2\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> list_ = [0, 1, 2, 3]\n        >>> index = slice(1, None, 2)\n        >>> result = take(list_, index)\n        >>> print(result)\n        [1, 3]\n    \"\"\"\n    try:\n        return [list_[index] for index in index_list]\n    except TypeError:\n        return list_[index_list]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef take_percentile(arr, percent):\n    size = len(arr)\n    stop = min(int(size * percent), len(arr))\n    return arr[0:stop]", "response": "take the top percent items in a list rounding up"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef take_percentile_parts(arr, front=None, mid=None, back=None):\n    slices = []\n    if front:\n        slices += [snapped_slice(len(arr), 0.0, front)]\n    if mid:\n        slices += [snapped_slice(len(arr), 0.5, mid)]\n    if back:\n        slices += [snapped_slice(len(arr), 1.0, back)]\n    parts = flatten([arr[sl] for sl in slices])\n    return parts", "response": "r Take parts from front back or middle of a list\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_inverse_take(list_, index_list):\n    output_list_ = [None] * len(index_list)\n    for item, index in zip(list_, index_list):\n        output_list_[index] = item\n    return output_list_", "response": "r This function takes a list of items from the unsorted domain and returns the list of items in the unsorted domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef broadcast_zip(list1, list2):\n    try:\n        len(list1)\n    except TypeError:\n        list1 = list(list1)\n    try:\n        len(list2)\n    except TypeError:\n        list2 = list(list2)\n    # if len(list1) == 0 or len(list2) == 0:\n    #     # Corner case where either list is empty\n    #     return []\n    if len(list1) == 1 and len(list2) > 1:\n        list1 = list1 * len(list2)\n    elif len(list1) > 1 and len(list2) == 1:\n        list2 = list2 * len(list1)\n    elif len(list1) != len(list2):\n        raise ValueError('out of alignment len(list1)=%r, len(list2)=%r' % (\n            len(list1), len(list2)))\n    # return list(zip(list1, list2))\n    return zip(list1, list2)", "response": "r Returns a new list of elementwise pairs between list1 and list2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef equal(list1, list2):\n    return [item1 == item2 for item1, item2 in broadcast_zip(list1, list2)]", "response": "returns indexes of True values in list1 and list2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scalar_input_map(func, input_):\n    if util_iter.isiterable(input_):\n        return list(map(func, input_))\n    else:\n        return func(input_)", "response": "Map like function\n\n    Args:\n        func: function to apply\n        input_ : either an iterable or scalar value\n\n    Returns:\n        If ``input_`` is iterable this function behaves like map\n        otherwise applies func to ``input_``"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_zip(items_list, num_samples, allow_overflow=False, per_bin=1):\n    # Prealloc a list of lists\n    samples_list = [[] for _ in range(num_samples)]\n    # Sample the ix-th value from every list\n    samples_iter = zip_longest(*items_list)\n    sx = 0\n    for ix, samples_ in zip(range(num_samples), samples_iter):\n        samples = filter_Nones(samples_)\n        samples_list[sx].extend(samples)\n        # Put per_bin from each sublist into a sample\n        if (ix + 1) % per_bin == 0:\n            sx += 1\n    # Check for overflow\n    if allow_overflow:\n        overflow_samples = flatten([filter_Nones(samples_) for samples_ in samples_iter])\n        return samples_list, overflow_samples\n    else:\n        try:\n            samples_iter.next()\n        except StopIteration:\n            pass\n        else:\n            raise AssertionError('Overflow occured')\n        return samples_list", "response": "Samples a list of items into a new base directory tree structure"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef issorted(list_, op=operator.le):\n    return all(op(list_[ix], list_[ix + 1]) for ix in range(len(list_) - 1))", "response": "Determines if a list is sorted by a given operation"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_nonconsec_values(values, min_=None, max_=None):\n    # values = sorted(set(values))\n    if min_ is None:\n        min_ = values[0]\n    if max_ is None:\n        max_ = values[-1]\n    valx   = 0\n    missing_values = []\n    for check in range(min_, max_ + 1):\n        if values[valx] != check:\n            missing_values.append(check)\n        else:\n            valx += 1\n    return missing_values", "response": "Determines if a list of values is consecutive"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn list of consecutive lists of numbers from data.", "response": "def group_consecutives(data, stepsize=1):\n    \"\"\"\n    Return list of consecutive lists of numbers from data (number list).\n\n    References:\n        http://stackoverflow.com/questions/7352684/how-to-find-the-groups-of-consecutive-elements-from-an-array-in-numpy\n    \"\"\"\n    run = []\n    result = [run]\n    expect = None\n    for item in data:\n        if (item == expect) or (expect is None):\n            run.append(item)\n        else:\n            run = [item]\n            result.append(run)\n        expect = item + stepsize\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of missing items missing indices duplicate items", "response": "def debug_consec_list(list_):\n    \"\"\"\n    Returns:\n        tuple of (missing_items, missing_indices, duplicate_items)\n    \"\"\"\n    if not issorted(list_):\n        print('warning list is not sorted. indices will not match')\n    sortedlist = sorted(list_)\n    start = sortedlist[0]\n    last = start - 1\n    missing_vals = []\n    missing_indices = []\n    duplicate_items = []\n    for count, item in enumerate(sortedlist):\n        diff = item - last\n        if diff > 1:\n            missing_indices.append(count)\n            for miss in range(last + 1, last + diff):\n                missing_vals.append(miss)\n        elif diff == 0:\n            duplicate_items.append(item)\n        elif diff == 1:\n            # Expected case\n            pass\n        else:\n            raise AssertionError('We sorted the list. diff can not be negative')\n        last = item\n    return missing_vals, missing_indices, duplicate_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_depth(list_, func=max, _depth=0):\n    depth_list = [list_depth(item, func=func, _depth=_depth + 1)\n                  for item in  list_ if util_type.is_listlike(item)]\n    if len(depth_list) > 0:\n        return func(depth_list)\n    else:\n        return _depth", "response": "Returns the deepest level of nesting within a list of lists"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef depth(sequence, func=max, _depth=0):\n    if isinstance(sequence, dict):\n        sequence = list(sequence.values())\n    depth_list = [depth(item, func=func, _depth=_depth + 1)\n                  for item in sequence if (isinstance(item, dict) or util_type.is_listlike(item))]\n    if len(depth_list) > 0:\n        return func(depth_list)\n    else:\n        return _depth", "response": "Find the nesting depth of a nested sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all types in a deep list", "response": "def list_deep_types(list_):\n    \"\"\"\n    Returns all types in a deep list\n    \"\"\"\n    type_list = []\n    for item in list_:\n        if util_type.is_listlike(item):\n            type_list.extend(list_deep_types(item))\n        else:\n            type_list.append(type(item))\n    return type_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_type_profile(sequence, compress_homogenous=True, with_dtype=True):\n    # For a pure bottom level list return the length\n    #if not any(map(util_type.is_listlike, sequence)) or (isinstance(sequence, np.ndarray) and sequence.dtype != object):\n    if not util_type.is_listlike(sequence) or (isinstance(sequence, np.ndarray) and sequence.dtype != object):\n        typename = str(type(sequence)).replace('<type \\'', '').replace('\\'>', '')\n        if six.PY3:\n            typename = str(type(sequence)).replace('<class \\'', '').replace('\\'>', '')\n        if with_dtype and typename == 'numpy.ndarray':\n            typename = typename.replace('numpy.', '')\n            typename += '[%s]' % (sequence.dtype,)\n\n        level_type_str = typename\n        return level_type_str\n    if len(sequence) == 0:\n        return ''\n\n    level_type_list = []\n    for item in sequence:\n        #if util_type.is_listlike(item):\n        item_type_profile = list_type_profile(item, with_dtype=with_dtype)\n        level_type_list.append(item_type_profile)\n\n    if compress_homogenous:\n        # removes redudant information by returning a type and number\n        if allsame(level_type_list):\n            type_ = level_type_list[0]\n            level_type_str = str(type_) + '*' + str(len(level_type_list))\n        else:\n            level_type_str = ', '.join(level_type_list)\n    typename = str(type(sequence)).replace('<type \\'', '').replace('\\'>', '')\n    if six.PY3:\n        typename = str(type(sequence)).replace('<class \\'', '').replace('\\'>', '')\n    level_type_str = typename + '(' + str(level_type_str) + ')'\n    return level_type_str", "response": "This function returns the type of the elements in the given sequence"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_index_lookup(list_, dict_factory=dict):\n    return dict_factory(zip(list_, range(len(list_))))", "response": "r This function will make a dictionary mapping from item to index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of unique items that are aligned to list2", "response": "def list_alignment(list1, list2, missing=False):\n    \"\"\"\n    Assumes list items are unique\n\n    Args:\n        list1 (list): a list of unique items to be aligned\n        list2 (list): a list of unique items in a desired ordering\n        missing (bool): True if list2 can contain items not in list1\n\n    Returns:\n        list: sorting that will map list1 onto list2\n\n    CommandLine:\n        python -m utool.util_list list_alignment\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> import utool as ut\n        >>> list1 = ['b', 'c', 'a']\n        >>> list2 = ['a', 'b', 'c']\n        >>> sortx = list_alignment(list1, list2)\n        >>> list1_aligned = take(list1, sortx)\n        >>> assert list1_aligned == list2\n\n    Example1:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> import utool as ut\n        >>> list1 = ['b', 'c', 'a']\n        >>> list2 = ['a', 'a2', 'b', 'c', 'd']\n        >>> sortx = ut.list_alignment(list1, list2, missing=True)\n        >>> print('sortx = %r' % (sortx,))\n        >>> list1_aligned = ut.none_take(list1, sortx)\n        >>> result = ('list1_aligned = %s' % (ut.repr2(list1_aligned),))\n        >>> print(result)\n        list1_aligned = ['a', None, 'b', 'c', None]\n    \"\"\"\n    import utool as ut\n    item1_to_idx = make_index_lookup(list1)\n    if missing:\n        sortx = ut.dict_take(item1_to_idx, list2, None)\n    else:\n        sortx = ut.take(item1_to_idx, list2)\n    return sortx"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn unique items and inverse of item_list", "response": "def unique_inverse(item_list):\n    \"\"\"\n    Like np.unique(item_list, return_inverse=True)\n    \"\"\"\n    import utool as ut\n    unique_items = ut.unique(item_list)\n    inverse = list_alignment(unique_items, item_list)\n    return unique_items, inverse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_items_by_index(list_, index_list, copy=False):\n    if copy:\n        list_ = list_[:]\n    # Rectify negative indicies\n    index_list_ = [(len(list_) + x if x < 0 else x) for x in index_list]\n    # Remove largest indicies first\n    index_list_ = sorted(index_list_, reverse=True)\n    for index in index_list_:\n        del list_[index]\n    return list_", "response": "Removes items from list_ at positions specified in index_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_list_items(list_, item_list, copy=False):\n    if copy:\n        list_ = list_[:]\n    for item in item_list:\n        list_.remove(item)\n    return list_", "response": "r Remove items in item_list from list_."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unflat_map(func, unflat_items, vectorized=False, **kwargs):\n    import utool as ut\n    # First flatten the list, and remember the original dimensions\n    flat_items, reverse_list = ut.invertible_flatten2(unflat_items)\n    # Then preform the lookup / implicit mapping\n    if vectorized:\n        # func is vectorized\n        flat_vals = func(flat_items, **kwargs)\n    else:\n        flat_vals = [func(item, **kwargs) for item in flat_items]\n    if True:\n        assert len(flat_vals) == len(flat_items), (\n            'flat lens not the same, len(flat_vals)=%d len(flat_items)=%d' %\n            (len(flat_vals), len(flat_items),))\n    # Then ut.unflatten2 the results to the original input dimensions\n    unflat_vals = ut.unflatten2(flat_vals, reverse_list)\n    if True:\n        assert len(unflat_vals) == len(unflat_items), (\n            'unflat lens not the same, len(unflat_vals)=%d len(unflat_rowids)=%d' %\n            (len(unflat_vals), len(unflat_items),))\n    return unflat_vals", "response": "r Uses an ibeis lookup function with a non - flat rowid list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_reshape(list_, new_shape, trail=False):\n    if not trail:\n        total = reduce(operator.mul, new_shape)\n        assert total == len(list_)\n    newlist = list_\n    for dim in reversed(new_shape):\n        slice_ = (newlist[i::dim] for i in range(dim))\n        newlist = list(map(list, zip(*slice_)))\n    if not trail:\n        newlist = newlist[0]\n    return newlist", "response": "r Reshapes the list_ to new_shape leaving trailing dimnsions in front"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstrip the specified items from the list", "response": "def list_strip(list_, to_strip, left=True, right=True):\n    \"\"\"\n    list_ = [1, 2, 1, 3, 1, 1]\n    to_strip = 1\n    stripped_list = ut.list_strip(list_, to_strip)\n    \"\"\"\n    import utool as ut\n    flags = [item != to_strip for item in list_]\n    flag_lists = []\n    if right:\n        rstrip_flags = ut.cumsum(flags[::-1])[::-1]\n        flag_lists.append(rstrip_flags)\n    if left:\n        lstrip_flags = ut.cumsum(flags)\n        flag_lists.append(lstrip_flags)\n    strip_flags = ut.and_lists(*flag_lists)\n    stripped_list = ut.compress(list_, strip_flags)\n    return stripped_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aslist(sequence):\n    if isinstance(sequence, list):\n        return sequence\n    elif util_type.HAVE_NUMPY and isinstance(sequence, np.ndarray):\n        list_ = sequence.tolist()\n    else:\n        list_ = list(sequence)\n    return list_", "response": "r Converts a Python list into a list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef length_hint(obj, default=0):\n    try:\n        return len(obj)\n    except TypeError:\n        try:\n            get_hint = type(obj).__length_hint__\n        except AttributeError:\n            return default\n        try:\n            hint = get_hint(obj)\n        except TypeError:\n            return default\n        if hint is NotImplemented:\n            return default\n        if not isinstance(hint, int):\n            raise TypeError(\"Length hint must be an integer, not %r\" %\n                            type(hint))\n        if hint < 0:\n            raise ValueError(\"__length_hint__() should return >= 0\")\n        return hint", "response": "Return an estimate of the number of items in the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_mutually_exclusive_args(parser, args, required=False, prefix=DATA_PREFIX):\n    parser = parser.add_mutually_exclusive_group(required=required)\n    for arg, kwargs in iteritems(args):\n        arg_name = kwargs.pop('arg', arg.replace('_', '-'))\n        if 'metavar' not in kwargs:\n            kwargs['metavar'] = arg.upper()\n        parser.add_argument('--' + arg_name, dest=prefix + arg, **kwargs)", "response": "Helper method that populates the parser with mutually exclusive arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_create_update_args(parser, required_args, optional_args, create=False):\n    if create:\n        for key in required_args:\n            required_args[key]['required'] = True\n        add_parser_arguments(parser, required_args, group='required arguments')\n    else:\n        optional_args.update(required_args)\n    add_parser_arguments(parser, optional_args)", "response": "Wrapper around add_parser_arguments that creates a new group of required_args and optional_args."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts arguments from the given arguments dictionary.", "response": "def extract_arguments(args, prefix=DATA_PREFIX):\n    \"\"\"Return a dict of arguments created by `add_parser_arguments`.\n\n    If the key in `args` contains two underscores, a nested dictionary will be\n    created. Only keys starting with given prefix are examined. The prefix is\n    stripped away and does not appear in the result.\n    \"\"\"\n    data = {}\n    for key, value in iteritems(args.__dict__):\n        if key.startswith(prefix) and value is not None:\n            parts = key[len(prefix):].split('__')\n            # Think of `d` as a pointer into the resulting nested dictionary.\n            # The `for` loop iterates over all parts of the key except the last\n            # to find the proper dict into which the value should be inserted.\n            # If the subdicts do not exist, they are created.\n            d = data\n            for p in parts[:-1]:\n                assert p not in d or isinstance(d[p], dict)\n                d = d.setdefault(p, {})\n            # At this point `d` points to the correct dict and value can be\n            # inserted.\n            d[parts[-1]] = value if value != '' else None\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a searchspace for the given database.", "response": "def create_searchspace(lookup, fastafn, proline_cut=False,\n                       reverse_seqs=True, do_trypsinize=True):\n    \"\"\"Given a FASTA database, proteins are trypsinized and resulting peptides\n    stored in a database or dict for lookups\"\"\"\n    allpeps = []\n    for record in SeqIO.parse(fastafn, 'fasta'):\n        if do_trypsinize:\n            pepseqs = trypsinize(record.seq, proline_cut)\n        else:\n            pepseqs = [record.seq]\n        # Exchange all leucines to isoleucines because MS can't differ\n        pepseqs = [(str(pep).replace('L', 'I'),) for pep in pepseqs]\n        allpeps.extend(pepseqs)\n        if len(allpeps) > 1000000:  # more than x peps, write to SQLite\n            lookup.write_peps(allpeps, reverse_seqs)\n            allpeps = []\n    # write remaining peps to sqlite\n    lookup.write_peps(allpeps, reverse_seqs)\n    lookup.index_peps(reverse_seqs)\n    lookup.close_connection()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef trypsinize(proseq, proline_cut=False):\n    # TODO add cysteine to non cut options, use enums\n    \"\"\"Trypsinize a protein sequence. Returns a list of peptides.\n    Peptides include both cut and non-cut when P is behind a tryptic\n    residue. Multiple consequent tryptic residues are treated as follows:\n    PEPKKKTIDE - [PEPK, PEPKK, PEPKKK, KKTIDE, KTIDE, TIDE, K, K, KK ]\n    \"\"\"\n    outpeps = []\n    currentpeps = ['']\n    trypres = set(['K', 'R'])\n    noncutters = set()\n    if not proline_cut:\n        noncutters.add('P')\n    for i, aa in enumerate(proseq):\n        currentpeps = ['{0}{1}'.format(x, aa) for x in currentpeps]\n        if i == len(proseq) - 1:\n            continue\n        if aa in trypres and proseq[i + 1] not in noncutters:\n            outpeps.extend(currentpeps)  # do actual cut by storing peptides\n            if proseq[i + 1] in trypres.union('P'):\n                # add new peptide to list if we are also to run on\n                currentpeps.append('')\n            elif trypres.issuperset(currentpeps[-1]):\n                currentpeps = [x for x in currentpeps if trypres.issuperset(x)]\n                currentpeps.append('')\n            else:\n                currentpeps = ['']\n\n    if currentpeps != ['']:\n        outpeps.extend(currentpeps)\n    return outpeps", "response": "Trypsinize a protein sequence. Returns a list of peptides.\n    Peptides include both cut and non - cut when P is behind a tryptic\n    residue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hashstr_arr(arr, lbl='arr', pathsafe=False, **kwargs):\n    if isinstance(arr, list):\n        arr = tuple(arr)  # force arrays into a tuple for hashability\n        # TODO: maybe for into numpy array instead? tuples might have problems\n    if pathsafe:\n        lbrace1, rbrace1, lbrace2, rbrace2 = '_', '_', '-', '-'\n    else:\n        lbrace1, rbrace1, lbrace2, rbrace2 = '(', ')', '(', ')'\n    if isinstance(arr, tuple):\n        arr_shape = lbrace1 + str(len(arr)) + rbrace1\n    else:\n        # Arr should be an ndarray here. append info about the ndarray\n        arr_shape = lbrace1 + ','.join(list(map(str, arr.shape))) + rbrace1\n    arr_hashstr_ = hashstr(arr, **kwargs)\n    arr_hashstr = ''.join([lbl, lbrace2, arr_shape, arr_hashstr_, rbrace2])\n    return arr_hashstr", "response": "r This function is used to hash a numpy array into a single string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the hasher with the data", "response": "def _update_hasher(hasher, data):\n    \"\"\"\n    This is the clear winner over the generate version.\n    Used by hash_data\n\n    Ignore:\n        import utool\n        rng = np.random.RandomState(0)\n        # str1 = rng.rand(0).dumps()\n        str1 = b'SEP'\n        str2 = rng.rand(10000).dumps()\n        for timer in utool.Timerit(100, label='twocall'):\n            hasher = hashlib.sha256()\n            with timer:\n                hasher.update(str1)\n                hasher.update(str2)\n        a = hasher.hexdigest()\n        for timer in utool.Timerit(100, label='concat'):\n            hasher = hashlib.sha256()\n            with timer:\n                hasher.update(str1 + str2)\n        b = hasher.hexdigest()\n        assert a == b\n        # CONCLUSION: Faster to concat in case of prefixes and seps\n\n        nested_data = {'1': [rng.rand(100), '2', '3'],\n                       '2': ['1', '2', '3', '4', '5'],\n                       '3': [('1', '2'), ('3', '4'), ('5', '6')]}\n        data = list(nested_data.values())\n\n\n        for timer in utool.Timerit(1000, label='cat-generate'):\n            hasher = hashlib.sha256()\n            with timer:\n                hasher.update(b''.join(_bytes_generator(data)))\n\n        for timer in utool.Timerit(1000, label='inc-generate'):\n            hasher = hashlib.sha256()\n            with timer:\n                for b in _bytes_generator(data):\n                    hasher.update(b)\n\n        for timer in utool.Timerit(1000, label='inc-generate'):\n            hasher = hashlib.sha256()\n            with timer:\n                for b in _bytes_generator(data):\n                    hasher.update(b)\n\n        for timer in utool.Timerit(1000, label='chunk-inc-generate'):\n            hasher = hashlib.sha256()\n            import ubelt as ub\n            with timer:\n                for chunk in ub.chunks(_bytes_generator(data), 5):\n                    hasher.update(b''.join(chunk))\n\n        for timer in utool.Timerit(1000, label='inc-update'):\n            hasher = hashlib.sha256()\n            with timer:\n                _update_hasher(hasher, data)\n\n        data = ut.lorium_ipsum()\n        hash_data(data)\n        ut.hashstr27(data)\n        %timeit hash_data(data)\n        %timeit ut.hashstr27(repr(data))\n\n        for timer in utool.Timerit(100, label='twocall'):\n            hasher = hashlib.sha256()\n            with timer:\n                hash_data(data)\n\n        hasher = hashlib.sha256()\n        hasher.update(memoryview(np.array([1])))\n        print(hasher.hexdigest())\n\n        hasher = hashlib.sha256()\n        hasher.update(np.array(['1'], dtype=object))\n        print(hasher.hexdigest())\n\n    \"\"\"\n    if isinstance(data, (tuple, list, zip)):\n        needs_iteration = True\n    elif (util_type.HAVE_NUMPY and isinstance(data, np.ndarray) and\n          data.dtype.kind == 'O'):\n        # ndarrays of objects cannot be hashed directly.\n        needs_iteration = True\n    else:\n        needs_iteration = False\n\n    if needs_iteration:\n        # try to nest quickly without recursive calls\n        SEP = b'SEP'\n        iter_prefix = b'ITER'\n        # if isinstance(data, tuple):\n        #     iter_prefix = b'TUP'\n        # else:\n        #     iter_prefix = b'LIST'\n        iter_ = iter(data)\n        hasher.update(iter_prefix)\n        try:\n            for item in iter_:\n                prefix, hashable = _covert_to_hashable(data)\n                binary_data = SEP + prefix + hashable\n                # b''.join([SEP, prefix, hashable])\n                hasher.update(binary_data)\n        except TypeError:\n            # need to use recursive calls\n            # Update based on current item\n            _update_hasher(hasher, item)\n            for item in iter_:\n                # Ensure the items have a spacer between them\n                hasher.update(SEP)\n                _update_hasher(hasher, item)\n    else:\n        prefix, hashable = _covert_to_hashable(data)\n        binary_data = prefix + hashable\n        # b''.join([prefix, hashable])\n        hasher.update(binary_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combine_hashes(bytes_list, hasher=None):\n    if hasher is None:\n        hasher = hashlib.sha256()\n    for b in bytes_list:\n        hasher.update(b)\n        hasher.update(SEP_BYTE)\n    return hasher.digest()", "response": "Combine the list of bytes into a single base64 - encoded version of the current base64 - encoded version of the current base64 - encoded version."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hash_data(data, hashlen=None, alphabet=None):\n    if alphabet is None:\n        alphabet = ALPHABET_27\n    if hashlen is None:\n        hashlen = HASH_LEN2\n    if isinstance(data, stringlike) and len(data) == 0:\n        # Make a special hash for empty data\n        text = (alphabet[0] * hashlen)\n    else:\n        hasher = hashlib.sha512()\n        _update_hasher(hasher, data)\n        # Get a 128 character hex string\n        text = hasher.hexdigest()\n        # Shorten length of string (by increasing base)\n        hashstr2 = convert_hexstr_to_bigbase(text, alphabet, bigbase=len(alphabet))\n        # Truncate\n        text = hashstr2[:hashlen]\n        return text", "response": "r This function calculates a unique hash of the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hashstr(data, hashlen=HASH_LEN, alphabet=ALPHABET):\n    if util_type.HAVE_NUMPY and isinstance(data, np.ndarray):\n        if data.dtype.kind == 'O':\n            msg = '[ut] hashing ndarrays with dtype=object is unstable'\n            warnings.warn(msg, RuntimeWarning)\n            # but tobytes is ok, but differs between python 2 and 3 for objects\n            data = data.dumps()\n            # data = data.tobytes()\n    if isinstance(data, tuple):\n        # should instead do\n        if False:\n            hasher = hashlib.sha512()\n            items = data\n            for item in items:\n                if isinstance(item, uuid.UUID):\n                    hasher.update(item.bytes)\n                else:\n                    hasher.update(item)\n            text = hasher.hexdigest()\n            hashstr2 = convert_hexstr_to_bigbase(text, alphabet, bigbase=len(alphabet))\n            # Truncate\n            text = hashstr2[:hashlen]\n            return text\n        else:\n            msg = '[ut] hashing tuples with repr is not a good idea. FIXME'\n            # warnings.warn(msg, RuntimeWarning)\n            data = repr(data)  # Hack?\n\n    # convert unicode into raw bytes\n    if isinstance(data, six.text_type):\n        data = data.encode('utf-8')\n\n    if isinstance(data, stringlike) and len(data) == 0:\n        # Make a special hash for empty data\n        text = (alphabet[0] * hashlen)\n    else:\n        # Get a 128 character hex string\n        text = hashlib.sha512(data).hexdigest()\n        # Shorten length of string (by increasing base)\n        hashstr2 = convert_hexstr_to_bigbase(text, alphabet, bigbase=len(alphabet))\n        # Truncate\n        text = hashstr2[:hashlen]\n    return text", "response": "hashstr - hash a string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_hexstr_to_bigbase(hexstr, alphabet=ALPHABET, bigbase=BIGBASE):\n    x = int(hexstr, 16)  # first convert to base 16\n    if x == 0:\n        return '0'\n    sign = 1 if x > 0 else -1\n    x *= sign\n    digits = []\n    while x:\n        digits.append(alphabet[x % bigbase])\n        x //= bigbase\n    if sign < 0:\n        digits.append('-')\n        digits.reverse()\n    newbase_str = ''.join(digits)\n    return newbase_str", "response": "r Convert a long hexstr into a shorter length string with a larger base"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_hash_file(fpath, hash_tag='md5', recompute=False):\n    hash_dict = {\n        'md5'    : hashlib.md5(),\n        'sha1'   : hashlib.sha1(),\n        'sha256' : hashlib.sha256(),\n    }\n    message = \"Unrecognized hashing function.  Use 'md5', 'sha1', or 'sha256\"\n    assert hash_tag in hash_dict, message\n    if fpath.endswith('.%s' % (hash_tag, )):\n        # No need to compute hashes on hashes\n        return\n    # Get hash path\n    hash_fpath = '%s.%s' % (fpath, hash_tag, )\n    if os.path.exists(hash_fpath) and not recompute:\n        return\n    # Assert this is a file\n    file_type = util_path.get_path_type(fpath)\n    if file_type == 'file':\n        # Compute hash\n        hasher = hash_dict[hash_tag]\n        hash_local = get_file_hash(fpath, hasher=hasher, hexdigest=True)\n        print('[utool] Adding:', fpath, hash_local)\n        with open(hash_fpath, 'w') as hash_file:\n            hash_file.write(hash_local)\n        return hash_fpath", "response": "r Write a hash file for each file in a path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_hash_file_for_path(path, recompute=False):\n    hash_fpath_list = []\n    for root, dname_list, fname_list in os.walk(path):\n        for fname in sorted(fname_list):\n            # fpath = os.path.join(path, fname)\n            fpath = os.path.join(root, fname)\n            hash_fpath = write_hash_file(fpath, recompute=recompute)\n            if hash_fpath is not None:\n                hash_fpath_list.append(hash_fpath)\n    return hash_fpath_list", "response": "r Creates a hash file for each file in a path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_file_uuid(fpath, hasher=None, stride=1):\n    if hasher is None:\n        hasher = hashlib.sha1()  # 20 bytes of output\n        #hasher = hashlib.sha256()  # 32 bytes of output\n    # sha1 produces a 20 byte hash\n    hashbytes_20 = get_file_hash(fpath, hasher=hasher, stride=stride)\n    # sha1 produces 20 bytes, but UUID requires 16 bytes\n    hashbytes_16 = hashbytes_20[0:16]\n    uuid_ = uuid.UUID(bytes=hashbytes_16)\n    return uuid_", "response": "Creates a uuid from the hash of a file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning uuid of image", "response": "def image_uuid(pil_img):\n    \"\"\"\n    UNSAFE: DEPRICATE: JPEG IS NOT GAURENTEED TO PRODUCE CONSITENT VALUES ON\n\n    MULTIPLE MACHINES image global unique id\n\n    References:\n        http://stackoverflow.com/questions/23565889/jpeg-images-have-different-pixel-values-across-multiple-devices\n    \"\"\"\n    print('WARNING DO NOT USE utool.util_hash.image_uuid UNSAFE AND DEPRICATED')\n    # Get the bytes of the image\n    img_bytes_ = pil_img.tobytes()\n    uuid_ = hashable_to_uuid(img_bytes_)\n    return uuid_"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a random string of length = length from alphabet", "response": "def random_nonce(length=64, alphabet=None):\n    \"\"\"\n    returns a random string of len=<length> from <alphabet>\n    I have no idea why this is named random_nonce\n    \"\"\"\n    assert length > 0\n    if alphabet is None:\n        alphabet = ALPHABET_16\n    return ''.join( [alphabet[random.randint(0, len(alphabet) - 1)] for _ in range(length)] )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a new connection and manage it from a new greenlet.", "response": "def __start_connection(self, context, node, ccallbacks=None):\n        \"\"\"Start a new connection, and manage it from a new greenlet.\"\"\"\n\n        _logger.debug(\"Creating connection object: CONTEXT=[%s] NODE=[%s]\", \n                      context, node)\n\n        c = nsq.connection.Connection(\n                context,\n                node, \n                self.__identify, \n                self.__message_handler,\n                self.__quit_ev,\n                ccallbacks,\n                ignore_quit=self.__connection_ignore_quit)\n\n        g = gevent.spawn(c.run)\n\n        # Now, wait for the thread to finish the connection.\n\n        timeout_s = nsq.config.client.NEW_CONNECTION_NEGOTIATE_TIMEOUT_S\n        if c.connected_ev.wait(timeout_s) is False:\n            _logger.error(\"New connection to server [%s] timed-out. Cleaning-\"\n                          \"up thread.\", node)\n\n            g.kill()\n            g.join()\n\n            # We'll try again on the next audit.\n\n            raise EnvironmentError(\"Connection to server [%s] failed.\" % \n                                   (node,))\n\n        self.__connections.append((node, c, g))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __wait_for_one_server_connection(self):\n\n        _logger.info(\"Waiting for first connection.\")\n\n        while 1:\n            is_connected_to_one = False\n            for (n, c, g) in self.__connections:\n                if c.is_connected is True:\n                    is_connected_to_one = True\n                    break\n                elif g.exception == nsq.exceptions.NsqConnectGiveUpError:\n                    raise IOError(\"One of the servers could not be connected \"\n                                  \"during startup: [%s]\" % (c))\n                elif g.exception is not None:\n                    raise IOError(\"One of the connection gthreads had an \"\n                                  \"uncaught exception during startup: [%s] \"\n                                  \"[%s]\" % \n                                  (g.exception.__class__.__name__, \n                                   str(g.exception)))\n                elif g.dead is True:\n                    raise SystemError(\"One of the connection gthreads died \"\n                                      \"during startup: [%s]\" % (c,))\n\n            if is_connected_to_one is True:\n                break\n\n            gevent.sleep(nsq.config.client.CONNECT_AUDIT_WAIT_INTERVAL_S)", "response": "Wait until at least one server is connected."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmonitor state of all connections and utility of all servers.", "response": "def __audit_connections(self, ccallbacks):\n        \"\"\"Monitor state of all connections, and utility of all servers.\"\"\"\n\n        while self.__quit_ev.is_set() is False:\n            # Remove any connections that are dead.\n            self.__connections = filter(\n                                    lambda (n, c, g): not g.ready(), \n                                    self.__connections)\n\n            connected_node_couplets_s = set([\n                (c.managed_connection.context, node)\n                for (node, c, g) \n                in self.__connections])\n\n            # Warn if there are any still-active connections that are no longer \n            # being advertised (probably where we were given some lookup servers \n            # that have dropped this particular *nsqd* server).\n\n            lingering_nodes_s = connected_node_couplets_s - \\\n                                self.__node_couplets_s\n\n            if lingering_nodes_s:\n                _logger.warning(\"Server(s) are connected but no longer \"\n                                \"advertised: %s\", lingering_nodes_s)\n\n            # Connect any servers that don't currently have a connection.\n\n            unused_nodes_s = self.__node_couplets_s - connected_node_couplets_s\n\n            for (context, node) in unused_nodes_s:\n                _logger.info(\"Trying to connect unconnected server: \"\n                             \"CONTEXT=[%s] NODE=[%s]\", context, node)\n\n                self.__start_connection(context, node, ccallbacks)\n            else:\n                # Are there both no unused servers and no connected servers?\n                if not connected_node_couplets_s:\n                    _logger.error(\"All servers have gone away. Stopping \"\n                                  \"client.\")\n\n                    # Clear our list of servers, and squash the \"no servers!\" \n                    # error so that we can shut things down in the right order.\n\n                    try:\n                        self.set_servers([])\n                    except EnvironmentError:\n                        pass\n\n                    self.__quit_ev.set()\n                    return\n\n            interval_s = \\\n                nsq.config.client.GRANULAR_CONNECTION_AUDIT_SLEEP_STEP_TIME_S\n\n            audit_wait_s = float(nsq.config.client.CONNECTION_AUDIT_WAIT_S)\n\n            while audit_wait_s > 0 and\\\n                  self.__quit_ev.is_set() is False:\n                gevent.sleep(interval_s)\n                audit_wait_s -= interval_s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __join_connections(self):\n\n        interval_s = nsq.config.client.CONNECTION_CLOSE_AUDIT_WAIT_S\n        graceful_wait_s = nsq.config.client.CONNECTION_QUIT_CLOSE_TIMEOUT_S\n        graceful = False\n\n        while graceful_wait_s > 0:\n            if not self.__connections:\n                break\n\n            connected_list = [c.is_connected for (n, c, g) in self.__connections]\n            if any(connected_list) is False:\n                graceful = True\n                break\n\n            # We need to give the greenlets periodic control, in order to finish \n            # up.\n\n            gevent.sleep(interval_s)\n            graceful_wait_s -= interval_s\n\n        if graceful is False:\n            connected_list = [c for (n, c, g) in self.__connections if c.is_connected]\n            _logger.error(\"We were told to terminate, but not all \"\n                          \"connections were stopped: [%s]\", connected_list)", "response": "Wait for all connections to close."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_servers(self, node_couplets):\n\n        node_couplets_s = set(node_couplets)\n\n        if node_couplets_s != self.__node_couplets_s:\n            _logger.info(\"Servers have changed. NEW: %s REMOVED: %s\", \n                         node_couplets_s - self.__node_couplets_s, \n                         self.__node_couplets_s - node_couplets_s)\n\n        # Since no servers means no connection greenlets, and the discover \n        # greenlet is technically scheduled and not running between \n        # invocations, this should successfully terminate the process.\n        if not node_couplets_s:\n            raise EnvironmentError(\"No servers available.\")\n\n        self.__node_couplets_s = node_couplets_s", "response": "Set the current collection of servers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestablishes and maintain connections.", "response": "def start(self, ccallbacks=None):\n        \"\"\"Establish and maintain connections.\"\"\"\n\n        self.__manage_g = gevent.spawn(self.__manage_connections, ccallbacks)\n        self.__ready_ev.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop(self):\n\n        _logger.debug(\"Emitting quit signal for connections.\")\n        self.__quit_ev.set()\n\n        _logger.info(\"Waiting for connection manager to stop.\")\n        self.__manage_g.join()", "response": "Stop all of the connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse command - line arguments.", "response": "def process_args():\n    \"\"\" \n    Parse command-line arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('-I', type=str,\n                        metavar='<Include directory>',\n                        action='append',\n                        help='Directory to be searched for included files')\n                        \n                        \n    parser.add_argument('lems_file', type=str, metavar='<LEMS file>', \n                        help='LEMS file to be simulated')\n                        \n    parser.add_argument('-nogui',\n                        action='store_true',\n                        help=\"If this is specified, just parse & simulate the model, but don't show any plots\")\n                        \n    parser.add_argument('-dlems',\n                        action='store_true',\n                        help=\"If this is specified, export the LEMS file as \"+dlems_info)\n    \n    return parser.parse_args()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning for running from a script or shell.", "response": "def run(file_path,include_dirs=[],dlems=False,nogui=False):\n    \"\"\"\n    Function for running from a script or shell.\n    \"\"\"\n    import argparse\n    args = argparse.Namespace()\n    args.lems_file = file_path\n    args.I = include_dirs\n    args.dlems = dlems\n    args.nogui = nogui\n    main(args=args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to the nsqlookupd server.", "response": "def connect(self, nice_quit_ev):\n        \"\"\"Connect the server. We expect this to implement backoff and all \n        connection logistics for servers that were discovered via a lookup \n        node.\n        \"\"\" \n\n        _logger.debug(\"Connecting to discovered node: [%s]\", self.server_host)\n\n        stop_epoch = time.time() + \\\n                        nsq.config.client.MAXIMUM_CONNECT_ATTEMPT_PERIOD_S\n\n        timeout_s = nsq.config.client.INITIAL_CONNECT_FAIL_WAIT_S\n        backoff_rate = nsq.config.client.CONNECT_FAIL_WAIT_BACKOFF_RATE\n\n        while stop_epoch >= time.time() and nice_quit_ev.is_set() is False:\n            try:\n                c = self.primitive_connect()\n            except gevent.socket.error:\n                _logger.exception(\"Could not connect to discovered server: \"\n                                  \"[%s]\", self.server_host)\n            else:\n                _logger.info(\"Discovered server-node connected: [%s]\", \n                             self.server_host)\n                \n                return c\n\n            timeout_s = min(timeout_s * backoff_rate,\n                            nsq.config.client.MAXIMUM_CONNECT_FAIL_WAIT_S)\n\n            _logger.info(\"Waiting for (%d) seconds before reconnecting.\", \n                         timeout_s)\n\n            gevent.sleep(timeout_s)\n\n        raise nsq.exceptions.NsqConnectGiveUpError(\n                \"Could not connect to the nsqlookupd server: [%s]\" % \n                (self.server_host,))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to the explicit server.", "response": "def connect(self, nice_quit_ev):\n        \"\"\"Connect the server. We expect this to implement connection logistics \n        for servers that were explicitly prescribed to us.\n        \"\"\" \n\n        _logger.debug(\"Connecting to explicit server node: [%s]\", \n                      self.server_host)\n\n        # According to the docs, a nsqlookupd-discovered server should fall-out \n        # of the lineup immediately if it fails. If it comes back, nsqlookupd \n        # will give it back to us.\n\n        try:\n            c = self.primitive_connect()\n        except gevent.socket.error:\n            _logger.exception(\"Could not connect to explicit server: [%s]\",\n                              self.server_host)\n\n            raise nsq.exceptions.NsqConnectGiveUpError(\n                    \"Could not connect to the nsqd server: [%s]\" % \n                    (self.server_host,))\n\n        _logger.info(\"Explicit server-node connected: [%s]\", self.server_host)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare the protein tables for the protein table", "response": "def prepare(self):\n        \"\"\"No percolator XML for protein tables\"\"\"\n        self.target = self.fn\n        self.targetheader = reader.get_tsv_header(self.target)\n        self.decoyheader = reader.get_tsv_header(self.decoyfn)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to obtain a token from all end - points that were ever used to serve the token.", "response": "def obtain_token(self):\n        \"\"\"\n        Try to obtain token from all end-points that were ever used to serve the\n        token. If the request returns 404 NOT FOUND, retry with older version of\n        the URL.\n        \"\"\"\n        token_end_points = ('token/obtain',\n                            'obtain-token',\n                            'obtain_token')\n        for end_point in token_end_points:\n            try:\n                return self.auth[end_point]._(page_size=None)['token']\n            except BeanBagException as e:\n                if e.response.status_code != 404:\n                    raise\n        raise Exception('Could not obtain token from any known URL.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_paged(self, res, **kwargs):\n        if self.page_size is not None:\n            kwargs['page_size'] = self.page_size\n            if self.page_size <= 0:\n                # If page_size <= 0, pagination will be disable.\n                return res(**kwargs)\n\n        def worker():\n            kwargs['page'] = 1\n            while True:\n                response = res(**kwargs)\n                yield response['results']\n                if response['next']:\n                    kwargs['page'] += 1\n                else:\n                    break\n        return itertools.chain.from_iterable(worker())", "response": "This method returns all pages and returns the results as a single iterable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an iterator with all pages of data.", "response": "def results(self, *args, **kwargs):\n        \"\"\"\n           Return an iterator with all pages of data.\n           Return NoResultsError with response if there is unexpected data.\n        \"\"\"\n        def worker():\n            kwargs['page'] = 1\n            while True:\n                response = self.client(*args, **kwargs)\n                if isinstance(response, list):\n                    yield response\n                    break\n                elif _is_page(response):\n                    yield response['results']\n                    if response['next']:\n                        kwargs['page'] += 1\n                    else:\n                        break\n                else:\n                    raise NoResultsError(response)\n\n        return itertools.chain.from_iterable(worker())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, value):\n        if self.pass_ and not value.strip():\n            return True\n\n        if not value:\n            return False\n        return True", "response": "Determines if value is empty."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if value character length equal self. length.", "response": "def run(self, value):\n        \"\"\" Determines if value character length equal self.length.\n        Keyword arguments:\n        value str -- the value of the associated field to compare\n        \"\"\"\n        if self.pass_ and not value.strip():\n            return True\n\n        if len((value.strip() if self.strip else value)) != self.length:\n            self.error = self.error.format(value, self.length)\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, value):\n        if self.pass_ and not value.strip():\n            return True\n\n        if self.minimum <= len((value.strip() if self.strip else value)) <= self.maximum:\n            return True\n\n        self.error = self.error.format(value, self.minimum, self.maximum)\n        return False", "response": "Determines if value character length is between self. minimum and self. maximum."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, value):\n        if self.pass_ and not value.strip():\n            return True\n\n        if (value.strip() if self.strip else value) not in self.given_list:\n            self.error = self.error.format(value)\n            return False\n        return True", "response": "Checks if value is included within self. given_list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck the value of the associated field against the asserted_type.", "response": "def run(self, value):\n        \"\"\" Compares value against self.asserted_type.\n        Keyword arguments:\n        value str -- the value of the associated field to compare\n        \"\"\"\n        if self.pass_ and not value.strip():\n            return True\n\n        if not isinstance(value, type(self.asserted_type)):\n            self.error = self.error.format(type(value), self.asserted_type)\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_general_header(headerfields, fieldtypes, firstfield, oldheader,\n                            group_by_field):\n    \"\"\"From headerfield object, this generates a full header as a list, ready\n    to write to a TSV file\n    E.g:\n    headerfield = {precusroquant: {HEADER_AREA: OD([(set1, set1_HEAD), (set2,\n    set2_HEAD), etc])}}\"\"\"\n    if not oldheader:\n        header = [firstfield]\n    else:\n        header = [firstfield] + oldheader[1:]\n    poolfields = OrderedDict()\n    poolfields[None] = []  # Have non-pool/set columns come before pool-columns\n    if group_by_field:\n        header.extend(poolfields[None])\n    for fieldtype in fieldtypes:\n        try:\n            fields = headerfields[fieldtype]\n        except KeyError:\n            continue\n        if type(fields) == list:\n            header.extend(fields)\n        elif group_by_field:\n            pfmatrix = [list(x.values()) for k, x in fields.items()\n                        if not HEADER_NO_PSMS_SUFFIX in k]\n            header.extend([x for y in transpose(pfmatrix) for x in y])\n            if fieldtype == 'isoquant':\n                pfmatrix = [list(x.values()) for k, x in fields.items()\n                            if HEADER_NO_PSMS_SUFFIX in k]\n                header.extend([x for y in transpose(pfmatrix) for x in y])\n        else:\n            for pool_field in fields.values():\n                for pool, field in pool_field.items():\n                    try:\n                        poolfields[pool].append(field)\n                    except KeyError:\n                        poolfields[pool] = [field]\n    if poolfields and not group_by_field:\n        for fields in poolfields.values():\n            header.extend(fields)\n    return header", "response": "This function generates a full header for a TSV file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a headerfields object which contains information on the header fields of the header", "response": "def generate_headerfields(headertypes, allfield_defs, poolnames, db=False, genecentric=False):\n    \"\"\"Returns a headerfields object (dict) which contains information on\n    fields of the header, including optional pool names\"\"\"\n    hfields = {}\n    for fieldtype in headertypes:\n        if fieldtype == 'isoquant':\n            continue\n        elif fieldtype == 'proteindata':\n            hfield_definitions = allfield_defs[fieldtype](poolnames, genecentric)\n        else:\n            hfield_definitions = allfield_defs[fieldtype](poolnames)\n        hfields[fieldtype] = OrderedDict()\n        for fieldname, poolnames in hfield_definitions.items():\n            hfields[fieldtype][fieldname] = get_header_field(fieldname,\n                                                             poolnames)\n    if 'isoquant' in headertypes:\n        hfield_definitions = allfield_defs['isoquant'](db, poolnames)\n        hfields['isoquant'] = OrderedDict()\n        for poolname in poolnames:\n            for fieldname in hfield_definitions:\n                hfields['isoquant'][fieldname] = get_header_field(\n                    fieldname, poolnames)\n    return hfields"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_isoquant_fields(pqdb=False, poolnames=False):\n    # FIXME when is a None database passed?\n    if pqdb is None:\n        return {}\n    try:\n        channels_psms = pqdb.get_isoquant_amountpsms_channels()\n    except OperationalError:\n        # FIXME what does this catch?\n        return {}\n    quantheader, psmsheader = OrderedDict(), OrderedDict()\n    for chan_name, amnt_psms_name in channels_psms:\n        quantheader[chan_name] = poolnames\n        if amnt_psms_name:\n            psmsheader[amnt_psms_name] = poolnames\n    quantheader.update(psmsheader)\n    return quantheader", "response": "Returns a headerfield dict for isobaric quant channels. Channels are\n    taken from DB and poolnames are ignored."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwait for events and print them to stdout.", "response": "def watch_for_events():\n    \"\"\"Wait for events and print them to stdout.\"\"\"\n    fd = inotify.init()\n    try:\n        wd = inotify.add_watch(fd, '/tmp', inotify.IN_CLOSE_WRITE)\n        while True:\n            for event in inotify.get_events(fd):\n                print(\"event:\", event.name, event.get_mask_description())\n    finally:\n        os.close(fd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_body(self, description, sys_info=None, traceback=None):\n        body = BODY_ITEM_TEMPLATE % {\n            'name': 'Description', 'value': description\n        }\n        if traceback:\n            traceback = '\\n'.join(traceback.splitlines()[-NB_LINES_MAX:])\n            body += BODY_ITEM_TEMPLATE % {\n                'name': 'Traceback', 'value': '```\\n%s\\n```' % traceback\n            }\n        if sys_info:\n            sys_info = '- %s' % '\\n- '.join(sys_info.splitlines())\n            body += BODY_ITEM_TEMPLATE % {\n                'name': 'System information', 'value': sys_info\n            }\n        return body", "response": "Formats the issue s body using markdown."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self):\n        define(\"port\", default=self.port, help=\"Run on given port\", type=int)\n        define(\"host\", default=self.host, help=\"Run on given host\", type=str)\n        define(\"debug\", default=self.debug, help=\"True for development\", type=bool)\n\n        parse_command_line()\n\n        print(Fore.GREEN + \"Starting Bast Server....\")\n        print(Fore.GREEN + \"Bast Server Running on %s:%s\" % (options.host, options.port))\n\n        application = Application(self.handler, debug=options.debug)\n        server = HTTPServer(application)\n        server.listen(options.port, options.host)\n        IOLoop.current().start()", "response": "Function to Run the server"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting EC2 name and public and private ip address", "response": "def list():\n    \"List EC2 name and public and private ip address\"\n    for node in env.nodes:\n        print \"%s (%s, %s)\" % (node.tags[\"Name\"], node.ip_address,\n            node.private_ip_address)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch for the resource in the cluster.", "response": "def search(\n        self,\n        filter_by,\n        return_fields=None,\n        sort_by=None,\n        desc=True,\n        limit=None,\n        offset=None,\n    ):\n        \"\"\"\n        Full search of games resource, supporting all search fields\n        available in API\n        http://www.giantbomb.com/api/documentation#toc-0-17\n\n        :param filter_by: dict\n        :param return_fields: tuple\n        :param sort_by: string\n        :param desc: bool\n        :param limit: int\n        :param offset: int\n        :return: pybomb.clients.Response\n        \"\"\"\n        self._validate_filter_fields(filter_by)\n        search_filter = self._create_search_filter(filter_by)\n\n        search_params = {\"filter\": search_filter}\n\n        if return_fields is not None:\n            self._validate_return_fields(return_fields)\n            field_list = \",\".join(return_fields)\n\n            search_params[\"field_list\"] = field_list\n\n        if sort_by is not None:\n            self._validate_sort_field(sort_by)\n\n            if desc:\n                direction = self.SORT_ORDER_DESCENDING\n            else:\n                direction = self.SORT_ORDER_ASCENDING\n\n            search_params[\"sort\"] = \"{0}:{1}\".format(sort_by, direction)\n\n        if limit is not None:\n            search_params[\"limit\"] = int(limit)\n\n        if offset is not None:\n            search_params[\"offset\"] = int(offset)\n\n        response = self._query(search_params)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a ping to the handler.", "response": "def send_ping(self, payload=None):\n        \"\"\"\n        Sends the ping after the interval specified when initializing\n        \"\"\"\n        yield from asyncio.sleep(self._interval)\n        self._handler.send_ping(payload=payload)\n        self._start_timer(payload=payload)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling when a pong is received.", "response": "def pong_received(self, payload=None):\n        \"\"\"\n        Called when a pong is received. So the timer is cancelled\n        \"\"\"\n        if self._timer is not None:\n            self._timer.cancel()\n            self._failures = 0\n            asyncio.async(self.send_ping(payload=payload))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks to see if var is an instance of known compatible types for type_", "response": "def is_comparable_type(var, type_):\n    \"\"\"\n    Check to see if `var` is an instance of known compatible types for `type_`\n\n    Args:\n        var (?):\n        type_ (?):\n\n    Returns:\n        bool:\n\n    CommandLine:\n        python -m utool.util_type is_comparable_type --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_type import *  # NOQA\n        >>> import utool as ut\n        >>> flags = []\n        >>> flags += [is_comparable_type(0, float)]\n        >>> flags += [is_comparable_type(0, np.float32)]\n        >>> flags += [is_comparable_type(0, np.int32)]\n        >>> flags += [is_comparable_type(0, int)]\n        >>> flags += [is_comparable_type(0.0, int)]\n        >>> result = ut.repr2(flags)\n        >>> print(result)\n        [True, True, True, True, False]\n    \"\"\"\n    other_types = COMPARABLE_TYPES.get(type_, type_)\n    return isinstance(var, other_types)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef smart_cast(var, type_):\n    #if isinstance(type_, tuple):\n    #    for trytype in type_:\n    #        try:\n    #            return trytype(var)\n    #        except Exception:\n    #            pass\n    #    raise TypeError('Cant figure out type=%r' % (type_,))\n    if type_ is None or var is None:\n        return var\n    #if not isinstance(type_, six.string_types):\n    try:\n        if issubclass(type_, type(None)):\n            return var\n    except TypeError:\n        pass\n    if is_str(var):\n        if type_ in VALID_BOOL_TYPES:\n            return bool_from_str(var)\n        elif type_ is slice:\n            args = [None if len(arg) == 0 else int(arg) for arg in var.split(':')]\n            return slice(*args)\n        elif type_ is list:\n            # need more intelligent parsing here\n            subvar_list = var.split(',')\n            return [smart_cast2(subvar) for subvar in subvar_list]\n        elif isinstance(type_, six.string_types):\n            if type_ == 'fuzzy_subset':\n                return fuzzy_subset(var)\n            if type_ == 'eval':\n                return eval(var, {}, {})\n            #elif type_ == 'fuzzy_int':\n            #    return fuzzy_subset(var)\n            else:\n                raise NotImplementedError('Uknown smart type_=%r' % (type_,))\n    return type_(var)", "response": "This function casts a variable to a type and tries to be clever when var is a string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef smart_cast2(var):\n    if var is None:\n        return None\n    if isinstance(var, six.string_types):\n        castvar = None\n        lower = var.lower()\n        if lower == 'true':\n            return True\n        elif lower == 'false':\n            return False\n        elif lower == 'none':\n            return None\n        if var.startswith('[') and var.endswith(']'):\n            #import re\n            #subvar_list = re.split(r',\\s*' + ut.negative_lookahead(r'[^\\[\\]]*\\]'), var[1:-1])\n            return smart_cast(var[1:-1], list)\n        elif var.startswith('(') and var.endswith(')'):\n            #import re\n            #subvar_list = re.split(r',\\s*' + ut.negative_lookahead(r'[^\\[\\]]*\\]'), var[1:-1])\n            return tuple(smart_cast(var[1:-1], list))\n        type_list = [int, float]\n        for type_ in type_list:\n            castvar = try_cast(var, type_)\n            if castvar is not None:\n                break\n        if castvar is None:\n            castvar = var\n    else:\n        castvar = var\n    return castvar", "response": "r This function takes a string and attempts to cast it to a reasonable value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all fuzzy names in a string", "response": "def fuzzy_subset(str_):\n    \"\"\"\n    converts a string into an argument to list_take\n    \"\"\"\n    if str_ is None:\n        return str_\n    if ':' in str_:\n        return smart_cast(str_, slice)\n    if str_.startswith('['):\n        return smart_cast(str_[1:-1], list)\n    else:\n        return smart_cast(str_, list)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fuzzy_int(str_):\n    try:\n        ret = int(str_)\n        return ret\n    except Exception:\n        # Parse comma separated values as ints\n        if re.match(r'\\d*,\\d*,?\\d*', str_):\n            return tuple(map(int, str_.split(',')))\n        # Parse range values as ints\n        if re.match(r'\\d*:\\d*:?\\d*', str_):\n            return tuple(range(*map(int, str_.split(':'))))\n        raise", "response": "lets some special strings be interpreted as ints"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget types accounting for numpy Taxonomy", "response": "def get_type(var):\n    \"\"\"\n    Gets types accounting for numpy\n\n    Ignore:\n        import utool as ut\n        import pandas as pd\n        var = np.array(['a', 'b', 'c'])\n        ut.get_type(var)\n        var = pd.Index(['a', 'b', 'c'])\n        ut.get_type(var)\n    \"\"\"\n    if HAVE_NUMPY and isinstance(var, np.ndarray):\n        if _WIN32:\n            # This is a weird system specific error\n            # https://github.com/numpy/numpy/issues/3667\n            type_ = var.dtype\n        else:\n            type_ = var.dtype.type\n    elif HAVE_PANDAS and isinstance(var, pd.Index):\n        if _WIN32:\n            type_ = var.dtype\n        else:\n            type_ = var.dtype.type\n    else:\n        type_ = type(var)\n    return type_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_homogenous_list_type(list_):\n    # TODO Expand and make work correctly\n    if HAVE_NUMPY and isinstance(list_, np.ndarray):\n        item = list_\n    elif isinstance(list_, list) and len(list_) > 0:\n        item = list_[0]\n    else:\n        item = None\n    if item is not None:\n        if is_float(item):\n            type_ = float\n        elif is_int(item):\n            type_ = int\n        elif is_bool(item):\n            type_ = bool\n        elif is_str(item):\n            type_ = str\n        else:\n            type_ = get_type(item)\n    else:\n        type_ = None\n    return type_", "response": "Returns the type of the homogenous list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npop a value off the top of the stack.", "response": "def pop(self):\n        \"\"\"\n        Pops a value off the top of the stack.\n\n        @return: Value popped off the stack.\n        @rtype: *\n\n        @raise StackError: Raised when there is a stack underflow.\n        \"\"\"\n\n        if self.stack:\n            val = self.stack[0]\n            self.stack = self.stack[1:]\n            return val\n        else:\n            raise StackError('Stack empty')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstoring all spectra rt injection time and scan nr in db", "response": "def create_spectra_lookup(lookup, fn_spectra):\n    \"\"\"Stores all spectra rt, injection time, and scan nr in db\"\"\"\n    to_store = []\n    mzmlmap = lookup.get_mzmlfile_map()\n    for fn, spectrum in fn_spectra:\n        spec_id = '{}_{}'.format(mzmlmap[fn], spectrum['scan'])\n        mzml_rt = round(float(spectrum['rt']), 12)\n        mzml_iit = round(float(spectrum['iit']), 12)\n        mz = float(spectrum['mz'])\n        to_store.append((spec_id, mzmlmap[fn], spectrum['scan'],\n                         spectrum['charge'], mz, mzml_rt, mzml_iit))\n        if len(to_store) == DB_STORE_CHUNK:\n            lookup.store_mzmls(to_store)\n            to_store = []\n    lookup.store_mzmls(to_store)\n    lookup.index_mzml()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lists_eq(list1, list2):\n    if len(list1) != len(list2):\n        return False\n    for count, (item1, item2) in enumerate(zip(list1, list2)):\n        if isinstance(item1, np.ndarray) or isinstance(item2, np.ndarray):\n            failed = not np.all(item1 == item2)  # lists_eq(item1, item2)\n        else:\n            failed = item1 != item2\n        if failed:\n            return False\n    return True", "response": "recursive function to compare lists"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_inbounds(num, low, high, msg='', eq=False, verbose=not util_arg.QUIET):\n    from utool import util_str\n    if util_arg.NO_ASSERTS:\n        return\n    passed = util_alg.inbounds(num, low, high, eq=eq)\n    if isinstance(passed, np.ndarray):\n        passflag = np.all(passed)\n    else:\n        passflag = passed\n    if not passflag:\n        failednum = num.compress(~passed) if isinstance(num, np.ndarray) else num\n        failedlow = low.compress(~passed) if isinstance(low, np.ndarray) else low\n        failedhigh = high.compress(~passed) if isinstance(high, np.ndarray) else high\n        msg_ = 'num=%r is out of bounds=(%r, %r)' % (failednum, failedlow, failedhigh)\n        raise AssertionError(msg_ + '\\n' + msg)\n    else:\n        if verbose:\n            op = '<=' if eq else '<'\n            fmtstr = 'Passed assert_inbounds: {low} {op} {num} {op} {high}'\n            print(fmtstr.format(low=low, op=op, num=util_str.truncate_str(str(num)), high=high))", "response": "r Asserts that num is within the given low and high bounds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_almost_eq(arr_test, arr_target, thresh=1E-11):\n    if util_arg.NO_ASSERTS:\n        return\n    import utool as ut\n    arr1 = np.array(arr_test)\n    arr2 = np.array(arr_target)\n    passed, error = ut.almost_eq(arr1, arr2, thresh, ret_error=True)\n    if not np.all(passed):\n        failed_xs = np.where(np.logical_not(passed))\n        failed_error = error.take(failed_xs)\n        failed_arr_test = arr1.take(failed_xs)\n        failed_arr_target = arr2.take(failed_xs)\n\n        msg_list = [\n            'FAILED ASSERT ALMOST EQUAL',\n            '  * failed_xs = %r' % (failed_xs,),\n            '  * failed_error = %r' % (failed_error,),\n            '  * failed_arr_test   = %r' % (failed_arr_test,),\n            '  * failed_arr_target = %r' % (failed_arr_target,),\n        ]\n        msg = '\\n'.join(msg_list)\n        raise AssertionError(msg)\n    return error", "response": "r Tests if two arrays are equal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninvoking the callback with a command - object for each connection.", "response": "def command_for_all_connections(self, cb):\n        \"\"\"Invoke the callback with a command-object for each connection.\"\"\"\n\n        for connection in self.__master.connections:\n            cb(connection.command)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dump_autogen_code(fpath, autogen_text, codetype='python', fullprint=None,\n                      show_diff=None, dowrite=None):\n    \"\"\"\n    Helper that write a file if -w is given on command line, otherwise\n    it just prints it out. It has the opption of comparing a diff to the file.\n    \"\"\"\n    import utool as ut\n    if dowrite is None:\n        dowrite = ut.get_argflag(('-w', '--write'))\n    if show_diff is None:\n        show_diff = ut.get_argflag('--diff')\n    num_context_lines = ut.get_argval('--diff', type_=int, default=None)\n    show_diff = show_diff or num_context_lines is not None\n\n    num_context_lines = ut.get_argval('--diff', type_=int, default=None)\n\n    if fullprint is None:\n        fullprint = True\n\n    if fullprint is False:\n        fullprint = ut.get_argflag('--print')\n\n    print('[autogen] Autogenerated %s...\\n+---\\n' % (fpath,))\n    if not dowrite:\n        if fullprint:\n            ut.print_code(autogen_text, lexer_name=codetype)\n            print('\\nL___')\n        else:\n            print('specify --print to write to stdout')\n            pass\n        print('specify -w to write, or --diff to compare')\n        print('...would write to: %s' % fpath)\n    if show_diff:\n        if ut.checkpath(fpath, verbose=True):\n            prev_text = ut.read_from(fpath)\n            textdiff = ut.get_textdiff(prev_text, autogen_text,\n                                       num_context_lines=num_context_lines)\n            try:\n                ut.print_difftext(textdiff)\n            except UnicodeDecodeError:\n                import unicodedata\n                textdiff = unicodedata.normalize('NFKD', textdiff).encode('ascii', 'ignore')\n                ut.print_difftext(textdiff)\n\n        if dowrite:\n            print('WARNING: Not writing. Remove --diff from command line')\n    elif dowrite:\n        ut.write_to(fpath, autogen_text)", "response": "Helper that writes a file if - w is given on command line otherwise it just prints it out."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_modscript_alias(fpath, modname, args='', pyscript='python'):\n    import utool as ut\n    from os.path import splitext\n    allargs_dict = {\n        '.sh': ' $@',\n        '.bat': ' %1', }\n    _, script_ext = splitext(fpath)\n    if script_ext not in ['.sh', '.bat']:\n        script_ext = '.bat' if ut.WIN32 else 'sh'\n    allargs = (args + allargs_dict[script_ext]).strip(' ')\n    if not modname.endswith('.py'):\n        fmtstr = '{pyscript} -m {modname} {allargs}'\n    else:\n        fmtstr = '{pyscript} {modname} {allargs}'\n\n    cmdstr = fmtstr.format(pyscript=pyscript, modname=modname, allargs=allargs)\n    ut.write_to(fpath, cmdstr)\n    os.system('chmod +x ' + fpath)", "response": "write a modscript alias to a file"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autofix_codeblock(codeblock, max_line_len=80,\n                      aggressive=False,\n                      very_aggressive=False,\n                      experimental=False):\n    r\"\"\"\n    Uses autopep8 to format a block of code\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import utool as ut\n        >>> codeblock = ut.codeblock(\n            '''\n            def func( with , some = 'Problems' ):\n\n\n             syntax ='Ok'\n             but = 'Its very messy'\n             if None:\n                    # syntax might not be perfect due to being cut off\n                    ommiting_this_line_still_works=   True\n            ''')\n        >>> fixed_codeblock = ut.autofix_codeblock(codeblock)\n        >>> print(fixed_codeblock)\n    \"\"\"\n    # FIXME idk how to remove the blank line following the function with\n    # autopep8. It seems to not be supported by them, but it looks bad.\n    import autopep8\n    arglist = ['--max-line-length', '80']\n    if aggressive:\n        arglist.extend(['-a'])\n    if very_aggressive:\n        arglist.extend(['-a', '-a'])\n    if experimental:\n        arglist.extend(['--experimental'])\n    arglist.extend([''])\n    autopep8_options = autopep8.parse_args(arglist)\n    fixed_codeblock = autopep8.fix_code(codeblock, options=autopep8_options)\n    return fixed_codeblock", "response": "r Autofixes a codeblock."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_func_from_module(modname, funcname, verbose=True, moddir=None, modpath=None):\n    import utool as ut\n    from os.path import join\n    import imp\n    func = None\n    module = None\n    error_str = None\n    print('modname = %r' % (modname,))\n    print('funcname = %r' % (funcname,))\n    print('moddir = %r' % (moddir,))\n\n    try:\n        from xdoctest import static_analysis as static\n        from xdoctest import utils\n        print('modpath = {!r}'.format(modpath))\n    except ImportError:\n        modpath = None\n    else:\n        if modpath is None:\n            modpath = static.modname_to_modpath(modname)\n            if modpath is None:\n                modname = modname.split('.')[-1]\n                if moddir is not None:\n                    modpath = join(moddir, modname + '.py')\n                    if not exists(modpath):\n                        modpath = None\n                if modpath is None:\n                    raise Exception('Cannot find modname={} in moddir={}'.format(modname, moddir))\n        print('modpath = {!r}'.format(modpath))\n        module = utils.import_module_from_path(modpath)\n        print('module = {!r}'.format(module))\n        try:\n            func = eval('module.{}'.format(funcname))\n        except AttributeError:\n            imp.reload(module)\n            func = eval('module.{}'.format(funcname))\n        print('func = {!r}'.format(func))\n        return func, module, error_str\n\n    if not isinstance(modname, six.string_types):\n        error_str = 'modname=%r is not a string. bad input' % (modname,)\n    else:\n        if False:\n            # TODO: static analysis\n            import jedi  # NOQA\n            modpath = ut.util_import.get_modpath_from_modname(modname)\n            script = jedi.Script(path=modpath)\n            mod = script._get_module()\n            # monkeypatch\n            func.script = script\n            func = None\n            for name in mod.names_dict[funcname]:\n                if name.parent.type == 'funcdef':\n                    func = name.parent\n                    break\n            return func, mod, error_str\n            # ut.get_modpath_from_modname(modname)\n        if module is None:\n            try:\n                module = __import__(modname)\n            except ImportError:\n                if moddir is not None:\n                    #parts =\n                    # There can be a weird double import error thing happening here\n                    # Rectify the dots in the filename\n                    module = ut.import_module_from_fpath(join(moddir, modname.split('.')[-1] + '.py'))\n                else:\n                    raise\n            #import inspect\n            # try:\n            #     imp.reload(module)\n            # except Exception as ex:\n            #     pass\n        # if False:\n        #     # Try removing pyc if it exists\n        #     if module.__file__.endswith('.pyc'):\n        #         ut.delete(module.__file__, verbose=False)\n        #         try:\n        #             module = __import__(modname)\n        #         except ImportError:\n        #             if moddir is not None:\n        #                 module = ut.import_module_from_fpath(join(moddir, modname.split('.')[-1] + '.py'))\n        #             else:\n        #                 raise\n        try:\n            imp.reload(module)\n        except Exception as ex:\n            pass\n        try:\n            # FIXME: PYTHON 3\n            execstr = ut.codeblock(\n                '''\n                try:\n                    import {modname}\n                    module = {modname}\n                    #print('Trying to reload module=%r' % (module,))\n                    imp.reload(module)\n                except Exception:\n                    # If it fails maybe the module is not in the path\n                    if moddir is not None:\n                        try:\n                            import imp\n                            import os\n                            orig_dir = os.getcwd()\n                            os.chdir(moddir)\n                            modname_str = '{modname}'\n                            modinfo = imp.find_module(modname_str, [moddir])\n                            module = imp.load_module(modname_str, *modinfo)\n                            #print('loaded module=%r' % (module,))\n                        except Exception as ex:\n                            ut.printex(ex, 'failed to imp.load_module')\n                            pass\n                        finally:\n                            os.chdir(orig_dir)\n                import imp\n                import utool as ut\n                imp.reload(ut.util_autogen)\n                imp.reload(ut.util_inspect)\n                try:\n                    func = module.{funcname}\n                except AttributeError:\n                    docstr = 'Could not find attribute funcname={funcname} in modname={modname} This might be a reloading issue'\n                    imp.reload(module)\n                '''\n            ).format(**locals())\n            exec_locals = locals()\n            exec_globals = globals()\n            exec(execstr, exec_globals, exec_locals)\n            func = exec_locals.get('func', None)\n            module = exec_locals.get('module', None)\n        except Exception as ex2:\n            docstr = 'error ' + str(ex2)\n            if verbose:\n                import utool as ut\n                #ut.printex(ex1, 'ex1')\n                ut.printex(ex2, 'ex2', tb=True)\n            testcmd = 'python -c \"import utool; print(utool.auto_docstr(\\'%s\\', \\'%s\\'))\"' % (modname, funcname)\n            error_str = ut.formatex(ex2, 'ex2', tb=True, keys=['modname', 'funcname', 'testcmd'])\n            error_str += '---' + execstr\n    return func, module, error_str", "response": "r Loads a function from a module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_args_docstr(argname_list, argtype_list, argdesc_list, ismethod,\n                     va_name=None, kw_name=None, kw_keys=[]):\n    r\"\"\"\n    Builds the argument docstring\n\n    Args:\n        argname_list (list): names\n        argtype_list (list): types\n        argdesc_list (list): descriptions\n        ismethod (bool): if generating docs for a method\n        va_name (Optional[str]): varargs name\n        kw_name (Optional[str]): kwargs name\n        kw_keys (Optional[list]): accepted kwarg keys\n\n    Returns:\n        str: arg_docstr\n\n    CommandLine:\n        python -m utool.util_autogen make_args_docstr\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_autogen import *  # NOQA\n        >>> argname_list = ['argname_list', 'argtype_list', 'argdesc_list']\n        >>> argtype_list = ['list', 'list', 'list']\n        >>> argdesc_list = ['names', 'types', 'descriptions']\n        >>> va_name = 'args'\n        >>> kw_name = 'kwargs'\n        >>> kw_keys = ['']\n        >>> ismethod = False\n        >>> arg_docstr = make_args_docstr(argname_list, argtype_list,\n        >>>                               argdesc_list, ismethod, va_name,\n        >>>                               kw_name, kw_keys)\n        >>> result = str(arg_docstr)\n        >>> print(result)\n        argname_list (list): names\n        argtype_list (list): types\n        argdesc_list (list): descriptions\n        *args:\n        **kwargs:\n\n    \"\"\"\n    import utool as ut\n    if ismethod:\n        # Remove self from the list\n        argname_list = argname_list[1:]\n        argtype_list = argtype_list[1:]\n        argdesc_list = argdesc_list[1:]\n\n    argdoc_list = [arg + ' (%s): %s' % (_type, desc)\n                   for arg, _type, desc in zip(argname_list, argtype_list, argdesc_list)]\n\n    # Add in varargs and kwargs\n    # References:\n    # http://www.sphinx-doc.org/en/stable/ext/example_google.html#example-google\n    if va_name is not None:\n        argdoc_list.append('*' + va_name + ':')\n    if kw_name is not None:\n        import textwrap\n        prefix = '**' + kw_name + ': '\n        wrapped_lines = textwrap.wrap(', '.join(kw_keys), width=70 - len(prefix))\n        sep = '\\n' + (' ' * len(prefix))\n        kw_keystr = sep.join(wrapped_lines)\n        argdoc_list.append((prefix + kw_keystr).strip())\n\n    # align?\n    align_args = False\n    if align_args:\n        argdoc_aligned_list = ut.align_lines(argdoc_list, character='(')\n        arg_docstr = '\\n'.join(argdoc_aligned_list)\n    else:\n        arg_docstr = '\\n'.join(argdoc_list)\n    return arg_docstr", "response": "r Creates the argument docstring for the current object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates skeleton code to build an example doctest Args: funcname (str): function name modname (str): module name argname_list (str): list of argument names defaults (None): return_type (None): return_name (str): return variable name ismethod (bool): Returns: str: examplecode CommandLine: python -m utool.util_autogen --test-make_example_docstr Example: >>> # ENABLE_DOCTEST >>> from utool.util_autogen import * # NOQA >>> funcname = 'make_example_docstr' >>> modname = 'utool.util_autogen' >>> argname_list = ['qaids', 'qreq_'] >>> defaults = None >>> return_type = tuple >>> return_name = 'foo' >>> ismethod = False >>> examplecode = make_example_docstr(funcname, modname, argname_list, defaults, return_type, return_name, ismethod) >>> result = str(examplecode) >>> print(result) # DISABLE_DOCTEST from utool.util_autogen import * # NOQA import utool as ut import ibeis species = ibeis.const.TEST_SPECIES.ZEB_PLAIN qaids = ibs.get_valid_aids(species=species) qreq_ = ibeis.testdata_qreq_() foo = make_example_docstr(qaids, qreq_) result = ('foo = %s' % (ut.repr2(foo),)) print(result)", "response": "def make_example_docstr(funcname=None, modname=None, argname_list=None,\n                        defaults=None, return_type=None, return_name=None,\n                        ismethod=False):\n    \"\"\"\n    Creates skeleton code to build an example doctest\n\n    Args:\n        funcname (str):  function name\n        modname (str):  module name\n        argname_list (str):  list of argument names\n        defaults (None):\n        return_type (None):\n        return_name (str):  return variable name\n        ismethod (bool):\n\n    Returns:\n        str: examplecode\n\n    CommandLine:\n        python -m utool.util_autogen --test-make_example_docstr\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_autogen import *  # NOQA\n        >>> funcname = 'make_example_docstr'\n        >>> modname = 'utool.util_autogen'\n        >>> argname_list = ['qaids', 'qreq_']\n        >>> defaults = None\n        >>> return_type = tuple\n        >>> return_name = 'foo'\n        >>> ismethod = False\n        >>> examplecode = make_example_docstr(funcname, modname, argname_list, defaults, return_type, return_name, ismethod)\n        >>> result = str(examplecode)\n        >>> print(result)\n        # DISABLE_DOCTEST\n        from utool.util_autogen import *  # NOQA\n        import utool as ut\n        import ibeis\n        species = ibeis.const.TEST_SPECIES.ZEB_PLAIN\n        qaids = ibs.get_valid_aids(species=species)\n        qreq_ = ibeis.testdata_qreq_()\n        foo = make_example_docstr(qaids, qreq_)\n        result = ('foo = %s' % (ut.repr2(foo),))\n        print(result)\n    \"\"\"\n    import utool as ut\n\n    examplecode_lines = []\n    top_import_fmstr = 'from {modname} import *  # NOQA'\n    top_import = top_import_fmstr.format(modname=modname)\n    import_lines = [top_import]\n    if modname.startswith('utool'):\n        import_lines += ['import utool as ut']\n    # is_show_func = not modname.startswith('utool') and not modname.startswith('mtgmonte')\n    is_show_func = modname.startswith('plottool')\n\n    # TODO: Externally register these\n    default_argval_map = {\n        'ibs'       : 'ibeis.opendb(defaultdb=\\'testdb1\\')',\n        'testres'   : 'ibeis.testdata_expts(\\'PZ_MTEST\\')',\n        'qreq_'     : 'ibeis.testdata_qreq_()',\n        'cm_list'   : 'qreq_.execute()',\n        'cm'        : 'qreq_.execute()[0]',\n        'aid_list'  : 'ibs.get_valid_aids()',\n        'nid_list'  : 'ibs._get_all_known_nids()',\n        'qaids'     : 'ibs.get_valid_aids(species=species)',\n        'daids'     : 'ibs.get_valid_aids(species=species)',\n        'species'   : 'ibeis.const.TEST_SPECIES.ZEB_PLAIN',\n        'kpts'      : 'vt.dummy.get_dummy_kpts()',\n        'dodraw'    : 'ut.show_was_requested()',\n        'img_fpath' : 'ut.grab_test_imgpath(\\'carl.jpg\\')',\n        'gfpath'    : 'ut.grab_test_imgpath(\\'carl.jpg\\')',\n        'img'       : 'vt.imread(img_fpath)',\n        'img_in'    : 'vt.imread(img_fpath)',\n        'bbox'      : '(10, 10, 50, 50)',\n        'theta'     : '0.0',\n        'rng'       : 'np.random.RandomState(0)',\n    }\n    import_depends_map = {\n        'ibeis':    'import ibeis',\n        'vt':       'import vtool as vt',\n        #'img':      'import vtool as vt',  # TODO: remove. fix dependency\n        #'species':  'import ibeis',\n    }\n    var_depends_map = {\n        'species':   ['ibeis'],\n        'ibs':       ['ibeis'],\n        'testres': ['ibeis'],\n        'kpts':      ['vt'],\n        #'qreq_':     ['ibs', 'species', 'daids', 'qaids'],\n        'qreq_':     ['ibeis'],\n        'qaids':     ['ibs'],\n        'daids':     ['ibs'],\n        'qaids':     ['species'],\n        'daids':     ['species'],\n        'img':       ['img_fpath', 'vt'],\n    }\n\n    def find_arg_defaultrepr(argname, val):\n        import types\n        if val == '?':\n            if argname in default_argval_map:\n                val = ut.PythonStatement(default_argval_map[argname])\n                if argname in import_depends_map:\n                    import_lines.append(import_depends_map[argname])\n        elif isinstance(val, types.ModuleType):\n            return val.__name__\n        return repr(val)\n\n    # augment argname list with dependencies\n    dependant_argnames = []  # deque()\n    def append_dependant_argnames(argnames, dependant_argnames):\n        \"\"\" use hints to add known dependencies for certain argument inputs \"\"\"\n        for argname in argnames:\n            # Check if argname just implies an import\n            if argname in import_depends_map:\n                import_lines.append(import_depends_map[argname])\n            # Check if argname was already added as dependency\n            if (argname not in dependant_argnames and argname not in\n                 argname_list and argname not in import_depends_map):\n                dependant_argnames.append(argname)\n            # Check if argname has dependants\n            if argname in var_depends_map:\n                argdeps = var_depends_map[argname]\n                # RECURSIVE CALL\n                append_dependant_argnames(argdeps, dependant_argnames)\n    append_dependant_argnames(argname_list, dependant_argnames)\n\n    # Define argnames and dependencies in example code\n    # argnames prefixed with dependeancies\n    argname_list_ = list(dependant_argnames) + argname_list\n\n    # Default example values\n    defaults_ = [] if defaults is None else defaults\n    num_unknown = (len(argname_list_) - len(defaults_))\n    default_vals = ['?'] * num_unknown + list(defaults_)\n    arg_val_iter = zip(argname_list_, default_vals)\n    inferred_defaults = [find_arg_defaultrepr(argname, val)\n                         for argname, val in arg_val_iter]\n    argdef_lines = ['%s = %s' % (argname, inferrepr)\n                    for argname, inferrepr in\n                    zip(argname_list_, inferred_defaults)]\n    import_lines = ut.unique_ordered(import_lines)\n\n    if any([inferrepr == repr('?') for inferrepr in inferred_defaults]):\n        examplecode_lines.append('# DISABLE_DOCTEST')\n    else:\n        # Enable the test if it can be run immediately\n        examplecode_lines.append('# DISABLE_DOCTEST')\n\n    examplecode_lines.extend(import_lines)\n    examplecode_lines.extend(argdef_lines)\n    # Default example result assignment\n    result_assign = ''\n    result_print = None\n    if 'return_name' in vars():\n        if return_type is not None:\n            if return_name is None:\n                return_name = 'result'\n            result_assign = return_name + ' = '\n            result_print = 'print(result)'  # + return_name + ')'\n    # Default example call\n    if ismethod:\n        selfname = argname_list[0]\n        methodargs = ', '.join(argname_list[1:])\n        tup = (selfname, '.', funcname, '(', methodargs, ')')\n        example_call = ''.join(tup)\n    else:\n        funcargs = ', '.join(argname_list)\n        tup = (funcname, '(', funcargs, ')')\n        example_call = ''.join(tup)\n    # Append call line\n    examplecode_lines.append(result_assign + example_call)\n    if result_print is not None:\n        if return_name != 'result':\n            #examplecode_lines.append('result = str(' + return_name + ')')\n            result_line_fmt = 'result = (\\'{return_name} = %s\\' % (ut.repr2({return_name}),))'\n            result_line = result_line_fmt.format(return_name=return_name)\n            examplecode_lines.append(result_line)\n        examplecode_lines.append(result_print)\n\n    # TODO: infer this\n    if is_show_func:\n        examplecode_lines += [\n            'ut.quit_if_noshow()',\n            'import plottool as pt',\n            'ut.show_if_requested()',\n        ]\n\n    examplecode = '\\n'.join(examplecode_lines)\n    return examplecode"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_default_docstr(func, with_args=True, with_ret=True,\n                        with_commandline=True, with_example=True,\n                        with_header=False, with_debug=False):\n    r\"\"\"\n    Tries to make a sensible default docstr so the user\n    can fill things in without typing too much\n\n    # TODO: Interleave old documentation with new documentation\n\n    Args:\n        func (function): live python function\n        with_args (bool):\n        with_ret (bool): (Defaults to True)\n        with_commandline (bool): (Defaults to True)\n        with_example (bool): (Defaults to True)\n        with_header (bool): (Defaults to False)\n        with_debug (bool): (Defaults to False)\n\n    Returns:\n        tuple: (argname, val)\n\n    Ignore:\n        pass\n\n    CommandLine:\n        python -m utool.util_autogen --exec-make_default_docstr --show\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_autogen import *  # NOQA\n        >>> import utool as ut\n        >>> func = ut.make_default_docstr\n        >>> #func = ut.make_args_docstr\n        >>> #func = PythonStatement\n        >>> func = auto_docstr\n        >>> default_docstr = make_default_docstr(func)\n        >>> result = str(default_docstr)\n        >>> print(result)\n\n    \"\"\"\n    import utool as ut\n    #from utool import util_inspect\n    funcinfo = ut.util_inspect.infer_function_info(func)\n\n    argname_list   = funcinfo.argname_list\n    argtype_list   = funcinfo.argtype_list\n    argdesc_list   = funcinfo.argdesc_list\n    return_header  = funcinfo.return_header\n    return_type    = funcinfo.return_type\n    return_name    = funcinfo.return_name\n    return_desc    = funcinfo.return_desc\n    funcname       = funcinfo.funcname\n    modname        = funcinfo.modname\n    defaults       = funcinfo.defaults\n    num_indent     = funcinfo.num_indent\n    needs_surround = funcinfo.needs_surround\n    funcname       = funcinfo.funcname\n    ismethod       = funcinfo.ismethod\n    va_name        = funcinfo.va_name\n    kw_name        = funcinfo.kw_name\n    kw_keys        = funcinfo.kw_keys\n\n    docstr_parts = []\n    # Header part\n    if with_header:\n        header_block = funcname\n        docstr_parts.append(header_block)\n\n    # Args part\n    if with_args and len(argname_list) > 0:\n        argheader = 'Args'\n        arg_docstr = make_args_docstr(argname_list, argtype_list, argdesc_list,\n                                      ismethod, va_name, kw_name, kw_keys)\n        argsblock = make_docstr_block(argheader, arg_docstr)\n\n        docstr_parts.append(argsblock)\n\n    # if False:\n    #     with_kw = with_args\n    #     if with_kw and len(kwarg_keys) > 0:\n    #         #ut.embed()\n    #         import textwrap\n    #         kwargs_docstr = ', '.join(kwarg_keys)\n    #         kwargs_docstr = '\\n'.join(textwrap.wrap(kwargs_docstr))\n    #         kwargsblock = make_docstr_block('Kwargs', kwargs_docstr)\n    #         docstr_parts.append(kwargsblock)\n\n    # Return / Yeild part\n    if with_ret and return_header is not None:\n        if return_header is not None:\n            return_doctr = make_returns_or_yeilds_docstr(return_type, return_name, return_desc)\n            returnblock = make_docstr_block(return_header, return_doctr)\n            docstr_parts.append(returnblock)\n\n    # Example part\n    # try to generate a simple and unit testable example\n    if with_commandline:\n        cmdlineheader = 'CommandLine'\n        cmdlinecode = make_cmdline_docstr(funcname, modname)\n        cmdlineblock = make_docstr_block(cmdlineheader, cmdlinecode)\n        docstr_parts.append(cmdlineblock)\n\n    if with_example:\n        exampleheader = 'Example'\n        examplecode = make_example_docstr(funcname, modname, argname_list,\n                                          defaults, return_type, return_name,\n                                          ismethod)\n        examplecode_ = ut.indent(examplecode, '>>> ')\n        exampleblock = make_docstr_block(exampleheader, examplecode_)\n        docstr_parts.append(exampleblock)\n\n    # DEBUG part (in case something goes wrong)\n    if with_debug:\n        debugheader = 'Debug'\n        debugblock = ut.codeblock(\n            '''\n            num_indent = {num_indent}\n            '''\n        ).format(num_indent=num_indent)\n        debugblock = make_docstr_block(debugheader, debugblock)\n        docstr_parts.append(debugblock)\n\n    # Enclosure / Indentation Parts\n    if needs_surround:\n        docstr_parts = ['r\"\"\"'] + ['\\n\\n'.join(docstr_parts)] + ['\"\"\"']\n        default_docstr = '\\n'.join(docstr_parts)\n    else:\n        default_docstr = '\\n\\n'.join(docstr_parts)\n\n    docstr_indent = ' ' * (num_indent + 4)\n    default_docstr = ut.indent(default_docstr, docstr_indent)\n    return default_docstr", "response": "r Creates a sensible default docstr for a live python function"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_codeblock_syntax_sentinals(code_text):\n    flags = re.MULTILINE | re.DOTALL\n    code_text_ = code_text\n    code_text_ = re.sub(r'^ *# *REM [^\\n]*$\\n?', '', code_text_, flags=flags)\n    code_text_ = re.sub(r'^ *# STARTBLOCK *$\\n', '', code_text_, flags=flags)\n    code_text_ = re.sub(r'^ *# ENDBLOCK *$\\n?', '', code_text_, flags=flags)\n    code_text_ = code_text_.rstrip()\n    return code_text_", "response": "r Removes template comments vim sentinals and vim sentinals from the code_text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort_protein_groups(pgroups, evidence):\n    sortfnxs = get_sortfnxs(evidence)\n    pgroups_out = {}\n    for master, pgroup in pgroups.items():\n        sorted_pg = sort_protein_group(pgroup, sortfnxs, 0)\n        pgroups_out[master] = sorted_pg\n    return pgroups_out", "response": "Returns a dict pgroups containing dict pgroups for each master in the dict pgroups that comprise a proteins group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sort_protein_group(pgroup, sortfunctions, sortfunc_index):\n    pgroup_out = []\n    subgroups = sortfunctions[sortfunc_index](pgroup)\n    sortfunc_index += 1\n    for subgroup in subgroups:\n        if len(subgroup) > 1 and sortfunc_index < len(sortfunctions):\n            pgroup_out.extend(sort_protein_group(subgroup,\n                                                 sortfunctions,\n                                                 sortfunc_index))\n        else:\n            pgroup_out.extend(subgroup)\n    return pgroup_out", "response": "Recursive function that sorts a protein group by a number of sorting\n    functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_amounts(proteins, sort_index):\n    amounts = {}\n    for protein in proteins:\n        amount_x_for_protein = protein[sort_index]\n        try:\n            amounts[amount_x_for_protein].append(protein)\n        except KeyError:\n            amounts[amount_x_for_protein] = [protein]\n    return [v for k, v in sorted(amounts.items(), reverse=True)]", "response": "Generic function for sorting peptides and psms. Assumes a higher\n    number is better for what is passed at sort_index position in protein."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind statistics that correspond to PSM / peptide feature s score. Loops through list of qvality generated scores until it finds a score that is greater than the score. Returns None if no statistics are found.", "response": "def lookup_statistic(score, stats):\n    \"\"\" Finds statistics that correspond to PSM/peptide/protein feature's\n    score. Loops through list of qvality generated scores until it finds\n    values closest to the feature's svm_score.\"\"\"\n    if score in stats:\n        return stats[score]['q'], stats[score]['PEP'], None\n    else:\n        lower, warning = None, None\n        for stat_score in sorted(stats.keys()):\n            if score < stat_score:\n                break\n            lower = stat_score\n        if score > stat_score:\n            warning = ('WARNING! Values found with higher svm_score than in '\n                       'qvality recalculation!')\n        if not lower:\n            warning = ('WARNING! Values found with lower svm_score than in '\n                       'qvality recalculation were set to PEP=1, qval=1.')\n            return '1', '1', warning\n        qval = (float(stats[stat_score]['q']) + float(stats[lower]['q'])) / 2\n        pep = (float(stats[stat_score]['PEP']) +\n               float(stats[lower]['PEP'])) / 2\n    return str(qval), str(pep), warning"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, position, array):\n        if not Gauged.map_append(self.ptr, position, array.ptr):\n            raise MemoryError", "response": "Append an array to the end of the map."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nslices the map from start and end.", "response": "def slice(self, start=0, end=0):\n        \"\"\"Slice the map from [start, end)\"\"\"\n        tmp = Gauged.map_new()\n        if tmp is None:\n            raise MemoryError\n        if not Gauged.map_concat(tmp, self.ptr, start, end, 0):\n            Gauged.map_free(tmp)  # pragma: no cover\n            raise MemoryError\n        return SparseMap(tmp)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef buffer(self, byte_offset=0):\n        contents = self.ptr.contents\n        ptr = addressof(contents.buffer.contents) + byte_offset\n        length = contents.length * 4 - byte_offset\n        return buffer((c_char * length).from_address(ptr).raw) \\\n            if length else None", "response": "Get a copy of the map buffer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef percentile(self, percentile):\n        percentile = float(percentile)\n        if percentile != percentile or percentile < 0 or percentile > 100:\n            raise ValueError('Expected a 0 <= percentile <= 100')\n        result = c_float()\n        if not Gauged.map_percentile(self.ptr, percentile, byref(result)):\n            raise MemoryError\n        return result.value", "response": "Get a percentile of all floats in the map."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a generator which yields ( offset array )", "response": "def iteritems(self):\n        \"\"\"Get a generator which yields (offset, array)\"\"\"\n        current_buf = self.ptr.contents.buffer\n        length = self.ptr.contents.length\n        seen_length = 0\n        header = c_size_t()\n        position = c_uint32()\n        arraylength = c_size_t()\n        arrayptr = FloatPtr()\n        header_ = byref(header)\n        position_ = byref(position)\n        arraylength_ = byref(arraylength)\n        arrayptr_ = byref(arrayptr)\n        advance = Gauged.map_advance\n        while seen_length < length:\n            current_buf = advance(current_buf, header_, position_,\n                                  arraylength_, arrayptr_)\n            seen_length += header.value + arraylength.value\n            address = addressof(arrayptr.contents)\n            arr = (c_float * arraylength.value).from_address(address)\n            yield position.value, list(arr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndoes the target match the whitelist entry?", "response": "def matches(target, entry):\n    \"\"\"Does the target match the whitelist entry?\"\"\"\n\n    # It must match all the non-empty entries.\n    for t, e in itertools.zip_longest(target, entry):\n        if e and t != e:\n            return False\n\n    # ...and the provider and user can't be empty.\n    return entry[0] and entry[1]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nthrow an exception if the entry isn t on the whitelist.", "response": "def check_entry(*entry):\n    \"\"\"Throws an exception if the entry isn't on the whitelist.\"\"\"\n    whitelist = read_whitelist()\n    if not check_allow_prompt(entry, whitelist):\n        whitelist.append(entry)\n        write_whitelist(whitelist)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_file(path, verbose=False):\n    for filehandle in filehandles(path, verbose=verbose):\n        return CTfile.load(filehandle)", "response": "Read CTfile formatted file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_files(path, verbose=True):\n    for filehandle in filehandles(path, verbose=verbose):\n        yield CTfile.load(filehandle)", "response": "Read multiple CTfile formatted files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting events from a file descriptor.", "response": "def get_events(fd, timeout=None):\n    \"\"\"get_events(fd[, timeout])\n\n    Return a list of InotifyEvent instances representing events read from\n    inotify.  If timeout is None, this will block forever until at least one\n    event can be read.  Otherwise, timeout should be an integer or float\n    specifying a timeout in seconds.  If get_events times out waiting for\n    events, an empty list will be returned.  If timeout is zero, get_events\n    will not block.\n    This version of get_events() will only block the current greenlet.\n    \"\"\"\n    (rlist, _, _) = select([fd], [], [], timeout)\n    if not rlist:\n        return []\n    events = []\n\n    while True:\n        buf = os.read(fd, _BUF_LEN)\n        i = 0\n        while i < len(buf):\n            (wd, mask, cookie, len_) = struct.unpack_from(_EVENT_FMT, buf, i)\n            name = None\n            if len_ > 0:\n                start = i + _EVENT_SIZE\n                end = start + len_\n                # remove \\0 terminator and padding\n                name = buf[start:end].rstrip(b'\\0').decode(ENCODING)\n\n            events.append(InotifyEvent(wd, mask, cookie, name))\n            i += _EVENT_SIZE + len_\n\n        (rlist, _, _) = select([fd], [], [], 0)\n        if not rlist:\n            break\n\n    return events"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading an uncached version of a Taxonomy from a file or URL.", "response": "def load_uncached(location, use_json=None):\n    \"\"\"\n    Return data at either a file location or at the raw version of a\n    URL, or raise an exception.\n\n    A file location either contains no colons like /usr/tom/test.txt,\n    or a single character and a colon like C:/WINDOWS/STUFF.\n\n    A URL location is anything that's not a file location.\n\n    If the URL ends in .json, .yml or .yaml and `json != False`,\n    or `json == True`, convert the data from YAML or JSON.\n    \"\"\"\n    if not whitelist.is_file(location):\n        r = requests.get(raw.raw(location))\n        if not r.ok:\n            raise ValueError('Couldn\\'t read %s with code %s:\\n%s' %\n                             (location, r.status_code, r.text))\n        data = r.text\n    else:\n        try:\n            f = os.path.realpath(os.path.abspath(os.path.expanduser(location)))\n            data = open(f).read()\n        except Exception as e:\n            e.args = (\n                'There was an error reading the file', location, f) + e.args\n            raise\n\n    if use_json is None:\n        use_json = any(location.endswith(s) for s in SUFFIXES)\n\n    if not use_json:\n        return data\n\n    try:\n        return yaml.load(data)\n    except Exception as e:\n        e.args = ('There was a JSON error in the file', location) + e.args\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grouping_delta_stats(old, new):\n    import pandas as pd\n    import utool as ut\n    group_delta = ut.grouping_delta(old, new)\n    stats = ut.odict()\n    unchanged = group_delta['unchanged']\n    splits = group_delta['splits']\n    merges = group_delta['merges']\n    hybrid = group_delta['hybrid']\n    statsmap = ut.partial(lambda x: ut.stats_dict(map(len, x), size=True))\n    stats['unchanged'] = statsmap(unchanged)\n    stats['old_split'] = statsmap(splits['old'])\n    stats['new_split'] = statsmap(ut.flatten(splits['new']))\n    stats['old_merge'] = statsmap(ut.flatten(merges['old']))\n    stats['new_merge'] = statsmap(merges['new'])\n    stats['old_hybrid'] = statsmap(hybrid['old'])\n    stats['new_hybrid'] = statsmap(hybrid['new'])\n    df = pd.DataFrame.from_dict(stats, orient='index')\n    df = df.loc[list(stats.keys())]\n    return df", "response": "Returns statistics about grouping changes between two sets of items"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diagonalized_iter(size):\n    for i in range(0, size + 1):\n        for r, c in zip(reversed(range(i)), (range(i))):\n            yield (r, c)\n    for i in range(1, size):\n        for r, c in zip(reversed(range(i, size)), (range(i, size))):\n            yield (r, c)", "response": "r Returns an iterator over the elements of the current language tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef product_nonsame(list1, list2):\n    for item1, item2 in itertools.product(list1, list2):\n        if item1 != item2:\n            yield (item1, item2)", "response": "product of list1 and list2 where items are non equal"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef greedy_max_inden_setcover(candidate_sets_dict, items, max_covers=None):\n    uncovered_set = set(items)\n    rejected_keys = set()\n    accepted_keys = set()\n    covered_items_list = []\n    while True:\n        # Break if we have enough covers\n        if max_covers is not None and len(covered_items_list) >= max_covers:\n            break\n        maxkey = None\n        maxlen = -1\n        # Loop over candidates to find the biggested unadded cover set\n        for key, candidate_items in six.iteritems(candidate_sets_dict):\n            if key in rejected_keys or key in accepted_keys:\n                continue\n            #print('Checking %r' % (key,))\n            lenval = len(candidate_items)\n            # len(uncovered_set.intersection(candidate_items)) == lenval:\n            if uncovered_set.issuperset(candidate_items):\n                if lenval > maxlen:\n                    maxkey = key\n                    maxlen = lenval\n            else:\n                rejected_keys.add(key)\n        # Add the set to the cover\n        if maxkey is None:\n            break\n        maxval = candidate_sets_dict[maxkey]\n        accepted_keys.add(maxkey)\n        covered_items_list.append(list(maxval))\n        # Add values in this key to the cover\n        uncovered_set.difference_update(maxval)\n    uncovered_items = list(uncovered_set)\n    covertup = uncovered_items, covered_items_list, accepted_keys\n    return covertup", "response": "Greedy algorithm for maximum independent set cover"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting cover exact algorithm for the set covering problem", "response": "def setcover_ilp(candidate_sets_dict, items=None, set_weights=None, item_values=None, max_weight=None, verbose=False):\n    \"\"\"\n    Set cover / Weighted Maximum Cover exact algorithm\n\n    https://en.wikipedia.org/wiki/Maximum_coverage_problem\n    \"\"\"\n    import utool as ut\n    import pulp\n    if items is None:\n        items = ut.flatten(candidate_sets_dict.values())\n    if set_weights is None:\n        set_weights = {i: 1 for i in candidate_sets_dict.keys()}\n    if item_values is None:\n        item_values = {e: 1 for e in items}\n\n    if max_weight is None:\n        max_weight = sum(ut.take(set_weights, candidate_sets_dict.keys()))\n\n    if False:\n        # This is true set coer\n        # Formulate integer program\n        prob = pulp.LpProblem(\"Set Cover\", pulp.LpMinimize)\n        # Solution variable indicates if set it chosen or not\n        set_indices = candidate_sets_dict.keys()\n        x = pulp.LpVariable.dicts(name='x', indexs=set_indices,\n                                  lowBound=0, upBound=1, cat=pulp.LpInteger)\n        # minimize the number of sets\n        prob.objective = sum(x[i] for i in set_indices)\n        # subject to\n        for e in items:\n            # each element is covered\n            containing_sets = [i for i in set_indices if e in candidate_sets_dict[i]]\n            prob.add(sum(x[i] for i in containing_sets) >= 1)\n        # Solve using with solver like CPLEX, GLPK, or SCIP.\n        #pulp.CPLEX().solve(prob)\n        pulp.PULP_CBC_CMD().solve(prob)\n        # Read solution\n        solution_keys = [i for i in set_indices if x[i].varValue]\n        solution_cover = {i: candidate_sets_dict[i] for i in solution_keys}\n        # Print summary\n        if verbose:\n            print(prob)\n            print('OPT:')\n            print('\\n'.join(['    %s = %s' % (x[i].name, x[i].varValue) for i in set_indices]))\n            print('solution_cover = %r' % (solution_cover,))\n    else:\n        prob = pulp.LpProblem(\"Maximum Cover\", pulp.LpMaximize)\n        # Solution variable indicates if set it chosen or not\n        item_indicies = items\n        set_indices = candidate_sets_dict.keys()\n        x = pulp.LpVariable.dicts(name='x', indexs=set_indices,\n                                  lowBound=0, upBound=1, cat=pulp.LpInteger)\n        y = pulp.LpVariable.dicts(name='y', indexs=item_indicies,\n                                  lowBound=0, upBound=1, cat=pulp.LpInteger)\n        r = pulp.LpVariable.dicts(name='r', indexs=item_indicies)\n        # maximize the value of the covered items\n        primary_objective = sum(item_values[e] * y[e] for e in item_indicies)\n        # minimize the number of sets used (make sure it does not influence the chosen primary objective)\n        # This is only possible when values are non-negative\n        # TODO: minimize redundency\n        min_influence = min(item_values.values())\n        secondary_weight = min_influence / (1.1 * len(set_indices))\n        secondary_objective = (sum(-x[i] for i in set_indices)) * secondary_weight\n        #\n        prob.objective = primary_objective + secondary_objective\n        # subject to\n        # no more than the maximum weight\n        prob.add(sum(x[i] * set_weights[i] for i in set_indices) <= max_weight)\n        # If an item is chosen than at least one set containing it is chosen\n        for e in item_indicies:\n            containing_sets = [i for i in set_indices if e in candidate_sets_dict[i]]\n            if len(containing_sets) > 0:\n                prob.add(sum(x[i] for i in containing_sets) >= y[e])\n                # record number of times each item is covered\n                prob.add(sum(x[i] for i in containing_sets) == r[e])\n        # Solve using with solver like CPLEX, GLPK, or SCIP.\n        #pulp.CPLEX().solve(prob)\n        pulp.PULP_CBC_CMD().solve(prob)\n        # Read solution\n        solution_keys = [i for i in set_indices if x[i].varValue]\n        solution_cover = {i: candidate_sets_dict[i] for i in solution_keys}\n        # Print summary\n        if verbose:\n            print(prob)\n            print('OPT:')\n            print('\\n'.join(['    %s = %s' % (x[i].name, x[i].varValue) for i in set_indices]))\n            print('\\n'.join(['    %s = %s' % (y[i].name, y[i].varValue) for i in item_indicies]))\n            print('solution_cover = %r' % (solution_cover,))\n    return solution_cover"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert xywh format to tlbr format", "response": "def xywh_to_tlbr(bbox, img_wh):\n    \"\"\" converts xywh format to (tlx, tly, blx, bly) \"\"\"\n    (img_w, img_h) = img_wh\n    if img_w == 0 or img_h == 0:\n        img_w = 1\n        img_h = 1\n        msg = '[cc2.1] Your csv tables have an invalid ANNOTATION.'\n        print(msg)\n        #warnings.warn(msg)\n        #ht = 1\n        #wt = 1\n    # Ensure ANNOTATION is within bounds\n    (x, y, w, h) = bbox\n    x1 = max(x, 0)\n    y1 = max(y, 0)\n    x2 = min(x + w, img_w - 1)\n    y2 = min(y + h, img_h - 1)\n    return (x1, y1, x2, y2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncounts the number of times each item appears in the dictionary", "response": "def item_hist(list_):\n    \"\"\" counts the number of times each item appears in the dictionary \"\"\"\n    dict_hist = {}\n    # Insert each item into the correct group\n    for item in list_:\n        if item not in dict_hist:\n            dict_hist[item] = 0\n        dict_hist[item] += 1\n    return dict_hist"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fibonacci_iterative(n):\n    a, b = 0, 1\n    for _ in range(0, n):\n        a, b = b, a + b\n    return a", "response": "This function is iterative version of fibonacci_iterative."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fibonacci_approx(n):\n    sqrt_5 = math.sqrt(5)\n    phi = (1 + sqrt_5) / 2\n    return ((phi ** n) - (-phi) ** (-n)) / sqrt_5", "response": "r Returns approximate value of n using closed form\n    expression"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_nth_prime(n, max_prime=4100, safe=True):\n    if n <= 100:\n        first_100_primes = (\n            2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61,\n            67, 71, 73, 79, 83, 89, 97, 101, 103, 107, 109, 113, 127, 131, 137,\n            139, 149, 151, 157, 163, 167, 173, 179, 181, 191, 193, 197, 199,\n            211, 223, 227, 229, 233, 239, 241, 251, 257, 263, 269, 271, 277,\n            281, 283, 293, 307, 311, 313, 317, 331, 337, 347, 349, 353, 359,\n            367, 373, 379, 383, 389, 397, 401, 409, 419, 421, 431, 433, 439,\n            443, 449, 457, 461, 463, 467, 479, 487, 491, 499, 503, 509, 521,\n            523, 541, )\n        #print(len(first_100_primes))\n        nth_prime = first_100_primes[n - 1]\n    else:\n        if safe:\n            primes = [num for num in range(2, max_prime) if is_prime(num)]\n            nth_prime = primes[n]\n        else:\n            # This can run for a while... get it? while?\n            nth_prime = get_nth_prime_bruteforce(n)\n    return nth_prime", "response": "find nth prime for small test"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nth_prime_bruteforce(n, start_guess=2, start_num_primes=0):\n    guess = start_guess\n    num_primes_found = start_num_primes\n    while True:\n        if is_prime(guess):\n            num_primes_found += 1\n        if num_primes_found == n:\n            nth_prime = guess\n            break\n        guess += 1\n    return nth_prime", "response": "Get the nth prime of a set of n - th ones"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef knapsack(items, maxweight, method='recursive'):\n    if method == 'recursive':\n        return knapsack_recursive(items, maxweight)\n    elif method == 'iterative':\n        return knapsack_iterative(items, maxweight)\n    elif method == 'ilp':\n        return knapsack_ilp(items, maxweight)\n    else:\n        raise NotImplementedError('[util_alg] knapsack method=%r' % (method,))", "response": "r Solve the knapsack problem by finding the most valuable subsequence of items subject that weighs no more than maxweight."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsolving knapsack using an integer linear program CommandLine: python -m utool.util_alg knapsack_ilp Example: >>> # DISABLE_DOCTEST >>> from utool.util_alg import * # NOQA >>> import utool as ut >>> # Solve https://xkcd.com/287/ >>> weights = [2.15, 2.75, 3.35, 3.55, 4.2, 5.8, 6.55] >>> values = [2.15, 2.75, 3.35, 3.55, 4.2, 5.8, 6.55] >>> indices = ['mixed fruit', 'french fries', 'side salad', >>> 'hot wings', 'mozzarella sticks', 'sampler plate', >>> 'barbecue'] >>> items = [(v, w, i) for v, w, i in zip(values, weights, indices)] >>> #items += [(3.95, 3.95, 'mystery plate')] >>> maxweight = 15.05 >>> verbose = True >>> total_value, items_subset = knapsack_ilp(items, maxweight, verbose) >>> print('items_subset = %s' % (ut.repr3(items_subset, nl=1),))", "response": "def knapsack_ilp(items, maxweight, verbose=False):\n    \"\"\"\n    solves knapsack using an integer linear program\n\n    CommandLine:\n        python -m utool.util_alg knapsack_ilp\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> import utool as ut\n        >>> # Solve https://xkcd.com/287/\n        >>> weights = [2.15, 2.75, 3.35, 3.55, 4.2, 5.8, 6.55]\n        >>> values  = [2.15, 2.75, 3.35, 3.55, 4.2, 5.8, 6.55]\n        >>> indices = ['mixed fruit', 'french fries', 'side salad',\n        >>>            'hot wings', 'mozzarella sticks', 'sampler plate',\n        >>>            'barbecue']\n        >>> items = [(v, w, i) for v, w, i in zip(values, weights, indices)]\n        >>> #items += [(3.95, 3.95, 'mystery plate')]\n        >>> maxweight = 15.05\n        >>> verbose = True\n        >>> total_value, items_subset = knapsack_ilp(items, maxweight, verbose)\n        >>> print('items_subset = %s' % (ut.repr3(items_subset, nl=1),))\n    \"\"\"\n    import pulp\n    # Given Input\n    values  = [t[0] for t in items]\n    weights = [t[1] for t in items]\n    indices = [t[2] for t in items]\n    # Formulate integer program\n    prob = pulp.LpProblem(\"Knapsack\", pulp.LpMaximize)\n    # Solution variables\n    x = pulp.LpVariable.dicts(name='x', indexs=indices,\n                              lowBound=0, upBound=1, cat=pulp.LpInteger)\n    # maximize objective function\n    prob.objective = sum(v * x[i] for v, i in zip(values, indices))\n    # subject to\n    prob.add(sum(w * x[i] for w, i in zip(weights, indices)) <= maxweight)\n    # Solve using with solver like CPLEX, GLPK, or SCIP.\n    #pulp.CPLEX().solve(prob)\n    pulp.PULP_CBC_CMD().solve(prob)\n    # Read solution\n    flags = [x[i].varValue for i in indices]\n    total_value = sum([val for val, flag in zip(values, flags) if flag])\n    items_subset = [item for item, flag in zip(items, flags) if flag]\n    # Print summary\n    if verbose:\n        print(prob)\n        print('OPT:')\n        print('\\n'.join(['    %s = %s' % (x[i].name, x[i].varValue) for i in indices]))\n        print('total_value = %r' % (total_value,))\n    return total_value, items_subset"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef knapsack_iterative_int(items, maxweight):\n    values  = [t[0] for t in items]\n    weights = [t[1] for t in items]\n    maxsize = maxweight + 1\n    # Sparse representation seems better\n    dpmat = defaultdict(lambda: defaultdict(lambda: np.inf))\n    kmat = defaultdict(lambda: defaultdict(lambda: False))\n    idx_subset = []  # NOQA\n    for w in range(maxsize):\n        dpmat[0][w] = 0\n    # For each item consider to include it or not\n    for idx in range(len(items)):\n        item_val = values[idx]\n        item_weight = weights[idx]\n        # consider at each possible bag size\n        for w in range(maxsize):\n            valid_item = item_weight <= w\n            if idx > 0:\n                prev_val = dpmat[idx - 1][w]\n                prev_noitem_val = dpmat[idx - 1][w - item_weight]\n            else:\n                prev_val = 0\n                prev_noitem_val = 0\n            withitem_val = item_val + prev_noitem_val\n            more_valuable = withitem_val > prev_val\n            if valid_item and more_valuable:\n                dpmat[idx][w] = withitem_val\n                kmat[idx][w] = True\n            else:\n                dpmat[idx][w] = prev_val\n                kmat[idx][w] = False\n    # Trace backwards to get the items used in the solution\n    K = maxweight\n    for idx in reversed(range(len(items))):\n        if kmat[idx][K]:\n            idx_subset.append(idx)\n            K = K - weights[idx]\n    idx_subset = sorted(idx_subset)\n    items_subset = [items[i] for i in idx_subset]\n    total_value = dpmat[len(items) - 1][maxweight]\n    return total_value, items_subset", "response": "r Return the iteration of the knapsack method for the given items and maxweight."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef knapsack_iterative_numpy(items, maxweight):\n    #import numpy as np\n    items = np.array(items)\n    weights = items.T[1]\n    # Find maximum decimal place (this problem is in NP)\n    max_exp = max([number_of_decimals(w_) for w_ in weights])\n    coeff = 10 ** max_exp\n    # Adjust weights to be integral\n    weights = (weights * coeff).astype(np.int)\n    values  = items.T[0]\n    MAXWEIGHT = int(maxweight * coeff)\n    W_SIZE = MAXWEIGHT + 1\n\n    dpmat = np.full((len(items), W_SIZE), np.inf)\n    kmat = np.full((len(items), W_SIZE), 0, dtype=np.bool)\n    idx_subset = []\n\n    for w in range(W_SIZE):\n        dpmat[0][w] = 0\n    for idx in range(1, len(items)):\n        item_val = values[idx]\n        item_weight = weights[idx]\n        for w in range(W_SIZE):\n            valid_item = item_weight <= w\n            prev_val = dpmat[idx - 1][w]\n            if valid_item:\n                prev_noitem_val = dpmat[idx - 1][w - item_weight]\n                withitem_val = item_val + prev_noitem_val\n                more_valuable = withitem_val > prev_val\n            else:\n                more_valuable = False\n            dpmat[idx][w] = withitem_val if more_valuable else prev_val\n            kmat[idx][w] = more_valuable\n    K = MAXWEIGHT\n    for idx in reversed(range(1, len(items))):\n        if kmat[idx, K]:\n            idx_subset.append(idx)\n            K = K - weights[idx]\n    idx_subset = sorted(idx_subset)\n    items_subset = [items[i] for i in idx_subset]\n    total_value = dpmat[len(items) - 1][MAXWEIGHT]\n    return total_value, items_subset", "response": "Iterative knapsack method for iteration over knapsack items in numpy format"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef knapsack_greedy(items, maxweight):\n    items_subset = []\n    total_weight = 0\n    total_value = 0\n    for item in items:\n        value, weight = item[0:2]\n        if total_weight + weight > maxweight:\n            continue\n        else:\n            items_subset.append(item)\n            total_weight += weight\n            total_value += value\n    return total_value, items_subset", "response": "r Greedy version of knapsack algorithm\n    does not sort input by largest value\n            first if desired."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef maximin_distance_subset1d(items, K=None, min_thresh=None, verbose=False):\n    if False:\n        import pulp\n        # Formulate integer program\n        prob = pulp.LpProblem(\"MaxSizeLargeDistSubset\", pulp.LpMaximize)\n        # Solution variable indicates if set it chosen or not\n        item_indices = list(range(len(items)))\n        pair_indices = list(ut.combinations(item_indices, 2))\n        x = pulp.LpVariable.dicts(name='x', indexs=pair_indices,\n                                  lowBound=0, upBound=1, cat=pulp.LpInteger)\n        y = pulp.LpVariable.dicts(name='y', indexs=item_indices,\n                                  lowBound=0, upBound=1, cat=pulp.LpInteger)\n        # minimize the number of sets\n        prob.objective = sum(y[i] for i in item_indices)\n\n        # subject to\n        count = 0\n        for u, v in pair_indices:\n            # Minimum thresh constraint\n            if abs(items[u] - items[v]) < min_thresh:\n                prob.add(x[(u, v)] == 0, name='thresh_%r' % (count,))\n                count += 1\n\n        count = 0\n        for u, v in pair_indices:\n            prob.add(y[u] + y[v] - x[(u, v)] <= 1, 'exclusion_%r' % (count,))\n            count += 1\n\n        pulp.PULP_CBC_CMD().solve(prob)\n        # Read solution\n        flags = [y[i].varValue >= 1.0 for i in item_indices]\n        chosen_items_idxs = ut.where(flags)\n        chosen_items = ut.take(items, chosen_items_idxs)\n\n        # total_value = sum([val for val, flag in zip(values, flags) if flag])\n        # items_subset = [item for item, flag in zip(items, flags) if flag]\n        # each element is covered\n        # containing_sets = [i for i in set_indices if e in candidate_sets_dict[i]]\n        # prob.add(sum(x[i] for i in containing_sets) >= 1)\n\n    import utool as ut\n    import vtool as vt\n    points = np.array(items)[:, None]\n    # Initial sorting of 1d points\n    initial_sortx = points.argsort(axis=0).flatten()\n    points = points.take(initial_sortx, axis=0)\n\n    if K is None:\n        K = len(items)\n\n    def distfunc(x, y):\n        return np.abs(x - y)\n\n    assert points.shape[1] == 1\n    assert len(points) >= K, 'cannot return subset'\n    if K == 1:\n        current_idx = [0]\n    else:\n        current_idx = [0, -1]\n        if min_thresh is not None and distfunc(points[0], points[-1])[0] < min_thresh:\n            current_idx = [0]\n    chosen_mask = vt.index_to_boolmask(current_idx, len(points))\n\n    for k in range(2, K):\n        unchosen_idx = np.nonzero(~chosen_mask)[0]\n        unchosen_items = points.compress(~chosen_mask, axis=0)\n        chosen_items = points.compress(chosen_mask, axis=0)\n        distances = distfunc(unchosen_items, chosen_items.T)\n        min_distances = distances.min(axis=1)\n        argx = min_distances.argmax()\n        if min_thresh is not None:\n            if min_distances[argx] < min_thresh:\n                break\n        new_idx = unchosen_idx[argx]\n        chosen_mask[new_idx] = True\n\n    # Put chosen mask back in the input order of items\n    chosen_items_mask = chosen_mask.take(initial_sortx.argsort())\n    chosen_items_idxs = np.nonzero(chosen_items_mask)[0]\n    chosen_items = ut.take(items, chosen_items_idxs)\n    #current_idx = np.nonzero(chosen_mask)[0]\n    if verbose:\n        print('Chose subset')\n        chosen_points = points.compress(chosen_mask, axis=0)\n        distances = (spdist.pdist(chosen_points, distfunc))\n        print('chosen_items_idxs = %r' % (chosen_items_idxs,))\n        print('chosen_items = %r' % (chosen_items,))\n        print('distances = %r' % (distances,))\n    return chosen_items_idxs, chosen_items", "response": "r Return the maximum distance between items in a set of items in a set of items."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maximum_distance_subset(items, K, verbose=False):\n    from utool import util_decor\n    if verbose:\n        print('maximum_distance_subset len(items)=%r, K=%r' % (len(items), K,))\n\n    points = np.array(items)[:, None]\n\n    if False:\n        # alternative definition (not sure if works)\n        distmat = spdist.squareform(spdist.pdist(points, lambda x, y: np.abs(x - y)))\n        D = np.triu(distmat)\n        remaining_idxs = np.arange(len(D))\n        for count in range(len(points) - K):\n            values = D.sum(axis=1) + D.sum(axis=0)\n            remove_idx = values.argmin()  # index with minimum pairwise distance\n            remaining_idxs = np.delete(remaining_idxs, remove_idx)\n            D = np.delete(np.delete(D, remove_idx, axis=0), remove_idx, axis=1)\n        value = D.sum()\n        subset_idx = remaining_idxs.tolist()\n        value, subset_idx\n        subset = points.take(subset_idx)\n        #print((value, subset_idx, subset))\n\n    sortx = points.T[0].argsort()[::-1]\n    sorted_points = points.take(sortx, axis=0)\n    pairwise_distance = spdist.pdist(sorted_points, lambda x, y: np.abs(x - y))\n    distmat = (spdist.squareform(pairwise_distance))\n\n    def condensed_idx(i, j):\n        if i >= len(sorted_points) or j >= len(sorted_points):\n            raise IndexError('i=%r j=%r out of range' % (i, j))\n        elif i == j:\n            return None\n        elif j < i:\n            i, j = j, i\n        return (i * len(sorted_points) + j) - (i * (i + 1) // 2) - (i) - (1)\n\n    def dist(i, j):\n        idx = condensed_idx(i, j)\n        return 0 if idx is None else pairwise_distance[idx]\n\n    @util_decor.memoize_nonzero\n    def optimal_solution(n, k):\n        \"\"\"\n        Givem sorted items sorted_points\n        Pick subset_idx of size k from sorted_points[:n] with maximum pairwise distance\n        Dynamic programming solution\n        \"\"\"\n        \"# FIXME BROKEN \"\n        assert n <= len(sorted_points) and k <= len(sorted_points)\n        if k < 2 or n < 2 or n < k:\n            # BASE CASE\n            value, subset_idx =  0, []\n        elif k == 2:\n            # BASE CASE\n            # when k==2 we choose the maximum pairwise pair\n            subdist = np.triu(distmat[0:n, 0:n])\n            maxpos = subdist.argmax()\n            ix, jx = np.unravel_index(maxpos, subdist.shape)\n            value = distmat[ix, jx]\n            subset_idx = [ix, jx]\n        else:\n            # RECURSIVE CASE\n            value = 0\n            subset_idx = None\n            # MAX OVER ALL OTHER NODES (might not need a full on loop here, but this will definitely work)\n            for m in range(k - 1, n):\n                # Choose which point to add would maximize the distance with the previous best answer.\n                prev_value, prev_subset_idx = optimal_solution(m, k - 1)\n                for o in range(n):\n                    if o in prev_subset_idx:\n                        continue\n                    add_value = sum([distmat[o, px] for px in prev_subset_idx])\n                    cur_value = prev_value + add_value\n                    if cur_value > value:\n                        value = cur_value\n                        subset_idx = prev_subset_idx + [o]\n        return value, subset_idx\n\n    value, sorted_subset_idx = optimal_solution(len(points), K)\n    subset_idx = sortx.take(sorted_subset_idx)\n    subset = points.take(subset_idx)\n    #print((value, subset_idx, subset))\n    return value, subset_idx, subset", "response": "Returns a subset of items with the maximum pairwise distance matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inbounds(num, low, high, eq=False):\n    less    = op.le if eq else op.lt\n    greater = op.ge if eq else op.gt\n    and_ = np.logical_and if isinstance(num, np.ndarray) else op.and_\n    is_inbounds = and_(greater(num, low), less(num, high))\n    return is_inbounds", "response": "r Tests if a single entry in the cache is in bounds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if floating point numbers are equal to a threshold", "response": "def almost_eq(arr1, arr2, thresh=1E-11, ret_error=False):\n    \"\"\" checks if floating point number are equal to a threshold\n    \"\"\"\n    error = np.abs(arr1 - arr2)\n    passed = error < thresh\n    if ret_error:\n        return passed, error\n    return passed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsafe version of pdist", "response": "def safe_pdist(arr, *args, **kwargs):\n    \"\"\"\n    Kwargs:\n        metric = ut.absdiff\n    SeeAlso:\n        scipy.spatial.distance.pdist\n\n    TODO: move to vtool\n\n    \"\"\"\n    if arr is None or len(arr) < 2:\n        return None\n    else:\n        import vtool as vt\n        arr_ = vt.atleast_nd(arr, 2)\n        return spdist.pdist(arr_, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nnormalize a numpy array from 0 to 1 based in its extent Args: array (ndarray): dim (int): Returns: ndarray: CommandLine: python -m utool.util_alg --test-norm_zero_one Example: >>> # ENABLE_DOCTEST >>> from utool.util_alg import * # NOQA >>> import utool as ut >>> array = np.array([ 22, 1, 3, 2, 10, 42, ]) >>> dim = None >>> array_norm = norm_zero_one(array, dim) >>> result = ut.repr2(list(array_norm), precision=3) >>> print(result) [0.512, 0.000, 0.049, 0.024, 0.220, 1.000]", "response": "def norm_zero_one(array, dim=None):\n    \"\"\"\n    normalizes a numpy array from 0 to 1 based in its extent\n\n    Args:\n        array (ndarray):\n        dim   (int):\n\n    Returns:\n        ndarray:\n\n    CommandLine:\n        python -m utool.util_alg --test-norm_zero_one\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> import utool as ut\n        >>> array = np.array([ 22, 1, 3, 2, 10, 42, ])\n        >>> dim = None\n        >>> array_norm = norm_zero_one(array, dim)\n        >>> result = ut.repr2(list(array_norm), precision=3)\n        >>> print(result)\n        [0.512, 0.000, 0.049, 0.024, 0.220, 1.000]\n    \"\"\"\n    if not util_type.is_float(array):\n        array = array.astype(np.float32)\n    array_max  = array.max(dim)\n    array_min  = array.min(dim)\n    array_exnt = np.subtract(array_max, array_min)\n    array_norm = np.divide(np.subtract(array, array_min), array_exnt)\n    return array_norm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef max_size_max_distance_subset(items, min_thresh=0, Kstart=2, verbose=False):\n    import utool as ut\n    assert Kstart >= 2, 'must start with group of size 2'\n    best_idxs = []\n    for K in range(Kstart, len(items)):\n        if verbose:\n            print('Running subset chooser')\n        value, subset_idx, subset = ut.maximum_distance_subset(items, K=K,\n                                                               verbose=verbose)\n        if verbose:\n            print('subset = %r' % (subset,))\n            print('subset_idx = %r' % (subset_idx,))\n            print('value = %r' % (value,))\n        distances = ut.safe_pdist(subset[:, None])\n        if np.any(distances < min_thresh):\n            break\n        best_idxs = subset_idx\n    return best_idxs", "response": "r Returns the index of the best entry in the items list that is within a given threshold"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef group_indices(groupid_list):\n    item_list = range(len(groupid_list))\n    grouped_dict = util_dict.group_items(item_list, groupid_list)\n    # Sort by groupid for cache efficiency\n    keys_ = list(grouped_dict.keys())\n    try:\n        keys = sorted(keys_)\n    except TypeError:\n        # Python 3 does not allow sorting mixed types\n        keys = util_list.sortedby2(keys_, keys_)\n    groupxs = util_dict.dict_take(grouped_dict, keys)\n    return keys, groupxs", "response": "Returns a list of tuples with the keys and groupxs of each item in groupid_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ungroup(grouped_items, groupxs, maxval=None, fill=None):\n    if maxval is None:\n        # Determine the number of items if unknown\n        maxpergroup = [max(xs) if len(xs) else 0 for xs in groupxs]\n        maxval = max(maxpergroup) if len(maxpergroup) else 0\n    # Allocate an array containing the newly flattened items\n    ungrouped_items = [fill] * (maxval + 1)\n    # Populate the array\n    for itemgroup, xs in zip(grouped_items, groupxs):\n        for item, x in zip(itemgroup, xs):\n            ungrouped_items[x] = item\n    return ungrouped_items", "response": "Ungroups items in a list of items by the given groupxs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ungroup_gen(grouped_items, groupxs, fill=None):\n    import utool as ut\n    # Determine the number of items if unknown\n    #maxpergroup = [max(xs) if len(xs) else 0 for xs in groupxs]\n    #maxval = max(maxpergroup) if len(maxpergroup) else 0\n\n    minpergroup = [min(xs) if len(xs) else 0 for xs in groupxs]\n    minval = min(minpergroup) if len(minpergroup) else 0\n\n    flat_groupx = ut.flatten(groupxs)\n    sortx = ut.argsort(flat_groupx)\n    # Indicates the index being yeilded\n    groupx_sorted = ut.take(flat_groupx, sortx)\n    flat_items = ut.iflatten(grouped_items)\n\n    # Storage for data weiting to be yeilded\n    toyeild = {}\n    items_yeilded = 0\n    # Indicates the index we are curently yeilding\n    current_index = 0\n\n    # Determine where fills need to happen\n    num_fills_before = [minval] + (np.diff(groupx_sorted) - 1).tolist() + [0]\n\n    # Check if there are fills before the first item\n    fills = num_fills_before[items_yeilded]\n    if fills > 0:\n        for _ in range(fills):\n            yield None\n            current_index += 1\n    # Yield items as possible\n    for yeild_at, item in zip(flat_groupx, flat_items):\n        if yeild_at > current_index:\n            toyeild[yeild_at] = item\n        elif yeild_at == current_index:\n            # When we find the next element to yeild\n            yield item\n            current_index += 1\n            items_yeilded += 1\n            # Check if there are fills before the next item\n            fills = num_fills_before[items_yeilded]\n            if fills > 0:\n                for _ in range(fills):\n                    yield None\n                    current_index += 1\n            # Now yield everything that came before this\n            while current_index in toyeild:\n                item = toyeild.pop(current_index)\n                yield item\n                current_index += 1\n                items_yeilded += 1\n                # Check if there are fills before the next item\n                fills = num_fills_before[items_yeilded]\n                if fills > 0:\n                    for _ in range(fills):\n                        yield None\n                        current_index += 1", "response": "Ungroups items returning a generator."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ungroup_unique(unique_items, groupxs, maxval=None):\n    if maxval is None:\n        maxpergroup = [max(xs) if len(xs) else 0 for xs in groupxs]\n        maxval = max(maxpergroup) if len(maxpergroup) else 0\n    ungrouped_items = [None] * (maxval + 1)\n    for item, xs in zip(unique_items, groupxs):\n        for x in xs:\n            ungrouped_items[x] = item\n    return ungrouped_items", "response": "Ungroups unique items to correspond to original non - unique list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit distance algorithm. String1 and string2 can be either strings or lists of strings pip install python-Levenshtein Args: string1 (str or list): string2 (str or list): CommandLine: python -m utool.util_alg edit_distance --show Example: >>> # DISABLE_DOCTEST >>> from utool.util_alg import * # NOQA >>> import utool as ut >>> string1 = 'hello world' >>> string2 = ['goodbye world', 'rofl', 'hello', 'world', 'lowo'] >>> edit_distance(['hello', 'one'], ['goodbye', 'two']) >>> edit_distance('hello', ['goodbye', 'two']) >>> edit_distance(['hello', 'one'], 'goodbye') >>> edit_distance('hello', 'goodbye') >>> distmat = edit_distance(string1, string2) >>> result = ('distmat = %s' % (ut.repr2(distmat),)) >>> print(result) >>> [7, 9, 6, 6, 7]", "response": "def edit_distance(string1, string2):\n    \"\"\"\n    Edit distance algorithm. String1 and string2 can be either\n    strings or lists of strings\n\n    pip install python-Levenshtein\n\n    Args:\n        string1 (str or list):\n        string2 (str or list):\n\n    CommandLine:\n        python -m utool.util_alg edit_distance --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> import utool as ut\n        >>> string1 = 'hello world'\n        >>> string2 = ['goodbye world', 'rofl', 'hello', 'world', 'lowo']\n        >>> edit_distance(['hello', 'one'], ['goodbye', 'two'])\n        >>> edit_distance('hello', ['goodbye', 'two'])\n        >>> edit_distance(['hello', 'one'], 'goodbye')\n        >>> edit_distance('hello', 'goodbye')\n        >>> distmat = edit_distance(string1, string2)\n        >>> result = ('distmat = %s' % (ut.repr2(distmat),))\n        >>> print(result)\n        >>> [7, 9, 6, 6, 7]\n    \"\"\"\n\n    import utool as ut\n    try:\n        import Levenshtein\n    except ImportError as ex:\n        ut.printex(ex, 'pip install python-Levenshtein')\n        raise\n    #np.vectorize(Levenshtein.distance, [np.int])\n    #vec_lev = np.frompyfunc(Levenshtein.distance, 2, 1)\n    #return vec_lev(string1, string2)\n    import utool as ut\n    isiter1 = ut.isiterable(string1)\n    isiter2 = ut.isiterable(string2)\n    strs1 = string1 if isiter1 else [string1]\n    strs2 = string2 if isiter2 else [string2]\n    distmat = [\n        [Levenshtein.distance(str1, str2) for str2 in strs2]\n        for str1 in strs1\n    ]\n    # broadcast\n    if not isiter2:\n        distmat = ut.take_column(distmat, 0)\n    if not isiter1:\n        distmat = distmat[0]\n    return distmat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the nth Bell number using recursion.", "response": "def get_nth_bell_number(n):\n    \"\"\"\n    Returns the (num_items - 1)-th Bell number using recursion.\n    The Bell numbers count the number of partitions of a set.\n\n    Args:\n        n (int): number of items in a set\n\n    Returns:\n        int:\n\n    References:\n        http://adorio-research.org/wordpress/?p=11460\n\n    CommandLine:\n        python -m utool.util_alg --exec-bell --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> n = 3\n        >>> result = get_nth_bell_number(n)\n        >>> print(result)\n        5\n    \"\"\"\n    import utool as ut\n    import scipy.special\n    @ut.memoize\n    def bell_(n):\n        if n < 2:\n            return 1\n        sum_ = 0\n        for k in range(1, n + 1):\n            sum_ = sum_ + scipy.special.binom(n - 1, k - 1) * bell_(k - 1)\n        return sum_\n    nth_bell = bell_(n)\n    return nth_bell"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef standardize_boolexpr(boolexpr_, parens=False):\n    import utool as ut\n    import re\n    onlyvars = boolexpr_\n    onlyvars = re.sub('\\\\bnot\\\\b', '', onlyvars)\n    onlyvars = re.sub('\\\\band\\\\b', '', onlyvars)\n    onlyvars = re.sub('\\\\bor\\\\b', '', onlyvars)\n    onlyvars = re.sub('\\\\(', '', onlyvars)\n    onlyvars = re.sub('\\\\)', '', onlyvars)\n    varnames = ut.remove_doublspaces(onlyvars).strip().split(' ')\n    varied_dict = {var: [True, False] for var in varnames}\n    bool_states = ut.all_dict_combinations(varied_dict)\n    outputs = [eval(boolexpr_, state.copy(), state.copy()) for state in bool_states]\n    true_states = ut.compress(bool_states, outputs)\n    true_tuples = ut.take_column(true_states, varnames)\n    true_cases = [str(''.join([str(int(t)) for t in tup])) for tup in true_tuples]\n\n    # Convert to binary\n    ones_bin = [int(x, 2) for x in true_cases]\n    #ones_str = [str(x) for x in true_cases]\n    from quine_mccluskey.qm import QuineMcCluskey\n    qm = QuineMcCluskey()\n    result = qm.simplify(ones=ones_bin, num_bits=len(varnames))\n    #result = qm.simplify_los(ones=ones_str, num_bits=len(varnames))\n\n    grouped_terms = [dict(ut.group_items(varnames, rs)) for rs in result]\n    def parenjoin(char, list_):\n        if len(list_) == 0:\n            return ''\n        else:\n            if parens:\n                return '(' + char.join(list_) + ')'\n            else:\n                return char.join(list_)\n\n    if parens:\n        expanded_terms = [\n            (\n                term.get('1', []) +\n                ['(not ' + b + ')' for b in term.get('0', [])] +\n                [\n                    parenjoin(' ^ ', term.get('^', [])),\n                    parenjoin(' ~ ', term.get('~', [])),\n                ]\n            ) for term in grouped_terms\n        ]\n    else:\n        expanded_terms = [\n            (\n                term.get('1', []) +\n                ['not ' + b  for b in term.get('0', [])] +\n                [\n                    parenjoin(' ^ ', term.get('^', [])),\n                    parenjoin(' ~ ', term.get('~', [])),\n                ]\n            ) for term in grouped_terms\n        ]\n\n    final_terms = [[t for t in term if t] for term in expanded_terms]\n\n    products = [parenjoin(' and ', [f for f in form if f]) for form in final_terms]\n    final_expr = ' or '.join(products)\n    return final_expr", "response": "r Standardizes a boolean expression into an or - ing of and - ed variables"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsolve the non - empty tree of the current node and return the tree of the tree of the tree that is not empty", "response": "def solve_boolexpr():\n    \"\"\"\n    sudo pip install git+https://github.com/tpircher/quine-mccluskey.git\n    sudo pip uninstall quine_mccluskey\n    pip uninstall quine_mccluskey\n\n    pip install git+https://github.com/tpircher/quine-mccluskey.git\n\n\n    Args:\n        varnames (?):\n\n    Returns:\n        ?:\n\n    CommandLine:\n        python -m utool.util_alg solve_boolexpr --show\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> import utool as ut\n        >>> varnames = ['sa', 'said', 'aid']\n        >>> result = solve_boolexpr()\n        >>> print(result)\n\n    \"\"\"\n    #false_cases = [\n    #    int('111', 2),\n    #    int('011', 2),\n    #    int('001', 2),\n    #]\n    #true_cases = list(set(range(2 ** 3)) - set(false_cases))\n    varnames = ['sa', 'said', 'aid']\n\n    #import utool as ut\n    truth_table = [\n        dict(sa=True,  said=True,  aid=True,  output=False),\n        dict(sa=True,  said=True,  aid=False, output=True),\n        dict(sa=True,  said=False, aid=True,  output=True),\n        dict(sa=True,  said=False, aid=False, output=True),\n        dict(sa=False, said=True,  aid=True,  output=False),\n        dict(sa=False, said=True,  aid=False, output=True),\n        dict(sa=False, said=False, aid=True,  output=False),\n        dict(sa=False, said=False, aid=False, output=True),\n    ]\n    truth_tuples = [ut.dict_take(d, varnames) for d in truth_table]\n    outputs = [d['output'] for d in truth_table]\n    true_tuples = ut.compress(truth_tuples, outputs)\n    #false_tuples = ut.compress(truth_tuples, ut.not_list(outputs))\n    true_cases = [''.join([str(int(t)) for t in tup]) for tup in true_tuples]\n    true_cases = [''.join([str(int(t)) for t in tup]) for tup in true_tuples]\n    #truth_nums = [int(s, 2) for s in true_cases]\n\n    from quine_mccluskey.qm import QuineMcCluskey\n    qm = QuineMcCluskey(use_xor=False)\n    result = qm.simplify_los(true_cases, num_bits=len(varnames))\n    print(result)\n    #ut.chr_range(3)\n\n    #symbol_map = {\n    #    '-': '',\n    #    '1': '{v}',\n    #    '0': 'not {v}',\n    #    '^': '^',\n    #}\n\n    #'-' don't care: this bit can be either zero or one.\n    #'1' the bit must be one.\n    #'0' the bit must be zero.\n    #'^' all bits with the caret are XOR-ed together.\n    #'~' all bits with the tilde are XNOR-ed together.\n\n    #formulas = [[symbol_map[r].format(v=v) for v, r in zip(varnames, rs)] for rs in result]\n    grouped_terms = [dict(ut.group_items(varnames, rs)) for rs in result]\n    def parenjoin(char, list_):\n        if len(list_) == 0:\n            return ''\n        else:\n            return '(' + char.join(list_) + ')'\n\n    expanded_terms = [\n        (\n            term.get('1', []) +\n            ['(not ' + b + ')' for b in term.get('0', [])] +\n            [\n                parenjoin(' ^ ', term.get('^', [])),\n                parenjoin(' ~ ', term.get('~', [])),\n            ]\n        ) for term in grouped_terms\n    ]\n\n    final_terms = [[t for t in term if t] for term in expanded_terms]\n\n    products = [parenjoin(' and ', [f for f in form if f]) for form in final_terms]\n    final_expr = ' or '.join(products)\n    print(final_expr)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the longest common substring of two strings.", "response": "def longest_common_substring(s1, s2):\n    \"\"\"\n    References:\n        # https://en.wikibooks.org/wiki/Algorithm_Implementation/Strings/Longest_common_substring#Python2\n    \"\"\"\n    m = [[0] * (1 + len(s2)) for i in range(1 + len(s1))]\n    longest, x_longest = 0, 0\n    for x in range(1, 1 + len(s1)):\n        for y in range(1, 1 + len(s2)):\n            if s1[x - 1] == s2[y - 1]:\n                m[x][y] = m[x - 1][y - 1] + 1\n                if m[x][y] > longest:\n                    longest = m[x][y]\n                    x_longest = x\n            else:\n                m[x][y] = 0\n    return s1[x_longest - longest: x_longest]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expensive_task_gen(num=8700):\n    import utool as ut\n    #time_list = []\n    for x in range(0, num):\n        with ut.Timer(verbose=False) as t:\n            ut.is_prime(x)\n        yield t.ellapsed", "response": "r A function that takes some time and returns a new set of expensive task terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes all the integer factors of the number n", "response": "def factors(n):\n    \"\"\"\n    Computes all the integer factors of the number `n`\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_alg import *  # NOQA\n        >>> import utool as ut\n        >>> result = sorted(ut.factors(10))\n        >>> print(result)\n        [1, 2, 5, 10]\n\n    References:\n        http://stackoverflow.com/questions/6800193/finding-all-the-factors\n    \"\"\"\n    return set(reduce(list.__add__,\n                      ([i, n // i] for i in range(1, int(n ** 0.5) + 1) if n % i == 0)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_protein_data(proteins, pgdb, headerfields, genecentric=False,\n                     pool_to_output=False):\n    \"\"\"First creates a map with all master proteins with data,\n    then outputs protein data dicts for rows of a tsv. If a pool\n    is given then only output for that pool will be shown in the\n    protein table.\"\"\"\n    proteindata = create_featuredata_map(pgdb, genecentric=genecentric,\n                                         psm_fill_fun=add_psms_to_proteindata,\n                                         pgene_fill_fun=add_protgene_to_protdata,\n                                         count_fun=count_peps_psms,\n                                         pool_to_output=pool_to_output,\n                                         get_uniques=True)\n    dataget_fun = {True: get_protein_data_genecentric,\n                   False: get_protein_data_pgrouped}[genecentric is not False]\n    firstfield = prottabledata.ACCESSIONS[genecentric]\n    for protein in proteins:\n        outprotein = {k: v for k, v in protein.items()}\n        outprotein[firstfield] = outprotein.pop(prottabledata.HEADER_PROTEIN)\n        protein_acc = protein[prottabledata.HEADER_PROTEIN]\n        outprotein.update(dataget_fun(proteindata, protein_acc, headerfields))\n        outprotein = {k: str(v) for k, v in outprotein.items()}\n        yield outprotein", "response": "Create a map with all master proteins with data and outputs protein data dicts for rows of a tsv."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_protein_data_pgrouped(proteindata, p_acc, headerfields):\n    report = get_protein_data_base(proteindata, p_acc, headerfields)\n    return get_cov_protnumbers(proteindata, p_acc, report)", "response": "Parses protein data for a certain protein into tsv output\n    dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(cls, return_results=False):\n        cls.result = []\n\n        passed = True\n\n        for field in cls.fields:\n            result, errors = field.run()\n\n            results = {\n                'field': field.name,\n                'value': field.value,\n                'passed': result,\n                'errors': None\n            }\n\n            if errors:\n                passed = False\n                results['errors'] = errors\n\n            cls.result.append(results)\n\n        if return_results:\n            return cls.result\n\n        return passed", "response": "Runs all associated Fields and applies all attached Rules. Returns a dictionary list containing the collated results of all attached Fields."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keys(self, namespace, prefix=None, limit=None, offset=None):\n        params = [namespace]\n        query = 'SELECT key FROM gauged_keys WHERE namespace = %s'\n        if prefix is not None:\n            query += ' AND key LIKE %s'\n            params.append(prefix + '%')\n        if limit is not None:\n            query += ' LIMIT %s'\n            params.append(limit)\n        if offset is not None:\n            query += ' OFFSET %s'\n            params.append(offset)\n        cursor = self.cursor\n        cursor.execute(query, params)\n        return [key for key, in cursor]", "response": "Get keys from a namespace"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lookup_ids(self, keys):\n        keys_len = len(keys)\n        ids = {namespace_key: None for namespace_key in keys}\n        start = 0\n        bulk_insert = self.bulk_insert\n        query = 'SELECT namespace, key, id FROM gauged_keys WHERE '\n        check = '(namespace = %s AND key = %s) '\n        cursor = self.cursor\n        execute = cursor.execute\n        while start < keys_len:\n            rows = keys[start:start+bulk_insert]\n            params = [param for params in rows for param in params]\n            id_query = query + (check + ' OR ') * (len(rows) - 1) + check\n            execute(id_query, params)\n            for namespace, key, id_ in cursor:\n                ids[(namespace, key)] = id_\n            start += bulk_insert\n        return ids", "response": "Lookup the integer ID associated with each namespace key in the the\n        keys list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the block identified by namespace offset key and value", "response": "def get_block(self, namespace, offset, key):\n        \"\"\"Get the block identified by namespace, offset, key and\n        value\"\"\"\n        cursor = self.cursor\n        cursor.execute('SELECT data, flags FROM gauged_data '\n                       'WHERE namespace = %s AND \"offset\" = %s AND key = %s',\n                       (namespace, offset, key))\n        row = cursor.fetchone()\n        return (None, None) if row is None else row"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninserts multiple blocks. If a block already exists the data is appended.", "response": "def insert_or_append_blocks(self, blocks):\n        \"\"\"Insert multiple blocks. If a block already exists, the data is\n        appended. blocks must be a list of tuples where each tuple consists\n        of (namespace, offset, key, data)\"\"\"\n        binary = self.psycopg2.Binary\n        execute = self.cursor.execute\n        query = 'UPDATE gauged_data SET data = data || %s, flags = %s ' \\\n            'WHERE namespace = %s AND \"offset\" = %s AND key = %s; ' \\\n            'INSERT INTO gauged_data (data, flags, namespace, \"offset\", key)' \\\n            'SELECT %s, %s, %s, %s, %s WHERE NOT EXISTS (' \\\n            'SELECT 1 FROM gauged_data WHERE namespace = %s ' \\\n            'AND \"offset\" = %s AND key = %s)'\n        for namespace, offset, key, data, flags in blocks:\n            data = binary(data)\n            execute(query, (data, flags, namespace, offset, key, data, flags,\n                            namespace, offset, key, namespace, offset, key))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef block_offset_bounds(self, namespace):\n        cursor = self.cursor\n        cursor.execute('SELECT MIN(\"offset\"), MAX(\"offset\") '\n                       'FROM gauged_statistics WHERE namespace = %s',\n                       (namespace,))\n        return cursor.fetchone()", "response": "Get the minimum and maximum block offset for the specified\n        namespace"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts a timestamp to keep track of the current writer position", "response": "def set_writer_position(self, name, timestamp):\n        \"\"\"Insert a timestamp to keep track of the current writer position\"\"\"\n        execute = self.cursor.execute\n        execute('DELETE FROM gauged_writer_history WHERE id = %s', (name,))\n        execute('INSERT INTO gauged_writer_history (id, timestamp) '\n                'VALUES (%s, %s)', (name, timestamp,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_cache(self, namespace, key, query_hash, length, cache):\n        start = 0\n        bulk_insert = self.bulk_insert\n        cache_len = len(cache)\n        row = '(%s,%s,%s,%s,%s,%s)'\n        query = 'INSERT INTO gauged_cache ' \\\n            '(namespace, key, \"hash\", length, start, value) VALUES '\n        execute = self.cursor.execute\n        query_hash = self.psycopg2.Binary(query_hash)\n        while start < cache_len:\n            rows = cache[start:start+bulk_insert]\n            params = []\n            for timestamp, value in rows:\n                params.extend((namespace, key, query_hash, length,\n                               timestamp, value))\n            insert = (row + ',') * (len(rows) - 1) + row\n            execute(query + insert, params)\n            start += bulk_insert\n        self.db.commit()", "response": "Add cached values for the specified date range and query"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_namespace_statistics(self, namespace, offset, data_points,\n                                 byte_count):\n        \"\"\"Update namespace statistics for the period identified by\n        offset\"\"\"\n        query = 'UPDATE gauged_statistics ' \\\n            'SET data_points = data_points + %s,' \\\n            'byte_count = byte_count + %s WHERE namespace = %s ' \\\n            'AND \"offset\" = %s; INSERT INTO gauged_statistics ' \\\n            'SELECT %s, %s, %s, %s WHERE NOT EXISTS (' \\\n            'SELECT 1 FROM gauged_statistics WHERE namespace = %s' \\\n            'AND \"offset\" = %s)'\n        self.cursor.execute(query, (data_points, byte_count, namespace,\n                                    offset, namespace, offset, data_points,\n                                    byte_count, namespace, offset))", "response": "Update the namespace statistics for the period identified by namespace and offset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_schema(self):\n        execute = self.cursor.execute\n        try:\n            return execute('SELECT 1 FROM gauged_statistics')\n        except self.psycopg2.ProgrammingError:\n            pass\n        self.db.rollback()\n        execute(\"\"\"CREATE TABLE IF NOT EXISTS gauged_data (\n                namespace integer NOT NULL,\n                \"offset\" integer NOT NULL,\n                key bigint NOT NULL,\n                data bytea NOT NULL,\n                flags integer NOT NULL,\n                PRIMARY KEY (\"offset\", namespace, key));\n            CREATE TABLE IF NOT EXISTS gauged_keys (\n                id serial PRIMARY KEY,\n                namespace integer NOT NULL,\n                key varchar NOT NULL);\n            CREATE UNIQUE INDEX ON gauged_keys (\n                namespace, key);\n            CREATE OR REPLACE RULE gauged_ignore_duplicate_keys\n                AS ON INSERT TO gauged_keys WHERE EXISTS (\n                SELECT 1 FROM gauged_keys WHERE key = NEW.key\n                    AND namespace = NEW.namespace)\n                DO INSTEAD NOTHING;\n            CREATE TABLE IF NOT EXISTS gauged_writer_history (\n                id varchar PRIMARY KEY,\n                timestamp bigint NOT NULL);\n            CREATE TABLE IF NOT EXISTS gauged_cache (\n                namespace integer NOT NULL,\n                key bigint NOT NULL,\n                \"hash\" bytea NOT NULL,\n                length bigint NOT NULL,\n                start bigint NOT NULL,\n                value real,\n                PRIMARY KEY(namespace, hash, length, start));\n            CREATE OR REPLACE RULE gauged_ignore_duplicate_cache\n                AS ON INSERT TO gauged_cache WHERE EXISTS (\n                SELECT 1 FROM gauged_cache WHERE namespace = NEW.namespace AND\n                \"hash\" = NEW.hash AND length = NEW.length\n                AND start = NEW.start)\n                DO INSTEAD NOTHING;\n            CREATE TABLE IF NOT EXISTS gauged_statistics (\n                namespace integer NOT NULL,\n                \"offset\" integer NOT NULL,\n                data_points integer NOT NULL,\n                byte_count integer NOT NULL,\n                PRIMARY KEY (namespace, \"offset\"));\n            CREATE TABLE IF NOT EXISTS gauged_metadata (\n                key varchar PRIMARY KEY,\n                value varchar NOT NULL);\n            CREATE OR REPLACE RULE gauged_ignore_duplicate_metadata\n                AS ON INSERT TO gauged_metadata WHERE EXISTS (\n                SELECT 1 FROM gauged_metadata WHERE key = NEW.key)\n                DO INSTEAD NOTHING\"\"\")\n        self.db.commit()", "response": "Create all necessary tables for the given object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef drop_schema(self):\n        try:\n            self.cursor.execute(\"\"\"\n                DROP TABLE IF EXISTS gauged_data;\n                DROP TABLE IF EXISTS gauged_keys;\n                DROP TABLE IF EXISTS gauged_writer_history;\n                DROP TABLE IF EXISTS gauged_cache;\n                DROP TABLE IF EXISTS gauged_statistics;\n                DROP TABLE IF EXISTS gauged_metadata\"\"\")\n            self.db.commit()\n        except self.psycopg2.InternalError:  # pragma: no cover\n            self.db.rollback()", "response": "Drop all gauged tables and all tables in the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_environment_vars(filename):\n    if sys.platform == \"linux\" or sys.platform == \"linux2\":\n        return {\n            'LD_PRELOAD': path.join(LIBFAKETIME_DIR, \"libfaketime.so.1\"),\n            'FAKETIME_SKIP_CMDS': 'nodejs',     # node doesn't seem to work in the current version.\n            'FAKETIME_TIMESTAMP_FILE': filename,\n        }\n    elif sys.platform == \"darwin\":\n        return {\n            'DYLD_INSERT_LIBRARIES': path.join(LIBFAKETIME_DIR, \"libfaketime.1.dylib\"),\n            'DYLD_FORCE_FLAT_NAMESPACE': '1',\n            'FAKETIME_TIMESTAMP_FILE': filename,\n        }\n    else:\n        raise RuntimeError(\"libfaketime does not support '{}' platform\".format(sys.platform))", "response": "Return a dict of environment variables required to run a service under faketime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the time of a process or group of processes by writing a new time to the time file.", "response": "def change_time(filename, newtime):\n    \"\"\"Change the time of a process or group of processes by writing a new time to the time file.\"\"\"\n    with open(filename, \"w\") as faketimetxt_handle:\n        faketimetxt_handle.write(\"@\" + newtime.strftime(\"%Y-%m-%d %H:%M:%S\"))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield peptides from generator as long as their sequence is not found in the known search space dict. Useful for excluding peptides that are not found in the known search space.", "response": "def filter_known_searchspace(elements, seqtype, lookup, ns, ntermwildcards,\n                             deamidation):\n    \"\"\"Yields peptides from generator as long as their sequence is not found in\n    known search space dict. Useful for excluding peptides that are found in\n    e.g. ENSEMBL or similar\"\"\"\n    for element in elements:\n        seq_is_known = False\n        for seq in get_seqs_from_element(element, seqtype, ns, deamidation):\n            if lookup.check_seq_exists(seq, ntermwildcards):\n                seq_is_known = True\n                break\n        if seq_is_known:\n            formatting.clear_el(element)\n        else:\n            yield formatting.string_and_clear(element, ns)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering unique peptides from multiple Percolator output XML files.", "response": "def filter_unique_peptides(peptides, score, ns):\n    \"\"\" Filters unique peptides from multiple Percolator output XML files.\n        Takes a dir with a set of XMLs, a score to filter on and a namespace.\n        Outputs an ElementTree.\n    \"\"\"\n    scores = {'q': 'q_value',\n              'pep': 'pep',\n              'p': 'p_value',\n              'svm': 'svm_score'}\n    highest = {}\n    for el in peptides:\n        featscore = float(el.xpath('xmlns:%s' % scores[score],\n                                   namespaces=ns)[0].text)\n        seq = reader.get_peptide_seq(el, ns)\n\n        if seq not in highest:\n            highest[seq] = {\n                'pep_el': formatting.stringify_strip_namespace_declaration(\n                    el, ns), 'score': featscore}\n        if score == 'svm':  # greater than score is accepted\n            if featscore > highest[seq]['score']:\n                highest[seq] = {\n                    'pep_el':\n                    formatting.stringify_strip_namespace_declaration(el, ns),\n                    'score': featscore}\n        else:  # lower than score is accepted\n            if featscore < highest[seq]['score']:\n                highest[seq] = {\n                    'pep_el':\n                    formatting.stringify_strip_namespace_declaration(el, ns),\n                    'score': featscore}\n        formatting.clear_el(el)\n\n    for pep in list(highest.values()):\n        yield pep['pep_el']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimporting a module or a typename within a module from its name.", "response": "def import_symbol(name=None, path=None, typename=None, base_path=None):\n    \"\"\"\n    Import a module, or a typename within a module from its name.\n\n    Arguments:\n\n    name: An absolute or relative (starts with a .) Python path\n    path: If name is relative, path is prepended to it.\n    base_path: (DEPRECATED) Same as path\n    typename: (DEPRECATED) Same as path\n    \"\"\"\n    _, symbol = _import(name or typename, path or base_path)\n    return symbol"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_stack2(dict_list, key_suffix=None, default=None):\n    if len(dict_list) > 0:\n        dict_list_ = [map_dict_vals(lambda x: [x], kw) for kw in dict_list]\n        # Reduce does not handle default quite correctly\n        default1 = []\n        default2 = [default]\n        accum_ = dict_list_[0]\n        for dict_ in dict_list_[1:]:\n            default1.append(default)\n            accum_ = dict_union_combine(accum_, dict_, default=default1,\n                                        default2=default2)\n        stacked_dict = accum_\n        # stacked_dict = reduce(partial(dict_union_combine, default=[default]), dict_list_)\n    else:\n        stacked_dict = {}\n    # Augment keys if requested\n    if key_suffix is not None:\n        stacked_dict = map_dict_keys(lambda x: x + key_suffix, stacked_dict)\n    return stacked_dict", "response": "This function will stacks vals from a list of dicts into a dict of lists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef invert_dict(dict_, unique_vals=True):\n    if unique_vals:\n        inverted_items = [(val, key) for key, val in six.iteritems(dict_)]\n        inverted_dict = type(dict_)(inverted_items)\n    else:\n        inverted_dict = group_items(dict_.keys(), dict_.values())\n    return inverted_dict", "response": "Invert the keys and values of a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_all_dict_combinations_ordered(varied_dict):\n    tups_list = [[(key, val) for val in val_list]\n                 for (key, val_list) in six.iteritems(varied_dict)]\n    dict_iter = (OrderedDict(tups) for tups in it.product(*tups_list))\n    return dict_iter", "response": "Same as all_dict_combinations but preserves order\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_dict_combinations_lbls(varied_dict, remove_singles=True, allow_lone_singles=False):\n    is_lone_single = all([\n        isinstance(val_list, (list, tuple)) and len(val_list) == 1\n        for key, val_list in iteritems_sorted(varied_dict)\n    ])\n    if not remove_singles or (allow_lone_singles and is_lone_single):\n        # all entries have one length\n        multitups_list = [\n            [(key, val) for val in val_list]\n            for key, val_list in iteritems_sorted(varied_dict)\n        ]\n    else:\n        multitups_list = [\n            [(key, val) for val in val_list]\n            for key, val_list in iteritems_sorted(varied_dict)\n            if isinstance(val_list, (list, tuple)) and len(val_list) > 1]\n    combtup_list = list(it.product(*multitups_list))\n    combtup_list2 = [\n        [(key, val) if isinstance(val, six.string_types) else (key, repr(val))\n         for (key, val) in combtup]\n        for combtup in combtup_list]\n    comb_lbls = [','.join(['%s=%s' % (key, val) for (key, val) in combtup])\n                 for combtup in combtup_list2]\n    #comb_lbls = list(map(str, comb_pairs))\n    return comb_lbls", "response": "This function returns a list of all the varied_dict combinations of the varied_dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_conflict_dict(key_list, val_list):\n    key_to_vals = defaultdict(list)\n    for key, val in zip(key_list, val_list):\n        key_to_vals[key].append(val)\n    return key_to_vals", "response": "Build dict where a list of values is associated with more than one key\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assert_keys_are_subset(dict1, dict2):\n    keys1 = set(dict1.keys())\n    keys2 = set(dict2.keys())\n    unknown_keys = keys2.difference(keys1)\n    assert len(unknown_keys) == 0, 'unknown_keys=%r' % (unknown_keys,)", "response": "Test that the keys of dict1 and dict2 are a subset of those keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_existing(dict1, dict2, copy=False, assert_exists=False,\n                    iswarning=False, alias_dict=None):\n    r\"\"\"\n    updates vals in dict1 using vals from dict2 only if the\n    key is already in dict1.\n\n    Args:\n        dict1 (dict):\n        dict2 (dict):\n        copy (bool): if true modifies dictionary in place (default = False)\n        assert_exists (bool): if True throws error if new key specified (default = False)\n        alias_dict (dict): dictionary of alias keys for dict2 (default = None)\n\n    Returns:\n        dict - updated dictionary\n\n    CommandLine:\n        python -m utool.util_dict --test-update_existing\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dict import *  # NOQA\n        >>> dict1 = {'a': 1, 'b': 2, 'c': 3}\n        >>> dict2 = {'a': 2, 'd': 3}\n        >>> dict1_ = update_existing(dict1, dict2)\n        >>> assert 'd' not in dict1\n        >>> assert dict1['a'] == 2\n        >>> assert dict1_ is dict1\n    \"\"\"\n    if assert_exists:\n        try:\n            assert_keys_are_subset(dict1, dict2)\n        except AssertionError as ex:\n            from utool import util_dbg\n            util_dbg.printex(ex, iswarning=iswarning, N=1)\n            if not iswarning:\n                raise\n    if copy:\n        dict1 = dict(dict1)\n    if alias_dict is None:\n        alias_dict = {}\n    for key, val in six.iteritems(dict2):\n        key = alias_dict.get(key, key)\n        if key in dict1:\n            dict1[key] = val\n    return dict1", "response": "r Updates the values in dict1 using vals from dict2 only if the key is already in dict1."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlike dict. update but does not overwrite items", "response": "def dict_update_newkeys(dict_, dict2):\n    \"\"\" Like dict.update, but does not overwrite items \"\"\"\n    for key, val in six.iteritems(dict2):\n        if key not in dict_:\n            dict_[key] = val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_dicteq(dict1_, dict2_, almosteq_ok=True, verbose_err=True):\n    import utool as ut\n    assert len(dict1_) == len(dict2_), 'dicts are not of same length'\n    try:\n        for (key1, val1), (key2, val2) in zip(dict1_.items(), dict2_.items()):\n            assert key1 == key2, 'key mismatch'\n            assert type(val1) == type(val2), 'vals are not same type'\n            if HAVE_NUMPY and np.iterable(val1):\n                if almosteq_ok and ut.is_float(val1):\n                    assert np.all(ut.almost_eq(val1, val2)), 'float vals are not within thresh'\n                else:\n                    assert all([np.all(x1 == x2) for (x1, x2) in zip(val1, val2)]), 'np vals are different'\n            elif isinstance(val1, dict):\n                is_dicteq(val1, val2, almosteq_ok=almosteq_ok, verbose_err=verbose_err)\n            else:\n                assert val1 == val2, 'vals are different'\n    except AssertionError as ex:\n        if verbose_err:\n            ut.printex(ex)\n        return False\n    return True", "response": "Checks to see if dicts are the same."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_dict_keys(dict_, key_list):\n    invalid_keys = set(key_list) - set(dict_.keys())\n    valid_keys = set(key_list) - invalid_keys\n    for key in valid_keys:\n        del dict_[key]\n    return dict_", "response": "r Delete items from a dictionary inplace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget multiple values from a dictionary", "response": "def dict_take(dict_, keys, *d):\n    \"\"\" get multiple values from a dictionary \"\"\"\n    try:\n        return list(dict_take_gen(dict_, keys, *d))\n    except TypeError:\n        return list(dict_take_gen(dict_, keys, *d))[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dict_take_pop(dict_, keys, *d):\n    if len(d) == 0:\n        return [dict_.pop(key) for key in keys]\n    elif len(d) == 1:\n        default = d[0]\n        return [dict_.pop(key, default) for key in keys]\n    else:\n        raise ValueError('len(d) must be 1 or 0')", "response": "like dict_take but pops values off\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dict_assign(dict_, keys, vals):\n    for key, val in zip(keys, vals):\n        dict_[key] = val", "response": "assigns values to the dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a dict of lists. Returns keys that have vals with no length. Returns keys that have vals with no length. Returns keys that have vals with no length.", "response": "def dict_where_len0(dict_):\n    \"\"\"\n    Accepts a dict of lists. Returns keys that have vals with no length\n    \"\"\"\n    keys = np.array(dict_.keys())\n    flags = np.array(list(map(len, dict_.values()))) == 0\n    indices = np.where(flags)[0]\n    return keys[indices]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dict_column(dict_, colx):\n    retdict_ = {key: util_list.list_take(val, colx)\n                for key, val in six.iteritems(dict_)}\n    return retdict_", "response": "r Returns a dictionary of lists with the values in the given column"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dictinfo(dict_):\n    import utool as ut\n    if not isinstance(dict_, dict):\n        return 'expected dict got %r' % type(dict_)\n\n    keys = list(dict_.keys())\n    vals = list(dict_.values())\n    num_keys  = len(keys)\n    key_types = list(set(map(type, keys)))\n    val_types = list(set(map(type, vals)))\n\n    fmtstr_ = '\\n' + ut.unindent('''\n    * num_keys  = {num_keys}\n    * key_types = {key_types}\n    * val_types = {val_types}\n    '''.strip('\\n'))\n\n    if len(val_types) == 1:\n        if val_types[0] == np.ndarray:\n            # each key holds an ndarray\n            val_shape_stats = ut.get_stats(set(map(np.shape, vals)), axis=0)\n            val_shape_stats_str = ut.repr4(val_shape_stats, strvals=True, newlines=False)\n            val_dtypes = list(set([val.dtype for val in vals]))\n            fmtstr_ += ut.unindent('''\n            * val_shape_stats = {val_shape_stats_str}\n            * val_dtypes = {val_dtypes}\n            '''.strip('\\n'))\n        elif val_types[0] == list:\n            # each key holds a list\n            val_len_stats =  ut.get_stats(set(map(len, vals)))\n            val_len_stats_str = ut.repr4(val_len_stats, strvals=True, newlines=False)\n            depth = ut.list_depth(vals)\n            deep_val_types = list(set(ut.list_deep_types(vals)))\n            fmtstr_ += ut.unindent('''\n            * list_depth = {depth}\n            * val_len_stats = {val_len_stats_str}\n            * deep_types = {deep_val_types}\n            '''.strip('\\n'))\n            if len(deep_val_types) == 1:\n                if deep_val_types[0] == np.ndarray:\n                    deep_val_dtypes = list(set([val.dtype for val in vals]))\n                    fmtstr_ += ut.unindent('''\n                    * deep_val_dtypes = {deep_val_dtypes}\n                    ''').strip('\\n')\n        elif val_types[0] in [np.uint8, np.int8, np.int32, np.int64, np.float16, np.float32, np.float64]:\n            # each key holds a scalar\n            val_stats = ut.get_stats(vals)\n            fmtstr_ += ut.unindent('''\n            * val_stats = {val_stats}\n            ''').strip('\\n')\n\n    fmtstr = fmtstr_.format(**locals())\n    return ut.indent(fmtstr)", "response": "In depth debugging info\nTaxonomy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind other keys in dict that have the same value", "response": "def dict_find_other_sameval_keys(dict_, key):\n    \"\"\"\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_dict import *  # NOQA\n        >>> dict_ = {'default': 1, 'hierarchical': 5, 'linear': 0, 'kdtree': 1,\n        ...          'composite': 3, 'autotuned': 255, 'saved': 254, 'kmeans': 2,\n        ...          'lsh': 6, 'kdtree_single': 4}\n        >>> key = 'default'\n        >>> found_dict = dict_find_keys(dict_, val_list)\n    \"\"\"\n    value = dict_[key]\n    found_dict = dict_find_keys(dict_, [value])\n    other_keys = found_dict[value]\n    other_keys.remove(key)\n    return other_keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict_hist(item_list, weight_list=None, ordered=False, labels=None):\n    if labels is None:\n        # hist_ = defaultdict(lambda: 0)\n        hist_ = defaultdict(int)\n    else:\n        hist_ = {k: 0 for k in labels}\n    if weight_list is None:\n        # weight_list = it.repeat(1)\n        for item in item_list:\n            hist_[item] += 1\n    else:\n        for item, weight in zip(item_list, weight_list):\n            hist_[item] += weight\n    # hist_ = dict(hist_)\n    if ordered:\n        # import utool as ut\n        # key_order = ut.sortedby(list(hist_.keys()), list(hist_.values()))\n        getval = op.itemgetter(1)\n        key_order = [key for (key, value) in sorted(hist_.items(), key=getval)]\n        hist_ = order_dict_by(hist_, key_order)\n    return hist_", "response": "r Builds a histogram of items in item_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a discrete histogram of items into a discrete histogram by values and or ranges.", "response": "def range_hist(items, bins):\n    \"\"\"\n    Bins items into a discrete histogram by values and/or ranges.\n\n        items = [1, 2, 3, 4, 5, 6, 7]\n        bins = [0, 1, 2, (3, float('inf'))]\n        ut.range_hist(items, bins)\n    \"\"\"\n    big_hist = ut.dict_hist(items)\n    hist = ut.odict([(b, 0) for b in bins])\n\n    for k, v in big_hist.items():\n        for b in bins:\n            if isinstance(b, (list, tuple)):\n                if k >= b[0] and k < b[1]:\n                    hist[b] += v\n            elif k == b:\n                hist[b] += v\n    return hist"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_hist_cumsum(hist_, reverse=True):\n    import utool as ut\n    items = hist_.items()\n    if reverse:\n        items = sorted(items)[::-1]\n    else:\n        items = sorted(items)\n    key_list = ut.get_list_column(items, 0)\n    val_list = ut.get_list_column(items, 1)\n    cumhist_ = dict(zip(key_list, np.cumsum(val_list)))\n    return cumhist_", "response": "A dict that returns the cumulative sum of the histogram"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dict_union3(dict1, dict2, combine_op=op.add):\n    keys1 = set(dict1.keys())\n    keys2 = set(dict2.keys())\n    # Combine common keys\n    keys3 = keys1.intersection(keys2)\n    if len(keys3) > 0 and combine_op is None:\n        raise AssertionError('Can only combine disjoint dicts when combine_op is None')\n    dict3 = {key: combine_op(dict1[key], dict2[key]) for key in keys3}\n    # Combine unique keys\n    for key in keys1.difference(keys3):\n        dict3[key] = dict1[key]\n    for key in keys2.difference(keys3):\n        dict3[key] = dict2[key]\n    return dict3", "response": "r Returns a dict that is the union of two dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_intersection(dict1, dict2, combine=False, combine_op=op.add):\n    isect_keys = set(dict1.keys()).intersection(set(dict2.keys()))\n    if combine:\n        # TODO: depricate this\n        dict_isect = {k: combine_op(dict1[k], dict2[k]) for k in isect_keys}\n    else:\n        # maintain order if possible\n        if isinstance(dict1, OrderedDict):\n            isect_keys_ = [k for k in dict1.keys() if k in isect_keys]\n            _dict_cls = OrderedDict\n        else:\n            isect_keys_ = isect_keys\n            _dict_cls = dict\n        dict_isect = _dict_cls(\n            (k, dict1[k]) for k in isect_keys_ if dict1[k] == dict2[k]\n        )\n    return dict_isect", "response": "r Returns the intersection of two dictionaries"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dict_isect_combine(dict1, dict2, combine_op=op.add):\n    keys3 = set(dict1.keys()).intersection(set(dict2.keys()))\n    dict3 = {key: combine_op(dict1[key], dict2[key]) for key in keys3}\n    return dict3", "response": "Intersects dict keys and combination of dict values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_union_combine(dict1, dict2, combine_op=op.add,\n                       default=util_const.NoParam,\n                       default2=util_const.NoParam):\n    \"\"\"\n    Combine of dict keys and uses dfault value when key does not exist\n\n    CAREFUL WHEN USING THIS WITH REDUCE. Use dict_stack2 instead\n    \"\"\"\n    keys3 = set(dict1.keys()).union(set(dict2.keys()))\n    if default is util_const.NoParam:\n        dict3 = {key: combine_op(dict1[key], dict2[key]) for key in keys3}\n    else:\n        if default2 is util_const.NoParam:\n            default2 = default\n        dict3 = {key: combine_op(dict1.get(key, default), dict2.get(key, default2))\n                 for key in keys3}\n    return dict3", "response": "Combine dict1 and dict2 into a single dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef groupby_tags(item_list, tags_list):\n    groupid_to_items = defaultdict(list)\n    for tags, item in zip(tags_list, item_list):\n        for tag in tags:\n            groupid_to_items[tag].append(item)\n    return groupid_to_items", "response": "r Returns a dict of items that belong to multiple groups"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef group_pairs(pair_list):\n    # Initialize dict of lists\n    groupid_to_items = defaultdict(list)\n    # Insert each item into the correct group\n    for item, groupid in pair_list:\n        groupid_to_items[groupid].append(item)\n    return groupid_to_items", "response": "Groups a list of items using the first element in each pair as the item and the second element as the groupid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group_items(items, by=None, sorted_=True):\n    if by is not None:\n        pairs = list(zip(by, items))\n        if sorted_:\n            # Sort by groupid for cache efficiency (does this even do anything?)\n            # I forgot why this is needed? Determenism?\n            try:\n                pairs = sorted(pairs, key=op.itemgetter(0))\n            except TypeError:\n                # Python 3 does not allow sorting mixed types\n                pairs = sorted(pairs, key=lambda tup: str(tup[0]))\n    else:\n        pairs = items\n\n    # Initialize a dict of lists\n    groupid_to_items = defaultdict(list)\n    # Insert each item into the correct group\n    for groupid, item in pairs:\n        groupid_to_items[groupid].append(item)\n    return groupid_to_items", "response": "Groups a list of items by a given group id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hierarchical_group_items(item_list, groupids_list):\n    # Construct a defaultdict type with the appropriate number of levels\n    num_groups = len(groupids_list)\n    leaf_type = partial(defaultdict, list)\n    if num_groups > 1:\n        node_type = leaf_type\n        for _ in range(len(groupids_list) - 2):\n            node_type = partial(defaultdict, node_type)\n        root_type = node_type\n    elif num_groups == 1:\n        root_type = list\n    else:\n        raise ValueError('must suply groupids')\n    tree = defaultdict(root_type)\n    #\n    groupid_tuple_list = list(zip(*groupids_list))\n    for groupid_tuple, item in zip(groupid_tuple_list, item_list):\n        node = tree\n        for groupid in groupid_tuple:\n            node = node[groupid]\n        node.append(item)\n    return tree", "response": "This function converts a list of items into a heirarchical dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all the keys in a dict", "response": "def iflatten_dict_values(node, depth=0):\n    \"\"\"\n        >>> from utool.util_dict import *  # NOQA\n    \"\"\"\n    if isinstance(node, dict):\n        _iter = (iflatten_dict_values(value) for value in six.itervalues(node))\n        return util_iter.iflatten(_iter)\n    else:\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hierarchical_map_vals(func, node, max_depth=None, depth=0):\n    #if not isinstance(node, dict):\n    if not hasattr(node, 'items'):\n        return func(node)\n    elif max_depth is not None and depth >= max_depth:\n        #return func(node)\n        return map_dict_vals(func, node)\n        #return {key: func(val) for key, val in six.iteritems(node)}\n    else:\n        # recursion\n        #return {key: hierarchical_map_vals(func, val, max_depth, depth + 1) for key, val in six.iteritems(node)}\n        #keyval_list = [(key, hierarchical_map_vals(func, val, max_depth, depth + 1)) for key, val in six.iteritems(node)]\n        keyval_list = [(key, hierarchical_map_vals(func, val, max_depth, depth + 1)) for key, val in node.items()]\n        if isinstance(node, OrderedDict):\n            return OrderedDict(keyval_list)\n        else:\n            return dict(keyval_list)", "response": "This function maps the values of a dict tree like structure with leaves of type list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_odict_item(odict, key, newpos):\n    odict[key] = odict.pop(key)\n    for i, otherkey in enumerate(list(odict.keys())):\n        if otherkey != key and i >= newpos:\n            odict[otherkey] = odict.pop(otherkey)\n    return odict", "response": "Move the item at the given position in the dictionary odict to the given newpos."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting a dictionary by keys or values or keys", "response": "def sort_dict(dict_, part='keys', key=None, reverse=False):\n    \"\"\"\n    sorts a dictionary by its values or its keys\n\n    Args:\n        dict_ (dict_):  a dictionary\n        part (str): specifies to sort by keys or values\n        key (Optional[func]): a function that takes specified part\n            and returns a sortable value\n        reverse (bool): (Defaults to False) - True for descinding order. False\n            for ascending order.\n\n    Returns:\n        OrderedDict: sorted dictionary\n\n    CommandLine:\n        python -m utool.util_dict sort_dict\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dict import *  # NOQA\n        >>> import utool as ut\n        >>> dict_ = {'a': 3, 'c': 2, 'b': 1}\n        >>> results = []\n        >>> results.append(sort_dict(dict_, 'keys'))\n        >>> results.append(sort_dict(dict_, 'vals'))\n        >>> results.append(sort_dict(dict_, 'vals', lambda x: -x))\n        >>> result = ut.repr4(results)\n        >>> print(result)\n        [\n            {'a': 3, 'b': 1, 'c': 2},\n            {'b': 1, 'c': 2, 'a': 3},\n            {'a': 3, 'c': 2, 'b': 1},\n        ]\n    \"\"\"\n    if part == 'keys':\n        index = 0\n    elif part in {'vals', 'values'}:\n        index = 1\n    else:\n        raise ValueError('Unknown method part=%r' % (part,))\n    if key is None:\n        _key = op.itemgetter(index)\n    else:\n        def _key(item):\n            return key(item[index])\n    sorted_items = sorted(six.iteritems(dict_), key=_key, reverse=reverse)\n    sorted_dict = OrderedDict(sorted_items)\n    return sorted_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iteritems_sorted(dict_):\n    if isinstance(dict_, OrderedDict):\n        return six.iteritems(dict_)\n    else:\n        return iter(sorted(six.iteritems(dict_)))", "response": "change to iteritems ordered"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten_dict_vals(dict_):\n    if isinstance(dict_, dict):\n        return dict([\n            ((key, augkey), augval)\n            for key, val in dict_.items()\n            for augkey, augval in flatten_dict_vals(val).items()\n        ])\n    else:\n        return {None: dict_}", "response": "Flattens only values in a heirarchical dictionary keys are nested."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten_dict_items(dict_):\n    import utool as ut\n    flat_dict = ut.flatten_dict_vals(dict_)\n    flatter_dict = dict([(tuple(ut.unpack_iterables(key)[:-1]), val)\n                         for key, val in flat_dict.items()])\n    return flatter_dict", "response": "Flattens a dictionary into a flat dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef depth_atleast(list_, depth):\n    if depth == 0:\n        return True\n    else:\n        try:\n            return all([depth_atleast(item, depth - 1) for item in list_])\n        except TypeError:\n            return False", "response": "r Returns if depth of list is at least depth"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives the static percolator xml root and process info nodes, and all psms and peptides as iterators in a dict {'peptide': pep_iterator, 'psm': psm_iterator}, this generates percolator out data into a file.", "response": "def write_percolator_xml(staticxml, feats, fn):\n    \"\"\"Given the static percolator xml root and process info nodes, and all\n    psms and peptides as iterators in a dict {'peptide': pep_iterator, 'psm':\n    psm_iterator}, this generates percolator out data into a file.\"\"\"\n\n    # First get xml until psms opening element is found.\n    etree.SubElement(staticxml, 'psms').text = '***psms***'\n    root = etree.tostring(staticxml, pretty_print=True,\n                          xml_declaration=True, encoding='UTF-8')\n    root = root.decode('utf-8')\n    root = root[:root.find('***psms***')]\n\n    # Write opening xml\n    with open(fn, 'w') as fp:\n        fp.write(root)\n        fp.write('\\n')\n\n    # Then write features\n    with open(fn, 'a') as fp:\n        psmcount = 0\n        for psm in feats['psm']:\n            psmcount += 1\n            fp.write(psm)\n            fp.write('\\n')\n        fp.write('</psms><peptides>\\n')\n\n        peptidecount = 0\n        for pep in feats['peptide']:\n            peptidecount += 1\n            fp.write(pep)\n            fp.write('\\n')\n        fp.write('</peptides></percolator_output>')\n    print('Wrote {0} psms, {1} peptides to file {2}'.format(psmcount,\n                                                            peptidecount, fn))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_splitcolnr(header, bioset, splitcol):\n    if bioset:\n        return header.index(mzidtsvdata.HEADER_SETNAME)\n    elif splitcol is not None:\n        return splitcol - 1\n    else:\n        raise RuntimeError('Must specify either --bioset or --splitcol')", "response": "Returns the column nr on which to split PSM table given via flags\n    bioset and splitcol"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_psms_split(fn, oldheader, bioset, splitcol):\n    try:\n        splitcolnr = get_splitcolnr(oldheader, bioset, splitcol)\n    except IndexError:\n        raise RuntimeError('Cannot find bioset header column in '\n                           'input file {}, though --bioset has '\n                           'been passed'.format(fn))\n    for psm in tsvreader.generate_tsv_psms(fn, oldheader):\n        yield {'psm': psm,\n               'split_pool': psm[oldheader[splitcolnr]]\n               }", "response": "Yields PSMs and outputs dictionaries containing the PSMs and info to which split pool the\n    respective PSM belongs to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a randomly ordered list of the integers between min and max", "response": "def rnumlistwithoutreplacement(min, max):\n    \"\"\"Returns a randomly ordered list of the integers between min and max\"\"\"\n    if checkquota() < 1:\n        raise Exception(\"Your www.random.org quota has already run out.\")\n    requestparam = build_request_parameterNR(min, max)\n    request = urllib.request.Request(requestparam)\n    request.add_header('User-Agent', 'randomwrapy/0.1 very alpha')\n    opener = urllib.request.build_opener()\n    numlist = opener.open(request).read()\n    return numlist.split()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rnumlistwithreplacement(howmany, max, min=0):\n    if checkquota() < 1:\n        raise Exception(\"Your www.random.org quota has already run out.\")\n    requestparam = build_request_parameterWR(howmany, min, max)\n    request = urllib.request.Request(requestparam)\n    request.add_header('User-Agent', 'randomwrapy/0.1 very alpha')\n    opener = urllib.request.build_opener()\n    numlist = opener.open(request).read()\n    return numlist.split()", "response": "Returns a list of howmany integers with a maximum value = max."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rrandom():\n    import urllib.request\n    import urllib.error\n    import urllib.parse\n    if checkquota() < 1:\n        raise Exception(\"Your www.random.org quota has already run out.\")\n    request = urllib.request.Request(\n        'http://www.random.org/integers/?num=1&min=0&max=1000000000&col=1&base=10&format=plain&rnd=new')\n    request.add_header('User-Agent', 'randomwrapy/0.1 very alpha')\n    opener = urllib.request.build_opener()\n    numlist = opener.open(request).read()\n    num = numlist.split()[0]\n    return float(num) / 1000000000", "response": "Get the next random number in the range [ 0 1. 0 ) Returns a float."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sigfig_str(number, sigfig):\n    assert(sigfig > 0)\n    try:\n        d = decimal.Decimal(number)\n    except TypeError:\n        d = float_to_decimal(float(number))\n    sign, digits, exponent = d.as_tuple()\n    if len(digits) < sigfig:\n        digits = list(digits)\n        digits.extend([0] * (sigfig - len(digits)))\n    shift = d.adjusted()\n    result = int(''.join(map(str, digits[:sigfig])))\n    # Round the result\n    if len(digits) > sigfig and digits[sigfig] >= 5:\n        result += 1\n    result = list(str(result))\n    # Rounding can change the length of result\n    # If so, adjust shift\n    shift += len(result) - sigfig\n    # reset len of result to sigfig\n    result = result[:sigfig]\n    if shift >= sigfig - 1:\n        # Tack more zeros on the end\n        result += ['0'] * (shift - sigfig + 1)\n    elif 0 <= shift:\n        # Place the decimal point in between digits\n        result.insert(shift + 1, '.')\n    else:\n        # Tack zeros on the front\n        assert(shift < 0)\n        result = ['0.'] + ['0'] * (-shift - 1) + result\n    if sign:\n        result.insert(0, '-')\n    return ''.join(result)", "response": "Returns a string representation of a number in a random number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef num_fmt(num, max_digits=None):\n    if num is None:\n        return 'None'\n    def num_in_mag(num, mag):\n        return mag > num and num > (-1 * mag)\n    if max_digits is None:\n        # TODO: generalize\n        if num_in_mag(num, 1):\n            if num_in_mag(num, .1):\n                max_digits = 4\n            else:\n                max_digits = 3\n        else:\n            max_digits = 1\n    if util_type.is_float(num):\n        num_str = ('%.' + str(max_digits) + 'f') % num\n        # Handle trailing and leading zeros\n        num_str = num_str.rstrip('0').lstrip('0')\n        if num_str.startswith('.'):\n            num_str = '0' + num_str\n        if num_str.endswith('.'):\n            num_str = num_str + '0'\n        return num_str\n    elif util_type.is_int(num):\n        return int_comma_str(num)\n    else:\n        return '%r'", "response": "r Return a string representation of a number in the current language."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload pickled features for train and test sets assuming they are saved in the features folder along with their column names.", "response": "def load_feature_lists(self, feature_lists):\n        \"\"\"\n        Load pickled features for train and test sets, assuming they are saved\n        in the `features` folder along with their column names.\n\n        Args:\n            feature_lists: A list containing the names of the feature lists to load.\n\n        Returns:\n            A tuple containing 3 items: train dataframe, test dataframe,\n            and a list describing the index ranges for the feature lists.\n        \"\"\"\n\n        column_names = []\n        feature_ranges = []\n        running_feature_count = 0\n\n        for list_id in feature_lists:\n            feature_list_names = load_lines(self.features_dir + 'X_train_{}.names'.format(list_id))\n            column_names.extend(feature_list_names)\n            start_index = running_feature_count\n            end_index = running_feature_count + len(feature_list_names) - 1\n            running_feature_count += len(feature_list_names)\n            feature_ranges.append([list_id, start_index, end_index])\n\n        X_train = np.hstack([\n            load(self.features_dir + 'X_train_{}.pickle'.format(list_id))\n            for list_id in feature_lists\n        ])\n        X_test = np.hstack([\n            load(self.features_dir + 'X_test_{}.pickle'.format(list_id))\n            for list_id in feature_lists\n        ])\n\n        df_train = pd.DataFrame(X_train, columns=column_names)\n        df_test = pd.DataFrame(X_test, columns=column_names)\n\n        return df_train, df_test, feature_ranges"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_features(self, train_features, test_features, feature_names, feature_list_id):\n\n        self.save_feature_names(feature_names, feature_list_id)\n        self.save_feature_list(train_features, 'train', feature_list_id)\n        self.save_feature_list(test_features, 'test', feature_list_id)", "response": "Save the training and test sets to disk along with their metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the names of the features for the given feature list to a metadata file.", "response": "def save_feature_names(self, feature_names, feature_list_id):\n        \"\"\"\n        Save the names of the features for the given feature list to a metadata file.\n        Example: `save_feature_names(['num_employees', 'stock_price'], 'company')`.\n\n        Args:\n            feature_names: A list containing the names of the features, matching the column order.\n            feature_list_id: The name for this feature list.\n        \"\"\"\n\n        save_lines(feature_names, self.features_dir + 'X_train_{}.names'.format(feature_list_id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_feature_list(self, obj, set_id, feature_list_id):\n\n        save(obj, self.features_dir + 'X_{}_{}.pickle'.format(set_id, feature_list_id))", "response": "Save the specified feature list to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef discover():\n\n        # Try ../data: we're most likely running a Jupyter notebook from the 'notebooks' directory\n        candidate_path = os.path.abspath(os.path.join(os.curdir, os.pardir, 'data'))\n        if os.path.exists(candidate_path):\n            return Project(os.path.abspath(os.path.join(candidate_path, os.pardir)))\n\n        # Try ./data\n        candidate_path = os.path.abspath(os.path.join(os.curdir, 'data'))\n        if os.path.exists(candidate_path):\n            return Project(os.path.abspath(os.curdir))\n\n        # Try ../../data\n        candidate_path = os.path.abspath(os.path.join(os.curdir, os.pardir, 'data'))\n        if os.path.exists(candidate_path):\n            return Project(os.path.abspath(os.path.join(candidate_path, os.pardir, os.pardir)))\n\n        # Out of ideas at this point.\n        raise ValueError('Cannot discover the structure of the project. Make sure that the data directory exists')", "response": "Automatically discover the paths to various data folders in this project\n        and compose a Project instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef init():\n\n        project = Project(os.path.abspath(os.getcwd()))\n        paths_to_create = [\n            project.data_dir,\n            project.notebooks_dir,\n            project.aux_dir,\n            project.features_dir,\n            project.preprocessed_data_dir,\n            project.submissions_dir,\n            project.trained_model_dir,\n            project.temp_dir,\n        ]\n\n        for path in paths_to_create:\n            os.makedirs(path, exist_ok=True)", "response": "Creates the project infrastructure assuming the current directory is the project root."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unique_justseen(iterable, key=None):\n    \"List unique elements, preserving order. Remember only the element just seen.\"\n    # unique_justseen('AAAABBBCCDAABBB') --> A B C D A B\n    # unique_justseen('ABBCcAD', str.lower) --> A B C A D\n    return imap(next, imap(operator.itemgetter(1), groupby(iterable, key)))", "response": "List unique elements preserving order. Remember only the element just seen."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_module(module_name=None, module=None, register=True):\n    if module is None and module_name is not None:\n        try:\n            module = sys.modules[module_name]\n        except KeyError as ex:\n            print(ex)\n            raise KeyError(('module_name=%r must be loaded before ' +\n                            'receiving injections') % module_name)\n    elif module is not None and module_name is None:\n        pass\n    else:\n        raise ValueError('module_name or module must be exclusively specified')\n    if register is True:\n        _add_injected_module(module)\n    return module", "response": "returns module if module_name is not None and module is not None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninjecting exceptions to be colored if not already there", "response": "def inject_colored_exceptions():\n    \"\"\"\n    Causes exceptions to be colored if not already\n\n    Hooks into sys.excepthook\n\n    CommandLine:\n        python -m utool.util_inject --test-inject_colored_exceptions\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_inject import *  # NOQA\n        >>> print('sys.excepthook = %r ' % (sys.excepthook,))\n        >>> #assert sys.excepthook is colored_pygments_excepthook, 'bad excepthook'\n        >>> raise Exception('should be in color')\n\n    \"\"\"\n    #COLORED_INJECTS = '--nocolorex' not in sys.argv\n    #COLORED_INJECTS = '--colorex' in sys.argv\n    # Ignore colored exceptions on win32\n    if VERBOSE:\n        print('[inject] injecting colored exceptions')\n    if not sys.platform.startswith('win32'):\n        if VERYVERBOSE:\n            print('[inject] injecting colored exceptions')\n        if '--noinject-color' in sys.argv:\n            print('Not injecting color')\n        else:\n            sys.excepthook = colored_pygments_excepthook\n    else:\n        if VERYVERBOSE:\n            print('[inject] cannot inject colored exceptions')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inject_print_functions(module_name=None, module_prefix='[???]',\n                           DEBUG=False, module=None):\n    \"\"\"\n    makes print functions to be injected into the module\n    \"\"\"\n    module = _get_module(module_name, module)\n    if SILENT:\n        def print(*args):\n            \"\"\" silent builtins.print \"\"\"\n            pass\n        def printDBG(*args):\n            \"\"\" silent debug print \"\"\"\n            pass\n        def print_(*args):\n            \"\"\" silent stdout.write \"\"\"\n            pass\n    else:\n        if DEBUG_PRINT:\n            # Turns on printing where a message came from\n            def print(*args):\n                \"\"\" debugging logging builtins.print \"\"\"\n                from utool._internal.meta_util_dbg import get_caller_name\n                calltag = ''.join(('[caller:', get_caller_name(N=DEBUG_PRINT_N), ']' ))\n                util_logging._utool_print()(calltag, *args)\n        else:\n            def print(*args):\n                \"\"\" logging builtins.print \"\"\"\n                util_logging._utool_print()(*args)\n\n        if __AGGROFLUSH__:\n            def print_(*args):\n                \"\"\" aggressive logging stdout.write \"\"\"\n                util_logging._utool_write()(*args)\n                util_logging._utool_flush()()\n        else:\n            def print_(*args):\n                \"\"\" logging stdout.write \"\"\"\n                util_logging._utool_write()(*args)\n\n        # turn on module debugging with command line flags\n        dotpos = module.__name__.rfind('.')\n        if dotpos == -1:\n            module_name = module.__name__\n        else:\n            module_name = module.__name__[dotpos + 1:]\n        def _replchars(str_):\n            return str_.replace('_', '-').replace(']', '').replace('[', '')\n        flag1 = '--debug-%s' % _replchars(module_name)\n        flag2 = '--debug-%s' % _replchars(module_prefix)\n        DEBUG_FLAG = any([flag in sys.argv for flag in [flag1, flag2]])\n        for curflag in ARGV_DEBUG_FLAGS:\n            if curflag in module_prefix:\n                DEBUG_FLAG = True\n        if __DEBUG_ALL__ or DEBUG or DEBUG_FLAG:\n            print('INJECT_PRINT: %r == %r' % (module_name, module_prefix))\n            def printDBG(*args):\n                \"\"\" debug logging print \"\"\"\n                msg = ', '.join(map(str, args))\n                util_logging.__UTOOL_PRINTDBG__(module_prefix + ' DEBUG ' + msg)\n        else:\n            def printDBG(*args):\n                \"\"\" silent debug logging print \"\"\"\n                pass\n    #_inject_funcs(module, print, print_, printDBG)\n    print_funcs = (print, print_, printDBG)\n    return print_funcs", "response": "Injects print functions into the module"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_module_reload_func(module_name=None, module_prefix='[???]', module=None):\n    module = _get_module(module_name, module, register=False)\n    if module_name is None:\n        module_name = str(module.__name__)\n    def rrr(verbose=True):\n        \"\"\" Dynamic module reloading \"\"\"\n        if not __RELOAD_OK__:\n            raise Exception('Reloading has been forced off')\n        try:\n            import imp\n            if verbose and not QUIET:\n                builtins.print('RELOAD: ' + str(module_prefix) + ' __name__=' + module_name)\n            imp.reload(module)\n        except Exception as ex:\n            print(ex)\n            print('%s Failed to reload' % module_prefix)\n            raise\n    # this doesn't seem to set anything on import *\n    #_inject_funcs(module, rrr)\n    return rrr", "response": "Creates a function that reloads the specified module."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef noinject(module_name=None, module_prefix='[???]', DEBUG=False, module=None, N=0, via=None):\n    if PRINT_INJECT_ORDER:\n        from utool._internal import meta_util_dbg\n        callername = meta_util_dbg.get_caller_name(N=N + 1, strict=False)\n        lineno = meta_util_dbg.get_caller_lineno(N=N + 1, strict=False)\n        suff = ' via %s' % (via,) if via else ''\n        fmtdict = dict(N=N, lineno=lineno, callername=callername,\n                       modname=module_name, suff=suff)\n        msg = '[util_inject] N={N} {modname} is imported by {callername} at lineno={lineno}{suff}'.format(**fmtdict)\n\n        if DEBUG_SLOW_IMPORT:\n            global PREV_MODNAME\n            seconds = tt.toc()\n            import_times[(PREV_MODNAME, module_name)] = seconds\n            PREV_MODNAME = module_name\n\n        builtins.print(msg)\n\n        if DEBUG_SLOW_IMPORT:\n            tt.tic()\n        # builtins.print(elapsed)\n        if EXIT_ON_INJECT_MODNAME == module_name:\n            builtins.print('...exiting')\n            assert False, 'exit in inject requested'", "response": "This function is used to not inject a module into the module"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inject(module_name=None, module_prefix='[???]', DEBUG=False, module=None, N=1):\n    #noinject(module_name, module_prefix, DEBUG, module, N=1)\n    noinject(module_name, module_prefix, DEBUG, module, N=N)\n    module = _get_module(module_name, module)\n    rrr         = make_module_reload_func(None, module_prefix, module)\n    profile_    = make_module_profile_func(None, module_prefix, module)\n    print_funcs = inject_print_functions(None, module_prefix, DEBUG, module)\n    (print, print_, printDBG) = print_funcs\n    return (print, print_, printDBG, rrr, profile_)", "response": "Injects a module with utool magic"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping that depricates print_ and printDBG", "response": "def inject2(module_name=None, module_prefix=None, DEBUG=False, module=None, N=1):\n    \"\"\" wrapper that depricates print_ and printDBG \"\"\"\n    if module_prefix is None:\n        module_prefix = '[%s]' % (module_name,)\n    noinject(module_name, module_prefix, DEBUG, module, N=N)\n    module = _get_module(module_name, module)\n    rrr      = make_module_reload_func(None, module_prefix, module)\n    profile_ = make_module_profile_func(None, module_prefix, module)\n    print    = make_module_print_func(module)\n    return print, rrr, profile_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_python_text_into_lines(text):\n    #import jedi\n    #script = jedi.Script(text, line=1, column=None, path='')\n    def parentesis_are_balanced(line):\n        \"\"\"\n        helper\n\n        References:\n            http://stackoverflow.com/questions/18007995/recursive-paren-balance\n        \"\"\"\n        def balanced(str_, i=0, cnt=0, left='(', right=')'):\n            if i == len(str_):\n                return cnt == 0\n            if cnt < 0:\n                return False\n            if str_[i] == left:\n                return  balanced(str_, i + 1, cnt + 1)\n            elif str_[i] == right:\n                return  balanced(str_, i + 1, cnt - 1)\n            return balanced(str_, i + 1, cnt)\n        return balanced(line)\n\n    lines = text.split('\\n')\n    new_lines = []\n    current_line = ''\n    for line in lines:\n        current_line += line\n        if parentesis_are_balanced(current_line):\n            new_lines.append(current_line)\n            current_line = ''\n    return lines", "response": "split python text into multiple lines"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inject_python_code2(fpath, patch_code, tag):\n    import utool as ut\n    text = ut.readfrom(fpath)\n    start_tag = '# <%s>' % tag\n    end_tag = '# </%s>' % tag\n    new_text = ut.replace_between_tags(text, patch_code, start_tag, end_tag)\n    ut.writeto(fpath, new_text)", "response": "Injects python code into the tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninject code into the file", "response": "def inject_python_code(fpath, patch_code, tag=None,\n                       inject_location='after_imports'):\n    \"\"\"\n    DEPRICATE\n    puts code into files on disk\n    \"\"\"\n    import utool as ut\n    assert tag is not None, 'TAG MUST BE SPECIFIED IN INJECTED CODETEXT'\n    text = ut.read_from(fpath)\n    comment_start_tag = '# <util_inject:%s>' % tag\n    comment_end_tag  = '# </util_inject:%s>' % tag\n\n    tagstart_txtpos = text.find(comment_start_tag)\n    tagend_txtpos = text.find(comment_end_tag)\n\n    text_lines = ut.split_python_text_into_lines(text)\n\n    # split the file into two parts and inject code between them\n    if tagstart_txtpos != -1 or tagend_txtpos != -1:\n        assert tagstart_txtpos != -1, 'both tags must not be found'\n        assert tagend_txtpos != -1, 'both tags must not be found'\n\n        for pos, line in enumerate(text_lines):\n            if line.startswith(comment_start_tag):\n                tagstart_pos = pos\n            if line.startswith(comment_end_tag):\n                tagend_pos = pos\n        part1 = text_lines[0:tagstart_pos]\n        part2 = text_lines[tagend_pos + 1:]\n    else:\n        if inject_location == 'after_imports':\n            first_nonimport_pos = 0\n            for line in text_lines:\n                list_ = ['import ', 'from ', '#', ' ']\n                isvalid = (len(line) == 0 or\n                           any(line.startswith(str_) for str_ in list_))\n                if not isvalid:\n                    break\n                first_nonimport_pos += 1\n            part1 = text_lines[0:first_nonimport_pos]\n            part2 = text_lines[first_nonimport_pos:]\n        else:\n            raise AssertionError('Unknown inject location')\n\n    newtext = (\n        '\\n'.join(part1 + [comment_start_tag]) +\n        '\\n' + patch_code + '\\n' +\n        '\\n'.join( [comment_end_tag] + part2)\n    )\n    text_backup_fname = fpath + '.' + ut.get_timestamp() + '.bak'\n    ut.write_to(text_backup_fname, text)\n    ut.write_to(fpath, newtext)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to make a nice type string for a value. Can also pass in a Printable parent object.", "response": "def printableType(val, name=None, parent=None):\n    \"\"\"\n    Tries to make a nice type string for a value.\n    Can also pass in a Printable parent object\n    \"\"\"\n    import numpy as np\n    if parent is not None and hasattr(parent, 'customPrintableType'):\n        # Hack for non - trivial preference types\n        _typestr = parent.customPrintableType(name)\n        if _typestr is not None:\n            return _typestr\n    if isinstance(val, np.ndarray):\n        info = npArrInfo(val)\n        _typestr = info.dtypestr\n    elif isinstance(val, object):\n        _typestr = val.__class__.__name__\n    else:\n        _typestr = str(type(val))\n        _typestr = _typestr.replace('type', '')\n        _typestr = re.sub('[\\'><]', '', _typestr)\n        _typestr = re.sub('  *', ' ', _typestr)\n        _typestr = _typestr.strip()\n    return _typestr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef printableVal(val, type_bit=True, justlength=False):\n    from utool import util_dev\n    # Move to util_dev\n    # NUMPY ARRAY\n    import numpy as np\n    if type(val) is np.ndarray:\n        info = npArrInfo(val)\n        if info.dtypestr.startswith('bool'):\n            _valstr = '{ shape:' + info.shapestr + ' bittotal: ' + info.bittotal + '}'\n            # + '\\n  |_____'\n        elif info.dtypestr.startswith('float'):\n            _valstr = util_dev.get_stats_str(val)\n        else:\n            _valstr = '{ shape:' + info.shapestr + ' mM:' + info.minmaxstr + ' }'  # + '\\n  |_____'\n    # String\n    elif isinstance(val, (str, unicode)):  # NOQA\n        _valstr = '\\'%s\\'' % val\n    # List\n    elif isinstance(val, list):\n        if justlength or len(val) > 30:\n            _valstr = 'len=' + str(len(val))\n        else:\n            _valstr = '[ ' + (', \\n  '.join([str(v) for v in val])) + ' ]'\n    # ??? isinstance(val, AbstractPrintable):\n    elif hasattr(val, 'get_printable') and type(val) != type:\n        _valstr = val.get_printable(type_bit=type_bit)\n    elif isinstance(val, dict):\n        _valstr = '{\\n'\n        for val_key in val.keys():\n            val_val = val[val_key]\n            _valstr += '  ' + str(val_key) + ' : ' + str(val_val) + '\\n'\n        _valstr += '}'\n    else:\n        _valstr = str(val)\n    if _valstr.find('\\n') > 0:  # Indent if necessary\n        _valstr = _valstr.replace('\\n', '\\n    ')\n        _valstr = '\\n    ' + _valstr\n    _valstr = re.sub('\\n *$', '', _valstr)  # Replace empty lines\n    return _valstr", "response": "Returns a string representation of the object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a DynStruct with the information of the array", "response": "def npArrInfo(arr):\n    \"\"\"\n    OLD update and refactor\n    \"\"\"\n    from utool.DynamicStruct import DynStruct\n    info = DynStruct()\n    info.shapestr  = '[' + ' x '.join([str(x) for x in arr.shape]) + ']'\n    info.dtypestr  = str(arr.dtype)\n    if info.dtypestr == 'bool':\n        info.bittotal = 'T=%d, F=%d' % (sum(arr), sum(1 - arr))\n    elif info.dtypestr == 'object':\n        info.minmaxstr = 'NA'\n    elif info.dtypestr[0] == '|':\n        info.minmaxstr = 'NA'\n    else:\n        if arr.size > 0:\n            info.minmaxstr = '(%r, %r)' % (arr.min(), arr.max())\n        else:\n            info.minmaxstr = '(None)'\n    return info"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling all the signals in the current module.", "response": "def enable_signals(self):\n        '''\n        e.g signal_dict = {signal_path:signal_receiver_path_list, ....}\n        :return:\n        '''\n        signal_dict = self.settings[self.signal_key] or {}\n        for i in signal_dict.keys():\n            sig_module, signal_class = self.import_class_from_path(i)\n            for j in signal_dict[i]:\n                recv_module, recv_coro = self.import_class_from_path(j)\n                signal_class.register(recv_coro)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_isobaric_ratios(psmfn, psmheader, channels, denom_channels, min_int,\n                        targetfn, accessioncol, normalize, normratiofn):\n    \"\"\"Main function to calculate ratios for PSMs, peptides, proteins, genes.\n    Can do simple ratios, median-of-ratios and median-centering\n    normalization.\"\"\"\n    psm_or_feat_ratios = get_psmratios(psmfn, psmheader, channels,\n                                       denom_channels, min_int, accessioncol)\n    if normalize and normratiofn:\n        normheader = reader.get_tsv_header(normratiofn)\n        normratios = get_ratios_from_fn(normratiofn, normheader, channels)\n        ch_medians = get_medians(channels, normratios, report=True)\n        outratios = calculate_normalized_ratios(psm_or_feat_ratios, ch_medians,\n                                                channels)\n    elif normalize:\n        flatratios = [[feat[ch] for ch in channels]\n                      for feat in psm_or_feat_ratios]\n        ch_medians = get_medians(channels, flatratios, report=True)\n        outratios = calculate_normalized_ratios(psm_or_feat_ratios, ch_medians,\n                                                channels)\n    else:\n        outratios = psm_or_feat_ratios\n    # at this point, outratios look like:\n    # [{ch1: 123, ch2: 456, ISOQUANTRATIO_FEAT_ACC: ENSG1244}, ]\n    if accessioncol and targetfn:\n        outratios = {x[ISOQUANTRATIO_FEAT_ACC]: x for x in outratios}\n        return output_to_target_accession_table(targetfn, outratios, channels)\n    elif not accessioncol and not targetfn:\n        return paste_to_psmtable(psmfn, psmheader, outratios)\n    elif accessioncol and not targetfn:\n        # generate new table with accessions\n        return ({(k if not k == ISOQUANTRATIO_FEAT_ACC\n                  else prottabledata.HEADER_ACCESSION): v\n                 for k, v in ratio.items()} for ratio in outratios)", "response": "Main function to calculate ratios for PSMs peptides proteins genes and their corresponding isobaric features."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the ratios for PSM tables containing isobaric channels with raw intensities. Normalizes the ratios by median. NA values are excluded from the normalization.", "response": "def calculate_normalized_ratios(ratios, ch_medians, channels):\n    \"\"\"Calculates ratios for PSM tables containing isobaric channels with\n    raw intensities. Normalizes the ratios by median. NA values or values\n    below min_intensity are excluded from the normalization.\"\"\"\n    outratios = []\n    for quant in ratios:\n        quant.update({ch: str(quant[ch] / ch_medians[ch])\n                      if quant[ch] != 'NA' else 'NA' for ch in channels})\n        outratios.append(quant)\n    return outratios"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating ratios for PSM tables containing isobaric channels with raw intensities.", "response": "def get_normalized_ratios(psmfn, header, channels, denom_channels,\n                          min_intensity, second_psmfn, secondheader):\n    \"\"\"Calculates ratios for PSM tables containing isobaric channels with\n    raw intensities. Normalizes the ratios by median. NA values or values\n    below min_intensity are excluded from the normalization.\"\"\"\n    ratios = []\n    if second_psmfn is not None:\n        median_psmfn = second_psmfn\n        medianheader = secondheader\n    else:\n        median_psmfn = psmfn\n        medianheader = header\n    for psm in reader.generate_tsv_psms(median_psmfn, medianheader):\n        ratios.append(calc_psm_ratios(psm, channels, denom_channels,\n                                      min_intensity))\n    ch_medians = isonormalizing.get_medians(channels, ratios)\n    report = ('Channel intensity medians used for normalization:\\n'\n              '{}'.format('\\n'.join(['{} - {}'.format(ch, ch_medians[ch])\n                                     for ch in channels])))\n    sys.stdout.write(report)\n    for psm in reader.generate_tsv_psms(psmfn, header):\n        psmratios = calc_psm_ratios(psm, channels, denom_channels,\n                                    min_intensity)\n        psm.update({ch: str(psmratios[ix] / ch_medians[ch])\n                    if psmratios[ix] != 'NA' else 'NA'\n                    for ix, ch in enumerate(channels)})\n        yield psm"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sanitize(value):\n\n    value = unicodedata.normalize('NFKD', value)\n    value = value.strip()\n    value = re.sub('[^./\\w\\s-]', '', value)\n    value = re.sub('[-\\s]+', '-', value)\n\n    return value", "response": "Strips all undesirable characters out of potential file paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a directory and removes that directory if an exception is thrown.", "response": "def remove_on_exception(dirname, remove=True):\n    \"\"\"Creates a directory, yields to the caller, and removes that directory\n       if an exception is thrown.\"\"\"\n    os.makedirs(dirname)\n    try:\n        yield\n    except:\n        if remove:\n            shutil.rmtree(dirname, ignore_errors=True)\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_percolator_to_mzidtsv(mzidfn, tsvfn, multipsm, oldheader):\n    namespace = readers.get_mzid_namespace(mzidfn)\n    try:\n        xmlns = '{%s}' % namespace['xmlns']\n    except TypeError:\n        xmlns = ''\n    specfnids = readers.get_mzid_specfile_ids(mzidfn, namespace)\n    mzidpepmap = {}\n    for peptide in readers.generate_mzid_peptides(mzidfn, namespace):\n        pep_id, seq = readers.get_mzid_peptidedata(peptide, xmlns)\n        mzidpepmap[pep_id] = seq\n    mzidpercomap = {}\n    for specid_data in readers.generate_mzid_spec_id_items(mzidfn, namespace,\n                                                           xmlns, specfnids):\n        scan, fn, pepid, spec_id = specid_data\n        percodata = readers.get_specidentitem_percolator_data(spec_id, xmlns)\n        try:\n            mzidpercomap[fn][scan][mzidpepmap[pepid]] = percodata\n        except KeyError:\n            try:\n                mzidpercomap[fn][scan] = {mzidpepmap[pepid]: percodata}\n            except KeyError:\n                mzidpercomap[fn] = {scan: {mzidpepmap[pepid]: percodata}}\n    for line in tsvreader.generate_tsv_psms(tsvfn, oldheader):\n        outline = {k: v for k, v in line.items()}\n        fn = line[mzidtsvdata.HEADER_SPECFILE]\n        scan = line[mzidtsvdata.HEADER_SCANNR]\n        seq = line[mzidtsvdata.HEADER_PEPTIDE]\n        outline.update(mzidpercomap[fn][scan][seq])\n        yield outline", "response": "Takes a MSGF + tsv and corresponding mzId adds percolator data to tsv lines. Generator yields the lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_rawprofile_blocks(text):\n    # The total time reported in the raw output is from pystone not kernprof\n    # The pystone total time is actually the average time spent in the function\n    delim = 'Total time: '\n    delim2 = 'Pystone time: '\n    #delim = 'File: '\n    profile_block_list = ut.regex_split('^' + delim, text)\n    for ix in range(1, len(profile_block_list)):\n        profile_block_list[ix] = delim2 + profile_block_list[ix]\n    return profile_block_list", "response": "Split the rawprofile output into blocks along delimters and put delimeters back in\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a map from times to line_profile blocks", "response": "def parse_timemap_from_blocks(profile_block_list):\n    \"\"\"\n    Build a map from times to line_profile blocks\n    \"\"\"\n    prefix_list = []\n    timemap = ut.ddict(list)\n    for ix in range(len(profile_block_list)):\n        block = profile_block_list[ix]\n        total_time = get_block_totaltime(block)\n        # Blocks without time go at the front of sorted output\n        if total_time is None:\n            prefix_list.append(block)\n        # Blocks that are not run are not appended to output\n        elif total_time != 0:\n            timemap[total_time].append(block)\n    return prefix_list, timemap"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_summary(profile_block_list, maxlines=20):\n    time_list = [get_block_totaltime(block) for block in profile_block_list]\n    time_list = [time if time is not None else -1 for time in time_list]\n    blockid_list = [get_block_id(block) for block in profile_block_list]\n    sortx = ut.list_argsort(time_list)\n    sorted_time_list = ut.take(time_list, sortx)\n    sorted_blockid_list = ut.take(blockid_list, sortx)\n\n    aligned_blockid_list = ut.util_str.align_lines(sorted_blockid_list, ':')\n    summary_lines = [('%6.2f seconds - ' % time) + line\n                     for time, line in\n                     zip(sorted_time_list, aligned_blockid_list)]\n    #summary_header = ut.codeblock(\n    #    '''\n    #    CLEANED PROFILE OUPUT\n\n    #    The Pystone timings are not from kernprof, so they may include kernprof\n    #    overhead, whereas kernprof timings do not (unless the line being\n    #    profiled is also decorated with kernrof)\n\n    #    The kernprof times are reported in Timer Units\n\n    #    ''')\n    summary_lines_ = ut.listclip(summary_lines, maxlines, fromback=True)\n    summary_text = '\\n'.join(summary_lines_)\n    return summary_text", "response": "Returns a summary of the profile_block_list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_line_profile_text(text):\n    #\n    profile_block_list = parse_rawprofile_blocks(text)\n    #profile_block_list = fix_rawprofile_blocks(profile_block_list)\n    #---\n    # FIXME can be written much nicer\n    prefix_list, timemap = parse_timemap_from_blocks(profile_block_list)\n    # Sort the blocks by time\n    sorted_lists = sorted(six.iteritems(timemap), key=operator.itemgetter(0))\n    newlist = prefix_list[:]\n    for key, val in sorted_lists:\n        newlist.extend(val)\n    # Rejoin output text\n    output_text = '\\n'.join(newlist)\n    #---\n    # Hack in a profile summary\n    summary_text = get_summary(profile_block_list)\n    output_text = output_text\n    return output_text, summary_text", "response": "Takes the output of line profile and returns the output text and summary text"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a. lprof file and cleans it", "response": "def clean_lprof_file(input_fname, output_fname=None):\n    \"\"\" Reads a .lprof file and cleans it \"\"\"\n    # Read the raw .lprof text dump\n    text = ut.read_from(input_fname)\n    # Sort and clean the text\n    output_text = clean_line_profile_text(text)\n    return output_text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the weights for each class based on the frequencies of the samples.", "response": "def get_class_weights(y, smooth_factor=0):\n    \"\"\"\n    Returns the weights for each class based on the frequencies of the samples.\n\n    Args:\n        y: A list of true labels (the labels must be hashable).\n        smooth_factor: A factor that smooths extremely uneven weights.\n\n    Returns:\n        A dictionary with the weight for each class.\n    \"\"\"\n\n    from collections import Counter\n    counter = Counter(y)\n\n    if smooth_factor > 0:\n        p = max(counter.values()) * smooth_factor\n        for k in counter.keys():\n            counter[k] += p\n\n    majority = max(counter.values())\n\n    return {cls: float(majority / count) for cls, count in counter.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_loss_history(history, figsize=(15, 8)):\n\n    plt.figure(figsize=figsize)\n\n    plt.plot(history.history[\"loss\"])\n    plt.plot(history.history[\"val_loss\"])\n\n    plt.xlabel(\"# Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend([\"Training\", \"Validation\"])\n    plt.title(\"Loss over time\")\n\n    plt.show()", "response": "Plots the learning history for a Keras model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrying calling the decorated function using an exponential backoff. Args: exceptions: The exception to check. may be a tuple of exceptions to check. tries: Number of times to try (not retry) before giving up. delay: Initial delay between retries in seconds. backoff: Backoff multiplier (e.g. value of 2 will double the delay each retry). logger: Logger to use. If None, print.", "response": "def retry(exceptions, tries=5, delay=1, backoff=2, logger=None):\n    \"\"\"\n    Retry calling the decorated function using an exponential backoff.\n\n    Args:\n        exceptions: The exception to check. may be a tuple of\n            exceptions to check.\n        tries: Number of times to try (not retry) before giving up.\n        delay: Initial delay between retries in seconds.\n        backoff: Backoff multiplier (e.g. value of 2 will double the delay\n            each retry).\n        logger: Logger to use. If None, print.\n    \"\"\"\n\n    def deco_retry(func):\n        @wraps(func)\n        async def f_retry(self, *args, **kwargs):\n            if not iscoroutine(func):\n                f = coroutine(func)\n            else:\n                f = func\n\n            mtries, mdelay = tries, delay\n            while mtries > 1:\n                try:\n                    return await f(self, *args, **kwargs)\n                except exceptions:\n                    if logger:\n                        logger.info('Retrying %s after %s seconds', f.__name__, mdelay)\n                    sleep(mdelay)\n                    mtries -= 1\n                    mdelay *= backoff\n            return await f(self, *args, **kwargs)\n\n        return f_retry\n\n    return deco_retry"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields the unique PSM IDs for each gene.", "response": "def get_unique_gene_psms(self, genetable, fields, firstjoin):\n        \"\"\"Uniques the results from get_proteins_psms so each PSM as defined\n        by gene ID / setname / psm_id will only occur once\"\"\"\n        lastgene = None\n        gpsms_out, gp_ids = [], []\n        for gpsm in self.get_proteins_psms(genetable, fields, firstjoin):\n            if gpsm[0] != lastgene:\n                for outpsm in gpsms_out:\n                    yield outpsm\n                lastgene = gpsm[0]\n                gpsms_out, gp_ids = [], []\n            gp_id = gpsm[0] + gpsm[1] + gpsm[3]\n            if gp_id not in gp_ids:\n                gp_ids.append(gp_id)\n                gpsms_out.append(gpsm)\n        for outpsm in gpsms_out:\n            yield outpsm"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prepare_mergetable_sql(self, precursor=False, isobaric=False,\n                               probability=False, fdr=False, pep=False):\n        \"\"\"Dynamically build SQL query to generate entries for the multi-set\n        merged protein and peptide tables. E.g.\n\n        SELECT g.gene_acc, pc.channel_name, pc.amount_psms_name,\n               giq.quantvalue giq.amount_psms gfdr.fdr\n        FROM genes AS g\n        JOIN biosets AS bs\n        JOIN gene_tables AS gt ON gt.set_id=bs.set_id\n        JOIN genequant_channels AS pc ON pc.gene_table_id=gt.genetable_id\n        JOIN gene_iso_quanted AS giq ON giq.gene_id=g.gene_id\n             AND giq.channel_id=pc.channel_id\n        JOIN gene_fdr AS gfdr ON gfdr.gene_id=g.gene_id\n             AND gfdr.genetable_id=gt.genetable_id\n        ORDER BY g.gene\n\n        This is multi-set output because we join on biosets. The output is\n        then parsed to its respective set by the action code.\n        \"\"\"\n        featcol = self.colmap[self.table_map[self.datatype]['feattable']][1]\n        selectmap, count = self.update_selects({}, ['p_acc', 'set_name'], 0)\n        joins = []\n        if self.datatype == 'protein':\n            selects = ['pgm.{}'.format(featcol), 'bs.set_name']\n            firstselect = 'pgm'\n            joins.append(('proteins', 'g', ['pgm']))\n        else:\n            selects = ['g.{}'.format(featcol), 'bs.set_name']\n            firstselect = 'g'\n        if isobaric:\n            selects.extend(['pc.channel_name',\n                            'pc.amount_psms_name', 'giq.quantvalue',\n                            'giq.amount_psms'])\n            joins.extend([(self.table_map[self.datatype]['isochtable'], 'pc',\n                           ['gt']),\n                          (self.table_map[self.datatype]['isoqtable'], 'giq',\n                           ['g', 'pc'], True),\n                          ])\n            fld = ['channel', 'isoq_psmsfield', 'isoq_val',\n                   'isoq_psms']\n            selectmap, count = self.update_selects(selectmap, fld, count)\n        if precursor:\n            selects.extend(['preq.quant'])\n            joins.append((self.table_map[self.datatype]['prectable'], 'preq',\n                          ['g', 'gt'], True))\n            fld = ['preq_val']\n            selectmap, count = self.update_selects(selectmap, fld, count)\n        if probability:\n            selects.extend(['gprob.probability'])\n            joins.append((self.table_map[self.datatype]['probabilitytable'],\n                          'gprob', ['g', 'gt'], True))\n            fld = ['prob_val']\n            selectmap, count = self.update_selects(selectmap, fld, count)\n        if fdr:\n            selects.extend(['gfdr.fdr'])\n            joins.append((self.table_map[self.datatype]['fdrtable'], 'gfdr',\n                          ['g', 'gt'], True))\n            fld = ['fdr_val']\n            selectmap, count = self.update_selects(selectmap, fld, count)\n        if pep:\n            selects.extend(['gpep.pep'])\n            joins.append((self.table_map[self.datatype]['peptable'], 'gpep',\n                          ['g', 'gt'], True))\n            fld = ['pep_val']\n            selectmap, count = self.update_selects(selectmap, fld, count)\n        sql = ('SELECT {} FROM {} AS {} JOIN biosets AS bs '\n               'JOIN {} AS gt ON gt.set_id=bs.set_id'.format(\n                   ', '.join(selects),\n                   self.table_map[self.datatype]['feattable'],\n                   firstselect,\n                   self.table_map[self.datatype]['fntable']))\n        sql = self.get_sql_joins_mergetable(sql, joins, self.datatype)\n        sql = '{} ORDER BY g.{}'.format(sql, featcol)\n        return sql, selectmap", "response": "Dynamically build the SQL query to generate the entries for the multi - set\n        merged protein and peptide tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wrap_iterable(obj):\n    was_scalar = not isiterable(obj)\n    wrapped_obj = [obj] if was_scalar else obj\n    return wrapped_obj, was_scalar", "response": "Wrap an iterable into a list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef itake_column(list_, colx):\n    if isinstance(colx, list):\n        # multi select\n        return ([row[colx_] for colx_ in colx] for row in list_)\n    else:\n        return (row[colx] for row in list_)", "response": "iterator version of get_list_column"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_window(iterable, size=2, step=1, wrap=False):\n    # it.tee may be slow, but works on all iterables\n    iter_list = it.tee(iterable, size)\n    if wrap:\n        # Secondary iterables need to be cycled for wraparound\n        iter_list = [iter_list[0]] + list(map(it.cycle, iter_list[1:]))\n    # Step each iterator the approprate number of times\n    try:\n        for count, iter_ in enumerate(iter_list[1:], start=1):\n            for _ in range(count):\n                six.next(iter_)\n    except StopIteration:\n        return iter(())\n    else:\n        _window_iter = zip(*iter_list)\n        # Account for the step size\n        window_iter = it.islice(_window_iter, 0, None, step)\n        return window_iter", "response": "r Returns an iterator over the items in a sequence of size with a generalization of itertwo\n "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to filter false items in the node tree", "response": "def ifilterfalse_items(item_iter, flag_iter):\n    \"\"\"\n    ifilterfalse_items\n\n    Args:\n        item_iter (list):\n        flag_iter (list): of bools\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_iter import *  # NOQA\n        >>> item_iter = [1, 2, 3, 4, 5]\n        >>> flag_iter = [False, True, True, False, True]\n        >>> false_items = ifilterfalse_items(item_iter, flag_iter)\n        >>> result = list(false_items)\n        >>> print(result)\n        [1, 4]\n    \"\"\"\n    false_items = (item for (item, flag) in zip(item_iter, flag_iter) if not flag)\n    return false_items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iter_multichunks(iterable, chunksizes, bordermode=None):\n    chunksize = reduce(operator.mul, chunksizes)\n    for chunk in ichunks(iterable, chunksize, bordermode=bordermode):\n        reshaped_chunk = chunk\n        for d in chunksizes[1:][::-1]:\n            reshaped_chunk = list(ichunks(reshaped_chunk, d))\n        yield reshaped_chunk", "response": "This function returns a generator that yields multichunks in the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ichunks(iterable, chunksize, bordermode=None):\n    if bordermode is None:\n        return ichunks_noborder(iterable, chunksize)\n    elif bordermode == 'cycle':\n        return ichunks_cycle(iterable, chunksize)\n    elif bordermode == 'replicate':\n        return ichunks_replicate(iterable, chunksize)\n    else:\n        raise ValueError('unknown bordermode=%r' % (bordermode,))", "response": "r This function generates successive n - sized chunks from an iterable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating over a list of chunks.", "response": "def ichunks_list(list_, chunksize):\n    \"\"\"\n    input must be a list.\n\n    SeeAlso:\n        ichunks\n\n    References:\n        http://stackoverflow.com/questions/434287/iterate-over-a-list-in-chunks\n    \"\"\"\n    return (list_[ix:ix + chunksize] for ix in range(0, len(list_), chunksize))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef random_product(items, num=None, rng=None):\n    import utool as ut\n    rng = ut.ensure_rng(rng, 'python')\n    seen = set()\n    items = [list(g) for g in items]\n    max_num = ut.prod(map(len, items))\n    if num is None:\n        num = max_num\n    if num > max_num:\n        raise ValueError('num exceedes maximum number of products')\n\n    # TODO: make this more efficient when num is large\n    if num > max_num // 2:\n        for prod in ut.shuffle(list(it.product(*items)), rng=rng):\n            yield prod\n    else:\n        while len(seen) < num:\n            # combo = tuple(sorted(rng.choice(items, size, replace=False)))\n            idxs = tuple(rng.randint(0, len(g) - 1) for g in items)\n            if idxs not in seen:\n                seen.add(idxs)\n                prod = tuple(g[x] for g, x in zip(items, idxs))\n                yield prod", "response": "Yields num items from the cartesian product of items in a random order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef random_combinations(items, size, num=None, rng=None):\n    import scipy.misc\n    import numpy as np\n    import utool as ut\n    rng = ut.ensure_rng(rng, impl='python')\n    num_ = np.inf if num is None else num\n    # Ensure we dont request more than is possible\n    n_max = int(scipy.misc.comb(len(items), size))\n    num_ = min(n_max, num_)\n    if num is not None and num_ > n_max // 2:\n        # If num is too big just generate all combinations and shuffle them\n        combos = list(it.combinations(items, size))\n        rng.shuffle(combos)\n        for combo in combos[:num]:\n            yield combo\n    else:\n        # Otherwise yield randomly until we get something we havent seen\n        items = list(items)\n        combos = set()\n        while len(combos) < num_:\n            # combo = tuple(sorted(rng.choice(items, size, replace=False)))\n            combo = tuple(sorted(rng.sample(items, size)))\n            if combo not in combos:\n                # TODO: store indices instead of combo values\n                combos.add(combo)\n                yield combo", "response": "Returns num combinations of length size from items in random order"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a connection string and return the associated driver", "response": "def parse_dsn(dsn_string):\n    \"\"\"Parse a connection string and return the associated driver\"\"\"\n    dsn = urlparse(dsn_string)\n    scheme = dsn.scheme.split('+')[0]\n    username = password = host = port = None\n    host = dsn.netloc\n    if '@' in host:\n        username, host = host.split('@')\n        if ':' in username:\n            username, password = username.split(':')\n            password = unquote(password)\n        username = unquote(username)\n    if ':' in host:\n        host, port = host.split(':')\n        port = int(port)\n    database = dsn.path.split('?')[0][1:]\n    query = dsn.path.split('?')[1] if '?' in dsn.path else dsn.query\n    kwargs = dict(parse_qsl(query, True))\n    if scheme == 'sqlite':\n        return SQLiteDriver, [dsn.path], {}\n    elif scheme == 'mysql':\n        kwargs['user'] = username or 'root'\n        kwargs['db'] = database\n        if port:\n            kwargs['port'] = port\n        if host:\n            kwargs['host'] = host\n        if password:\n            kwargs['passwd'] = password\n        return MySQLDriver, [], kwargs\n    elif scheme == 'postgresql':\n        kwargs['user'] = username or 'postgres'\n        kwargs['database'] = database\n        if port:\n            kwargs['port'] = port\n        if 'unix_socket' in kwargs:\n            kwargs['host'] = kwargs.pop('unix_socket')\n        elif host:\n            kwargs['host'] = host\n        if password:\n            kwargs['password'] = password\n        return PostgreSQLDriver, [], kwargs\n    else:\n        raise ValueError('Unknown driver %s' % dsn_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the database for the given model.", "response": "def db_for_write(self, model, **hints):\n        \"\"\"\n        Prevent write actions on read-only tables.\n\n        Raises:\n            WriteNotSupportedError: If models.sf_access is ``read_only``.\n\n        \"\"\"\n        try:\n            if model.sf_access == READ_ONLY:\n                raise WriteNotSupportedError(\"%r is a read-only model.\" % model)\n        except AttributeError:\n            pass\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef partial(f, x, i):\n    result = f(*[AdFloat(x_j, j == i) for j, x_j in enumerate(x)])\n    return result.dx", "response": "Computes the partial derivative of f with respect to x_i\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef jacobian(f, x, param):\n\n    N = len(x)\n    M = len(param)\n    J = np.zeros((N, M))\n    for i, x_i in enumerate(x):\n        for j in range(M):\n            parameters=[]\n            for k, p_k in enumerate(param):\n                    parameters.append(AdFloat(p_k, k == j))\n            # parameters = [AdFloat(p_k, k == j) for k, p_k in enumerate(param)]\n            val = f(AdFloat(x_i,0), parameters)\n            J[i, j] = val.dx\n\n    return J", "response": "Calculates the jacobian matrix for d_i f"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a hook in all the plugins and invoke it with args and kwargs.", "response": "def run_hook(self, hook, *args, **kwargs):\n        \"\"\"\n        Loop over all plugins and invoke function `hook` with `args` and\n        `kwargs` in each of them. If the plugin does not have the function, it\n        is skipped.\n        \"\"\"\n        for plugin in self.raw_plugins:\n            if hasattr(plugin, hook):\n                self.logger.debug('Calling hook {0} in plugin {1}'.format(hook, plugin.__name__))\n                getattr(plugin, hook)(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_tsv(headerfields, features, outfn):\n    with open(outfn, 'w') as fp:\n        write_tsv_line_from_list(headerfields, fp)\n        for line in features:\n            write_tsv_line_from_list([str(line[field]) for field\n                                      in headerfields], fp)", "response": "Writes header and generator of lines to tab separated file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef theta_str(theta, taustr=TAUSTR, fmtstr='{coeff:,.1f}{taustr}'):\n    coeff = theta / TAU\n    theta_str = fmtstr.format(coeff=coeff, taustr=taustr)\n    return theta_str", "response": "r Returns a string representation of the theta in tau units"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bbox_str(bbox, pad=4, sep=', '):\n    if bbox is None:\n        return 'None'\n    fmtstr = sep.join(['%' + six.text_type(pad) + 'd'] * 4)\n    return '(' + fmtstr % tuple(bbox) + ')'", "response": "r makes a string from an integer bounding box"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef verts_str(verts, pad=1):\n    if verts is None:\n        return 'None'\n    fmtstr = ', '.join(['%' + six.text_type(pad) + 'd' +\n                        ', %' + six.text_type(pad) + 'd'] * 1)\n    return ', '.join(['(' + fmtstr % vert + ')' for vert in verts])", "response": "r makes a string from a list of integer verticies"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves all chars in char_list from str_", "response": "def remove_chars(str_, char_list):\n    \"\"\"\n    removes all chars in char_list from str_\n\n    Args:\n        str_ (str):\n        char_list (list):\n\n    Returns:\n        str: outstr\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_str import *  # NOQA\n        >>> str_ = '1, 2, 3, 4'\n        >>> char_list = [',']\n        >>> result = remove_chars(str_, char_list)\n        >>> print(result)\n        1 2 3 4\n    \"\"\"\n    outstr = str_[:]\n    for char in char_list:\n        outstr = outstr.replace(char, '')\n    return outstr"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_minimum_indentation(text):\n    lines = text.split('\\n')\n    indentations = [get_indentation(line_)\n                    for line_ in lines  if len(line_.strip()) > 0]\n    if len(indentations) == 0:\n        return 0\n    return min(indentations)", "response": "r Returns the minimum indentation of the text"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef textblock(multiline_text):\n    new_lines = list(map(flatten_textlines, multiline_text.split('\\n\\n')))\n    new_text = '\\n\\n'.join(new_lines)\n    return new_text", "response": "r Textblock function that returns a single string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef indentjoin(strlist, indent='\\n    ', suffix=''):\n    indent_ = indent\n    strlist = list(strlist)\n    if len(strlist) == 0:\n        return ''\n    return indent_ + indent_.join([six.text_type(str_) + suffix\n                                   for str_ in strlist])", "response": "r Used to join a list of strings with indent and suffix"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntruncates the middle part of any string over maxlen characters.", "response": "def truncate_str(str_, maxlen=110, truncmsg=' ~~~TRUNCATED~~~ '):\n    \"\"\"\n    Removes the middle part of any string over maxlen characters.\n    \"\"\"\n    if NO_TRUNCATE:\n        return str_\n    if maxlen is None or maxlen == -1 or len(str_) < maxlen:\n        return str_\n    else:\n        maxlen_ = maxlen - len(truncmsg)\n        lowerb  = int(maxlen_ * .8)\n        upperb  = maxlen_ - lowerb\n        tup = (str_[:lowerb], truncmsg, str_[-upperb:])\n        return ''.join(tup)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npacking a string into a new file", "response": "def packstr(instr, textwidth=160, breakchars=' ', break_words=True,\n            newline_prefix='', indentation='', nlprefix=None, wordsep=' ',\n            remove_newlines=True):\n    \"\"\" alias for pack_into. has more up to date kwargs \"\"\"\n    if not isinstance(instr, six.string_types):\n        instr = repr(instr)\n    if nlprefix is not None:\n        newline_prefix = nlprefix\n    str_ = pack_into(instr, textwidth, breakchars, break_words, newline_prefix,\n                     wordsep, remove_newlines)\n    if indentation != '':\n        str_ = indent(str_, indentation)\n    return str_"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef packtext(text, width=80):\n    import utool as ut\n    import textwrap\n    new_text = '\\n'.join(textwrap.wrap(text, width))\n    new_text = ut.remove_doublspaces(new_text).strip()\n    return new_text", "response": "r Packs a text into a single nparray tree"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef order_of_magnitude_str(num, base=10.0,\n                           prefix_list=None,\n                           exponent_list=None,\n                           suffix='', prefix=None):\n    \"\"\"\n    TODO: Rewrite byte_str to use this func\n    Returns:\n        str\n    \"\"\"\n    abs_num = abs(num)\n    # Find the right magnidue\n    for prefix_, exponent in zip(prefix_list, exponent_list):\n        # Let user request the prefix\n        requested = False\n        if prefix is not None:\n            if prefix != prefix_:\n                continue\n            requested = True\n        # Otherwise find the best prefix\n        magnitude = base ** exponent\n        # Be less than this threshold to use this unit\n        thresh_mag = magnitude * base\n        if requested or abs_num <= thresh_mag:\n            break\n    unit_str = _magnitude_str(abs_num, magnitude, prefix_, suffix)\n    return unit_str", "response": "Returns a string that is the order of the magnitude of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the bytes into a list of bytes", "response": "def parse_bytes(bytes_str):\n    \"\"\"\n    uint8_size = ut.parse_bytes('1B')\n    image_size = ut.parse_bytes('3.5MB')\n    float32_size = ut.parse_bytes('32bit')\n    desc_size = 128 * uint8_size\n    kpts_size = 6 * float32_size\n    chip_size = ut.parse_bytes('400 KB')\n    probchip_size = ut.parse_bytes('50 KB')\n    nImgs = 80000 # 80,000\n    nAnnots = nImgs * 2\n    desc_per_img = 3000\n    size_stats = {\n        'image': nImgs * image_size,\n        'chips': nAnnots * chip_size,\n        'probchips': nAnnots * probchip_size,\n        'desc': nAnnots * desc_size * desc_per_img,\n        'kpts': nAnnots * kpts_size * desc_per_img,\n    }\n    print(ut.repr3(ut.map_dict_vals(ut.byte_str2, size_stats), align=True))\n    print('total = ' + ut.byte_str2(sum(size_stats.values())))\n    \"\"\"\n    import utool as ut\n    import re\n    numstr = ut.named_field('num', r'\\d\\.?\\d*')\n    unitstr = ut.named_field('unit', r'[a-zA-Z]+')\n    match = re.match(numstr + ' *' + unitstr, bytes_str)\n    nUnits = float(match.groupdict()['num'])\n    unit = match.groupdict()['unit'].upper()\n    nBytes = get_bytes(nUnits, unit)\n    return nBytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef byte_str2(nBytes, precision=2):\n    nAbsBytes = abs(nBytes)\n    if nAbsBytes < 2.0 ** 10:\n        return byte_str(nBytes, 'KB', precision=precision)\n    if nAbsBytes < 2.0 ** 20:\n        return byte_str(nBytes, 'KB', precision=precision)\n    if nAbsBytes < 2.0 ** 30:\n        return byte_str(nBytes, 'MB', precision=precision)\n    if nAbsBytes < 2.0 ** 40:\n        return byte_str(nBytes, 'GB', precision=precision)\n    else:\n        return byte_str(nBytes, 'TB', precision=precision)", "response": "Prints out some bytes in a unicode string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string representation of the number of bytes with the chosen unit", "response": "def byte_str(nBytes, unit='bytes', precision=2):\n    \"\"\"\n    representing the number of bytes with the chosen unit\n\n    Returns:\n        str\n    \"\"\"\n    #return (nBytes * ureg.byte).to(unit.upper())\n    if unit.lower().startswith('b'):\n        nUnit = nBytes\n    elif unit.lower().startswith('k'):\n        nUnit =  nBytes / (2.0 ** 10)\n    elif unit.lower().startswith('m'):\n        nUnit =  nBytes / (2.0 ** 20)\n    elif unit.lower().startswith('g'):\n        nUnit = nBytes / (2.0 ** 30)\n    elif unit.lower().startswith('t'):\n        nUnit = nBytes / (2.0 ** 40)\n    else:\n        raise NotImplementedError('unknown nBytes=%r unit=%r' % (nBytes, unit))\n    return repr2(nUnit, precision=precision) + ' ' + unit"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to get a string representation of a function definition", "response": "def func_str(func, args=[], kwargs={}, type_aliases=[], packed=False,\n             packkw=None, truncate=False):\n    \"\"\"\n    string representation of function definition\n\n    Returns:\n        str: a representation of func with args, kwargs, and type_aliases\n\n    Args:\n        func (function):\n        args (list): argument values (default = [])\n        kwargs (dict): kwargs values (default = {})\n        type_aliases (list): (default = [])\n        packed (bool): (default = False)\n        packkw (None): (default = None)\n\n    Returns:\n        str: func_str\n\n    CommandLine:\n        python -m utool.util_str --exec-func_str\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_str import *  # NOQA\n        >>> func = byte_str\n        >>> args = [1024, 'MB']\n        >>> kwargs = dict(precision=2)\n        >>> type_aliases = []\n        >>> packed = False\n        >>> packkw = None\n        >>> _str = func_str(func, args, kwargs, type_aliases, packed, packkw)\n        >>> result = _str\n        >>> print(result)\n        byte_str(1024, 'MB', precision=2)\n    \"\"\"\n    import utool as ut\n    # if truncate:\n    # truncatekw = {'maxlen': 20}\n    # else:\n    truncatekw = {}\n\n    argrepr_list = ([] if args is None else\n                    ut.get_itemstr_list(args, nl=False, truncate=truncate,\n                                        truncatekw=truncatekw))\n    kwrepr_list = ([] if kwargs is None else\n                   ut.dict_itemstr_list(kwargs, explicit=True, nl=False,\n                                        truncate=truncate,\n                                        truncatekw=truncatekw))\n    repr_list = argrepr_list + kwrepr_list\n\n    argskwargs_str = ', '.join(repr_list)\n    _str = '%s(%s)' % (meta_util_six.get_funcname(func), argskwargs_str)\n    if packed:\n        packkw_ = dict(textwidth=80, nlprefix='    ', break_words=False)\n        if packkw is not None:\n            packkw_.update(packkw_)\n        _str = packstr(_str, **packkw_)\n    return _str"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string of function definition signature", "response": "def func_defsig(func, with_name=True):\n    \"\"\"\n    String of function definition signature\n\n    Args:\n        func (function): live python function\n\n    Returns:\n        str: defsig\n\n    CommandLine:\n        python -m utool.util_str --exec-func_defsig\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_str import *  # NOQA\n        >>> func = func_str\n        >>> defsig = func_defsig(func)\n        >>> result = str(defsig)\n        >>> print(result)\n        func_str(func, args=[], kwargs={}, type_aliases=[], packed=False, packkw=None, truncate=False)\n    \"\"\"\n    import inspect\n    argspec = inspect.getargspec(func)\n    (args, varargs, varkw, defaults) = argspec\n    defsig = inspect.formatargspec(*argspec)\n    if with_name:\n        defsig = get_callable_name(func) + defsig\n    return defsig"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef func_callsig(func, with_name=True):\n    import inspect\n    argspec = inspect.getargspec(func)\n    (args, varargs, varkw, defaults) = argspec\n    callsig = inspect.formatargspec(*argspec[0:3])\n    if with_name:\n        callsig = get_callable_name(func) + callsig\n    return callsig", "response": "Function to call the function with signature"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef numpy_str(arr, strvals=False, precision=None, pr=None,\n              force_dtype=False,\n              with_dtype=None, suppress_small=None, max_line_width=None,\n              threshold=None, **kwargs):\n    \"\"\"\n    suppress_small = False turns off scientific representation\n    \"\"\"\n    # strvals = kwargs.get('strvals', False)\n    itemsep = kwargs.get('itemsep', ' ')\n    # precision = kwargs.get('precision', None)\n    # suppress_small = kwargs.get('supress_small', None)\n    # max_line_width = kwargs.get('max_line_width', None)\n    # with_dtype = kwargs.get('with_dtype', False)\n    newlines = kwargs.pop('nl', kwargs.pop('newlines', 1))\n    data = arr\n\n    # if with_dtype and strvals:\n    #     raise ValueError('cannot format with strvals and dtype')\n\n    separator = ',' + itemsep\n\n    if strvals:\n        prefix = ''\n        suffix = ''\n    else:\n        modname = type(data).__module__\n        # substitute shorthand for numpy module names\n        np_nice = 'np'\n        modname = re.sub('\\\\bnumpy\\\\b', np_nice, modname)\n        modname = re.sub('\\\\bma.core\\\\b', 'ma', modname)\n\n        class_name = type(data).__name__\n        if class_name == 'ndarray':\n            class_name = 'array'\n\n        prefix = modname + '.' + class_name + '('\n\n        if with_dtype:\n            dtype_repr = data.dtype.name\n            # dtype_repr = np.core.arrayprint.dtype_short_repr(data.dtype)\n            suffix = ',{}dtype={}.{})'.format(itemsep, np_nice, dtype_repr)\n        else:\n            suffix = ')'\n\n    if not strvals and data.size == 0 and data.shape != (0,):\n        # Special case for displaying empty data\n        prefix = modname + '.empty('\n        body = repr(tuple(map(int, data.shape)))\n    else:\n        body = np.array2string(data, precision=precision,\n                               separator=separator,\n                               suppress_small=suppress_small,\n                               prefix=prefix,\n                               max_line_width=max_line_width)\n    if not newlines:\n        # remove newlines if we need to\n        body = re.sub('\\n *', '', body)\n    formatted = prefix + body + suffix\n    return formatted", "response": "Return a string representation of a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string that can be used to summarize a list of items.", "response": "def list_str_summarized(list_, list_name, maxlen=5):\n    \"\"\"\n    prints the list members when the list is small and the length when it is\n    large\n    \"\"\"\n    if len(list_) > maxlen:\n        return 'len(%s)=%d' % (list_name, len(list_))\n    else:\n        return '%s=%r' % (list_name, list_)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _rectify_countdown_or_bool(count_or_bool):\n    if count_or_bool is True or count_or_bool is False:\n        count_or_bool_ = count_or_bool\n    elif isinstance(count_or_bool, int):\n        if count_or_bool == 0:\n            return 0\n        sign_ =  math.copysign(1, count_or_bool)\n        count_or_bool_ = int(count_or_bool - sign_)\n        #if count_or_bool_ == 0:\n        #    return sign_ == 1\n    else:\n        count_or_bool_ = False\n    return count_or_bool_", "response": "This function will count down or count up a single entry in a resource tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef repr2(obj_, **kwargs):\n    kwargs['nl'] = kwargs.pop('nl', kwargs.pop('newlines', False))\n    val_str = _make_valstr(**kwargs)\n    return val_str(obj_)", "response": "Return a pretty version of obj_."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a python object to json", "response": "def repr2_json(obj_, **kwargs):\n    \"\"\" hack for json reprs \"\"\"\n    import utool as ut\n    kwargs['trailing_sep'] = False\n    json_str = ut.repr2(obj_, **kwargs)\n    json_str = str(json_str.replace('\\'', '\"'))\n    json_str = json_str.replace('(', '[')\n    json_str = json_str.replace(')', ']')\n    json_str = json_str.replace('None', 'null')\n    return json_str"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict_str(dict_, **dictkw):\n    import utool as ut\n\n    stritems = dictkw.pop('si', dictkw.pop('stritems', False))\n    if stritems:\n        dictkw['strkeys'] = True\n        dictkw['strvals'] = True\n\n    dictkw['strkeys'] = dictkw.pop('sk', dictkw.pop('strkeys', False))\n    dictkw['strvals'] = dictkw.pop('sv', dictkw.pop('strvals', False))\n\n    newlines = dictkw.pop('nl', dictkw.pop('newlines', True))\n    truncate = dictkw.pop('truncate', False)\n    dictkw['nl'] = _rectify_countdown_or_bool(newlines)\n    dictkw['truncate'] = _rectify_countdown_or_bool(truncate)\n\n    nobraces = dictkw.pop('nobr', dictkw.pop('nobraces', False))\n    align = dictkw.pop('align', False)\n\n    # Doesn't actually put in trailing comma if on same line\n    trailing_sep = dictkw.get('trailing_sep', True)\n    explicit = dictkw.get('explicit', False)\n    with_comma  = True\n    itemsep = dictkw.get('itemsep', ' ')\n\n    if len(dict_) == 0:\n        return 'dict()' if explicit else '{}'\n\n    itemstr_list = dict_itemstr_list(dict_, **dictkw)\n\n    do_truncate = truncate is not False and (truncate is True or truncate == 0)\n    if do_truncate:\n        truncatekw = dictkw.get('truncatekw', {})\n        itemstr_list = [truncate_str(item, **truncatekw) for item in itemstr_list]\n\n    if nobraces:\n        lbr, rbr = '', ''\n    elif explicit:\n        lbr, rbr = 'dict(', ')'\n    else:\n        lbr, rbr = '{', '}'\n\n    if newlines:\n        sep = ',\\n' if with_comma else '\\n'\n        if nobraces:\n            retstr =  sep.join(itemstr_list)\n            if trailing_sep:\n                retstr += ','\n        else:\n            parts = [ut.indent(itemstr, '    ') for itemstr in itemstr_list]\n            body_str = sep.join(parts)\n            if trailing_sep:\n                body_str += ','\n            retstr =  (lbr + '\\n' + body_str + '\\n' + rbr)\n            if align:\n                retstr = ut.align(retstr, ':')\n    else:\n        sep = ',' + itemsep if with_comma else itemsep\n        # hack away last trailing comma\n        sequence_str = sep.join(itemstr_list)\n        retstr = lbr +  sequence_str + rbr\n    # Is there a way to make truncate for dict_str compatible with list_str?\n    return retstr", "response": "r Converts a dictionary into a pretty printable string representation of a\nTaxonomy dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dict_itemstr_list(dict_, **dictkw):\n    import utool as ut\n\n    explicit = dictkw.get('explicit', False)\n    dictkw['explicit'] = _rectify_countdown_or_bool(explicit)\n\n    dosort = dictkw.get('sorted_', None)\n    if dosort is None:\n        dosort = True\n\n    if dosort and not isinstance(dict_, collections.OrderedDict):\n        key_order = dictkw.get('key_order', None)\n        def iteritems(d):\n            if key_order is None:\n                # specify order explicilty\n                try:\n                    return iter(sorted(six.iteritems(d)))\n                except TypeError:\n                    # catches case where keys are of different types\n                    return six.iteritems(d)\n            else:\n                # Enforce specific key order\n                # TODO: depricate and just use ordered dicts\n                unordered_keys = list(d.keys())\n                other_keys = sorted(list(set(unordered_keys) - set(key_order)))\n                keys = key_order + other_keys\n                return ((key, d[key]) for key in keys)\n    else:\n        iteritems = six.iteritems\n\n    _valstr = _make_valstr(**dictkw)\n\n    precision = dictkw.get('precision', None)\n    kvsep = dictkw.get('kvsep', ': ')\n    if explicit:\n        kvsep = '='\n\n    def make_item_str(key, val):\n        if explicit or dictkw.get('strkeys', False):\n            key_str = six.text_type(key)\n        else:\n            key_str = repr2(key, precision=precision)\n\n        prefix = key_str + kvsep\n        val_str = _valstr(val)\n\n        # FIXME: get indentation right\n        if util_type.HAVE_NUMPY and isinstance(val, np.ndarray):\n            item_str = hz_str(prefix, val_str)\n        else:\n            # padded_indent = ' ' * min(len(indent_), len(prefix))\n            # val_str = val_str.replace('\\n', '\\n' + padded_indent)\n            item_str = prefix + val_str\n        return item_str\n\n    itemstr_list = [make_item_str(key, val)\n                    for (key, val) in iteritems(dict_)]\n\n    reverse = False\n    key_order_metric = dictkw.get('key_order_metric', None)\n    if key_order_metric is not None:\n        if key_order_metric.startswith('-'):\n            key_order_metric = key_order_metric[1:]\n            reverse = True\n\n    if key_order_metric == 'strlen':\n        metric_list = [len(itemstr) for itemstr in itemstr_list]\n        itemstr_list = ut.sortedby(itemstr_list, metric_list, reverse=reverse)\n    elif key_order_metric == 'val':\n        metric_list = [val for (key, val) in iteritems(dict_)]\n        itemstr_list = ut.sortedby(itemstr_list, metric_list, reverse=reverse)\n\n    maxlen = dictkw.get('maxlen', None)\n    if maxlen is not None and len(itemstr_list) > maxlen:\n        itemstr_list = itemstr_list[0:maxlen]\n    return itemstr_list", "response": "r Returns a list of human - readable items from a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_str(list_, **listkw):\n    import utool as ut\n\n    newlines = listkw.pop('nl', listkw.pop('newlines', 1))\n    packed = listkw.pop('packed', False)\n    truncate = listkw.pop('truncate', False)\n    listkw['nl'] = _rectify_countdown_or_bool(newlines)\n    listkw['truncate'] = _rectify_countdown_or_bool(truncate)\n    listkw['packed'] = _rectify_countdown_or_bool(packed)\n\n    nobraces = listkw.pop('nobr', listkw.pop('nobraces', False))\n\n    itemsep = listkw.get('itemsep', ' ')\n    # Doesn't actually put in trailing comma if on same line\n    trailing_sep = listkw.get('trailing_sep', True)\n    with_comma = True\n\n    itemstr_list = get_itemstr_list(list_, **listkw)\n    is_tuple = isinstance(list_, tuple)\n    is_set = isinstance(list_, (set, frozenset, ut.oset))\n    is_onetup = isinstance(list_, (tuple)) and len(list_) <= 1\n    if nobraces:\n        lbr, rbr = '', ''\n    elif is_tuple:\n        lbr, rbr  = '(', ')'\n    elif is_set:\n        lbr, rbr  = '{', '}'\n    else:\n        lbr, rbr  = '[', ']'\n\n    if len(itemstr_list) == 0:\n        newlines = False\n\n    if newlines is not False and (newlines is True or newlines > 0):\n        sep = ',\\n' if with_comma else '\\n'\n        if nobraces:\n            body_str = sep.join(itemstr_list)\n            if trailing_sep:\n                body_str += ','\n            retstr = body_str\n        else:\n            if packed:\n                # DEPRICATE?\n                joinstr = sep + itemsep * len(lbr)\n                body_str = joinstr.join([itemstr for itemstr in itemstr_list])\n                if trailing_sep:\n                    body_str += ','\n                braced_body_str = (lbr + '' + body_str + '' + rbr)\n            else:\n                body_str = sep.join([\n                    ut.indent(itemstr) for itemstr in itemstr_list])\n                if trailing_sep:\n                    body_str += ','\n                braced_body_str = (lbr + '\\n' + body_str + '\\n' + rbr)\n            retstr = braced_body_str\n    else:\n        sep = ',' + itemsep if with_comma else itemsep\n        body_str = sep.join(itemstr_list)\n        if is_onetup:\n            body_str += ','\n        retstr  = (lbr + body_str +  rbr)\n\n    # TODO: rectify with dict_truncate\n    do_truncate = truncate is not False and (truncate is True or truncate == 0)\n    if do_truncate:\n        truncatekw = listkw.get('truncatekw', {})\n        retstr = truncate_str(retstr, **truncatekw)\n    return retstr", "response": "r Returns a pretty list of the names of the items in the list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a list of itemstr strings for a node in the order they appear in the tree.", "response": "def get_itemstr_list(list_, **listkw):\n    \"\"\"\n    TODO: have this replace dict_itemstr list or at least most functionality in\n    it. have it make two itemstr lists over keys and values and then combine\n    them.\n    \"\"\"\n    import utool as ut\n    _valstr = _make_valstr(**listkw)\n\n    def make_item_str(item):\n        item_str = _valstr(item)\n        return item_str\n\n    items = list(list_)\n    itemstr_list = [make_item_str(item) for item in items]\n\n    dosort = listkw.get('sorted_', None)\n    if dosort is None:\n        # Force orderings on sets.\n        dosort = isinstance(list_, (set, frozenset))\n    if dosort:\n        # First try to sort items by their normal values\n        # If that doesnt work, then sort by their string values\n        try:\n            # Set ordering is not unique. Sort by strings values instead.\n            if _peek_isinstance(items, (set, frozenset)):\n                raise Exception\n            sortx = ut.argsort2(items)\n        except Exception:\n            sortx = ut.argsort2(itemstr_list)\n        itemstr_list = ut.take(itemstr_list, sortx)\n    return itemstr_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef horiz_string(*args, **kwargs):\n    import unicodedata\n\n    precision = kwargs.get('precision', None)\n    sep = kwargs.get('sep', '')\n\n    if len(args) == 1 and not isinstance(args[0], six.string_types):\n        val_list = args[0]\n    else:\n        val_list = args\n\n    val_list = [unicodedata.normalize('NFC', ensure_unicode(val))\n                for val in val_list]\n    all_lines = []\n    hpos = 0\n    # for each value in the list or args\n    for sx in range(len(val_list)):\n        # Ensure value is a string\n        val = val_list[sx]\n        str_ = None\n        if precision is not None:\n            # Hack in numpy precision\n            if util_type.HAVE_NUMPY:\n                try:\n                    if isinstance(val, np.ndarray):\n                        str_ = np.array_str(val, precision=precision,\n                                            suppress_small=True)\n                except ImportError:\n                    pass\n        if str_ is None:\n            str_ = six.text_type(val_list[sx])\n        # continue with formating\n        lines = str_.split('\\n')\n        line_diff = len(lines) - len(all_lines)\n        # Vertical padding\n        if line_diff > 0:\n            all_lines += [' ' * hpos] * line_diff\n        # Add strings\n        for lx, line in enumerate(lines):\n            all_lines[lx] += line\n            hpos = max(hpos, len(all_lines[lx]))\n        # Horizontal padding\n        for lx in range(len(all_lines)):\n            hpos_diff = hpos - len(all_lines[lx])\n            all_lines[lx] += ' ' * hpos_diff + sep\n    all_lines = [line.rstrip(' ') for line in all_lines]\n    ret = '\\n'.join(all_lines)\n    return ret", "response": "Horizontally concatenates strings reprs preserving indentation and prints the result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_callable_name(func):\n    try:\n        return meta_util_six.get_funcname(func)\n    except AttributeError:\n        if isinstance(func, type):\n            return repr(func).replace('<type \\'', '').replace('\\'>', '')\n        elif hasattr(func, '__name__'):\n            return func.__name__\n        else:\n            raise NotImplementedError(('cannot get func_name of func=%r'\n                                        'type(func)=%r') % (func, type(func)))", "response": "Returns the name of the callable object"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align(text, character='=', replchar=None, pos=0):\n    line_list = text.splitlines()\n    new_lines = align_lines(line_list, character, replchar, pos=pos)\n    new_text = '\\n'.join(new_lines)\n    return new_text", "response": "r Aligns text on the left side of a character at a given position"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef align_lines(line_list, character='=', replchar=None, pos=0):\n\n    # FIXME: continue to fix ansi\n    if pos is None:\n        # Align all occurences\n        num_pos = max([line.count(character) for line in line_list])\n        pos = list(range(num_pos))\n\n    # Allow multiple alignments\n    if isinstance(pos, list):\n        pos_list = pos\n        # recursive calls\n        new_lines = line_list\n        for pos in pos_list:\n            new_lines = align_lines(new_lines, character=character,\n                                    replchar=replchar, pos=pos)\n        return new_lines\n\n    # base case\n    if replchar is None:\n        replchar = character\n\n    # the pos-th character to align\n    lpos = pos\n    rpos = lpos + 1\n\n    tup_list = [line.split(character) for line in line_list]\n\n    handle_ansi = True\n    if handle_ansi:\n        # Remove ansi from length calculation\n        # References: http://stackoverflow.com/questions/14693701remove-ansi\n        ansi_escape = re.compile(r'\\x1b[^m]*m')\n\n    # Find how much padding is needed\n    maxlen = 0\n    for tup in tup_list:\n        if len(tup) >= rpos + 1:\n            if handle_ansi:\n                tup = [ansi_escape.sub('', x) for x in tup]\n            left_lenlist = list(map(len, tup[0:rpos]))\n            left_len = sum(left_lenlist) + lpos * len(replchar)\n            maxlen = max(maxlen, left_len)\n\n    # Pad each line to align the pos-th occurence of the chosen character\n    new_lines = []\n    for tup in tup_list:\n        if len(tup) >= rpos + 1:\n            lhs = character.join(tup[0:rpos])\n            rhs = character.join(tup[rpos:])\n            # pad the new line with requested justification\n            newline = lhs.ljust(maxlen) + replchar + rhs\n            new_lines.append(newline)\n        else:\n            new_lines.append(replchar.join(tup))\n    return new_lines", "response": "r aligns a list of lines of text on the left side of a character"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef long_fname_format(fmt_str, fmt_dict, hashable_keys=[], max_len=64,\n                      hashlen=16, ABS_MAX_LEN=255, hack27=False):\n    r\"\"\"\n    DEPRICATE\n\n    Formats a string and hashes certain parts if the resulting string becomes\n    too long. Used for making filenames fit onto disk.\n\n    Args:\n        fmt_str (str): format of fname\n        fmt_dict (str): dict to format fname with\n        hashable_keys (list): list of dict keys you are willing to have hashed\n        max_len (int): tries to fit fname into this length\n        ABS_MAX_LEN (int): throws AssertionError if fname over this length\n\n    CommandLine:\n        python -m utool.util_str --exec-long_fname_format\n\n    Example:\n        >>> # ENABLE_DOCTET\n        >>> import utool as ut\n        >>> fmt_str = 'qaid={qaid}_res_{cfgstr}_quuid={quuid}'\n        >>> quuid_str = 'blahblahblahblahblahblah'\n        >>> cfgstr = 'big_long_string__________________________________'\n        >>> qaid = 5\n        >>> fmt_dict = dict(cfgstr=cfgstr, qaid=qaid, quuid=quuid_str)\n        >>> hashable_keys = ['cfgstr', 'quuid']\n        >>> max_len = 64\n        >>> hashlen = 8\n        >>> fname0 = ut.long_fname_format(fmt_str, fmt_dict, max_len=None)\n        >>> fname1 = ut.long_fname_format(fmt_str, fmt_dict, hashable_keys,\n        >>>                                  max_len=64, hashlen=8)\n        >>> fname2 = ut.long_fname_format(fmt_str, fmt_dict, hashable_keys, max_len=42,\n        >>>                         hashlen=8)\n        >>> result = fname0 + '\\n' + fname1 + '\\n' + fname2\n        >>> print(result)\n        qaid=5_res_big_long_string___________________________________quuid=blahblahblahblahblahblah\n        qaid=5_res_racfntgq_quuid=blahblahblahblahblahblah\n        qaid=5_res_racfntgq_quuid=yvuaffrp\n    \"\"\"\n    from utool import util_hash\n    fname = fmt_str.format(**fmt_dict)\n    if max_len is None:\n        return fname\n    if len(fname) > max_len:\n        # Copy because we will overwrite fmt_dict values with hashed values\n        fmt_dict_ = fmt_dict.copy()\n        for key in hashable_keys:\n            if hack27:\n                fmt_dict_[key] = util_hash.hashstr27(fmt_dict_[key], hashlen=hashlen)\n            else:\n                fmt_dict_[key] = util_hash.hashstr(fmt_dict_[key], hashlen=hashlen)\n            fname = fmt_str.format(**fmt_dict_)\n            if len(fname) <= max_len:\n                break\n        if len(fname) > max_len:\n            diff = len(fname) - max_len\n            msg = ('[util_str] Warning: Too big by %d chars. Exausted all options'\n                   'to make fname fit into size. ')  % (diff,)\n            print(msg)\n            print('* len(fname) = %r' % len(fname))\n            print('* fname = %r' % fname)\n            if ABS_MAX_LEN is not None and len(fname) > ABS_MAX_LEN:\n                raise AssertionError(msg)\n    return fname", "response": "r Formats a string and hashes certain parts if the resulting string becomes too long."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multi_replace(str_, search_list, repl_list):\n    if isinstance(repl_list, six.string_types):\n        repl_list_ = [repl_list] * len(search_list)\n    else:\n        repl_list_ = repl_list\n    newstr = str_\n    assert len(search_list) == len(repl_list_), 'bad lens'\n    for search, repl in zip(search_list, repl_list_):\n        newstr = newstr.replace(search, repl)\n    return newstr", "response": "r Performs multiple replace functions for item in search_list and repl_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef msgblock(key, text, side='|'):\n    blocked_text = ''.join(\n        [' + --- ', key, ' ---\\n'] +\n        [' ' + side + ' ' + line + '\\n' for line in text.split('\\n')] +\n        [' L ___ ', key, ' ___\\n']\n    )\n    return blocked_text", "response": "puts text inside a visual ascii block"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef number_text_lines(text):\n    numbered_linelist = [\n        ''.join((('%2d' % (count + 1)), ' >>> ', line))\n        for count, line in enumerate(text.splitlines())\n    ]\n    text_with_lineno = '\\n'.join(numbered_linelist)\n    return text_with_lineno", "response": "r Numberes the lines of a text string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_textdiff(text1, text2, num_context_lines=0, ignore_whitespace=False):\n    import difflib\n    text1 = ensure_unicode(text1)\n    text2 = ensure_unicode(text2)\n    text1_lines = text1.splitlines()\n    text2_lines = text2.splitlines()\n    if ignore_whitespace:\n        text1_lines = [t.rstrip() for t in text1_lines]\n        text2_lines = [t.rstrip() for t in text2_lines]\n        ndiff_kw = dict(linejunk=difflib.IS_LINE_JUNK,\n                        charjunk=difflib.IS_CHARACTER_JUNK)\n    else:\n        ndiff_kw = {}\n    all_diff_lines = list(difflib.ndiff(text1_lines, text2_lines, **ndiff_kw))\n\n    if num_context_lines is None:\n        diff_lines = all_diff_lines\n    else:\n        from utool import util_list\n        # boolean for every line if it is marked or not\n        ismarked_list = [len(line) > 0 and line[0] in '+-?'\n                         for line in all_diff_lines]\n        # flag lines that are within num_context_lines away from a diff line\n        isvalid_list = ismarked_list[:]\n        for i in range(1, num_context_lines + 1):\n            isvalid_list[:-i] = util_list.or_lists(isvalid_list[:-i],\n                                                   ismarked_list[i:])\n            isvalid_list[i:]  = util_list.or_lists(isvalid_list[i:],\n                                                   ismarked_list[:-i])\n        USE_BREAK_LINE = True\n        if USE_BREAK_LINE:\n            # insert a visual break when there is a break in context\n            diff_lines = []\n            prev = False\n            visual_break = '\\n <... FILTERED CONTEXT ...> \\n'\n            #print(isvalid_list)\n            for line, valid in zip(all_diff_lines, isvalid_list):\n                if valid:\n                    diff_lines.append(line)\n                elif prev:\n                    if False:\n                        diff_lines.append(visual_break)\n                prev = valid\n        else:\n            diff_lines = util_list.compress(all_diff_lines, isvalid_list)\n    return '\\n'.join(diff_lines)", "response": "r Returns a string that is the difference between two texts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef conj_phrase(list_, cond='or'):\n    if len(list_) == 0:\n        return ''\n    elif len(list_) == 1:\n        return list_[0]\n    elif len(list_) == 2:\n        return ' '.join((list_[0], cond, list_[1]))\n    else:\n        condstr = ''.join((', ' + cond, ' '))\n        return ', '.join((', '.join(list_[:-2]), condstr.join(list_[-2:])))", "response": "Joins a list of words using English conjunction rules"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unformat_text_as_docstr(formated_text):\n    import utool as ut\n    import re\n    min_indent = ut.get_minimum_indentation(formated_text)\n    indent_ =  ' ' * min_indent\n    unformated_text = re.sub('^' + indent_ + '>>> ', '' + indent_,\n                             formated_text, flags=re.MULTILINE)\n    return unformated_text", "response": "r Unformat the text of a cluster node as a docstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_title_caps(underscore_case):\n    words = underscore_case.split('_')\n    words2 = [\n        word[0].upper() + word[1:]\n        for count, word in enumerate(words)\n    ]\n    title_str = ' '.join(words2)\n    return title_str", "response": "r Converts the underscore_case to title_caps"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_underscore_case(camelcase_str):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', camelcase_str)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "r Converts a camelcase string to underscore_str"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_camel_case(underscore_case, mixed=False):\n    thresh = 0 if mixed else -1\n    words = underscore_case.split('_')\n    words2 = [\n        word[0].upper() + word[1:]\n        if count > thresh else\n        word\n        for count, word in enumerate(words)\n    ]\n    camel_case_str = ''.join(words2)\n    return camel_case_str", "response": "r Converts a string from the underscore_case to the camel_case_str"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef autoformat_pep8(sourcecode, **kwargs):\n    import autopep8\n    default_ignore = {\n        'E126',  # continuation line hanging-indent\n        'E127',  # continuation line over-indented for visual indent\n        'E201',  # whitespace after '('\n        'E202',  # whitespace before ']'\n        'E203',  # whitespace before ', '\n        'E221',  # multiple spaces before operator\n        'E222',  # multiple spaces after operator\n        'E241',  # multiple spaces after ,\n        'E265',  # block comment should start with \"# \"\n        'E271',  # multiple spaces after keyword\n        'E272',  # multiple spaces before keyword\n        'E301',  # expected 1 blank line, found 0\n        'E501',  # line length > 79\n        'W602',  # Old reraise syntax\n        'E266',  # too many leading '#' for block comment\n        'N801',  # function name should be lowercase [N806]\n        'N802',  # function name should be lowercase [N806]\n        'N803',  # argument should be lowercase [N806]\n        'N805',  # first argument of a method should be named 'self'\n        'N806',  # variable in function should be lowercase [N806]\n        'N811',  # constant name imported as non constant\n        'N813',  # camel case\n    }\n    # My defaults\n    kwargs['ignore'] = kwargs.get('ignore', default_ignore)\n    kwargs['aggressive'] = kwargs.get('aggressive', 1)\n    pep8_options = autopep8._get_options(kwargs, False)\n    new_source = autopep8.fix_code(sourcecode, pep8_options)\n    return new_source", "response": "r Automatically formats the source code for PEP - 8."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef chr_range(*args, **kw):\n    if len(args) == 1:\n        stop, = args\n        start, step = 0, 1\n    elif len(args) == 2:\n        start, stop = args\n        step = 1\n    elif len(args) == 3:\n        start, stop, step = args\n    else:\n        raise ValueError('incorrect args')\n\n    chr_ = six.unichr\n\n    base = ord(kw.get('base', 'a'))\n    if isinstance(start, int):\n        start = base + start\n    if isinstance(stop, int):\n        stop = base + stop\n\n    if isinstance(start, six.string_types):\n        start = ord(start)\n    if isinstance(stop, six.string_types):\n        stop = ord(stop)\n    if step is None:\n        step = 1\n    list_ = list(map(six.text_type, map(chr_, range(start, stop, step))))\n    return list_", "response": "r Like range but returns characters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef color_text(text, color):\n    import utool as ut\n    if color is None or not ENABLE_COLORS:\n        return text\n    elif color == 'python':\n        return highlight_text(text, color)\n    elif color == 'sql':\n        return highlight_text(text, 'sql')\n    try:\n        import pygments\n        import pygments.console\n        # if color == 'guess':\n        #     import linguist  # NOQA\n        #     pygments.lexers.guess_lexer(text)\n        #     return highlight_text(text, color)\n        ansi_text = pygments.console.colorize(color, text)\n        if ut.WIN32:\n            import colorama\n            ansi_reset = (colorama.Style.RESET_ALL)\n        else:\n            ansi_reset = pygments.console.colorize('reset', '')\n        ansi_text = ansi_text + ansi_reset\n        return ansi_text\n    except ImportError:\n        return text", "response": "r Colorize text using pygments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhighlight a string by regex pattern.", "response": "def highlight_regex(str_, pat, reflags=0, color='red'):\n    \"\"\"\n    FIXME Use pygments instead\n    \"\"\"\n    #import colorama\n    # from colorama import Fore, Style\n    #color = Fore.MAGENTA\n    # color = Fore.RED\n    #match = re.search(pat, str_, flags=reflags)\n    matches = list(re.finditer(pat, str_, flags=reflags))\n\n    colored = str_\n\n    for match in reversed(matches):\n        #pass\n        #if match is None:\n        #    return str_\n        #else:\n        start = match.start()\n        end = match.end()\n        #colorama.init()\n        colored_part = color_text(colored[start:end], color)\n        colored = colored[:start] + colored_part + colored[end:]\n        # colored = (colored[:start] + color + colored[start:end] +\n        #            Style.RESET_ALL + colored[end:])\n        #colorama.deinit()\n    return colored"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhighlighting a string by a set of patterns.", "response": "def highlight_multi_regex(str_, pat_to_color, reflags=0):\n    \"\"\"\n    FIXME Use pygments instead. must be mututally exclusive\n    \"\"\"\n    #import colorama\n    # from colorama import Fore, Style\n    #color = Fore.MAGENTA\n    # color = Fore.RED\n    #match = re.search(pat, str_, flags=reflags)\n\n    colored = str_\n\n    to_replace = []\n    for pat, color in pat_to_color.items():\n        matches = list(re.finditer(pat, str_, flags=reflags))\n\n        for match in matches:\n            start = match.start()\n            end = match.end()\n            to_replace.append((end, start, color))\n\n    for tup in reversed(sorted(to_replace)):\n        end, start, color = tup\n        colored_part = color_text(colored[start:end], color)\n        colored = colored[:start] + colored_part + colored[end:]\n\n    return colored"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the end of a block in a line_list", "response": "def find_block_end(row, line_list, sentinal, direction=1):\n    \"\"\"\n    Searches up and down until it finds the endpoints of a block Rectify\n    with find_paragraph_end in pyvim_funcs\n    \"\"\"\n    import re\n    row_ = row\n    line_ = line_list[row_]\n    flag1 = row_ == 0 or row_ == len(line_list) - 1\n    flag2 = re.match(sentinal, line_)\n    if not (flag1 or flag2):\n        while True:\n            if (row_ == 0 or row_ == len(line_list) - 1):\n                break\n            line_ = line_list[row_]\n            if re.match(sentinal, line_):\n                break\n            row_ += direction\n    return row_"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compress_pdf(pdf_fpath, output_fname=None):\n    import utool as ut\n    ut.assertpath(pdf_fpath)\n    suffix = '_' + ut.get_datestamp(False) + '_compressed'\n    print('pdf_fpath = %r' % (pdf_fpath,))\n    output_pdf_fpath = ut.augpath(pdf_fpath, suffix, newfname=output_fname)\n    print('output_pdf_fpath = %r' % (output_pdf_fpath,))\n    gs_exe = find_ghostscript_exe()\n    cmd_list = (\n        gs_exe,\n        '-sDEVICE=pdfwrite',\n        '-dCompatibilityLevel=1.4',\n        '-dNOPAUSE',\n        '-dQUIET',\n        '-dBATCH',\n        '-sOutputFile=' + output_pdf_fpath,\n        pdf_fpath\n    )\n    ut.cmd(*cmd_list)\n    return output_pdf_fpath", "response": "uses ghostscript to write a pdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompiles latex and shows the result", "response": "def render_latex_text(input_text, nest_in_doc=False, preamb_extra=None,\n                      appname='utool', verbose=None):\n    \"\"\" compiles latex and shows the result \"\"\"\n    import utool as ut\n    if verbose is None:\n        verbose = ut.VERBOSE\n    dpath = ut.ensure_app_resource_dir(appname, 'latex_tmp')\n    # put a latex framgent in a full document\n    # print(input_text)\n    fname = 'temp_render_latex'\n    pdf_fpath = ut.compile_latex_text(\n        input_text, dpath=dpath, fname=fname, preamb_extra=preamb_extra,\n        verbose=verbose)\n    ut.startfile(pdf_fpath)\n    return pdf_fpath"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_latex(input_text, dpath=None, fname=None, preamb_extra=None,\n                 verbose=1, **kwargs):\n    \"\"\"\n    Renders latex text into a jpeg.\n\n    Whitespace that would have appeared in the PDF is removed, so the jpeg is\n    cropped only the the relevant part.  This is ideal for figures that only\n    take a single page.\n\n    Args:\n        input_text (?):\n        dpath (str):  directory path(default = None)\n        fname (str):  file name(default = None)\n        preamb_extra (None): (default = None)\n        verbose (int):  verbosity flag(default = 1)\n\n    Returns:\n        str: jpg_fpath -  file path string\n\n    CommandLine:\n        python -m utool.util_latex render_latex '$O(n^2)$' --fpath=~/slides/tmp.jpg\n\n    Script:\n        >>> # SCRIPT\n        >>> from utool.util_latex import *  # NOQA\n        >>> from os.path import split, expanduser\n        >>> import utool as ut\n        >>> input_text = ' '.join(ut.get_varargs()[1:])\n        >>> dpath, fname = split(ut.argval('--fpath', ''))\n        >>> dpath = expanduser(ut.argval('--dpath', dpath))\n        >>> fname = ut.argval('--fname', fname)\n        >>> kwargs = ut.dict_subset(ut.argparse_funckw(ut.convert_pdf_to_image), ['dpi', 'quality'])\n        >>> jpg_fpath = render_latex(input_text, dpath, fname, **kwargs)\n        >>> if ut.argflag('--diskshow'):\n        >>>     ut.startfile(jpg_fpath)\n    \"\"\"\n    import utool as ut\n    import vtool as vt\n    # turn off page numbers\n    input_text_ = '\\pagenumbering{gobble}\\n' + input_text\n    # fname, _ = splitext(fname)\n    img_fname = ut.ensure_ext(fname, ['.jpg'] + list(ut.IMG_EXTENSIONS))\n    img_fpath = join(dpath, img_fname)\n    pdf_fpath = ut.compile_latex_text(\n        input_text_, fname=fname, dpath=dpath, preamb_extra=preamb_extra,\n        verbose=verbose, move=False)\n    ext = splitext(img_fname)[1]\n    fpath_in = ut.convert_pdf_to_image(pdf_fpath, ext=ext, verbose=verbose)\n    # Clip of boundaries of the pdf imag\n    vt.clipwhite_ondisk(fpath_in, fpath_out=img_fpath, verbose=verbose > 1)\n    return img_fpath", "response": "Render a latex text into a jpg file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_latex_figure_str2(fpath_list, cmdname, **kwargs):\n    import utool as ut\n    from os.path import relpath\n    # Make relative paths\n    if kwargs.pop('relpath', True):\n        start = ut.truepath('~/latex/crall-candidacy-2015')\n        fpath_list = [relpath(fpath, start) for fpath in fpath_list]\n    cmdname = ut.latex_sanitize_command_name(cmdname)\n\n    kwargs['caption_str'] = kwargs.get('caption_str', cmdname)\n    figure_str  = ut.get_latex_figure_str(fpath_list, **kwargs)\n    latex_block = ut.latex_newcommand(cmdname, figure_str)\n    return latex_block", "response": "get_latex_figure_str2 - get a LaTeX figure string from a list of files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_latex_figure_str(fpath_list, caption_str=None, label_str=None,\n                         width_str=r'\\textwidth', height_str=None, nCols=None,\n                         dpath=None, colpos_sep=' ', nlsep='',\n                         use_sublbls=None, use_frame=False):\n    r\"\"\"\n    Args:\n        fpath_list (list):\n        dpath (str): directory relative to main tex file\n\n    Returns:\n        str: figure_str\n\n    CommandLine:\n        python -m utool.util_latex --test-get_latex_figure_str\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_latex import *  # NOQA\n        >>> fpath_list = ['figures/foo.png']\n        >>> figure_str = get_latex_figure_str(fpath_list)\n        >>> result = str(figure_str)\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n\n    if nCols is None:\n        nCols = len(fpath_list)\n\n    USE_SUBFIGURE = True\n\n    if width_str is not None:\n        colwidth = (1.0 / nCols)\n        if USE_SUBFIGURE:\n            colwidth *= .95\n            graphics_sizestr = ('%.2f' % (colwidth,)) + width_str\n        else:\n            graphics_sizestr = '[width=%.1f%s]' % (colwidth, width_str)\n    elif height_str is not None:\n        graphics_sizestr = '[height=%s]' % (height_str)\n    else:\n        graphics_sizestr =  ''\n\n    if dpath is not None:\n        fpath_list = [ut.relpath_unix(fpath_, dpath) for fpath_ in fpath_list]\n\n    if USE_SUBFIGURE:\n        # References: https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions#Subfloats\n        # TODO ? http://tex.stackexchange.com/questions/159290/how-can-i-place-a-vertical-rule-between-subfigures\n        # Use subfigures\n        graphics_list = []\n        sublbl_prefix = label_str if label_str is not None else ''\n        for count, fpath in enumerate(fpath_list):\n            \"\"\"\n            print(', '.join([str(x) + ':' + chr(x) for x in range(65, 123)]))\n            print(', '.join([str(x) + ':' + chr(x) for x in range(97, 123)]))\n            \"\"\"\n            CHRLBLS = True\n            if CHRLBLS:\n                #subchar = chr(97 + count)\n                subchar = chr(65 + count)\n            else:\n                subchar = str(count)\n            parts = []\n            subfigure_str = ''\n            if len(fpath_list) > 1:\n                parts.append('\\\\begin{subfigure}[h]{' + graphics_sizestr + '}')\n                parts.append('\\\\centering')\n            graphics_part = '\\\\includegraphics[width=%s]{%s}' % (width_str, fpath,)\n            if use_frame:\n                parts.append('\\\\fbox{%s}' % (graphics_part,))\n            else:\n                parts.append(graphics_part)\n            if use_sublbls is True or use_sublbls is None and len(fpath_list) > 1:\n                parts.append('\\\\caption{}\\\\label{sub:' + sublbl_prefix + subchar + '}')\n            if len(fpath_list) > 1:\n                parts.append('\\\\end{subfigure}')\n            subfigure_str = ''.join(parts)\n            graphics_list.append(subfigure_str)\n    else:\n        if True:\n            graphics_list = [\n                r'\\includegraphics%s{%s}\\captionof{figure}{%s}' % (\n                    graphics_sizestr, fpath, 'fd',\n                    #'(' + str(count) + ')'\n                    #'(' + chr(97 + count) + ')'\n                )\n                for count, fpath in enumerate(fpath_list)]\n        else:\n            graphics_list = [r'\\includegraphics%s{%s}' % (graphics_sizestr, fpath,) for fpath in fpath_list]\n        #graphics_list = [r'\\includegraphics%s{%s}' % (graphics_sizestr, fpath,) ]\n    #nRows = len(graphics_list) // nCols\n\n    # Add separators\n    NL = '\\n'\n    if USE_SUBFIGURE:\n        col_spacer_mid = NL + '~~' + '% --' + NL\n        col_spacer_end = NL + r'\\\\' + '% --' + NL\n    else:\n        col_spacer_mid = NL + '&' + NL\n        col_spacer_end = NL + r'\\\\' + nlsep + NL\n    sep_list = [\n        col_spacer_mid  if count % nCols > 0 else col_spacer_end\n        for count in range(1, len(graphics_list) + 1)\n    ]\n    if len(sep_list) > 0:\n        sep_list[-1] = ''\n    graphics_list_ = [graphstr + sep for graphstr, sep in zip(graphics_list, sep_list)]\n\n    #graphics_body = '\\n&\\n'.join(graphics_list)\n    graphics_body = ''.join(graphics_list_)\n    header_str = colpos_sep.join(['c'] * nCols)\n\n    if USE_SUBFIGURE:\n        figure_body = graphics_body\n    else:\n        figure_body =  ut.codeblock(\n            r'''\n            \\begin{tabular}{%s}\n            %s\n            \\end{tabular}\n            '''\n        ) % (header_str, graphics_body)\n    if caption_str is not None:\n        #tabular_body += '\\n\\caption{\\\\footnotesize{%s}}' % (caption_str,)\n        if label_str is not None:\n            figure_body += '\\n\\caption[%s]{%s}' % (label_str, caption_str,)\n        else:\n            figure_body += '\\n\\caption{%s}' % (caption_str,)\n    if label_str is not None:\n        figure_body += '\\n\\label{fig:%s}' % (label_str,)\n    #figure_fmtstr = ut.codeblock(\n    #    r'''\n    #    \\begin{figure*}\n    #    \\begin{center}\n    #    %s\n    #    \\end{center}\n    #    \\end{figure*}\n    #    '''\n    #)\n    figure_fmtstr = ut.codeblock(\n        r'''\n        \\begin{figure}[ht!]\n        \\centering\n        %s\n        \\end{figure}\n        '''\n    )\n    figure_str = figure_fmtstr % (figure_body)\n    return figure_str", "response": "r Get a LaTeX figure string from a list of files"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a header for a given set of fields", "response": "def generate_protein_header(headerfields, oldheader, group_by_field, genecentric):\n    \"\"\"Returns a header as a list, ready to write to TSV file\"\"\"\n    fieldtypes = ['proteindata', 'probability', 'proteinfdr', 'proteinpep',\n                  'precursorquant', 'isoquant', 'bestpepscore']\n    firstfield = prottabledata.ACCESSIONS[genecentric]\n    return generate_general_header(headerfields, fieldtypes, firstfield, oldheader,\n                                   group_by_field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_prottable_headerfields(headertypes, lookup=False, poolnames=False, genecentric=False):\n    field_defs = {'isoquant': get_isoquant_fields,\n                  'precursorquant': get_precursorquant_fields,\n                  'probability': get_probability_fields,\n                  'proteindata': get_proteininfo_fields,\n                  'proteinfdr': get_proteinfdr_fields,\n                  'proteinpep': get_proteinpep_fields,\n                  'bestpepscore': get_bestpeptide_fields,\n                  }\n    return generate_headerfields(headertypes, field_defs, poolnames, lookup, genecentric)", "response": "Returns a generator object that can be used to generate headerfields for a proteins table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_proteininfo_fields(poolnames=False, genecentric=False):\n    allfields = OrderedDict()\n    basefields = {\n            False: [\n                prottabledata.HEADER_GENEID, prottabledata.HEADER_GENENAME,\n                prottabledata.HEADER_DESCRIPTION, prottabledata.HEADER_COVERAGE,\n                prottabledata.HEADER_NO_PROTEIN, prottabledata.HEADER_CONTENTPROT,\n                ],\n            'genes': [\n                prottabledata.HEADER_GENENAME, prottabledata.HEADER_PROTEINS,\n                prottabledata.HEADER_DESCRIPTION],\n            'assoc': [\n                prottabledata.HEADER_GENEID, prottabledata.HEADER_PROTEINS,\n                prottabledata.HEADER_DESCRIPTION],\n            }[genecentric]\n    poolfields = [prottabledata.HEADER_NO_UNIPEP,\n                  prottabledata.HEADER_NO_PEPTIDE,\n                  prottabledata.HEADER_NO_PSM,\n                  ]\n    for field in basefields:\n        allfields[field] = False\n    for field in poolfields:\n        allfields[field] = poolnames\n    return allfields", "response": "Returns header fields for protein group information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the sort_by is valid.", "response": "def _validate_sort_field(self, sort_by):\n        \"\"\"\n        :param sort_by: string\n        :raises: pybomb.exceptions.InvalidSortFieldException\n        \"\"\"\n        if (\n            sort_by not in self.RESPONSE_FIELD_MAP\n            or not self.RESPONSE_FIELD_MAP[sort_by].is_sort\n        ):\n            raise InvalidSortFieldException(\n                '\"{0}\" is an invalid sort field'.format(sort_by)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_filter_fields(self, filter_by):\n        for filter_field in filter_by:\n            if (\n                filter_field not in self.RESPONSE_FIELD_MAP\n                or not self.RESPONSE_FIELD_MAP[filter_field].is_filter\n            ):\n                raise InvalidFilterFieldException(\n                    '\"{0}\" is an invalid filter field'.format(filter_field)\n                )", "response": "Validate the filter_by dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_search_filter(filter_by):\n        return \",\".join(\n            [\n                \"{0}:{1}\".format(key, value)\n                for key, value in filter_by.items()\n                if value is not None\n            ]\n        )", "response": "Create a search filter for the related items in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries the related resources.", "response": "def _query(self, params, direct=False):\n        \"\"\"\n        :param params: dict\n        :return: pybomb.clients.response\n        \"\"\"\n        params[\"api_key\"] = self._api_key\n\n        if \"format\" not in params:\n            params[\"format\"] = self._default_format\n\n        response = self._query_api(params, direct)\n        self._validate_response(response)\n\n        return Response.from_response_data(response)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _query_api(self, params, direct=False):\n        if not direct:\n            return get(\n                self.URI_BASE + self.RESOURCE_NAME, params=params, headers=self._headers\n            )\n\n        id = params.pop(\"id\")\n        return get(\n            self.URI_BASE + self.RESOURCE_NAME + \"/{0}\".format(id),\n            params=params,\n            headers=self._headers,\n        )", "response": "Query the API for the related resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_response(self, response):\n        try:\n            response.raise_for_status()\n        except HTTPError as http_error:\n            raise BadRequestException(str(http_error))\n\n        response_data = response.json()\n        if response_data[\"status_code\"] != self.RESPONSE_STATUS_OK:\n            raise InvalidResponseException(\n                \"Response code {0}: {1}\".format(\n                    response_data[\"status_code\"], response_data[\"error\"]\n                )\n            )", "response": "Validate the response from the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating iterator to write to new tsv. Contains input tsv lines plus quant data for these.", "response": "def get_psms(self):\n        \"\"\"Creates iterator to write to new tsv. Contains input tsv\n        lines plus quant data for these.\"\"\"\n        self.header = actions.create_header(self.oldheader)\n        self.psms = actions.add_genes_to_psm_table(self.fn, self.oldheader,\n                                                   self.lookup)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, data, value=None, timestamp=None, namespace=None,\n            debug=False):\n        \"\"\"Queue a gauge or gauges to be written\"\"\"\n        if value is not None:\n            return self.add(((data, value),), timestamp=timestamp,\n                            namespace=namespace, debug=debug)\n        writer = self.writer\n        if writer is None:\n            raise GaugedUseAfterFreeError\n        if timestamp is None:\n            timestamp = long(time() * 1000)\n        config = self.config\n        block_size = config.block_size\n        this_block = timestamp // block_size\n        this_array = (timestamp % block_size) // config.resolution\n        if namespace is None:\n            namespace = config.namespace\n        if this_block < self.current_block or \\\n                (this_block == self.current_block and\n                 this_array < self.current_array):\n            if config.append_only_violation == Writer.ERROR:\n                msg = 'Gauged is append-only; timestamps must be increasing'\n                raise GaugedAppendOnlyError(msg)\n            elif config.append_only_violation == Writer.REWRITE:\n                this_block = self.current_block\n                this_array = self.current_array\n            else:\n                return\n        if isinstance(data, unicode):\n            data = data.encode('utf8')\n        if debug:\n            return self.debug(timestamp, namespace, data)\n        if this_block > self.current_block:\n            self.flush_blocks()\n            self.current_block = this_block\n            self.current_array = this_array\n        elif this_array > self.current_array:\n            if not Gauged.writer_flush_arrays(writer, self.current_array):\n                raise MemoryError\n            self.current_array = this_array\n        data_points = 0\n        namespace_statistics = self.statistics[namespace]\n        whitelist = config.key_whitelist\n        skip_long_keys = config.key_overflow == Writer.IGNORE\n        skip_gauge_nan = config.gauge_nan == Writer.IGNORE\n        if isinstance(data, str) and skip_gauge_nan \\\n                and skip_long_keys and whitelist is None:  # fast path\n            data_points = c_uint32(0)\n            if not Gauged.writer_emit_pairs(writer, namespace, data,\n                                            byref(data_points)):\n                raise MemoryError\n            data_points = data_points.value\n        else:\n            if isinstance(data, dict):\n                data = data.iteritems()\n            elif isinstance(data, str):\n                data = self.parse_query(data)\n            emit = Gauged.writer_emit\n            for key, value in data:\n                key = to_bytes(key)\n                if whitelist is not None and key not in whitelist:\n                    continue\n                try:\n                    value = float(value)\n                except ValueError:\n                    value = float('nan')\n                if value != value:  # => NaN?\n                    if skip_gauge_nan:\n                        continue\n                    raise GaugedNaNError\n                success = emit(writer, namespace, key, c_float(value))\n                if success != 1:\n                    if not success:\n                        raise MemoryError\n                    elif success == Writer.KEY_OVERFLOW and not skip_long_keys:\n                        msg = 'Key is larger than the driver allows '\n                        msg += '(%s)' % key\n                        raise GaugedKeyOverflowError(msg)\n                data_points += 1\n        namespace_statistics.data_points += data_points\n        if self.flush_now:\n            self.flush()", "response": "Add a new entry to the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef flush(self):\n        writer = self.writer\n        if writer is None:\n            raise GaugedUseAfterFreeError\n        self.flush_writer_position()\n        keys = self.translate_keys()\n        blocks = []\n        current_block = self.current_block\n        statistics = self.statistics\n        driver = self.driver\n        flags = 0  # for future extensions, e.g. block compression\n        for namespace, key, block in self.pending_blocks():\n            length = block.byte_length()\n            if not length:\n                continue\n            key_id = keys[(namespace, key)]\n            statistics[namespace].byte_count += length\n            blocks.append((namespace, current_block, key_id, block.buffer(),\n                           flags))\n        if self.config.overwrite_blocks:\n            driver.replace_blocks(blocks)\n        else:\n            driver.insert_or_append_blocks(blocks)\n            if not Gauged.writer_flush_maps(writer, True):\n                raise MemoryError\n        update_namespace = driver.add_namespace_statistics\n        for namespace, stats in statistics.iteritems():\n            update_namespace(namespace, self.current_block,\n                             stats.data_points, stats.byte_count)\n        statistics.clear()\n        driver.commit()\n        self.flush_now = False", "response": "Flush all pending gauges and all associated gauges."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resume_from(self):\n        position = self.driver.get_writer_position(self.config.writer_name)\n        return position + self.config.resolution if position else 0", "response": "Get a timestamp representing the position just after the last acquired timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclear all data from timestamp onwards.", "response": "def clear_from(self, timestamp):\n        \"\"\"Clear all data from `timestamp` onwards. Note that the timestamp\n        is rounded down to the nearest block boundary\"\"\"\n        block_size = self.config.block_size\n        offset, remainder = timestamp // block_size, timestamp % block_size\n        if remainder:\n            raise ValueError('Timestamp must be on a block boundary')\n        self.driver.clear_from(offset, timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing all data before a given timestamp for a given key.", "response": "def clear_key_before(self, key, namespace=None, timestamp=None):\n        \"\"\"Clear all data before `timestamp` for a given key. Note that the\n        timestamp is rounded down to the nearest block boundary\"\"\"\n        block_size = self.config.block_size\n        if namespace is None:\n            namespace = self.config.namespace\n        if timestamp is not None:\n            offset, remainder = divmod(timestamp, block_size)\n            if remainder:\n                raise ValueError('timestamp must be on a block boundary')\n            if offset == 0:\n                raise ValueError('cannot delete before offset zero')\n            offset -= 1\n            self.driver.clear_key_before(key, namespace, offset, timestamp)\n        else:\n            self.driver.clear_key_before(key, namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a query string and return an iterator which yields ( key value", "response": "def parse_query(self, query):\n        \"\"\"Parse a query string and return an iterator which yields\n        (key, value)\"\"\"\n        writer = self.writer\n        if writer is None:\n            raise GaugedUseAfterFreeError\n        Gauged.writer_parse_query(writer, query)\n        position = 0\n        writer_contents = writer.contents\n        size = writer_contents.buffer_size\n        pointers = writer_contents.buffer\n        while position < size:\n            yield pointers[position], pointers[position+1]\n            position += 2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loadstr(cls, string):\n        try:\n            string = string.decode('utf-8')\n        except AttributeError:\n            pass\n\n        lexer = tokenizer(string)\n\n        if cls.is_molfile(string):\n            molfile = Molfile()\n\n            try:\n                molfile.update(json.loads(string))\n            except ValueError:\n                molfile._build(lexer)\n            return molfile\n\n        elif cls.is_sdfile(string):\n            sdfile = SDfile()\n\n            try:\n                sdfile.update(json.loads(string))\n            except ValueError:\n                sdfile._build(lexer)\n            return sdfile\n\n        else:\n            raise ValueError('Cannot determine the format of string.')", "response": "Load data into CTfile object from string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the data into file.", "response": "def write(self, filehandle, file_format):\n        \"\"\"Write :class:`~ctfile.ctfile.CTfile` data into file. \n\n        :param filehandle: File-like object.\n        :param str file_format: Format to use to write data: ``ctfile`` or ``json``.\n        :return: None.\n        :rtype: :py:obj:`None`.\n        \"\"\"\n        try:\n            filehandle.write(self.writestr(file_format=file_format))\n        except IOError:\n            raise IOError('\"filehandle\" parameter must be writable.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the contents of the object to a file or stdout.", "response": "def print_file(self, file_format='ctfile', f=sys.stdout):\n        \"\"\"Print representation of :class:`~ctfile.ctfile.CTfile`.\n\n        :param str file_format: Format to use: ``ctfile`` or ``json``.\n        :param f: Print to file or stdout.\n        :type f: File-like \n        :return: None.\n        :rtype: :py:obj:`None`.\n        \"\"\"\n        print(self.writestr(file_format=file_format), file=f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the CtabAtomBond object into a JSON string.", "response": "def _to_json(self, sort_keys=False, indent=4):\n        \"\"\"Convert :class:`~ctfile.ctfile.CTfile` into JSON string.\n\n        :return: ``JSON`` formatted string.\n        :rtype: :py:class:`str`.\n        \"\"\"\n        return json.dumps(self, sort_keys=sort_keys, indent=indent, cls=CtabAtomBondEncoder)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _build(self, lexer):\n        atom_number = 1\n        while True:\n            token = next(lexer)\n            key = token.__class__.__name__\n\n            if key == 'CtabCountsLine':\n                self[key].update(token._asdict())\n\n            elif key == 'CtabAtomBlock':\n                self[key].append(Atom(atom_number=str(atom_number), **token._asdict()))\n                atom_number += 1\n\n            elif key == 'CtabBondBlock':\n                first_atom_number, second_atom_number, bond_type, bond_stereo, \\\n                not_used1, bond_topology, reacting_center_status = token\n\n                first_atom = self.atoms[int(first_atom_number) - 1]\n                second_atom = self.atoms[int(second_atom_number) - 1]\n                first_atom.neighbors.append(second_atom)\n                second_atom.neighbors.append(first_atom)\n\n                bond = Bond(first_atom=first_atom, second_atom=second_atom, bond_type=bond_type,\n                            bond_stereo=bond_stereo, not_used1=not_used1, bond_topology=bond_topology,\n                            reacting_center_status=reacting_center_status)\n                self[key].append(bond)\n\n            elif key == 'CtabPropertiesBlock':\n                property_name = token.name\n                keys = self.ctab_conf[self.version][property_name]['values']\n                ctab_properties = more_itertools.sliced(token.line.split()[3:], len(keys))\n\n                for ctab_property in ctab_properties:\n                    atom_number, property_value = ctab_property\n                    self.atoms[int(atom_number) - 1]._ctab_property_data[property_name] = property_value\n\n            elif key == 'CtabBlockEnd':\n                break\n\n            else:\n                raise KeyError('Ctab object does not supposed to have any other information: \"{}\".'.format(key))", "response": "Build the list of Ctab objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the list of all the elements of the current object into a CTfile formatted string.", "response": "def _to_ctfile(self):\n        \"\"\"Convert :class:`~ctfile.ctfile.CTfile` into `CTfile` formatted string.\n\n        :return: `CTfile` formatted string.\n        :rtype: :py:class:`str`.\n        \"\"\"\n        output = io.StringIO()\n        for key in self:\n            if key == 'CtabCountsLine':\n                output.write(self._to_ctfile_counts_line(key=key))\n\n            elif key == 'CtabAtomBlock':\n                output.write(self._to_ctfile_atom_block(key=key))\n\n            elif key == 'CtabBondBlock':\n                output.write(self._to_ctfile_bond_block(key=key))\n\n            else:\n                raise KeyError('Ctab object does not supposed to have any other information: \"{}\".'.format(key))\n\n        output.write(self._to_ctfile_property_block())\n        output.write(self.ctab_conf[self.version]['END']['fmt'])\n        output.write('\\n')\n\n        return output.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates counts line in CTfile format.", "response": "def _to_ctfile_counts_line(self, key):\n        \"\"\"Create counts line in ``CTfile`` format.\n\n        :param str key: Counts line key. \n        :return: Counts line string.\n        :rtype: :py:class:`str`\n        \"\"\"\n        counter = OrderedCounter(self.counts_line_format)\n        self[key]['number_of_atoms'] = str(len(self.atoms))\n        self[key]['number_of_bonds'] = str(len(self.bonds))\n        counts_line = ''.join([str(value).rjust(spacing) for value, spacing\n                               in zip(self[key].values(), counter.values())])\n        return '{}\\n'.format(counts_line)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates atom block in CTfile format.", "response": "def _to_ctfile_atom_block(self, key):\n        \"\"\"Create atom block in `CTfile` format.\n\n        :param str key: Ctab atom block key. \n        :return: Ctab atom block.\n        :rtype: :py:class:`str`\n        \"\"\"\n        counter = OrderedCounter(Atom.atom_block_format)\n        ctab_atom_block = '\\n'.join([''.join([str(value).rjust(spacing) for value, spacing\n                                              in zip(atom._ctab_data.values(), counter.values())])\n                                     for atom in self[key]])\n        return '{}\\n'.format(ctab_atom_block)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _to_ctfile_bond_block(self, key):\n        counter = OrderedCounter(Bond.bond_block_format)\n        ctab_bond_block = '\\n'.join([''.join([str(value).rjust(spacing) for value, spacing\n                                              in zip(bond._ctab_data.values(), counter.values())])\n                                     for bond in self[key]])\n        return '{}\\n'.format(ctab_bond_block)", "response": "Create bond block in CTfile format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_ctfile_property_block(self):\n        ctab_properties_data = defaultdict(list)\n        for atom in self.atoms:\n            for ctab_property_key, ctab_property_value in atom._ctab_property_data.items():\n                ctab_properties_data[ctab_property_key].append(OrderedDict(\n                    zip(self.ctab_conf[self.version][ctab_property_key]['values'],\n                        [atom.atom_number, ctab_property_value])))\n\n        ctab_property_lines = []\n        for ctab_property_key, ctab_property_value in ctab_properties_data.items():\n            for entry in ctab_property_value:\n                ctab_property_line = '{}  {}{}'.format(self.ctab_conf[self.version][ctab_property_key]['fmt'],\n                                                       1, ''.join([str(value).rjust(4) for value in entry.values()]))\n                ctab_property_lines.append(ctab_property_line)\n\n        if ctab_property_lines:\n            return '{}\\n'.format('\\n'.join(ctab_property_lines))\n        return ''", "response": "Create ctab properties block in CTfile format from atom - specific properties."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_atom(self, *atom_numbers):\n        for atom_number in atom_numbers:\n            deletion_atom = self.atom_by_number(atom_number=atom_number)\n\n            # update atom numbers\n            for atom in self.atoms:\n                if int(atom.atom_number) > int(atom_number):\n                    atom.atom_number = str(int(atom.atom_number) - 1)\n\n            # find index of a bond to remove and update ctab data dict with new atom numbers\n            for index, bond in enumerate(self.bonds):\n                bond.update_atom_numbers()\n                if atom_number in {bond.first_atom_number, bond.second_atom_number}:\n                    self.bonds.remove(bond)\n\n            # remove atom from neighbors list\n            for atom in self.atoms:\n                if deletion_atom in atom.neighbors:\n                    atom.neighbors.remove(deletion_atom)\n\n            self.atoms.remove(deletion_atom)", "response": "Delete atoms by atom number."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build(self, lexer):\n        key = ''\n        while key != 'EndOfFile':\n\n            token = next(lexer)\n            key = token.__class__.__name__\n\n            if key == 'HeaderBlock':\n                self[key].update(token._asdict())\n\n            elif key == 'CtabBlockStart':\n                ctab = Ctab()\n                ctab._build(lexer)\n                self['Ctab'] = ctab\n\n            elif key == 'MolfileStart':\n                pass\n\n            elif key in ('MolfileEnd', 'EndOfFile'):\n                break\n\n            else:\n                raise KeyError('Molfile object does not supposed to have any other information: \"{}\".'.format(key))\n\n        return self", "response": "Build the Molfile instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts the Molfile object into a CTfile formatted string.", "response": "def _to_ctfile(self):\n        \"\"\"Convert :class:`~ctfile.ctfile.CTfile` into `CTfile` formatted string.\n\n        :return: ``CTfile`` formatted string.\n        :rtype: :py:class:`str`.\n        \"\"\"\n        output = io.StringIO()\n\n        for key in self:\n            if key == 'HeaderBlock':\n                for line in self[key].values():\n                    output.write(line)\n                    output.write('\\n')\n\n            elif key == 'Ctab':\n                ctab_str = self[key]._to_ctfile()\n                output.write(ctab_str)\n\n            else:\n                raise KeyError('Molfile object does not supposed to have any other information: \"{}\".'.format(key))\n\n        return output.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_molfile(cls, molfile, data=None):\n        if not data:\n            data = OrderedDict()\n\n        if not isinstance(molfile, Molfile):\n            raise ValueError('Not a Molfile type: \"{}\"'.format(type(molfile)))\n\n        if not isinstance(data, dict):\n            raise ValueError('Not a dict type: \"{}\"'.format(type(data)))\n\n        sdfile = cls()\n        sdfile['1'] = OrderedDict()\n        sdfile['1']['molfile'] = molfile\n        sdfile['1']['data'] = data\n        return sdfile", "response": "Construct a new SDfile object from a Molfile object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_data(self, id, key, value):\n        self[str(id)]['data'].setdefault(key, [])\n        self[str(id)]['data'][key].append(value)", "response": "Add new data item."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_molfile(self, molfile, data):\n        if not isinstance(molfile, Molfile):\n            raise ValueError('Not a Molfile type: \"{}\"'.format(type(molfile)))\n\n        if not isinstance(data, dict):\n            raise ValueError('Not a dict type: \"{}\"'.format(type(data)))\n\n        entry_ids = sorted(self.keys(), key=lambda x: int(x))\n        if entry_ids:\n            last_entry_id = str(entry_ids[-1])\n        else:\n            last_entry_id = '0'\n\n        new_entry_id = str(int(last_entry_id) + 1)\n        self[new_entry_id] = OrderedDict()\n        self[new_entry_id]['molfile'] = molfile\n        self[new_entry_id]['data'] = data", "response": "Add a Molfile and data to the SDfile object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_sdfile(self, sdfile):\n        if not isinstance(sdfile, SDfile):\n            raise ValueError('Not a SDfile type: \"{}\"'.format(type(sdfile)))\n\n        for entry_id in sdfile:\n            self.add_molfile(molfile=sdfile[entry_id]['molfile'],\n                             data=sdfile[entry_id]['data'])", "response": "Adds new SDfile to current SDfile instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _build(self, lexer):\n        current_entry_id = 0\n\n        while True:\n            token = next(lexer)\n            key = token.__class__.__name__\n\n            if key == 'MolfileStart':\n                current_entry_id += 1\n                molfile = Molfile()\n                molfile._build(lexer)\n                self[str(current_entry_id)] = OrderedDict(molfile=molfile, data=OrderedDict())\n\n            elif key == 'DataBlockStart':\n                data_block = self._build_data_block(lexer)\n                self[str(current_entry_id)]['data'].update(data_block)\n\n            elif key == 'EndOfFile':\n                break\n\n            else:\n                raise KeyError('SDfile does not supposed to have any other information: \"{}\".'.format(key))\n\n        return self", "response": "Build :class:`~ctfile.ctfile.SDfile` instance.\n\n        :return: :class:`~ctfile.ctfile.SDfile` instance.\n        :rtype: :class:`~ctfile.ctfile.SDfile`."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_data_block(self, lexer):\n        data_block = OrderedDict()\n        header = ''\n\n        while True:\n            token = next(lexer)\n            key = token.__class__.__name__\n\n            if key == 'DataHeader':\n                header = token.header[1:-1]\n                data_block.setdefault(header, [])\n\n            elif key == 'DataItem':\n                data_block[header].append(token.data_item)\n\n            elif key == 'DataBlockEnd':\n                break\n\n            else:\n                raise KeyError('SDfile data block does not supposed to have any other information: \"{}\".'.format(key))\n\n        return data_block", "response": "Build the data block of the SDfile instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_ctfile(self):\n        output = io.StringIO()\n\n        for entry in self.values():\n            output.write(entry['molfile']._to_ctfile())\n\n            for header, values in entry['data'].items():\n                output.write('> <{}>\\n'.format(header))\n                output.write('\\n'.join(values))\n                output.write('\\n')\n            output.write('\\n$$$$\\n')\n\n        return output.getvalue()", "response": "Convert the MoleculeFile into CTfile formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef neighbor_atoms(self, atom_symbol=None):\n        if not atom_symbol:\n            return self.neighbors\n        else:\n            return [atom for atom in self.neighbors if atom['atom_symbol'] == atom_symbol]", "response": "Access neighbor atoms.\n\n        :param str atom_symbol: Atom symbol.\n        :return: List of neighbor atoms.\n        :rtype: :py:class:`list`."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate links first_atom_number -> second_atom_number", "response": "def update_atom_numbers(self):\n        \"\"\"Update links \"first_atom_number\" -> \"second_atom_number\"\n\n        :return: None.\n        :rtype: :py:obj:`None`.\n        \"\"\"\n        self._ctab_data['first_atom_number'] = self.first_atom.atom_number\n        self._ctab_data['second_atom_number'] = self.second_atom.atom_number"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default(self, o):\n        if isinstance(o, Atom) or isinstance(o, Bond):\n            return o._ctab_data\n        else:\n            return o.__dict__", "response": "Default encoder.\n\n        :param o: Atom or Bond instance.\n        :type o: :class:`~ctfile.ctfile.Atom` or :class:`~ctfile.ctfile.Bond`.\n        :return: Dictionary that contains information required for atom and bond block of ``Ctab``.\n        :rtype: :py:class:`collections.OrderedDict`"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a ServerConfig instance with configuration given server.", "response": "def get(self, server):\n        \"\"\"\n        Returns ServerConfig instance with configuration given server.\n\n        :raises ServerConfigConflictError:\n            if configuration directory contains configuration for same server\n            multiple times\n\n        :raises ServerConfigMissingUrlError: if URL is not specified in the configuration\n\n        :raises ServerConfigNotFoundError: if configuration for given server is not found\n        \"\"\"\n        server_config = self.config.get(server)\n        try:\n            while server_config is None:\n                new_config = self._read_next_config()\n                server_config = new_config.get(server)\n                new_config.update(self.config)\n                self.config = new_config\n        except StopIteration:\n            return _default_server_configuration(server)\n\n        if CONFIG_URL_KEY_NAME not in server_config:\n            message = \"'%s' must be specified in configuration for '%s'\" \\\n                % (CONFIG_URL_KEY_NAME, server)\n            raise ServerConfigMissingUrlError(message)\n\n        return ServerConfig(server_config)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef free(self):\n        if self._ptr is None:\n            return\n        Gauged.array_free(self.ptr)\n        FloatArray.ALLOCATIONS -= 1\n        self._ptr = None", "response": "Free the underlying C array."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles the command line.", "response": "def handle(self, *args, **options):\n\n        \"\"\"\n        Check, how much options are true.\n        If option '--force', '-f' is set this part will be skipped.\n        \"\"\"\n        if not options['force']:\n            counter = 0\n            for key in options:\n                if options[key]:\n                    counter += 1\n\n            # If no options are set, do a normal patch\n            if counter == 1:\n                options['patch'] = True\n\n            # more then one options are not enabled per default\n            if counter >= 3:\n                # TODO: Raise Error!\n                exit('It is not recommended to use more then one parameter. Use -f to force your command.')\n        ###########################################################################################\n\n        if options['major']:\n            Version.set_major()\n\n        if options['minor']:\n            Version.set_minor()\n\n        if options['patch']:\n            Version.set_patch()\n\n        if options['dev']:\n            Version.set_patch(Version.DEV)\n\n        if options['alpha']:\n            Version.set_patch(Version.ALPHA)\n\n        if options['beta']:\n            Version.set_patch(Version.BETA)\n\n        if options['rc']:\n            Version.set_patch(Version.RC)\n\n        \"\"\"\n        Automatic commands.\n        Depends on User Settings.\n        \"\"\"\n        if APISettings.PATCH_AUTO_COMMIT:\n            Git.add()\n            Git.commit()\n\n        if APISettings.PATCH_AUTO_TAG:\n            Git.tag()\n            if APISettings.PATCH_AUTO_TAG_PUSH:\n                Git.push_tags()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_psms_quanted(quantdb, tsvfn, isob_header, oldheader,\n                          isobaric=False, precursor=False):\n    \"\"\"Takes dbfn and connects, gets quants for each line in tsvfn, sorts\n    them in line by using keys in quantheader list.\"\"\"\n    allquants, sqlfields = quantdb.select_all_psm_quants(isobaric, precursor)\n    quant = next(allquants)\n    for rownr, psm in enumerate(readers.generate_tsv_psms(tsvfn, oldheader)):\n        outpsm = {x: y for x, y in psm.items()}\n        if precursor:\n            pquant = quant[sqlfields['precursor']]\n            if pquant is None:\n                pquant = 'NA'\n            outpsm.update({mzidtsvdata.HEADER_PRECURSOR_QUANT: str(pquant)})\n        if isobaric:\n            isoquants = {}\n            while quant[0] == rownr:\n                isoquants.update({quant[sqlfields['isochan']]:\n                                  str(quant[sqlfields['isoquant']])})\n                try:\n                    quant = next(allquants)\n                except StopIteration:\n                    # last PSM, break from while loop or it is not yielded at all\n                    break\n            outpsm.update(get_quant_NAs(isoquants, isob_header))\n        else:\n            try:\n                quant = next(allquants)\n            except StopIteration:\n                # last PSM, needs explicit yield/break or it will not be yielded\n                yield outpsm\n                break\n        yield outpsm", "response": "Takes tsvfn and connects gets quants for each line in tsvfn sorts them by using keys in isobaric and precursor are used to get the next PSM in the list of all possible PSMs in line by using keys in oldheader."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes quantdata in a dict and header with quantkeys . Returns dict of quant intensities with missing keys set to NA.", "response": "def get_quant_NAs(quantdata, quantheader):\n    \"\"\"Takes quantdata in a dict and header with quantkeys\n    (eg iTRAQ isotopes). Returns dict of quant intensities\n    with missing keys set to NA.\"\"\"\n    out = {}\n    for qkey in quantheader:\n        out[qkey] = quantdata.get(qkey, 'NA')\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef t_escaped_BACKSPACE_CHAR(self, t):\n        r'\\x62'  # 'b'\n        t.lexer.pop_state()\n        t.value = unichr(0x0008)\n        return t", "response": "t_escaped_BACKSPACE_CHAR - Handles backspace characters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef t_escaped_TAB_CHAR(self, t):\n        r'\\x74'  # 't'\n        t.lexer.pop_state()\n        t.value = unichr(0x0009)\n        return t", "response": "t_escaped_TAB_CHAR - Handle tab characters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninvoke the lexer on an input string and return the list of tokens.", "response": "def tokenize(self, data, *args, **kwargs):\n        \"\"\"Invoke the lexer on an input string an return the list of tokens.\n        This is relatively inefficient and should only be used for\n        testing/debugging as it slurps up all tokens into one list.\n        Args:\n          data: The input to be tokenized.\n        Returns:\n          A list of LexTokens\n        \"\"\"\n        self.lexer.input(data)\n        tokens = list()\n        while True:\n            token = self.lexer.token()\n            if not token:\n                break\n            tokens.append(token)\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_values(self, p):\n        if len(p) == 1:\n            p[0] = list()\n        else:\n            p[1].append(p[2])\n            p[0] = p[1]", "response": "values | values value VALUE_SEPARATOR\n                  | values value VALUE_SEPARATOR\n                  | values value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_chars(self, p):\n        if len(p) == 1:\n            p[0] = unicode()\n        else:\n            p[0] = p[1] + p[2]", "response": "chars | chars char"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the input JSON data string into a python dict or list of python dicts.", "response": "def parse(self, data, lexer=None, *args, **kwargs):\n        \"\"\"Parse the input JSON data string into a python data structure.\n        Args:\n          data: An input data string\n          lexer:  An optional ply.lex instance that overrides the default lexer.\n        Returns:\n          A python dict or list representing the input JSON data.\n        \"\"\"\n        if lexer is None:\n            lexer = self.lexer\n        return self.parser.parse(data, lexer=lexer, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_mzid_specfile_ids(mzidfn, namespace):\n    sid_fn = {}\n    for specdata in mzid_specdata_generator(mzidfn, namespace):\n        sid_fn[specdata.attrib['id']] = specdata.attrib['name']\n    return sid_fn", "response": "Returns mzid spectra data filenames and their IDs used in the\n    mzIdentML file as a dict. Keys == IDs values == fns"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_specidentitem_percolator_data(item, xmlns):\n    percomap = {'{0}userParam'.format(xmlns): PERCO_HEADERMAP, }\n    percodata = {}\n    for child in item:\n        try:\n            percoscore = percomap[child.tag][child.attrib['name']]\n        except KeyError:\n            continue\n        else:\n            percodata[percoscore] = child.attrib['value']\n    outkeys = [y for x in list(percomap.values()) for y in list(x.values())]\n    for key in outkeys:\n        try:\n            percodata[key]\n        except KeyError:\n            percodata[key] = 'NA'\n    return percodata", "response": "Loop through SpecIdentificationItem children and find the percolator data by matching to a dict lookup. Return a\n    dict containing percolator data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if you are running inside a python virtual environment.", "response": "def in_virtual_env():\n    \"\"\"\n    returns True if you are running inside a python virtual environment.\n    (DOES NOT WORK IF IN IPYTHON AND USING A VIRTUALENV)\n\n    sys.prefix gives the location of the virtualenv\n\n    Notes:\n        It seems IPython does not respect virtual environments properly.\n        TODO: find a solution\n        http://stackoverflow.com/questions/7335992/ipython-and-virtualenv-ignoring-site-packages\n\n    References:\n        http://stackoverflow.com/questions/1871549/python-determine-if-running-inside-virtualenv\n\n    CommandLine:\n        python -m utool.util_sysreq in_virtual_env\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_sysreq import *  # NOQA\n        >>> import utool as ut\n        >>> result = in_virtual_env()\n        >>> print(result)\n    \"\"\"\n    import sys\n    has_venv = False\n    if hasattr(sys, 'real_prefix'):\n        # For virtualenv module\n        has_venv = True\n    elif hasattr(sys, 'base_prefix'):\n        # For venv module\n        has_venv = sys.base_prefix != sys.prefix\n    return has_venv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_global_dist_packages_dir():\n    import utool as ut\n    if not ut.in_virtual_env():\n        # Non venv case\n        return get_site_packages_dir()\n    else:\n        candidates = []\n        if ut.LINUX:\n            import sys\n            candidates += [\n                '/usr/lib/python%s/dist-packages' % (sys.version[0:3],),\n                '/usr/lib/python%s/dist-packages' % (sys.version[0:1],),\n            ]\n        else:\n            raise NotImplementedError()\n        for path in candidates:\n            if ut.checkpath(path):\n                return path", "response": "Returns the path to the dist - packages directory for the current environment"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the path to the dist - packages directory for the current site", "response": "def get_local_dist_packages_dir():\n    \"\"\"\n    Attempts to work around virtualenvs and find the system dist_pacakges.\n    Essentially this is implmenented as a lookuptable\n    \"\"\"\n    import utool as ut\n    if not ut.in_virtual_env():\n        # Non venv case\n        return get_site_packages_dir()\n    else:\n        candidates = []\n        if ut.LINUX:\n            candidates += [\n                '/usr/local/lib/python2.7/dist-packages',\n            ]\n        else:\n            raise NotImplementedError()\n        for path in candidates:\n            if ut.checkpath(path):\n                return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the path to a resource in the current directory.", "response": "def locate_path(dname, recurse_down=True):\n    \"\"\" Search for a path \"\"\"\n    tried_fpaths = []\n    root_dir = os.getcwd()\n    while root_dir is not None:\n        dpath = join(root_dir, dname)\n        if exists(dpath):\n            return dpath\n        else:\n            tried_fpaths.append(dpath)\n        _new_root = dirname(root_dir)\n        if _new_root == root_dir:\n            root_dir = None\n            break\n        else:\n            root_dir = _new_root\n        if not recurse_down:\n            break\n    msg = 'Cannot locate dname=%r' % (dname,)\n    msg = ('\\n[sysreq!] Checked: '.join(tried_fpaths))\n    print(msg)\n    raise ImportError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef total_purge_developed_repo(repodir):\n    assert repodir is not None\n    import utool as ut\n    import os\n    repo = ut.util_git.Repo(dpath=repodir)\n\n    user = os.environ['USER']\n\n    fmtdict = dict(\n        user=user,\n        modname=repo.modname,\n        reponame=repo.reponame,\n        dpath=repo.dpath,\n        global_site_pkgs=ut.get_global_dist_packages_dir(),\n        local_site_pkgs=ut.get_local_dist_packages_dir(),\n        venv_site_pkgs=ut.get_site_packages_dir(),\n    )\n\n    commands = [_.format(**fmtdict) for _ in [\n        'pip uninstall {modname}',\n        'sudo -H pip uninstall {modname}',\n        'sudo pip uninstall {modname}',\n        'easy_install -m {modname}',\n        'cd {dpath} && python setup.py develop --uninstall',\n        # If they still exist try chowning to current user\n        'sudo chown -R {user}:{user} {dpath}',\n    ]]\n    print('Normal uninstall commands')\n    print('\\n'.join(commands))\n\n    possible_link_paths = [_.format(**fmtdict) for _ in [\n        '{dpath}/{modname}.egg-info',\n        '{dpath}/build',\n        '{venv_site_pkgs}/{reponame}.egg-info',\n        '{local_site_pkgs}/{reponame}.egg-info',\n        '{venv_site_pkgs}/{reponame}.egg-info',\n    ]]\n    from os.path import exists, basename\n    existing_link_paths = [path for path in possible_link_paths]\n    print('# Delete paths and eggs')\n    for path in existing_link_paths:\n        if exists(path):\n            if ut.get_file_info(path)['owner'] != user:\n                print('sudo /bin/rm -rf {path}'.format(path=path))\n            else:\n                print('/bin/rm -rf {path}'.format(path=path))\n        #ut.delete(path)\n\n    print('# Make sure nothing is in the easy install paths')\n    easyinstall_paths = [_.format(**fmtdict) for _ in [\n        '{venv_site_pkgs}/easy-install.pth',\n        '{local_site_pkgs}/easy-install.pth',\n        '{venv_site_pkgs}/easy-install.pth',\n    ]]\n    for path in easyinstall_paths:\n        if exists(path):\n            easy_install_list = ut.readfrom(path, verbose=False).strip().split('\\n')\n            easy_install_list_ = [basename(p) for p in easy_install_list]\n            index1 = ut.listfind(easy_install_list_, repo.reponame)\n            index2 = ut.listfind(easy_install_list_, repo.modname)\n            if index1 is not None or index2 is not None:\n                print('Found at index1=%r, index=%r' % (index1, index2))\n                if ut.get_file_info(path)['owner'] != user:\n                    print('sudo gvim {path}'.format(path=path))\n                else:\n                    print('gvim {path}'.format(path=path))\n\n    checkcmds = [_.format(**fmtdict) for _ in [\n        'python -c \"import {modname}; print({modname}.__file__)\"'\n    ]]\n    import sys\n    assert repo.modname not in sys.modules\n    print(\"# CHECK STATUS\")\n    for cmd in checkcmds:\n        print(cmd)", "response": "r Purges all developed packages in a repository"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_dict(self, dyn_dict):\n        'Adds a dictionary to the prefs'\n        if not isinstance(dyn_dict, dict):\n            raise Exception('DynStruct.add_dict expects a dictionary.' +\n                            'Recieved: ' + six.text_type(type(dyn_dict)))\n        for (key, val) in six.iteritems(dyn_dict):\n            self[key] = val", "response": "Adds a dictionary to the prefs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the object to a dictionary.", "response": "def to_dict(self):\n        \"\"\"Converts dynstruct to a dictionary.  \"\"\"\n        dyn_dict = {}\n        for (key, val) in six.iteritems(self.__dict__):\n            if key not in self._printable_exclude:\n                dyn_dict[key] = val\n        return dyn_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string which when evaluated will add the stored variables to the current namespace localname is the name of the variable in the current scope", "response": "def execstr(self, local_name):\n        \"\"\"returns a string which when evaluated will\n           add the stored variables to the current namespace\n\n           localname is the name of the variable in the current scope\n           * use locals().update(dyn.to_dict()) instead\n        \"\"\"\n        execstr = ''\n        for (key, val) in six.iteritems(self.__dict__):\n            if key not in self._printable_exclude:\n                execstr += key + ' = ' + local_name + '.' + key + '\\n'\n        return execstr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of proteins for a passed psm_id", "response": "def get_proteins_for_peptide(self, psm_id):\n        \"\"\"Returns list of proteins for a passed psm_id\"\"\"\n        protsql = self.get_sql_select(['protein_acc'], 'protein_psm')\n        protsql = '{0} WHERE psm_id=?'.format(protsql)\n        cursor = self.get_cursor()\n        proteins = cursor.execute(protsql, psm_id).fetchall()\n        return [x[0] for x in proteins]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle(self, *args, **options):\n        counter = 0\n        for key in options:\n            if options[key]:\n                counter += 1\n\n        # If no options are set, do a normal patch\n        if counter == 1:\n            options['default'] = True\n        ###########################################################################################\n\n        tag_succeed = 1\n\n        if APISettings.GIT_TAG_AUTO_COMMIT:\n            Git.add()\n            Git.commit()\n\n        if options['default']:\n            tag_succeed = Git.tag()\n\n        if options['staging']:\n            tag_succeed = Git.tag(APISettings.GIT_STAGING_PRE_TAG)\n\n        if options['production']:\n            tag_succeed = Git.tag(APISettings.GIT_ACTIVATE_PRE_TAG)\n\n        if options['push'] | APISettings.GIT_TAG_AUTO_TAG_PUSH:\n            if tag_succeed:\n                Git.push_tags()", "response": "Handles the command line arguments and options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck a frame and raises the relevant exception if required.", "response": "def raise_if_error(frame):\n    \"\"\"\n    Checks a frame and raises the relevant exception if required.\n    \"\"\"\n    if \"status\" not in frame or frame[\"status\"] == b\"\\x00\":\n        return\n    codes_and_exceptions = {\n        b\"\\x01\": exceptions.ZigBeeUnknownError,\n        b\"\\x02\": exceptions.ZigBeeInvalidCommand,\n        b\"\\x03\": exceptions.ZigBeeInvalidParameter,\n        b\"\\x04\": exceptions.ZigBeeTxFailure\n    }\n    if frame[\"status\"] in codes_and_exceptions:\n        raise codes_and_exceptions[frame[\"status\"]]()\n    raise exceptions.ZigBeeUnknownStatus()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hex_to_int(value):\n    if version_info.major >= 3:\n        return int.from_bytes(value, \"big\")\n    return int(value.encode(\"hex\"), 16)", "response": "Convert hex string like \\ x0A \\ xxE3 to 2787."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adc_to_percentage(value, max_volts, clamp=True):\n    percentage = (100.0 / const.ADC_MAX_VAL) * value\n    return max(min(100, percentage), 0) if clamp else percentage", "response": "Convert the ADC raw value to a percentage."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_adc(value, output_type, max_volts):\n    return {\n        const.ADC_RAW: lambda x: x,\n        const.ADC_PERCENTAGE: adc_to_percentage,\n        const.ADC_VOLTS: adc_to_volts,\n        const.ADC_MILLIVOLTS: adc_to_millivolts\n    }[output_type](value, max_volts)", "response": "Converts the output from the ADC into the desired type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef next_frame_id(self):\n        # Python 2/3 compatible way of converting 1 to \"\\x01\" in py2 or b\"\\x01\"\n        # in py3.\n        fid = bytes(bytearray((self._frame_id,)))\n        self._frame_id += 1\n        if self._frame_id > 0xFF:\n            self._frame_id = 1\n        try:\n            del self._rx_frames[fid]\n        except KeyError:\n            pass\n        return fid", "response": "Gets a byte of the next valid frame ID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle a received frame.", "response": "def _frame_received(self, frame):\n        \"\"\"\n        Put the frame into the _rx_frames dict with a key of the frame_id.\n        \"\"\"\n        try:\n            self._rx_frames[frame[\"frame_id\"]] = frame\n        except KeyError:\n            # Has no frame_id, ignore?\n            pass\n        _LOGGER.debug(\"Frame received: %s\", frame)\n        # Give the frame to any interested functions\n        for handler in self._rx_handlers:\n            handler(frame)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _send(self, **kwargs):\n        if kwargs.get(\"dest_addr_long\") is not None:\n            self.zb.remote_at(**kwargs)\n        else:\n            self.zb.at(**kwargs)", "response": "Send a frame to either the local ZigBee or a remote device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a frame to either the local ZigBee or a remote device and wait for its response.", "response": "def _send_and_wait(self, **kwargs):\n        \"\"\"\n        Send a frame to either the local ZigBee or a remote device and wait\n        for a pre-defined amount of time for its response.\n        \"\"\"\n        frame_id = self.next_frame_id\n        kwargs.update(dict(frame_id=frame_id))\n        self._send(**kwargs)\n        timeout = datetime.now() + const.RX_TIMEOUT\n        while datetime.now() < timeout:\n            try:\n                frame = self._rx_frames.pop(frame_id)\n                raise_if_error(frame)\n                return frame\n            except KeyError:\n                sleep(0.1)\n                continue\n        _LOGGER.exception(\n            \"Did not receive response within configured timeout period.\")\n        raise exceptions.ZigBeeResponseTimeout()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_parameter(self, parameter, dest_addr_long=None):\n        frame = self._send_and_wait(\n            command=parameter, dest_addr_long=dest_addr_long)\n        return frame[\"parameter\"]", "response": "Fetches and returns the value of the specified parameter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sample(self, dest_addr_long=None):\n        frame = self._send_and_wait(\n            command=b\"IS\", dest_addr_long=dest_addr_long)\n        if \"parameter\" in frame:\n            # @TODO: Is there always one value? Is it always a list?\n            return frame[\"parameter\"][0]\n        return {}", "response": "Initiate a sample and return its data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a digital pin from the ZigBee and returns the boolean value of the requested digital facility.", "response": "def read_digital_pin(self, pin_number, dest_addr_long=None):\n        \"\"\"\n        Fetches a sample and returns the boolean value of the requested digital\n        pin.\n        \"\"\"\n        sample = self.get_sample(dest_addr_long=dest_addr_long)\n        try:\n            return sample[const.DIGITAL_PINS[pin_number]]\n        except KeyError:\n            raise exceptions.ZigBeePinNotConfigured(\n                \"Pin %s (%s) is not configured as a digital input or output.\"\n                % (pin_number, const.IO_PIN_COMMANDS[pin_number]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading an analog entry from the ZigBee and returns the value of the requested analog entry.", "response": "def read_analog_pin(\n            self, pin_number, adc_max_volts,\n            dest_addr_long=None, output_type=const.ADC_RAW):\n        \"\"\"\n        Fetches a sample and returns the integer value of the requested analog\n        pin. output_type should be one of the following constants from\n        xbee_helper.const:\n\n        - ADC_RAW\n        - ADC_PERCENTAGE\n        - ADC_VOLTS\n        - ADC_MILLIVOLTS\n        \"\"\"\n        sample = self.get_sample(dest_addr_long=dest_addr_long)\n        try:\n            return convert_adc(\n                sample[const.ANALOG_PINS[pin_number]],\n                output_type,\n                adc_max_volts\n            )\n        except KeyError:\n            raise exceptions.ZigBeePinNotConfigured(\n                \"Pin %s (%s) is not configured as an analog input.\" % (\n                    pin_number, const.IO_PIN_COMMANDS[pin_number]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a gpio pin setting.", "response": "def set_gpio_pin(self, pin_number, setting, dest_addr_long=None):\n        \"\"\"\n        Set a gpio pin setting.\n        \"\"\"\n        assert setting in const.GPIO_SETTINGS.values()\n        self._send_and_wait(\n            command=const.IO_PIN_COMMANDS[pin_number],\n            parameter=setting.value,\n            dest_addr_long=dest_addr_long)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a gpio pin setting.", "response": "def get_gpio_pin(self, pin_number, dest_addr_long=None):\n        \"\"\"\n        Get a gpio pin setting.\n        \"\"\"\n        frame = self._send_and_wait(\n            command=const.IO_PIN_COMMANDS[pin_number],\n            dest_addr_long=dest_addr_long\n        )\n        value = frame[\"parameter\"]\n        return const.GPIO_SETTINGS[value]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the supply voltage from the resource.", "response": "def get_supply_voltage(self, dest_addr_long=None):\n        \"\"\"\n        Fetches the value of %V and returns it as volts.\n        \"\"\"\n        value = self._get_parameter(b\"%V\", dest_addr_long=dest_addr_long)\n        return (hex_to_int(value) * (1200/1024.0)) / 1000"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, key):\n        if key not in self._map:\n            self._map[key] = link = _Link()\n            root = self._root\n            last = root.prev\n            link.prev, link.next, link.key = last, root, key\n            last.next = root.prev = weakref.proxy(link)", "response": "Store new key in a new link at the end of the linked list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef union(cls, *sets):\n        import utool as ut\n        lists_ = ut.flatten([list(s) for s in sets])\n        return cls(lists_)", "response": "Return a new set with the union of sets."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef index(self, item):\n        for count, other in enumerate(self):\n            if item == other:\n                return count\n        raise ValueError('%r is not in OrderedSet' % (item,))", "response": "Find the index of item in the OrderedSet"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value(self, key, timestamp=None, namespace=None):\n        return self.make_context(key=key, end=timestamp,\n                                 namespace=namespace).value()", "response": "Get the value of a gauge at the specified time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an aggregate of all gauge data stored in the specified date range", "response": "def aggregate(self, key, aggregate, start=None, end=None,\n                  namespace=None, percentile=None):\n        \"\"\"Get an aggregate of all gauge data stored in the specified date\n        range\"\"\"\n        return self.make_context(key=key, aggregate=aggregate, start=start,\n                                 end=end, namespace=namespace,\n                                 percentile=percentile).aggregate()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value_series(self, key, start=None, end=None, interval=None,\n                     namespace=None, cache=None):\n        \"\"\"Get a time series of gauge values\"\"\"\n        return self.make_context(key=key, start=start, end=end,\n                                 interval=interval, namespace=namespace,\n                                 cache=cache).value_series()", "response": "Get a time series of gauge values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a time series of gauge aggregates", "response": "def aggregate_series(self, key, aggregate, start=None, end=None,\n                         interval=None, namespace=None, cache=None,\n                         percentile=None):\n        \"\"\"Get a time series of gauge aggregates\"\"\"\n        return self.make_context(key=key, aggregate=aggregate, start=start,\n                                 end=end, interval=interval,\n                                 namespace=namespace, cache=cache,\n                                 percentile=percentile).aggregate_series()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting write statistics for the specified namespace and date range", "response": "def statistics(self, start=None, end=None, namespace=None):\n        \"\"\"Get write statistics for the specified namespace and date range\"\"\"\n        return self.make_context(start=start, end=end,\n                                 namespace=namespace).statistics()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sync(self):\n        self.driver.create_schema()\n        self.driver.set_metadata({\n            'current_version': Gauged.VERSION,\n            'initial_version': Gauged.VERSION,\n            'block_size': self.config.block_size,\n            'resolution': self.config.resolution,\n            'created_at': long(time() * 1000)\n        }, replace=False)", "response": "Sync the schema of the current version of the gauged instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_context(self, **kwargs):\n        self.check_schema()\n        return Context(self.driver, self.config, **kwargs)", "response": "Create a new context for reading data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_schema(self):\n        if self.valid_schema:\n            return\n        config = self.config\n        metadata = self.metadata()\n        if 'current_version' not in metadata:\n            raise GaugedSchemaError('Gauged schema not found, '\n                                    'try a gauged.sync()')\n        if metadata['current_version'] != Gauged.VERSION:\n            msg = 'The schema is version %s while this Gauged is version %s. '\n            msg += 'Try upgrading Gauged and/or running gauged_migrate.py'\n            msg = msg % (metadata['current_version'], Gauged.VERSION)\n            raise GaugedVersionMismatchError(msg)\n        expected_block_size = '%s/%s' % (config.block_size, config.resolution)\n        block_size = '%s/%s' % (metadata['block_size'], metadata['resolution'])\n        if block_size != expected_block_size:\n            msg = 'Expected %s and got %s' % (expected_block_size, block_size)\n            warn(msg, GaugedBlockSizeMismatch)\n        self.valid_schema = True", "response": "Check the schema exists and matches the configuration"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nx_topsort_rank(graph, nodes=None):\n    import utool as ut\n    if False:\n        # Determenistic version\n        # Ok, this doesn't work.\n        dag_ranks = nx_dag_node_rank(graph, nodes)\n        toprank = ut.argsort(dag_ranks, list(map(str, nodes)))\n    else:\n        # Non-determenistic version\n        dag_ranks = nx_dag_node_rank(graph, nodes)\n        topsort = list(nx.topological_sort(graph))\n        # print('topsort = %r' % (topsort,))\n        node_to_top_rank = ut.make_index_lookup(topsort)\n        toprank = ut.dict_take(node_to_top_rank, nodes)\n    return toprank", "response": "Topological sort of a node - ranked list of nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nx_transitive_reduction(G, mode=1):\n\n    import utool as ut\n    has_cycles = not nx.is_directed_acyclic_graph(G)\n    if has_cycles:\n        # FIXME: this does not work for cycle graphs.\n        # Need to do algorithm on SCCs\n        G_orig = G\n        G = nx.condensation(G_orig)\n\n    nodes = list(G.nodes())\n    node2_idx = ut.make_index_lookup(nodes)\n\n    # For each node u, perform DFS consider its set of (non-self) children C.\n    # For each descendant v, of a node in C, remove any edge from u to v.\n\n    if mode == 1:\n        G_tr = G.copy()\n\n        for parent in G_tr.nodes():\n            # Remove self loops\n            if G_tr.has_edge(parent, parent):\n                G_tr.remove_edge(parent, parent)\n            # For each child of the parent\n            for child in list(G_tr.successors(parent)):\n                # Preorder nodes includes its argument (no added complexity)\n                for gchild in list(G_tr.successors(child)):\n                    # Remove all edges from parent to non-child descendants\n                    for descendant in nx.dfs_preorder_nodes(G_tr, gchild):\n                        if G_tr.has_edge(parent, descendant):\n                            G_tr.remove_edge(parent, descendant)\n\n        if has_cycles:\n            # Uncondense graph\n            uncondensed_G_tr = G.__class__()\n            mapping = G.graph['mapping']\n            uncondensed_G_tr.add_nodes_from(mapping.keys())\n            inv_mapping = ut.invert_dict(mapping, unique_vals=False)\n            for u, v in G_tr.edges():\n                u_ = inv_mapping[u][0]\n                v_ = inv_mapping[v][0]\n                uncondensed_G_tr.add_edge(u_, v_)\n\n            for key, path in inv_mapping.items():\n                if len(path) > 1:\n                    directed_cycle = list(ut.itertwo(path, wrap=True))\n                    uncondensed_G_tr.add_edges_from(directed_cycle)\n            G_tr = uncondensed_G_tr\n\n    else:\n\n        def make_adj_matrix(G):\n            edges = list(G.edges())\n            edge2_idx = ut.partial(ut.dict_take, node2_idx)\n            uv_list = ut.lmap(edge2_idx, edges)\n            A = np.zeros((len(nodes), len(nodes)))\n            A[tuple(np.array(uv_list).T)] = 1\n            return A\n\n        G_ = nx.dag.transitive_closure(G)\n\n        A = make_adj_matrix(G)\n        B = make_adj_matrix(G_)\n\n        #AB = A * B\n        #AB = A.T.dot(B)\n        AB = A.dot(B)\n        #AB = A.dot(B.T)\n\n        A_and_notAB = np.logical_and(A, np.logical_not(AB))\n        tr_uvs = np.where(A_and_notAB)\n\n        #nodes = G.nodes()\n        edges = list(zip(*ut.unflat_take(nodes, tr_uvs)))\n\n        G_tr = G.__class__()\n        G_tr.add_nodes_from(nodes)\n        G_tr.add_edges_from(edges)\n\n        if has_cycles:\n            # Uncondense graph\n            uncondensed_G_tr = G.__class__()\n            mapping = G.graph['mapping']\n            uncondensed_G_tr.add_nodes_from(mapping.keys())\n            inv_mapping = ut.invert_dict(mapping, unique_vals=False)\n            for u, v in G_tr.edges():\n                u_ = inv_mapping[u][0]\n                v_ = inv_mapping[v][0]\n                uncondensed_G_tr.add_edge(u_, v_)\n\n            for key, path in inv_mapping.items():\n                if len(path) > 1:\n                    directed_cycle = list(ut.itertwo(path, wrap=True))\n                    uncondensed_G_tr.add_edges_from(directed_cycle)\n            G_tr = uncondensed_G_tr\n    return G_tr", "response": "This function is a simple test graph that uses the transitive closure of the graph G and returns a list of the unique identifiers for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning rank of nodes that define the level each node is on in a", "response": "def nx_dag_node_rank(graph, nodes=None):\n    \"\"\"\n    Returns rank of nodes that define the \"level\" each node is on in a\n    topological sort. This is the same as the Graphviz dot rank.\n\n    Ignore:\n        simple_graph = ut.simplify_graph(exi_graph)\n        adj_dict = ut.nx_to_adj_dict(simple_graph)\n        import plottool as pt\n        pt.qt4ensure()\n        pt.show_nx(graph)\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> adj_dict = {0: [5], 1: [5], 2: [1], 3: [4], 4: [0], 5: [], 6: [4], 7: [9], 8: [6], 9: [1]}\n        >>> nodes = [2, 1, 5]\n        >>> f_graph = ut.nx_from_adj_dict(adj_dict, nx.DiGraph)\n        >>> graph = f_graph.reverse()\n        >>> #ranks = ut.nx_dag_node_rank(graph, nodes)\n        >>> ranks = ut.nx_dag_node_rank(graph, nodes)\n        >>> result = ('ranks = %r' % (ranks,))\n        >>> print(result)\n        ranks = [3, 2, 1]\n    \"\"\"\n    import utool as ut\n    source = list(ut.nx_source_nodes(graph))[0]\n    longest_paths = dict([(target, dag_longest_path(graph, source, target))\n                          for target in graph.nodes()])\n    node_to_rank = ut.map_dict_vals(len, longest_paths)\n    if nodes is None:\n        return node_to_rank\n    else:\n        ranks = ut.dict_take(node_to_rank, nodes)\n        return ranks"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nx_all_nodes_between(graph, source, target, data=False):\n    import utool as ut\n    if source is None:\n        # assume there is a single source\n        sources = list(ut.nx_source_nodes(graph))\n        assert len(sources) == 1, (\n            'specify source if there is not only one')\n        source = sources[0]\n    if target is None:\n        # assume there is a single source\n        sinks = list(ut.nx_sink_nodes(graph))\n        assert len(sinks) == 1, (\n            'specify sink if there is not only one')\n        target = sinks[0]\n    all_simple_paths = list(nx.all_simple_paths(graph, source, target))\n    nodes = sorted(set.union(*map(set, all_simple_paths)))\n    return nodes", "response": "Find all nodes with on paths between source and target."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all simple edge paths from source to target.", "response": "def nx_all_simple_edge_paths(G, source, target, cutoff=None, keys=False,\n                             data=False):\n    \"\"\"\n    Returns each path from source to target as a list of edges.\n\n    This function is meant to be used with MultiGraphs or MultiDiGraphs.\n    When ``keys`` is True each edge in the path is returned with its unique key\n    identifier. In this case it is possible to distinguish between different\n    paths along different edges between the same two nodes.\n\n    Derived from simple_paths.py in networkx\n    \"\"\"\n    if cutoff is None:\n        cutoff = len(G) - 1\n    if cutoff < 1:\n        return\n    import utool as ut\n    import six\n    visited_nodes = [source]\n    visited_edges = []\n    if G.is_multigraph():\n        get_neighbs = ut.partial(G.edges, keys=keys, data=data)\n    else:\n        get_neighbs = ut.partial(G.edges, data=data)\n    edge_stack = [iter(get_neighbs(source))]\n    while edge_stack:\n        children_edges = edge_stack[-1]\n        child_edge = six.next(children_edges, None)\n        if child_edge is None:\n            edge_stack.pop()\n            visited_nodes.pop()\n            if len(visited_edges) > 0:\n                visited_edges.pop()\n        elif len(visited_nodes) < cutoff:\n            child_node = child_edge[1]\n            if child_node == target:\n                yield visited_edges + [child_edge]\n            elif child_node not in visited_nodes:\n                visited_nodes.append(child_node)\n                visited_edges.append(child_edge)\n                edge_stack.append(iter(get_neighbs(child_node)))\n        else:\n            for edge in [child_edge] + list(children_edges):\n                if edge[1] == target:\n                    yield visited_edges + [edge]\n            edge_stack.pop()\n            visited_nodes.pop()\n            if len(visited_edges) > 0:\n                visited_edges.pop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves node attributes from the given node graph", "response": "def nx_delete_node_attr(graph, name, nodes=None):\n    \"\"\"\n    Removes node attributes\n\n    Doctest:\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.karate_club_graph()\n        >>> nx.set_node_attributes(G, name='foo', values='bar')\n        >>> datas = nx.get_node_attributes(G, 'club')\n        >>> assert len(nx.get_node_attributes(G, 'club')) == 34\n        >>> assert len(nx.get_node_attributes(G, 'foo')) == 34\n        >>> ut.nx_delete_node_attr(G, ['club', 'foo'], nodes=[1, 2])\n        >>> assert len(nx.get_node_attributes(G, 'club')) == 32\n        >>> assert len(nx.get_node_attributes(G, 'foo')) == 32\n        >>> ut.nx_delete_node_attr(G, ['club'])\n        >>> assert len(nx.get_node_attributes(G, 'club')) == 0\n        >>> assert len(nx.get_node_attributes(G, 'foo')) == 32\n    \"\"\"\n    if nodes is None:\n        nodes = list(graph.nodes())\n    removed = 0\n    # names = [name] if not isinstance(name, list) else name\n    node_dict = nx_node_dict(graph)\n\n    if isinstance(name, list):\n        for node in nodes:\n            for name_ in name:\n                try:\n                    del node_dict[node][name_]\n                    removed += 1\n                except KeyError:\n                    pass\n    else:\n        for node in nodes:\n            try:\n                del node_dict[node][name]\n                removed += 1\n            except KeyError:\n                pass\n    return removed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove an attribute from specific edges in a graph", "response": "def nx_delete_edge_attr(graph, name, edges=None):\n    \"\"\"\n    Removes an attributes from specific edges in the graph\n\n    Doctest:\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.karate_club_graph()\n        >>> nx.set_edge_attributes(G, name='spam', values='eggs')\n        >>> nx.set_edge_attributes(G, name='foo', values='bar')\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 78\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 78\n        >>> ut.nx_delete_edge_attr(G, ['spam', 'foo'], edges=[(1, 2)])\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 77\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 77\n        >>> ut.nx_delete_edge_attr(G, ['spam'])\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 0\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 77\n\n    Doctest:\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.MultiGraph()\n        >>> G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 5), (4, 5), (1, 2)])\n        >>> nx.set_edge_attributes(G, name='spam', values='eggs')\n        >>> nx.set_edge_attributes(G, name='foo', values='bar')\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 6\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 6\n        >>> ut.nx_delete_edge_attr(G, ['spam', 'foo'], edges=[(1, 2, 0)])\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 5\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 5\n        >>> ut.nx_delete_edge_attr(G, ['spam'])\n        >>> assert len(nx.get_edge_attributes(G, 'spam')) == 0\n        >>> assert len(nx.get_edge_attributes(G, 'foo')) == 5\n    \"\"\"\n    removed = 0\n    keys = [name] if not isinstance(name, (list, tuple)) else name\n    if edges is None:\n        if graph.is_multigraph():\n            edges = graph.edges(keys=True)\n        else:\n            edges = graph.edges()\n    if graph.is_multigraph():\n        for u, v, k in edges:\n            for key_ in keys:\n                try:\n                    del graph[u][v][k][key_]\n                    removed += 1\n                except KeyError:\n                    pass\n    else:\n        for u, v in edges:\n            for key_ in keys:\n                try:\n                    del graph[u][v][key_]\n                    removed += 1\n                except KeyError:\n                    pass\n    return removed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nx_gen_node_values(G, key, nodes, default=util_const.NoParam):\n    node_dict = nx_node_dict(G)\n    if default is util_const.NoParam:\n        return (node_dict[n][key] for n in nodes)\n    else:\n        return (node_dict[n].get(key, default) for n in nodes)", "response": "Generates attributes values of specific nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimproves generator version of nx.get_node_attributes Args: on_missing (str): Strategy for handling nodes missing from G. Can be {'error', 'default', 'filter'}. defaults to 'error'. on_keyerr (str): Strategy for handling keys missing from node dicts. Can be {'error', 'default', 'filter'}. defaults to 'default' if default is specified, otherwise defaults to 'error'. Notes: strategies are: error - raises an error if key or node does not exist default - returns node, but uses value specified by default filter - skips the node Example: >>> # ENABLE_DOCTEST >>> from utool.util_graph import * # NOQA >>> import utool as ut >>> G = nx.Graph([(1, 2), (2, 3)]) >>> nx.set_node_attributes(G, name='part', values={1: 'bar', 3: 'baz'}) >>> nodes = [1, 2, 3, 4] >>> # >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', default=None, on_missing='error', on_keyerr='default'))) == 3 >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', default=None, on_missing='error', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', on_missing='error', on_keyerr='error')) >>> # >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='filter', on_keyerr='default'))) == 3 >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='filter', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', nodes, on_missing='filter', on_keyerr='error')) >>> # >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='default', on_keyerr='default'))) == 4 >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='default', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', nodes, on_missing='default', on_keyerr='error')) Example: >>> # DISABLE_DOCTEST >>> # ALL CASES >>> from utool.util_graph import * # NOQA >>> import utool as ut >>> G = nx.Graph([(1, 2), (2, 3)]) >>> nx.set_node_attributes(G, name='full', values={1: 'A', 2: 'B', 3: 'C'}) >>> nx.set_node_attributes(G, name='part', values={1: 'bar', 3: 'baz'}) >>> nodes = [1, 2, 3, 4] >>> attrs = dict(ut.nx_gen_node_attrs(G, 'full')) >>> input_grid = { >>> 'nodes': [None, (1, 2, 3, 4)], >>> 'key': ['part', 'full'], >>> 'default': [util_const.NoParam, None], >>> } >>> inputs = ut.all_dict_combinations(input_grid) >>> kw_grid = { >>> 'on_missing': ['error', 'default', 'filter'], >>> 'on_keyerr': ['error', 'default', 'filter'], >>> } >>> kws = ut.all_dict_combinations(kw_grid) >>> for in_ in inputs: >>> for kw in kws: >>> kw2 = ut.dict_union(kw, in_) >>> #print(kw2) >>> on_missing = kw['on_missing'] >>> on_keyerr = kw['on_keyerr'] >>> if on_keyerr == 'default' and in_['default'] is util_const.NoParam: >>> on_keyerr = 'error' >>> will_miss = False >>> will_keyerr = False >>> if on_missing == 'error': >>> if in_['key'] == 'part' and in_['nodes'] is not None: >>> will_miss = True >>> if in_['key'] == 'full' and in_['nodes'] is not None: >>> will_miss = True >>> if on_keyerr == 'error': >>> if in_['key'] == 'part': >>> will_keyerr = True >>> if on_missing == 'default': >>> if in_['key'] == 'full' and in_['nodes'] is not None: >>> will_keyerr = True >>> want_error = will_miss or will_keyerr >>> gen = ut.nx_gen_node_attrs(G, **kw2) >>> try: >>> attrs = list(gen) >>> except KeyError: >>> if not want_error: >>> raise AssertionError('should not have errored') >>> else: >>> if want_error: >>> raise AssertionError('should have errored')", "response": "def nx_gen_node_attrs(G, key, nodes=None, default=util_const.NoParam,\n                      on_missing='error', on_keyerr='default'):\n    \"\"\"\n    Improved generator version of nx.get_node_attributes\n\n    Args:\n        on_missing (str): Strategy for handling nodes missing from G.\n            Can be {'error', 'default', 'filter'}.  defaults to 'error'.\n        on_keyerr (str): Strategy for handling keys missing from node dicts.\n            Can be {'error', 'default', 'filter'}.  defaults to 'default'\n            if default is specified, otherwise defaults to 'error'.\n\n    Notes:\n        strategies are:\n            error - raises an error if key or node does not exist\n            default - returns node, but uses value specified by default\n            filter - skips the node\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.Graph([(1, 2), (2, 3)])\n        >>> nx.set_node_attributes(G, name='part', values={1: 'bar', 3: 'baz'})\n        >>> nodes = [1, 2, 3, 4]\n        >>> #\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', default=None, on_missing='error', on_keyerr='default'))) == 3\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', default=None, on_missing='error', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', on_missing='error', on_keyerr='error'))\n        >>> #\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='filter', on_keyerr='default'))) == 3\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='filter', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', nodes, on_missing='filter', on_keyerr='error'))\n        >>> #\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='default', on_keyerr='default'))) == 4\n        >>> assert len(list(ut.nx_gen_node_attrs(G, 'part', nodes, default=None, on_missing='default', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, ut.nx_gen_node_attrs(G, 'part', nodes, on_missing='default', on_keyerr='error'))\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> # ALL CASES\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.Graph([(1, 2), (2, 3)])\n        >>> nx.set_node_attributes(G, name='full', values={1: 'A', 2: 'B', 3: 'C'})\n        >>> nx.set_node_attributes(G, name='part', values={1: 'bar', 3: 'baz'})\n        >>> nodes = [1, 2, 3, 4]\n        >>> attrs = dict(ut.nx_gen_node_attrs(G, 'full'))\n        >>> input_grid = {\n        >>>     'nodes': [None, (1, 2, 3, 4)],\n        >>>     'key': ['part', 'full'],\n        >>>     'default': [util_const.NoParam, None],\n        >>> }\n        >>> inputs = ut.all_dict_combinations(input_grid)\n        >>> kw_grid = {\n        >>>     'on_missing': ['error', 'default', 'filter'],\n        >>>     'on_keyerr': ['error', 'default', 'filter'],\n        >>> }\n        >>> kws = ut.all_dict_combinations(kw_grid)\n        >>> for in_ in inputs:\n        >>>     for kw in kws:\n        >>>         kw2 = ut.dict_union(kw, in_)\n        >>>         #print(kw2)\n        >>>         on_missing = kw['on_missing']\n        >>>         on_keyerr = kw['on_keyerr']\n        >>>         if on_keyerr == 'default' and in_['default'] is util_const.NoParam:\n        >>>             on_keyerr = 'error'\n        >>>         will_miss = False\n        >>>         will_keyerr = False\n        >>>         if on_missing == 'error':\n        >>>             if in_['key'] == 'part' and in_['nodes'] is not None:\n        >>>                 will_miss = True\n        >>>             if in_['key'] == 'full' and in_['nodes'] is not None:\n        >>>                 will_miss = True\n        >>>         if on_keyerr == 'error':\n        >>>             if in_['key'] == 'part':\n        >>>                 will_keyerr = True\n        >>>             if on_missing == 'default':\n        >>>                 if in_['key'] == 'full' and in_['nodes'] is not None:\n        >>>                     will_keyerr = True\n        >>>         want_error = will_miss or will_keyerr\n        >>>         gen = ut.nx_gen_node_attrs(G, **kw2)\n        >>>         try:\n        >>>             attrs = list(gen)\n        >>>         except KeyError:\n        >>>             if not want_error:\n        >>>                 raise AssertionError('should not have errored')\n        >>>         else:\n        >>>             if want_error:\n        >>>                 raise AssertionError('should have errored')\n\n    \"\"\"\n    if on_missing is None:\n        on_missing = 'error'\n    if default is util_const.NoParam and on_keyerr == 'default':\n        on_keyerr = 'error'\n    if nodes is None:\n        nodes = G.nodes()\n    # Generate `node_data` nodes and data dictionary\n    node_dict = nx_node_dict(G)\n    if on_missing == 'error':\n        node_data = ((n, node_dict[n]) for n in nodes)\n    elif on_missing == 'filter':\n        node_data = ((n, node_dict[n]) for n in nodes if n in G)\n    elif on_missing == 'default':\n        node_data = ((n, node_dict.get(n, {})) for n in nodes)\n    else:\n        raise KeyError('on_missing={} must be error, filter or default'.format(\n            on_missing))\n    # Get `node_attrs` desired value out of dictionary\n    if on_keyerr == 'error':\n        node_attrs = ((n, d[key]) for n, d in node_data)\n    elif on_keyerr == 'filter':\n        node_attrs = ((n, d[key]) for n, d in node_data if key in d)\n    elif on_keyerr == 'default':\n        node_attrs = ((n, d.get(key, default)) for n, d in node_data)\n    else:\n        raise KeyError('on_keyerr={} must be error filter or default'.format(on_keyerr))\n    return node_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating edge values of specific nodes and attributes of specific nodes.", "response": "def nx_gen_edge_values(G, key, edges=None, default=util_const.NoParam,\n                       on_missing='error', on_keyerr='default'):\n    \"\"\"\n    Generates attributes values of specific edges\n\n    Args:\n        on_missing (str): Strategy for handling nodes missing from G.\n            Can be {'error', 'default'}.  defaults to 'error'.\n        on_keyerr (str): Strategy for handling keys missing from node dicts.\n            Can be {'error', 'default'}.  defaults to 'default'\n            if default is specified, otherwise defaults to 'error'.\n    \"\"\"\n    if edges is None:\n        edges = G.edges()\n    if on_missing is None:\n        on_missing = 'error'\n    if on_keyerr is None:\n        on_keyerr = 'default'\n    if default is util_const.NoParam and on_keyerr == 'default':\n        on_keyerr = 'error'\n    # Generate `data_iter` edges and data dictionary\n    if on_missing == 'error':\n        data_iter = (G.adj[u][v] for u, v in edges)\n    elif on_missing == 'default':\n        data_iter = (G.adj[u][v] if G.has_edge(u, v) else {}\n                     for u, v in edges)\n    else:\n        raise KeyError('on_missing={} must be error, filter or default'.format(\n            on_missing))\n    # Get `value_iter` desired value out of dictionary\n    if on_keyerr == 'error':\n        value_iter = (d[key] for d in data_iter)\n    elif on_keyerr == 'default':\n        value_iter = (d.get(key, default) for d in data_iter)\n    else:\n        raise KeyError('on_keyerr={} must be error or default'.format(on_keyerr))\n    return value_iter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimproving generator version of nx.get_edge_attributes Args: on_missing (str): Strategy for handling nodes missing from G. Can be {'error', 'default', 'filter'}. defaults to 'error'. is on_missing is not error, then we allow any edge even if the endpoints are not in the graph. on_keyerr (str): Strategy for handling keys missing from node dicts. Can be {'error', 'default', 'filter'}. defaults to 'default' if default is specified, otherwise defaults to 'error'. Example: >>> # ENABLE_DOCTEST >>> from utool.util_graph import * # NOQA >>> import utool as ut >>> G = nx.Graph([(1, 2), (2, 3), (3, 4)]) >>> nx.set_edge_attributes(G, name='part', values={(1, 2): 'bar', (2, 3): 'baz'}) >>> edges = [(1, 2), (2, 3), (3, 4), (4, 5)] >>> func = ut.partial(ut.nx_gen_edge_attrs, G, 'part', default=None) >>> # >>> assert len(list(func(on_missing='error', on_keyerr='default'))) == 3 >>> assert len(list(func(on_missing='error', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, func(on_missing='error', on_keyerr='error')) >>> # >>> assert len(list(func(edges, on_missing='filter', on_keyerr='default'))) == 3 >>> assert len(list(func(edges, on_missing='filter', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, func(edges, on_missing='filter', on_keyerr='error')) >>> # >>> assert len(list(func(edges, on_missing='default', on_keyerr='default'))) == 4 >>> assert len(list(func(edges, on_missing='default', on_keyerr='filter'))) == 2 >>> ut.assert_raises(KeyError, list, func(edges, on_missing='default', on_keyerr='error'))", "response": "def nx_gen_edge_attrs(G, key, edges=None, default=util_const.NoParam,\n                      on_missing='error', on_keyerr='default'):\n    \"\"\"\n    Improved generator version of nx.get_edge_attributes\n\n    Args:\n        on_missing (str): Strategy for handling nodes missing from G.\n            Can be {'error', 'default', 'filter'}.  defaults to 'error'.\n            is on_missing is not error, then we allow any edge even if the\n            endpoints are not in the graph.\n        on_keyerr (str): Strategy for handling keys missing from node dicts.\n            Can be {'error', 'default', 'filter'}.  defaults to 'default'\n            if default is specified, otherwise defaults to 'error'.\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> G = nx.Graph([(1, 2), (2, 3), (3, 4)])\n        >>> nx.set_edge_attributes(G, name='part', values={(1, 2): 'bar', (2, 3): 'baz'})\n        >>> edges = [(1, 2), (2, 3), (3, 4), (4, 5)]\n        >>> func = ut.partial(ut.nx_gen_edge_attrs, G, 'part', default=None)\n        >>> #\n        >>> assert len(list(func(on_missing='error', on_keyerr='default'))) == 3\n        >>> assert len(list(func(on_missing='error', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, func(on_missing='error', on_keyerr='error'))\n        >>> #\n        >>> assert len(list(func(edges, on_missing='filter', on_keyerr='default'))) == 3\n        >>> assert len(list(func(edges, on_missing='filter', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, func(edges, on_missing='filter', on_keyerr='error'))\n        >>> #\n        >>> assert len(list(func(edges, on_missing='default', on_keyerr='default'))) == 4\n        >>> assert len(list(func(edges, on_missing='default', on_keyerr='filter'))) == 2\n        >>> ut.assert_raises(KeyError, list, func(edges, on_missing='default', on_keyerr='error'))\n    \"\"\"\n    if on_missing is None:\n        on_missing = 'error'\n    if default is util_const.NoParam and on_keyerr == 'default':\n        on_keyerr = 'error'\n\n    if edges is None:\n        if G.is_multigraph():\n            raise NotImplementedError('')\n            # uvk_iter = G.edges(keys=True)\n        else:\n            edges = G.edges()\n    # Generate `edge_data` edges and data dictionary\n    if on_missing == 'error':\n        edge_data = (((u, v), G.adj[u][v]) for u, v in edges)\n    elif on_missing == 'filter':\n        edge_data = (((u, v), G.adj[u][v]) for u, v in edges if G.has_edge(u, v))\n    elif on_missing == 'default':\n        edge_data = (((u, v), G.adj[u][v])\n                     if G.has_edge(u, v) else ((u, v), {})\n                     for u, v in edges)\n    else:\n        raise KeyError('on_missing={}'.format(on_missing))\n    # Get `edge_attrs` desired value out of dictionary\n    if on_keyerr == 'error':\n        edge_attrs = ((e, d[key]) for e, d in edge_data)\n    elif on_keyerr == 'filter':\n        edge_attrs = ((e, d[key]) for e, d in edge_data if key in d)\n    elif on_keyerr == 'default':\n        edge_attrs = ((e, d.get(key, default)) for e, d in edge_data)\n    else:\n        raise KeyError('on_keyerr={}'.format(on_keyerr))\n    return edge_attrs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring colors to hex strings on graph attrs", "response": "def nx_ensure_agraph_color(graph):\n    \"\"\" changes colors to hex strings on graph attrs \"\"\"\n    from plottool import color_funcs\n    import plottool as pt\n    #import six\n    def _fix_agraph_color(data):\n        try:\n            orig_color = data.get('color', None)\n            alpha = data.get('alpha', None)\n            color = orig_color\n            if color is None and alpha is not None:\n                color = [0, 0, 0]\n            if color is not None:\n                color = pt.ensure_nonhex_color(color)\n                #if isinstance(color, np.ndarray):\n                #    color = color.tolist()\n                color = list(color_funcs.ensure_base255(color))\n                if alpha is not None:\n                    if len(color) == 3:\n                        color += [int(alpha * 255)]\n                    else:\n                        color[3] = int(alpha * 255)\n                color = tuple(color)\n                if len(color) == 3:\n                    data['color'] = '#%02x%02x%02x' % color\n                else:\n                    data['color'] = '#%02x%02x%02x%02x' % color\n        except Exception as ex:\n            import utool as ut\n            ut.printex(ex, keys=['color', 'orig_color', 'data'])\n            raise\n\n    for node, node_data in graph.nodes(data=True):\n        data = node_data\n        _fix_agraph_color(data)\n\n    for u, v, edge_data in graph.edges(data=True):\n        data = edge_data\n        _fix_agraph_color(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the longest path in a dag between two nodes", "response": "def dag_longest_path(graph, source, target):\n    \"\"\"\n    Finds the longest path in a dag between two nodes\n    \"\"\"\n    if source == target:\n        return [source]\n    allpaths = nx.all_simple_paths(graph, source, target)\n    longest_path = []\n    for l in allpaths:\n        if len(l) > len(longest_path):\n            longest_path = l\n    return longest_path"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reverse_path(dict_, root, child_to_parents):\n    # Hacky but illustrative\n    # TODO; implement non-hacky version\n    allkeys = get_allkeys(dict_)\n    mat = np.zeros((len(allkeys), len(allkeys)))\n    for key in allkeys:\n        if key != root:\n            for parent in child_to_parents[key]:\n                rx = allkeys.index(parent)\n                cx = allkeys.index(key)\n                mat[rx][cx] = 1\n    end = None\n    seen_ = set([])\n    reversed_ = {root: traverse_path(root, end, seen_, allkeys, mat)}\n    return reversed_", "response": "This function is used to reverse the path of a dictionary of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simplify_graph(graph):\n    import utool as ut\n    nodes = sorted(list(graph.nodes()))\n    node_lookup = ut.make_index_lookup(nodes)\n    if graph.is_multigraph():\n        edges = list(graph.edges(keys=True))\n    else:\n        edges = list(graph.edges())\n    new_nodes = ut.take(node_lookup, nodes)\n    if graph.is_multigraph():\n        new_edges = [(node_lookup[e[0]], node_lookup[e[1]], e[2], {}) for e in edges]\n    else:\n        new_edges = [(node_lookup[e[0]], node_lookup[e[1]]) for e in edges]\n    cls = graph.__class__\n    new_graph = cls()\n    new_graph.add_nodes_from(new_nodes)\n    new_graph.add_edges_from(new_edges)\n    return new_graph", "response": "This function is used to simplify a networkx graph by removing everything but connectivity"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subgraph_from_edges(G, edge_list, ref_back=True):\n\n    # TODO: support multi-di-graph\n    sub_nodes = list({y for x in edge_list for y in x[0:2]})\n    #edge_list_no_data = [edge[0:2] for edge in edge_list]\n    multi_edge_list = [edge[0:3] for edge in edge_list]\n\n    if ref_back:\n        G_sub = G.subgraph(sub_nodes)\n        for edge in G_sub.edges(keys=True):\n            if edge not in multi_edge_list:\n                G_sub.remove_edge(*edge)\n    else:\n        G_sub = G.subgraph(sub_nodes).copy()\n        for edge in G_sub.edges(keys=True):\n            if edge not in multi_edge_list:\n                G_sub.remove_edge(*edge)\n\n    return G_sub", "response": "Creates a subgraph of G from a list of edges in edge_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate edges in a breadth - first - search starting at source.", "response": "def bfs_multi_edges(G, source, reverse=False, keys=True, data=False):\n    \"\"\"Produce edges in a breadth-first-search starting at source.\n    -----\n    Based on http://www.ics.uci.edu/~eppstein/PADS/BFS.py\n    by D. Eppstein, July 2004.\n    \"\"\"\n    from collections import deque\n    from functools import partial\n    if reverse:\n        G = G.reverse()\n    edges_iter = partial(G.edges_iter, keys=keys, data=data)\n\n    list(G.edges_iter('multitest', keys=True, data=True))\n\n    visited_nodes = set([source])\n    # visited_edges = set([])\n    queue = deque([(source, edges_iter(source))])\n    while queue:\n        parent, edges = queue[0]\n        try:\n            edge = next(edges)\n            edge_nodata = edge[0:3]\n            # if edge_nodata not in visited_edges:\n            yield edge\n            # visited_edges.add(edge_nodata)\n            child = edge_nodata[1]\n            if child not in visited_nodes:\n                visited_nodes.add(child)\n                queue.append((child, edges_iter(child)))\n        except StopIteration:\n            queue.popleft()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dfs_conditional(G, source, state, can_cross):\n    # stack based version\n    visited = {source}\n    stack = [(source, iter(G[source]), state)]\n    while stack:\n        parent, children, state = stack[-1]\n        try:\n            child = next(children)\n            if child not in visited:\n                edge = (parent, child)\n                flag, new_state = can_cross(G, edge, state)\n                if flag:\n                    yield child\n                    visited.add(child)\n                    stack.append((child, iter(G[child]), new_state))\n        except StopIteration:\n            stack.pop()", "response": "A recursive function that returns a list of nodes that can be visited and the state of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of nodes that satisfy a condition", "response": "def bfs_conditional(G, source, reverse=False, keys=True, data=False,\n                    yield_nodes=True, yield_if=None,\n                    continue_if=None, visited_nodes=None,\n                    yield_source=False):\n    \"\"\"\n    Produce edges in a breadth-first-search starting at source, but only return\n    nodes that satisfiy a condition, and only iterate past a node if it\n    satisfies a different condition.\n\n    conditions are callables that take (G, child, edge) and return true or false\n\n    CommandLine:\n        python -m utool.util_graph bfs_conditional\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import networkx as nx\n        >>> import utool as ut\n        >>> G = nx.Graph()\n        >>> G.add_edges_from([(1, 2), (1, 3), (2, 3), (2, 4)])\n        >>> continue_if = lambda G, child, edge: True\n        >>> result = list(ut.bfs_conditional(G, 1, yield_nodes=False))\n        >>> print(result)\n        [(1, 2), (1, 3), (2, 1), (2, 3), (2, 4), (3, 1), (3, 2), (4, 2)]\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> import networkx as nx\n        >>> import utool as ut\n        >>> G = nx.Graph()\n        >>> continue_if = lambda G, child, edge: (child % 2 == 0)\n        >>> yield_if = lambda G, child, edge: (child % 2 == 1)\n        >>> G.add_edges_from([(0, 1), (1, 3), (3, 5), (5, 10),\n        >>>                   (4, 3), (3, 6),\n        >>>                   (0, 2), (2, 4), (4, 6), (6, 10)])\n        >>> result = list(ut.bfs_conditional(G, 0, continue_if=continue_if,\n        >>>                                  yield_if=yield_if))\n        >>> print(result)\n        [1, 3, 5]\n    \"\"\"\n    if reverse and hasattr(G, 'reverse'):\n        G = G.reverse()\n    if isinstance(G, nx.Graph):\n        neighbors = functools.partial(G.edges, data=data)\n    else:\n        neighbors = functools.partial(G.edges, keys=keys, data=data)\n\n    queue = collections.deque([])\n\n    if visited_nodes is None:\n        visited_nodes = set([])\n    else:\n        visited_nodes = set(visited_nodes)\n\n    if source not in visited_nodes:\n        if yield_nodes and yield_source:\n            yield source\n        visited_nodes.add(source)\n        new_edges = neighbors(source)\n        if isinstance(new_edges, list):\n            new_edges = iter(new_edges)\n        queue.append((source, new_edges))\n\n    while queue:\n        parent, edges = queue[0]\n        for edge in edges:\n            child = edge[1]\n            if yield_nodes:\n                if child not in visited_nodes:\n                    if yield_if is None or yield_if(G, child, edge):\n                        yield child\n            else:\n                if yield_if is None or yield_if(G, child, edge):\n                    yield edge\n            if child not in visited_nodes:\n                visited_nodes.add(child)\n                # Add new children to queue if the condition is satisfied\n                if continue_if is None or continue_if(G, child, edge):\n                    new_edges = neighbors(child)\n                    if isinstance(new_edges, list):\n                        new_edges = iter(new_edges)\n                    queue.append((child, new_edges))\n        queue.popleft()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef color_nodes(graph, labelattr='label', brightness=.878,\n                outof=None, sat_adjust=None):\n    \"\"\" Colors edges and nodes by nid \"\"\"\n    import plottool as pt\n    import utool as ut\n    node_to_lbl = nx.get_node_attributes(graph, labelattr)\n    unique_lbls = sorted(set(node_to_lbl.values()))\n    ncolors = len(unique_lbls)\n    if outof is None:\n        if (ncolors) == 1:\n            unique_colors = [pt.LIGHT_BLUE]\n        elif (ncolors) == 2:\n            # https://matplotlib.org/examples/color/named_colors.html\n            unique_colors = ['royalblue', 'orange']\n            unique_colors = list(map(pt.color_funcs.ensure_base01, unique_colors))\n        else:\n            unique_colors = pt.distinct_colors(ncolors, brightness=brightness)\n    else:\n        unique_colors = pt.distinct_colors(outof, brightness=brightness)\n\n    if sat_adjust:\n        unique_colors = [\n            pt.color_funcs.adjust_hsv_of_rgb(c, sat_adjust=sat_adjust)\n            for c in unique_colors\n        ]\n    # Find edges and aids strictly between two nids\n    if outof is None:\n        lbl_to_color = ut.dzip(unique_lbls, unique_colors)\n    else:\n        gray = pt.color_funcs.ensure_base01('lightgray')\n        unique_colors = [gray] + unique_colors\n        offset = max(1, min(unique_lbls)) - 1\n        node_to_lbl = ut.map_vals(lambda nid: max(0, nid - offset), node_to_lbl)\n        lbl_to_color = ut.dzip(range(outof + 1), unique_colors)\n    node_to_color = ut.map_vals(lbl_to_color, node_to_lbl)\n    nx.set_node_attributes(graph, name='color', values=node_to_color)\n    ut.nx_ensure_agraph_color(graph)", "response": "Color nodes in a tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of G with only the nodes that are contracted by u and v.", "response": "def nx_contracted_nodes(G, u, v, self_loops=True, inplace=False):\n    \"\"\"\n    copy of networkx function with inplace modification\n    TODO: commit to networkx\n    \"\"\"\n    import itertools as it\n    if G.is_directed():\n        in_edges = ((w, u, d) for w, x, d in G.in_edges(v, data=True)\n                    if self_loops or w != u)\n        out_edges = ((u, w, d) for x, w, d in G.out_edges(v, data=True)\n                     if self_loops or w != u)\n        new_edges = it.chain(in_edges, out_edges)\n    else:\n        new_edges = ((u, w, d) for x, w, d in G.edges(v, data=True)\n                     if self_loops or w != u)\n    if inplace:\n        H = G\n        new_edges = list(new_edges)\n    else:\n        H = G.copy()\n    node_dict = nx_node_dict(H)\n    v_data = node_dict[v]\n    H.remove_node(v)\n    H.add_edges_from(new_edges)\n    if 'contraction' in node_dict[u]:\n        node_dict[u]['contraction'][v] = v_data\n    else:\n        node_dict[u]['contraction'] = {v: v_data}\n    return H"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding approximate minimum number of connected components possible Each edge represents that two nodes must be separated This code doesn't solve the problem. The problem is NP-complete and reduces to minimum clique cover (MCC). This is only an approximate solution. Not sure what the approximation ratio is. CommandLine: python -m utool.util_graph approx_min_num_components Example: >>> # ENABLE_DOCTEST >>> from utool.util_graph import * # NOQA >>> import utool as ut >>> nodes = [1, 2, 3, 4, 5, 6, 7, 8, 9] >>> edges = [(1, 2), (2, 3), (3, 1), >>> (4, 5), (5, 6), (6, 4), >>> (7, 8), (8, 9), (9, 7), >>> (1, 4), (4, 7), (7, 1), >>> ] >>> g_pos = nx.Graph() >>> g_pos.add_edges_from(edges) >>> g_neg = nx.complement(g_pos) >>> #import plottool as pt >>> #pt.qt4ensure() >>> #pt.show_nx(g_pos) >>> #pt.show_nx(g_neg) >>> negative_edges = g_neg.edges() >>> nodes = [1, 2, 3, 4, 5, 6, 7] >>> negative_edges = [(1, 2), (2, 3), (4, 5)] >>> result = approx_min_num_components(nodes, negative_edges) >>> print(result) 2", "response": "def approx_min_num_components(nodes, negative_edges):\n    \"\"\"\n    Find approximate minimum number of connected components possible\n    Each edge represents that two nodes must be separated\n\n    This code doesn't solve the problem. The problem is NP-complete and\n    reduces to minimum clique cover (MCC). This is only an approximate\n    solution. Not sure what the approximation ratio is.\n\n    CommandLine:\n        python -m utool.util_graph approx_min_num_components\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_graph import *  # NOQA\n        >>> import utool as ut\n        >>> nodes = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n        >>> edges = [(1, 2), (2, 3), (3, 1),\n        >>>          (4, 5), (5, 6), (6, 4),\n        >>>          (7, 8), (8, 9), (9, 7),\n        >>>          (1, 4), (4, 7), (7, 1),\n        >>>         ]\n        >>> g_pos = nx.Graph()\n        >>> g_pos.add_edges_from(edges)\n        >>> g_neg = nx.complement(g_pos)\n        >>> #import plottool as pt\n        >>> #pt.qt4ensure()\n        >>> #pt.show_nx(g_pos)\n        >>> #pt.show_nx(g_neg)\n        >>> negative_edges = g_neg.edges()\n        >>> nodes = [1, 2, 3, 4, 5, 6, 7]\n        >>> negative_edges = [(1, 2), (2, 3), (4, 5)]\n        >>> result = approx_min_num_components(nodes, negative_edges)\n        >>> print(result)\n        2\n    \"\"\"\n    import utool as ut\n    num = 0\n    g_neg = nx.Graph()\n    g_neg.add_nodes_from(nodes)\n    g_neg.add_edges_from(negative_edges)\n\n    # Collapse all nodes with degree 0\n    if nx.__version__.startswith('2'):\n        deg0_nodes = [n for n, d in g_neg.degree() if d == 0]\n    else:\n        deg0_nodes = [n for n, d in g_neg.degree_iter() if d == 0]\n\n    for u, v in ut.itertwo(deg0_nodes):\n        nx_contracted_nodes(g_neg, v, u, inplace=True)\n        # g_neg = nx.contracted_nodes(g_neg, v, u, self_loops=False)\n\n    # Initialize unused nodes to be everything\n    unused = list(g_neg.nodes())\n    # complement of the graph contains all possible positive edges\n    g_pos = nx.complement(g_neg)\n\n    if False:\n        from networkx.algorithms.approximation import clique\n        maxiset, cliques = clique.clique_removal(g_pos)\n        num = len(cliques)\n        return num\n\n    # Iterate until we have used all nodes\n    while len(unused) > 0:\n        # Seed a new \"minimum component\"\n        num += 1\n        # Grab a random unused node n1\n        #idx1 = np.random.randint(0, len(unused))\n        idx1 = 0\n        n1 = unused[idx1]\n        unused.remove(n1)\n        neigbs = list(g_pos.neighbors(n1))\n        neigbs = ut.isect(neigbs, unused)\n        while len(neigbs) > 0:\n            # Find node n2, that n1 could be connected to\n            #idx2 = np.random.randint(0, len(neigbs))\n            idx2 = 0\n            n2 = neigbs[idx2]\n            unused.remove(n2)\n            # Collapse negative information of n1 and n2\n            g_neg = nx.contracted_nodes(g_neg, n1, n2)\n            # Compute new possible positive edges\n            g_pos = nx.complement(g_neg)\n            # Iterate until n1 has no more possible connections\n            neigbs = list(g_pos.neighbors(n1))\n            neigbs = ut.isect(neigbs, unused)\n    print('num = %r' % (num,))\n    return num"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a function, initial conditions, step size and end value, this will calculate an unforced system. The default start time is t=0.0, but this can be changed. y - initial state h - step size n - stop time", "response": "def solve(self, y, h, t_end):\n\t\t\"\"\"\n\t\tGiven a function, initial conditions, step size and end value, this will\n\t\tcalculate an unforced system. The default start time is t=0.0, but this\n\t\tcan be changed.\n\n\t\ty - initial state\n\t\th - step size\n\t\tn - stop time\n\t\t\"\"\"\n\t\tts = []\n\t\tys = []\n\t\tyi = y\n\t\tti = 0.0\n\t\twhile ti < t_end:\n\t\t\tts.append(ti)\n\t\t\tyi = self.step(yi, None, ti, h)\n\t\t\tys.append(yi)\n\t\t\tti += h\n\t\treturn ts, ys"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef step(self, y, u, t, h):\n\t\tk1 = h * self.func(t, y, u)\n\t\tk2 = h * self.func(t + .5*h, y + .5*h*k1, u)\n\t\tk3 = h * self.func(t + .5*h, y + .5*h*k2, u)\n\t\tk4 = h * self.func(t + h, y + h*k3, u)\n\t\treturn y + (k1 + 2*k2 + 2*k3 + k4) / 6.0", "response": "This function is called by solve and is called by solve_one_step."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toxml(self):\n\n        return '<With ' + \\\n          (' instance=\"{0}\"'.format(self.instance) if self.instance else '') +\\\n          (' list=\"{0}\"'.format(self.list) if self.list else '') + \\\n          (' index=\"{0}\"'.format(self.index) if self.index else '') + \\\n          ' as=\"{1}\"/>'.format(self.instance, self.as_)", "response": "Returns a LEMS XML object representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toxml(self):\n\n        return '<Tunnel name=\"{0}\"'.format(self.name) + \\\n               ' endA=\"{0}\"'.format(self.end_a) + \\\n               ' endB=\"{0}\"'.format(self.end_b) + \\\n               ' componentA=\"{0}\"'.format(self.component_a) + \\\n               ' componentB=\"{0}\"'.format(self.component_b) + '/>'", "response": "Returns a LEMS XML string representation of the tunnel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef toxml(self):\n\n        return '<EventConnection' +\\\n          (' from=\"{0}\"'.format(self.from_) if self.from_ else '') +\\\n          (' to=\"{0}\"'.format(self.to) if self.to else '') +\\\n          (' sourcePort=\"{0}\"'.format(self.source_port) if self.source_port else '') +\\\n          (' targetPort=\"{0}\"'.format(self.target_port) if self.target_port else '') +\\\n          (' receiver=\"{0}\"'.format(self.receiver) if self.receiver else '') +\\\n          (' receiverContainer=\"{0}\"'.format(self.receiver_container) if self.receiver_container else '') +\\\n          '/>'", "response": "Returns a LEMS XML string representation of the EventConnection object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, child):\n\n        if isinstance(child, Assign):\n            self.add_assign(child)\n        else:\n            raise ModelError('Unsupported child element')", "response": "Adds a typed child object to the structure object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toxml(self):\n        argstr = ''\n        if self.component:\n            argstr += 'component=\"{0}\" '.format(self.component)\n        if self.component_type:\n            argstr += 'componentType=\"{0}\" '.format(self.component_type)\n        if self.number:\n            argstr += 'number=\"{0}\" '.format(self.number)\n        if self.assignments:\n            chxmlstr = ''\n            for assign in self.assignments:\n                chxmlstr += assign.toxml()\n            return '<MultiInstantiate {0}>{1}</MultiInstantiate>'.format(argstr, chxmlstr)\n        else:\n            return '<MultiInstantiate {0}/>'.format(argstr)", "response": "Returns a string representation of the object as an LEMS XML object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toxml(self):\n        chxmlstr = ''\n\n        for event_connection in self.event_connections:\n            chxmlstr += event_connection.toxml()\n\n        for for_each in self.for_eachs:\n            chxmlstr += for_each.toxml()\n\n\n        return '<ForEach instances=\"{0}\" as=\"{1}\">{2}</ForEach>'.format(self.instances, self.as_, chxmlstr)", "response": "Returns a LEMS XML string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, child):\n\n        if isinstance(child, With):\n            self.add_with(child)\n        elif isinstance(child, EventConnection):\n            self.add_event_connection(child)\n        elif isinstance(child, ChildInstance):\n            self.add_child_instance(child)\n        elif isinstance(child, MultiInstantiate):\n            self.add_multi_instantiate(child)\n        elif isinstance(child, ForEach):\n            self.add_for_each(child)\n        else:\n            raise ModelError('Unsupported child element')", "response": "Adds a typed child object to the structure object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a LEMS XML string representation of the object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n        chxmlstr = ''\n\n        for with_ in self.withs:\n            chxmlstr += with_.toxml()\n\n        for event_connection in self.event_connections:\n            chxmlstr += event_connection.toxml()\n\n        for child_instance in self.child_instances:\n            chxmlstr += child_instance.toxml()\n\n        for multi_instantiate in self.multi_instantiates:\n            chxmlstr += multi_instantiate.toxml()\n\n        for for_each in self.for_eachs:\n            chxmlstr += for_each.toxml()\n\n        if chxmlstr:\n            xmlstr = '<Structure>' + chxmlstr + '</Structure>'\n        else:\n            xmlstr = ''\n\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_proteins(pepfn, proteins, pepheader, scorecol, minlog,\n                      higherbetter=True, protcol=False):\n    \"\"\"Best peptide for each protein in a table\"\"\"\n    protein_peptides = {}\n    if minlog:\n        higherbetter = False\n    if not protcol:\n        protcol = peptabledata.HEADER_MASTERPROTEINS\n    for psm in reader.generate_tsv_psms(pepfn, pepheader):\n        p_acc = psm[protcol]\n        if ';' in p_acc:\n            continue\n        protein_peptides = evaluate_peptide(protein_peptides, psm, p_acc,\n                                            higherbetter, scorecol,\n                                            fncol=False)\n    if minlog:\n        try:\n            nextbestscore = min([pep['score'] for pep in\n                                 protein_peptides.values()\n                                 if pep['score'] > 0])\n        except ValueError:\n            import sys\n            sys.stderr.write('Cannot find score of type {} which is above 0. '\n                             'Only scores above zero can have a -log value. '\n                             'Exiting.'.format(scorecol))\n            sys.exit(1)\n        nextbestscore = -log(nextbestscore, 10)\n    for protein in proteins:\n        try:\n            peptide = protein_peptides[protein[prottabledata.HEADER_PROTEIN]]\n        except KeyError:\n            print('WARNING - protein {} not found in peptide '\n                  'table'.format(protein[prottabledata.HEADER_PROTEIN]))\n            peptide = {'score': 'NA'}\n        if minlog and peptide['score'] != 'NA':\n            peptide['score'] = log_score(peptide['score'], nextbestscore)\n        protein[prottabledata.HEADER_QSCORE] = str(\n            peptide['score'])\n        yield protein", "response": "Generate a list of proteins from a table of protein peptides."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef toxml(self):\n\n        return '<Run component=\"{0}\" variable=\"{1}\" increment=\"{2}\" total=\"{3}\"/>'.format(self.component,\n                                                                                          self.variable,\n                                                                                          self.increment,\n                                                                                          self.total)", "response": "Returns a LEMS XML string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toxml(self):\n\n        return '<Record quantity=\"{0}\" scale=\"{1}\" color=\"{2}\" id=\"{3}\"/>'.format(self.quantity,\n                                                                         self.scale,\n                                                                         self.color,\n                                                                         self.id)", "response": "Returns a LEMS XML string representation of the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a typed child object to the simulation spec.", "response": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the simulation spec.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Run):\n            self.add_run(child)\n        elif isinstance(child, Record):\n            self.add_record(child)\n        elif isinstance(child, EventRecord):\n            self.add_event_record(child)\n        elif isinstance(child, DataDisplay):\n            self.add_data_display(child)\n        elif isinstance(child, DataWriter):\n            self.add_data_writer(child)\n        elif isinstance(child, EventWriter):\n            self.add_event_writer(child)\n        else:\n            raise ModelError('Unsupported child element')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting this object into a LEMS XML object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        chxmlstr = ''\n\n        for run in self.runs:\n            chxmlstr += run.toxml()\n\n        for record in self.records:\n            chxmlstr += record.toxml()\n\n        for event_record in self.event_records:\n            chxmlstr += event_record.toxml()\n\n        for data_display in self.data_displays:\n            chxmlstr += data_display.toxml()\n\n        for data_writer in self.data_writers:\n            chxmlstr += data_writer.toxml()\n\n        for event_writer in self.event_writers:\n            chxmlstr += event_writer.toxml()\n\n        if chxmlstr:\n            xmlstr = '<Simulation>' + chxmlstr + '</Simulation>'\n        else:\n            xmlstr = ''\n\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch(self, id_, return_fields=None):\n        game_params = {\"id\": id_}\n\n        if return_fields is not None:\n            self._validate_return_fields(return_fields)\n            field_list = \",\".join(return_fields)\n\n            game_params[\"field_list\"] = field_list\n\n        response = self._query(game_params, direct=True)\n\n        return response", "response": "Wrapper for fetching details of game by ID\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef define_options(self, names, parser_options=None):\n        def copy_option(options, name):\n            return {k: v for k, v in options[name].items()}\n        if parser_options is None:\n            parser_options = {}\n        options = {}\n        for name in names:\n            try:\n                option = copy_option(parser_options, name)\n            except KeyError:\n                option = copy_option(shared_options, name)\n            try:\n                options.update({option['clarg']: option})\n            except TypeError:\n                options.update({option['clarg'][0]: option})\n        return options", "response": "Given a list of option names this returns a list of dicts containing all options defined in all_options and shared_options. These can then be used to populate the argparser with the option values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_column_header_for_number(self, column_var_names, header=False):\n        if not header:\n            header = self.oldheader\n        for col in column_var_names:\n            value = getattr(self, col)\n            if not value or value is None:\n                continue\n            setattr(self, col, self.number_to_headerfield(value, header))", "response": "This function subtracts 1 from inputted column number to comply\n            with programmers counting. For TSV data this function returns the column header field for the specified column_var_names."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_response_data(cls, response_data):\n\n        response_json = response_data.json()\n\n        return cls(\n            response_data.url,\n            response_json[\"number_of_page_results\"],\n            response_json[\"number_of_total_results\"],\n            response_json[\"results\"],\n        )", "response": "Create a new object from response data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns this programs current memory usage in bytes", "response": "def current_memory_usage():\n    \"\"\"\n    Returns this programs current memory usage in bytes\n    \"\"\"\n    import psutil\n    proc = psutil.Process(os.getpid())\n    #meminfo = proc.get_memory_info()\n    meminfo = proc.memory_info()\n    rss = meminfo[0]  # Resident Set Size / Mem Usage\n    vms = meminfo[1]  # Virtual Memory Size / VM Size  # NOQA\n    return rss"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_matching_process_ids(cmd_pattern, user_pattern):\n    import psutil\n    import re\n    process_list = list(psutil.process_iter())\n    def matches_pattern(proc, user_pattern, cmd_pattern):\n        matches_user = True if user_pattern is None else re.match(user_pattern, proc.username())\n        cmdline_str = ' '.join(proc.cmdline())\n        matches_name = True if cmd_pattern is None else re.search(cmd_pattern, cmdline_str)\n        return matches_user and matches_name\n    filtered_proc_list = [proc for proc in process_list if matches_pattern(proc, user_pattern, cmd_pattern)]\n\n    for proc in filtered_proc_list:\n        print(' | '.join([str(proc.username()), str(proc.nice()), str(proc), ' '.join(proc.cmdline())]))\n        #print(proc.cmdline())\n        #print(proc.pid)\n        #print('---')\n\n    important_process_list = [proc for proc in process_list if proc.nice() < -4]\n    for proc in important_process_list:\n        print(' -- '.join([str(proc.username()), str(proc.nice()), str(proc), ' '.join(proc.cmdline())]))\n\n    #for proc in filtered_proc_list:\n    #    print('---')\n    #    print(proc)\n    #    print(proc.cmdline())\n    #    print(proc.nice())\n    #    print(proc.pid)\n    filtered_pid_list = [proc.pid for proc in filtered_proc_list]\n    return filtered_pid_list", "response": "Get a list of processes that match the given pattern."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef num_unused_cpus(thresh=10):\n    import psutil\n    cpu_usage = psutil.cpu_percent(percpu=True)\n    return sum([p < thresh for p in cpu_usage])", "response": "Returns the number of cpus with utilization less than thresh percent\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_python_datastructure_sizes():\n    import sys\n    import decimal\n    import six\n\n    empty_types = {\n        'int'     : 0,\n        'float'   : 0.0,\n        'dict'    : dict(),\n        'set'     : set(),\n        'tuple'   : tuple(),\n        'list'    : list(),\n        'str'     : '',\n        'unicode' : u'',\n        'decimal' : decimal.Decimal(0),\n        'object'  : object(),\n    }\n    type_sizes = {key: sys.getsizeof(val)\n                  for key, val in six.iteritems(empty_types)}\n    return type_sizes", "response": "Returns a dict of size of the python datastructure"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_masters(ppgraph):\n    masters = {}\n    for protein, peps in ppgraph.items():\n        ismaster = True\n        peps = set(peps)\n        multimaster = set()\n        for subprotein, subpeps in ppgraph.items():\n            if protein == subprotein:\n                continue\n            if peps.issubset(subpeps):\n                if peps.union(subpeps) > peps:\n                    ismaster = False\n                    break\n                elif peps.intersection(subpeps) == peps:\n                    multimaster.update({protein, subprotein})\n        if not ismaster:\n            continue\n        elif multimaster:\n            premaster = sorted(list(multimaster))[0]\n        else:\n            premaster = protein\n        for pep in peps:\n            try:\n                masters[pep].add(premaster)\n            except KeyError:\n                masters[pep] = {premaster}\n    return masters", "response": "Given a protein - peptide graph return a dictionary of master proteins aka those which have no proteins whose peptides are supersets of them."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a generator that calculates coverages for each protein and returns the accession and coverage percentage.", "response": "def generate_coverage(seqinfo):\n    \"\"\"From a dict containing protein accessions and sequences/PSM sequences,\n    this function returns a generator that calculates coverages for each\n    protein and returns the accession and coverage percentage.\n    Coverage is done by finding peptides in the protein seq using seq.index\n    and marking the range. May be slow.\"\"\"\n    for acc, protinfo in seqinfo.items():\n        coverage_aa_indices = set()\n        seq = protinfo['seq']\n        for psmseq in protinfo['psms']:\n            psmseq = tsvreader.strip_modifications(psmseq)\n            # FIXME try block is for problems with coverage, see if it is\n            # needed\n            try:\n                start = seq.index(psmseq)\n            except:\n                print('CANNOT FIND PSM seq {0} in seq {1} '\n                      'for acc {2}'.format(psmseq, seq, acc))\n            coverage_aa_indices.update(range(start, start + len(psmseq)))\n        yield (acc, len(coverage_aa_indices) / len(seq))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the protein group proteins complete with sequences psm_ids and scores.", "response": "def get_protein_group_content(pgmap, master):\n    \"\"\"For each master protein, we generate the protein group proteins\n    complete with sequences, psm_ids and scores. Master proteins are included\n    in this group.\n\n    Returns a list of [protein, master, pep_hits, psm_hits, protein_score],\n    which is ready to enter the DB table.\n    \"\"\"\n    # first item (0) is only a placeholder so the lookup.INDEX things get the\n    # correct number. Would be nice with a solution, but the INDEXes were\n    # originally made for mzidtsv protein group adding.\n    pg_content = [[0, master, protein, len(peptides), len([psm for pgpsms in\n                                                           peptides.values()\n                                                           for psm in pgpsms]),\n                   sum([psm[1] for pgpsms in peptides.values()\n                        for psm in pgpsms]),  # score\n                   next(iter(next(iter(peptides.values()))))[3],  # coverage\n                   next(iter(next(iter(peptides.values()))))[2],  # evid level\n                   ]\n                  for protein, peptides in pgmap.items()]\n    return pg_content"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_peptidetable(pqdb, headerfields, isobaric=False,\n                       precursor=False, fdr=False, pep=False,\n                       genecentric=False):\n    \"\"\"Fetches peptides and quants from joined lookup table, loops through\n    them and when all of a peptides quants/data have been collected, yields\n    peptide quant information.\"\"\"\n    peptidedatamap = create_featuredata_map(pqdb, genecentric=genecentric, \n            psm_fill_fun=add_psm_to_peptidedata, pgene_fill_fun=add_protgene_to_pepdata,\n            is_peptides=True)\n    count_psms(peptidedatamap)\n    empty_return = lambda x, y, z: {}\n    iso_fun = {True: get_isobaric_quant, False: empty_return}[isobaric]\n    ms1_fun = {True: get_precursor_quant, False: empty_return}[precursor]\n    fdr_fun = {True: get_pep_fdr,\n               False: empty_return}[fdr]\n    pep_fun = {True: get_peptide_pep,\n               False: empty_return}[pep]\n    pdata_fun = get_protein_data\n    peptide_sql, sqlfieldmap = pqdb.prepare_mergetable_sql(precursor, isobaric,\n                                                           probability=False,\n                                                           fdr=fdr, pep=pep)\n    peptides = pqdb.get_merged_features(peptide_sql)\n    peptide = next(peptides)\n    outpeptide = {peptabledata.HEADER_PEPTIDE: peptide[sqlfieldmap['p_acc']]}\n    check_pep = {k: v for k, v in outpeptide.items()}\n    fill_mergefeature(outpeptide, iso_fun, ms1_fun, empty_return, fdr_fun,\n                      pep_fun, pdata_fun, peptide, sqlfieldmap,\n                      headerfields, peptidedatamap)\n    for peptide in peptides:\n        p_seq = peptide[sqlfieldmap['p_acc']]\n        if p_seq != outpeptide[peptabledata.HEADER_PEPTIDE]:\n            if outpeptide != check_pep:\n                yield outpeptide\n            outpeptide = {peptabledata.HEADER_PEPTIDE: p_seq}\n            check_pep = {k: v for k, v in outpeptide.items()}\n        fill_mergefeature(outpeptide, iso_fun, ms1_fun, empty_return, fdr_fun,\n                          pep_fun, pdata_fun, peptide, sqlfieldmap,\n                          headerfields, peptidedatamap)\n    yield outpeptide", "response": "Builds a protein - level peptide - idetable from a joined lookup table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_protein_data(peptide, pdata, headerfields, accfield):\n    report = get_proteins(peptide, pdata, headerfields)\n    return get_cov_descriptions(peptide, pdata, report)", "response": "Get protein data from a protein data set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_progress(lbl='Progress: ', length=0, flushfreq=4, startafter=-1,\n                 start=True, repl=False, approx=False, disable=False,\n                 writefreq=1, with_time=False, backspace=True,\n                 pad_stdout=False, wfreq=None, ffreq=None, freq=None, total=None,\n                 num=None, with_totaltime=None):\n    \"\"\"\n    DEPRICATE\n    FIXME: depricate for ProgressIter.\n    still used in util_dev\n    \"\"\"\n    global AGGROFLUSH\n    # Alias kwargs with simpler names\n    if num is not None:\n        length = num\n    if total is not None:\n        length = total\n    if wfreq is not None:\n        writefreq = wfreq\n    if ffreq is not None:\n        flushfreq = ffreq\n    if freq is not None:\n        writefreq = flushfreq = freq\n    if with_totaltime is not None:\n        with_time = with_totaltime\n    # flush frequency must be a multiple of write frequency\n    flushfreq = max(int(round(flushfreq / writefreq)), 1) * writefreq\n    if length < startafter or disable:\n        # Do not mark progress if only executing a small number of tasks\n        def mark_progress(*args):\n            pass\n        def end_progress(*args):\n            pass\n        return mark_progress, end_progress\n    else:\n        write_fn = util_logging._utool_write()\n        flush_fn = util_logging._utool_flush()\n        # build format string for displaying progress\n        fmt_str = progress_str(length, lbl=lbl, repl=repl, approx=approx,\n                               backspace=backspace)\n        if AGGROFLUSH:\n            # Progress function which automatically flushes\n            def mark_progress(count, flush_fn=flush_fn):\n                count_ = count + 1\n                write_fn(fmt_str % (count_))\n                flush_fn()\n        else:\n            # Progress function flushes every <flushfreq> times\n            def mark_progress(count, fmt_str=fmt_str, flushfreq=flushfreq,\n                              writefreq=writefreq, write_fn=write_fn,\n                              flush_fn=flush_fn):\n                count_ = count + 1\n                if count_ % writefreq == 0:\n                    write_fn(fmt_str % count_)\n                    if count_ % flushfreq == 0:\n                        flush_fn()\n\n        if pad_stdout:\n            write_fn('\\n')\n            write_fn('\\n')\n            flush_fn()\n\n        if with_time:\n            tt = util_time.tic(lbl)\n\n        def end_progress(count_=length, write_fn=write_fn, flush_fn=flush_fn):\n            write_fn(fmt_str % (count_))\n            write_fn('\\n')\n            flush_fn()\n            if with_time:\n                util_time.toc(tt)\n            if pad_stdout:\n                write_fn('\\n\\n')\n                flush_fn()\n        #mark_progress(0)\n        if start:\n            mark_progress(-1)\n        return mark_progress, end_progress", "response": "Function to log a progress message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_msg_fmtstr2(lbl, length, invert_rate, backspace):\n        with_wall = True\n        tzname = time.tzname[0]\n        if util_cplat.WIN32:\n            tzname = tzname.replace('Eastern Standard Time', 'EST')\n        # ansii/vt100 code for clearline\n        # CLEARLINE_L2 = '\\33[2K'\n        # BEFORE_PROG = '\\r\\033[?25l'\n\n        CLEARLINE_EL0 = '\\33[0K'  # clear line to right\n        # CLEARLINE_EL1 = '\\33[1K'  # clear line to left\n        CLEARLINE_EL2 = '\\33[2K'  # clear line\n        # DECTCEM_HIDE = '\\033[?25l'  # hide cursor\n\n        CLEAR_BEFORE = '\\r' + CLEARLINE_EL2  # + DECTCEM_HIDE\n        # FIXME: hideing cursor persists if the program crashes\n        CLEAR_AFTER = CLEARLINE_EL0\n\n        msg_head = ProgressIter.build_msg_fmtstr_head_cols(length, lbl)\n        if backspace:\n            msg_head = [CLEAR_BEFORE] + msg_head\n\n        msg_tail = [\n            (\n                'rate={rate:4.2f} sec/iter, '\n                if invert_rate else\n                'rate={rate:4.2f} Hz,'\n            ),\n            (\n                ''\n                if length == 0 else\n                ' etr={etr},'\n            ),\n            ' ellapsed={ellapsed},',\n            (\n                ' wall={wall} ' + tzname\n                if with_wall\n                else ''\n            ),\n            # backslash-r is a carrage return and undoes all previous output on\n            # a written line\n            (' {extra}'),\n            CLEAR_AFTER if backspace else '\\n',\n        ]\n        msg_fmtstr_time = ''.join((msg_head + msg_tail))\n        return msg_fmtstr_time", "response": "r Builds a message format string for a specific locale."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npun not intended # TODO: record iteration times for analysis # TODO Incorporate this better # FIXME; pad_stdout into subfunctions import dis dis.dis(ut.ProgressIter.iter_rate)", "response": "def iter_rate(self):\n        \"\"\"\n        pun not intended\n\n        # TODO: record iteration times for analysis\n        # TODO Incorporate this better\n        # FIXME; pad_stdout into subfunctions\n\n        import dis\n        dis.dis(ut.ProgressIter.iter_rate)\n        \"\"\"\n        #class IterState(object):\n        #    def __init__(state):\n        #        state.freq = 1\n        #        state.freq = 1\n        #        pass\n        adjust = self.autoadjust\n        self._cursor_at_newline = not self.backspace\n        # SETUP VARIABLES\n        # HACK: reaquire logging print funcs in case they have changed\n        if self.stream is None:\n            self.write = util_logging._utool_write()\n            self.flush = util_logging._utool_flush()\n        else:\n            self.write = lambda msg: self.stream.write(msg)  # NOQA\n            self.flush = lambda: self.stream.flush()  # NOQA\n\n        length        = self.length * self.parent_length  # hack\n        freq          = self.freq\n        self.count    = 0\n        between_count = 0\n        last_count    = 0\n\n        # how long iterations should be before a flush\n        # (used for freq adjustment)\n        time_thresh = (self._get_timethresh_heuristics()\n                       if self.time_thresh is None else\n                       self.time_thresh)\n        time_thresh_growth = self.time_thresh_growth\n        if time_thresh_growth > 1:\n            # time_thresh_growth is specified for very long processes\n            # print out the starting timestamp in that case\n            timestamp = time.strftime('%Y-%m-%d %H:%M:%S') + ' ' + time.tzname[0]\n            print('Start progress lbl= %s at %s' % (self.lbl, timestamp,))\n        #time_thresh = 0.5\n        max_between_time = -1.0\n        max_between_count = -1.0  # why is this different? # because frequency varies\n\n        # TODO: should be kept as a statistic that uses the max time from a\n        # list of iterations divided by the size of that list that will account\n        # for buffering issues\n        iters_per_second = 0\n        self.iters_per_second = float('nan')\n        self.est_seconds_left = 0\n        self.total_seconds = 0\n\n        # Write initial message\n        #force_newlines = not self.backspace\n        start_msg_fmt = ''.join(self.build_msg_fmtstr_head_cols(length, self.lbl))\n        self.msg_fmtstr = self.build_msg_fmtstr2(self.lbl, length,\n                                                 self.invert_rate,\n                                                 self.backspace)\n\n        try:\n            util_logging._utool_flush()()\n        except IOError as ex:\n            # There is some weird error when doing progress in IPython notebook\n            if util_arg.VERBOSE:\n                print('IOError flushing %s' % (ex,))\n        if not self.prehack:\n            if self.backspace:\n                self.display_message()\n            elif self.verbose:\n                start_msg = start_msg_fmt.format(count=self.parent_offset)\n                util_logging._utool_write()(start_msg + '\\n')\n\n            self._cursor_at_newline = not self.backspace\n\n            try:\n                util_logging._utool_flush()()\n            except IOError as ex:\n                # There is some weird error when doing progress in IPython notebook\n                if util_arg.VERBOSE:\n                    print('IOError flushing %s' % (ex,))\n        else:\n            self._cursor_at_newline = True\n\n        if self.prog_hook is not None:\n            self.prog_hook(self.count, length)\n\n        # TODO: on windows is time.clock better?\n        # http://exnumerus.blogspot.com/2011/02/how-to-quickly-plot-multiple-line.html\n        start_time    = default_timer()\n        last_time     = start_time\n\n        start = 1 + self.parent_offset\n\n        if self.freq_est_strat == 'between':\n            FREQ_EST = 0\n        elif self.freq_est_strat == 'absolute':\n            FREQ_EST = 1\n        else:\n            FREQ_EST = 1\n\n        USE_RECORD = True\n\n        # use last 64 times to compute a more stable average rate\n        measure_between_time = collections.deque([], maxlen=self.est_window)\n\n        # Wrap the for loop with a generator\n        for self.count, item in enumerate(self.iterable, start=start):\n            if self.prehack:\n                # hack to print before yeilding\n                # so much for efficiency\n                self.set_extra((self.lbl + '=' + self.prehack) % item)\n                self.display_message()\n                self.ensure_newline()\n\n            # GENERATE\n            yield item\n\n            if self.prehack or (self.count) % freq == 0:\n                now_time          = default_timer()\n                between_time      = (now_time - last_time)\n                between_count     = self.count - last_count\n                total_seconds     = (now_time - start_time)\n                self.total_seconds = total_seconds\n                if FREQ_EST == 0:\n                    if USE_RECORD:\n                        measure_between_time.append(between_count / (float(between_time) + 1E-9))\n                        iters_per_second = sum(measure_between_time) / len(measure_between_time)\n                    else:\n                        iters_per_second = between_count / (float(between_time) + 1E-9)\n                elif FREQ_EST == 1:\n                    iters_per_second = (now_time - start_time) / self.count\n\n                self.iters_per_second = iters_per_second\n                # If the future is known\n                if length is None:\n                    est_seconds_left = -1\n                else:\n                    iters_left = length - self.count\n                    est_seconds_left = iters_left / (iters_per_second + 1E-9)\n                self.est_seconds_left = est_seconds_left\n\n                # /future\n                last_count        = self.count\n                last_time         = now_time\n                # ADJUST FREQ IF NEEDED\n                # Adjust frequency if printing too quickly\n                # so progress doesnt slow down actual function\n                # TODO: better adjust algorithm\n                time_thresh *= time_thresh_growth\n                if adjust and (between_time < time_thresh or between_time > time_thresh * 2.0):\n                    max_between_time = max(max(max_between_time, between_time),\n                                           1E-9)\n                    max_between_count = max(max_between_count, between_count)\n                    # If progress was uniform and all time estimates were\n                    # perfect this would be the new freq to achieve time_thresh\n                    new_freq = max(int(time_thresh * max_between_count /\n                                       max_between_time), 1)\n                    if DEBUG_FREQ_ADJUST:\n                        print('\\n+---')\n                        print('[prog] between_count = %r' % between_count)\n                        print('[prog] between_time = %.8r' % between_time)\n                        print('[prog] time_thresh = %r' % time_thresh)\n                        print('[prog] max_between_count = %r' % max_between_count)\n                        print('[prog] max_between_time = %.8r' % max_between_time)\n                        print('[prog] Adusting frequency from: %r' % freq)\n                        print('[prog] Adusting frequency to: %r' % new_freq)\n                        print('L___')\n                    # But things are not perfect. So, don't make drastic changes\n                    max_freq_change_up = max(256, freq * 2)\n                    max_freq_change_down = freq // 2\n                    if (new_freq - freq) > max_freq_change_up:\n                        freq += max_freq_change_up\n                    elif (freq - new_freq) > max_freq_change_down:\n                        freq -= max_freq_change_down\n                    else:\n                        freq = new_freq\n\n                if not self.prehack:\n                    self.display_message()\n\n                # DO PROGRESS INFO\n                if self.prog_hook is not None:\n                    # From the point of view of the progress iter, we are about\n                    # to enter the body of a for loop. (But we may have\n                    # executed the body implicitly in the yeild....  so it is\n                    # ambiguous. In the second case 0 will be executed twice.\n                    self.prog_hook(self.count, length)\n\n        if self.prehack:\n            self.set_extra('')\n\n        # --- end of main loop\n        # cleanup\n        if (self.count) % freq != 0:\n            # If the final line of progress was not written in the loop, write\n            # it here\n            self.est_seconds_left = 0\n            self.total_seconds = (default_timer() - start_time)\n            self.display_message()\n            if self.prog_hook is not None:\n                # From the point of view of the progress iter, we are about to\n                # enter the body of a for loop. (But we may have executed the\n                # body implicitly in the yeild....  so it is ambiguous. In the\n                # second case 0 will be executed twice.\n                self.prog_hook(self.count, length)\n\n        self.ensure_newline()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that the current line is at the end of the current line", "response": "def ensure_newline(self):\n        \"\"\"\n        use before any custom printing when using the progress iter to ensure\n        your print statement starts on a new line instead of at the end of a\n        progress line\n        \"\"\"\n        DECTCEM_SHOW = '\\033[?25h'  # show cursor\n        AT_END = DECTCEM_SHOW + '\\n'\n        if not self._cursor_at_newline:\n            self.write(AT_END)\n            self._cursor_at_newline = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the time - thresh of the resource table.", "response": "def _get_timethresh_heuristics(self):\n        \"\"\"\n        resonably decent hueristics for how much time to wait before\n        updating progress.\n        \"\"\"\n        if self.length > 1E5:\n            time_thresh = 2.5\n        elif self.length > 1E4:\n            time_thresh = 2.0\n        elif self.length > 1E3:\n            time_thresh = 1.0\n        else:\n            time_thresh = 0.5\n        return time_thresh"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all tables in the database.", "response": "def get_tablenames(cur):\n    \"\"\" Conveinience: \"\"\"\n    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    tablename_list_ = cur.fetchall()\n    tablename_list = [str(tablename[0]) for tablename in tablename_list_ ]\n    return tablename_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a string that can be used to convert a tablename to csv format", "response": "def get_table_csv(cur, tablename, exclude_columns=[]):\n    \"\"\" Conveinience: Converts a tablename to csv format\n\n    Args:\n        tablename (str):\n        exclude_columns (list):\n\n    Returns:\n        str: csv_table\n\n    CommandLine:\n        python -m ibeis.control.SQLDatabaseControl --test-get_table_csv\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from ibeis.control.SQLDatabaseControl import *  # NOQA\n        >>> # build test data\n        >>> import ibeis\n        >>> ibs = ibeis.opendb('testdb1')\n        >>> db = ibs.db\n        >>> tablename = ibeis.const.NAME_TABLE\n        >>> exclude_columns = []\n        >>> # execute function\n        >>> csv_table = db.get_table_csv(tablename, exclude_columns)\n        >>> # verify results\n        >>> result = str(csv_table)\n        >>> print(result)\n    \"\"\"\n    import utool as ut\n    colnames_ = ut.get_table_columnname_list(cur, tablename)\n    colnames = tuple([colname for colname in colnames_ if colname not in exclude_columns])\n    row_list = ut.get_table_rows(cur, tablename, colnames, unpack=False)\n    column_list = zip(*row_list)\n    #=None, column_list=[], header='', column_type=None\n    #import utool as ut\n    #column_list, column_names = db.get_table_column_data(tablename, exclude_columns)\n    # remove column prefix for more compact csvs\n    column_lbls = [name.replace(tablename[:-1] + '_', '') for name in colnames]\n    #header = db.get_table_csv_header(tablename)\n    header = ''\n    csv_table = ut.make_csv_table(column_list, column_lbls, header)\n    return csv_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of SQLColumnRichInfo objects for the given table.", "response": "def get_table_columninfo_list(cur, tablename):\n    \"\"\"\n    Args:\n        tablename (str): table name\n\n    Returns:\n        column_list : list of tuples with format:\n            (\n                [0] column_id  : id of the column\n                [1] name       : the name of the column\n                [2] type_      : the type of the column (TEXT, INT, etc...)\n                [3] notnull    : 0 or 1 if the column can contains null values\n                [4] dflt_value : the default value\n                [5] pk         : 0 or 1 if the column partecipate to the primary key\n            )\n\n    References:\n        http://stackoverflow.com/questions/17717829/how-to-get-column-names-from-a-table-in-sqlite-via-pragma-net-c\n\n    CommandLine:\n        python -m utool.util_sqlite --test-get_table_columninfo_list\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_sqlite import *  # NOQA\n    \"\"\"\n    cur.execute('PRAGMA TABLE_INFO(\"{tablename}\")'.format(tablename=tablename))\n    colinfo_list = cur.fetchall()\n    colrichinfo_list = [SQLColumnRichInfo(*colinfo) for colinfo in colinfo_list]\n    return colrichinfo_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_location(url, base_path=None, module=False):\n    if base_path and ':' not in url:\n        slashes = base_path.endswith('/') + url.startswith('/')\n        if slashes == 0:\n            url = base_path + '/' + url\n        elif slashes == 1:\n            url = base_path + url\n        else:\n            url = base_path[:-1] + url\n\n    slash = url.rfind('/')\n    url_root, filepath = url[:slash + 1], url[slash + 1:]\n    filename, *python_path = filepath.split('.')\n\n    whitelist.check_url(url_root)\n\n    file_url = url_root + filename + '.py'\n    source = data.load(file_url, False)\n    compiled = compile(source, file_url, mode='exec')\n    local = {}\n    exec(compiled, local)\n\n    try:\n        names = local['__all__']\n    except KeyError:\n        names = local\n\n    if python_path and python_path[0] == 'py':\n        python_path.pop(0)\n\n    if not python_path:\n        if module:\n            return local\n        python_path = [importer.guess_name(names, filename, url)]\n\n    first, *rest = python_path\n\n    try:\n        result = local[first]\n    except:\n        raise AttributeError(first)\n\n    for r in rest:\n        result = getattr(result, r)\n\n    return result", "response": "Load a single Python file and extract members from it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_code(name, base_path=None, recurse=False):\n    if '/' in name:\n        return load_location(name, base_path, module=False)\n\n    return importer.import_code(name, base_path, recurse=recurse)", "response": "Load executable code from a URL or a path"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a module from a URL or a path", "response": "def load(name, base_path=None):\n    \"\"\"Load a module from a URL or a path\"\"\"\n    if '/' in name:\n        return load_location(name, base_path, module=True)\n\n    return importer.import_symbol(name, base_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extend(path=None, cache=None):\n    if path is None:\n        path = config.PATH\n    try:\n        path = path.split(':')\n    except:\n        pass\n\n    sys.path.extend([library.to_path(p, cache) for p in path])", "response": "Extend sys. path by a list of git paths."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a LEMS XML string representation of this StateVariable.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        return '<StateVariable name=\"{0}\" dimension=\"{1}\"'.format(self.name, self.dimension) +\\\n          (' exposure=\"{0}\"'.format(self.exposure) if self.exposure else '') +\\\n          '/>'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef toxml(self):\n\n        return '<DerivedVariable name=\"{0}\"'.format(self.name) +\\\n          (' dimension=\"{0}\"'.format(self.dimension) if self.dimension else '') +\\\n          (' exposure=\"{0}\"'.format(self.exposure) if self.exposure else '') +\\\n          (' select=\"{0}\"'.format(self.select) if self.select else '') +\\\n          (' value=\"{0}\"'.format(self.value) if self.value else '') +\\\n          (' reduce=\"{0}\"'.format(self.reduce) if self.reduce else '') +\\\n          (' required=\"{0}\"'.format(self.required) if self.required else '') +\\\n          '/>'", "response": "Returns a LEMS XML object representation of this DerivedVariable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, child):\n\n        if isinstance(child, Case):\n            self.add_case(child)\n        else:\n            raise ModelError('Unsupported child element')", "response": "Adds a typed child object to the conditional derived variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a LEMS XML string representation of the object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        xmlstr = '<ConditionalDerivedVariable name=\"{0}\"'.format(self.name) +\\\n          (' dimension=\"{0}\"'.format(self.dimension) if self.dimension else '') +\\\n          (' exposure=\"{0}\"'.format(self.exposure) if self.exposure else '') \n          \n        chxmlstr = ''\n\n        for case in self.cases:\n            chxmlstr += case.toxml()\n\n        if chxmlstr:\n            xmlstr += '>' + chxmlstr + '</ConditionalDerivedVariable>'\n        else:\n            xmlstr += '/>'\n            \n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, child):\n\n        if isinstance(child, Action):\n            self.add_action(child)\n        else:\n            raise ModelError('Unsupported child element')", "response": "Adds a typed child object to the event handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a LEMS XML string representation of the object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        xmlstr = '<OnStart'\n\n        chxmlstr = ''\n\n        for action in self.actions:\n            chxmlstr += action.toxml()\n\n        if chxmlstr:\n            xmlstr += '>' + chxmlstr + '</OnStart>'\n        else:\n            xmlstr += '/>'\n\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toxml(self):\n\n        xmlstr = '<OnCondition test=\"{0}\"'.format(self.test)\n\n        chxmlstr = ''\n\n        for action in self.actions:\n            chxmlstr += action.toxml()\n\n        if chxmlstr:\n            xmlstr += '>' + chxmlstr + '</OnCondition>'\n        else:\n            xmlstr += '/>'\n\n        return xmlstr", "response": "Returns a LEMS XML string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef toxml(self):\n\n        xmlstr = '<OnEvent port=\"{0}\"'.format(self.port)\n\n        chxmlstr = ''\n\n        for action in self.actions:\n            chxmlstr += action.toxml()\n\n        if chxmlstr:\n            xmlstr += '>' + chxmlstr + '</OnEvent>'\n        else:\n            xmlstr += '/>'\n\n        return xmlstr", "response": "Returns a LEMS XML string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toxml(self):\n\n        return ('<KineticScheme '\n                'name=\"{0}\" '\n                'nodes=\"{1}\" '\n                'edges=\"{2}\" '\n                'stateVariable=\"{3}\" '\n                'edgeSource=\"{4}\" '\n                'edgeTarget=\"{5}\" '\n                'forwardRate=\"{6}\" '\n                'reverseRate=\"{7}\"/>').format(self.name,\n                                              self.nodes,\n                                              self.edges,\n                                              self.state_variable,\n                                              self.edge_source,\n                                              self.edge_target,\n                                              self.forward_rate,\n                                              self.reverse_rate)", "response": "Returns a LEMS XML string representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a typed child object to the behavioral object.", "response": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the behavioral object.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, StateVariable):\n            self.add_state_variable(child)\n        elif isinstance(child, DerivedVariable):\n            self.add_derived_variable(child)\n        elif isinstance(child, ConditionalDerivedVariable):\n            self.add_conditional_derived_variable(child)\n        elif isinstance(child, TimeDerivative):\n            self.add_time_derivative(child)\n        elif isinstance(child, EventHandler):\n            self.add_event_handler(child)\n        elif isinstance(child, KineticScheme):\n            self.add_kinetic_scheme(child)\n        else:\n            raise ModelError('Unsupported child element')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a LEMS XML string representation of the object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        chxmlstr = ''\n\n        for state_variable in self.state_variables:\n            chxmlstr += state_variable.toxml()\n\n        for derived_variable in self.derived_variables:\n            chxmlstr += derived_variable.toxml()\n            \n        for conditional_derived_variable in self.conditional_derived_variables:\n            chxmlstr += conditional_derived_variable.toxml()\n\n        for time_derivative in self.time_derivatives:\n            chxmlstr += time_derivative.toxml()\n\n        for event_handler in self.event_handlers:\n            chxmlstr += event_handler.toxml()\n\n        for kinetic_scheme in self.kinetic_schemes:\n            chxmlstr += kinetic_scheme.toxml()\n\n        if isinstance(self, Dynamics):\n            for regime in self.regimes:\n                chxmlstr += regime.toxml()\n\n        if isinstance(self, Dynamics):\n            xmlprefix = 'Dynamics'\n            xmlsuffix = 'Dynamics'\n            xmlempty = ''\n        else:\n            xmlprefix = 'Regime name=\"{0}\"'.format(self.name) +\\\n              (' initial=\"true\"' if self.initial else '')\n            xmlsuffix = 'Regime'\n            xmlempty = '<{0}/>',format(xmlprefix)\n\n        if chxmlstr:\n            xmlstr = '<{0}>'.format(xmlprefix) + chxmlstr + '</{0}>'.format(xmlsuffix)\n        else:\n            xmlstr = xmlempty\n\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a typed child object to the dynamics object.", "response": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the dynamics object.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Regime):\n            self.add_regime(child)\n        else:\n            Behavioral.add(self, child)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfill lookup database with biological set names", "response": "def create_bioset_lookup(lookupdb, spectrafns, set_names):\n    \"\"\"Fills lookup database with biological set names\"\"\"\n    unique_setnames = set(set_names)\n    lookupdb.store_biosets(((x,) for x in unique_setnames))\n    set_id_map = lookupdb.get_setnames()\n    mzmlfiles = ((os.path.basename(fn), set_id_map[setname])\n                 for fn, setname in zip(spectrafns, set_names))\n    lookupdb.store_mzmlfiles(mzmlfiles)\n    lookupdb.index_biosets()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting the given module and return the new object.", "response": "def import_star(modname, parent=None):\n    \"\"\"\n    Args:\n        modname (str): module name\n\n    Tutorial:\n        Replacement for\n        from modname import *\n\n        Usage is like this\n        globals().update(ut.import_star('<module>'))\n        OR\n        ut.import_star('<module>', __name__)\n\n    Ignore:\n        >>> from utool.util_import import *  # NOQA\n        modname = 'opengm'\n        submod = None\n        parent = __name__\n    \"\"\"\n    import six\n    from os.path import dirname\n    if parent is None:\n        parent_module = None\n    else:\n        parent_module = sys.modules[parent]\n        if isinstance(parent, six.string_types):\n            parent_module = sys.modules[parent]\n        else:\n            raise ValueError('parent must be the module __name__ attribute')\n\n    try:\n        module = sys.modules[modname]\n    except KeyError:\n        try:\n            module = __import__(modname, {}, {}, fromlist=[], level=0)\n        except ImportError:\n            if parent_module is None:\n                print('Maybe try specifying parent?')\n                raise\n            # Inject into the parent if given\n            # Temporilly put this module dir in the pythonpath to simulate\n            # relative imports\n            relative_to = dirname(parent_module.__file__)\n            sys.path.append(relative_to)\n            try:\n                module = __import__(modname, {}, {}, fromlist=[], level=0)\n            except Exception:\n                raise\n            finally:\n                sys.path.pop()\n    # get public attributes\n    module_attrs = [attr for attr in dir(module) if not attr.startswith('_')]\n    module_vars = {attr: getattr(module, attr) for attr in module_attrs}\n    if parent is not None:\n        # Inject into the parent if given\n        if isinstance(parent, six.string_types):\n            parent_module = sys.modules[parent]\n            for attr, var in module_vars.items():\n                setattr(parent_module, attr, var)\n        else:\n            raise ValueError('parent must be the module __name__ attribute')\n    # return the module dictionary\n    return module_vars"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of possible import patterns for a module", "response": "def possible_import_patterns(modname):\n    \"\"\"\n    does not support from x import *\n    does not support from x import z, y\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> import utool as ut\n        >>> modname = 'package.submod.submod2.module'\n        >>> result = ut.repr3(ut.possible_import_patterns(modname))\n        >>> print(result)\n        [\n            'import\\\\spackage.submod.submod2.module',\n            'from\\\\spackage\\\\.submod\\\\.submod2\\\\simportmodule',\n        ]\n    \"\"\"\n    # common regexes\n    WS = r'\\s'\n    import_ = 'import'\n    from_ = 'from'\n    dot_ = r'\\.'\n    patterns = [import_ + WS + modname]\n    if '.' in modname:\n        parts = modname.split('.')\n        modpart = dot_.join(parts[0:-1])\n        imppart = parts[-1]\n        patterns += [from_ + WS + modpart + WS + import_ + imppart]\n    NONSTANDARD = False\n    if NONSTANDARD:\n        if '.' in modname:\n            for i in range(1, len(parts) - 1):\n                modpart = '.'.join(parts[i:-1])\n                imppart = parts[-1]\n                patterns += [from_ + WS + modpart + WS + import_ + imppart]\n            imppart = parts[-1]\n            patterns += [import_ + WS + imppart]\n    return patterns"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the path of the module with the given name.", "response": "def get_modpath_from_modname(modname, prefer_pkg=False, prefer_main=False):\n    \"\"\"\n    Same as get_modpath but doesnt import directly\n\n    SeeAlso:\n        get_modpath\n    \"\"\"\n    from os.path import dirname, basename, join, exists\n    initname = '__init__.py'\n    mainname = '__main__.py'\n    if modname in sys.modules:\n        modpath = sys.modules[modname].__file__.replace('.pyc', '.py')\n    else:\n        import pkgutil\n        loader = pkgutil.find_loader(modname)\n        modpath = loader.filename.replace('.pyc', '.py')\n        if '.' not in basename(modpath):\n            modpath = join(modpath, initname)\n    if prefer_pkg:\n        if modpath.endswith(initname) or modpath.endswith(mainname):\n            modpath = dirname(modpath)\n    if prefer_main:\n        if modpath.endswith(initname):\n            main_modpath = modpath[:-len(initname)] + mainname\n            if exists(main_modpath):\n                modpath = main_modpath\n    return modpath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a python module is installed without attempting to import it.", "response": "def check_module_installed(modname):\n    \"\"\"\n    Check if a python module is installed without attempting to import it.\n    Note, that if ``modname`` indicates a child module, the parent module is\n    always loaded.\n\n    Args:\n        modname (str):  module name\n\n    Returns:\n        bool: found\n\n    References:\n        http://stackoverflow.com/questions/14050281/module-exists-without-importing\n\n    CommandLine:\n        python -m utool.util_import check_module_installed --show --verbimp --modname=this\n        python -m utool.util_import check_module_installed --show --verbimp --modname=guitool\n        python -m utool.util_import check_module_installed --show --verbimp --modname=guitool.__PYQT__\n        python -m utool.util_import check_module_installed --show --verbimp --modname=ibeis.scripts.iccv\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_import import *  # NOQA\n        >>> import utool as ut\n        >>> modname = ut.get_argval('--modname', default='this')\n        >>> is_installed = check_module_installed(modname)\n        >>> is_imported = modname in sys.modules\n        >>> print('module(%r).is_installed = %r' % (modname, is_installed))\n        >>> print('module(%r).is_imported = %r' % (modname, is_imported))\n        >>> assert 'this' not in sys.modules, 'module(this) should not have ever been imported'\n    \"\"\"\n    import pkgutil\n    if '.' in modname:\n        # Prevent explicit import if possible\n        parts = modname.split('.')\n        base = parts[0]\n        submods = parts[1:]\n        loader = pkgutil.find_loader(base)\n        if loader is not None:\n            # TODO: check to see if path to the submod exists\n            submods\n            return True\n    loader = pkgutil.find_loader(modname)\n    is_installed = loader is not None\n    return is_installed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to import a module and return the module object.", "response": "def tryimport(modname, pipiname=None, ensure=False):\n    \"\"\"\n    CommandLine:\n        python -m utool.util_import --test-tryimport\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_tests import *  # NOQA\n        >>> import utool as ut\n        >>> modname = 'pyfiglet'\n        >>> pipiname = 'git+https://github.com/pwaller/pyfiglet'\n        >>> pyfiglet = ut.tryimport(modname, pipiname)\n        >>> assert pyfiglet is None or isinstance(pyfiglet, types.ModuleType), 'unknown error'\n\n    Example2:\n        >>> # UNSTABLE_DOCTEST\n        >>> # disabled because not everyone has access to being a super user\n        >>> from utool.util_tests import *  # NOQA\n        >>> import utool as ut\n        >>> modname = 'lru'\n        >>> pipiname = 'git+https://github.com/amitdev/lru-dict'\n        >>> lru = ut.tryimport(modname, pipiname, ensure=True)\n        >>> assert isinstance(lru, types.ModuleType), 'did not ensure lru'\n    \"\"\"\n    if pipiname is None:\n        pipiname = modname\n    try:\n        if util_inject.PRINT_INJECT_ORDER:\n            if modname not in sys.modules:\n                util_inject.noinject(modname, N=2, via='ut.tryimport')\n        module = __import__(modname)\n        return module\n    except ImportError as ex:\n        import utool as ut\n        base_pipcmd = 'pip install %s' % pipiname\n        sudo  = not ut.WIN32 and not ut.in_virtual_env()\n        if sudo:\n            pipcmd = 'sudo ' + base_pipcmd\n        else:\n            pipcmd = base_pipcmd\n        msg = 'unable to find module %s. Please install: %s' % ((modname), (pipcmd))\n        print(msg)\n        ut.printex(ex, msg, iswarning=True)\n        if ensure:\n            raise AssertionError('Ensure is dangerous behavior and is is no longer supported.')\n            #raise NotImplementedError('not ensuring')\n            ut.cmd(base_pipcmd, sudo=sudo)\n            module = tryimport(modname, pipiname, ensure=False)\n            if module is None:\n                raise AssertionError('Cannot ensure modname=%r please install using %r'  % (modname, pipcmd))\n            return module\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_module_from_fpath(module_fpath):\n    from os.path import basename, splitext, isdir, join, exists, dirname, split\n    import platform\n    if isdir(module_fpath):\n        module_fpath = join(module_fpath, '__init__.py')\n    print('module_fpath = {!r}'.format(module_fpath))\n    if not exists(module_fpath):\n        raise ImportError('module_fpath={!r} does not exist'.format(\n            module_fpath))\n    python_version = platform.python_version()\n    modname = splitext(basename(module_fpath))[0]\n    if modname == '__init__':\n        modname = split(dirname(module_fpath))[1]\n    if util_inject.PRINT_INJECT_ORDER:\n        if modname not in sys.argv:\n            util_inject.noinject(modname, N=2, via='ut.import_module_from_fpath')\n    if python_version.startswith('2.7'):\n        import imp\n        module = imp.load_source(modname, module_fpath)\n    elif python_version.startswith('3'):\n        import importlib.machinery\n        loader = importlib.machinery.SourceFileLoader(modname, module_fpath)\n        module = loader.load_module()\n        # module = loader.exec_module(modname)\n    else:\n        raise AssertionError('invalid python version={!r}'.format(\n            python_version))\n    return module", "response": "r Import a module from a file path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_events(fd, *args):\n    '''\n    get_events(fd[, timeout])\n\n    Return a list of InotifyEvent instances representing events read from\n    inotify.  If timeout is None, this will block forever until at least one\n    event can be read.  Otherwise, timeout should be an integer or float\n    specifying a timeout in seconds.  If get_events times out waiting for\n    events, an empty list will be returned.  If timeout is zero, get_events\n    will not block.\n    '''\n    return [\n      InotifyEvent(wd, mask, cookie, name)\n      for wd, mask, cookie, name in binding.get_events(fd, *args)\n    ]", "response": "Get events from inotify."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mask_description(self):\n        '''\n        Return an ASCII string describing the mask field in terms of\n        bitwise-or'd IN_* constants, or 0.  The result is valid Python code\n        that could be eval'd to get the value of the mask field.  In other\n        words, for a given event:\n\n        >>> from inotifyx import *\n        >>> assert (event.mask == eval(event.get_mask_description()))\n        '''\n\n        parts = []\n        for name, value in list(constants.items()):\n            if self.mask & value:\n                parts.append(name)\n        if parts:\n            return '|'.join(parts)\n        return '0'", "response": "Return an ASCII string describing the mask field in terms of IN_* constants or 0."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_difftext(text, other=None):\n    if other is not None:\n        # hack\n        text = util_str.difftext(text, other)\n    colortext = util_str.color_diff_text(text)\n    try:\n        print(colortext)\n    except UnicodeEncodeError as ex:  # NOQA\n        import unicodedata\n        colortext = unicodedata.normalize('NFKD', colortext).encode('ascii', 'ignore')\n        print(colortext)", "response": "Print the diff text of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints local variables in function.", "response": "def print_locals(*args, **kwargs):\n    \"\"\"\n    Prints local variables in function.\n\n    If no arguments all locals are printed.\n\n    Variables can be specified directly (variable values passed in) as varargs\n    or indirectly (variable names passed in) in kwargs by using keys and a list\n    of strings.\n    \"\"\"\n    from utool import util_str\n    from utool import util_dbg\n    from utool import util_dict\n    locals_ = util_dbg.get_parent_frame().f_locals\n    keys = kwargs.get('keys', None if len(args) == 0 else [])\n    to_print = {}\n    for arg in args:\n        varname = util_dbg.get_varname_from_locals(arg, locals_)\n        to_print[varname] = arg\n    if keys is not None:\n        to_print.update(util_dict.dict_take(locals_, keys))\n    if not to_print:\n        to_print = locals_\n    locals_str = util_str.repr4(to_print)\n    print(locals_str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_archive(archive_fpath, archive_file, archive_namelist, output_dir,\n                     force_commonprefix=True, prefix=None,\n                     dryrun=False, verbose=not QUIET, overwrite=None):\n    \"\"\"\n    archive_fpath = zip_fpath\n    archive_file = zip_file\n    \"\"\"\n    # force extracted components into a subdirectory if force_commonprefix is\n    # on return_path = output_diG\n    # FIXMpathE doesn't work right\n    if prefix is not None:\n        output_dir = join(output_dir, prefix)\n        util_path.ensurepath(output_dir)\n\n    archive_basename, ext = split_archive_ext(basename(archive_fpath))\n    if force_commonprefix and commonprefix(archive_namelist) == '':\n        # use the archivename as the default common prefix\n        output_dir = join(output_dir, archive_basename)\n        util_path.ensurepath(output_dir)\n\n    for member in archive_namelist:\n        (dname, fname) = split(member)\n        dpath = join(output_dir, dname)\n        util_path.ensurepath(dpath)\n        if verbose:\n            print('[utool] Unarchive ' + fname + ' in ' + dpath)\n\n        if not dryrun:\n            if overwrite is False:\n                if exists(join(output_dir, member)):\n                    continue\n            archive_file.extract(member, path=output_dir)\n    return output_dir", "response": "Extract the archive into the output_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_url_in_browser(url, browsername=None, fallback=False):\n    import webbrowser\n    print('[utool] Opening url=%r in browser' % (url,))\n    if browsername is None:\n        browser = webbrowser.open(url)\n    else:\n        browser = get_prefered_browser(pref_list=[browsername], fallback=fallback)\n    return browser.open(url)", "response": "r Opens a url in the specified browser"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef url_read(url, verbose=True):\n    if url.find('://') == -1:\n        url = 'http://' + url\n    if verbose:\n        print('Reading data from url=%r' % (url,))\n    try:\n        file_ = _urllib.request.urlopen(url)\n        #file_ = _urllib.urlopen(url)\n    except IOError:\n        raise\n    data = file_.read()\n    file_.close()\n    return data", "response": "r Directly reads data from url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_dropbox_link(dropbox_url):\n    cleaned_url = dropbox_url.replace('www.dropbox', 'dl.dropbox')\n    postfix_list = [\n        '?dl=0'\n    ]\n    for postfix in postfix_list:\n        if cleaned_url.endswith(postfix):\n            cleaned_url = cleaned_url[:-1 * len(postfix)]\n    # cleaned_url = cleaned_url.rstrip('?dl=0')\n    return cleaned_url", "response": "Clean up the dropbox link"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grab_selenium_driver(driver_name=None):\n    from selenium import webdriver\n    if driver_name is None:\n        driver_name = 'firefox'\n    if driver_name.lower() == 'chrome':\n        grab_selenium_chromedriver()\n        return webdriver.Chrome()\n    elif driver_name.lower() == 'firefox':\n        # grab_selenium_chromedriver()\n        return webdriver.Firefox()\n    else:\n        raise AssertionError('unknown name = %r' % (driver_name,))", "response": "Returns a Selenium driver object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grab_file_url(file_url, appname='utool', download_dir=None, delay=None,\n                  spoof=False, fname=None, verbose=True, redownload=False,\n                  check_hash=False):\n    r\"\"\"\n    Downloads a file and returns the local path of the file.\n\n    The resulting file is cached, so multiple calls to this function do not\n    result in multiple dowloads.\n\n    Args:\n        file_url (str): url to the file\n        appname (str): (default = 'utool')\n        download_dir custom directory (None): (default = None)\n        delay (None): delay time before download (default = None)\n        spoof (bool): (default = False)\n        fname (str):  custom file name (default = None)\n        verbose (bool):  verbosity flag (default = True)\n        redownload (bool): if True forces redownload of the file\n            (default = False)\n        check_hash (bool or iterable): if True, defaults to checking 4 hashes\n            (in order): custom, md5, sha1, sha256.  These hashes are checked\n            for remote copies and, if found, will check the local file.  You may\n            also specify a list of hashes to check, for example ['md5', 'sha256']\n            in the specified order.  The first verified hash to be found is used\n            (default = False)\n\n    Returns:\n        str: fpath - file path string\n\n    CommandLine:\n        python -m utool.util_grabdata --test-grab_file_url:0\n        python -m utool.util_grabdata --test-grab_file_url:1\n\n    Example0:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_grabdata import *  # NOQA\n        >>> import utool as ut  # NOQA\n        >>> from os.path import basename\n        >>> ut.exec_funckw(ut.grab_file_url, locals())\n        >>> file_url = 'http://i.imgur.com/JGrqMnV.png'\n        >>> redownload = True\n        >>> fname = 'lena.png'\n        >>> lena_fpath = ut.grab_file_url(file_url, fname=fname,\n        >>>                               redownload=redownload)\n        >>> result = basename(lena_fpath)\n        >>> print(result)\n        lena.png\n\n    Example1:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_grabdata import *  # NOQA\n        >>> import utool as ut  # NOQA\n        >>> ut.exec_funckw(ut.grab_file_url, locals())\n        >>> file_url = 'https://lev.cs.rpi.edu/public/models/detect.yolo.12.classes'\n        >>> fname = 'detect.yolo.12.classes'\n        >>> check_hash = True\n        >>> fpath = ut.grab_file_url(file_url, fname=fname, check_hash=check_hash)\n    \"\"\"\n    file_url = clean_dropbox_link(file_url)\n    if fname is None:\n        fname = basename(file_url)\n    # Download zipfile to\n    if download_dir is None:\n        download_dir = util_cplat.get_app_cache_dir(appname)\n    # Zipfile should unzip to:\n    fpath = join(download_dir, fname)\n    # If check hash, get remote hash and assert local copy is the same\n    if check_hash:\n        if isinstance(check_hash, (list, tuple)):\n            hash_list = check_hash\n        else:\n            hash_list = ['md5']\n            # hash_list = ['sha1.custom', 'md5', 'sha1', 'sha256']\n        # Get expected remote file\n        hash_remote, hash_tag_remote = grab_file_remote_hash(file_url, hash_list, verbose=verbose)\n        hash_list = [hash_tag_remote]\n        # We have a valid candidate hash from remote, check for same hash locally\n        hash_local, hash_tag_local = get_file_local_hash(fpath, hash_list, verbose=verbose)\n        if verbose:\n            print('[utool] Pre Local Hash:  %r' % (hash_local, ))\n            print('[utool] Pre Remote Hash: %r' % (hash_remote, ))\n        # Check all 4 hash conditions\n        if hash_remote is None:\n            # No remote hash provided, turn off post-download hash check\n            check_hash = False\n        elif hash_local is None:\n            if verbose:\n                print('[utool] Remote hash provided but local hash missing, redownloading.')\n            redownload = True\n        elif hash_local == hash_remote:\n            assert hash_tag_local == hash_tag_remote, ('hash tag disagreement')\n        else:\n            if verbose:\n                print('[utool] Both hashes provided, but they disagree, redownloading.')\n            redownload = True\n\n    # Download\n    util_path.ensurepath(download_dir)\n    if redownload or not exists(fpath):\n        # Download testdata\n        if verbose:\n            print('[utool] Downloading file %s' % fpath)\n        if delay is not None:\n            print('[utool] delay download by %r seconds' % (delay,))\n            time.sleep(delay)\n        download_url(file_url, fpath, spoof=spoof)\n    else:\n        if verbose:\n            print('[utool] Already have file %s' % fpath)\n\n    util_path.assert_exists(fpath)\n    # Post-download local hash verification\n    if check_hash:\n        # File has been successfuly downloaded, write remote hash to local hash file\n        hash_fpath = '%s.%s' % (fpath, hash_tag_remote, )\n        with open(hash_fpath, 'w') as hash_file:\n            hash_file.write(hash_remote)\n        # For sanity check (custom) and file verification (hashing), get local hash again\n        hash_local, hash_tag_local = get_file_local_hash(fpath, hash_list, verbose=verbose)\n        if verbose:\n            print('[utool] Post Local Hash: %r' % (hash_local, ))\n        assert hash_local == hash_remote, 'Post-download hash disagreement'\n        assert hash_tag_local == hash_tag_remote, 'Post-download hash tag disagreement'\n    return fpath", "response": "r Downloads a file and returns the local path of the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_remote(remote_uri, verbose=False):\n    remote_uri1, remote_dpath = remote_uri.split(':')\n    if not remote_dpath:\n        remote_dpath = '.'\n    import utool as ut\n    out = ut.cmd('ssh', remote_uri1, 'ls -l %s' % (remote_dpath,), verbose=verbose)\n    import re\n    # Find lines that look like ls output\n    split_lines = [re.split(r'\\s+', t) for t in out[0].split('\\n')]\n    paths = [' '.join(t2[8:]) for t2 in split_lines if len(t2) > 8]\n    return paths", "response": "List remote user s content"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace multiple blocks. blocks must be a list of tuples where each tuple consists of namespace offset key data flags", "response": "def replace_blocks(self, blocks):\n        \"\"\"Replace multiple blocks. blocks must be a list of tuples where\n        each tuple consists of (namespace, offset, key, data, flags)\"\"\"\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,?,?'\n        query = 'REPLACE INTO gauged_data (namespace, offset, `key`, ' \\\n            'data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = [param for params in rows for param in params]\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef insert_or_append_blocks(self, blocks):\n        start = 0\n        bulk_insert = self.bulk_insert\n        blocks_len = len(blocks)\n        select = 'SELECT ?,?,?,\"\",0'\n        query = 'INSERT OR IGNORE INTO gauged_data (namespace, offset, ' \\\n            '`key`, data, flags) '\n        execute = self.cursor.execute\n        while start < blocks_len:\n            rows = blocks[start:start+bulk_insert]\n            params = []\n            for namespace, offset, key, _, _ in rows:\n                params.extend((namespace, offset, key))\n            insert = (select + ' UNION ') * (len(rows) - 1) + select\n            execute(query + insert, params)\n            start += bulk_insert\n        for namespace, offset, key, data, flags in blocks:\n            execute('UPDATE gauged_data SET data = CAST(data || ? AS BLOB),'\n                    'flags = ? WHERE namespace = ? AND offset = ? AND '\n                    '`key` = ?', (data, flags, namespace, offset, key))", "response": "Insert multiple blocks. If a block already exists the data is\n        appended."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cache(self, namespace, query_hash, length, start, end):\n        query = 'SELECT start, value FROM gauged_cache WHERE namespace = ? ' \\\n            'AND hash = ? AND length = ? AND start BETWEEN ? AND ?'\n        cursor = self.cursor\n        cursor.execute(query, (namespace, query_hash, length, start, end))\n        return tuple(cursor.fetchall())", "response": "Get a cached value for the specified date range and query"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef review(cls, content, log, parent, window_icon):  # pragma: no cover\n        dlg = DlgReview(content, log, parent, window_icon)\n        if dlg.exec_():\n            return dlg.ui.edit_main.toPlainText(), \\\n                dlg.ui.edit_log.toPlainText()\n        return None, None", "response": "Reviews the final bug report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning version from setup. py", "response": "def get_version():\n        \"\"\"\n        Return version from setup.py\n        \"\"\"\n        version_desc = open(os.path.join(os.path.abspath(APISettings.VERSION_FILE)))\n        version_file = version_desc.read()\n\n        try:\n            version = re.search(r\"version=['\\\"]([^'\\\"]+)['\\\"]\", version_file).group(1)\n            return version\n        except FileNotFoundError:\n            Shell.fail('File not found!')\n            raise FileNotFoundError\n        except ValueError:\n            Shell.fail('Version not found in file ' + version_file + '!')\n            raise ValueError\n        finally:\n            version_desc.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting new version into VERSION_FILE", "response": "def set_version(old_version, new_version):\n        \"\"\"\n        Write new version into VERSION_FILE\n        \"\"\"\n        try:\n            if APISettings.DEBUG:\n                Shell.debug('* ' + old_version + ' --> ' + new_version)\n                return True\n\n            for line in fileinput.input(os.path.abspath(APISettings.VERSION_FILE), inplace=True):\n                print(line.replace(old_version, new_version), end='')\n            Shell.success('* ' + old_version + ' --> ' + new_version)\n        except FileNotFoundError:\n            Shell.warn('File not found!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_major(self):\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])+1) + '.0.0'\n        self.set_version(old_version, new_version)", "response": "Increment the major number of project\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nincrement the minor number of project", "response": "def set_minor(self):\n        \"\"\"\n        Increment the minor number of project\n        \"\"\"\n        old_version = self.get_version()\n        new_version = str(int(old_version.split('.', 5)[0])) + '.' + \\\n            str(int(old_version.split('.', 5)[1])+1) + '.0'\n        self.set_version(old_version, new_version)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the patch number of project based on the current version and the pre_release_tag.", "response": "def set_patch(self, pre_release_tag=''):\n        \"\"\"\n        Increment the patch number of project\n\n        :var release_tag describes the tag ('a', 'b', 'rc', ...)\n        :var release_tag_version describes the number behind the 'a', 'b' or 'rc'\n        For e.g.:\n        \"\"\"\n\n        current_version = self.get_version()\n        current_patch = self.get_patch_version(current_version)\n        current_pre_release_tag = self.get_current_pre_release_tag(current_patch)\n        current_RELEASE_SEPARATOR = self.get_current_RELEASE_SEPARATOR(current_patch)\n        new_patch = ''\n\n        # The new patch should get a release tag\n        if pre_release_tag:\n\n            # Check, if the current patch already contains a pre_release_tag.\n            if current_pre_release_tag:\n                new_patch = str(current_patch.split(current_pre_release_tag, 2)[0]) + pre_release_tag\n\n                if pre_release_tag == current_pre_release_tag:\n                    new_patch += str(int(current_patch.split(current_pre_release_tag, 2)[1])+1)\n                else:\n                    new_patch += '0'\n\n            # The current patch does not contains a pre_release_tag.\n            else:\n                new_patch = str(int(current_patch)+1) + \\\n                            APISettings.RELEASE_SEPARATOR + \\\n                            pre_release_tag + \\\n                            '0'\n\n        # The new patch should not contain any tag. So just increase it.\n        else:\n            if current_RELEASE_SEPARATOR:\n                new_patch = str(int(current_patch.split(current_RELEASE_SEPARATOR, 2)[0])+1)\n            elif current_pre_release_tag:\n                new_patch = str(int(current_patch.split(current_pre_release_tag, 2)[0])+1)\n            else:\n                new_patch = str(int(current_patch)+1)\n\n        new_version = str(int(current_version.split('.', 5)[0])) + '.' + \\\n            str(int(current_version.split('.', 5)[1])) + '.' + \\\n            str(new_patch)\n        self.set_version(current_version, new_version)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef flush(self):\n\n        (slice_, self.__buffer) = (self.__buffer, '')\n        self.__size = 0\n\n        return slice_", "response": "Return all buffered data and clear the stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single message from the internal buffer.", "response": "def __read_frame(self):\n        \"\"\"*Attempt* to read a frame. If we get an EAGAIN on the frame header, \n        it'll raise to our caller. If we get it *after* we already got the \n        header, wait-out the rest of the frame.\n        \"\"\"\n\n        if self.__frame_header_cache is None:\n            _logger.debug(\"Reading frame header.\")\n            (length, frame_type) = struct.unpack('!II', self.__read(8))\n            self.__frame_header_cache = (length, frame_type)\n        else:\n            (length, frame_type) = self.__frame_header_cache\n\n        try:\n            data = self.__read(length - 4)\n        except errno.EAGAIN:\n            self.__frame_header_cache = (length, frame_type)\n            raise\n\n        self.__frame_header_cache = None\n        self.__process_message(frame_type, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __receiver(self):\n\n        # If we're ignoring the quit, the connections will have to be closed \n        # by the server.\n        while (self.__ignore_quit is True or \\\n               self.__nice_quit_ev.is_set() is False) and \\\n              self.__force_quit_ev.is_set() is False:\n\n# TODO(dustin): The quit-signals aren't being properly set after a producer \n#               stop.\n\n# TODO(dustin): Consider breaking the loop if we haven't yet retried to \n#               reconnect a couple of times. A connection will automatically be \n#               reattempted.\n\n            try:\n                self.__read_frame()\n            except errno.EAGAIN:\n                gevent.sleep(nsq.config.client.READ_THROTTLE_S)\n\n        self.__receive_thread_ev.set()", "response": "Receives messages from the server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect the server and maintain the connection.", "response": "def run(self):\n        \"\"\"Connect the server, and maintain the connection. This shall not \n        return until a connection has been determined to absolutely not be \n        available.\n        \"\"\"\n\n        while self.__nice_quit_ev.is_set() is False:\n            self.__connect()\n\n        _logger.info(\"Connection re-connect loop has terminated: %s\", self.__mc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save(obj, filename, protocol=4):\n\n    with open(filename, 'wb') as f:\n        pickle.dump(obj, f, protocol=protocol)", "response": "Serialize an object to disk using pickle protocol."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_json(filename, **kwargs):\n\n    with open(filename, 'r', encoding='utf-8') as f:\n        return json.load(f, **kwargs)", "response": "Load a JSON object from the specified file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_json(obj, filename, **kwargs):\n\n    with open(filename, 'w', encoding='utf-8') as f:\n        json.dump(obj, f, **kwargs)", "response": "Save an object as a JSON file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_lines(filename):\n\n    with open(filename, 'r', encoding='utf-8') as f:\n        return [line.rstrip('\\n') for line in f.readlines()]", "response": "Load a text file as an array of strings each representing an individual line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave an array of lines to a file.", "response": "def save_lines(lines, filename):\n    \"\"\"\n    Save an array of lines to a file.\n\n    Args:\n        lines: An array of strings that will be saved as individual lines.\n        filename: Path to the output file.\n    \"\"\"\n\n    with open(filename, 'w', encoding='utf-8') as f:\n        f.write('\\n'.join(lines))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a LEMS XML property name.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        return '<Property name=\"{0}\"'.format(self.name) +\\\n          (' dimension=\"{0}\"'.format(self.dimension) if self.dimension else 'none') +\\\n          (' defaultValue = \"{0}\"'.format(self.default_value) if self.default_value else '') +\\\n          '/>'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a LEMS XML object representation of this DerivedParameter.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        return '<DerivedParameter name=\"{0}\"'.format(self.name) +\\\n          (' dimension=\"{0}\"'.format(self.dimension) if self.dimension else '') +\\\n          (' value=\"{0}\"'.format(self.value) if self.value else '') +\\\n          '/>'"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef toxml(self):\n\n        return '<Constant' +\\\n          (' name = \"{0}\"'.format(self.name) if self.name else '') +\\\n          (' symbol = \"{0}\"'.format(self.symbol) if self.symbol else '') +\\\n          (' value = \"{0}\"'.format(self.value) if self.value else '') +\\\n          (' dimension = \"{0}\"'.format(self.dimension) if self.dimension else '') +\\\n          (' description = \"{0}\"'.format(self.description) if self.description else '') +\\\n          '/>'", "response": "Returns a LEMS XML string representation of this Constant object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef toxml(self):\n\n        return '<ComponentRequirement name=\"{0}\"'.format(self.name) + '' + \\\n            (' description = \"{0}\"'.format(self.description) if self.description else '') +\\\n            '/>'", "response": "Returns a LEMS XML string representation of this component requirement."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a LEMS XML string representation of this ComponentReference.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        return '<ComponentReference name=\"{0}\" type=\"{1}\"'.format(self.name, self.type) + \\\n          (' local = \"{0}\"'.format(self.local) if self.local else '') + \\\n            '/>'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a typed child object to the component type.", "response": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the component type.\n\n        @param child: Child object to be added.\n        \"\"\"\n\n        if isinstance(child, Parameter):\n            self.add_parameter(child)\n        elif isinstance(child, Property):\n            self.add_property(child)\n        elif isinstance(child, DerivedParameter):\n            self.add_derived_parameter(child)\n        elif isinstance(child, IndexParameter):\n            self.add_index_parameter(child)\n        elif isinstance(child, Constant):\n            self.add_constant(child)\n        elif isinstance(child, Exposure):\n            self.add_exposure(child)\n        elif isinstance(child, Requirement):\n            self.add_requirement(child)\n        elif isinstance(child, ComponentRequirement):\n            self.add_component_requirement(child)\n        elif isinstance(child, InstanceRequirement):\n            self.add_instance_requirement(child)\n        elif isinstance(child, Children):\n            self.add_children(child)\n        elif isinstance(child, Text):\n            self.add_text(child)\n        elif isinstance(child, Link):\n            self.add_link(child)\n        elif isinstance(child, Path):\n            self.add_path(child)\n        elif isinstance(child, EventPort):\n            self.add_event_port(child)\n        elif isinstance(child, ComponentReference):\n            self.add_component_reference(child)\n        elif isinstance(child, Attachments):\n            self.add_attachments(child)\n        else:\n            raise ModelError('Unsupported child element')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert this object into a LEMS XML object.", "response": "def toxml(self):\n        \"\"\"\n        Exports this object into a LEMS XML object\n        \"\"\"\n\n        xmlstr = '<ComponentType name=\"{0}\"'.format(self.name) +\\\n          (' extends=\"{0}\"'.format(self.extends) if self.extends else '') +\\\n          (' description=\"{0}\"'.format(self.description) if self.description else '')\n\n        chxmlstr = ''\n\n        for property in self.properties:\n            chxmlstr += property.toxml()\n\n        for parameter in self.parameters:\n            chxmlstr += parameter.toxml()\n            \n        for derived_parameter in self.derived_parameters:\n            chxmlstr += derived_parameter.toxml()\n            \n        for index_parameter in self.index_parameters:\n            chxmlstr += index_parameter.toxml()\n\n        for constant in self.constants:\n            chxmlstr += constant.toxml()\n            \n        childxml = ''\n        childrenxml = ''\n        \n        for children in self.children:\n            if children.multiple:\n                childrenxml += children.toxml()\n            else:\n                childxml += children.toxml()\n            \n        chxmlstr += childxml\n        chxmlstr += childrenxml\n            \n        for link in self.links:\n            chxmlstr += link.toxml()\n            \n        for component_reference in self.component_references:\n            chxmlstr += component_reference.toxml()\n            \n        for attachment in self.attachments:\n            chxmlstr += attachment.toxml()\n            \n        for event_port in self.event_ports:\n            chxmlstr += event_port.toxml()\n            \n        for exposure in self.exposures:\n            chxmlstr += exposure.toxml()\n            \n        for requirement in self.requirements:\n            chxmlstr += requirement.toxml()\n            \n        for component_requirement in self.component_requirements:\n            chxmlstr += component_requirement.toxml()\n            \n        for instance_requirement in self.instance_requirements:\n            chxmlstr += instance_requirement.toxml()\n            \n        for path in self.paths:\n            chxmlstr += path.toxml()\n            \n        for text in self.texts:\n            chxmlstr += text.toxml()\n\n        chxmlstr += self.dynamics.toxml()\n        chxmlstr += self.structure.toxml()\n        chxmlstr += self.simulation.toxml()\n            \n        if chxmlstr:\n            xmlstr += '>' + chxmlstr + '</ComponentType>'\n        else:\n            xmlstr += '/>'\n\n        return xmlstr"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, child):\n\n        if isinstance(child, Component):\n            self.add_child(child)\n        else:\n            raise ModelError('Unsupported child element')", "response": "Adds a typed child object to the component."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef toxml(self):\n\n        xmlstr = '<Component id=\"{0}\" type=\"{1}\"'.format(self.id, self.type)\n\n        for (k, v) in self.parameters.items():\n            xmlstr += ' {0}=\"{1}\"'.format(k, v)\n\n        if self.children:\n            xmlstr += '>'\n            for child in self.children:\n                xmlstr += child.toxml()\n            xmlstr += '</Component>'\n        else:\n            xmlstr += '/>'\n\n        return xmlstr", "response": "Returns a LEMS XML string representation of the object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, child):\n\n        if isinstance(child, FatComponent):\n            self.add_child_component(child)\n        else:\n            Fat.add(self, child)", "response": "Adds a typed child object to the component type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the content of the object file into a ModernGL object.", "response": "def fromstring(data) -> 'Obj':\n        '''\n            Args:\n                data (str): The obj file content.\n\n            Returns:\n                Obj: The object.\n\n            Examples:\n\n                .. code-block:: python\n\n                    import ModernGL\n                    from ModernGL.ext import obj\n\n                    content = open('box.obj').read()\n                    model = obj.Obj.fromstring(content)\n        '''\n\n        vert = []\n        text = []\n        norm = []\n        face = []\n\n        data = RE_COMMENT.sub('\\n', data)\n\n        for line in data.splitlines():\n            line = line.strip()\n\n            if not line:\n                continue\n\n            match = RE_VERT.match(line)\n\n            if match:\n                vert.append(tuple(map(safe_float, match.groups())))\n                continue\n\n            match = RE_TEXT.match(line)\n            if match:\n                text.append(tuple(map(safe_float, match.groups())))\n                continue\n\n            match = RE_NORM.match(line)\n\n            if match:\n                norm.append(tuple(map(safe_float, match.groups())))\n                continue\n\n            match = RE_TRIANGLE_FACE.match(line)\n\n            if match:\n                v, t, n = match.group(1, 3, 5)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                v, t, n = match.group(6, 8, 10)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                v, t, n = match.group(11, 13, 15)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                continue\n            \n            match = RE_QUAD_FACE.match(line)\n            if match:\n                # we convert the face in two triangles\n                v, t, n = match.group(1, 3, 5)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                v, t, n = match.group(6, 8, 10)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                v, t, n = match.group(11, 13, 15)\n                face.append((int(v), int_or_none(t), int_or_none(n)))\n                v, t, n = match.group(1, 3, 5)\n                face.append((int(v), int_or_none(t), int_or_none(n)))  \n                v, t, n = match.group(11, 13, 15)\n                face.append((int(v), int_or_none(t), int_or_none(n)))                \n                v, t, n = match.group(16, 18, 20)\n                face.append((int(v), int_or_none(t), int_or_none(n)))                \n                continue            \n\n            log.debug('unknown line \"%s\"', line)\n\n        if not face:\n            raise Exception('empty')\n\n        t0, n0 = face[0][1:3]\n\n        for v, t, n in face:\n            if (t0 is None) ^ (t is None):\n                raise Exception('inconsinstent')\n\n            if (n0 is None) ^ (n is None):\n                raise Exception('inconsinstent')\n\n        return Obj(vert, text, norm, face)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npack the current object into a byte string.", "response": "def pack(self, packer=default_packer) -> bytes:\n        '''\n            Args:\n                packer (str or lambda): The vertex attributes to pack.\n\n            Returns:\n                bytes: The packed vertex data.\n\n            Examples:\n\n                .. code-block:: python\n\n                    import ModernGL\n                    from ModernGL.ext import obj\n\n                    model = obj.Obj.open('box.obj')\n\n                    # default packer\n                    data = model.pack()\n\n                    # same as the default packer\n                    data = model.pack('vx vy vz tx ty tz nx ny nz')\n\n                    # pack vertices\n                    data = model.pack('vx vy vz')\n\n                    # pack vertices and texture coordinates (xy)\n                    data = model.pack('vx vy vz tx ty')\n\n                    # pack vertices and normals\n                    data = model.pack('vx vy vz nx ny nz')\n\n                    # pack vertices with padding\n                    data = model.pack('vx vy vz 0.0')\n        '''\n\n        if isinstance(packer, str):\n            nodes = packer.split()\n            packer = eval(PACKER % (len(nodes), ', '.join(nodes)))\n\n        result = bytearray()\n\n        for v, t, n in self.face:\n            vx, vy, vz = self.vert[v - 1]\n            tx, ty, tz = self.text[t - 1] if t is not None else (0.0, 0.0, 0.0)\n            nx, ny, nz = self.norm[n - 1] if n is not None else (0.0, 0.0, 0.0)\n            result += packer(vx, vy, vz, tx, ty, tz, nx, ny, nz)\n\n        return bytes(result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def publish(self, endpoint: str, payload: str):\n        if self._conn is not None:\n            try:\n                await self._conn.publish(endpoint, payload)\n                return True\n            except redis.Error as e:\n                self._logger.error('Publish failed with error %s', repr(e))\n        return False", "response": "Publish to an endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def subscribe(self, endpoints: list, handler):\n        connection = await self._get_conn()\n        subscriber = await connection.start_subscribe()\n        await subscriber.subscribe(endpoints)\n        while True:\n            payload = await subscriber.next_published()\n            handler(payload.channel, payload.value)", "response": "Subscribe to a list of endpoints"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the given list of peptides to the database.", "response": "def write_peps(self, peps, reverse_seqs):\n        \"\"\"Writes peps to db. We can reverse to be able to look up\n        peptides that have some amino acids missing at the N-terminal.\n        This way we can still use the index.\n        \"\"\"\n        if reverse_seqs:\n            peps = [(x[0][::-1],) for x in peps]\n        cursor = self.get_cursor()\n        cursor.executemany(\n            'INSERT INTO known_searchspace(seqs) VALUES (?)', peps)\n        self.conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if a sequence exists in sqlite DB. Returns True or False if it doesn t exist.", "response": "def check_seq_exists(self, seq, amount_ntermwildcards):\n        \"\"\"Look up sequence in sqlite DB. Returns True or False if it\n        exists (or not). When looking up a reversed DB with\n        ntermwildcards: we reverse the sequence of the pep and add\n        a LIKE and %-suffix to the query.\n        \"\"\"\n        cursor = self.get_cursor()\n        if amount_ntermwildcards > 0:\n            seq = seq[::-1]\n            sqlseq = '{}{}'.format(seq, '%')\n            # FIXME non-parametrized string binding because if ? binding is\n            # used the INDEX is not used when looking up, apparently because\n            # the query cant be optimized when using LIKE and binding.\n            sql = ('select seqs from known_searchspace where seqs LIKE '\n                   '\"{}\"'.format(sqlseq))\n            for match in cursor.execute(sql):\n                if match[0][:-amount_ntermwildcards] in seq:\n                    return True\n            return False\n        else:\n            sql = ('select exists(select seqs from known_searchspace '\n                   'where seqs=? limit 1)')\n            return cursor.execute(sql, (seq, )).fetchone()[0] == 1"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_autogen_str():\n    import utool as ut\n    def get_regen_cmd():\n        try:\n            if len(sys.argv) > 0 and ut.checkpath(sys.argv[0]):\n                # Check if running python command\n                if ut.is_python_module(sys.argv[0]):\n                    python_exe = ut.python_executable(check=False)\n                    modname = ut.get_modname_from_modpath(sys.argv[0])\n                    new_argv = [python_exe, '-m', modname] + sys.argv[1:]\n                    return ' '.join(new_argv)\n        except Exception as ex:\n            ut.printex(ex, iswarning=True)\n        return ' '.join(sys.argv)\n\n    autogenkw = dict(\n        stamp=ut.timestamp('printable'),\n        regen_cmd=get_regen_cmd()\n        #' '.join(sys.argv)\n    )\n    return ut.codeblock(\n        '''\n        # Autogenerated on {stamp}\n        # Regen Command:\n        #    {regen_cmd}\n        #\n        '''\n    ).format(**autogenkw)", "response": "r Creates a string that can be used to generate a new version of the current language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_ipython_notebook(notebook_str):\n    from runipy.notebook_runner import NotebookRunner\n    import nbformat\n    import logging\n    log_format = '%(asctime)s %(levelname)s: %(message)s'\n    log_datefmt = '%m/%d/%Y %I:%M:%S %p'\n    logging.basicConfig(\n        level=logging.INFO, format=log_format, datefmt=log_datefmt\n    )\n    #fpath = 'tmp.ipynb'\n    #notebook_str = ut.readfrom(fpath)\n    #nb3 = IPython.nbformat.reads(notebook_str, 3)\n    #cell = nb4.cells[1]\n    #self = runner\n    #runner = NotebookRunner(nb3, mpl_inline=True)\n    print('Executing IPython notebook')\n    nb4 = nbformat.reads(notebook_str, 4)\n    runner = NotebookRunner(nb4)\n    runner.run_notebook(skip_exceptions=False)\n    run_nb = runner.nb\n    return run_nb", "response": "Runs an IPython notebook"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef markdown_cell(markdown):\n    import utool as ut\n    markdown_header = ut.codeblock(\n        '''\n          {\n           \"cell_type\": \"markdown\",\n           \"metadata\": {},\n           \"source\": [\n        '''\n    )\n    markdown_footer = ut.codeblock(\n        '''\n           ]\n          }\n        '''\n    )\n    return (markdown_header + '\\n' +\n            ut.indent(repr_single_for_md(markdown), ' ' * 2) +\n            '\\n' + markdown_footer)", "response": "r Converts markdown cell into json formatted ipython notebook markdown cell"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_notebook(cell_list):\n    import utool as ut\n    header = ut.codeblock(\n        '''\n        {\n         \"cells\": [\n        '''\n    )\n\n    footer = ut.codeblock(\n        '''\n         ],\n         \"metadata\": {\n          \"kernelspec\": {\n           \"display_name\": \"Python 2\",\n           \"language\": \"python\",\n           \"name\": \"python2\"\n          },\n          \"language_info\": {\n           \"codemirror_mode\": {\n            \"name\": \"ipython\",\n            \"version\": 2\n           },\n           \"file_extension\": \".py\",\n           \"mimetype\": \"text/x-python\",\n           \"name\": \"python\",\n           \"nbconvert_exporter\": \"python\",\n           \"pygments_lexer\": \"ipython2\",\n           \"version\": \"2.7.6\"\n          }\n         },\n         \"nbformat\": 4,\n         \"nbformat_minor\": 0\n        }\n        ''')\n\n    cell_body = ut.indent(',\\n'.join(cell_list), '  ')\n    notebook_str = header + '\\n' + cell_body +  '\\n' +  footer\n    try:\n        import json\n        json.loads(notebook_str)\n    except ValueError as ex:\n        ut.printex(ex, 'Invalid notebook JSON')\n        raise\n    return notebook_str", "response": "Creates a notebook from a list of cell names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninstall an except hook that will show the crash report dialog when an unhandled exception has occured.", "response": "def install_except_hook(except_hook=_hooks.except_hook):\n    \"\"\"\n    Install an except hook that will show the crash report dialog when an\n    unhandled exception has occured.\n\n    :param except_hook: except_hook function that will be called on the main\n        thread whenever an unhandled exception occured. The function takes\n        two parameters: the exception object and the traceback string.\n    \"\"\"\n    if not _backends:\n        raise ValueError('no backends found, you must at least install one '\n                         'backend before calling this function')\n    global _except_hook\n    _except_hook = _hooks.QtExceptHook(except_hook)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show_report_dialog(window_title='Report an issue...',\n                       window_icon=None, traceback=None, issue_title='',\n                       issue_description='', parent=None,\n                       modal=None, include_log=True, include_sys_info=True):\n    \"\"\"\n    Show the issue report dialog manually.\n\n    :param window_title: Title of dialog window\n    :param window_icon: the icon to use for the dialog window\n    :param traceback: optional traceback string to include in the report.\n    :param issue_title: optional issue title\n    :param issue_description: optional issue description\n    :param parent: parent widget\n    :param include_log: Initial state of the include log check box\n    :param include_sys_info: Initial state of the include system info check box\n    \"\"\"\n    if not _backends:\n        raise ValueError('no backends found, you must at least install one '\n                         'backend before calling this function')\n    from ._dialogs.report import DlgReport\n    dlg = DlgReport(_backends, window_title=window_title,\n                    window_icon=window_icon, traceback=traceback,\n                    issue_title=issue_title,\n                    issue_description=issue_description, parent=parent,\n                    include_log=include_log, include_sys_info=include_sys_info)\n    if modal:\n        dlg.show()\n        return dlg\n    else:\n        dlg.exec_()", "response": "Show the issue report dialog manually."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nappends a Middleware to the route which runs before the route runs", "response": "def middleware(self, args):\n        \"\"\"\n        Appends a Middleware to the route which is to be executed before the route runs\n        \"\"\"\n        if self.url[(len(self.url) - 1)] == (self.url_, self.controller, dict(method=self.method, request_type=self.request_type, middleware=None)):\n            self.url.pop()\n        self.url.append((self.url_, self.controller, dict(method=self.method, request_type=self.request_type, middleware=args)))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, url, controller):\n\n        self.request_type = 'GET'\n        controller_class, controller_method = self.__return_controller__(controller)\n\n        self.controller = controller_class\n        self.method = controller_method\n        self.url_ = url\n\n        self.url.append((url, controller_class, dict(method=controller_method, request_type=self.request_type, middleware=None)))\n        return self", "response": "Gets the Controller and adds the route controller and method to the url list for GET request"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_bytes(value):\n    if isinstance(value, unicode):\n        return value.encode('utf8')\n    elif not isinstance(value, str):\n        return str(value)\n    return value", "response": "Get a byte array representing the value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef table_repr(columns, rows, data, padding=2):\n    padding = ' ' * padding\n    column_lengths = [len(column) for column in columns]\n    for row in rows:\n        for i, column in enumerate(columns):\n            item = str(data[row][column])\n            column_lengths[i] = max(len(item), column_lengths[i])\n    max_row_length = max(len(row) for row in rows) if len(rows) else 0\n    table_row = ' ' * max_row_length\n    for i, column in enumerate(columns):\n        table_row += padding + column.rjust(column_lengths[i])\n    table_rows = [table_row]\n    for row in rows:\n        table_row = row.rjust(max_row_length)\n        for i, column in enumerate(columns):\n            item = str(data[row][column])\n            table_row += padding + item.rjust(column_lengths[i])\n        table_rows.append(table_row)\n    return '\\n'.join(table_rows)", "response": "Generate a table for cli output"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_proteins_for_db(fastafn):\n    objects = {}\n    for record in parse_fasta(fastafn):\n        objects[parse_protein_identifier(record)] = record\n    return (((acc,) for acc in list(objects)),\n            ((acc, str(record.seq)) for acc, record in objects.items()),\n            ((acc, get_uniprot_evidence_level(record.description))\n             for acc, record in objects.items()))", "response": "Runs through a FASTA file and returns proteins accession nrs sequences\n    and evidence levels for storage in lookup DB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_proteins_genes(fastafn, fastadelim=None, genefield=None):\n    with open(fastafn) as fp:\n        firstline = next(fp).strip()\n    if firstline[0] == '>':\n        for record in parse_fasta(fastafn):\n            rectype = get_record_type(record)\n            yield (record.id, get_gene(record.description, rectype,\n                                       fastadelim, genefield),\n                   None, record.description)\n    elif 'Ensembl Gene ID' in firstline.split('\\t'):\n        for line in parse_biomart_fn(fastafn, 'Ensembl Gene ID',\n                                     'Ensembl Protein ID', 'Description',\n                                     'HGNC symbol', 'Associated Gene Name'):\n            yield line\n    elif 'Gene ID' in firstline.split('\\t'):\n        for line in parse_biomart_fn(fastafn, 'Gene ID', 'Protein ID',\n                                     'Description', 'HGNC symbol',\n                                     'Associated Gene Name'):\n            yield line", "response": "This function returns a tuple of protein gene HGNC symbol and description from a Biomart mapping file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the uniprot protein existence evidence level for a fasta header.", "response": "def get_uniprot_evidence_level(header):\n    \"\"\"Returns uniprot protein existence evidence level for a fasta header.\n    Evidence levels are 1-5, but we return 5 - x since sorting still demands\n    that higher is better.\"\"\"\n    header = header.split()\n    for item in header:\n        item = item.split('=')\n        try:\n            if item[0] == 'PE':\n                return 5 - int(item[1])\n        except IndexError:\n            continue\n    return -1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a raw version of the URL if there is one.", "response": "def raw(url):\n    \"\"\"Return a raw version of the URL if there is one.\n    Otherwise returns the original URL.\n\n    Many repos have \"raw\" and \"user-friendly\" versions of each URL.  Usually\n    when you want to download something programmatically, you want the raw\n    version, but users will enter the user-friendly version as it is the one\n    they usually see.\n\n    If this function recognizes one of those cases, it converts the\n    user-friendly URL into the raw - otherwise it returns the original URL.\n\n    The function works by default for two git providers: github and gitlab.\n    You can use others by passing in your own url_rewriters list.\n\n    There is also a special case for Github gists.\n    \"\"\"\n    try:\n        # If it's a user-friendly gist URL, get the real data by parsing\n        # the file.\n        parts = url.split('/')\n        if parts[2] == 'gist.github.com' and '.' not in parts[:-1]:\n            soup = BeautifulSoup(requests.get(url).text, 'html.parser')\n            raw_links = [i for i in soup.find_all('a') if i.text == 'Raw']\n            return ('https://gist.githubusercontent.com' +\n                    raw_links[0].attrs['href'])\n    except Exception as e:\n        print('Failed open and parse', url, e)\n        pass\n\n    # https: /     /github.com/user/ project/ blob/ master/tox.ini\n    try:\n        protocol, empty, provider, user, project, _, *rest = url.split('/')\n    except:\n        return url\n\n    rewriter = URL_REWRITERS.get(provider)\n\n    if protocol and (not empty) and user and project and rest and rewriter:\n        parts = [protocol, empty, rewriter['provider'], user, project]\n        return '/'.join(parts + rewriter['path'] + rest)\n\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        self.pre_run()\n        first = True\n        while self.runnable:\n            self.pre_call_message()\n\n            if first:\n                self.pre_first_call_message()\n            \n            message, payload = self.listener.get()\n            getattr(self, message)(payload)\n\n            if first:\n                first = False\n                self.post_first_call_message()\n                \n            self.post_call_message()\n\n        self.post_run()", "response": "Run our loop and any defined hooks..."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_multiplicities(times, tmax=20):\n    n = times.shape[0]\n    mtp = np.ones(n, dtype='<i4')    # multiplicities\n    cid = np.zeros(n, '<i4')    # coincidence id\n    idx0 = 0\n    _mtp = 1\n    _cid = 0\n    t0 = times[idx0]\n    for i in range(1, n):\n        dt = times[i] - t0\n        if dt > tmax:\n            mtp[idx0:i] = _mtp\n            cid[idx0:i] = _cid\n            _mtp = 0\n            _cid += 1\n            idx0 = i\n            t0 = times[i]\n        _mtp += 1\n        if i == n - 1:\n            mtp[idx0:] = _mtp\n            cid[idx0:] = _cid\n            break\n\n    return mtp, cid", "response": "Calculate an array of multiplicities and corresponding coincidence IDs for a single node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef func_load(code, defaults=None, closure=None, globs=None):\n    if isinstance(code, (tuple, list)):  # unpack previous dump\n        code, defaults, closure = code\n    code = marshal.loads(code)\n    if closure is not None:\n        closure = func_reconstruct_closure(closure)\n    if globs is None:\n        globs = globals()\n    return types.FunctionType(code, globs, name=code.co_name, argdefs=defaults, closure=closure)", "response": "Load a function from a code object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_machine(lines):\n    if lines == []:\n        raise SyntaxError('Empty file')\n    else:\n        machine = Machine(lines[0].split())\n        for line in lines[1:]:\n            if line.strip() != '':\n                machine.add_state(line)\n    machine.check()\n    return machine", "response": "Build a machine from a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses rule and add it to machine.", "response": "def _add_rule(self, state, rule):\n        \"\"\"Parse rule and add it to machine (for internal use).\"\"\"\n        if rule.strip() == \"-\":\n            parsed_rule = None\n        else:\n            parsed_rule = rule.split(',')\n\n            if  (len(parsed_rule) != 3 or\n                 parsed_rule[1] not in ['L', 'N', 'R'] or\n                 len(parsed_rule[2]) > 1):\n                raise SyntaxError('Wrong format of rule: ' + rule)\n\n            if parsed_rule[0] == \"\":\n                parsed_rule[0] = self.alphabet[len(self.states[state])]\n            if parsed_rule[2] == \"\":\n                parsed_rule[2] = state\n\n        self.states[state].append(parsed_rule)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd state and rules to machine.", "response": "def add_state(self, string):\n        \"\"\"Add state and rules to machine.\"\"\"\n        parsed_string = string.split()\n        if len(parsed_string) > 0:\n            state, rules = parsed_string[0], parsed_string[1:]\n\n            if len(rules) != len(self.alphabet):\n                raise SyntaxError('Wrong count of rules ({cur}/{exp}): {string}'\n                                  .format(cur=len(rules), exp=len(self.alphabet),\n                                          string=string))\n\n            if state in self.states or state == self.TERM_STATE:\n                raise SyntaxError('Double definition of state: ' + state)\n            else:\n                self.states[state] = []\n\n            for rule in rules:\n                try:\n                    self._add_rule(state, rule)\n                except SyntaxError as err:\n                    self.states.pop(state)\n                    raise err"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the system values.", "response": "def init_tape(self, string):\n        \"\"\"Init system values.\"\"\"\n        for char in string:\n            if char not in self.alphabet and not char.isspace() and char != self.EMPTY_SYMBOL:\n                raise RuntimeError('Invalid symbol: \"' + char + '\"')\n\n        self.check()\n        self.state = self.START_STATE\n        self.head = 0\n\n        self.tape = {}\n        for i in range(len(string)):\n            symbol = string[i] if not string[i].isspace() else self.EMPTY_SYMBOL\n            self.tape[i] = symbol"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget content of tape.", "response": "def get_tape(self):\n        \"\"\"Get content of tape.\"\"\"\n        result = ''\n        for i in range(min(self.tape), max(self.tape) + 1):\n            symbol = self.tape[i] if self.tape[i] != self.EMPTY_SYMBOL else ' '\n            result += symbol\n        # Remove unnecessary empty symbols on tape\n        return result.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting one step of execution.", "response": "def execute_once(self):\n        \"\"\"One step of execution.\"\"\"\n        symbol = self.tape.get(self.head, self.EMPTY_SYMBOL)\n\n        index = self.alphabet.index(symbol)\n        rule = self.states[self.state][index]\n\n        if rule is None:\n            raise RuntimeError('Unexpected symbol: ' + symbol)\n\n        self.tape[self.head] = rule[0]\n\n        if rule[1] == 'L':\n            self.head -= 1\n        elif rule[1] == 'R':\n            self.head += 1\n\n        self.state = rule[2]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, string, max_tacts=None):\n        self.init_tape(string)\n        counter = 0\n\n        while True:\n            self.execute_once()\n            if self.state == self.TERM_STATE:\n                break\n            counter += 1\n            if max_tacts is not None and counter >= max_tacts:\n                raise TimeoutError(\"algorithm hasn't been stopped\")\n\n        return self.get_tape()", "response": "Execute algorithm (if max_times = None, there can be forever loop)."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compile(self):\n        result = TEMPLATE\n        result += 'machine = Machine(' + repr(self.alphabet) + ')\\n'\n\n        for state in self.states:\n            repr_state = state[0]\n            for rule in self.states[state]:\n                repr_state += ' ' + (','.join(rule) if rule is not None else '-')\n\n            result += (\"machine.add_state({repr_state})\\n\".format(repr_state=repr(repr_state)))\n\n        result += \"for line in stdin:\\n\"\n        result += \"    print(machine.execute(line))\"\n        return result", "response": "Return python code for create and execute machine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if all required services are provided", "response": "def get_missing_services(self, services):\n        \"\"\"\n        Check if all required services are provided\n\n        Args:\n            services: List with the service names which are required\n        Returns:\n            List with missing services\n        \"\"\"\n        required_services = set(services)\n        provided_services = set(self._services.keys())\n        missing_services = required_services.difference(provided_services)\n\n        return sorted(missing_services)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef attach(self, module_factory, name=None, **kwargs):\n        if self.anybar: self.anybar.change(\"yellow\")\n\n        fac = module_factory\n\n        if name is None:\n            name = fac.__name__\n\n        log.info(\"Attaching module '{0}'\".format(name))\n\n        if (inspect.isclass(fac) and issubclass(fac, Module)) or \\\n                name == 'GenericPump':\n            log.debug(\"Attaching as regular module\")\n            if name in self.module_configuration:\n                log.debug(\n                    \"Applying pipeline configuration file for module '%s'\" %\n                    name\n                )\n                for key, value in self.module_configuration[name].items():\n                    if key in kwargs:\n                        self.log.info(\n                            \"Overwriting parameter '%s' in module '%s' from \"\n                            \"the pipeline configuration file.\" % (key, name)\n                        )\n                    kwargs[key] = value\n            module = fac(name=name, **kwargs)\n            if hasattr(module, \"provided_services\"):\n                for service_name, obj in module.provided_services.items():\n                    self.services.register(service_name, obj)\n            if hasattr(module, \"required_services\"):\n                updated_required_services = {}\n                updated_required_services.update(self.required_services)\n                updated_required_services.update(module.required_services)\n                self.required_services = updated_required_services\n            module.services = self.services\n            module.pipeline = self\n        else:\n            if isinstance(fac, types.FunctionType):\n                log.debug(\"Attaching as function module\")\n            else:\n                log.critical(\n                    \"Don't know how to attach module '{0}'!\\n\"\n                    \"But I'll do my best\".format(name)\n                )\n            module = fac\n            module.name = name\n            module.timeit = self.timeit\n\n        # Special parameters\n        if 'only_if' in kwargs:\n            required_keys = kwargs['only_if']\n            if isinstance(required_keys, basestring):\n                required_keys = [required_keys]\n            module.only_if = set(required_keys)\n        else:\n            module.only_if = set()\n\n        if 'blob_keys' in kwargs:\n            module.blob_keys = kwargs['blob_keys']\n        else:\n            module.blob_keys = None\n\n        if 'every' in kwargs:\n            module.every = kwargs['every']\n        else:\n            module.every = 1\n\n        self._timeit[module] = {\n            'process': deque(maxlen=STAT_LIMIT),\n            'process_cpu': deque(maxlen=STAT_LIMIT),\n            'finish': 0,\n            'finish_cpu': 0\n        }\n\n        if hasattr(module, 'get_detector'):    # Calibration-like module\n            self.log.deprecation(\n                \"Calibration-like modules will not be supported in future \"\n                \"versions of KM3Pipe. Please use services instead.\\n\"\n                \"If you are attaching the `calib.Calibration` as a module, \"\n                \"switch to `calib.CalibrationService` and use the \"\n                \"`self.services['calibrate']()` method in your modules \"\n                \"to apply calibration.\\n\\n\"\n                \"This means:\\n\\n\"\n                \"    pipe.attach(kp.calib.CalibrationService, ...)\\n\\n\"\n                \"And inside the attached modules, you can apply the \"\n                \"calibration with e.g.:\\n\\n\"\n                \"    cal_hits = self.services['calibrate'](blob['Hits'])\\n\"\n            )\n            self.calibration = module\n            if module._should_apply:\n                self.modules.append(module)\n        else:    # normal module\n            module.calibration = self.calibration\n            self.modules.append(module)", "response": "Attach a module to the pipeline system."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nactivates the pump and let the flow go. This will call the process() method on each attached module until a StopIteration is raised, usually by a pump when it reached the EOF. A StopIteration is also raised when self.cycles was set and the number of cycles has reached that limit.", "response": "def _drain(self, cycles=None):\n        \"\"\"Activate the pump and let the flow go.\n\n        This will call the process() method on each attached module until\n        a StopIteration is raised, usually by a pump when it reached the EOF.\n\n        A StopIteration is also raised when self.cycles was set and the\n        number of cycles has reached that limit.\n\n        \"\"\"\n        log.info(\"Now draining...\")\n        if not cycles:\n            log.info(\"No cycle count, the pipeline may be drained forever.\")\n\n        if self.calibration:\n            log.info(\"Setting up the detector calibration.\")\n            for module in self.modules:\n                module.detector = self.calibration.get_detector()\n\n        try:\n            while not self._stop:\n                cycle_start = timer()\n                cycle_start_cpu = process_time()\n\n                log.debug(\"Pumping blob #{0}\".format(self._cycle_count))\n                self.blob = Blob()\n\n                for module in self.modules:\n                    if self.blob is None:\n                        log.debug(\n                            \"Skipping {0}, due to empty blob.\".format(\n                                module.name\n                            )\n                        )\n                        continue\n                    if module.only_if and not module.only_if.issubset(set(\n                            self.blob.keys())):\n                        log.debug(\n                            \"Skipping {0}, due to missing required key\"\n                            \"'{1}'.\".format(module.name, module.only_if)\n                        )\n                        continue\n\n                    if (self._cycle_count + 1) % module.every != 0:\n                        log.debug(\n                            \"Skipping {0} (every {1} iterations).\".format(\n                                module.name, module.every\n                            )\n                        )\n                        continue\n\n                    if module.blob_keys is not None:\n                        blob_to_send = Blob({\n                            k: self.blob[k]\n                            for k in module.blob_keys\n                            if k in self.blob\n                        })\n                    else:\n                        blob_to_send = self.blob\n\n                    log.debug(\"Processing {0} \".format(module.name))\n                    start = timer()\n                    start_cpu = process_time()\n                    new_blob = module(blob_to_send)\n                    if self.timeit or module.timeit:\n                        self._timeit[module]['process'] \\\n                            .append(timer() - start)\n                        self._timeit[module]['process_cpu'] \\\n                            .append(process_time() - start_cpu)\n\n                    if module.blob_keys is not None:\n                        if new_blob is not None:\n                            for key in new_blob.keys():\n                                self.blob[key] = new_blob[key]\n                    else:\n                        self.blob = new_blob\n\n                self._timeit['cycles'].append(timer() - cycle_start)\n                self._timeit['cycles_cpu'].append(\n                    process_time() - cycle_start_cpu\n                )\n                self._cycle_count += 1\n                if cycles and self._cycle_count >= cycles:\n                    raise StopIteration\n        except StopIteration:\n            log.info(\"Nothing left to pump through.\")\n        return self.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting _drain while trapping KeyboardInterrupt", "response": "def drain(self, cycles=None):\n        \"\"\"Execute _drain while trapping KeyboardInterrupt\"\"\"\n        if not self._check_service_requirements():\n            self.init_timer.stop()\n            return self.finish()\n\n        if self.anybar: self.anybar.change(\"orange\")\n        self.init_timer.stop()\n        log.info(\"Trapping CTRL+C and starting to drain.\")\n        signal.signal(signal.SIGINT, self._handle_ctrl_c)\n        with ignored(KeyboardInterrupt):\n            return self._drain(cycles)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall finish on each attached module", "response": "def finish(self):\n        \"\"\"Call finish() on each attached module\"\"\"\n        if self.anybar: self.anybar.change(\"purple\")\n\n        finish_blob = Blob()\n        for module in self.modules:\n            if hasattr(module, 'pre_finish'):\n                log.info(\"Finishing {0}\".format(module.name))\n                start_time = timer()\n                start_time_cpu = process_time()\n                finish_blob[module.name] = module.pre_finish()\n                self._timeit[module]['finish'] = timer() - start_time\n                self._timeit[module]['finish_cpu'] = \\\n                    process_time() - start_time_cpu\n            else:\n                log.info(\"Skipping function module {0}\".format(module.name))\n        self._timeit['finish'] = timer()\n        self._timeit['finish_cpu'] = process_time()\n        self._print_timeit_statistics()\n        self._finished = True\n\n        if self.anybar: self.anybar.change(\"green\")\n        return finish_blob"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_ctrl_c(self, *args):\n        if self.anybar: self.anybar.change(\"exclamation\")\n\n        if self._stop:\n            print(\"\\nForced shutdown...\")\n            raise SystemExit\n        if not self._stop:\n            hline = 42 * '='\n            print(\n                '\\n' + hline + \"\\nGot CTRL+C, waiting for current cycle...\\n\"\n                \"Press CTRL+C again if you're in hurry!\\n\" + hline\n            )\n            self._stop = True", "response": "Handle the keyboard interrupts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the value of the requested parameter or default.", "response": "def get(self, name, default=None):\n        \"\"\"Return the value of the requested parameter or `default` if None.\"\"\"\n        value = self.parameters.get(name)\n        self._processed_parameters.append(name)\n        if value is None:\n            return default\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef require(self, name):\n        value = self.get(name)\n        if value is None:\n            raise TypeError(\n                \"{0} requires the parameter '{1}'.\".format(\n                    self.__class__, name\n                )\n            )\n        return value", "response": "Return the value of the requested parameter. Raise an error if the requested parameter is not present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if any of the parameters passed in are ignored", "response": "def _check_unused_parameters(self):\n        \"\"\"Check if any of the parameters passed in are ignored\"\"\"\n        all_params = set(self.parameters.keys())\n        processed_params = set(self._processed_parameters)\n        unused_params = all_params - processed_params - RESERVED_ARGS\n\n        if unused_params:\n            self.log.warning(\n                \"The following parameters were ignored: {}\".format(\n                    ', '.join(sorted(unused_params))\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_file(self, filename):\n        try:\n            if filename.endswith('.gz'):\n                self.blob_file = gzip.open(filename, 'rb')\n            else:\n                self.blob_file = open(filename, 'rb')\n        except TypeError:\n            log.error(\"Please specify a valid filename.\")\n            raise SystemExit\n        except IOError as error_message:\n            log.error(error_message)\n            raise SystemExit", "response": "Open the file with filename"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_data(self, raw_data, var_filter, time_extents):\n\n        retval = defaultdict(list)\n        p = parser()\n\n        begin_time, end_time = time_extents\n\n        for line in raw_data.splitlines():\n            if len(line) == 0:\n                continue\n\n            fields = line.split(\"|\")[0:-1]\n            if var_filter is None or fields[2] in var_filter:\n                dt = p.parse(fields[3]).replace(tzinfo=pytz.utc)\n                if (begin_time is None or dt >= begin_time) and (\n                    end_time is None or dt <= end_time\n                ):\n                    try:\n                        value = (\n                            float(fields[4]) if fields[4] != \"NaN\" else npNan\n                        )\n                    except ValueError:\n                        value = npNan\n                    retval[fields[0]].append((fields[2], dt, value))\n\n        return dict(retval)", "response": "Parses the HADS data into a dictionary of station code - > list of variable - > value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_metadata(self, metadata):\n        retval = {}\n\n        # these are the first keys, afterwards follows a var-len list of variables/props\n        # first key always blank so skip it\n        field_keys = [\n            \"nesdis_id\",\n            \"nwsli\",\n            \"location_text\",\n            \"latitude\",\n            \"longitude\",\n            \"hsa\",\n            \"state\",\n            \"owner\",\n            \"manufacturer\",\n            \"channel\",\n            \"init_transmit\",  # HHMM\n            \"trans_interval\",\n        ]  # min\n\n        # repeat in blocks of 7 after field_keys\n        var_keys = [\n            \"pe_code\",\n            \"data_interval\",  # min\n            \"coefficient\",\n            \"constant\",\n            \"time_offset\",  # min\n            \"base_elevation\",  # ft\n            \"gauge_correction\",\n        ]  # ft\n\n        lines = metadata.splitlines()\n        for line in lines:\n            if len(line) == 0:\n                continue\n\n            raw_fields = line.split(\"|\")\n\n            fields = dict(zip(field_keys, raw_fields[1 : len(field_keys)]))\n\n            # how many blocks of var_keys after initial fields\n            var_offset = len(field_keys) + 1\n            var_blocks = (len(raw_fields) - var_offset) // len(\n                var_keys\n            )  # how many variables\n            vars_only = raw_fields[var_offset:]\n            variables = {}\n\n            for offset in range(var_blocks):\n                var_dict = dict(\n                    zip(\n                        var_keys,\n                        vars_only[\n                            offset\n                            * len(var_keys) : (offset + 1)\n                            * len(var_keys)\n                        ],\n                    )\n                )\n                variables[var_dict[\"pe_code\"]] = var_dict\n\n                var_dict[\"base_elevation\"] = float(var_dict[\"base_elevation\"])\n                var_dict[\"gauge_correction\"] = float(\n                    var_dict[\"gauge_correction\"]\n                )\n                del var_dict[\"pe_code\"]  # no need to duplicate\n\n            line_val = {\"variables\": variables}\n            line_val.update(fields)\n\n            # conversions\n            def dms_to_dd(dms):\n                parts = dms.split(\" \")\n                sec = int(parts[1]) * 60 + int(parts[2])\n                return float(parts[0]) + (\n                    sec / 3600.0\n                )  # negative already in first portion\n\n            line_val[\"latitude\"] = dms_to_dd(line_val[\"latitude\"])\n            line_val[\"longitude\"] = dms_to_dd(line_val[\"longitude\"])\n\n            retval[line_val[\"nesdis_id\"]] = line_val\n\n        return retval", "response": "Parses the HADS metadata into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tuple of the variable name units english name and english description of the variable.", "response": "def get_variable_info(cls, hads_var_name):\n        \"\"\"\n        Returns a tuple of (mmi name, units, english name, english description) or None.\n        \"\"\"\n        if hads_var_name == \"UR\":\n            return (\n                \"wind_gust_from_direction\",\n                \"degrees from N\",\n                \"Wind Gust from Direction\",\n                \"Direction from which wind gust is blowing when maximum wind speed is observed.  Meteorological Convention. Wind is motion of air relative to the surface of the earth.\",\n            )\n        elif hads_var_name in [\"VJA\", \"TX\"]:\n            return (\n                \"air_temperature_maximum\",\n                \"f\",\n                \"Air Temperature Maximum\",\n                \"\",\n            )\n        elif hads_var_name in [\"VJB\", \"TN\"]:\n            return (\n                \"air_temperature_minimum\",\n                \"f\",\n                \"Air Temperature Minumum\",\n                \"\",\n            )\n        elif hads_var_name == \"PC\":  # PC2?\n            return (\n                \"precipitation_accumulated\",\n                \"in\",\n                \"Precipitation Accumulated\",\n                \"Amount of liquid equivalent precipitation accumulated or totaled for a defined period of time, usually hourly, daily, or annually.\",\n            )\n        elif hads_var_name == \"PP\":\n            return (\n                \"precipitation_rate\",\n                \"in\",\n                \"Precipitation Rate\",\n                \"Amount of wet equivalent precipitation per unit time.\",\n            )\n        elif hads_var_name == \"US\":\n            return (\n                \"wind_speed\",\n                \"mph\",\n                \"Wind Speed\",\n                \"Magnitude of wind velocity. Wind is motion of air relative to the surface of the earth.\",\n            )\n        elif hads_var_name == \"UD\":\n            return (\n                \"wind_from_direction\",\n                \"degrees_true\",\n                \"Wind from Direction\",\n                \"Direction from which wind is blowing.  Meteorological Convention. Wind is motion of air relative to the surface of the earth.\",\n            )\n        elif hads_var_name in [\"UP\", \"UG\", \"VUP\"]:\n            return (\n                \"wind_gust\",\n                \"mph\",\n                \"Wind Gust Speed\",\n                \"Maximum instantaneous wind speed (usually no more than but not limited to 10 seconds) within a sample averaging interval. Wind is motion of air relative to the surface of the earth.\",\n            )\n        elif hads_var_name in [\"TA\", \"TA2\"]:\n            return (\n                \"air_temperature\",\n                \"f\",\n                \"Air Temperature\",\n                \"Air temperature is the bulk temperature of the air, not the surface (skin) temperature.\",\n            )\n        elif hads_var_name == \"MT\":\n            return (\"fuel_temperature\", \"f\", \"Fuel Temperature\", \"\")\n        elif hads_var_name == \"XR\":\n            return (\"relative_humidity\", \"percent\", \"Relative Humidity\", \"\")\n        elif hads_var_name == \"VB\":\n            return (\"battery_voltage\", \"voltage\", \"Battery Voltage\", \"\")\n        elif hads_var_name == \"MM\":\n            return (\"fuel_moisture\", \"percent\", \"Fuel Moisture\", \"\")\n        elif hads_var_name == \"RW\":\n            return (\"solar_radiation\", \"watt/m^2\", \"Solar Radiation\", \"\")\n        elif hads_var_name == \"RS\":\n            return (\n                \"photosynthetically_active_radiation\",\n                \"watt/m^2\",\n                \"Photosynthetically Active Radiation\",\n                \"\",\n            )\n        elif hads_var_name == \"TW\":  # TW2?\n            return (\n                \"sea_water_temperature\",\n                \"f\",\n                \"Sea Water Temperature\",\n                \"Sea water temperature is the in situ temperature of the sea water.\",\n            )\n        elif hads_var_name == \"WT\":\n            return (\n                \"turbidity\",\n                \"nephelometric turbidity units\",\n                \"Turbidity\",\n                \"\",\n            )\n        elif hads_var_name == \"WC\":\n            return (\n                \"sea_water_electrical_conductivity\",\n                \"micro mhos/cm\",\n                \"Sea Water Electrical Conductivity\",\n                \"\",\n            )\n        elif hads_var_name == \"WP\":\n            return (\n                \"sea_water_ph_reported_on_total_scale\",\n                \"std units\",\n                \"Sea Water PH reported on Total Scale\",\n                \"the measure of acidity of seawater\",\n            )\n        elif hads_var_name == \"WO\":\n            return (\"dissolved_oxygen\", \"ppm\", \"Dissolved Oxygen\", \"\")\n        elif hads_var_name == \"WX\":\n            return (\n                \"dissolved_oxygen_saturation\",\n                \"percent\",\n                \"Dissolved Oxygen Saturation\",\n                \"\",\n            )\n        elif hads_var_name == \"TD\":\n            return (\n                \"dew_point_temperature\",\n                \"f\",\n                \"Dew Point Temperature\",\n                \"the temperature at which a parcel of air reaches saturation upon being cooled at constant pressure and specific humidity.\",\n            )\n        elif hads_var_name == \"HG\":  # HG2?\n            return (\"stream_gage_height\", \"ft\", \"Stream Gage Height\", \"\")\n        elif hads_var_name == \"HP\":\n            return (\n                \"water_surface_height_above_reference_datum\",\n                \"ft\",\n                \"Water Surface Height Above Reference Datum\",\n                \"means the height of the upper surface of a body of liquid water, such as sea, lake or river, above an arbitrary reference datum.\",\n            )\n        elif hads_var_name == \"WS\":\n            return (\"salinity\", \"ppt\", \"Salinity\", \"\")\n        elif hads_var_name == \"HM\":\n            return (\"water_level\", \"ft\", \"Water Level\", \"\")\n        elif hads_var_name == \"PA\":\n            return (\"air_pressure\", \"hp\", \"Air Pressure\", \"\")\n        elif hads_var_name == \"SD\":\n            return (\"snow_depth\", \"in\", \"Snow Depth\", \"\")\n        elif hads_var_name == \"SW\":\n            return (\"snow_water_equivalent\", \"m\", \"Snow Water Equivalent\", \"\")\n        elif hads_var_name == \"TS\":\n            return (\n                \"soil_temperature\",\n                \"f\",\n                \"Soil Temperature\",\n                \"Soil temperature is the bulk temperature of the soil, not the surface (skin) temperature.\",\n            )\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing any time string.", "response": "def parse(cls, date_string):\n        \"\"\"\n            Parse any time string.  Use a custom timezone matching if\n            the original matching does not pull one out.\n        \"\"\"\n        try:\n            date = dateparser.parse(date_string)\n            if date.tzinfo is None:\n                date = dateparser.parse(date_string, tzinfos=cls.tzd)\n            return date\n        except Exception:\n            raise ValueError(\"Could not parse date string!\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef epsg_code(geojson):\n\n    if isinstance(geojson, dict):\n        if 'crs' in geojson:\n            urn = geojson['crs']['properties']['name'].split(':')\n            if 'EPSG' in urn:\n                try:\n                    return int(urn[-1])\n                except (TypeError, ValueError):\n                    return None\n\n    return None", "response": "get the espg code from the crs system"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_coordinates(coords, origin, wgs84, wrapped):\n    if isinstance(coords, list) or isinstance(coords, tuple):\n        try:\n            if isinstance(coords[0], list) or isinstance(coords[0], tuple):\n                return [convert_coordinates(list(c), origin, wgs84, wrapped) for c in coords]\n            elif isinstance(coords[0], float):\n                c = list(transform(origin, wgs84, *coords))\n                if wrapped and c[0] < -170:\n                    c[0] = c[0] + 360\n                return c\n\n        except IndexError:\n            pass\n\n    return None", "response": "Convert coordinates from one crs to another"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a given geojson to wgs84.", "response": "def to_latlon(geojson, origin_espg=None):\n    \"\"\"\n    Convert a given geojson to wgs84. The original epsg must be included insde the crs\n    tag of geojson\n    \"\"\"\n\n    if isinstance(geojson, dict):\n\n        # get epsg code:\n        if origin_espg:\n            code = origin_espg\n        else:\n            code = epsg_code(geojson)\n        if code:\n            origin = Proj(init='epsg:%s' % code)\n            wgs84 = Proj(init='epsg:4326')\n            wrapped = test_wrap_coordinates(geojson['coordinates'], origin, wgs84)\n            new_coords = convert_coordinates(geojson['coordinates'], origin, wgs84, wrapped)\n            if new_coords:\n                geojson['coordinates'] = new_coords\n            try:\n                del geojson['crs']\n            except KeyError:\n                pass\n\n    return geojson"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef camelcase_underscore(name):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "Convert camelcase names to underscore"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of all tile names from Product_Organisation element in metadata. xml", "response": "def get_tiles_list(element):\n    \"\"\"\n    Returns the list of all tile names from Product_Organisation element\n    in metadata.xml\n    \"\"\"\n\n    tiles = {}\n\n    for el in element:\n        g = (el.findall('.//Granules') or el.findall('.//Granule'))[0]\n        name = g.attrib['granuleIdentifier']\n\n        name_parts = name.split('_')\n        mgs = name_parts[-2]\n        tiles[mgs] = name\n\n    return tiles"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes the metadata. xml file of sentinel product and extracts useful keys Returns a python dict", "response": "def metadata_to_dict(metadata):\n    \"\"\" Looks at metadata.xml file of sentinel product and extract useful keys\n    Returns a python dict \"\"\"\n\n    tree = etree.parse(metadata)\n    root = tree.getroot()\n\n    meta = OrderedDict()\n\n    keys = [\n        'SPACECRAFT_NAME',\n        'PRODUCT_STOP_TIME',\n        'Cloud_Coverage_Assessment',\n        'PROCESSING_LEVEL',\n        'PRODUCT_TYPE',\n        'PROCESSING_BASELINE',\n        'SENSING_ORBIT_NUMBER',\n        'SENSING_ORBIT_DIRECTION',\n        'PRODUCT_FORMAT',\n    ]\n\n    # grab important keys from the file\n    for key in keys:\n        try:\n            meta[key.lower()] = root.findall('.//' + key)[0].text\n        except IndexError:\n            meta[key.lower()] = None\n\n    meta['product_cloud_coverage_assessment'] = float(meta.pop('cloud_coverage_assessment'))\n\n    meta['sensing_orbit_number'] = int(meta['sensing_orbit_number'])\n\n    # get tile list\n    meta['tiles'] = get_tiles_list(root.findall('.//Product_Organisation')[0])\n\n    # get available bands\n    if root.findall('.//Band_List'):\n        bands = root.findall('.//Band_List')[0]\n        meta['band_list'] = []\n        for b in bands:\n            band = b.text.replace('B', '')\n            if len(band) == 1:\n                band = 'B' + pad(band, 2)\n            else:\n                band = b.text\n            meta['band_list'].append(band)\n    else:\n        bands = root.findall('.//Spectral_Information_List')[0]\n        meta['band_list'] = []\n        for b in bands:\n            band = b.attrib['physicalBand'].replace('B', '')\n            if len(band) == 1:\n                band = 'B' + pad(band, 2)\n            else:\n                band = b.attrib['physicalBand']\n            meta['band_list'].append(band)\n\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the data and tile geometry for sentinel - 2 tiles", "response": "def get_tile_geometry(path, origin_espg, tolerance=500):\n    \"\"\" Calculate the data and tile geometry for sentinel-2 tiles \"\"\"\n\n    with rasterio.open(path) as src:\n\n        # Get tile geometry\n        b = src.bounds\n        tile_shape = Polygon([(b[0], b[1]), (b[2], b[1]), (b[2], b[3]), (b[0], b[3]), (b[0], b[1])])\n        tile_geojson = mapping(tile_shape)\n\n        # read first band of the image\n        image = src.read(1)\n\n        # create a mask of zero values\n        mask = image == 0.\n\n        # generate shapes of the mask\n        novalue_shape = shapes(image, mask=mask, transform=src.affine)\n\n        # generate polygons using shapely\n        novalue_shape = [Polygon(s['coordinates'][0]) for (s, v) in novalue_shape]\n\n        if novalue_shape:\n\n            # Make sure polygons are united\n            # also simplify the resulting polygon\n            union = cascaded_union(novalue_shape)\n\n            # generates a geojson\n            data_shape = tile_shape.difference(union)\n\n            # If there are multipolygons, select the largest one\n            if data_shape.geom_type == 'MultiPolygon':\n                areas = {p.area: i for i, p in enumerate(data_shape)}\n                largest = max(areas.keys())\n                data_shape = data_shape[areas[largest]]\n\n            # if the polygon has interior rings, remove them\n            if list(data_shape.interiors):\n                data_shape = Polygon(data_shape.exterior.coords)\n\n            data_shape = data_shape.simplify(tolerance, preserve_topology=False)\n            data_geojson = mapping(data_shape)\n\n        else:\n            data_geojson = tile_geojson\n\n        # convert cooridnates to degrees\n        return (to_latlon(tile_geojson, origin_espg), to_latlon(data_geojson, origin_espg))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate metadata for a given tile", "response": "def tile_metadata(tile, product, geometry_check=None):\n    \"\"\" Generate metadata for a given tile\n\n    - geometry_check is a function the determines whether to calculate the geometry by downloading\n    B01 and override provided geometry in tilejson. The meta object is passed to this function.\n    The function return a True or False response.\n    \"\"\"\n\n    grid = 'T{0}{1}{2}'.format(pad(tile['utmZone'], 2), tile['latitudeBand'], tile['gridSquare'])\n\n    meta = OrderedDict({\n        'tile_name': product['tiles'][grid]\n    })\n\n    logger.info('%s Processing tile %s' % (threading.current_thread().name, tile['path']))\n\n    meta['date'] = tile['timestamp'].split('T')[0]\n\n    meta['thumbnail'] = '{1}/{0}/preview.jp2'.format(tile['path'], s3_url)\n\n    # remove unnecessary keys\n    product.pop('tiles')\n    tile.pop('datastrip')\n    bands = product.pop('band_list')\n\n    for k, v in iteritems(tile):\n        meta[camelcase_underscore(k)] = v\n\n    meta.update(product)\n\n    # construct download links\n    links = ['{2}/{0}/{1}.jp2'.format(meta['path'], b, s3_url) for b in bands]\n\n    meta['download_links'] = {\n        'aws_s3': links\n    }\n\n    meta['original_tile_meta'] = '{0}/{1}/tileInfo.json'.format(s3_url, meta['path'])\n\n    def internal_latlon(meta):\n        keys = ['tile_origin', 'tile_geometry', 'tile_data_geometry']\n        for key in keys:\n            if key in meta:\n                meta[key] = to_latlon(meta[key])\n        return meta\n\n    # change coordinates to wsg4 degrees\n    if geometry_check:\n        if geometry_check(meta):\n            meta = get_tile_geometry_from_s3(meta)\n        else:\n            meta = internal_latlon(meta)\n    else:\n        meta = internal_latlon(meta)\n\n    # rename path key to aws_path\n    meta['aws_path'] = meta.pop('path')\n\n    return meta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_markov(argv, stdin):\n    if len(argv) > 3:\n        with open(argv[3]) as input_file:\n            return Algorithm(input_file.readlines())\n    else:\n        return Algorithm(stdin.readlines())", "response": "Load and return markov algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_turing(argv, stdin):\n    if len(argv) > 3:\n        with open(argv[3]) as input_file:\n            return build_machine(input_file.readlines())\n    else:\n        return build_machine(stdin.readlines())", "response": "Load and return turing machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main(argv, stdin, stdout):\n    if len(argv) > 1 and argv[1:3] == [\"compile\", \"markov\"]:\n        algo = load_markov(argv, stdin)\n        print(algo.compile(), file=stdout)\n    elif len(argv) == 4 and argv[1:3] == [\"run\", \"markov\"]:\n        algo = load_markov(argv, stdin)\n        for line in stdin:\n            print(algo.execute(''.join(line.split())), file=stdout)\n\n    elif len(argv) > 1 and argv[1:3] == [\"compile\", \"turing\"]:\n        machine = load_turing(argv, stdin)\n        print(machine.compile(), file=stdout)\n    elif len(argv) == 4 and argv[1:3] == [\"run\", \"turing\"]:\n        machine = load_turing(argv, stdin)\n        for line in stdin:\n            print(machine.execute(line), file=stdout)\n\n    elif len(argv) == 2 and argv[1] == \"test\":\n        path = os.path.abspath(os.path.dirname(__file__))\n        argv[1] = path\n        pytest.main()\n    elif len(argv) == 2 and argv[1] == \"version\":\n        print(\"TuringMarkov\", VERSION, file=stdout)\n\n    else:\n        print(USAGE, file=stdout)\n        if not (len(argv) == 2 and argv[1] == \"help\"):\n            exit(1)", "response": "Execute when user call turingmarkov."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef retrieve(run_id, det_id, outfile=None):\n    try:\n        det_id = int(det_id)\n    except ValueError:\n        pass\n    path = irods_filepath(det_id, run_id)\n    suffix = '' if outfile is None else outfile\n    os.system(\"iget -Pv {0} {1}\".format(path, suffix))", "response": "Retrieve a run from iRODS for a given detector ID"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef detectors(regex=None, sep='\\t', temporary=False):\n    db = DBManager(temporary=temporary)\n    dt = db.detectors\n    if regex is not None:\n        try:\n            re.compile(regex)\n        except re.error:\n            log.error(\"Invalid regex!\")\n            return\n        dt = dt[dt['OID'].str.contains(regex) | dt['CITY'].str.contains(regex)]\n    dt.to_csv(sys.stdout, sep=sep)", "response": "Print the detectors table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rundetsn(run_id, detector=\"ARCA\", temporary=False):\n    db = DBManager(temporary=temporary)\n    dts = db.detectors\n    for det_id in dts[dts.OID.str.contains(detector)].SERIALNUMBER:\n        if run_id in db.run_table(det_id).RUN.values:\n            print(det_id)\n            return", "response": "Print the detector serial number for a given run of ARCA or ORCA"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mse(mean, estimator):\r\n    return np.mean((np.asarray(estimator) - np.asarray(mean)) ** 2, axis=0)", "response": "Returns the Mean Squared Error of a single record in a flat numpy ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the sum of squared errors of a given time series", "response": "def sse(mean, estimator):\r\n    \"\"\"\r\n    Description:\r\n        Calculates the Sum of Squared Errors (SSE) of\r\n        an estimation on flat numpy ndarrays.\r\n    Parameters:\r\n        mean:      actual value (numpy ndarray)\r\n        estimator: estimated value of the mean (numpy ndarray)\r\n    \"\"\"\r\n    return np.sum((np.asarray(estimator) - np.asarray(mean)) ** 2, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the mean squared error of an estimator over the inverse permutations of the first model in the model - 2 Plackett - Luce models.", "response": "def mix2PL_mse(mean, estimator, m):\r\n    \"\"\"\r\n    Description:\r\n        Calculates the Mean Squared Error (MSE) of an\r\n        estimator of a mixture of 2 Plackett-Luce models,\r\n        on flat numpy ndarrays, where the first element is\r\n        the mixing proportion of the first model defined\r\n        as the minimum MSE over the inverse permutations of\r\n        the estimator.\r\n    Parameters:\r\n        mean:      actual value (numpy ndarray)\r\n        estimator: estimated value of the mean (numpy ndarray)\r\n        m:         number of alternatives in each of the two models\r\n    \"\"\"\r\n    mse1 = mse(mean, estimator)\r\n    estimator = np.hstack((1 - estimator[0], estimator[m+1:], estimator[1:m+1]))\r\n    mse2 = mse(mean, estimator)\r\n    return min(mse1, mse2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mix2PL_sse(mean, estimator, m):\r\n    sse1 = sse(mean, estimator)\r\n    estimator = np.hstack((1 - estimator[0], estimator[m+1:], estimator[1:m+1]))\r\n    sse2 = sse(mean, estimator)\r\n    return min(sse1, sse2)", "response": "Returns the sum of squared errors of an estimator over the inverse permutations of the first model in the estimator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the minimum WSSE over the inverse permutations of the estimator of the 2 Plackett - Luce models.", "response": "def mix2PL_wsse(mean, estimator, m):\r\n    \"\"\"\r\n    Description:\r\n        Calculates the weighted Sum of Squared Errors (WSSE)\r\n        of an estimator of a mixture of 2 Plackett-Luce models,\r\n        on flat numpy ndarrays, where the first element is\r\n        the mixing proportion of the first model defined\r\n        as the minimum WSSE over the inverse permutations of\r\n        the estimator.\r\n    Parameters:\r\n        mean:      actual value (numpy ndarray)\r\n        estimator: estimated value of the mean (numpy ndarray)\r\n        m:         number of alternatives in each of the two models\r\n    \"\"\"\r\n    def wsse(mean1, est1, m1):\r\n        return (((est1[0] - mean1[0])**2) +\r\n                (mean1[0]*np.sum((np.asarray(est1[1:m1+1]) - np.asarray(mean1[1:m1+1]))**2)) +\r\n                ((1 - mean1[0]) * np.sum((np.asarray(est1[m1+1:]) - np.asarray(mean1[m1+1:]))**2))\r\n               )\r\n    wsse1 = wsse(mean, estimator, m)\r\n    estimator = np.hstack((1 - estimator[0], estimator[m+1:], estimator[1:m+1]))\r\n    wsse2 = wsse(mean, estimator, m)\r\n    return min(wsse1, wsse2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_product_metadata_path(product_name):\n\n    string_date = product_name.split('_')[-1]\n    date = datetime.datetime.strptime(string_date, '%Y%m%dT%H%M%S')\n    path = 'products/{0}/{1}/{2}/{3}'.format(date.year, date.month, date.day, product_name)\n\n    return {\n        product_name: {\n            'metadata': '{0}/{1}'.format(path, 'metadata.xml'),\n            'tiles': get_tile_metadata_path('{0}/{1}'.format(path, 'productInfo.json'))\n        }\n    }", "response": "gets a single product metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_products_metadata_path(year, month, day):\n\n    products = {}\n    path = 'products/{0}/{1}/{2}/'.format(year, month, day)\n    for key in bucket.objects.filter(Prefix=path):\n        product_path = key.key.replace(path, '').split('/')\n        name = product_path[0]\n        if name not in products:\n            products[name] = {}\n\n        if product_path[1] == 'metadata.xml':\n            products[name]['metadata'] = key.key\n\n        if product_path[1] == 'productInfo.json':\n            products[name]['tiles'] = get_tile_metadata_path(key.key)\n\n    return products", "response": "Get paths to multiple products metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef metadata(\n        self, output_format=None, feature_name_callback=None, **kwargs\n    ):\n        \"\"\"\n        Gets SensorML objects for all procedures in your filtered features.\n\n        You should override the default output_format for servers that do not\n        respond properly.\n        \"\"\"\n        callback = feature_name_callback or str\n        if output_format is None:\n            output_format = (\n                'text/xml; subtype=\"sensorML/1.0.1/profiles/ioos_sos/1.0\"'\n            )\n\n        responses = []\n        if self.features is not None:\n            for feature in self.features:\n                ds_kwargs = kwargs.copy()\n                ds_kwargs.update(\n                    {\n                        \"outputFormat\": output_format,\n                        \"procedure\": callback(feature),\n                    }\n                )\n\n                responses.append(\n                    SensorML(self.server.describe_sensor(**ds_kwargs))\n                )\n\n        return responses", "response": "Gets SensorML objects for all procedures in your filtered features."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef metadata_plus_exceptions(\n        self, output_format=None, feature_name_callback=None, **kwargs\n    ):\n        \"\"\"\n        Gets SensorML objects for all procedures in your filtered features.\n\n        Return two dictionaries for service responses keyed by 'feature':\n            responses: values are SOS DescribeSensor response text\n            response_failures: values are exception text content furnished from ServiceException, ExceptionReport\n\n        You should override the default output_format for servers that do not\n        respond properly.\n        \"\"\"\n        callback = feature_name_callback or str\n        if output_format is None:\n            output_format = (\n                'text/xml; subtype=\"sensorML/1.0.1/profiles/ioos_sos/1.0\"'\n            )\n\n        responses = {}\n        response_failures = {}\n        if self.features is not None:\n            for feature in self.features:\n                ds_kwargs = kwargs.copy()\n                ds_kwargs.update(\n                    {\n                        \"outputFormat\": output_format,\n                        \"procedure\": callback(feature),\n                    }\n                )\n                try:\n                    responses[feature] = SensorML(\n                        self.server.describe_sensor(**ds_kwargs)\n                    )\n                except (ServiceException, ExceptionReport) as e:\n                    response_failures[feature] = str(e)\n\n        return (responses, response_failures)", "response": "Returns two dictionaries for all services and their associated responses and responses with exception text content furnished from ServiceException and ExceptionReport objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the term that is used in an unconstrained optimization formulation.", "response": "def uncons_term(params, c):\n    \"\"\"\n    Description:\n        Computes an additional value for the objective function value\n        when used in an unconstrained optimization formulation.\n    Parameters:\n        params: all parameters for the Plackett-Luce mixture model (numpy ndarray)\n        c:      constant multiplier scaling factor of the returned term\n    \"\"\"\n    return (c * ((np.sum(params[1:5]) - 1)**2)) + (c * ((np.sum(params[5:]) - 1)**2))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef top3_reduced(params, moments):\n    params = np.asarray(params)\n    alpha = params[0]\n    a = params[1:5]\n    b = params[5:]\n    p = np.asarray(moments)\n    p1 = alpha*a+(1-alpha)*b-p[:4]\n    p21 = alpha*a[0]*a[2:]/(1-a[0])+(1-alpha)*b[0]*b[2:]/(1-b[0])-p[4:6]\n    p22 = alpha*a[1]*np.hstack((a[0],a[3]))/(1-a[1])+(1-alpha)*b[1]*np.hstack((b[0],b[3]))/(1-b[1])-p[6:8]\n    p23 = alpha*a[2]*a[:2]/(1-a[2])+(1-alpha)*b[2]*b[:2]/(1-b[2])-p[8:10]\n    p24 = alpha*a[3]*a[1:3]/(1-a[3])+(1-alpha)*b[3]*b[1:3]/(1-b[3])-p[10:12]\n    p3 = np.array([\n        alpha*a[0]*a[2]*a[3]/(1-a[2])/(a[0]+a[1])+(1-alpha)*b[0]*b[2]*b[3]/(1-b[2])/(b[0]+b[1])-p[12],\n        alpha*a[0]*a[1]*a[3]/(1-a[3])/(a[1]+a[2])+(1-alpha)*b[0]*b[1]*b[3]/(1-b[3])/(b[1]+b[2])-p[13],\n        alpha*a[0]*a[1]*a[2]/(1-a[0])/(a[3]+a[2])+(1-alpha)*b[0]*b[1]*b[2]/(1-b[0])/(b[3]+b[2])-p[14],\n        alpha*a[2]*a[1]*a[3]/(1-a[1])/(a[0]+a[3])+(1-alpha)*b[2]*b[1]*b[3]/(1-b[1])/(b[0]+b[3])-p[15]\n        ])\n    allp = np.concatenate((p1,p21,p22,p23,p24,p3))\n    return np.sum(allp**2)", "response": "This function calculates the top 3 reduced objective function for the given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef top3_full(params, moments):\n    #variables\n    params = np.asarray(params) #convert numpy matrix to list\n    alpha = params[0] #first parameter is the alpha value\n    half = int((len(params) - 1) / 2) #assuming 2 mixtures\n    a = params[1:half + 1] #first mixture\n    b = params[half + 1:] #second mixture\n    p = np.asarray(moments) #convert numpy matrix to list\n    p1 = list(alpha*a+(1-alpha)*b-p[:half]) #new list with one element\n    p2 = [] #new empty list\n\n    #iterate through each \n    for i in range(0, half):\n        #alpha times the score of a given point in mixture one, mutiplied by\n            #each of the other scores, divided by the sum of the other values\n        #Each of these top two plackett-luce values is added to the same values\n            #from the other mixture, then the moment value is subtracted for those\n            #top two from the vote\n        p1 += list(alpha*a[i]*np.hstack((a[:i],a[i + 1:]))/(1-a[i])\n            +(1-alpha)*b[i]*np.hstack((b[:i],b[i + 1:]))/(1-b[i])\n            -p[half + (half - 1) * i:half + (half - 1) * (i + 1)])\n\n    #iterate through each value in each mixture\n    for i in range(0, half):\n        #begin with alpha values for given mixture\n        num_a = alpha\n        num_b = 1 - alpha\n\n        #iterate again\n        for j in range(0, half):\n            #this eventually multiplies all values to its alpha\n            num_a *= a[j]\n            num_b *= b[j]\n            #divide by the sum of other values\n            if j > i:\n                num_a /= np.sum(np.concatenate((a[j:], a[:i])))\n                num_b /= np.sum(np.concatenate((b[j:], b[:i])))\n            elif j < i:\n                num_a /= np.sum(a[j:i])\n                num_b /= np.sum(b[j:i])\n        p2.append(num_a + num_b - p[half + (half * (half - 1)) + i])\n    p3 = np.array(p2)\n    #create one array\n    allp = np.concatenate((p1,p3))\n    return np.sum(allp**2)", "response": "This function is used to compute the top 3 of the Plackett - Luce mixture model for a given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start(backdate=None):\n    if f.s.cum:\n        raise StartError(\"Already have stamps, can't start again (must reset).\")\n    if f.t.subdvsn_awaiting or f.t.par_subdvsn_awaiting:\n        raise StartError(\"Already have subdivisions, can't start again (must reset).\")\n    if f.t.stopped:\n        raise StoppedError(\"Timer already stopped (must open new or reset).\")\n    t = timer()\n    if backdate is None:\n        t_start = t\n    else:\n        if f.t is f.root:\n            raise BackdateError(\"Cannot backdate start of root timer.\")\n        if not isinstance(backdate, float):\n            raise TypeError(\"Backdate must be type float.\")\n        if backdate > t:\n            raise BackdateError(\"Cannot backdate to future time.\")\n        if backdate < f.tm1.last_t:\n            raise BackdateError(\"Cannot backdate start to time previous to latest stamp in parent timer.\")\n        t_start = backdate\n    f.t.paused = False\n    f.t.tmp_total = 0.  # (In case previously paused.)\n    f.t.start_t = t_start\n    f.t.last_t = t_start\n    return t", "response": "Start a new subdivision."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stamp(name, backdate=None,\n          unique=None, keep_subdivisions=None, quick_print=None,\n          un=None, ks=None, qp=None):\n    \"\"\"\n    Mark the end of a timing interval.\n\n    Notes:\n        If keeping subdivisions, each subdivision currently awaiting\n        assignment to a stamp (i.e. ended since the last stamp in this level)\n        will be assigned to this one.  Otherwise, all awaiting ones will be\n        discarded after aggregating their self times into the current timer.\n\n        If both long- and short-form are present, they are OR'ed together.  If\n        neither are present, the current global default is used.\n\n        Backdating: record a stamp as if it happened at an earlier time.\n        Backdate time must be in the past but more recent than the latest stamp.\n        (This can be useful for parallel applications, wherein a sub- process\n        can return times of interest to the master process.)\n\n    Warning:\n        When backdating, awaiting subdivisions will be assigned as normal, with\n        no additional checks for validity.\n\n    Args:\n        name (any): The identifier for this interval, processed through str()\n        backdate (float, optional): time to use for stamp instead of current\n        unique (bool, optional): enforce uniqueness\n        keep_subdivisions (bool, optional): keep awaiting subdivisions\n        quick_print (bool, optional): print elapsed interval time\n        un (bool, optional): short-form for unique\n        ks (bool, optional): short-form for keep_subdivisions\n        qp (bool, optional): short-form for quick_print\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        BackdateError: If the given backdate time is out of range.\n        PausedError: If the timer is paused.\n        StoppedError: If the timer is stopped.\n        TypeError: If the given backdate value is not type float.\n    \"\"\"\n    t = timer()\n    if f.t.stopped:\n        raise StoppedError(\"Cannot stamp stopped timer.\")\n    if f.t.paused:\n        raise PausedError(\"Cannot stamp paused timer.\")\n    if backdate is None:\n        t_stamp = t\n    else:\n        if not isinstance(backdate, float):\n            raise TypeError(\"Backdate must be type float.\")\n        if backdate > t:\n            raise BackdateError(\"Cannot backdate to future time.\")\n        if backdate < f.t.last_t:\n            raise BackdateError(\"Cannot backdate to time earlier than last stamp.\")\n        t_stamp = backdate\n    elapsed = t_stamp - f.t.last_t\n    # Logic: default unless either arg used.  if both args used, 'or' them.\n    unique = SET['UN'] if (unique is None and un is None) else bool(unique or un)  # bool(None) becomes False\n    keep_subdivisions = SET['KS'] if (keep_subdivisions is None and ks is None) else bool(keep_subdivisions or ks)\n    quick_print = SET['QP'] if (quick_print is None and qp is None) else bool(quick_print or qp)\n    _stamp(name, elapsed, unique, keep_subdivisions, quick_print)\n    tmp_self = timer() - t\n    f.t.self_cut += tmp_self\n    f.t.last_t = t_stamp + tmp_self\n    return t", "response": "This function is used to mark a new time for a given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stop(name=None, backdate=None,\n         unique=None, keep_subdivisions=None, quick_print=None,\n         un=None, ks=None, qp=None):\n    \"\"\"\n    Mark the end of timing.  Optionally performs a stamp, hence accepts the\n    same arguments.\n\n    Notes:\n        If keeping subdivisions and not calling a stamp, any awaiting subdivisions\n        will be assigned to a special 'UNASSIGNED' position to indicate that they\n        are not properly accounted for in the hierarchy (these can happen at\n        different places and may be combined inadvertently).\n\n        Backdating: For subdivisions only.  Backdate time must be in the past\n        but more recent than the latest stamp.\n\n    Args:\n        name (any, optional): If used, passed to a call to stamp()\n        backdate (float, optional): time to use for stop instead of current\n        unique (bool, optional): see stamp()\n        keep_subdivisions (bool, optional): keep awaiting subdivisions\n        quick_print (bool, optional): boolean, print total time\n        un (bool, optional): see stamp()\n        ks (bool, optional): see stamp()\n        qp (bool, optional): see stamp()\n\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        BackdateError: If given backdate is out of range, or if used in root timer.\n        PausedError: If attempting stamp in paused timer.\n        StoppedError: If timer already stopped.\n        TypeError: If given backdate value is not type float.\n    \"\"\"\n    t = timer()\n    if f.t.stopped:\n        raise StoppedError(\"Timer already stopped.\")\n    if backdate is None:\n        t_stop = t\n    else:\n        if f.t is f.root:\n            raise BackdateError(\"Cannot backdate stop of root timer.\")\n        if not isinstance(backdate, float):\n            raise TypeError(\"Backdate must be type float.\")\n        if backdate > t:\n            raise BackdateError(\"Cannot backdate to future time.\")\n        if backdate < f.t.last_t:\n            raise BackdateError(\"Cannot backdate to time earlier than last stamp.\")\n        t_stop = backdate\n    unique = SET['UN'] if (unique is None and un is None) else bool(unique or un)  # bool(None) becomes False\n    keep_subdivisions = SET['KS'] if (keep_subdivisions is None and ks is None) else bool(keep_subdivisions or ks)\n    quick_print = SET['QP'] if (quick_print is None and qp is None) else bool(quick_print or qp)\n    if name is not None:\n        if f.t.paused:\n            raise PausedError(\"Cannot stamp paused timer.\")\n        elapsed = t_stop - f.t.last_t\n        _stamp(name, elapsed, unique, keep_subdivisions, quick_print)\n    else:\n        times_priv.assign_subdivisions(UNASGN, keep_subdivisions)\n    for s in f.t.rgstr_stamps:\n        if s not in f.s.cum:\n            f.s.cum[s] = 0.\n            f.s.order.append(s)\n    if not f.t.paused:\n        f.t.tmp_total += t_stop - f.t.start_t\n    f.t.tmp_total -= f.t.self_cut\n    f.t.self_cut += timer() - t  # AFTER subtraction from tmp_total, before dump\n    times_priv.dump_times()\n    f.t.stopped = True\n    if quick_print:\n        print(\"({}) Total: {:.4f}\".format(f.t.name, f.r.total))\n    return t", "response": "Mark the end of timing.  Optionally performs a stamp, hence accepts the\n    same arguments.\n\n    Notes:\n        If keeping subdivisions and not calling a stamp, any awaiting subdivisions\n        will be assigned to a special 'UNASSIGNED' position to indicate that they\n        are not properly accounted for in the hierarchy (these can happen at\n        different places and may be combined inadvertently).\n\n        Backdating: For subdivisions only.  Backdate time must be in the past\n        but more recent than the latest stamp.\n\n    Args:\n        name (any, optional): If used, passed to a call to stamp()\n        backdate (float, optional): time to use for stop instead of current\n        unique (bool, optional): see stamp()\n        keep_subdivisions (bool, optional): keep awaiting subdivisions\n        quick_print (bool, optional): boolean, print total time\n        un (bool, optional): see stamp()\n        ks (bool, optional): see stamp()\n        qp (bool, optional): see stamp()\n\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        BackdateError: If given backdate is out of range, or if used in root timer.\n        PausedError: If attempting stamp in paused timer.\n        StoppedError: If timer already stopped.\n        TypeError: If given backdate value is not type float."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npause the timer preventing subsequent time from accumulating in the the total.", "response": "def pause():\n    \"\"\"\n    Pause the timer, preventing subsequent time from accumulating in the\n    total.  Renders the timer inactive, disabling other timing commands.\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        PausedError: If timer already paused.\n        StoppedError: If timer already stopped.\n    \"\"\"\n    t = timer()\n    if f.t.stopped:\n        raise StoppedError(\"Cannot pause stopped timer.\")\n    if f.t.paused:\n        raise PausedError(\"Timer already paused.\")\n    f.t.paused = True\n    f.t.tmp_total += t - f.t.start_t\n    f.t.start_t = None\n    f.t.last_t = None\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resume():\n    t = timer()\n    if f.t.stopped:\n        raise StoppedError(\"Cannot resume stopped timer.\")\n    if not f.t.paused:\n        raise PausedError(\"Cannot resume timer that is not paused.\")\n    f.t.paused = False\n    f.t.start_t = t\n    f.t.last_t = t\n    return t", "response": "Resume a paused timer re - activating it."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nblanks the current time of a new interval.", "response": "def blank_stamp(name=None, backdate=None,\n                unique=None, keep_subdivisions=False, quick_print=None,\n                un=None, ks=False, qp=None):\n    \"\"\"\n    Mark the beginning of a new interval, but the elapsed time\n    of the previous interval is discarded.  Intentionally the same signature\n    as stamp().\n\n    Notes:\n        The default for keep_subdivisions is False (does not refer to an\n        adjustable global setting), meaning that any subdivisons awaiting would be\n        discarded after having their self times aggregated into this timer.  If\n        this is set to True, subdivisions are put in the 'UNASSIGNED' position,\n        indicating they are not properly accounted for in the hierarchy.\n\n    Args:\n        name (any, optional): Inactive.\n        backdate (any, optional): Inactive.\n        unique (any, optional): Inactive.\n        keep_subdivisions (bool, optional): Keep subdivisions awaiting\n        quick_print (any, optional): Inactive.\n        un (any, optional): Inactive.\n        ks (bool, optional): see stamp().\n        qp (any, optional): Inactive.\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        StoppedError: If timer is already stopped.\n    \"\"\"\n    t = timer()\n    if f.t.stopped:\n        raise StoppedError(\"Cannot blank_stamp stopped timer.\")\n    keep_subdivisions = (keep_subdivisions or ks)\n    times_priv.assign_subdivisions(UNASGN, keep_subdivisions)\n    f.t.last_t = timer()\n    f.t.self_cut += f.t.last_t - t\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nresetting the timer at the current level in the hierarchy.", "response": "def reset():\n    \"\"\"\n    Reset the timer at the current level in the hierarchy (i.e. might or\n    might not be the root).\n\n    Notes:\n        Erases timing data but preserves relationship to the hierarchy.  If the\n        current timer level was not previously stopped, any timing data from this\n        timer (including subdivisions) will be discarded and not added to the next\n        higher level in the data structure.  If the current timer was previously\n        stopped, then its data has already been pushed into the next higher level.\n\n    Returns:\n        float: The current time.\n\n    Raises:\n        LoopError: If in a timed loop.\n    \"\"\"\n    if f.t.in_loop:\n        raise LoopError(\"Cannot reset a timer while it is in timed loop.\")\n    f.t.reset()\n    f.refresh_shortcuts()\n    return f.t.start_t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap(func, subdivide=True, name=None, rgstr_stamps=None, save_itrs=None):\n    subdivision = bool(subdivide)\n    if subdivision:\n        name = func.__name__ if name is None else str(name)\n        rgstr_stamps = sanitize_rgstr_stamps(rgstr_stamps)\n        wrap_save_itrs = save_itrs\n\n        def gtimer_wrapped(*args, **kwargs):\n            save_itrs = SET['SI'] if wrap_save_itrs is None else wrap_save_itrs\n            _auto_subdivide(name, rgstr_stamps, save_itrs=save_itrs)\n            result = func(*args, **kwargs)\n            _end_auto_subdivision()\n            return result\n    else:\n        def gtimer_wrapped(*args, **kwargs):\n            return func(*args, **kwargs)\n\n    return gtimer_wrapped", "response": "Decorator for creating a new function or method with timing hierarchy management."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subdivide(name, rgstr_stamps=None, save_itrs=SET['SI']):\n    _auto_subdivide(name, rgstr_stamps, save_itrs)\n    f.t.is_user_subdvsn = True", "response": "Induce a new subdivision of a timer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef end_subdivision():\n    if not f.t.is_user_subdvsn:\n        raise GTimerError('Attempted to end a subdivision not started by user.')\n    if f.t.in_loop:\n        raise LoopError('Cannot close a timer while it is in timed loop.')\n    _close_subdivision()", "response": "End a user - induced timing subdivision."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenames the root timer.", "response": "def rename_root(name):\n    \"\"\"\n    Rename the root timer (regardless of current timing level).\n\n    Args:\n        name (any): Identifier, passed through str()\n\n    Returns:\n        str: Implemented identifier.\n    \"\"\"\n    name = str(name)\n    f.root.name = name\n    f.root.times.name = name\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_save_itrs_root(setting):\n    setting = bool(setting)\n    f.root.times.save_itrs = setting\n    return setting", "response": "Adjust the root timer save_itrs setting such as for use in a parallel subdivision."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister stamps with the root timer.", "response": "def rgstr_stamps_root(rgstr_stamps):\n    \"\"\"\n    Register stamps with the root timer (see subdivision()).\n\n    Args:\n        rgstr_stamps (list, tuple): Collection of identifiers, passed through\n            set(), then each is passed through str().\n\n    Returns:\n        list: Implemented registered stamp collection.\n    \"\"\"\n    rgstr_stamps = sanitize_rgstr_stamps(rgstr_stamps)\n    f.root.rgstr_stamps = rgstr_stamps\n    return rgstr_stamps"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_active_lineage():\n    lin_str = ''\n    for active_timer in f.timer_stack:\n        lin_str += \"{}-->\".format(active_timer.name)\n    try:\n        return lin_str[:-3]\n    except IndexError:\n        pass", "response": "Query the lineage of the current timer level."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_pl_dataset(n, m, outfile, useDirichlet):\r\n\r\n    gamma, votes = generate_pl_dataset(n, m, useDirichlet)\r\n    outfile.write(str(len(gamma)) + ',' + str(len(votes)) + '\\n')\r\n    outfile.write(','.join(map(str, gamma)) + '\\n')\r\n    for vote in votes:\r\n        outfile.write(','.join(map(str, vote)) + '\\n')\r\n    return (gamma, votes)", "response": "Generate a Plackett - Luce dataset and save it to disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_pl_dataset(n, m, useDirichlet=True):\r\n    gamma = None\r\n    if useDirichlet:\r\n        gamma = np.random.dirichlet(np.ones(m))\r\n    else:\r\n        gamma = np.random.rand(m)\r\n        gamma /= np.sum(gamma) # normalize sum to 1.0 (not needed for Dirichlet)\r\n    votes = []\r\n    for i in range(n): # generate vote for every agent\r\n        votes.append(draw_pl_vote(m, gamma))\r\n    return (gamma, votes)", "response": "Generate a Plackett - Luce dataset and return the parameters and votes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_pl_dataset(infile):\r\n\r\n    m, n = [int(i) for i in infile.readline().split(',')]\r\n    gamma = np.array([float(f) for f in infile.readline().split(',')])\r\n    if len(gamma) != m:\r\n        infile.close()\r\n        raise ValueError(\"malformed file: len(gamma) != m\")\r\n    votes = []\r\n    i = 0\r\n    for line in infile:\r\n        vote = [int(v) for v in line.split(',')]\r\n        if len(vote) != m:\r\n            infile.close()\r\n            raise ValueError(\"malformed file: len(vote) != m\")\r\n        votes.append(vote)\r\n        i += 1\r\n    infile.close()\r\n    if i != n:\r\n        raise ValueError(\"malformed file: number of votes != n\")\r\n    return (gamma, np.array(votes))", "response": "Reads from disk a Plackett - Luce dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of candidates that are in the Plackett - Luce model.", "response": "def draw_pl_vote(m, gamma):\r\n    \"\"\"\r\n    Description:\r\n        Generate a Plackett-Luce vote given the model parameters.\r\n    Parameters:\r\n        m:     number of alternatives\r\n        gamma: parameters of the Plackett-Luce model\r\n    \"\"\"\r\n\r\n    localgamma = np.copy(gamma) # work on a copy of gamma\r\n    localalts = np.arange(m) # enumeration of the candidates\r\n    vote = []\r\n    for j in range(m): # generate position in vote for every alternative\r\n\r\n        # transform local gamma into intervals up to 1.0\r\n        localgammaintervals = np.copy(localgamma)\r\n        prev = 0.0\r\n        for k in range(len(localgammaintervals)):\r\n            localgammaintervals[k] += prev\r\n            prev = localgammaintervals[k]\r\n\r\n        selection = np.random.random() # pick random number\r\n\r\n        # selection will fall into a gamma interval\r\n        for l in range(len(localgammaintervals)): # determine position\r\n            if selection <= localgammaintervals[l]:\r\n                vote.append(localalts[l])\r\n                localgamma = np.delete(localgamma, l) # remove that gamma\r\n                localalts = np.delete(localalts, l) # remove the alternative\r\n                localgamma /= np.sum(localgamma) # renormalize\r\n                break\r\n    return vote"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read_mix2pl_dataset(infile, numVotes=None):\r\n    m, n = [int(i) for i in infile.readline().split(',')]\r\n    if numVotes is not None and n < numVotes:\r\n        raise ValueError(\"invalid number of votes to read: exceeds file amount\")\r\n    params = np.array([float(f) for f in infile.readline().split(',')])\r\n    if len(params) != (2*m + 1):\r\n        infile.close()\r\n        raise ValueError(\"malformed file: len(params) != 2*m + 1\")\r\n    votes = []\r\n    i = 0\r\n    for line in infile:\r\n        if i > (numVotes - 1):\r\n            break\r\n        vote = [int(v) for v in line.split(',')]\r\n        if len(vote) != m:\r\n            infile.close()\r\n            raise ValueError(\"malformed file: len(vote) != m\")\r\n        votes.append(vote)\r\n        i += 1\r\n    infile.close()\r\n    return (params, np.array(votes))", "response": "Reads a Mixture of 2 Plackett - Luce models dataset from disk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_mix2pl_dataset(n, m, useDirichlet=True):\r\n\r\n    alpha = np.random.rand()\r\n    gamma1 = None\r\n    gamma2 = None\r\n\r\n    if useDirichlet:\r\n        gamma1 = np.random.dirichlet(np.ones(m))\r\n        gamma2 = np.random.dirichlet(np.ones(m))\r\n    else:\r\n        gamma1 = np.random.rand(m)\r\n        gamma1 /= np.sum(gamma1) # normalize sum to 1.0 (not needed for Dirichlet)\r\n        gamma2 = np.random.rand(m)\r\n        gamma2 /= np.sum(gamma1)\r\n\r\n    votes = []\r\n\r\n    for i in range(n):\r\n        vote = None\r\n        draw = np.random.rand()\r\n        if draw <= alpha:\r\n            vote = draw_pl_vote(m, gamma1)\r\n        else: # draw > alpha\r\n            vote = draw_pl_vote(m, gamma2)\r\n        votes.append(vote)\r\n    params = np.hstack((alpha, gamma1, gamma2))\r\n    return (params, votes)", "response": "Generate a mixture of 2 Plackett - Luce models dataset\r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collapse_times():\n    orig_ts = f.timer_stack\n    orig_ls = f.loop_stack\n    copy_ts = _copy_timer_stack()\n    copy_ls = copy.deepcopy(f.loop_stack)\n    f.timer_stack = copy_ts\n    f.loop_stack = copy_ls\n    f.refresh_shortcuts()\n    while (len(f.timer_stack) > 1) or f.t.in_loop:\n        _collapse_subdivision()\n    timer_pub.stop()\n    collapsed_times = f.r\n    f.timer_stack = orig_ts  # (loops throw error if not same object!)\n    f.loop_stack = orig_ls\n    f.refresh_shortcuts()\n    return collapsed_times", "response": "Collapse all times in the current hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a specific plate from the database.", "response": "def delete_plate(self, plate_id, delete_meta_data=False):\n        \"\"\"\n        Delete a plate from the database\n        \n        :param plate_id: The plate id\n        :param delete_meta_data: Optionally delete all meta data associated with this plate as well\n        :return: None\n        \"\"\"\n\n        if plate_id not in self.plates:\n            logging.info(\"Plate {} not found for deletion\".format(plate_id))\n            return\n\n        plate = self.plates[plate_id]\n\n        if delete_meta_data:\n            for pv in plate.values:\n                identifier = \".\".join(map(lambda x: \"_\".join(x), pv))\n                self.meta_data_manager.delete(identifier=identifier)\n\n        with switch_db(PlateDefinitionModel, \"hyperstream\"):\n            try:\n                p = PlateDefinitionModel.objects.get(plate_id=plate_id)\n                p.delete()\n                del self.plates[plate_id]\n            except DoesNotExist as e:\n                logging.warn(e)\n        logging.info(\"Plate {} deleted\".format(plate_id))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_plate(self, plate_id, description, meta_data_id, values, complement, parent_plate):\n        # Make sure the plate id doesn't already exist\n        with switch_db(PlateDefinitionModel, db_alias='hyperstream'):\n            try:\n                p = PlateDefinitionModel.objects.get(plate_id=plate_id)\n                if p:\n                    logging.info(\"Plate with id {} already exists\".format(plate_id))\n                    return self.plates[plate_id]\n            except DoesNotExist:\n                pass\n            except MultipleObjectsReturned:\n                raise\n\n            plate_definition = PlateDefinitionModel(\n                plate_id=plate_id,\n                description=description,\n                meta_data_id=meta_data_id,\n                values=values,\n                complement=complement,\n                parent_plate=parent_plate\n            )\n\n            self.add_plate(plate_definition)\n            plate_definition.save()\n            return self.plates[plate_id]", "response": "Create a new plate and commit it to the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_plate(self, plate_definition):\n        values = self.get_plate_values(plate_definition)\n\n        # TODO: We should also be checking that the plate is defined over all of the values of the parent plate\n        self.plates[plate_definition.plate_id] = Plate(\n            plate_id=plate_definition.plate_id,\n            meta_data_id=plate_definition.meta_data_id,\n            values=values,\n            parent_plate=self.plates[plate_definition.parent_plate] if plate_definition.parent_plate else None)\n\n        logging.debug(\"Added plate: {}\".format(self.plates[plate_definition.plate_id]))", "response": "Adds a plate using the plate definition\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_plate_values(self, plate_definition):\n        if not plate_definition.values and not plate_definition.complement:\n            raise PlateDefinitionError()\n\n        values = []\n        for n in self.meta_data_manager.global_plate_definitions.all_nodes():\n            if n.tag == plate_definition.meta_data_id:\n                if not plate_definition.values or n.data in plate_definition.values:\n                    if plate_definition.parent_plate:\n                        # This plate has parent plates, so we need to get parent data for the node\n                        parent_plate_value = self.get_parent_plate_value(\n                            self.meta_data_manager.global_plate_definitions, n)\n                        # if tuple(parent_plate_value) not in self.plates[plate_definition.parent_plate].values:\n                        if set(parent_plate_value) not in map(set, self.plates[plate_definition.parent_plate].values):\n                            continue\n                        values.insert(0, self.get_parent_data(\n                            self.meta_data_manager.global_plate_definitions, n, [(n.tag, n.data), ]))\n                    else:\n                        values.insert(0, [(n.tag, n.data), ])\n        if not values:\n            # raise PlateEmptyError(plate_definition.plate_id)\n            logging.warn(\"Plate {} is empty during the creation\".format(plate_definition.plate_id))\n        return values", "response": "Gets the values of the given plate definition from the global meta data according to the given plate definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_parent_plate_value(tree, node, value=None):\n        if value is None:\n            value = []\n        parent = tree.parent(node.identifier)\n        if parent.is_root():\n            # value.append((parent.tag, parent.identifier))\n            return value\n        value = PlateManager.get_parent_plate_value(tree, parent, value)\n        if \".\" in parent.identifier:\n            pass\n        value.append((parent.tag, parent.data))\n        return value", "response": "Recursively gets the parent plate value of the given node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_parent_data(tree, node, current):\n        if not current:\n            current = []\n\n        parent = tree.parent(node.identifier)\n        if parent.is_root():\n            return current\n\n        current.insert(0, (parent.tag, parent.data))\n        return PlateManager.get_parent_data(tree, parent, current)", "response": "Recurse up the tree getting the parent data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timed_loop(name=None,\n               rgstr_stamps=None,\n               save_itrs=SET['SI'],\n               loop_end_stamp=None,\n               end_stamp_unique=SET['UN'],\n               keep_prev_subdivisions=SET['KS'],\n               keep_end_subdivisions=SET['KS'],\n               quick_print=SET['QP']):\n    \"\"\"\n    Instantiate a TimedLoop object for measuring loop iteration timing data.\n    Can be used with either for or while loops.\n\n    Example::\n\n        loop = timed_loop()\n        while x > 0:  # or for x in <iterable>:\n            next(loop)  # or loop.next()\n            <body of loop, with gtimer stamps>\n        loop.exit()\n\n    Notes:\n        Can be used as a context manager around the loop, without requiring\n        separate call to exit().  Redundant calls to exit() do no harm.  Loop\n        functionality is implemented in the next() or __next__() methods.\n\n        Each instance can only be used once, so for an inner loop, this function\n        must be called within the outer loop.\n\n        Any awaiting subdivisions kept at entrance to a loop section will go to\n        the 'UNASSIGNED' position to indicate that they are not properly accounted\n        for in the hierarchy.  Likewise for any awaiting subdivisions kept at the\n        end of loop iterations without a named stamp.\n\n    Args:\n        name (any, optional): Identifier (makes the loop a subdivision), passed\n            through str().\n        rgstr_stamps (list, tuple, optional): Identifiers, see subdivision().\n        save_itrs (bool, optional): see subdivision().\n        loop_end_stamp (any, optional): Identifier, automatic stamp at end of\n            every iteration.\n        end_stamp_unique (bool, optional): see stamp().\n        keep_prev_subdivisions (bool, optional): Keep awaiting subdivisions on\n            entering loop.\n        keep_end_subdivisions (bool, optional): Keep awaiting subdivisions at\n            end of iterations.\n        quick_print (bool, optional): Named loop only, print at end of each iteration.\n\n    Returns:\n        TimedLoop: Custom gtimer object for measuring loops.\n    \"\"\"\n    return TimedLoop(name=name,\n                     rgstr_stamps=rgstr_stamps,\n                     save_itrs=save_itrs,\n                     loop_end_stamp=loop_end_stamp,\n                     end_stamp_unique=end_stamp_unique,\n                     keep_prev_subdivisions=keep_prev_subdivisions,\n                     keep_end_subdivisions=keep_end_subdivisions)", "response": "Return a TimedLoop object that can be used to measure loop timing data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a TimedLoop object for the given iterable of times.", "response": "def timed_for(iterable,\n              name=None,\n              rgstr_stamps=None,\n              save_itrs=SET['SI'],\n              loop_end_stamp=None,\n              end_stamp_unique=SET['UN'],\n              keep_prev_subdivisions=SET['KS'],\n              keep_end_subdivisions=SET['KS'],\n              quick_print=SET['QP']):\n    \"\"\"\n    Instantiate a TimedLoop object for measuring for loop iteration timing data.\n    Can be used only on for loops.\n\n    Example::\n\n        for i in gtimer.timed_for(iterable, ..):\n            <body of loop with gtimer stamps>\n\n    Notes:\n        Can be used as a context manager around the loop.  When breaking out of\n        the loop, requires usage either as a context manager or with a reference\n        to the object on which to call the exit() method after leaving the loop\n        body.  Redundant calls to exit() do no harm.  Loop functionality is\n        implemented in the __iter__() method.\n\n        Each instance can only be used once, so for an inner loop, this function\n        must be called within the outer loop.\n\n        Any awaiting subdivisions kept at entrance to a loop section will go to\n        the 'UNASSIGNED' position to indicate that they are not properly accounted\n        for in the hierarchy.  Likewise for any awaiting subdivisions kept at the\n        end of loop iterations without a named stamp.\n\n    Args:\n        iterable: Same as provided to regular 'for' command.\n        name (any, optional): Identifier (makes the loop a subdivision), passed\n            through str().\n        rgstr_stamps (list,tuple, optional): Identifiers, see subdivision().\n        save_itrs (bool, optional): see subdivision().\n        loop_end_stamp (any, optional): Identifier, automatic stamp at end of\n            every iteration, passed through str().\n        end_stamp_unique (bool, optional): see stamp().\n        keep_prev_subdivisions (bool, optional): Keep awaiting subdivisions on\n            entering loop.\n        keep_end_subdivisions (bool, optional): Keep awaiting subdivisions at\n            end of iterations.\n        quick_print (bool, optional): Named loop only, print at end of each iteration.\n\n\n    Returns:\n        TimedFor: Custom gtimer object for measuring for loops.\n    \"\"\"\n    return TimedFor(iterable,\n                    name=name,\n                    rgstr_stamps=rgstr_stamps,\n                    save_itrs=save_itrs,\n                    loop_end_stamp=loop_end_stamp,\n                    end_stamp_unique=end_stamp_unique,\n                    keep_prev_subdivisions=keep_prev_subdivisions,\n                    keep_end_subdivisions=keep_end_subdivisions)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_calibration(calib, f, loc):\n    for i, node in enumerate(\n        [p + '_' + s for p in ['pos', 'dir'] for s in 'xyz']):\n        h5loc = loc + '/' + node\n        ca = f.get_node(h5loc)\n        ca.append(calib[:, i])\n\n    du = f.get_node(loc + '/du')\n    du.append(calib[:, 7].astype('u1'))\n\n    floor = f.get_node(loc + '/floor')\n    floor.append(calib[:, 8].astype('u1'))\n\n    t0 = f.get_node(loc + '/t0')\n    t0.append(calib[:, 6])\n\n    if loc == \"/hits\":\n        time = f.get_node(loc + \"/time\")\n        offset = len(time)\n        chunk_size = len(calib)\n        time[offset - chunk_size:offset] += calib[:, 6]", "response": "Write calibration set to file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialise_arrays(group, f):\n    for node in ['pos_x', 'pos_y', 'pos_z', 'dir_x', 'dir_y', 'dir_z', 'du',\n                 'floor', 't0']:\n        if node in ['floor', 'du']:\n            atom = U1_ATOM\n        else:\n            atom = F4_ATOM\n        f.create_earray(group, node, atom, (0, ), filters=FILTERS)", "response": "Initialise the EArrays for calibrated hits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a blob counter.", "response": "def blob_counter(self):\n        \"\"\"Create a blob counter.\"\"\"\n        import aa    # pylint: disablF0401        # noqa\n        from ROOT import EventFile    # pylint: disable F0401\n\n        try:\n            event_file = EventFile(self.filename)\n        except Exception:\n            raise SystemExit(\"Could not open file\")\n\n        num_blobs = 0\n        for event in event_file:\n            num_blobs += 1\n\n        return num_blobs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a blob generator.", "response": "def blob_generator(self):\n        \"\"\"Create a blob generator.\"\"\"\n\n        # pylint: disable:F0401,W0612\n        import aa    # pylint: disablF0401        # noqa\n        from ROOT import EventFile    # pylint: disable F0401\n\n        filename = self.filename\n        log.info(\"Reading from file: {0}\".format(filename))\n        if not os.path.exists(filename):\n            log.warning(filename + \" not available: continue without it\")\n\n        try:\n            event_file = EventFile(filename)\n        except Exception:\n            raise SystemExit(\"Could not open file\")\n\n        log.info(\"Generating blobs through new aanet API...\")\n\n        self.print(\"Reading metadata using 'JPrintMeta'\")\n        meta_parser = MetaParser(filename=filename)\n        meta = meta_parser.get_table()\n        if meta is None:\n            self.log.warning(\n                \"No metadata found, this means no data provenance!\"\n            )\n\n        if self.bare:\n            log.info(\"Skipping data conversion, only passing bare aanet data\")\n            for event in event_file:\n                yield Blob({'evt': event, 'event_file': event_file})\n\n        else:\n            log.info(\"Unpacking aanet header into dictionary...\")\n            hdr = self._parse_header(event_file.header)\n            if not hdr:\n                log.info(\"Empty header dict found, skipping...\")\n                self.raw_header = None\n            else:\n                log.info(\"Converting Header dict to Table...\")\n                self.raw_header = self._convert_header_dict_to_table(hdr)\n                log.info(\"Creating HDF5Header\")\n                self.header = HDF5Header.from_table(self.raw_header)\n            for event in event_file:\n                log.debug('Reading event...')\n                blob = self._read_event(event, filename)\n                log.debug('Reading header...')\n                blob[\"RawHeader\"] = self.raw_header\n                blob[\"Header\"] = self.header\n\n                if meta is not None:\n                    blob['Meta'] = meta\n\n                self.group_id += 1\n                yield blob\n\n        del event_file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_string(self, string):\n        self.log.info(\"Parsing ASCII data\")\n\n        if not string:\n            self.log.warning(\"Empty metadata\")\n            return\n\n        lines = string.splitlines()\n        application_data = []\n\n        application = lines[0].split()[0]\n        self.log.debug(\"Reading meta information for '%s'\" % application)\n\n        for line in lines:\n            if application is None:\n                self.log.debug(\n                    \"Reading meta information for '%s'\" % application\n                )\n                application = line.split()[0]\n            application_data.append(line)\n            if line.startswith(application + b' Linux'):\n                self._record_app_data(application_data)\n                application_data = []\n                application = None", "response": "Parse the ASCII output of JPrintMeta"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _record_app_data(self, data):\n        name, revision = data[0].split()\n        root_version = data[1].split()[1]\n        command = b'\\n'.join(data[3:]).split(b'\\n' + name + b' Linux')[0]\n        self.meta.append({\n            'application_name': np.string_(name),\n            'revision': np.string_(revision),\n            'root_version': np.string_(root_version),\n            'command': np.string_(command)\n        })", "response": "Parse raw metadata output for a single application and record it as meta data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_table(self, name='Meta', h5loc='/meta'):\n        if not self.meta:\n            return None\n\n        data = defaultdict(list)\n        for entry in self.meta:\n            for key, value in entry.items():\n                data[key].append(value)\n        dtypes = []\n        for key, values in data.items():\n            max_len = max(map(len, values))\n            dtype = 'S{}'.format(max_len)\n            dtypes.append((key, dtype))\n        tab = Table(\n            data, dtype=dtypes, h5loc=h5loc, name='Meta', h5singleton=True\n        )\n        return tab", "response": "Convert metadata to a KM3Pipe Table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the aggregate ranking of a given alternative in the aggregate ranking of the given key.", "response": "def get_ranking(self, alt):\r\n        \"\"\"\r\n        Description:\r\n            Returns the ranking of a given alternative in the\r\n            computed aggregate ranking.  An error is thrown if\r\n            the alternative does not exist.  The ranking is the\r\n            index in the aggregate ranking, which is 0-indexed.\r\n        Parameters:\r\n            alt: the key that represents an alternative\r\n        \"\"\"\r\n        if self.alts_to_ranks is None:\r\n            raise ValueError(\"Aggregate ranking must be created first\")\r\n        try:\r\n            rank = self.alts_to_ranks[alt]\r\n            return rank\r\n        except KeyError:\r\n            raise KeyError(\"No alternative \\\"{}\\\" found in \".format(str(alt)) +\r\n                           \"the aggregate ranking\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the alternative(s) with the given ranking in the aggregate ranking.", "response": "def get_alternatives(self, rank):\r\n        \"\"\"\r\n        Description:\r\n            Returns the alternative(s) with the given ranking in the\r\n            computed aggregate ranking.  An error is thrown if the\r\n            ranking does not exist.  \r\n        \"\"\"\r\n        if self.ranks_to_alts is None:\r\n            raise ValueError(\"Aggregate ranking must be created first\")\r\n        try:\r\n            alts = self.ranks_to_alts[rank]\r\n            return alts\r\n        except KeyError:\r\n            raise KeyError(\"No ranking \\\"{}\\\" found in \".format(str(rank)) +\r\n                           \"the aggregate ranking\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_rank_dicts(self, alt_scores):\r\n        self.alts_to_ranks = dict()\r\n        cur_score = max(alt_scores.values())\r\n        cur_rank = 0\r\n        self.ranks_to_alts = {cur_rank:[]}\r\n        for i in sorted(alt_scores.keys(), key=lambda x: -alt_scores[x]):\r\n            if alt_scores[i] == cur_score:\r\n                self.ranks_to_alts[cur_rank].append(i)\r\n            elif alt_scores[i] < cur_score:\r\n                cur_rank += 1\r\n                cur_score = alt_scores[i]\r\n                self.ranks_to_alts[cur_rank] = [i]\r\n            self.alts_to_ranks[i] = cur_rank", "response": "Create the ranking and ranking dictionaries for the given set of alternatives."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef itermovieshash(self):\n        cur = self._db.firstkey()\n        while cur is not None:\n            yield cur\n            cur = self._db.nextkey(cur)", "response": "Iterate over the movies hash stored in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_run_results(self, run_id, request_type=\"plan\", timeout_count=120):\n\n        if request_type is not \"plan\" and request_type is not \"apply\":\n            raise KeyError(\"request_type must be Plan or Apply\")\n\n        for x in range(0, timeout_count):\n\n            request = self.client.get(path=\"/runs/\" + run_id).json()\n            if request['data']['attributes']['status'] is not \"planning\" and \\\n                            request['data']['attributes']['status'] is not \"applying\":\n                return request['data']\n\n            print(\"Job Status: \" + request_type + \"ing | \" + str(x * 10) + \" seconds\")\n            time.sleep(10)\n\n        raise TimeoutError(\"Plan took too long to resolve\")", "response": "Get the results of a single run."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the configuration derived from the environment variable.", "response": "def get_consul_configuration_from_environment() -> ConsulConfiguration:\n    \"\"\"\n    Gets configuration on how to connect to Consul from the environment.\n    :return: the configuration derived from the environment\n    :raises KeyError: if a required environment variable has not been set\n    \"\"\"\n    address = os.environ[CONSUL_ADDRESS_ENVIRONMENT_VARIABLE]\n    if \"://\" in address:\n        raise EnvironmentError(f\"Invalid host: {address}. Do not specify scheme in host - set that in \"\n                               f\"{CONSUL_SCHEME_ENVIRONMENT_VARIABLE}\")\n\n    host_port_split = address.split(\":\")\n    host = host_port_split[0]\n    if len(host_port_split) == 1:\n        _get_logger().info(\n            f\"No port specified in address read from the environment - using default port: {DEFAULT_CONSUL_PORT}\")\n        port = DEFAULT_CONSUL_PORT\n    else:\n        port = host_port_split[1]\n\n    return ConsulConfiguration(\n        host=host,\n        port=port,\n        token=os.environ.get(CONSUL_TOKEN_ENVIRONMENT_VARIABLE, DEFAULT_CONSUL_TOKEN),\n        scheme=os.environ.get(CONSUL_SCHEME_ENVIRONMENT_VARIABLE, DEFAULT_CONSUL_SCHEME),\n        datacentre=os.environ.get(CONSUL_DATACENTRE_ENVIRONMENT_VARIABLE, DEFAULT_CONSUL_DATACENTRE),\n        verify=os.environ.get(CONSUL_VERIFY_ENVIRONMENT_VARIABLE, DEFAULT_CONSUL_VERIFY),\n        certificate=os.environ.get(CONSUL_CERTIFICATE_ENVIRONMENT_VARIABLE, DEFAULT_CONSUL_CERTIFICATE))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(self):\n        '''\n        Return a :class:`dict` of errors with keys as the type of error and\n        values as a list of errors.\n\n        :returns dict: a dictionary of errors\n        '''\n\n        errors = copy.deepcopy(self._errors)\n        for key, val in iteritems(self._errors):\n            if not len(val):\n                errors.pop(key)\n        return errors", "response": "Return a dictionary of errors with keys as the type of error and values as a list of errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _collect_fields(self):\n        '''\n        Collects all the attributes that are instance of\n        :class:`incoming.datatypes.Types`. These attributes are used for\n        defining rules of validation for every field/key in the incoming JSON\n        payload.\n\n        :returns: a tuple of attribute names from an instance of a sub-class\n                  of :class:`PayloadValidator`.\n        '''\n\n        fields = []\n        for prop in dir(self):\n            if isinstance(getattr(self, prop), Types):\n                fields.append(prop)\n\n        if not len(fields):\n            raise Exception('No keys/fields defined in the validator class.')\n\n        return tuple(fields)", "response": "Collect all the attributes that are instance of\n        and return a tuple of the names of the fields that are instance of\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _replace_string_args(self):\n        '''\n        A helper method that makes passing custom validators implemented as\n        methods to :class:`incoming.datatypes.Function` instances.\n        '''\n\n        if self._string_args_replaced:\n            return\n\n        for field in self._fields:\n            field = getattr(self, field)\n            if isinstance(field, Function):\n                if isinstance(field.func, str):\n                    field.func = getattr(self, field.func)\n            elif isinstance(field, JSON):\n                if isinstance(field.cls, str):\n                    field.cls = getattr(self, field.cls)\n\n        self._string_args_replaced = True", "response": "A helper method that makes passing custom validators implemented as\n        methods to JSON instances."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate(self, payload, required=None, strict=None):\n        '''\n        Validates a given JSON payload according to the rules defiined for all\n        the fields/keys in the sub-class.\n\n        :param dict payload: deserialized JSON object.\n        :param bool required: if every field/key is required and must be\n                              present in the payload.\n        :param bool strict: if :py:meth:`validate` should detect and report any\n                            fields/keys that are present in the payload but not\n                            defined in the sub-class.\n\n        :returns: a tuple of two items. First item is a :class:`bool`\n                  indicating if the payload was successfully validated and the\n                  second item is ``None``. If the payload was not valid, then\n                  then the second item is a :py:class:`dict` of errors.\n        '''\n\n        # replace datatypes.Function.func if not already replaced\n        self._replace_string_args()\n\n        required = required if required is not None else self.required\n        strict = strict if strict is not None else self.strict\n\n        errors = PayloadErrors()\n        fields = copy.deepcopy(list(self._fields))\n\n        for key, value in iteritems(payload):\n            if key not in self._fields:\n                if strict:\n                    errors[key].append(self.strict_error)\n            else:\n                getattr(self, key).test(key, value, payload=payload,\n                                        errors=errors[key])\n\n                # Remove the key that has been checked\n                fields.remove(key)\n\n        for field in fields:\n            rule = getattr(self, field)\n\n            if rule.required is None:\n                required = required\n            else:\n                required = rule.required\n\n            if required:\n                errors[field].append(self.required_error)\n            elif isinstance(rule, Function):\n                rule.test(field, payload.get(field, None),\n                          payload=payload, errors=errors[field])\n\n        return (False, errors.to_dict()) if errors.has_errors() else (True,\n                                                                      None)", "response": "Validates a given JSON payload according to the rules defined for all the fields in the sub - class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle Datastore response errors according to their documentation.", "response": "def _max_retries_for_error(self, error):\n        \"\"\"Handles Datastore response errors according to their documentation.\n\n        Parameters:\n          error(dict)\n\n        Returns:\n          int or None: The max number of times this error should be\n          retried or None if it shouldn't.\n\n        See also:\n          https://cloud.google.com/datastore/docs/concepts/errors\n        \"\"\"\n        status = error.get(\"status\")\n        if status == \"ABORTED\" and get_transactions() > 0:\n            # Avoids retrying Conflicts when inside a transaction.\n            return None\n        return self._MAX_RETRIES.get(status)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs in to Globus services.", "response": "def login(credentials=None, app_name=None, services=None, client_id=None, make_clients=True,\n          clear_old_tokens=False, token_dir=DEFAULT_CRED_PATH, **kwargs):\n    \"\"\"Log in to Globus services\n\n    Arguments:\n        credentials (str or dict): A string filename, string JSON, or dictionary\n                with credential and config information.\n                By default, looks in ``~/mdf/credentials/globus_login.json``.\n                Contains ``app_name``, ``services``, and ``client_id`` as described below.\n        app_name (str): Name of script/client. This will form the name of the token cache file.\n                **Default**: ``'UNKNOWN'``.\n        services (list of str): Services to authenticate with.\n                **Default**: ``[]``.\n        client_id (str): The ID of the client, given when registered with Globus.\n                **Default**: The MDF Native Clients ID.\n        make_clients (bool): If ``True``, will make and return appropriate clients with\n                generated tokens. If ``False``, will only return authorizers.\n                **Default**: ``True``.\n        clear_old_tokens (bool): If ``True``, delete old token file if it exists,\n                forcing user to re-login. If ``False``, use existing token file if there is one.\n                **Default**: ``False``.\n        token_dir (str): The path to the directory to save tokens in and look for\n                credentials by default. **Default**: ``DEFAULT_CRED_PATH``.\n\n    Returns:\n        dict: The clients and authorizers requested, indexed by service name.\n                For example, if ``login()`` is told to auth with ``'search'``\n                then the search client will be in the ``'search'`` field.\n\n        Note:\n            Previously requested tokens (which are cached) will be returned alongside\n            explicitly requested ones.\n    \"\"\"\n    NATIVE_CLIENT_ID = \"98bfc684-977f-4670-8669-71f8337688e4\"\n    DEFAULT_CRED_FILENAME = \"globus_login.json\"\n\n    def _get_tokens(client, scopes, app_name, force_refresh=False):\n        token_path = os.path.join(token_dir, app_name + \"_tokens.json\")\n        if force_refresh:\n            if os.path.exists(token_path):\n                os.remove(token_path)\n        if os.path.exists(token_path):\n            with open(token_path, \"r\") as tf:\n                try:\n                    tokens = json.load(tf)\n                    # Check that requested scopes are present\n                    # :all scopes should override any scopes with lesser permissions\n                    # Some scopes are returned in multiples and should be separated\n                    existing_scopes = []\n                    for sc in [val[\"scope\"] for val in tokens.values()]:\n                        if \" \" in sc:\n                            existing_scopes += sc.split(\" \")\n                        else:\n                            existing_scopes.append(sc)\n                    permissive_scopes = [scope.replace(\":all\", \"\")\n                                         for scope in existing_scopes\n                                         if scope.endswith(\":all\")]\n                    missing_scopes = [scope for scope in scopes.split(\" \")\n                                      if scope not in existing_scopes\n                                      and not any([scope.startswith(per_sc)\n                                                   for per_sc in permissive_scopes])\n                                      and not scope.strip() == \"\"]\n                    # If some scopes are missing, regenerate tokens\n                    # Get tokens for existing scopes and new scopes\n                    if len(missing_scopes) > 0:\n                        scopes = \" \".join(existing_scopes + missing_scopes)\n                        os.remove(token_path)\n                except ValueError:\n                    # Tokens corrupted\n                    os.remove(token_path)\n        if not os.path.exists(token_path):\n            try:\n                os.makedirs(token_dir)\n            except (IOError, OSError):\n                pass\n            client.oauth2_start_flow(requested_scopes=scopes, refresh_tokens=True)\n            authorize_url = client.oauth2_get_authorize_url()\n\n            print(\"It looks like this is the first time you're accessing this service.\",\n                  \"\\nPlease log in to Globus at this link:\\n\", authorize_url)\n            auth_code = input(\"Copy and paste the authorization code here: \").strip()\n\n            # Handle 401s\n            try:\n                token_response = client.oauth2_exchange_code_for_tokens(auth_code)\n            except globus_sdk.GlobusAPIError as e:\n                if e.http_status == 401:\n                    raise ValueError(\"\\nSorry, that code isn't valid.\"\n                                     \" You can try again, or contact support.\")\n                else:\n                    raise\n            tokens = token_response.by_resource_server\n\n            os.umask(0o077)\n            with open(token_path, \"w\") as tf:\n                json.dump(tokens, tf)\n            print(\"Thanks! You're now logged in.\")\n\n        return tokens\n\n    # If creds supplied in 'credentials', process\n    if credentials:\n        if type(credentials) is str:\n            try:\n                with open(credentials) as cred_file:\n                    creds = json.load(cred_file)\n            except IOError:\n                try:\n                    creds = json.loads(credentials)\n                except ValueError:\n                    raise ValueError(\"Credential string unreadable\")\n        elif type(credentials) is dict:\n            creds = credentials\n        else:\n            try:\n                with open(os.path.join(os.getcwd(), DEFAULT_CRED_FILENAME)) as cred_file:\n                    creds = json.load(cred_file)\n            except IOError:\n                try:\n                    with open(os.path.join(token_dir, DEFAULT_CRED_FILENAME)) as cred_file:\n                        creds = json.load(cred_file)\n                except IOError:\n                    raise ValueError(\"Credentials/configuration must be passed as a \"\n                                     + \"filename string, JSON string, or dictionary, \"\n                                     + \"or provided in '\"\n                                     + DEFAULT_CRED_FILENAME\n                                     + \"' or '\"\n                                     + token_dir\n                                     + \"'.\")\n        app_name = creds.get(\"app_name\")\n        services = creds.get(\"services\", services)\n        client_id = creds.get(\"client_id\")\n    if not app_name:\n        app_name = \"UNKNOWN\"\n    if not services:\n        services = []\n    elif isinstance(services, str):\n        services = [services]\n    if not client_id:\n        client_id = NATIVE_CLIENT_ID\n\n    native_client = globus_sdk.NativeAppAuthClient(client_id, app_name=app_name)\n\n    servs = []\n    for serv in services:\n        serv = serv.lower().strip()\n        if type(serv) is str:\n            servs += serv.split(\" \")\n        else:\n            servs += list(serv)\n    # Translate services into scopes as possible\n    scopes = \" \".join([KNOWN_SCOPES.get(sc, sc) for sc in servs])\n\n    all_tokens = _get_tokens(native_client, scopes, app_name, force_refresh=clear_old_tokens)\n\n    # Make authorizers with every returned token\n    all_authorizers = {}\n    for key, tokens in all_tokens.items():\n        # TODO: Allow non-Refresh authorizers\n        try:\n            all_authorizers[key] = globus_sdk.RefreshTokenAuthorizer(tokens[\"refresh_token\"],\n                                                                     native_client)\n        except KeyError:\n            print(\"Error: Unable to retrieve tokens for '{}'.\\n\"\n                  \"You may need to delete your old tokens and retry.\".format(key))\n    returnables = {}\n    # Populate clients and named services\n    # Only translate back services - if user provides scope directly, don't translate back\n    # ex. transfer => urn:transfer.globus.org:all => transfer,\n    #     but urn:transfer.globus.org:all !=> transfer\n    for service in servs:\n        token_key = KNOWN_TOKEN_KEYS.get(service)\n        # If the .by_resource_server key (token key) for the service was returned\n        if token_key in all_authorizers.keys():\n            # If there is an applicable client (all clients have known token key)\n            # Pop from all_authorizers to remove from final return value\n            if make_clients and KNOWN_CLIENTS.get(service):\n                try:\n                    returnables[service] = KNOWN_CLIENTS[service](\n                                                authorizer=all_authorizers.pop(token_key),\n                                                http_timeout=STD_TIMEOUT)\n                except globus_sdk.GlobusAPIError as e:\n                    print(\"Error: Unable to create {} client: {}\".format(service, e.message))\n            # If no applicable client, just translate the key\n            else:\n                returnables[service] = all_authorizers.pop(token_key)\n    # Add authorizers not associated with service to returnables\n    returnables.update(all_authorizers)\n\n    return returnables"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confidential_login(credentials=None, client_id=None, client_secret=None, services=None,\n                       make_clients=True, token_dir=DEFAULT_CRED_PATH):\n    \"\"\"Log in to Globus services as a confidential client\n    (a client with its own login information).\n\n    Arguments:\n        credentials (str or dict): A string filename, string JSON, or dictionary\n                with credential and config information.\n                By default, uses the ``DEFAULT_CRED_FILENAME`` and token_dir.\n                Contains ``client_id``, ``client_secret``, and ``services`` as defined below.\n        client_id (str): The ID of the client.\n        client_secret (str): The client's secret for authentication.\n        services (list of str): Services to authenticate with.\n        make_clients (bool): If ``True``, will make and return appropriate clients\n                with generated tokens.\n                If ``False``, will only return authorizers.\n                **Default**: ``True``.\n        token_dir (str): The path to the directory to save tokens in and look for\n                credentials by default.\n                **Default**: ``DEFAULT_CRED_PATH``.\n\n    Returns:\n        dict: The clients and authorizers requested, indexed by service name.\n    \"\"\"\n    DEFAULT_CRED_FILENAME = \"confidential_globus_login.json\"\n    # Read credentials if supplied\n    if credentials:\n        if type(credentials) is str:\n            try:\n                with open(credentials) as cred_file:\n                    creds = json.load(cred_file)\n            except IOError:\n                try:\n                    creds = json.loads(credentials)\n                except ValueError:\n                    raise ValueError(\"Credentials unreadable or missing\")\n        elif type(credentials) is dict:\n            creds = credentials\n        else:\n            try:\n                with open(os.path.join(os.getcwd(), DEFAULT_CRED_FILENAME)) as cred_file:\n                    creds = json.load(cred_file)\n            except IOError:\n                try:\n                    with open(os.path.join(token_dir, DEFAULT_CRED_FILENAME)) as cred_file:\n                        creds = json.load(cred_file)\n                except IOError:\n                    raise ValueError(\"Credentials/configuration must be passed as a \"\n                                     \"filename string, JSON string, or dictionary, or provided \"\n                                     \"in '{}' or '{}'.\".format(DEFAULT_CRED_FILENAME,\n                                                               token_dir))\n        client_id = creds.get(\"client_id\")\n        client_secret = creds.get(\"client_secret\")\n        services = creds.get(\"services\", services)\n    if not client_id or not client_secret:\n        raise ValueError(\"A client_id and client_secret are required.\")\n    if not services:\n        services = []\n    elif isinstance(services, str):\n        services = [services]\n\n    conf_client = globus_sdk.ConfidentialAppAuthClient(client_id, client_secret)\n    servs = []\n    for serv in services:\n        serv = serv.lower().strip()\n        if type(serv) is str:\n            servs += serv.split(\" \")\n        else:\n            servs += list(serv)\n    # Translate services into scopes as possible\n    scopes = [KNOWN_SCOPES.get(sc, sc) for sc in servs]\n\n    # Make authorizers with every returned token\n    all_authorizers = {}\n    for scope in scopes:\n        # TODO: Allow non-CC authorizers?\n        try:\n            all_authorizers[scope] = globus_sdk.ClientCredentialsAuthorizer(conf_client, scope)\n        except Exception as e:\n            print(\"Error: Cannot create authorizer for scope '{}' ({})\".format(scope, str(e)))\n    returnables = {}\n    # Populate clients and named services\n    # Only translate back services - if user provides scope directly, don't translate back\n    # ex. transfer => urn:transfer.globus.org:all => transfer,\n    #     but urn:transfer.globus.org:all !=> transfer\n    for service in servs:\n        token_key = KNOWN_SCOPES.get(service)\n        # If the .by_resource_server key (token key) for the service was returned\n        if token_key in all_authorizers.keys():\n            # If there is an applicable client (all clients have known token key)\n            # Pop from all_authorizers to remove from final return value\n            if make_clients and KNOWN_CLIENTS.get(service):\n                try:\n                    returnables[service] = KNOWN_CLIENTS[service](\n                                                authorizer=all_authorizers.pop(token_key),\n                                                http_timeout=STD_TIMEOUT)\n                except globus_sdk.GlobusAPIError as e:\n                    print(\"Error: Unable to create {} client: {}\".format(service, e.message))\n            # If no applicable client, just translate the key\n            else:\n                returnables[service] = all_authorizers.pop(token_key)\n    # Add authorizers not associated with service to returnables\n    returnables.update(all_authorizers)\n\n    return returnables", "response": "Log in to Globus services as a confidential globus login."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef anonymous_login(services):\n    if isinstance(services, str):\n        services = [services]\n\n    clients = {}\n    # Initialize valid services\n    for serv in services:\n        try:\n            clients[serv] = KNOWN_CLIENTS[serv](http_timeout=STD_TIMEOUT)\n        except KeyError:  # No known client\n            print(\"Error: No known client for '{}' service.\".format(serv))\n        except Exception:  # Other issue, probably auth\n            print(\"Error: Unable to create client for '{}' service.\\n\"\n                  \"Anonymous access may not be allowed.\".format(serv))\n\n    return clients", "response": "Initialize services without authenticating to Globus Auth."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving all tokens in the token directory.", "response": "def logout(token_dir=DEFAULT_CRED_PATH):\n    \"\"\"Remove ALL tokens in the token directory.\n    This will force re-authentication to all services.\n\n    Arguments:\n        token_dir (str): The path to the directory to save tokens in and look for\n                credentials by default. If this argument was given to a ``login()`` function,\n                the same value must be given here to properly logout.\n                **Default**: ``DEFAULT_CRED_PATH``.\n    \"\"\"\n    for f in os.listdir(token_dir):\n        if f.endswith(\"tokens.json\"):\n            try:\n                os.remove(os.path.join(token_dir, f))\n            except OSError as e:\n                # Eat ENOENT (no such file/dir, tokens already deleted) only,\n                # raise any other issue (bad permissions, etc.)\n                if e.errno != errno.ENOENT:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_gmeta(data, acl=None, identifier=None):\n    if isinstance(data, dict):\n        if acl is None or identifier is None:\n            raise ValueError(\"acl and identifier are required when formatting a GMetaEntry.\")\n        if isinstance(acl, str):\n            acl = [acl]\n        # \"Correctly\" format ACL entries into URNs\n        prefixed_acl = []\n        for uuid in acl:\n            # If entry is not special value \"public\" and is not a URN, make URN\n            # It is not known what the type of UUID is, so use both\n            # This solution is known to be hacky\n            if uuid != \"public\" and not uuid.lower().startswith(\"urn:\"):\n                prefixed_acl.append(\"urn:globus:auth:identity:\"+uuid.lower())\n                prefixed_acl.append(\"urn:globus:groups:id:\"+uuid.lower())\n            # Otherwise, no modification\n            else:\n                prefixed_acl.append(uuid)\n\n        return {\n            \"@datatype\": \"GMetaEntry\",\n            \"@version\": \"2016-11-09\",\n            \"subject\": identifier,\n            \"visible_to\": prefixed_acl,\n            \"content\": data\n            }\n\n    elif isinstance(data, list):\n        return {\n            \"@datatype\": \"GIngest\",\n            \"@version\": \"2016-11-09\",\n            \"ingest_type\": \"GMetaList\",\n            \"ingest_data\": {\n                \"@datatype\": \"GMetaList\",\n                \"@version\": \"2016-11-09\",\n                \"gmeta\": data\n                }\n            }\n\n    else:\n        raise TypeError(\"Cannot format '\" + str(type(data)) + \"' into GMeta.\")", "response": "Format input into GMeta format suitable for ingesting into Globus Search."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves GMeta wrapping from a Globus Search result.", "response": "def gmeta_pop(gmeta, info=False):\n    \"\"\"Remove GMeta wrapping from a Globus Search result.\n    This function can be called on the raw GlobusHTTPResponse that Search returns,\n    or a string or dictionary representation of it.\n\n    Arguments:\n        gmeta (dict, str, or GlobusHTTPResponse): The Globus Search result to unwrap.\n        info (bool): If ``False``, will return a list of the results\n                and discard the metadata. If ``True``, will return a tuple containing\n                the results list, and other information about the query.\n                **Default**: ``False``.\n\n    Returns:\n        list (if ``info=False``): The unwrapped results.\n        tuple (if ``info=True``): The unwrapped results, and a dictionary of query information.\n    \"\"\"\n    if type(gmeta) is GlobusHTTPResponse:\n        gmeta = json.loads(gmeta.text)\n    elif type(gmeta) is str:\n        gmeta = json.loads(gmeta)\n    elif type(gmeta) is not dict:\n        raise TypeError(\"gmeta must be dict, GlobusHTTPResponse, or JSON string\")\n    results = []\n    for res in gmeta[\"gmeta\"]:\n        for con in res[\"content\"]:\n            results.append(con)\n    if info:\n        fyi = {\n            \"total_query_matches\": gmeta.get(\"total\")\n            }\n        return results, fyi\n    else:\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntranslates a known Globus Search index into the index UUID.", "response": "def translate_index(index_name):\n    \"\"\"Translate a known Globus Search index into the index UUID.\n    The UUID is the proper way to access indices, and will eventually be the only way.\n    This method will return names it cannot disambiguate.\n\n    Arguments:\n        index_name (str): The name of the index.\n\n    Returns:\n        str: The UUID of the index. If the index is not known and is not unambiguous,\n                this will be the ``index_name`` unchanged instead.\n    \"\"\"\n    uuid = SEARCH_INDEX_UUIDS.get(index_name.strip().lower())\n    if not uuid:\n        try:\n            index_info = globus_sdk.SearchClient().get_index(index_name).data\n            if not isinstance(index_info, dict):\n                raise ValueError(\"Multiple UUIDs possible\")\n            uuid = index_info.get(\"id\", index_name)\n        except Exception:\n            uuid = index_name\n    return uuid"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef custom_transfer(transfer_client, source_ep, dest_ep, path_list, interval=DEFAULT_INTERVAL,\n                    inactivity_time=DEFAULT_INACTIVITY_TIME, notify=True):\n    \"\"\"Perform a Globus Transfer.\n\n    Arguments:\n        transfer_client (TransferClient): An authenticated Transfer client.\n        source_ep (str): The source Globus Endpoint ID.\n        dest_ep (str): The destination Globus Endpoint ID.\n        path_list (list of tuple of 2 str): A list of tuples containing the paths to transfer as\n                ``(source, destination)``.\n\n                **Example**::\n\n                    [(\"/source/files/file.dat\", \"/dest/mydocs/doc.dat\"),\n                     (\"/source/all_reports/\", \"/dest/reports/\")]\n\n        interval (int): Number of seconds to wait before polling Transfer status.\n                Minimum ``1``. **Default**: ``DEFAULT_INTERVAL``.\n        inactivity_time (int): Number of seconds a Transfer is allowed to go without progress\n                before being cancelled. **Default**: ``DEFAULT_INACTIVITY_TIME``.\n        notify (bool): When ``True``, trigger a notification email from Globus to the user when\n                the Transfer succeeds or fails. When ``False``, disable the notification.\n                **Default**: ``True``.\n\n    Yields:\n        dict: An error from the transfer, or (last) a success status.\n\n    Accepts via ``.send()``:\n        *bool*: ``True``: Continue the Transfer\n                ``False``: Cancel the Transfer\n                **Default**: ``True``\n    \"\"\"\n    # TODO: (LW) Handle transfers with huge number of files\n    # If a TransferData object is too large, Globus might timeout\n    #   before it can be completely uploaded.\n    # So, we need to be able to check the size of the TD object and, if need be, send it early.\n    if interval < 1:\n        interval = 1\n    deadline = datetime.utcfromtimestamp(int(time.time()) + inactivity_time)\n    tdata = globus_sdk.TransferData(transfer_client, source_ep, dest_ep,\n                                    deadline=deadline, verify_checksum=True,\n                                    notify_on_succeeded=notify, notify_on_failed=notify,\n                                    notify_on_inactive=notify)\n    for item in path_list:\n        # Check if source path is directory or missing\n        try:\n            transfer_client.operation_ls(source_ep, path=item[0])\n            source_is_dir = True\n        except globus_sdk.exc.TransferAPIError as e:\n            # If error indicates path exists but is not dir, is not dir\n            if e.code == \"ExternalError.DirListingFailed.NotDirectory\":\n                source_is_dir = False\n            # Too many files in dir indicates is dir\n            elif e.code == \"ExternalError.DirListingFailed.SizeLimit\":\n                source_is_dir = True\n            # Not found is real error\n            elif e.code == \"ClientError.NotFound\":\n                raise globus_sdk.GlobusError(\"Path '{}' not found on source endpoint '{}'\"\n                                             .format(item[0], source_ep))\n            # Else, retry on parent dir\n            else:\n                try:\n                    parent, item_name = os.path.split(item[0])\n                    parent_ls = transfer_client.operation_ls(source_ep, path=parent)\n                    type_list = [x[\"type\"] for x in parent_ls[\"DATA\"] if x[\"name\"] == item_name]\n                    if len(type_list) < 1:\n                        raise globus_sdk.GlobusError(\"No items with name '{}' in path '{}' on \"\n                                                     \"endpoint '{}'\"\n                                                     .format(item_name, parent, source_ep))\n                    elif len(type_list) > 1:\n                        raise globus_sdk.GlobusError(\"Multiple items with name '{}' in path '{}'\"\n                                                     \"on endpoint '{}'\"\n                                                     .format(item_name, parent, source_ep))\n                    item_type = type_list[0]\n                    if item_type == \"dir\":\n                        source_is_dir = True\n                    elif item_type == \"file\":\n                        source_is_dir = False\n                    else:\n                        raise ValueError(\"Path '{}' does not lead to a file or a directory ({})\"\n                                         .format(item[0], item_type))\n                except globus_sdk.exc.TransferAPIError as e:\n                    # Size limit means we can't figure out this path\n                    if e.code == \"ExternalError.DirListingFailed.SizeLimit\":\n                        raise globus_sdk.GlobusError(\"Unable to check type of {}\".format(item[0]))\n                    # Not found is still an error\n                    elif e.code == \"ClientError.NotFound\":\n                        raise globus_sdk.GlobusError(\"Parent path '{}' not found on source \"\n                                                     \"endpoint '{}'\".format(item[0], source_ep))\n                    else:\n                        raise\n\n        # Check if dest path is directory\n        dest_exists = False\n        try:\n            transfer_client.operation_ls(dest_ep, path=item[1])\n            dest_exists = True\n            dest_is_dir = True\n        except globus_sdk.exc.TransferAPIError as e:\n            if e.code == \"ExternalError.DirListingFailed.NotDirectory\":\n                dest_exists = True\n                dest_is_dir = False\n            elif e.code == \"ExternalError.DirListingFailed.SizeLimit\":\n                dest_exists = True\n                dest_is_dir = True\n            elif e.code == \"ClientError.NotFound\":\n                # Destination will be created, not an issue if not found\n                pass\n            else:\n                try:\n                    parent, item_name = os.path.split(item[1])\n                    parent_ls = transfer_client.operation_ls(dest_ep, path=parent)\n                    type_list = [x[\"type\"] for x in parent_ls[\"DATA\"] if x[\"name\"] == item_name]\n                    if len(type_list) < 1:\n                        raise globus_sdk.GlobusError(\"No items with name '{}' in path '{}' on \"\n                                                     \"endpoint '{}'\"\n                                                     .format(item_name, parent, dest_ep))\n                    elif len(type_list) > 1:\n                        raise globus_sdk.GlobusError(\"Multiple items with name '{}' in path '{}'\"\n                                                     \"on endpoint '{}'\"\n                                                     .format(item_name, parent, dest_ep))\n                    item_type = type_list[0]\n                    if item_type == \"dir\":\n                        dest_exists = True\n                        dest_is_dir = True\n                    elif item_type == \"file\":\n                        dest_exists = True\n                        dest_is_dir = False\n                    else:\n                        # Assume we're overwriting whatever dest is, as if it doesn't exist\n                        pass\n                except globus_sdk.exc.TransferAPIError as e:\n                    # Size limit means we can't figure out this path\n                    if e.code == \"ExternalError.DirListingFailed.SizeLimit\":\n                        raise globus_sdk.GlobusError(\"Unable to check type of {}\".format(item[0]))\n                    # Not found is not our problem for dest\n                    elif e.code == \"ClientError.NotFound\":\n                        pass\n                    else:\n                        raise\n        # Transfer dir\n        # Short-circuit OR/AND eval means if not dest_exists, dest_is_dir can be unassigned\n        if source_is_dir and (not dest_exists or dest_is_dir):\n            tdata.add_item(item[0], item[1], recursive=True)\n        # Transfer non-dir\n        elif not source_is_dir and (not dest_exists or not dest_is_dir):\n            tdata.add_item(item[0], item[1])\n        # Transfer non-dir into dir\n        # TODO: Is this logic user-friendly or is it surprising?\n        # Take non-dir source filename, Transfer to dest dir+filename\n        elif not source_is_dir and (dest_exists and dest_is_dir):\n            new_dest = os.path.join(item[1], os.path.basename(item[0]))\n            tdata.add_item(item[0], new_dest)\n        # Malformed - Cannot transfer dir into non-dir\n        else:\n            raise globus_sdk.GlobusError(\"Cannot transfer a directory into a file: \"\n                                         + str(item))\n\n    res = transfer_client.submit_transfer(tdata)\n    if res[\"code\"] != \"Accepted\":\n        raise globus_sdk.GlobusError(\"Failed to transfer files: Transfer \" + res[\"code\"])\n\n    error_timestamps = set()\n    # while Transfer is active\n    while not transfer_client.task_wait(res[\"task_id\"],\n                                        timeout=interval, polling_interval=interval):\n        for event in transfer_client.task_event_list(res[\"task_id\"]):\n            # Only process error events that have not been presented to the user\n            # Events do not have UUIDs, so if there are multiple simultaneous errors\n            #   only the last (chronologically) error will be processed\n            if event[\"is_error\"] and event[\"time\"] not in error_timestamps:\n                error_timestamps.add(event[\"time\"])\n                ret_event = event.data\n                # yield value should always have success: bool\n                ret_event[\"success\"] = False\n                ret_event[\"finished\"] = False\n                # User can cancel Transfer with .send(False)\n                cont = yield ret_event\n                if cont is False:\n                    transfer_client.cancel_task(res[\"task_id\"])\n                    # Wait until Transfer is no longer active after cancellation\n                    while not transfer_client.task_wait(res[\"task_id\"],\n                                                        timeout=1, polling_interval=1):\n                        pass\n                    break\n            # If progress has been made, move deadline forward\n            elif event[\"code\"] == \"PROGRESS\":\n                new_deadline = datetime.utcfromtimestamp(int(time.time()) + inactivity_time)\n                new_doc = {\n                    \"DATA_TYPE\": \"task\",\n                    \"deadline\": str(new_deadline)\n                }\n                transfer_client.update_task(res[\"task_id\"], new_doc)\n    # Transfer is no longer active; now check if succeeded\n    task = transfer_client.get_task(res[\"task_id\"]).data\n    task[\"success\"] = (task[\"status\"] == \"SUCCEEDED\")\n    task[\"finished\"] = True\n    yield task", "response": "This function is used to perform a custom Globus Transfer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef quick_transfer(transfer_client, source_ep, dest_ep, path_list, interval=None, retries=10,\n                   notify=True):\n    \"\"\"Perform a Globus Transfer and monitor for success.\n\n    Arguments:\n        transfer_client (TransferClient): An authenticated Transfer client.\n        source_ep (str): The source Globus Endpoint ID.\n        dest_ep (str): The destination Globus Endpoint ID.\n        path_list (list of tuple of 2 str): A list of tuples containing the paths to transfer as\n                ``(source, destination)``.\n\n                **Example**::\n\n                    [(\"/source/files/file.dat\", \"/dest/mydocs/doc.dat\"),\n                     (\"/source/all_reports/\", \"/dest/reports/\")]\n\n        interval (int): Number of seconds to wait before polling Transfer status.\n                Minimum ``1``.**Default**: ``DEFAULT_INTERVAL``.\n        retries (int): The number of errors to tolerate before cancelling the task.\n                Globus Transfer makes no distinction between hard errors\n                (e.g. \"permission denied\") and soft errors\n                (e.g. \"endpoint [temporarily] too busy\") so requiring retries is\n                not uncommon for large Transfers.\n                ``-1`` for infinite tries (Transfer still fails after a period of no activity).\n                ``None`` is synonymous with ``0``.\n                **Default**: ``10``.\n        notify (bool): When ``True``, trigger a notification email from Globus to the user when\n                the Transfer succeeds or fails. When ``False``, disable the notification.\n                **Default**: ``True``.\n\n    Returns:\n        str: ID of the Globus Transfer.\n    \"\"\"\n    if retries is None:\n        retries = 0\n    iterations = 0\n\n    transfer = custom_transfer(transfer_client, source_ep, dest_ep, path_list, notify=notify)\n    res = next(transfer)\n    try:\n        # Loop ends on StopIteration from generator exhaustion\n        while True:\n            if iterations < retries or retries == -1:\n                res = transfer.send(True)\n                iterations += 1\n            else:\n                res = transfer.send(False)\n    except StopIteration:\n        pass\n    if res[\"success\"]:\n        error = \"No error\"\n    else:\n        error = \"{}: {}\".format(res.get(\"fatal_error\", {}).get(\"code\", \"Error\"),\n                                res.get(\"fatal_error\", {}).get(\"description\", \"Unknown\"))\n    return {\n        \"success\": res[\"success\"],\n        \"task_id\": res[\"task_id\"],\n        \"error\": error\n    }", "response": "Perform a Globus Transfer and monitor for success."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_local_ep(*args, **kwargs):\n    if kwargs.get(\"warn\", True):\n        raise DeprecationWarning(\"'get_local_ep()' has been deprecated in favor of \"\n                                 \"'globus_sdk.LocalGlobusConnectPersonal().endpoint_id'. \"\n                                 \"To override, pass in 'warn=False'.\")\n    else:\n        import warnings\n        warnings.warn(\"'get_local_ep()' has been deprecated in favor of \"\n                      \"'globus_sdk.LocalGlobusConnectPersonal().endpoint_id'.\")\n    return globus_sdk.LocalGlobusConnectPersonal().endpoint_id", "response": "Deprecated. Use globus_sdk. LocalGlobusConnectPersonal. endpoint_id instead."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge one dictionary with another recursively.", "response": "def dict_merge(base, addition, append_lists=False):\n    \"\"\"Merge one dictionary with another, recursively.\n    Fields present in addition will be added to base if not present or merged\n    if both values are dictionaries or lists (with append_lists=True). If\n    the values are different data types, the value in addition will be discarded.\n    No data from base is deleted or overwritten.\n    This function does not modify either dictionary.\n    Dictionaries inside of other container types (list, etc.) are not merged,\n    as the rules for merging would be ambiguous.\n    If values from base and addition are of differing types, the value in\n    addition is discarded.\n\n    This utility could be expanded to merge Mapping and Container types in the future,\n    but currently works only with dict and list.\n\n    Arguments:\n        base (dict): The dictionary being added to.\n        addition (dict): The dictionary with additional data.\n        append_lists (bool): When ``True``, fields present in base and addition\n                that are lists will also be merged. Extra values from addition\n                will be appended to the list in base.\n\n    Returns:\n        dict: The merged base.\n    \"\"\"\n    if not isinstance(base, dict) or not isinstance(addition, dict):\n        raise TypeError(\"dict_merge only works with dicts.\")\n\n    new_base = deepcopy(base)\n    for key, value in addition.items():\n        # Simplest case: Key not in base, so add value to base\n        if key not in new_base.keys():\n            new_base[key] = value\n        # If the value is a dict, and base's value is also a dict, merge\n        # If there is a type disagreement, merging cannot and should not happen\n        if isinstance(value, dict) and isinstance(new_base[key], dict):\n            new_base[key] = dict_merge(new_base[key], value)\n        # If value is a list, lists should be merged, and base is compatible\n        elif append_lists and isinstance(value, list) and isinstance(new_base[key], list):\n            new_list = deepcopy(new_base[key])\n            [new_list.append(item) for item in value if item not in new_list]\n            new_base[key] = new_list\n        # If none of these trigger, discard value from addition implicitly\n\n    return new_base"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompare two items without regard to order.", "response": "def insensitive_comparison(item1, item2, type_insensitive=False, string_insensitive=False):\n    \"\"\"Compare two items without regard to order.\n\n    The following rules are used to determine equivalence:\n        * Items that are not of the same type can be equivalent only when ``type_insensitive=True``.\n        * Mapping objects are equal iff the keys in each item exist in both items and have\n          the same value (with the same ``insensitive_comparison``).\n        * Other containers except for strings are equal iff every element in each item exists\n          in both items (duplicate items must be present the same number of times).\n        * Containers must be ``Iterable`` to be compared in this way.\n        * Non-containers are equivalent if the equality operator returns ``True``.\n        * Strings are treated as non-containers when ``string_insensitive=False``,\n          and are treated as containers when ``string_insensitive=True``. When treated as\n          containers, each (case-insensitive) character is treated as an element and\n          whitespace is ignored.\n        * If the items are in different categories above, they are never equivalent,\n          even when ``type_insensitive=True``.\n\n    Arguments:\n        item1 (any): The first item to compare.\n        item2 (any): The second item to compare.\n        type_insensitive (bool): When ``True``, items of a different type are not automatically\n                unequivalent. When ``False``, items must be the same type to be equivalent.\n                **Default**: ``False``.\n        string_insensitive (bool): When ``True``, strings are treated as containers, with each\n                character being one element in the container.\n                When ``False``, strings are treated as non-containers and compared directly.\n                **Default**: ``False``.\n\n    Returns:\n        bool: ``True`` iff the two items are equivalent (see above).\n                ``False`` otherwise.\n    \"\"\"\n    # If type-sensitive, check types\n    if not type_insensitive and type(item1) != type(item2):\n        return False\n\n    # Handle Mapping objects (dict)\n    if isinstance(item1, Mapping):\n        # Second item must be Mapping\n        if not isinstance(item2, Mapping):\n            return False\n        # Items must have the same number of elements\n        if not len(item1) == len(item2):\n            return False\n        # Keys must be the same\n        if not insensitive_comparison(list(item1.keys()), list(item2.keys()),\n                                      type_insensitive=True):\n            return False\n        # Each key's value must be the same\n        # We can just check item1.items because the keys are the same\n        for key, val in item1.items():\n            if not insensitive_comparison(item1[key], item2[key],\n                                          type_insensitive=type_insensitive,\n                                          string_insensitive=string_insensitive):\n                return False\n        # Keys and values are the same\n        return True\n    # Handle strings\n    elif isinstance(item1, str):\n        # Second item must be string\n        if not isinstance(item2, str):\n            return False\n        # Items must have the same number of elements (except string_insensitive)\n        if not len(item1) == len(item2) and not string_insensitive:\n            return False\n        # If we're insensitive to case, spaces, and order, compare characters\n        if string_insensitive:\n            # If the string is one character long, skip additional comparison\n            if len(item1) <= 1:\n                return item1.lower() == item2.lower()\n            # Make strings into containers (lists) and discard whitespace\n            item1_list = [c for c in item1.lower() if not c.isspace()]\n            item2_list = [c for c in item2.lower() if not c.isspace()]\n            # The insensitive args shouldn't matter, but they're here just in case\n            return insensitive_comparison(item1_list, item2_list,\n                                          type_insensitive=type_insensitive,\n                                          string_insensitive=string_insensitive)\n        # Otherwise, case and order matter\n        else:\n            return item1 == item2\n    # Handle other Iterable Containers\n    elif isinstance(item1, Container) and isinstance(item1, Iterable):\n        # Second item must be an Iterable Container\n        if not isinstance(item2, Container) or not isinstance(item2, Iterable):\n            return False\n        # Items must have the same number of elements\n        if not len(item1) == len(item2):\n            return False\n        # Every element in item1 must be in item2, and vice-versa\n        # Painfully slow, but unavoidable for deep comparison\n        # Each match in item1 removes the corresponding element from item2_copy\n        # If they're the same, item2_copy should be empty at the end,\n        #   unless a .remove() failed, in which case we have to re-match using item2\n        item2_copy = list(deepcopy(item2))\n        remove_failed = False\n        for elem in item1:\n            matched = False\n            # Try every element\n            for candidate in item2:\n                # If comparison succeeds, flag a match, remove match from copy, and dump out\n                if insensitive_comparison(elem, candidate,\n                                          type_insensitive=type_insensitive,\n                                          string_insensitive=string_insensitive):\n                    matched = True\n                    try:\n                        item2_copy.remove(candidate)\n                    except ValueError:  # list.remove(x): x not in list\n                        remove_failed = True\n                    break\n            # One failure indicates unequivalence\n            if not matched:\n                return False\n        # If all removes succeeded, we can shortcut checking all item2 elements in item1\n        if not remove_failed:\n            # If the Containers are equivalent, all elements in item2_copy should be removed\n            # Otherwise\n            return len(item2_copy) == 0\n        # If something failed, we have to verify all of item2\n        # We can't assume item2 != item1, because removal is comparative\n        else:\n            for elem in item2:\n                matched = False\n                # Try every element\n                for candidate in item1:\n                    # If comparison succeeds, flag a match, remove match from copy, and dump out\n                    if insensitive_comparison(elem, candidate,\n                                              type_insensitive=type_insensitive,\n                                              string_insensitive=string_insensitive):\n                        matched = True\n                        break\n                # One failure indicates unequivalence\n                if not matched:\n                    return False\n            # All elements have a match\n            return True\n    # Handle otherwise unhandled type (catchall)\n    else:\n        return item1 == item2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a json file and yield screen_name text tuples.", "response": "def parse_json(json_file, include_date=False):\n    \"\"\" Yield screen_name, text tuples from a json file. \"\"\"\n    if json_file[-2:] == 'gz':\n        fh = gzip.open(json_file, 'rt')\n    else:\n        fh = io.open(json_file, mode='rt', encoding='utf8')\n    for line in fh:\n        try:\n            jj = json.loads(line)\n            if type(jj) is not list:\n                jj = [jj]\n            for j in jj:\n                if include_date:\n                    yield (j['user']['screen_name'].lower(), j['text'], j['created_at'])\n                else:\n                    if 'full_text' in j: # get untruncated text if available.\n                        yield (j['user']['screen_name'].lower(), j['full_text'])\n                    else:\n                        yield (j['user']['screen_name'].lower(), j['text'])\n\n        except Exception as e:\n            sys.stderr.write('skipping json error: %s\\n' % e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding screen_name string tuples where the string is the concatenation of all tweets of this user.", "response": "def extract_tweets(json_file):\n    \"\"\" Yield screen_name, string tuples, where the string is the\n    concatenation of all tweets of this user. \"\"\"\n    for screen_name, tweet_iter in groupby(parse_json(json_file), lambda x: x[0]):\n        tweets = [t[1] for t in tweet_iter]\n        yield screen_name, ' '.join(tweets)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vectorize(json_file, vec, dofit=True):\n    ## CountVectorizer, efficiently.\n    screen_names = [x[0] for x in extract_tweets(json_file)]\n    if dofit:\n        X = vec.fit_transform(x[1] for x in extract_tweets(json_file))\n    else:\n        X = vec.transform(x[1] for x in extract_tweets(json_file))\n    return screen_names, X", "response": "Vectorize the Tweets in a Twitter account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a file of follower information and return a dictionary mapping screen_name to a set of follower ids.", "response": "def read_follower_file(fname, min_followers=0, max_followers=1e10, blacklist=set()):\n    \"\"\" Read a file of follower information and return a dictionary mapping screen_name to a set of follower ids. \"\"\"\n    result = {}\n    with open(fname, 'rt') as f:\n        for line in f:\n            parts = line.split()\n            if len(parts) > 3:\n                if parts[1].lower() not in blacklist:\n                    followers = set(int(x) for x in parts[2:])\n                    if len(followers) > min_followers and len(followers) <= max_followers:\n                        result[parts[1].lower()] = followers\n                else:\n                    print('skipping exemplar', parts[1].lower())\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_follower_file(fname):\n    with open(fname, 'rt') as f:\n        for line in f:\n            parts = line.split()\n            if len(parts) > 3:\n                yield parts[1].lower(), set(int(x) for x in parts[2:])", "response": "Iterator from a file of follower information and return a tuple of screen_name follower ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the average Jaccard similarity between a brand s followers and an exemplar s followers and the followers of each exemplar. We merge all followers of each brand into one .", "response": "def jaccard_merge(brands, exemplars):\n    \"\"\" Return the average Jaccard similarity between a brand's followers and\n    the followers of each exemplar. We merge all exemplar followers into one\n    big pseudo-account.\"\"\"\n    scores = {}\n    exemplar_followers = set()\n    for followers in exemplars.values():\n        exemplar_followers |= followers\n\n    for brand, followers in brands:\n        scores[brand] = _jaccard(followers, exemplar_followers)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_log_degrees(brands, exemplars):\n    counts = Counter()\n    for followers in brands.values():  # + exemplars.values():  # Include exemplars in these counts? No, don't want to penalize people who follow many exemplars.\n        counts.update(followers)\n    counts.update(counts.keys())  # Add 1 to each count.\n    for k in counts:\n        counts[k] = 1. / math.log(counts[k])\n    return counts", "response": "Compute the log degrees of each follower."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef proportion_merge(brands, exemplars):\n    scores = {}\n    exemplar_followers = set()\n    for followers in exemplars.values():\n        exemplar_followers |= followers\n\n    for brand, followers in brands:\n        scores[brand] = _proportion(followers, exemplar_followers)\n    return scores", "response": "Return the proportion of a brand s followers who also follower an\n    exemplar. We merge all followers who also follower an\n    exemplar. We merge all exemplar followers into one big pseudo - account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _cosine(a, b):\n    return 1. * len(a & b) / (math.sqrt(len(a)) * math.sqrt(len(b)))", "response": "Return the cosine of two sequence elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cosine(brands, exemplars, weighted_avg=False, sqrt=False):\n    scores = {}\n    for brand, followers in brands:\n        if weighted_avg:\n            scores[brand] = np.average([_cosine(followers, others) for others in exemplars.values()],\n                                       weights=[1. / len(others) for others in exemplars.values()])\n        else:\n            scores[brand] = 1. * sum(_cosine(followers, others) for others in exemplars.values()) / len(exemplars)\n    if sqrt:\n        scores = dict([(b, math.sqrt(s)) for b, s in scores.items()])\n    return scores", "response": "Return the cosine similarity betwee a brand s followers and the exemplars."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the proportion of a brand s followers who also follower an exemplar. We merge all followers who also follower an exemplar. We merge all exemplar followers into one big pseudo - account.", "response": "def cosine_merge(brands, exemplars):\n    \"\"\" Return the proportion of a brand's followers who also follower an\n    exemplar. We merge all exemplar followers into one big pseudo-account.\"\"\"\n    scores = {}\n    exemplar_followers = set()\n    for followers in exemplars.values():\n        exemplar_followers |= followers\n\n    for brand, followers in brands:\n        scores[brand] = _cosine(followers, exemplar_followers)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the average Adamic / Adar similarity between a brand s followers and the followers of each exemplar.", "response": "def adamic(brands, exemplars):\n    \"\"\" Return the average Adamic/Adar similarity between a brand's followers\n    and the followers of each exemplar. We approximate the number of followed\n    accounts per user by only considering those in our brand set.\"\"\"\n    print('adamic deprecated...requires loading all brands in memory.')\n    return\n    degrees = compute_log_degrees(brands, exemplars)\n    scores = {}\n    exemplar_sums = dict([(exemplar, sum(degrees[z] for z in exemplars[exemplar])) for exemplar in exemplars])\n\n    for brand in sorted(brands):\n        brand_sum = sum(degrees[z] for z in brands[brand])\n        total = 0.\n        for exemplar in exemplars:\n            total += sum(degrees[z] for z in brands[brand] & exemplars[exemplar]) / (brand_sum + exemplar_sums[exemplar])\n        scores[brand] = total / len(exemplars)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing a score for each follower that is sum_i ( 1 / n_i ) where n_i is the degree of the ith exemplar they follow.", "response": "def compute_rarity_scores(exemplars):\n    \"\"\" Compute a score for each follower that is sum_i (1/n_i), where n_i is\n    the degree of the ith exemplar they follow.\n    >>> compute_rarity_scores({'e1':{1,2,3,4}, 'e2':{4,5}}).items()\n    [(1, 0.25), (2, 0.25), (3, 0.25), (4, 0.75), (5, 0.5)]\n    \"\"\"\n    scores = defaultdict(lambda: 0.)\n    for followers in exemplars.values():\n        score = 1. / len(followers)\n        for f in followers:\n            scores[f] += score\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes a score for each follower that is sum_i ( 1 / n_i where n_i is the degree of the ith exemplar they follow.", "response": "def rarity(brands, exemplars):\n    \"\"\" Compute a score for each follower that is sum_i (1/n_i), where n_i is the degree of the ith exemplar they follow.\n    The score for a brand is then the average of their follower scores.\"\"\"\n    rarity = compute_rarity_scores(exemplars)\n    scores = {}\n    for brand, followers in brands:\n        scores[brand] = sum(rarity[f] for f in followers) / len(followers)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing a score for each follower that is sum_i ( 1 / n_i ) where n_i is the degree of the ith exemplar they follow.", "response": "def compute_rarity_scores_log(exemplars):\n    \"\"\" Compute a score for each follower that is sum_i (1/n_i), where n_i is\n    the degree of the ith exemplar they follow.\n    >>> compute_rarity_scores({'e1':{1,2,3,4}, 'e2':{4,5}}).items()\n    [(1, 0.25), (2, 0.25), (3, 0.25), (4, 0.75), (5, 0.5)]\n    \"\"\"\n    scores = defaultdict(lambda: 0.)\n    for followers in exemplars.values():\n        score = 1. / math.log(len(followers))\n        for f in followers:\n            scores[f] += score\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes a score for each follower that is sum_i ( 1 / log ( n_i ) where n_i is the degree of the ith exemplar they follow.", "response": "def rarity_log(brands, exemplars):\n    \"\"\" Compute a score for each follower that is sum_i (1/log(n_i)), where n_i is the degree of the ith exemplar they follow.\n    The score for a brand is then the average of their follower scores.\"\"\"\n    rarity = compute_rarity_scores_log(exemplars)\n    scores = {}\n    for brand, followers in brands:\n        scores[brand] = sum(rarity[f] for f in followers) / len(followers)\n    return scores"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef suggest_filename(metadata):\n\n\tif 'title' in metadata and 'track_number' in metadata:  # Music Manager.\n\t\tsuggested_filename = f\"{metadata['track_number']:0>2} {metadata['title']}\"\n\telif 'title' in metadata and 'trackNumber' in metadata:  # Mobile.\n\t\tsuggested_filename = f\"{metadata['trackNumber']:0>2} {metadata['title']}\"\n\telif 'title' in metadata and 'tracknumber' in metadata:  # audio-metadata/mutagen.\n\t\ttrack_number = _split_number_field(\n\t\t\tlist_to_single_value(\n\t\t\t\tmetadata['tracknumber']\n\t\t\t)\n\t\t)\n\t\ttitle = list_to_single_value(metadata['title'])\n\n\t\tsuggested_filename = f\"{track_number:0>2} {title}\"\n\telse:\n\t\tsuggested_filename = f\"00 {list_to_single_value(metadata.get('title', ['']))}\"\n\n\treturn _replace_invalid_characters(suggested_filename)", "response": "Suggest a filename based on metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a filepath based on a template.", "response": "def template_to_filepath(template, metadata, template_patterns=None):\n\t\"\"\"Create directory structure and file name based on metadata template.\n\n\tNote:\n\n\tA template meant to be a base directory for suggested\n\tnames should have a trailing slash or backslash.\n\n\tParameters:\n\t\ttemplate (str or ~os.PathLike): A filepath which can include template patterns as defined by :param template_patterns:.\n\n\t\tmetadata (~collections.abc.Mapping): A metadata dict.\n\n\t\ttemplate_patterns (~collections.abc.Mapping): A dict of ``pattern: field`` pairs used to replace patterns with metadata field values.\n\t\t\tDefault: :const:`~google_music_utils.constants.TEMPLATE_PATTERNS`\n\n\tReturns:\n\t\t~pathlib.Path: A filepath.\n\t\"\"\"\n\n\tpath = Path(template)\n\n\tif template_patterns is None:\n\t\ttemplate_patterns = TEMPLATE_PATTERNS\n\n\tsuggested_filename = suggest_filename(metadata)\n\n\tif (\n\t\tpath == Path.cwd()\n\t\tor path == Path('%suggested%')\n\t):\n\t\tfilepath = Path(suggested_filename)\n\telif any(template_pattern in path.parts for template_pattern in template_patterns):\n\t\tif template.endswith(('/', '\\\\')):\n\t\t\ttemplate += suggested_filename\n\n\t\tpath = Path(template.replace('%suggested%', suggested_filename))\n\n\t\tparts = []\n\t\tfor part in path.parts:\n\t\t\tif part == path.anchor:\n\t\t\t\tparts.append(part)\n\t\t\telse:\n\t\t\t\tfor key in template_patterns:\n\t\t\t\t\tif (  # pragma: no branch\n\t\t\t\t\t\tkey in part\n\t\t\t\t\t\tand any(field in metadata for field in template_patterns[key])\n\t\t\t\t\t):\n\t\t\t\t\t\tfield = more_itertools.first_true(\n\t\t\t\t\t\t\ttemplate_patterns[key],\n\t\t\t\t\t\t\tpred=lambda k: k in metadata\n\t\t\t\t\t\t)\n\n\t\t\t\t\t\tif key.startswith(('%disc', '%track')):\n\t\t\t\t\t\t\tnumber = _split_number_field(\n\t\t\t\t\t\t\t\tstr(\n\t\t\t\t\t\t\t\t\tlist_to_single_value(\n\t\t\t\t\t\t\t\t\t\tmetadata[field]\n\t\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t\t)\n\t\t\t\t\t\t\t)\n\n\t\t\t\t\t\t\tif key.endswith('2%'):\n\t\t\t\t\t\t\t\tmetadata[field] = number.zfill(2)\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tmetadata[field] = number\n\n\t\t\t\t\t\tpart = part.replace(\n\t\t\t\t\t\t\tkey,\n\t\t\t\t\t\t\tlist_to_single_value(\n\t\t\t\t\t\t\t\tmetadata[field]\n\t\t\t\t\t\t\t)\n\t\t\t\t\t\t)\n\n\t\t\t\tparts.append(_replace_invalid_characters(part))\n\n\t\tfilepath = Path(*parts)\n\telif '%suggested%' in template:\n\t\tfilepath = Path(template.replace('%suggested%', suggested_filename))\n\telif template.endswith(('/', '\\\\')):\n\t\tfilepath = path / suggested_filename\n\telse:\n\t\tfilepath = path\n\n\treturn filepath"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _match_field(field_value, pattern, ignore_case=False, normalize_values=False):\n\n\tif normalize_values:\n\t\tignore_case = True\n\n\tnormalize = normalize_value if normalize_values else lambda x: str(x)\n\tsearch = functools.partial(re.search, flags=re.I) if ignore_case else re.search\n\n\t# audio_metadata fields contain a list of values.\n\tif isinstance(field_value, list):\n\t\treturn any(search(pattern, normalize(value)) for value in field_value)\n\telse:\n\t\treturn search(pattern, normalize(field_value))", "response": "Match an item metadata field value by pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmatches items by metadata.", "response": "def _match_item(item, any_all=any, ignore_case=False, normalize_values=False, **kwargs):\n\t\"\"\"Match items by metadata.\n\n\tNote:\n\t\tMetadata values are lowercased when ``normalized_values`` is ``True``,\n\t\tso ``ignore_case`` is automatically set to ``True``.\n\n\tParameters:\n\t\titem (~collections.abc.Mapping, str, os.PathLike): Item dict or filepath.\n\t\tany_all (callable): A callable to determine if any or all filters must match to match item.\n\t\t\tExpected values :obj:`any` (default) or :obj:`all`.\n\t\tignore_case (bool): Perform case-insensitive matching.\n\t\t\tDefault: ``False``\n\t\tnormalize_values (bool): Normalize metadata values to remove common differences between sources.\n\t\t\tDefault: ``False``\n\t\tkwargs (list): Lists of values to match the given metadata field.\n\n\tReturns:\n\t\tbool: True if matched, False if not.\n\t\"\"\"\n\n\tit = get_item_tags(item)\n\n\treturn any_all(\n\t\t_match_field(\n\t\t\tget_field(it, field), pattern, ignore_case=ignore_case, normalize_values=normalize_values\n\t\t) for field, patterns in kwargs.items() for pattern in patterns\n\t)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexclude items from a list of dicts or filepaths.", "response": "def exclude_items(items, any_all=any, ignore_case=False, normalize_values=False, **kwargs):\n\t\"\"\"Exclude items by matching metadata.\n\n\tNote:\n\t\tMetadata values are lowercased when ``normalized_values`` is ``True``,\n\t\tso ``ignore_case`` is automatically set to ``True``.\n\n\tParameters:\n\t\titems (list): A list of item dicts or filepaths.\n\t\tany_all (callable): A callable to determine if any or all filters must match to exclude items.\n\t\t\tExpected values :obj:`any` (default) or :obj:`all`.\n\t\tignore_case (bool): Perform case-insensitive matching.\n\t\t\tDefault: ``False``\n\t\tnormalize_values (bool): Normalize metadata values to remove common differences between sources.\n\t\t\tDefault: ``False``\n\t\tkwargs (list): Lists of values to match the given metadata field.\n\n\tYields:\n\t\tdict: The next item to be included.\n\n\tExample:\n\t\t>>> from google_music_utils import exclude_items\n\t\t>>> list(exclude_items(song_list, any_all=all, ignore_case=True, normalize_values=True, artist=['Beck'], album=['Golden Feelings']))\n\t\"\"\"\n\n\tif kwargs:\n\t\tmatch = functools.partial(\n\t\t\t_match_item, any_all=any_all, ignore_case=ignore_case, normalize_values=normalize_values, **kwargs\n\t\t)\n\n\t\treturn filterfalse(match, items)\n\telse:\n\t\treturn iter(items)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef include_items(items, any_all=any, ignore_case=False, normalize_values=False, **kwargs):\n\n\tif kwargs:\n\t\tmatch = functools.partial(\n\t\t\t_match_item, any_all=any_all, ignore_case=ignore_case, normalize_values=normalize_values, **kwargs\n\t\t)\n\n\t\treturn filter(match, items)\n\telse:\n\t\treturn iter(items)", "response": "Include items by matching metadata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the qth percentile of the data along the specified axis.", "response": "def percentile(a, q):\n    \"\"\"\n    Compute the qth percentile of the data along the specified axis.\n    Simpler version than the numpy version that always flattens input arrays.\n\n    Examples\n    --------\n    >>> a = [[10, 7, 4], [3, 2, 1]]\n    >>> percentile(a, 20)\n    2.0\n    >>> percentile(a, 50)\n    3.5\n    >>> percentile(a, [20, 80])\n    [2.0, 7.0]\n    >>> a = list(range(40))\n    >>> percentile(a, 25)\n    9.75\n\n    :param a: Input array or object that can be converted to an array.\n    :param q: Percentile to compute, which must be between 0 and 100 inclusive.\n    :return: the qth percentile(s) of the array elements.\n    \"\"\"\n    if not a:\n        return None\n\n    if isinstance(q, (float, int)):\n        qq = [q]\n    elif isinstance(q, (tuple, list)):\n        qq = q\n    else:\n        raise ValueError(\"Quantile type {} not understood\".format(type(q)))\n\n    if isinstance(a, (float, int)):\n        a = [a]\n\n    for i in range(len(qq)):\n        if qq[i] < 0. or qq[i] > 100.:\n            raise ValueError(\"Percentiles must be in the range [0,100]\")\n        qq[i] /= 100.\n\n    a = sorted(flatten(a))\n    r = []\n\n    for q in qq:\n        k = (len(a) - 1) * q\n        f = math.floor(k)\n        c = math.ceil(k)\n        if f == c:\n            r.append(float(a[int(k)]))\n            continue\n        d0 = a[int(f)] * (c - k)\n        d1 = a[int(c)] * (k - f)\n\n        r.append(float(d0 + d1))\n\n    if len(r) == 1:\n        return r[0]\n    return r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def get(cls, websession, lat, lon):\n\n        self = Station(websession)\n        \n        stations = await self.api.stations()\n\n        self.station = self._filter_closest(lat, lon, stations)\n\n        logger.info(\"Using %s as weather station\", self.station.local)\n\n        return self", "response": "Retrieve the nearest station."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving next 5 days forecast.", "response": "async def forecast(self):\n        \"\"\"Retrieve next 5 days forecast.\"\"\"\n\n        _forecasts = await self.api.forecast(self.station.globalIdLocal)\n\n        return _forecasts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve current weather observation.", "response": "async def observation(self):\n        \"\"\"Retrieve current weather observation.\"\"\"\n\n        try:\n            observations = await self.api.observations()\n        except:\n            return self._last_observation\n\n        closest = self._filter_closest(self.station.latitude,\n                                       self.station.longitude,\n                                       observations)\n\n        if closest is not None:\n            self._last_observation = closest.currentObs\n             \n        return self._last_observation"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_channels(self):\n        channels = []\n\n        # Try to get channels\n        for channel_name in self.channel_names:\n            channel_path = os.path.join(self.path, \"channels\")\n            sys.path.append(self.path)\n            mod = imp.load_module(channel_name, *imp.find_module(channel_name, [channel_path]))\n            cls = getattr(mod, channel_name.title().replace(\"_\", \"\"))\n            channel_id = channel_name.split(\"_\")[0]\n            # TODO: what about up_to_timestamp?\n            try:\n                channels.append(cls(channel_id, up_to_timestamp=None))\n            except TypeError:\n                channels.append(cls(channel_id))\n\n        # Try to get tools\n        if self.has_tools:\n            tool_path = os.path.join(self.path, \"tools\")\n            # Create a tool channel using this path\n            channel_id = self.channel_id_prefix + \"_\" + \"tools\"\n            channel = ToolChannel(channel_id, tool_path, up_to_timestamp=utcnow())\n            channels.append(channel)\n\n        if self.has_assets:\n            assset_path = os.path.join(os.path.abspath(self.path), \"assets\")\n            channel_id = self.channel_id_prefix + \"_\" + \"assets\"\n            channel = AssetsFileChannel(channel_id, assset_path, up_to_timestamp=utcnow())\n            channels.append(channel)\n            #\n            # from . import TimeInterval\n            # channel.streams.values()[0].window(TimeInterval.up_to_now()).items()\n\n        return channels", "response": "Loads the channels and tools given the plugin path specified\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chunk_count(swatch):\n    if type(swatch) is dict:\n        if 'data' in swatch:\n            return 1\n        if 'swatches' in swatch:\n            return 2 + len(swatch['swatches'])\n    else:\n        return sum(map(chunk_count, swatch))", "response": "return the number of byte - chunks in a swatch object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild up a byte - chunk for a color object", "response": "def chunk_for_color(obj):\n    \"\"\"builds up a byte-chunk for a color\n\n    the format for this is\n        b'\\x00\\x01' +\n        Big-Endian Unsigned Int == len(bytes that follow in this block)\n          \u2022 Big-Endian Unsigned Short == len(color_name)\n              in practice, because utf-16 takes up 2 bytes per letter\n              this will be 2 * (len(name) + 1)\n              so a color named 'foo' would be 8 bytes long\n          \u2022 UTF-16BE Encoded color_name terminated with '\\0'\n              using 'foo', this yields '\\x00f\\x00o\\x00o\\x00\\x00'\n          \u2022 A 4-byte char for Color mode ('RGB ', 'Gray', 'CMYK', 'LAB ')\n              note the trailing spaces\n          \u2022 a variable-length number of 4-byte length floats\n              this depends entirely on the color mode of the color.\n          \u2022 A Big-Endian short int for either a global, spot, or process color\n              global == 0, spot == 1, process == 2\n\n    the chunk has no terminating string although other sites have indicated\n    that the global/spot/process short is a terminator, it's actually used\n    to indicate how illustrator should deal with the color.\n    \"\"\"\n    title = obj['name'] + '\\0'\n    title_length = len(title)\n    chunk = struct.pack('>H', title_length)\n    chunk += title.encode('utf-16be')\n\n    mode = obj['data']['mode'].encode()\n    values = obj['data']['values']\n    color_type = obj['type']\n\n    fmt = {b'RGB': '!fff', b'Gray': '!f', b'CMYK': '!ffff', b'LAB': '!fff'}\n    if mode in fmt:\n        padded_mode = mode.decode().ljust(4).encode()\n        chunk += struct.pack('!4s', padded_mode) # the color mode\n        chunk += struct.pack(fmt[mode], *values) # the color values\n\n    color_types = ['Global', 'Spot', 'Process']\n    if color_type in color_types:\n        color_int = color_types.index(color_type)\n        chunk += struct.pack('>h', color_int) # append swatch mode\n\n    chunk = struct.pack('>I', len(chunk)) + chunk # prepend the chunk size\n    return b'\\x00\\x01' + chunk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproduce a byte - chunk for a folder of colors", "response": "def chunk_for_folder(obj):\n    \"\"\"produce a byte-chunk for a folder of colors\n\n    the structure is very similar to a color's data:\n    \u2022 Header\n        b'\\xC0\\x01' +\n        Big Endian Unsigned Int == len(Bytes in the Header Block)\n          note _only_ the header, this doesn't include the length of color data\n          \u2022 Big Endian Unsigned Short == len(Folder Name + '\\0')\n              Note that Folder Name is assumed to be utf-16be so this\n              will always be an even number\n          \u2022 Folder Name + '\\0', encoded UTF-16BE\n    \u2022 body\n        chunks for each color, see chunk_for_color\n    \u2022 folder terminator\n        b'\\xC0\\x02' +\n        b'\\x00\\x00\\x00\\x00'\n\n    Perhaps the four null bytes represent something, but i'm pretty sure\n    they're just a terminating string, but there's something nice about\n    how the b'\\xC0\\x02' matches with the folder's header\n    \"\"\"\n    title = obj['name'] + '\\0'\n    title_length = len(title)\n    chunk_body = struct.pack('>H', title_length) # title length\n    chunk_body += title.encode('utf-16be') # title\n\n    chunk_head = b'\\xC0\\x01' # folder header\n    chunk_head += struct.pack('>I', len(chunk_body))\n    # precede entire chunk by folder header and size of folder\n    chunk = chunk_head + chunk_body\n\n    chunk += b''.join([chunk_for_color(c) for c in obj['swatches']])\n\n    chunk += b'\\xC0\\x02' # folder terminator chunk\n    chunk += b'\\x00\\x00\\x00\\x00' # folder terminator\n    return chunk"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\niterate over screen names in a file one per line.", "response": "def iter_lines(filename):\n    \"\"\" Iterate over screen names in a file, one per line.\"\"\"\n    with open(filename, 'rt') as idfile:\n        for line in idfile:\n            screen_name = line.strip()\n            if len(screen_name) > 0:\n                yield screen_name.split()[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching up to limit followers for each Twitter account in account_file. Write out a gzipped file of the complete list of followers.", "response": "def fetch_followers(account_file, outfile, limit, do_loop):\n    \"\"\" Fetch up to limit followers for each Twitter account in\n    account_file. Write results to outfile file in format:\n\n    screen_name user_id follower_id_1 follower_id_2 ...\"\"\"\n    print('Fetching followers for accounts in %s' % account_file)\n    niters = 1\n    while True:\n        outf = gzip.open(outfile, 'wt')\n        for screen_name in iter_lines(account_file):\n            timestamp = datetime.datetime.now().isoformat()\n            print('collecting followers for', screen_name)\n            followers = twutil.collect.followers_for_screen_name(screen_name, limit)\n            if len(followers) > 0:\n                outf.write('%s %s %s\\n' % (timestamp, screen_name, ' '.join(followers)))\n                outf.flush()\n            else:\n                print('unknown user', screen_name)\n        outf.close()\n        if not do_loop:\n            return\n        else:\n            if niters == 1:\n                outfile = '%s.%d' % (outfile, niters)\n            else:\n                outfile = outfile[:outfile.rindex('.')] + '.%d' % niters\n            niters += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching up to limit tweets for each account in account_file and write tooutfile.", "response": "def fetch_tweets(account_file, outfile, limit):\n    \"\"\" Fetch up to limit tweets for each account in account_file and write to\n    outfile. \"\"\"\n    print('fetching tweets for accounts in', account_file)\n    outf = io.open(outfile, 'wt')\n    for screen_name in iter_lines(account_file):\n        print('\\nFetching tweets for %s' % screen_name)\n        for tweet in twutil.collect.tweets_for_user(screen_name, limit):\n            tweet['user']['screen_name'] = screen_name\n            outf.write('%s\\n' % json.dumps(tweet, ensure_ascii=False))\n            outf.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfetches the urls of up to max_results Twitter lists that match the provided keyword.", "response": "def fetch_lists(keyword,max_results=20):\n    \"\"\"\n    Fetch the urls of up to max_results Twitter lists that match the provided keyword.\n    >>> len(fetch_lists('politics', max_results=4))\n    4\n    \"\"\"\n    #CONFIG FILE READ\n    api_key=config.get('GOOGLE_CSE_KEYS','API_KEY')\n    cse_id=config.get('GOOGLE_CSE_KEYS','CSE_ID')\n\n    results = []\n    start_c = 1\n    search_term = \"inurl:lists + \"+keyword\n    while len(results)<max_results:\n        temp_res = google_search(search_term,api_key,cse_id,num=10,start=start_c)\n        if len(temp_res) == 0:\n            print(\"Google API Error, returning retrieved results\")\n            return results\n        results.extend(temp_res)\n        start_c += 10\n    return results[:max_results]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all members of the list specified by the given url.", "response": "def fetch_list_members(list_url):\n    \"\"\" Get all members of the list specified by the given url. E.g., https://twitter.com/lore77/lists/libri-cultura-education \"\"\"\n    match = re.match(r'.+twitter\\.com\\/(.+)\\/lists\\/(.+)', list_url)\n    if not match:\n        print('cannot parse list url %s' % list_url)\n        return []\n    screen_name, slug = match.groups()\n    print('collecting list %s/%s' % (screen_name, slug))\n    return twutil.collect.list_members(slug, screen_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch all exemplars matching this keyword then return Twitter screen names along with the number of different lists on which each appers are.", "response": "def fetch_exemplars(keyword, outfile, n=50):\n    \"\"\" Fetch top lists matching this keyword, then return Twitter screen\n    names along with the number of different lists on which each appers.. \"\"\"\n    list_urls = fetch_lists(keyword, n)\n    print('found %d lists for %s' % (len(list_urls), keyword))\n    counts = Counter()\n    for list_url in list_urls:\n        counts.update(fetch_list_members(list_url))\n    # Write to file.\n    outf = io.open(outfile, 'wt')\n    for handle in sorted(counts):\n        outf.write('%s\\t%d\\n' % (handle, counts[handle]))\n    outf.close()\n    print('saved exemplars to', outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_controlhost(self):\n        log.debug(\"Connecting to JLigier\")\n        self.client = Client(self.host, self.port)\n        self.client._connect()\n        log.debug(\"Subscribing to tags: {0}\".format(self.tags))\n        for tag in self.tags.split(','):\n            self.client.subscribe(tag.strip(), mode=self.subscription_mode)\n        log.debug(\"Controlhost initialisation done.\")", "response": "Initialize the controlhost connection"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process(self, blob):\n        # self._add_process_dt()\n        try:\n            log.debug(\"Waiting for queue items.\")\n            prefix, data = self.queue.get(timeout=self.timeout)\n            log.debug(\"Got {0} bytes from queue.\".format(len(data)))\n        except Empty:\n            log.warning(\n                \"ControlHost timeout ({0}s) reached\".format(self.timeout)\n            )\n            raise StopIteration(\"ControlHost timeout reached.\")\n        blob[self.key_for_prefix] = prefix\n        blob[self.key_for_data] = data\n        return blob", "response": "Wait for next packet and put it in the blob."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean up the JLigier controlhost connection", "response": "def finish(self):\n        \"\"\"Clean up the JLigier controlhost connection\"\"\"\n        log.debug(\"Disconnecting from JLigier.\")\n        self.client.socket.shutdown(socket.SHUT_RDWR)\n        self.client._disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tokenizer(self):\n        profile = self.dir / 'etc' / 'orthography.tsv'\n        if profile.exists():\n            profile = Profile.from_file(str(profile), form='NFC')\n            default_spec = list(next(iter(profile.graphemes.values())).keys())\n            for grapheme in ['^', '$']:\n                if grapheme not in profile.graphemes:\n                    profile.graphemes[grapheme] = {k: None for k in default_spec}\n            profile.tree = Tree(list(profile.graphemes.keys()))\n            tokenizer = Tokenizer(profile=profile, errors_replace=lambda c: '<{0}>'.format(c))\n\n            def _tokenizer(item, string, **kw):\n                kw.setdefault(\"column\", \"IPA\")\n                kw.setdefault(\"separator\", \" + \")\n                return tokenizer(unicodedata.normalize('NFC', '^' + string + '$'), **kw).split()\n            return _tokenizer", "response": "A custom tokenizer for segmentation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_variables(self):\n        station_codes = self._get_station_codes()\n        station_codes = self._apply_features_filter(station_codes)\n        variables = self._list_variables(station_codes)\n\n        if hasattr(self, \"_variables\") and self.variables is not None:\n            variables.intersection_update(set(self.variables))\n\n        return list(variables)", "response": "List available variables and applies any filters."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a tuple of metadata and raw data for the current locale.", "response": "def raw(self, format=None, **kwargs):\n        \"\"\"\n        Returns a tuple of (metadata, raw data)\n        \"\"\"\n        station_codes = self._apply_features_filter(self._get_station_codes())\n        metadata = self._get_metadata(station_codes, **kwargs)\n        raw_data = self._get_raw_data(station_codes, **kwargs)\n\n        return (metadata, raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_station_codes(self, force=False):\n        if not force and self.station_codes is not None:\n            return self.station_codes\n\n        state_urls = self._get_state_urls()\n\n        # filter by bounding box against a shapefile\n        state_matches = None\n\n        if self.bbox:\n            with collection(\n                os.path.join(\n                    \"resources\",\n                    \"ne_50m_admin_1_states_provinces_lakes_shp.shp\",\n                ),\n                \"r\",\n            ) as c:\n                geom_matches = [\n                    x[\"properties\"] for x in c.filter(bbox=self.bbox)\n                ]\n                state_matches = [\n                    x[\"postal\"] if x[\"admin\"] != \"Canada\" else \"CN\"\n                    for x in geom_matches\n                ]\n\n        self.station_codes = []\n\n        for state_url in state_urls:\n            if state_matches is not None:\n                state_abbr = state_url.split(\"/\")[-1].split(\".\")[0]\n                if state_abbr not in state_matches:\n                    continue\n\n            self.station_codes.extend(self._get_stations_for_state(state_url))\n\n        if self.bbox:\n            # retrieve metadata for all stations to properly filter them\n            metadata = self._get_metadata(self.station_codes)\n            parsed_metadata = self.parser._parse_metadata(metadata)\n\n            def in_bbox(code):\n                lat = parsed_metadata[code][\"latitude\"]\n                lon = parsed_metadata[code][\"longitude\"]\n\n                return (\n                    lon >= self.bbox[0]\n                    and lon <= self.bbox[2]\n                    and lat >= self.bbox[1]\n                    and lat <= self.bbox[3]\n                )\n\n            self.station_codes = list(filter(in_bbox, self.station_codes))\n\n        return self.station_codes", "response": "Gets and caches a list of station codes optionally within a bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap that handles the actual asynchronous monitoring of the task.", "response": "def _monitor_task(self):\n        \"\"\"Wrapper that handles the actual asynchronous monitoring of the task\n        state.\n\n        \"\"\"\n        if self.task.state in states.UNREADY_STATES:\n            reactor.callLater(self.POLL_PERIOD, self._monitor_task)\n            return\n\n        if self.task.state == 'SUCCESS':\n            self.callback(self.task.result)\n        elif self.task.state == 'FAILURE':\n            self.errback(Failure(self.task.result))\n        elif self.task.state == 'REVOKED':\n            self.errback(\n                Failure(defer.CancelledError('Task {0}'.format(self.task.id))))\n        else:\n            self.errback(ValueError(\n                'Cannot respond to `{}` state'.format(self.task.state)\n            ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning up a query string for searching.", "response": "def _clean_query_string(q):\n    \"\"\"Clean up a query string for searching.\n\n    Removes unmatched parentheses and joining operators.\n\n    Arguments:\n        q (str): Query string to be cleaned\n\n    Returns:\n        str: The clean query string.\n    \"\"\"\n    q = q.replace(\"()\", \"\").strip()\n    if q.endswith(\"(\"):\n        q = q[:-1].strip()\n    # Remove misplaced AND/OR/NOT at end\n    if q[-3:] == \"AND\" or q[-3:] == \"NOT\":\n        q = q[:-3]\n    elif q[-2:] == \"OR\":\n        q = q[:-2]\n\n    # Balance parentheses\n    while q.count(\"(\") > q.count(\")\"):\n        q += \")\"\n    while q.count(\")\") > q.count(\"(\"):\n        q = \"(\" + q\n\n    return q.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates and clean up a query string.", "response": "def _validate_query(query):\n    \"\"\"Validate and clean up a query to be sent to Search.\n    Cleans the query string, removes unneeded parameters, and validates for correctness.\n    Does not modify the original argument.\n    Raises an Exception on invalid input.\n\n    Arguments:\n        query (dict): The query to validate.\n\n    Returns:\n        dict: The validated query.\n    \"\"\"\n    query = deepcopy(query)\n    # q is always required\n    if query[\"q\"] == BLANK_QUERY[\"q\"]:\n        raise ValueError(\"No query specified.\")\n\n    query[\"q\"] = _clean_query_string(query[\"q\"])\n\n    # limit should be set to appropriate default if not specified\n    if query[\"limit\"] is None:\n        query[\"limit\"] = SEARCH_LIMIT if query[\"advanced\"] else NONADVANCED_LIMIT\n    # If specified, the limit should not be greater than the Search maximum\n    elif query[\"limit\"] > SEARCH_LIMIT:\n        warnings.warn('Reduced result limit from {} to the Search maximum: {}'\n                      .format(query[\"limit\"], SEARCH_LIMIT), RuntimeWarning)\n        query[\"limit\"] = SEARCH_LIMIT\n\n    # Remove all blank/default values\n    for key, val in BLANK_QUERY.items():\n        # Default for get is NaN so comparison is always False\n        if query.get(key, float('nan')) == val:\n            query.pop(key)\n\n    # Remove unsupported fields\n    to_remove = [field for field in query.keys() if field not in BLANK_QUERY.keys()]\n    [query.pop(field) for field in to_remove]\n\n    return query"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a term to the query.", "response": "def _term(self, term):\n        \"\"\"Add a term to the query.\n\n        Arguments:\n            term (str): The term to add.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        # All terms must be strings for Elasticsearch\n        term = str(term)\n        if term:\n            self.__query[\"q\"] += term\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _field(self, field, value):\n        # Fields and values must be strings for Elasticsearch\n        field = str(field)\n        value = str(value)\n\n        # Check if quotes required and allowed, and quotes not present\n        # If the user adds improper double-quotes, this will not fix them\n        if (any([char in value for char in QUOTE_LIST]) and '\"' not in value\n                and not any([char in value for char in UNQUOTE_LIST])):\n            value = '\"' + value + '\"'\n\n        # Cannot add field:value if one is blank\n        if field and value:\n            self.__query[\"q\"] += field + \":\" + value\n            # Field matches are advanced queries\n            self.__query[\"advanced\"] = True\n\n        return self", "response": "Add a field value term to the query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _operator(self, op, close_group=False):\n        op = op.upper().strip()\n        if op not in OP_LIST:\n            raise ValueError(\"Error: '{}' is not a valid operator.\".format(op))\n        else:\n            if close_group:\n                op = \") \" + op + \" (\"\n            else:\n                op = \" \" + op + \" \"\n            self.__query[\"q\"] += op\n        return self", "response": "Add an operator between terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _and_join(self, close_group=False):\n        if not self.initialized:\n            raise ValueError(\"You must add a search term before adding an operator.\")\n        else:\n            self._operator(\"AND\", close_group=close_group)\n        return self", "response": "Combine terms with AND."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _or_join(self, close_group=False):\n        if not self.initialized:\n            raise ValueError(\"You must add a search term before adding an operator.\")\n        else:\n            self._operator(\"OR\", close_group=close_group)\n        return self", "response": "Combine terms with OR."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_sort(self, field, ascending=True):\n        # Fields must be strings for Elasticsearch\n        field = str(field)\n        # No-op on blank sort field\n        if field:\n            self.__query[\"sort\"].append({\n                'field_name': field,\n                'order': 'asc' if ascending else 'desc'\n            })\n        return self", "response": "Add a sort to the search results."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a search and return the results.", "response": "def _ex_search(self, limit=None, info=False, retries=3):\n        \"\"\"Execute a search and return the results, up to the ``SEARCH_LIMIT``.\n\n        Uses the query currently in this SearchHelper.\n\n        Arguments:\n            limit (int): Maximum number of entries to return. **Default**: ``10`` for basic\n                queries, and ``10000`` for advanced.\n            info (bool): If ``False``, search will return a list of the results.\n                    If ``True``, search will return a tuple containing the results list\n                    and other information about the query.\n                    **Default:** ``False``.\n            retries (int): The number of times to retry a Search query if it fails.\n                           **Default:** 3.\n\n        Returns:\n            If ``info`` is ``False``, *list*: The search results.\n            If ``info`` is ``True``, *tuple*: The search results,\n            and a dictionary of query information.\n        \"\"\"\n        # Make sure there is query information present\n        if not self.initialized:\n            raise ValueError('No query has been set.')\n\n        # Create Search-ready query\n        if limit is not None:\n            self.__query[\"limit\"] = limit\n        query = _validate_query(self.__query)\n\n        tries = 0\n        errors = []\n        while True:\n            # Try searching until success or `retries` number of failures\n            # Raise exception after `retries` failures\n            try:\n                search_res = self.__search_client.post_search(self.index, query)\n            except globus_sdk.SearchAPIError as e:\n                if tries >= retries:\n                    raise\n                else:\n                    errors.append(repr(e))\n            except Exception as e:\n                if tries >= retries:\n                    raise\n                else:\n                    errors.append(repr(e))\n            else:\n                break\n            tries += 1\n\n        # Remove the wrapping on each entry from Globus search\n        res = mdf_toolbox.gmeta_pop(search_res, info=info)\n\n        # Add more information to output if requested\n        if info:\n            # Add everything from the query itself\n            info_dict = mdf_toolbox.dict_merge(res[1], query)\n            # But rename \"q\" to \"query\" for clarity\n            info_dict[\"query\"] = info_dict.pop(\"q\")\n            # Add other useful/interesting parameters\n            info_dict[\"index_uuid\"] = self.index\n            info_dict[\"retries\"] = tries\n            info_dict[\"errors\"] = errors\n            # Remake tuple because tuples don't suport assignment\n            res = (res[0], info_dict)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _mapping(self):\n        return (self.__search_client.get(\n                    \"/unstable/index/{}/mapping\".format(mdf_toolbox.translate_index(self.index)))\n                [\"mappings\"])", "response": "Fetch the entire mapping for the specified index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a fulltext search term to the query.", "response": "def match_term(self, value, required=True, new_group=False):\n        \"\"\"Add a fulltext search term to the query.\n\n        Warning:\n            Do not use this method with any other query-building helpers. This method\n            is only for building fulltext queries (in non-advanced mode). Using other\n            helpers, such as ``match_field()``, will cause the query to run in advanced mode.\n            If a fulltext term query is run in advanced mode, it will have unexpected\n            results.\n\n        Arguments:\n            value (str): The term to match.\n            required (bool): If ``True``, will add term with ``AND``.\n                    If ``False``, will use ``OR``. **Default:** ``True``.\n            new_group (bool): If ``True``, will separate the term into a new parenthetical group.\n                    If ``False``, will not.\n                    **Default:** ``False``.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        # If not the start of the query string, add an AND or OR\n        if self.initialized:\n            if required:\n                self._and_join(new_group)\n            else:\n                self._or_join(new_group)\n        self._term(value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a field : value term to the query.", "response": "def match_field(self, field, value, required=True, new_group=False):\n        \"\"\"Add a ``field:value`` term to the query.\n        Matches will have the ``value`` in the ``field``.\n\n        Arguments:\n            field (str): The field to check for the value.\n                    The field must be namespaced according to Elasticsearch rules\n                    using the dot syntax.\n                    For example, ``\"mdf.source_name\"`` is the ``source_name`` field\n                    of the ``mdf`` dictionary.\n            value (str): The value to match.\n            required (bool): If ``True``, will add term with ``AND``.\n                    If ``False``, will use ``OR``. **Default:** ``True``.\n            new_group (bool): If ``True``, will separate the term into a new parenthetical group.\n                    If ``False``, will not.\n                    **Default:** ``False``.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        # If not the start of the query string, add an AND or OR\n        if self.initialized:\n            if required:\n                self._and_join(new_group)\n            else:\n                self._or_join(new_group)\n        self._field(field, value)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exclude_field(self, field, value, new_group=False):\n        # No-op on missing arguments\n        if not field and not value:\n            return self\n        # If not the start of the query string, add an AND\n        # OR would not make much sense for excluding\n        if self.initialized:\n            self._and_join(new_group)\n        self._negate()._field(str(field), str(value))\n        return self", "response": "Exclude a field value term from the query."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if a field exists in the results.", "response": "def match_exists(self, field, required=True, new_group=False):\n        \"\"\"Require a field to exist in the results.\n        Matches will have some value in ``field``.\n\n        Arguments:\n            field (str): The field to check.\n                    The field must be namespaced according to Elasticsearch rules\n                    using the dot syntax.\n                    For example, ``\"mdf.source_name\"`` is the ``source_name`` field\n                    of the ``mdf`` dictionary.\n            required (bool): If ``True``, will add term with ``AND``.\n                    If ``False``, will use ``OR``. **Default:** ``True``.\n            new_group (bool): If ``True``, will separate the term into a new parenthetical group.\n                    If ``False``, will not.\n                    **Default:** ``False``.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        return self.match_field(field, \"*\", required=required, new_group=new_group)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_not_exists(self, field, new_group=False):\n        return self.exclude_field(field, \"*\", new_group=new_group)", "response": "Require a field to not exist in the results. Matches will not have field present."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_range(self, field, start=None, stop=None, inclusive=True,\n                    required=True, new_group=False):\n        \"\"\"Add a ``field:[some range]`` term to the query.\n        Matches will have a ``value`` in the range in the ``field``.\n\n        Arguments:\n            field (str): The field to check for the value.\n                    The field must be namespaced according to Elasticsearch rules\n                    using the dot syntax.\n                    For example, ``\"mdf.source_name\"`` is the ``source_name`` field\n                    of the ``mdf`` dictionary.\n            start (str or int): The starting value, or ``None`` for no lower bound.\n                    **Default:** ``None``.\n            stop (str or int): The ending value, or ``None`` for no upper bound.\n                    **Default:** ``None``.\n            inclusive (bool): If ``True``, the ``start`` and ``stop`` values will be included\n                    in the search.\n                    If ``False``, the start and stop values will not be included\n                    in the search.\n                    **Default:** ``True``.\n            required (bool): If ``True``, will add term with ``AND``.\n                    If ``False``, will use ``OR``. **Default:** ``True``.\n            new_group (bool): If ``True``, will separate the term into a new parenthetical group.\n                    If ``False``, will not.\n                    **Default:** ``False``.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        # Accept None as *\n        if start is None:\n            start = \"*\"\n        if stop is None:\n            stop = \"*\"\n        # *-* is the same as field exists\n        if start == \"*\" and stop == \"*\":\n            return self.match_exists(field, required=required, new_group=new_group)\n\n        if inclusive:\n            value = \"[\" + str(start) + \" TO \" + str(stop) + \"]\"\n        else:\n            value = \"{\" + str(start) + \" TO \" + str(stop) + \"}\"\n        return self.match_field(field, value, required=required, new_group=new_group)", "response": "This method will add a term to the query that matches a value in a range of values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexclude a field from the search.", "response": "def exclude_range(self, field, start=\"*\", stop=\"*\", inclusive=True, new_group=False):\n        \"\"\"Exclude a ``field:[some range]`` term from the query.\n        Matches will not have any ``value`` in the range in the ``field``.\n\n        Arguments:\n            field (str): The field to check for the value.\n                    The field must be namespaced according to Elasticsearch rules\n                    using the dot syntax.\n                    For example, ``\"mdf.source_name\"`` is the ``source_name`` field\n                    of the ``mdf`` dictionary.\n            start (str or int): The starting value, or ``None`` for no lower bound.\n                    **Default:** ``None``.\n            stop (str or int): The ending value, or ``None`` for no upper bound.\n                    **Default:** ``None``.\n            inclusive (bool): If ``True``, the ``start`` and ``stop`` values will be excluded\n                    from the search.\n                    If ``False``, the ``start`` and ``stop`` values will not be excluded\n                    from the search.\n                    **Default:** ``True``.\n            new_group (bool): If ``True``, will separate the term into a new parenthetical group.\n                    If ``False``, will not.\n                    **Default:** ``False``.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        # Accept None as *\n        if start is None:\n            start = \"*\"\n        if stop is None:\n            stop = \"*\"\n        # *-* is the same as field doesn't exist\n        if start == \"*\" and stop == \"*\":\n            return self.match_not_exists(field, new_group=new_group)\n\n        if inclusive:\n            value = \"[\" + str(start) + \" TO \" + str(stop) + \"]\"\n        else:\n            value = \"{\" + str(start) + \" TO \" + str(stop) + \"}\"\n        return self.exclude_field(field, value, new_group=new_group)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch exactly the given value with no other data in the field.", "response": "def exclusive_match(self, field, value):\n        \"\"\"Match exactly the given value(s), with no other data in the field.\n\n        Arguments:\n            field (str): The field to check for the value.\n                    The field must be namespaced according to Elasticsearch rules\n                    using the dot syntax.\n                    For example, ``\"mdf.source_name\"`` is the ``source_name`` field\n                    of the ``mdf`` dictionary.\n            value (str or list of str): The value(s) to match exactly.\n\n        Returns:\n            SearchHelper: Self\n        \"\"\"\n        if isinstance(value, str):\n            value = [value]\n\n        # Hacky way to get ES to do exclusive search\n        # Essentially have a big range search that matches NOT anything\n        # Except for the actual values\n        # Example: [foo, bar, baz] =>\n        #   (NOT {* TO foo} AND [foo TO foo] AND NOT {foo to bar} AND [bar TO bar]\n        #    AND NOT {bar TO baz} AND [baz TO baz] AND NOT {baz TO *})\n        # Except it must be sorted to not overlap\n        value.sort()\n\n        # Start with removing everything before first value\n        self.exclude_range(field, \"*\", value[0], inclusive=False, new_group=True)\n        # Select first value\n        self.match_range(field, value[0], value[0])\n        # Do the rest of the values\n        for index, val in enumerate(value[1:]):\n            self.exclude_range(field, value[index-1], val, inclusive=False)\n            self.match_range(field, val, val)\n        # Add end\n        self.exclude_range(field, value[-1], \"*\", inclusive=False)\n        # Done\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_sort(self, field, ascending=True):\n        # No-op on blank field\n        if not field:\n            return self\n        self._add_sort(field, ascending=ascending)\n        return self", "response": "Sort the search results by a certain field."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a search and return the results.", "response": "def search(self, q=None, advanced=False, limit=None, info=False, reset_query=True):\n        \"\"\"Execute a search and return the results, up to the ``SEARCH_LIMIT``.\n\n        Arguments:\n            q (str): The query to execute. **Default:** The current helper-formed query, if any.\n                    There must be some query to execute.\n            advanced (bool): Whether to treat ``q`` as a basic or advanced query.\n                Has no effect if a query is not supplied in ``q``.\n                **Default:** ``False``\n            limit (int): The maximum number of results to return.\n                    The max for this argument is the ``SEARCH_LIMIT`` imposed by Globus Search.\n                    **Default:** ``SEARCH_LIMIT`` for advanced queries, 10 for basic queries.\n            info (bool): If ``False``, search will return a list of the results.\n                    If ``True``, search will return a tuple containing the results list\n                    and other information about the query.\n                    **Default:** ``False``.\n            reset_query (bool): If ``True``, will destroy the current query after execution\n                    and start a fresh one.\n                    If ``False``, will keep the current query set.\n                    Has no effect if a query is supplied in ``q``.\n                    **Default:** ``True``.\n\n        Returns:\n            If ``info`` is ``False``, *list*: The search results.\n            If ``info`` is ``True``, *tuple*: The search results,\n            and a dictionary of query information.\n\n        Note:\n            If a query is specified in ``q``, the current, helper-built query (if any)\n            will not be used in the search or modified.\n        \"\"\"\n        # If q not specified, use internal, helper-built query\n        if q is None:\n            res = self._ex_search(info=info, limit=limit)\n            if reset_query:\n                self.reset_query()\n            return res\n        # If q was specified, run a totally independent query with a new SearchHelper\n        # Init SearchHelper with query, then call .search(), which will use it\n        # ._ex_search() not canonical way to perform single-statement search, so not used\n        # reset_query is False to skip the unnecessary query reset - SH not needed after search\n        else:\n            return SearchHelper(index=self.index, search_client=self.__search_client, q=q,\n                                advanced=advanced).search(info=info, limit=limit,\n                                                          reset_query=False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve and return the mapping for the given metadata block.", "response": "def show_fields(self, block=None):\n        \"\"\"Retrieve and return the mapping for the given metadata block.\n\n        Arguments:\n            block (str): The top-level field to fetch the mapping for (for example, ``\"mdf\"``),\n                    or the special values ``None`` for everything or ``\"top\"`` for just the\n                    top-level fields.\n                    **Default:** ``None``.\n            index (str): The Search index to map. **Default:** The current index.\n\n        Returns:\n            dict: ``field:datatype`` pairs.\n        \"\"\"\n        mapping = self._mapping()\n        if block is None:\n            return mapping\n        elif block == \"top\":\n            blocks = set()\n            for key in mapping.keys():\n                blocks.add(key.split(\".\")[0])\n            block_map = {}\n            for b in blocks:\n                block_map[b] = \"object\"\n        else:\n            block_map = {}\n            for key, value in mapping.items():\n                if key.startswith(block):\n                    block_map[key] = value\n        return block_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inflate_dtype(arr, names):\n    arr = np.asanyarray(arr)\n    if has_structured_dt(arr):\n        return arr.dtype\n    s_dt = arr.dtype\n    dt = [(n, s_dt) for n in names]\n    dt = np.dtype(dt)\n    return dt", "response": "Create structured dtype from a 2d ndarray with unstructured dtype."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a table from a dictionary of arrays.", "response": "def from_dict(cls, arr_dict, dtype=None, fillna=False, **kwargs):\n        \"\"\"Generate a table from a dictionary of arrays.\n        \"\"\"\n        # i hope order of keys == order or values\n        if dtype is None:\n            names = sorted(list(arr_dict.keys()))\n        else:\n            dtype = np.dtype(dtype)\n            dt_names = [f for f in dtype.names]\n            dict_names = [k for k in arr_dict.keys()]\n            missing_names = set(dt_names) - set(dict_names)\n            if missing_names:\n                if fillna:\n                    dict_names = dt_names\n                    for missing_name in missing_names:\n                        arr_dict[missing_name] = np.nan\n                else:\n                    raise KeyError(\n                        'Dictionary keys and dtype fields do not match!'\n                    )\n            names = list(dtype.names)\n\n        arr_dict = cls._expand_scalars(arr_dict)\n        data = [arr_dict[key] for key in names]\n        return cls(np.rec.fromarrays(data, names=names, dtype=dtype), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_template(cls, data, template):\n        name = DEFAULT_NAME\n        if isinstance(template, str):\n            name = template\n            table_info = TEMPLATES[name]\n        else:\n            table_info = template\n        if 'name' in table_info:\n            name = table_info['name']\n        dt = table_info['dtype']\n        loc = table_info['h5loc']\n        split = table_info['split_h5']\n        h5singleton = table_info['h5singleton']\n\n        return cls(\n            data,\n            h5loc=loc,\n            dtype=dt,\n            split_h5=split,\n            name=name,\n            h5singleton=h5singleton\n        )", "response": "Create a new instance of the class from a predefined datatype."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend new columns to the table.", "response": "def append_columns(self, colnames, values, **kwargs):\n        \"\"\"Append new columns to the table.\n\n        When appending a single column, ``values`` can be a scalar or an\n        array of either length 1 or the same length as this array (the one\n        it's appended to). In case of multiple columns, values must have\n        the shape ``list(arrays)``, and the dimension of each array\n        has to match the length of this array.\n\n        See the docs for ``numpy.lib.recfunctions.append_fields`` for an\n        explanation of the remaining options.\n        \"\"\"\n        n = len(self)\n        if np.isscalar(values):\n            values = np.full(n, values)\n\n        values = np.atleast_1d(values)\n        if not isinstance(colnames, str) and len(colnames) > 1:\n            values = np.atleast_2d(values)\n            self._check_column_length(values, n)\n\n        if values.ndim == 1:\n            if len(values) > n:\n                raise ValueError(\"New Column is longer than existing table!\")\n            elif len(values) > 1 and len(values) < n:\n                raise ValueError(\n                    \"New Column is shorter than existing table, \"\n                    \"but not just one element!\"\n                )\n            elif len(values) == 1:\n                values = np.full(n, values[0])\n        new_arr = rfn.append_fields(\n            self, colnames, values, usemask=False, asrecarray=True, **kwargs\n        )\n        return self.__class__(\n            new_arr,\n            h5loc=self.h5loc,\n            split_h5=self.split_h5,\n            name=self.name,\n            h5singleton=self.h5singleton\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndrop columns from the table.", "response": "def drop_columns(self, colnames, **kwargs):\n        \"\"\"Drop  columns from the table.\n\n        See the docs for ``numpy.lib.recfunctions.drop_fields`` for an\n        explanation of the remaining options.\n        \"\"\"\n        new_arr = rfn.drop_fields(\n            self, colnames, usemask=False, asrecarray=True, **kwargs\n        )\n        return self.__class__(\n            new_arr,\n            h5loc=self.h5loc,\n            split_h5=self.split_h5,\n            name=self.name,\n            h5singleton=self.h5singleton\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the array by a column.", "response": "def sorted(self, by, **kwargs):\n        \"\"\"Sort array by a column.\n\n        Parameters\n        ==========\n        by: str\n            Name of the columns to sort by(e.g. 'time').\n        \"\"\"\n        sort_idc = np.argsort(self[by], **kwargs)\n        return self.__class__(\n            self[sort_idc],\n            h5loc=self.h5loc,\n            split_h5=self.split_h5,\n            name=self.name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge a list of tables into a single table.", "response": "def merge(cls, tables, fillna=False):\n        \"\"\"Merge a list of tables\"\"\"\n        cols = set(itertools.chain(*[table.dtype.descr for table in tables]))\n\n        tables_to_merge = []\n        for table in tables:\n            missing_cols = cols - set(table.dtype.descr)\n\n            if missing_cols:\n                if fillna:\n                    n = len(table)\n                    n_cols = len(missing_cols)\n                    col_names = []\n                    for col_name, col_dtype in missing_cols:\n                        if 'f' not in col_dtype:\n                            raise ValueError(\n                                \"Cannot create NaNs for non-float\"\n                                \" type column '{}'\".format(col_name)\n                            )\n                        col_names.append(col_name)\n\n                    table = table.append_columns(\n                        col_names, np.full((n_cols, n), np.nan)\n                    )\n                else:\n                    raise ValueError(\n                        \"Table columns do not match. Use fill_na=True\"\n                        \" if you want to append missing values with NaNs\"\n                    )\n            tables_to_merge.append(table)\n\n        first_table = tables_to_merge[0]\n\n        merged_table = sum(tables_to_merge[1:], first_table)\n\n        merged_table.h5loc = first_table.h5loc\n        merged_table.h5singleton = first_table.h5singleton\n        merged_table.split_h5 = first_table.split_h5\n        merged_table.name = first_table.name\n\n        return merged_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_index_tuple(group_ids):\n    max_group_id = np.max(group_ids)\n\n    start_idx_arr = np.full(max_group_id + 1, 0)\n    n_items_arr = np.full(max_group_id + 1, 0)\n\n    current_group_id = group_ids[0]\n    current_idx = 0\n    item_count = 0\n\n    for group_id in group_ids:\n        if group_id != current_group_id:\n            start_idx_arr[current_group_id] = current_idx\n            n_items_arr[current_group_id] = item_count\n            current_idx += item_count\n            item_count = 0\n            current_group_id = group_id\n        item_count += 1\n    else:\n        start_idx_arr[current_group_id] = current_idx\n        n_items_arr[current_group_id] = item_count\n\n    return (start_idx_arr, n_items_arr)", "response": "Create index tuples for fast lookup in HDF5Pump"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntraversing the internal dictionary and set the getters", "response": "def _set_attributes(self):\n        \"\"\"Traverse the internal dictionary and set the getters\"\"\"\n        for parameter, data in self._data.items():\n            if isinstance(data, dict) or isinstance(data, OrderedDict):\n                field_names, field_values = zip(*data.items())\n                sorted_indices = np.argsort(field_names)\n                attr = namedtuple(\n                    parameter, [field_names[i] for i in sorted_indices]\n                )\n                setattr(\n                    self, parameter,\n                    attr(*[field_values[i] for i in sorted_indices])\n                )\n            else:\n                setattr(self, parameter, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting all the cached NDArrays to disk and empties the cache", "response": "def _write_ndarrays_cache_to_disk(self):\n        \"\"\"Writes all the cached NDArrays to disk and empties the cache\"\"\"\n        for h5loc, arrs in self._ndarrays_cache.items():\n            title = arrs[0].title\n            chunkshape = (self.chunksize,) + arrs[0].shape[1:] if self.chunksize is not\\\n                                                           None else None\n\n            arr = NDArray(np.concatenate(arrs), h5loc=h5loc, title=title)\n\n            if h5loc not in self._ndarrays:\n                loc, tabname = os.path.split(h5loc)\n                ndarr = self.h5file.create_earray(\n                    loc,\n                    tabname,\n                    tb.Atom.from_dtype(arr.dtype),\n                    (0, ) + arr.shape[1:],\n                    chunkshape=chunkshape,\n                    title=title,\n                    filters=self.filters,\n                    createparents=True,\n                )\n                self._ndarrays[h5loc] = ndarr\n            else:\n                ndarr = self._ndarrays[h5loc]\n\n            idx_table_h5loc = h5loc + '_indices'\n            if idx_table_h5loc not in self.indices:\n                self.indices[idx_table_h5loc] = HDF5IndexTable(idx_table_h5loc)\n            idx_tab = self.indices[idx_table_h5loc]\n\n            for arr_length in (len(a) for a in arrs):\n                idx_tab.append(arr_length)\n\n            ndarr.append(arr)\n\n        self._ndarrays_cache = defaultdict(list)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflush tables and arrays to disk", "response": "def flush(self):\n        \"\"\"Flush tables and arrays to disk\"\"\"\n        self.log.info('Flushing tables and arrays to disk...')\n        for tab in self._tables.values():\n            tab.flush()\n        self._write_ndarrays_cache_to_disk()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef top2_reduced(votes):\n    res = np.zeros(12)\n    for vote in votes:\n        # the top ranked alternative is in vote[0][0], second in vote[1][0]\n        if vote[0][0] == 0: # i.e. the first alt is ranked first\n            res[0] += 1\n            if vote[1][0] == 2:\n                res[4] += 1\n            elif vote[1][0] == 3:\n                res[5] += 1\n        elif vote[0][0] == 1:\n            res[1] += 1\n            if vote[1][0] == 0:\n                res[6] += 1\n            elif vote[1][0] == 3:\n                res[7] += 1\n        elif vote[0][0] == 2:\n            res[2] += 1\n            if vote[1][0] == 0:\n                res[8] += 1\n            elif vote[1][0] == 1:\n                res[9] += 1\n        elif vote[0][0] == 3:\n            res[3] += 1\n            if vote[1][0] == 1:\n                res[10] += 1\n            elif vote[1][0] == 2:\n                res[11] += 1\n    res /= len(votes)\n    return res", "response": "This function calculates the 12 top 2 alternatives for the given set of votes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef top2_full(votes):\n    res = np.zeros(16)\n    for vote in votes:\n        # the top ranked alternative is in vote[0][0], second in vote[1][0]\n        if vote[0][0] == 0: # i.e. the first alt is ranked first\n            res[0] += 1\n            if vote[1][0] == 1: # i.e. the second alt is ranked second\n                res[4] += 1\n            elif vote[1][0] == 2:\n                res[5] += 1\n            elif vote[1][0] == 3:\n                res[6] += 1\n        elif vote[0][0] == 1:\n            res[1] += 1\n            if vote[1][0] == 0:\n                res[7] += 1\n            elif vote[1][0] == 2:\n                res[8] += 1\n            elif vote[1][0] == 3:\n                res[9] += 1\n        elif vote[0][0] == 2:\n            res[2] += 1\n            if vote[1][0] == 0:\n                res[10] += 1\n            elif vote[1][0] == 1:\n                res[11] += 1\n            elif vote[1][0] == 3:\n                res[12] += 1\n        elif vote[0][0] == 3:\n            res[3] += 1\n            if vote[1][0] == 0:\n                res[13] += 1\n            elif vote[1][0] == 1:\n                res[14] += 1\n            elif vote[1][0] == 2:\n                res[15] += 1\n    res /= len(votes)\n    return res", "response": "This function calculates the top 2 alternatives of the main language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef top3_reduced(votes):\n    res = np.zeros(16)\n    for vote in votes:\n        # the top ranked alternative is in vote[0][0], second in vote[1][0]\n        if vote[0][0] == 0: # i.e. the first alt is ranked first\n            res[0] += 1\n            if vote[1][0] == 2:\n                res[4] += 1\n            elif vote[1][0] == 3:\n                res[5] += 1\n            elif vote[1][0] == 1 and vote[2][0] == 2:\n                res[14] += 1\n        elif vote[0][0] == 1:\n            res[1] += 1\n            if vote[1][0] == 0:\n                res[6] += 1\n            elif vote[1][0] == 3:\n                res[7] += 1\n            elif vote[1][0] == 2 and vote[2][0] == 3:\n                res[15] += 1\n        elif vote[0][0] == 2:\n            res[2] += 1\n            if vote[1][0] == 0:\n                res[8] += 1\n            elif vote[1][0] == 1:\n                res[9] += 1\n            elif vote[1][0] == 3 and vote[2][0] == 0:\n                res[12] += 1\n        elif vote[0][0] == 3:\n            res[3] += 1\n            if vote[1][0] == 1:\n                res[10] += 1\n            elif vote[1][0] == 2:\n                res[11] += 1\n            elif vote[1][0] == 0 and vote[2][0] == 1:\n                res[13] += 1\n    res /= len(votes)\n    return res", "response": "This function calculates the top 3 alternatives 16 moment conditions values calculation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef top3_full(votes):\n    #create array of zeros, length = q\n    res = np.zeros(2 * len(votes[0]) + (len(votes[0]) * (len(votes[0]) - 1)))\n\n    #iterate through each vote\n    for vote in votes:\n        #set verification boolean to true\n        ver = True\n        #check if vote belongs to c1 < c2 < c3, c2 < c3 < c1... moment\n        for i in range(0, len(votes[0])):\n            if vote[i][0] != vote[i - 1][0] + 1 and vote[i][0] != 0:\n                ver = False\n                break\n        if ver:\n            res[len(votes[0]) + (len(votes[0]) * (len(votes[0]) - 1)) + vote[0][0]] += 1\n\n        #increment moment of top ranked choice ranked at the top\n        res[vote[0][0]] += 1\n\n        #top two moment\n        add = 0\n        if vote[0][0] > vote[1][0]:\n            add = 1\n        res[(vote[0][0] + 1) * (len(votes[0]) - 1) + add + vote[1][0]] += 1\n\n    res /= len(votes) #normalize moments\n    \n    return res", "response": "This function calculates the top 3 alternatives of the given set of entries."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsample program to test GLWindow.", "response": "def main():\n    '''\n        Sample program to test GLWindow.\n    '''\n\n    print('GLWindow:', GLWindow.__version__)\n    print('Python:', sys.version)\n    print('Platform:', sys.platform)\n\n    wnd = GLWindow.create_window((480, 480), title='GLWindow Sample')\n    wnd.vsync = False\n\n    ctx = ModernGL.create_context()\n\n    prog = ctx.program([\n        ctx.vertex_shader('''\n            #version 330\n            in vec2 vert;\n            in vec4 vert_color;\n            out vec4 frag_color;\n            uniform vec2 scale;\n            uniform float rotation;\n            void main() {\n                frag_color = vert_color;\n                float r = rotation * (0.5 + gl_InstanceID * 0.05);\n                mat2 rot = mat2(cos(r), sin(r), -sin(r), cos(r));\n                gl_Position = vec4((rot * vert) * scale, 0.0, 1.0);\n            }\n        '''),\n        ctx.fragment_shader('''\n            #version 330\n            in vec4 frag_color;\n            out vec4 color;\n            void main() {\n                color = vec4(frag_color);\n            }\n        '''),\n    ])\n\n    scale = prog.uniforms['scale']\n    rotation = prog.uniforms['rotation']\n\n    vbo = ctx.buffer(struct.pack(\n        '18f',\n        1.0, 0.0, 1.0, 0.0, 0.0, 0.5,\n        -0.5, 0.86, 0.0, 1.0, 0.0, 0.5,\n        -0.5, -0.86, 0.0, 0.0, 1.0, 0.5,\n    ))\n\n    vao = ctx.simple_vertex_array(prog, vbo, ['vert', 'vert_color'])\n\n    while wnd.update():\n        wnd.clear(0.95, 0.95, 0.95)\n\n        width, height = wnd.size\n        scale.value = (height / width * 0.75, 0.75)\n        ctx.viewport = wnd.viewport\n        ctx.enable(ModernGL.BLEND)\n        rotation.value = wnd.time\n        vao.render(instances=10)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_header(fobj):\n    fobj.write(\"# K40 calibration results\\n\")\n    fobj.write(\"det_id\\trun_id\\tdom_id\")\n    for param in ['t0', 'qe']:\n        for i in range(31):\n            fobj.write(\"\\t{}_ch{}\".format(param, i))", "response": "Add the header to the CSV file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef neutrino_to_source_direction(phi, theta, radian=True):\n    phi = np.atleast_1d(phi).copy()\n    theta = np.atleast_1d(theta).copy()\n    if not radian:\n        phi *= np.pi / 180\n        theta *= np.pi / 180\n    assert np.all(phi <= 2 * np.pi)\n    assert np.all(theta <= np.pi)\n    azimuth = (phi + np.pi) % (2 * np.pi)\n    zenith = np.pi - theta\n    if not radian:\n        azimuth *= 180 / np.pi\n        zenith *= 180 / np.pi\n    return azimuth, zenith", "response": "Flip the direction of a neutrino in the source direction."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef source_to_neutrino_direction(azimuth, zenith, radian=True):\n    azimuth = np.atleast_1d(azimuth).copy()\n    zenith = np.atleast_1d(zenith).copy()\n    if not radian:\n        azimuth *= np.pi / 180\n        zenith *= np.pi / 180\n    phi = (azimuth - np.pi) % (2 * np.pi)\n    theta = np.pi - zenith\n    if not radian:\n        phi *= 180 / np.pi\n        theta *= 180 / np.pi\n    return phi, theta", "response": "Flip the direction from source to neutrino."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef theta(v):\n    v = np.atleast_2d(v)\n    dir_z = v[:, 2]\n    return theta_separg(dir_z)", "response": "Returns the angle in polar coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef azimuth(v):\n    v = np.atleast_2d(v)\n    azi = phi(v) - np.pi\n    azi[azi < 0] += 2 * np.pi\n    if len(azi) == 1:\n        return azi[0]\n    return azi", "response": "Return the azimuth angle in radians."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the angle in radians between vectors v1 and v2.", "response": "def angle_between(v1, v2):\n    \"\"\"Returns the angle in radians between vectors 'v1' and 'v2'.\n\n    >>> angle_between((1, 0, 0), (0, 1, 0))\n    1.5707963267948966\n    >>> angle_between((1, 0, 0), (1, 0, 0))\n    0.0\n    >>> angle_between((1, 0, 0), (-1, 0, 0))\n    3.141592653589793\n\n    \"\"\"\n    v1_u = unit_vector(v1)\n    v2_u = unit_vector(v2)\n    # Don't use `np.dot`, does not work with all shapes\n    angle = np.arccos(np.inner(v1_u, v2_u))\n    return angle"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unit_vector(vector, **kwargs):\n    # This also works for a dataframe with columns ['x', 'y', 'z']\n    # However, the division operation is picky about the shapes\n    # So, remember input vector shape, cast all up to 2d,\n    # do the (ugly) conversion, then return unit in same shape as input\n    # of course, the numpy-ized version of the input...\n    vector = np.array(vector)\n    out_shape = vector.shape\n    vector = np.atleast_2d(vector)\n    unit = vector / np.linalg.norm(vector, axis=1, **kwargs)[:, None]\n    return unit.reshape(out_shape)", "response": "Returns the unit vector of the vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the point - line - distance for given point and line.", "response": "def pld3(pos, line_vertex, line_dir):\n    \"\"\"Calculate the point-line-distance for given point and line.\"\"\"\n    pos = np.atleast_2d(pos)\n    line_vertex = np.atleast_1d(line_vertex)\n    line_dir = np.atleast_1d(line_dir)\n    c = np.cross(line_dir, line_vertex - pos)\n    n1 = np.linalg.norm(c, axis=1)\n    n2 = np.linalg.norm(line_dir)\n    out = n1 / n2\n    if out.ndim == 1 and len(out) == 1:\n        return out[0]\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the distance between two points.", "response": "def dist(x1, x2, axis=0):\n    \"\"\"Return the distance between two points.\n\n    Set axis=1 if x1 is a vector and x2 a matrix to get a vector of distances.\n    \"\"\"\n    return np.linalg.norm(x2 - x1, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating center of mass for given points.", "response": "def com(points, masses=None):\n    \"\"\"Calculate center of mass for given points.\n    If masses is not set, assume equal masses.\"\"\"\n    if masses is None:\n        return np.average(points, axis=0)\n    else:\n        return np.average(points, axis=0, weights=masses)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the circular permutation for a given list of items.", "response": "def circ_permutation(items):\n    \"\"\"Calculate the circular permutation for a given list of items.\"\"\"\n    permutations = []\n    for i in range(len(items)):\n        permutations.append(items[i:] + items[:i])\n    return permutations"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the angle between two points in a single space.", "response": "def space_angle(phi1, theta1, phi2, theta2):\n    \"\"\"Also called Great-circle-distance --\n    use long-ass formula from wikipedia (last in section):\n    https://en.wikipedia.org/wiki/Great-circle_distance#Computational_formulas\n\n    Space angle only makes sense in lon-lat, so convert zenith -> latitude.\n    \"\"\"\n    from numpy import pi, sin, cos, arctan2, sqrt, square\n    lamb1 = pi / 2 - theta1\n    lamb2 = pi / 2 - theta2\n    lambdelt = lamb2 - lamb1\n    under = sin(phi1) * sin(phi2) + cos(phi1) * cos(phi2) * cos(lambdelt)\n    over = sqrt(\n        np.square((cos(phi2) * sin(lambdelt))) +\n        square(cos(phi1) * sin(phi2) - sin(phi1) * cos(phi2) * cos(lambdelt))\n    )\n    angle = arctan2(over, under)\n    return angle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inertia(x, y, z, weight=None):\n    if weight is None:\n        weight = 1\n    tensor_of_inertia = np.zeros((3, 3), dtype=float)\n    tensor_of_inertia[0][0] = (y * y + z * z) * weight\n    tensor_of_inertia[0][1] = (-1) * x * y * weight\n    tensor_of_inertia[0][2] = (-1) * x * z * weight\n    tensor_of_inertia[1][0] = (-1) * x * y * weight\n    tensor_of_inertia[1][1] = (x * x + z * z) * weight\n    tensor_of_inertia[1][2] = (-1) * y * z * weight\n    tensor_of_inertia[2][0] = (-1) * x * z * weight\n    tensor_of_inertia[2][1] = (-1) * z * y * weight\n    tensor_of_inertia[2][2] = (x * x + y * y) * weight\n\n    eigen_values = np.linalg.eigvals(tensor_of_inertia)\n    small_inertia = eigen_values[2][2]\n    middle_inertia = eigen_values[1][1]\n    big_inertia = eigen_values[0][0]\n    return small_inertia, middle_inertia, big_inertia", "response": "Inertia tensor stolen of thomas"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef qrot(vector, quaternion):\n    t = 2 * np.cross(quaternion[1:], vector)\n    v_rot = vector + quaternion[0] * t + np.cross(quaternion[1:], t)\n    return v_rot", "response": "Rotate a 3D vector using quaternion algebra.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef qeuler(yaw, pitch, roll):\n    yaw = np.radians(yaw)\n    pitch = np.radians(pitch)\n    roll = np.radians(roll)\n\n    cy = np.cos(yaw * 0.5)\n    sy = np.sin(yaw * 0.5)\n    cr = np.cos(roll * 0.5)\n    sr = np.sin(roll * 0.5)\n    cp = np.cos(pitch * 0.5)\n    sp = np.sin(pitch * 0.5)\n\n    q = np.array((\n        cy * cr * cp + sy * sr * sp, cy * sr * cp - sy * cr * sp,\n        cy * cr * sp + sy * sr * cp, sy * cr * cp - cy * sr * sp\n    ))\n    return q", "response": "Convert Euler angle to quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intersect_3d(p1, p2):\n    v = p2 - p1\n    normed_v = unit_vector(v)\n    nx = normed_v[:, 0]\n    ny = normed_v[:, 1]\n    nz = normed_v[:, 2]\n    xx = np.sum(nx**2 - 1)\n    yy = np.sum(ny**2 - 1)\n    zz = np.sum(nz**2 - 1)\n    xy = np.sum(nx * ny)\n    xz = np.sum(nx * nz)\n    yz = np.sum(ny * nz)\n    M = np.array([(xx, xy, xz), (xy, yy, yz), (xz, yz, zz)])\n    x = np.sum(\n        p1[:, 0] * (nx**2 - 1) + p1[:, 1] * (nx * ny) + p1[:, 2] * (nx * nz)\n    )\n    y = np.sum(\n        p1[:, 0] * (nx * ny) + p1[:, 1] * (ny * ny - 1) + p1[:, 2] * (ny * nz)\n    )\n    z = np.sum(\n        p1[:, 0] * (nx * nz) + p1[:, 1] * (ny * nz) + p1[:, 2] * (nz**2 - 1)\n    )\n    return np.linalg.lstsq(M, np.array((x, y, z)), rcond=None)[0]", "response": "Find the closest point of the intersections between two sets of lines in 3D."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef swarm(workingDir):\n  name = os.path.splitext(os.path.basename(workingDir))[0]\n  swarmDescriptionFile = os.path.join(workingDir, \"swarm_description.json\")\n  with open(swarmDescriptionFile) as swarmDesc:\n    swarmDescription = json.loads(swarmDesc.read())\n  print \"=================================================\"\n  print \"= Swarming on %s data...\" % name\n  _printSwarmSizeWarning(swarmDescription[\"swarmSize\"])\n  print \"=================================================\"\n  modelParams = _swarmForBestModelParams(swarmDescription, name, workingDir)\n  return modelParams", "response": "Runs a swarm in the given working directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef timeslice_generator(self):\n        slice_id = 0\n        while slice_id < self.n_timeslices:\n            blob = self.get_blob(slice_id)\n            yield blob\n            slice_id += 1", "response": "Uses slice ID as iterator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the blob for the given index.", "response": "def get_blob(self, index):\n        \"\"\"Index is slice ID\"\"\"\n        blob = self._current_blob\n        self.r.retrieve_timeslice(index)\n        timeslice_info = Table.from_template({\n            'frame_index': self.r.frame_index,\n            'slice_id': index,\n            'timestamp': self.r.utc_seconds,\n            'nanoseconds': self.r.utc_nanoseconds,\n            'n_frames': self.r.n_frames,\n        }, 'TimesliceInfo')\n        hits = self._extract_hits()\n        hits.group_id = index\n        blob['TimesliceInfo'] = timeslice_info\n        blob[self._hits_blob_key] = hits\n        return blob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _slice_generator(self, index):\n        start, stop, step = index.indices(len(self))\n        for i in range(start, stop, step):\n            yield self.get_blob(i)", "response": "A simple slice generator for iterations"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreport the overall correlation with the validation scores using each exemplar in isolation.", "response": "def correlation_by_exemplar(brands, exemplars, validation_scores, analyze_fn_str, outf):\n    \"\"\" Report the overall correlation with the validation scores using each exemplar in isolation. \"\"\"\n    analyze_fn = getattr(analyze, analyze_fn_str)\n    keys = sorted(k for k in validation_scores.keys() if k in set(x[0] for x in brands))\n    truth = [validation_scores[k] for k in keys]\n    result = {}\n    outf.write('exemplar\\tcorr\\tn_followers\\n')\n    outf.flush()\n    for exemplar in exemplars:\n        single_exemplar = {exemplar: exemplars[exemplar]}\n        social_scores = analyze_fn(brands, single_exemplar)\n        predicted = [social_scores[k] for k in keys]\n        outf.write('%s\\t%g\\t%d\\n' % (exemplar, scistat.pearsonr(predicted, truth)[0], len(exemplars[exemplar])))\n        outf.flush()\n        result[exemplar] = scistat.pearsonr(predicted, truth)[0]\n    outf.close()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intersection(self, meta):\n        keys = self._streams[0].stream_id.meta_data.keys()\n        return StreamId(self.node_id, dict(*zip((kk, meta[kk]) for kk in keys)))", "response": "Get the intersection between this node s meta data and the given meta data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsummarises the differences between this node and the other node.", "response": "def difference(self, other):\n        \"\"\"\n        Summarise the differences between this node and the other node.\n\n        :param other: The other node\n        :return: A tuple containing the diff, the counts of the diff, and whether this plate is a sub-plate of the other\n        :type other: Node\n        \"\"\"\n        diff = (tuple(set(self.plates) - set(other.plates)), tuple(set(other.plates) - set(self.plates)))\n        counts = map(len, diff)\n        # is_sub_plate = counts == [1, 1] and diff[1][0].is_sub_plate(diff[0][0])\n        is_sub_plate = counts == [1, 1] and diff[0][0].is_sub_plate(diff[1][0])  # MK fixed\n        if len(other.plates) == 1 and counts == [1, 0] and diff[0][0].parent == other.plates[0].parent:\n            is_sub_plate = True\n        return diff, counts, is_sub_plate"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the first n values from the streams in the given time interval.", "response": "def print_head(self, parent_plate_value, plate_values, interval, n=10, print_func=logging.info):\n        \"\"\"\n        Print the first n values from the streams in the given time interval.\n        The parent plate value is the value of the parent plate,\n        and then the plate values are the values for the plate that are to be printed.\n        e.g. print_head(None, (\"house\", \"1\"))\n\n        :param parent_plate_value: The (fixed) parent plate value\n        :param plate_values: The plate values over which to loop\n        :param interval: The time interval\n        :param n: The maximum number of elements to print\n        :param print_func: The function used for printing (e.g. logging.info() or print())\n        :return: None\n        \"\"\"\n        if isinstance(plate_values, Plate):\n            self.print_head(parent_plate_value, plate_values.values, interval, n, print_func)\n            return\n\n        if len(plate_values) == 1 and len(plate_values[0]) == 2 and isinstance(plate_values[0][0], str):\n            self.print_head(parent_plate_value, (plate_values,), interval, n, print_func)\n            return\n\n        found = False\n        for plate_value in plate_values:\n            combined_plate_value = Plate.combine_values(parent_plate_value, plate_value)\n\n            if combined_plate_value not in self._streams:\n                # This can happen if we have created a compound plate and only certain plate values are valid\n                continue\n\n            found = True\n            print_func(\"Plate value: {}\".format(combined_plate_value))\n            data = False\n            for k, v in self._streams[combined_plate_value].window(interval).head(n):\n                data = True\n                print_func(\"{}, {}\".format(k, v))\n            if not data:\n                print_func(\"No data\")\n            print_func(\"\")\n        if not found:\n            print_func(\"No streams found for the given plate values\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nproduce a formatted report of the current timing data.", "response": "def report(times=None,\n           include_itrs=True,\n           include_stats=True,\n           delim_mode=False,\n           format_options=None):\n    \"\"\"\n    Produce a formatted report of the current timing data.\n\n    Notes:\n        When reporting a collection of parallel subdivisions, only the one with\n        the greatest total time is reported on, and the rest are ignored (no\n        branching).  To compare parallel subdivisions use compare().\n\n    Args:\n        times (Times, optional): Times object to report on.  If not provided,\n            uses current root timer.\n        include_itrs (bool, optional): Display invidual iteration times.\n        include_stats (bool, optional): Display iteration statistics.\n        delim_mode (bool, optional): If True, format for spreadsheet.\n        format_options (dict, optional): Formatting options, see below.\n\n    Formatting Keywords & Defaults:\n        Human-Readable Mode\n            - 'stamp_name_width': 20\n            - 'itr_tab_width': 2\n            - 'itr_num_width': 6\n            - 'itr_name_width': 12\n            - 'indent_symbol': '  ' (two spaces)\n            - 'parallel_symbol': '(par)'\n        Delimited Mode\n            - 'delimiter': '\\t' (tab)\n            - 'ident_symbol': '+'\n            - 'parallel_symbol': '(par)'\n\n    Returns:\n        str: Timing data report as formatted string.\n\n    Raises:\n        TypeError: If 'times' param is used and value is not a Times object.\n    \"\"\"\n    if times is None:\n        if f.root.stopped:\n            return report_loc.report(f.root.times,\n                                     include_itrs,\n                                     include_stats,\n                                     delim_mode,\n                                     format_options)\n        else:\n            t = timer()\n            rep = report_loc.report(collapse.collapse_times(),\n                                    include_itrs,\n                                    include_stats,\n                                    delim_mode,\n                                    format_options,\n                                    timer_state='running')\n            f.root.self_cut += timer() - t\n            return rep\n    else:\n        if not isinstance(times, Times):\n            raise TypeError(\"Expected Times instance for param 'times' (default is root).\")\n        return report_loc.report(times,\n                                 include_itrs,\n                                 include_stats,\n                                 delim_mode,\n                                 format_options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a formatted comparison of the times data of the current timer and the times data of the current root timer.", "response": "def compare(times_list=None,\n            name=None,\n            include_list=True,\n            include_stats=True,\n            delim_mode=False,\n            format_options=None):\n    \"\"\"\n    Produce a formatted comparison of timing datas.\n\n    Notes:\n        If no times_list is provided, produces comparison reports on all parallel\n        subdivisions present at the root level of the current timer.  To compare\n        parallel subdivisions at a lower level, get the times data, navigate\n        within it to the parallel list of interest, and provide that as input\n        here.  As with report(), any further parallel subdivisions encountered\n        have only their member with the greatest total time reported on (no\n        branching).\n\n    Args:\n        times_list (Times, optional): list or tuple of Times objects.  If not\n            provided, uses current root timer.\n        name (any, optional): Identifier, passed through str().\n        include_list (bool, optional): Display stamps hierarchy.\n        include_stats (bool, optional): Display stamp comparison statistics.\n        delim_mode (bool, optional): If True, format for spreadsheet.\n        format_options (None, optional): Formatting options, see below.\n\n    Formatting Keywords & Defaults:\n        Human-readable Mode\n            - 'stamp_name_width': 18\n            - 'list_column_width': 12\n            - 'list_tab_width': 2\n            - 'stat_column_width': 8\n            - 'stat_tab_width': 2\n            - 'indent_symbol: ' ' (one space)\n        Delimited Mode\n            - 'delimiter': '\\t' (tab)\n            - 'ident_symbol': '+'\n\n    Returns:\n        str: Times data comparison as formatted string.\n\n    Raises:\n        TypeError: If any element of provided collection is not a Times object.\n    \"\"\"\n    if times_list is None:\n        rep = ''\n        for par_dict in itervalues(f.root.times.par_subdvsn):\n            for par_name, par_list in iteritems(par_dict):\n                rep += report_loc.compare(par_list,\n                                          par_name,\n                                          include_list,\n                                          include_stats,\n                                          delim_mode,\n                                          format_options)\n    else:\n        if not isinstance(times_list, (list, tuple)):\n            raise TypeError(\"Expected a list/tuple of times instances for param 'times_list'.\")\n        if not all([isinstance(times, Times) for times in times_list]):\n            raise TypeError(\"At least one member of param 'times_list' is not a Times object.\")\n        rep = report_loc.compare(times_list,\n                                 name,\n                                 include_list,\n                                 include_stats,\n                                 delim_mode,\n                                 format_options)\n    return rep"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_structure(times=None):\n    if times is None:\n        return report_loc.write_structure(f.root.times)\n    else:\n        if not isinstance(times, Times):\n            raise TypeError(\"Expected Times instance for param 'times' (default is root).\")\n        return report_loc.write_structure(times)", "response": "Produce a formatted record of a times data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites all muons from McTracks to Muons.", "response": "def filter_muons(blob):\n    \"\"\"Write all muons from McTracks to Muons.\"\"\"\n    tracks = blob['McTracks']\n    muons = tracks[tracks.type == -13]    # PDG particle code\n    blob[\"Muons\"] = Table(muons)\n    return blob"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses command line options.", "response": "def parse_cmdline(argv=None):\n    \"\"\"Parse command line options.\n    \n    @param argv: List of command line arguments. If None, get list from system.\n    @return:     Tuple of Option List and Argument List.\n    \n    \"\"\"\n    parser = optparse.OptionParser()\n    parser.add_option('-c', '--conf', help='Configuration file path.',\n                      dest='confpath',default=None)\n    parser.add_option('-p', '--bindport',\n                      help='Bind to TCP Port. (Default: %d)' % conf['bindport'],\n                      dest='bindport', type='int', default=None, action='store')\n    parser.add_option('-b', '--bindaddr',\n                      help='Bind to IP Address. (Default: %s)' % conf['bindaddr'],\n                      dest='bindaddr', default=None, action='store')\n    parser.add_option('-u', '--baseurl', \n                      help='Base URL. (Default: %s)' % conf['baseurl'],\n                      dest='baseurl', default=None, action='store')\n    parser.add_option('-D', '--devel', help='Enable development mode.',\n                      dest='devel', default=False, action='store_true')\n    if argv is None:\n        return parser.parse_args()\n    else:\n        return parser.parse_args(argv[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_conf_files(conf_paths):\n    conf_file = ConfigParser.RawConfigParser()\n    conf_read = conf_file.read(conf_paths)\n    conf = {}\n    try:\n        if conf_read:\n            conf['client_id'] = conf_file.get('runkeeper', 'client_id')\n            conf['client_secret'] = conf_file.get('runkeeper', 'client_secret')\n            if conf_file.has_option('runkeeper', 'bindport'):\n                conf['bindport'] = conf_file.getint('runkeeper', 'bindport')\n            if conf_file.has_option('runkeeper', 'bindaddr'):\n                conf['bindaddr'] = conf_file.get('runkeeper', 'bindaddr')\n            if conf_file.has_option('runkeeper', 'baseurl'):\n                conf['baseurl'] = conf_file.get('runkeeper', 'baseurl')\n            return conf\n    except ConfigParser.Error:\n        raise ConfigurationError(\"Error parsing configuration file(s): %s\\n\" \n                                 % sys.exc_info()[1])\n    else:\n        raise ConfigurationError(\"No valid configuration file (%s) found.\" \n                                 % defaultConfFilename)", "response": "Parse the configuration file and return dictionary of configuration options."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the hash of the movie depending on the input string.", "response": "def get_hash(input_string):\n    \"\"\" Return the hash of the movie depending on the input string.\n\n    If the input string looks like a symbolic link to a movie in a Kolekto\n    tree, return its movies hash, else, return the input directly in lowercase.\n    \"\"\"\n\n    # Check if the input looks like a link to a movie:\n    if os.path.islink(input_string):\n        directory, movie_hash = os.path.split(os.readlink(input_string))\n        input_string = movie_hash\n\n    return input_string.lower()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, key):\n        return self._object_class(json.loads(self._db[key]))", "response": "Get data associated with provided key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves data associated with key.", "response": "def save(self, key, data):\n        \"\"\" Save data associated with key.\n        \"\"\"\n        self._db[key] = json.dumps(data)\n        self._db.sync()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the global meta data for a given resource ID", "response": "def global_meta_data(self):\n        \"\"\"\n        Get the global meta data, which will be stored in a tree structure\n\n        :return: The global meta data\n        \"\"\"\n        with switch_db(MetaDataModel, 'hyperstream'):\n            return sorted(map(lambda x: x.to_dict(), MetaDataModel.objects),\n                          key=lambda x: len(x['identifier'].split('.')),\n                          reverse=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts the given meta data into the database and the global plate definitions.", "response": "def insert(self, tag, identifier, parent, data):\n        \"\"\"\n        Insert the given meta data into the database\n\n        :param tag: The tag (equates to meta_data_id)\n        :param identifier: The identifier (a combination of the meta_data_id and the plate value)\n        :param parent: The parent plate identifier\n        :param data: The data (plate value)\n        :return: None\n        \"\"\"\n        # First try to add it into the tree\n        if self.global_plate_definitions.contains(identifier):\n            raise KeyError(\"Identifier {} already exists in tree\".format(identifier))\n\n        self.global_plate_definitions.create_node(tag=tag, identifier=identifier, parent=parent, data=data)\n\n        # Now try to add it into the database\n        with switch_db(MetaDataModel, 'hyperstream'):\n            meta_data = MetaDataModel(tag=tag, parent=parent, data=data)\n            meta_data.save()\n\n        logging.info(\"Meta data {} inserted\".format(identifier))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the meta data with the given identifier from the database.", "response": "def delete(self, identifier):\n        \"\"\"\n        Delete the meta data with the given identifier from the database\n\n        :param identifier: The identifier\n        :return: None\n        \"\"\"\n\n        try:\n            node = self.global_plate_definitions[identifier]\n        except NodeIDAbsentError:\n            logging.info(\"Meta data {} not present during deletion\".format(identifier))\n            return\n\n        # First delete any children of the node: REMOVED as this seemed to be unreliable\n        # It's now better to call delete_plate with delete_meta_data=True\n        # for child in node.fpointer:\n        #     self.delete(child)\n\n        self.global_plate_definitions.remove_node(identifier)\n\n        with switch_db(MetaDataModel, 'hyperstream'):\n            meta_data = MetaDataModel.objects(tag=node.tag, data=node.data, parent=node.bpointer).first()\n            if meta_data is not None:\n                meta_data.delete()\n\n        logging.info(\"Meta data {} deleted\".format(identifier))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delta(x_i, j, s, N):\n        flag = j == EMMMixPLAggregator.c(x_i, s)\n        if flag and s < len(x_i):\n            return 1\n        elif s == N:\n            found_equal = False\n            for l in range(len(x_i)):\n                if j == EMMMixPLAggregator.c(x_i, l):\n                    found_equal = True\n                    break\n            if not found_equal:\n                return 1\n        return 0", "response": "delta_i_j_s returns 1 if the i j is less than or equal to the s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggregate(self, rankings, K, epsilon, epsilon_mm, iters):\n        x = rankings # shorter pseudonym for voting data\n        self.n = len(rankings) # number of votes\n\n        # \"fixed\" iterations type variables\n        outer_iters = None\n        inner_iters = None\n        inner_range = None\n\n        # Additional \"scaling\" iterations type variables\n        inner_iters_base = None\n        scaling_divisor = None\n\n        # Additional \"total\" iterations type variables\n        total_iters = None\n        isIncremented = False\n\n        if \"type\" not in iters:\n            raise ValueError(\"iters dict must contain key \\\"type\\\"\")\n        iters_type = iters[\"type\"]\n        if iters_type == \"fixed\":\n            outer_iters = iters[\"em_iters\"]\n            inner_iters = iters[\"mm_iters\"]\n        elif iters_type == \"scaling\":\n            outer_iters = iters[\"em_iters\"]\n            inner_iters_base = iters[\"mm_iters_base\"]\n            scaling_divisor = iters[\"scaling_divisor\"]\n        elif iters_type == \"total\":\n            total_iters = iters[\"total_iters\"]\n            outer_iters = iters[\"em_iters\"]\n            inner_iters = total_iters // outer_iters\n        else:\n            raise ValueError(\"iters dict value for key \\\"type\\\" is invalid: \" + str(iters_type))\n\n        # pre-compute the delta values\n        delta_i_j_s = np.empty((self.n, self.m, self.m + 1))\n        for i in range(self.n):\n            for j in range(self.m):\n                for s in range(self.m + 1):\n                    delta_i_j_s[i][j][s] = EMMMixPLAggregator.delta(x[i], j, s, self.m)\n\n        # generate initial values for p and pi:\n        p_h0 = np.random.rand(K, self.m)\n        p_h0 /= np.sum(p_h0, axis=1, keepdims=True)\n\n        pi_h0 =  np.random.rand(K)\n        pi_h0 /= np.sum(pi_h0)\n\n        p_h = np.copy(p_h0)\n        pi_h = np.copy(pi_h0)\n\n        for g in range(outer_iters):\n\n            p_h1 = np.empty((K, self.m))\n            pi_h1 = np.empty(K)\n            z_h1 = np.empty((self.n, K))\n\n            # E-Step:\n            self._EStep(K, x, z_h1, pi_h, p_h)\n\n            # M-Step:\n            if iters_type == \"fixed\":\n                inner_range = range(inner_iters)\n            elif iters_type == \"total\" and not isIncremented:\n                test = (g + 1) * inner_iters + (outer_iters - g - 1) * (inner_iters + 1)\n                if test < total_iters:\n                    inner_iters += 1\n                    isIncremented = True\n                inner_range = range(inner_iters)\n            elif iters_type == \"scaling\":\n                inner_range = range(int(g/scaling_divisor) + inner_iters_base)\n\n            for l in inner_range:\n                self._MStep(l, K, x, z_h1, pi_h1, p_h1, p_h, delta_i_j_s)\n\n                if (epsilon_mm != None and\n                    np.all(np.absolute(p_h1 - p_h) < epsilon_mm)):\n                        break\n\n                p_h = np.copy(p_h1) # deep copy p for next MM iteration\n                # pi does not change across MM iterations, no copy needed\n\n            if (epsilon != None and\n                np.all(np.absolute(p_h1 - p_h) < epsilon) and\n                np.all(np.absolute(pi_h1 - pi_h) < epsilon)):\n                    break\n\n            # remember that assignments below are references only, not copies\n            p_h = p_h1\n            pi_h = pi_h1\n\n        return (pi_h1, p_h1, pi_h0, p_h0)", "response": "This method computes the model parameters for a mixture of Plackett - Luce models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _MStep(self, l, K, x, z_h1, pi_h1, p_h1, p_h, delta_i_j_s):\n        for k in range(K):\n            normconst = 0\n            if l == 0: # only need to compute pi at first MM iteration\n                pi_h1[k] = np.sum(z_h1.T[k]) / len(z_h1)\n            for j in range(self.m):\n                omega_k_j = EMMMixPLAggregator.omega(k, j, z_h1, x) # numerator\n                denom_sum = 0\n                for i in range(self.n):\n                    sum1 = 0\n                    for t in range(len(x[i])):\n                        sum2 = 0\n                        sum3 = 0\n                        for s in range(t, self.m):\n                            sum2 += p_h[k][EMMMixPLAggregator.c(x[i], s)]\n                        for s in range(t, self.m + 1):\n                            sum3 += delta_i_j_s[i][j][s]\n                        sum1 += z_h1[i][k] * (sum2 ** -1) * sum3\n                    denom_sum += sum1\n                p_h1[k][j] = omega_k_j / denom_sum\n                normconst += p_h1[k][j]\n            for j in range(self.m):\n                p_h1[k][j] /= normconst", "response": "This function computes the M - Step of the EMM algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the data from the stream by calling River View for data.", "response": "def load(self):\n    \"\"\"\n    Loads this stream by calling River View for data.\n    \"\"\"\n    print \"Loading data for %s...\" % self.getName()\n    self._dataHandle = self._stream.data(\n      since=self._since, until=self._until, \n      limit=self._limit, aggregate=self._aggregate\n    )\n    self._data = self._dataHandle.data()\n    self._headers = self._dataHandle.headers()\n    print \"Loaded %i rows.\" % len(self)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n    out = self.peek()[self._headers.index(self._field)]\n    self._cursor += 1\n    if out is not None:\n      self._lastValue = out\n    return out", "response": "Returns the next data value in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef advance(self, myDateTime):\n    if self.getTime() == myDateTime:\n      out =  self.next()\n      # Sometimes, the stream has no value for this field and returns None, in \n      # this case we'll use the last value as well.\n      if out is None:\n        out = self.last()\n    else:\n      out = self.last()\n    # If there's no more data, we must fetch more\n    if len(self) is 0:\n      self._fetchNextData()\n    \n    self._updateMinMax(out)\n    \n    if isinstance(out, float):\n      self._dataType = \"float\"\n    \n    # Convert to proper data type\n    if self._dataType is \"float\":\n      out = float(out)\n    else:\n      out = int(out)\n\n    return out", "response": "Advance the stream to the next value and returns an appropriate value for the given time."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the time for the next data point.", "response": "def getTime(self):\n    \"\"\"\n    Gets the time for the next data point.\n    :return: (datetime)\n    \"\"\"\n    headers = self._headers\n    timeStringIndex = headers.index(TIMESTAMP_FIELD)\n    timeString = self.peek()[timeStringIndex]\n    return  datetime.strptime(timeString, \"%Y/%m/%d %H:%M:%S\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide a field description dict for swarm description.", "response": "def createFieldDescription(self):\n    \"\"\"\n    Provides a field description dict for swarm description.\n    :return: (dict)\n    \"\"\"\n    return {\n      \"fieldName\": self.getName(),\n      \"fieldType\": self._dataType,\n      \"minValue\": self._min,\n      \"maxValue\": self._max\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_lexemes(self, **kw):\n        lexemes = []\n\n        # Do we have morpheme segmentation on top of phonemes?\n        with_morphemes = '+' in self['FormTable', 'Segments'].separator\n\n        for i, form in enumerate(self.dataset.split_forms(kw, kw['Value'])):\n            kw_ = kw.copy()\n            if form:\n                if form != kw_['Value']:\n                    self.dataset.log.debug(\n                        'iter_forms split: \"{0}\" -> \"{1}\"'.format(kw_['Value'], form))\n                if form:\n                    kw_.setdefault('Segments', self.tokenize(kw_, form) or [])\n                    kw_.update(ID=self.lexeme_id(kw), Form=form)\n                    lexemes.append(self._add_object(self.dataset.lexeme_class, **kw_))\n\n                    if kw_['Segments']:\n                        analysis = self.dataset.tr_analyses.setdefault(\n                            kw_['Language_ID'], Analysis())\n                        try:\n                            segments = kw_['Segments']\n                            if with_morphemes:\n                                segments = list(chain(*[s.split() for s in segments]))\n                            _, _bipa, _sc, _analysis = analyze(segments, analysis)\n\n                            # update the list of `bad_words` if necessary; we precompute a\n                            # list of data types in `_bipa` just to make the conditional\n                            # checking easier\n                            _bipa_types = [type(s) for s in _bipa]\n                            if pyclts.models.UnknownSound in _bipa_types or '?' in _sc:\n                                self.dataset.tr_bad_words.append(kw_)\n                        except ValueError:  # pragma: no cover\n                            self.dataset.tr_invalid_words.append(kw_)\n                        except (KeyError, AttributeError):  # pragma: no cover\n                            print(kw_['Form'], kw_)\n                            raise\n\n        return lexemes", "response": "Add new Lexemes to the database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_languages(self, id_factory=lambda d: d['ID']):\n        ids = set()\n        for kw in self.dataset.languages:\n            if (not kw.get('Glottocode')) and kw.get('ISO639P3code'):\n                kw['Glottocode'] = self.dataset.glottolog.glottocode_by_iso.get(kw['ISO639P3code'])\n            kw['ID'] = id_factory(kw)\n            ids.add(kw['ID'])\n            self.add_language(**kw)\n        return ids", "response": "Add languages as specified in a dataset s etc / languages. csv file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_concepts(self, id_factory=lambda d: d.number):\n        ids, concepts = set(), []\n        if self.dataset.conceptlist:\n            concepts = self.dataset.conceptlist.concepts.values()\n        else:\n            fields = Concept.public_fields()\n            for i, concept in enumerate(self.dataset.concepts, start=1):\n                kw, attrs = {}, {}\n                for k, v in concept.items():\n                    if k.lower() in fields:\n                        kw[k.lower()] = v\n                    else:\n                        attrs[k.lower()] = v\n\n                if not kw.get('id'):\n                    kw['id'] = str(i)\n                if not kw.get('number'):\n                    kw['number'] = str(i)\n                concepts.append(Concept(attributes=attrs, **kw))\n\n        fieldnames = {f.lower(): f for f in self.dataset.concept_class.fieldnames()}\n        for c in concepts:\n            attrs = dict(\n                ID=id_factory(c),\n                Name=c.label,\n                Concepticon_ID=c.concepticon_id,\n                Concepticon_Gloss=c.concepticon_gloss)\n            for fl, f in fieldnames.items():\n                if fl in c.attributes:\n                    attrs[f] = c.attributes[fl]\n            ids.add(attrs['ID'])\n            self.add_concept(**attrs)\n        return ids", "response": "Adds concepts as specified in a dataset s associated Concepticon concept list or infrastructure. csv file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hexbin(x, y, color=\"purple\", **kwargs):\n    if HAS_SEABORN:\n        cmap = sns.light_palette(color, as_cmap=True)\n    else:\n        cmap = \"Purples\"\n    plt.hexbin(x, y, cmap=cmap, **kwargs)", "response": "Seaborn - compatible hexbin plot."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the diagonal of the sequence.", "response": "def diag(ax=None, linecolor='0.0', linestyle='--', **kwargs):\n    \"\"\"Plot the diagonal.\"\"\"\n    ax = get_ax(ax)\n    xy_min = np.min((ax.get_xlim(), ax.get_ylim()))\n    xy_max = np.max((ax.get_ylim(), ax.get_xlim()))\n    return ax.plot([xy_min, xy_max], [xy_min, xy_max],\n                   ls=linestyle,\n                   c=linecolor,\n                   **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef automeshgrid(\n        x, y, step=0.02, xstep=None, ystep=None, pad=0.5, xpad=None, ypad=None\n):\n    \"\"\"Make a meshgrid, inferred from data.\"\"\"\n    if xpad is None:\n        xpad = pad\n    if xstep is None:\n        xstep = step\n    if ypad is None:\n        ypad = pad\n    if ystep is None:\n        ystep = step\n    xmin = x.min() - xpad\n    xmax = x.max() + xpad\n    ymin = y.min() - ypad\n    ymax = y.max() + ypad\n    return meshgrid(xmin, xmax, step, ymin, ymax, ystep)", "response": "Make a meshgrid inferred from data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef meshgrid(x_min, x_max, x_step, y_min=None, y_max=None, y_step=None):\n    if y_min is None:\n        y_min = x_min\n    if y_max is None:\n        y_max = x_max\n    if y_step is None:\n        y_step = x_step\n    xx, yy = np.meshgrid(\n        np.arange(x_min, x_max, x_step), np.arange(y_min, y_max, y_step)\n    )\n    return xx, yy", "response": "Make a meshgrid e. g. when plotting decision surfaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots a histogram with counts and binlims already given.", "response": "def prebinned_hist(counts, binlims, ax=None, *args, **kwargs):\n    \"\"\"Plot a histogram with counts, binlims already given.\n\n    Example\n    =======\n    >>> gaus = np.random.normal(size=100)\n    >>> counts, binlims = np.histogram(gaus, bins='auto')\n    >>> prebinned_hist(countsl binlims)\n    \"\"\"\n    ax = get_ax(ax)\n    x = bincenters(binlims)\n    weights = counts\n    return ax.hist(x, bins=binlims, weights=weights, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef joint_hex(x, y, **kwargs):\n    return sns.jointplot(\n        x, y, kind='hex', stat_func=None, marginal_kws={'kde': True}, **kwargs\n    )", "response": "Seaborn Joint Hexplot with marginal KDE + hists."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute(self, time_interval):\n        # TODO: What if the leaf nodes have different time intervals?\n\n        # if not self._hyperstream:\n        #     raise ValueError(\"\")\n        with WorkflowMonitor(self):\n            # First look for asset writers\n            for factor in self.factors[::-1]:\n                if factor.tool.name == \"asset_writer\":\n                    factor.execute(time_interval)\n\n            for factor in self.factors[::-1]:\n                if factor.sink is None or factor.sink.is_leaf and factor.tool.name != \"asset_writer\":\n                    factor.execute(time_interval)", "response": "Execute the factors over the streams in the workflow."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a node to the workflow", "response": "def _add_node(self, node):\n        \"\"\"\n        Add a node to the workflow\n\n        :param node: The node object\n        :type node: Node\n        :return: None\n        \"\"\"\n        self.nodes[node.node_id] = node\n        logging.info(\"Added node with id {} containing {} streams\".format(node.node_id, len(node.streams)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a factor to the workflow", "response": "def _add_factor(self, factor):\n        \"\"\"\n        Add a factor to the workflow\n\n        :param factor: The factor object\n        :type factor: Factor | MultiOutputFactor | NodeCreationFactor\n        :return: None\n        \"\"\"\n        self.factors.append(factor)\n        logging.info(\"Added factor with tool {} \".format(factor.tool))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a node in the graph.", "response": "def create_node(self, stream_name, channel, plates):\n        \"\"\"\n        Create a node in the graph. Note: assumes that the streams already exist\n\n        :param stream_name: The name of the stream\n        :param channel: The channel where this stream lives\n        :param plates: The plates. The stream meta-data will be auto-generated from these\n        :return: The streams associated with this node\n        \"\"\"\n        streams = {}\n\n        plate_values = Plate.get_overlapping_values(plates)\n\n        if plates:\n            if not plate_values:\n                raise NodeDefinitionError(\"No overlapping plate values found\")\n            for pv in plate_values:\n                # Construct stream id\n                stream_id = StreamId(name=stream_name, meta_data=pv)\n\n                # Now try to locate the stream and add it (raises StreamNotFoundError if not found)\n                streams[pv] = channel.get_or_create_stream(stream_id=stream_id)\n\n        else:\n            streams[None] = channel.get_or_create_stream(stream_id=StreamId(name=stream_name))\n\n        if not streams:\n            raise NodeDefinitionError(\"No streams created for node with id {}\".format(stream_name))\n\n        node = Node(channel, stream_name, streams, plates)\n        self._add_node(node)\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_factor(self, tool, sources, sink, alignment_node=None):\n        # if isinstance(tool, dict):\n        #     tool = self.channels.get_tool(**tool)\n\n        if not isinstance(tool, BaseTool):\n            raise ValueError(\"Expected Tool, got {}\".format(type(tool)))\n\n        if sink.plates:\n            if isinstance(tool, (AggregateTool, SelectorTool)):\n                if not sources or len(sources) > 2:\n                    raise FactorDefinitionError(\"{} requires one or two source nodes\".format(type(tool)))\n\n                if len(sources) == 2 and sources[0].plates:\n                    raise FactorDefinitionError(\"{} requires the first source to have no plates\".format(type(tool)))\n\n                if not sources[-1].plates:\n                    raise FactorDefinitionError(\"{} source must live on a plate\".format(type(tool)))\n\n                if len(sources[-1].plates) != 1:\n                    # Make sure that there are exactly two plates that don't match: one from each side\n                    diff, counts, is_sub_plate = sources[-1].difference(sink)\n                    if counts == [1, 1]:\n                        # TODO: This sub-plate selection is deprecated\n                        if not is_sub_plate:\n                            raise IncompatiblePlatesError(\"Sink plate is not a simplification of source plate\")\n                    else:\n                        # If there are two plates, and one (or both) of them is a root plate, than assume that we are\n                        # simplifying by removing that plate\n                        if next(p.is_root for p in sources[-1].plates):\n                            if len(sink.plates) != 1:\n                                raise IncompatiblePlatesError(\n                                    \"Multiple sink plates defined. \"\n                                    \"Did you intend a simplification of 2 source plates to a sink plate?\")\n                            if sink.plates[0] not in sources[-1].plates:\n                                raise IncompatiblePlatesError(\n                                    \"Source and sink plates do not match. \"\n                                    \"Did you intend a simplification of 2 source plates to a sink plate?\")\n                        else:\n                            if len(sink.plates) > 1:\n                                raise NotImplementedError\n                            source_plates = sources[-1].plates\n                            sink_plate = sink.plates[0]\n                            if len(source_plates) != 2:\n                                raise IncompatiblePlatesError(\n                                    \"Sink plate is not a simplification of source plate (source must be 2 plates)\")\n                            plate_diff = set(source_plates).difference({sink_plate, })\n                            if len(plate_diff) != 1:\n                                raise IncompatiblePlatesError(\n                                    \"Sink plate is not a simplification of source plate \"\n                                    \"(the number of plates in the set difference of source and sink is not 1\")\n                            plate_diff = list(plate_diff)[0]\n                            if plate_diff.parent != sink_plate.parent:\n                                raise IncompatiblePlatesError(\n                                    \"Sink plate is not a simplification of source plate (parents do not match)\")\n                else:\n                    # Check if the parent plate is valid instead\n                    source_plate = sources[-1].plates[0]\n                    sink_plate = sink.plates[0]\n\n                    error = self.check_plate_compatibility(tool, source_plate, sink_plate)\n                    if error is not None:\n                        raise IncompatiblePlatesError(error)\n            else:\n                if sources:\n                    # Check that the plates are compatible\n                    source_plates = list(itertools.chain(*(source.plate_ids for source in sources)))\n                    for p in sink.plate_ids:\n                        if p not in set(source_plates):\n                            raise IncompatiblePlatesError(\"{} not in source plates\".format(p))\n                    for p in source_plates:\n                        if p not in set(sink.plate_ids):\n                            raise IncompatiblePlatesError(\"{} not in sink plates\".format(p))\n            plates = sink.plates\n        else:\n            plates = None\n\n        factor = Factor(tool=tool, source_nodes=sources,\n                        sink_node=sink, alignment_node=alignment_node,\n                        plates=plates)\n\n        self._add_factor(factor)\n        return factor", "response": "Creates a factor for the given tool and source and sink nodes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_multi_output_factor(self, tool, source, splitting_node, sink):\n        if source and not isinstance(source, Node):\n            raise ValueError(\"Expected Node, got {}\".format(type(source)))\n\n        if not isinstance(sink, Node):\n            raise ValueError(\"Expected Node, got {}\".format(type(sink)))\n\n        # if isinstance(tool, dict):\n        #     tool = self.channels.get_tool(**tool)\n\n        if not isinstance(tool, MultiOutputTool):\n            raise ValueError(\"Expected MultiOutputTool, got {}\".format(type(tool)))\n\n        # Check that the input_plate are compatible - note this is the opposite way round to a normal factor\n        input_plates = source.plates if source else []\n        output_plates = sink.plates\n\n        if len(input_plates) > 1:\n            raise NotImplementedError\n\n        if len(output_plates) == 0:\n            raise ValueError(\"No output plate found\")\n\n        if len(output_plates) == 1:\n            if not self.check_multi_output_plate_compatibility(input_plates, output_plates[0]):\n                raise IncompatiblePlatesError(\"Parent plate does not match input plate\")\n\n            factor = MultiOutputFactor(tool=tool, source_node=source, splitting_node=splitting_node, sink_node=sink,\n                                       input_plate=input_plates[0] if input_plates else None,\n                                       output_plates=output_plates[0])\n        else:\n            # The output plates should be the same as the input plates, except for one\n            # additional plate. Since we're currently only supporting one input plate,\n            # we can safely assume that there is a single matching plate.\n            # Finally, note that the output plate must either have no parents\n            # (i.e. it is at the root of the tree), or the parent plate is somewhere\n            # in the input plate's ancestry\n            if len(output_plates) > 2:\n                raise NotImplementedError\n            if len(input_plates) != 1:\n                raise IncompatiblePlatesError(\"Require an input plate to match all but one of the output plates\")\n            if output_plates[0] == input_plates[0]:\n                # Found a match, so the output plate should be the other plate\n                output_plate = output_plates[1]\n            else:\n                if output_plates[1].plate_id != input_plates[0].plate_id:\n                    raise IncompatiblePlatesError(\"Require an input plate to match all but one of the output plates\")\n                output_plate = output_plates[0]\n                # Swap them round so the new plate is the last plate - this is required by the factor\n                output_plates[1], output_plates[0] = output_plates[0], output_plates[1]\n\n            if not output_plate.is_root:\n                # We need to walk up the input plate's parent tree\n                match = False\n                parent = input_plates[0].parent\n                while parent is not None:\n                    if parent.plate_id == output_plate.parent.plate_id:\n                        match = True\n                        break\n                    parent = parent.parent\n                if not match:\n                    raise IncompatiblePlatesError(\"Require an input plate to match all but one of the output plates\")\n\n            factor = MultiOutputFactor(\n                tool=tool, source_node=source, sink_node=sink,\n                splitting_node=splitting_node, input_plate=input_plates[0], output_plates=output_plates)\n\n        self._add_factor(factor)\n        return factor", "response": "Creates a multi - output factor for all of the input plate values and connects the source and sink nodes with that tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_node_creation_factor(self, tool, source, output_plate, plate_manager):\n        # if isinstance(tool, dict):\n        #     tool = self.channels.get_tool(**tool)\n\n        if not isinstance(tool, PlateCreationTool):\n            raise ValueError(\"Expected PlateCreationTool, got {}\".format(type(tool)))\n\n        input_plates = source.plates if source else []\n\n        if len(input_plates) > 1:\n            raise NotImplementedError\n\n        factor = NodeCreationFactor(\n            tool=tool,\n            source_node=source,\n            input_plate=input_plates[0] if input_plates else None,\n            output_plate=output_plate,\n            plate_manager=plate_manager\n        )\n\n        self._add_factor(factor)\n        return factor", "response": "Creates a factor that creates an output node and ensures that the plate for the output node exists along with all relevant meta - data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether the source and sink plates are compatible with the tool.", "response": "def check_plate_compatibility(tool, source_plate, sink_plate):\n        \"\"\"\n        Checks whether the source and sink plate are compatible given the tool\n\n        :param tool: The tool\n        :param source_plate: The source plate\n        :param sink_plate: The sink plate\n        :return: Either an error, or None\n        :type tool: Tool\n        :type source_plate: Plate\n        :type sink_plate: Plate\n        :rtype: None | str\n        \"\"\"\n        if sink_plate == source_plate.parent:\n            return None\n\n        # could be that they have the same meta data, but the sink plate is a simplification of the source\n        # plate (e.g. when using IndexOf tool)\n        if sink_plate.meta_data_id == source_plate.meta_data_id:\n            if sink_plate.is_sub_plate(source_plate):\n                return None\n            return \"Sink plate {} is not a simplification of source plate {}\".format(\n                sink_plate.plate_id, source_plate.plate_id)\n\n        # Also check to see if the meta data differs by only one value\n        meta_data_diff = set(source_plate.ancestor_meta_data_ids) - set(sink_plate.ancestor_meta_data_ids)\n        if len(meta_data_diff) == 1:\n            # Is the diff value the same as the aggregation meta id passed to the aggregate tool\n            if tool.aggregation_meta_data not in meta_data_diff:\n                return \"Aggregate tool meta data ({}) \" \\\n                       \"does not match the diff between source and sink plates ({})\".format(\n                        tool.aggregation_meta_data, list(meta_data_diff)[0])\n        else:\n            return \"{} not in source's parent plates\".format(sink_plate.plate_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_multi_output_plate_compatibility(source_plates, sink_plate):\n        if len(source_plates) == 0:\n            if sink_plate.parent is not None:\n                return False\n        else:\n            if sink_plate.parent is None:\n                return False\n            else:\n                if sink_plate.parent.plate_id != source_plates[0].plate_id:\n                    return False\n        return True", "response": "Checks if the source and sink plates are compatible with a multi - output plate."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef requested_intervals(self):\n        with switch_db(WorkflowStatusModel, db_alias='hyperstream'):\n            workflow_statuses = WorkflowStatusModel.objects(workflow_id=self.workflow_id)\n            if len(workflow_statuses) == 1:\n                return TimeIntervals(map(lambda x: TimeInterval(x.start, x.end),\n                                         workflow_statuses[0].requested_intervals))\n            else:\n                return TimeIntervals([])", "response": "Get the requested intervals from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the requested intervals to the database.", "response": "def requested_intervals(self, intervals):\n        \"\"\"\n        Set the requested intervals (to the database)\n\n        :param intervals: The intervals\n        :type intervals: TimeIntervals\n        :return: None\n        \"\"\"\n        with switch_db(WorkflowStatusModel, db_alias='hyperstream'):\n            workflow_statuses = WorkflowStatusModel.objects(workflow_id=self.workflow_id)\n            if len(workflow_statuses) != 1:\n                workflow_status = WorkflowStatusModel(\n                    workflow_id=self.workflow_id,\n                    requested_intervals=[]\n                )\n\n            else:\n                workflow_status = workflow_statuses[0]\n\n            workflow_status.last_updated = utcnow()\n            workflow_status.last_accessed = utcnow()\n            workflow_status.requested_intervals = tuple(\n                map(lambda x: TimeIntervalModel(start=x.start, end=x.end), intervals))\n\n            workflow_status.save()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary representation of the workflow as a dictionary for display purposes.", "response": "def to_dict(self, tool_long_names=True):\n        \"\"\"\n        Get a representation of the workflow as a dictionary for display purposes\n\n        :param tool_long_names: Indicates whether to use long names, such as\n                                SplitterFromStream(element=None, use_mapping_keys_only=True)\n                                or short names, such as\n                                splitter_from_stream\n        :type tool_long_names: bool\n        :return: The dictionary of nodes, factors and plates\n        \"\"\"\n        d = dict(nodes=[], factors=[], plates=defaultdict(list))\n        for node in self.nodes:\n            node_id = self.nodes[node].node_id\n            d['nodes'].append({'id': node_id})\n            for plate_id in self.nodes[node].plate_ids:\n                d['plates'][plate_id].append({'id': node_id, 'type': 'node'})\n        for factor in self.factors:\n            tool = str(factor.tool) if tool_long_names else factor.tool.name\n\n            try:\n                sources = [s.node_id for s in factor.sources]\n            except AttributeError:\n                if factor.source:\n                    sources = [factor.source.node_id]\n                else:\n                    sources = []\n\n            d['factors'].append({\n                'id': tool,\n                'sources': sources,\n                'sink': factor.sink.node_id})\n\n            try:\n                if factor.plates:\n                    for plate in factor.plates:\n                        d['plates'][plate.plate_id].append({'id': tool, 'type': 'factor'})\n                else:\n                    d['plates']['root'].append({'id': tool, 'type': 'factor'})\n            except AttributeError:\n                pass\n\n        d['plates'] = dict(d['plates'])\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a JSON representation of the workflow s entry set in the format that is suitable for JSON serialization.", "response": "def to_json(self, formatter=None, tool_long_names=True, **kwargs):\n        \"\"\"\n        Get a JSON representation of the workflow\n\n        :param tool_long_names: Indicates whether to use long names, such as\n                                SplitterFromStream(element=None, use_mapping_keys_only=True)\n                                or short names, such as\n                                splitter_from_stream\n        :param formatter: The formatting function\n        :param kwargs: Keyword arguments for the json output\n        :return: A JSON string\n        \"\"\"\n        d = self.to_dict(tool_long_names=tool_long_names)\n        if formatter:\n            d = formatter(d)\n        return json.dumps(d, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping the dictionary into factorgraph -viz format. See https://github. com / mbforbes / factorgraph -viz format.", "response": "def factorgraph_viz(d):\n        \"\"\"\n        Map the dictionary into factorgraph-viz format. See https://github.com/mbforbes/factorgraph-viz\n\n        :param d: The dictionary\n        :return: The formatted dictionary\n        \"\"\"\n        m = defaultdict(list)\n\n        for node in d['nodes']:\n            m['nodes'].append(dict(\n                id=node['id'],\n                type='rv'\n            ))\n\n        for factor in d['factors']:\n            m['nodes'].append(dict(\n                id=factor['id'],\n                type='fac'\n            ))\n\n            for source in factor['sources']:\n                m['links'].append(dict(\n                    source=source,\n                    target=factor['id']\n                ))\n            if factor['sink']:\n                m['links'].append(dict(\n                    source=factor['id'],\n                    target=factor['sink']\n                ))\n\n        return dict(m)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MoVScoring(profile, scoringVector):\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"csv\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    winners = MechanismPosScoring(scoringVector).getWinners(profile)\n    if len(winners) > 1:\n        return 1\n    n = profile.numVoters\n    m = profile.numCands\n    if len(scoringVector) != m:\n        print(\"ERROR: the length of the scoring vector is not correct!\")\n        exit()\n\n    # Construct the score matrix--values\n    prefcounts = array(profile.getPreferenceCounts())\n    rankmaps = profile.getRankMaps()\n    len_prefcounts = len(prefcounts)\n    values = zeros([len_prefcounts, m], dtype=int)\n\n    if min(list(rankmaps[0].keys())) == 0:\n        delta = 0\n    else:\n        delta = 1\n\n    for i in range(len_prefcounts):\n        for j in range(delta, m + delta):\n            values[i][j - delta] = scoringVector[rankmaps[i][j] - 1]\n\n    # Compute the scores of all the candidates\n    score = dot(array(prefcounts), values)\n    # Compute the winner of the original profile\n\n    d = argmax(score, axis=0) + delta\n    # print(\"d=\",d)\n    alter = delete(range(delta, m + delta), d - delta)\n    # Initialize\n    MoV = n * ones(m, dtype=int)\n    # for c in [3]:\n    for c in alter:\n        # The difference vector of d and c\n        difference = values[:, c - delta] - values[:, d - delta]\n        # print(\"dif=\", difference)\n        index = argsort(difference, axis=0, kind='mergesort')\n        # The vector that each element is the gain in the difference\n        # between d and c if the pattern of the vote changed to [c > others > d]\n        change = scoringVector[0] - difference\n\n        # The total_difference between score(d) and score(c)\n        total_difference = score[d - delta] - score[c - delta]\n        # print(\"total-dif=\", total_difference)\n        for i in range(len_prefcounts):\n            # The number of votes of the first i kinds of patterns\n            temp_sum = sum(prefcounts[index][0:i])\n            # print(\"temp_sum=\", temp_sum)\n\n            # The aggregate gain (of the first i kinds of patterns)\n            # in the difference between d and c if changed to [c > others > d]\n            lower_bound = dot(prefcounts[index][0:i], change[index][0:i])\n            # print(\"lower_bound=\", lower_bound)\n\n            # The aggregate gain (of the first i+1 kinds of patterns)\n            # in the difference between d and c if changed to [c > others > d]\n            upper_bound = dot(prefcounts[index][0:i + 1], change[index][0:i + 1])\n            # print(\"upper_bound=\", upper_bound)\n            # if lower_bound < total_difference <= upper_bound:\n            if lower_bound <= total_difference < upper_bound:\n                # MoV[c - delta] = temp_sum + math.floor(float(total_difference - lower_bound)/change[index][i]) + 1\n                # Update on Apr 13 2019\n                MoV[c - delta] = temp_sum + math.ceil(float(total_difference - lower_bound) / change[index][i])\n                break\n    # print(\"MoV=\", MoV)\n    return min(MoV)", "response": "Returns an integer that represents the winning candidate given an election profile and a scoring vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MoVSimplifiedBucklin(profile):\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"csv\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    # Initialization\n    n = profile.numVoters\n    m = profile.numCands\n    half = math.floor(float(n) / 2)\n    prefcounts = profile.getPreferenceCounts()\n    len_prefcounts = len(prefcounts)\n    rankmaps = profile.getRankMaps()\n    values = zeros([len_prefcounts, m], dtype=int)\n    if min(list(rankmaps[0].keys())) == 0:\n        delta = 0\n    else:\n        delta = 1\n    for i in range(len_prefcounts):\n        for j in range(delta, m + delta):\n            values[i][j - delta] = rankmaps[i][j]\n\n    winners = MechanismSimplifiedBucklin().getWinners(profile)  # the winner list\n    d = winners[0]  # the winner under the numerically tie-breaking rule\n    alter = delete(range(delta, m + delta), d - delta)\n    # Initialize MoV\n    MoV = n * ones(m, dtype=int)\n    for c in alter:\n        for ell in range(1, int(math.floor(float(m) / 2)) + 2):\n            numcond1 = sum(dot(array(prefcounts), logical_and(values[:, c - delta] > ell, values[:, d - delta] <= ell - 1)))\n            numcond2 = sum(dot(array(prefcounts), logical_and(values[:, c - delta] > ell, values[:, d - delta] > ell - 1)))\n            numcond3 = sum(dot(array(prefcounts), logical_and(values[:, c - delta] <= ell, values[:, d - delta] <= ell - 1)))\n            diff_c = half - sum(dot(array(prefcounts), (values[:, c - delta] <= ell)))\n            diff_d = half - sum(dot(array(prefcounts), (values[:, d - delta] <= ell - 1)))\n            if diff_c < 0:\n                if diff_d < 0 and numcond1 + numcond3 > abs(diff_d):\n                    MoV[c - delta] = min(MoV[c - delta], abs(diff_d))\n                continue\n            # -------diff_c >= 0------------\n            if diff_d >= 0:\n                if numcond1 + numcond2 > diff_c >= 0:\n                    MoV[c - delta] = min(MoV[c - delta], diff_c + 1)\n            else:\n                if numcond1 > diff_c and numcond1 > abs(diff_d):\n                    MoV[c - delta] = min(MoV[c - delta], max(diff_c + 1, abs(diff_d)))\n                elif diff_c >= numcond1 > abs(diff_d):\n                    if numcond1 + numcond2 > diff_c:\n                        MoV[c - delta] = min(MoV[c - delta], diff_c + 1)\n                elif abs(diff_d) >= numcond1 > diff_c:\n                    if numcond1 + numcond3 > abs(diff_d):\n                        MoV[c - delta] = min(MoV[c - delta], abs(diff_d))\n                else:  # numcond1 <= diff_c and numcond1 <= abs(diff_d)\n                    if numcond1 + numcond2 > diff_c and numcond1 + numcond3 > abs(diff_d):\n                        MoV[c - delta] = min(MoV[c - delta], numcond1 + abs(diff_c) + 1 + abs(diff_d))\n\n    return min(MoV)", "response": "Returns the MoV of the given profile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an integer that is equal to the margin of victory of the election profile that can change the winners.", "response": "def MoVPluRunOff(profile):\n    \"\"\"\n    Returns an integer that is equal to the margin of victory of the election profile, that is,\n    the smallest number k such that changing k votes can change the winners.\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    # Initialization\n    prefcounts = profile.getPreferenceCounts()\n    len_prefcounts = len(prefcounts)\n    rankmaps = profile.getRankMaps()\n    # print(rankmaps)\n    ranking = MechanismPlurality().getRanking(profile)\n    # print(\"ranking=\", ranking)\n\n    # 1st round: find the top 2 candidates in plurality scores\n    # Compute the 1st-place candidate in plurality scores\n    max_cand = ranking[0][0][0]\n\n    # Compute the 2nd-place candidate in plurality scores\n    # Automatically using tie-breaking rule--numerically increasing order\n    if len(ranking[0][0]) > 1:\n        second_max_cand = ranking[0][0][1]\n        if len(ranking[0][0]) > 2:\n            third_max_cand = ranking[0][0][2]\n        else:\n            third_max_cand = ranking[0][1][0]\n    else:\n        second_max_cand = ranking[0][1][0]\n        if len(ranking[0][1]) > 1:\n            third_max_cand = ranking[0][1][1]\n        else:\n            third_max_cand = ranking[0][2][0]\n\n    top_2 = [max_cand, second_max_cand]\n    # 2nd round: find the candidate with maximum plurality score\n    dict_top2 = {max_cand: 0, second_max_cand: 0}\n    for i in range(len_prefcounts):\n        vote_top2 = {key: value for key, value in rankmaps[i].items() if key in top_2}\n        # print(vote_top2)\n        top_position = min(vote_top2.values())\n        keys = [x for x in vote_top2.keys() if vote_top2[x] == top_position]\n        for key in keys:\n            dict_top2[key] += prefcounts[i]\n\n    # the original winner-- d\n    # print(\"dict_top2=\", dict_top2)\n    d = max(dict_top2.items(), key=lambda x: x[1])[0]\n    c_1 = top_2[0] if top_2[1] == d else top_2[1]\n    # the candidate with third highest plurality score\n    c_2 = third_max_cand\n    # print(\"d=\", d, c_1, c_2)\n\n    Type1_1 = Type1_2 = 0\n    plu_d = plu_c_1 = plu_c_2 = 0\n\n    # ------------count the votes of CASE I & II---------------\n    for i in range(len_prefcounts):\n        if rankmaps[i][d] < rankmaps[i][c_1]:\n            Type1_1 += prefcounts[i]\n        elif rankmaps[i][d] > rankmaps[i][c_1]:\n            Type1_2 += prefcounts[i]\n\n        if rankmaps[i][d] == 1:\n            plu_d += prefcounts[i]\n        elif rankmaps[i][c_1] == 1:\n            plu_c_1 += prefcounts[i]\n        elif rankmaps[i][c_2] == 1:\n            plu_c_2 += prefcounts[i]\n    # print(\"plu=\", plu_d, plu_c_1, plu_c_2)\n    # -------------------CASE I------------------------------\n    MoV_I = math.floor((Type1_1 - Type1_2)/2) + 1\n\n    # -------------------CASE II-------------------------------\n    if math.floor((plu_d + plu_c_2)/2) + 1 <= plu_c_1:\n        MoV_II = math.floor((plu_d - plu_c_2)/2) + 1\n    else:\n        MoV_II = plu_d - math.floor((plu_d + plu_c_1 + plu_c_2)/3) + 1\n        # MoV_II = math.floor((plu_d * 2 - plu_c_1 - plu_c_2) / 3) + 1  # old version\n\n    # -------------------CASE III-----------------------------\n    MoV_d = dict()\n    remaining = sorted(rankmaps[0].keys())\n    remaining.remove(d)\n    remaining.remove(c_1)\n\n    for e in remaining:\n        # ------------count the votes of CASE III---------------\n        T1 = T2 = T3 = T4 = T5 = T6 = T7 = T8 = 0\n        for i in range(len_prefcounts):\n            if rankmaps[i][d] == 1:\n                if rankmaps[i][c_1] < rankmaps[i][e]:\n                    T1 += prefcounts[i]\n                elif rankmaps[i][e] < rankmaps[i][c_1]:\n                    T2 += prefcounts[i]\n            elif rankmaps[i][c_1] == 1:\n                if rankmaps[i][d] < rankmaps[i][e]:\n                    T3 += prefcounts[i]\n                elif rankmaps[i][e] < rankmaps[i][d]:\n                    T4 += prefcounts[i]\n            elif rankmaps[i][e] == 1:\n                if rankmaps[i][d] < rankmaps[i][c_1]:\n                    T5 += prefcounts[i]\n                elif rankmaps[i][c_1] < rankmaps[i][d]:\n                    T6 += prefcounts[i]\n            else:\n                if rankmaps[i][d] < rankmaps[i][e]:\n                    T7 += prefcounts[i]\n                elif rankmaps[i][e] < rankmaps[i][d]:\n                    T8 += prefcounts[i]\n\n        if math.floor((T3 + T4 + T5 + T6)/2) + 1 <= T1 + T2:\n            CHANGE1 = math.floor((T3 + T4 - T5 - T6)/2) + 1\n        else:\n            CHANGE1 = T3 + T4 - T1 -T2 + 1\n\n        x = min(T3, CHANGE1)\n        if T1 + T2 + T3 + T7 - x < T4 + T5 + T6 + T8 + x:\n            MoV_d[e] = CHANGE1\n        else:\n            CHANGE2 = math.floor((T1 + T2 + T3 + T7 - T4 - T5 - T6 - T8)/2) - x + 1\n            MoV_d[e] = CHANGE1 + CHANGE2\n\n    MoV_III = min(MoV_d.items(), key=lambda x: x[1])[1]\n    # ------------------------Overall MoV---------------------------------\n    # print(MoV_d)\n    # print(MoV_I, MoV_II, MoV_III)\n    MoV = min(MoV_I, MoV_II, MoV_III)\n\n    return MoV"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the maximum MOF V maximin score for a single election profile.", "response": "def AppMoVMaximin(profile):\n    \"\"\"\n    Returns an integer that is equal to the margin of victory of the election profile, that is,\n    the smallest number k such that changing k votes can change the winners.\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    # Initialization\n    n = profile.numVoters\n    m = profile.numCands\n\n    # Compute the original winner d\n    wmgMap = profile.getWmg()\n    # Initialize each Copeland score as infinity.\n    maximinscores = {}\n    for cand in wmgMap.keys():\n        maximinscores[cand] = float(\"inf\")\n\n    # For each pair of candidates, calculate the number of votes in which one beat the other.\n\n    # For each pair of candidates, calculate the number of times each beats the other.\n    for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n        if cand2 in wmgMap[cand1].keys():\n            maximinscores[cand1] = min(maximinscores[cand1], wmgMap[cand1][cand2])\n            maximinscores[cand2] = min(maximinscores[cand2], wmgMap[cand2][cand1])\n    d = max(maximinscores.items(), key=lambda x: x[1])[0]\n\n    #Compute c* = argmax_c maximinscores(c)\n    scores_without_d = maximinscores.copy()\n    del scores_without_d[d]\n\n    c_star = max(scores_without_d.items(), key=lambda x: x[1])[0]\n\n    return (maximinscores[d] - maximinscores[c_star])/2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an integer that represents the winning candidate given an election profile.", "response": "def MaximinWinner(profile):\n    \"\"\"\n    Returns an integer that represents the winning candidate given an election profile.\n    Tie-breaking rule: numerically increasing order\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    maximinscores = getMaximinScores(profile)\n    winner = max(maximinscores.items(), key=lambda x: x[1])[0]\n    return winner"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary that associates integer representations of each candidate with their maximin scores.", "response": "def getMaximinScores(profile):\n    \"\"\"\n    Returns a dictionary that associates integer representations of each candidate with their\n    Copeland score.\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n\n    # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n    # allowed however.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported election type\")\n        exit()\n\n    wmgMap = profile.getWmg()\n    # Initialize each Copeland score as infinity.\n    maximinscores = {}\n    for cand in wmgMap.keys():\n        maximinscores[cand] = float(\"inf\")\n\n    # For each pair of candidates, calculate the number of votes in which one beat the other.\n\n    # For each pair of candidates, calculate the number of times each beats the other.\n    for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n        if cand2 in wmgMap[cand1].keys():\n            maximinscores[cand1] = min(maximinscores[cand1], wmgMap[cand1][cand2])\n            maximinscores[cand2] = min(maximinscores[cand2], wmgMap[cand2][cand1])\n\n    return maximinscores"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the Mo - V winner of an election profile.", "response": "def AppMoVCopeland(profile, alpha=0.5):\n    \"\"\"\n    Returns an integer that is equal to the margin of victory of the election profile, that is,\n    the smallest number k such that changing k votes can change the winners.\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    # Initialization\n    n = profile.numVoters\n    m = profile.numCands\n\n    # Compute the original winner d\n    # Initialize each Copeland score as 0.0.\n    copelandscores = {}\n    for cand in profile.candMap.keys():\n        copelandscores[cand] = 0.0\n\n    # For each pair of candidates, calculate the number of votes in which one beat the other.\n    wmgMap = profile.getWmg()\n    for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n        if cand2 in wmgMap[cand1].keys():\n            if wmgMap[cand1][cand2] > 0:\n                copelandscores[cand1] += 1.0\n            elif wmgMap[cand1][cand2] < 0:\n                copelandscores[cand2] += 1.0\n\n            # If a pair of candidates is tied, we add alpha to their score for each vote.\n            else:\n                copelandscores[cand1] += alpha\n                copelandscores[cand2] += alpha\n    d = max(copelandscores.items(), key=lambda x: x[1])[0]\n\n    #Compute c* = argmin_c RM(d,c)\n    relative_margin = {}\n    alter_without_d = delete(range(1, m + 1), d - 1)\n    for c in alter_without_d:\n        relative_margin[c] = RM(wmgMap, n, m, d, c, alpha)\n    c_star = min(relative_margin.items(), key=lambda x: x[1])[0]\n\n    return relative_margin[c_star]*(math.ceil(log(m)) + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CopelandWinner(profile, alpha=0.5):\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    copelandscores = getCopelandScores(profile, alpha)\n    winner = max(copelandscores.items(), key=lambda x: x[1])[0]\n    return winner", "response": "Returns an integer that represents the winning candidate given an election profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getCopelandScores(profile, alpha=0.5, normalize=False):\n\n    # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n    # allowed however.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported election type\")\n        exit()\n\n    # Initialize each Copeland score as 0.0.\n    copelandscores = {}\n    for cand in profile.candMap.keys():\n        copelandscores[cand] = 0.0\n\n    # For each pair of candidates, calculate the number of votes in which one beat the other.\n    wmgMap = profile.getWmg()\n    for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n        if cand2 in wmgMap[cand1].keys():\n            if wmgMap[cand1][cand2] > 0:\n                copelandscores[cand1] += 1.0\n            elif wmgMap[cand1][cand2] < 0:\n                copelandscores[cand2] += 1.0\n\n            # If a pair of candidates is tied, we add alpha to their score for each vote.\n            else:\n                copelandscores[cand1] += alpha\n                copelandscores[cand2] += alpha\n\n    if normalize:\n        m = profile.numCands\n        for cand in profile.candMap.keys():\n            copelandscores[cand] /= (m - 1)\n\n    return copelandscores", "response": "Returns a dictionary that associates integer representations of each candidate with their Copeland score."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MoV_SNTV(profile, K):\n\n    # Currently, we expect the profile to contain complete ordering over candidates.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n        print(\"ERROR: unsupported profile type\")\n        exit()\n\n    m = profile.numCands\n    candScoresMap = MechanismPlurality().getCandScoresMap(profile)\n    if K >= m:\n        return float(\"inf\")\n    # print(candScoresMap)\n    sorted_items = sorted(candScoresMap.items(), key=lambda x: x[1], reverse=True)\n    sorted_dict = {key: value for key, value in sorted_items}\n    sorted_cand = list(sorted_dict.keys())\n    MoV = math.floor((sorted_dict[sorted_cand[K - 1]] - sorted_dict[sorted_cand[K]]) / 2) + 1\n    return MoV", "response": "Returns the number of winning candidates given an election profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the tool parameters as a simple dictionary", "response": "def parameters_dict(self):\n        \"\"\"\n        Get the tool parameters as a simple dictionary\n\n        :return: The tool parameters\n        \"\"\"\n        d = {}\n        for k, v in self.__dict__.items():\n            if not k.startswith(\"_\"):\n                d[k] = v\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parameters(self):\n        parameters = []\n        for k, v in self.__dict__.items():\n            if k.startswith(\"_\"):\n                continue\n\n            is_function = False\n            is_set = False\n\n            if callable(v):\n                value = pickle.dumps(func_dump(v))\n                is_function = True\n            elif isinstance(v, set):\n                value = list(v)\n                is_set = True\n            else:\n                value = v\n\n            parameters.append(dict(\n                key=k,\n                value=value,\n                is_function=is_function,\n                is_set=is_set\n            ))\n\n        return parameters", "response": "Get the tool parameters along with additional information"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the tool parameters model from a dictionary of parameters.", "response": "def parameters_from_model(parameters_model):\n        \"\"\"\n        Get the tool parameters model from dictionaries\n\n        :param parameters_model: The parameters as a mongoengine model\n        :return: The tool parameters as a dictionary\n        \"\"\"\n        parameters = {}\n        for p in parameters_model:\n            if p.is_function:\n                code, defaults, closure = pickle.loads(p.value)\n                parameters[p.key] = func_load(code, defaults, closure, globs=globals())\n            elif p.is_set:\n                parameters[p.key] = set(p.value)\n            else:\n                parameters[p.key] = p.value\n        return parameters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_model(self):\n\n        return ToolModel(\n            name=self.name,\n            version=\"0.0.0\",\n            parameters=self.parameters_from_dicts(self.parameters)\n        )", "response": "Gets the mongoengine model for this tool which serializes parameters that are functions\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite to the history of executions of this tool", "response": "def write_to_history(**kwargs):\n        \"\"\"\n        Write to the history of executions of this tool\n\n        :param kwargs: keyword arguments describing the executions\n        :return: None\n        \"\"\"\n        from hyperstream import HyperStream\n        hs = HyperStream(loglevel=logging.CRITICAL, file_logger=False, console_logger=False, mqtt_logger=None)\n        if hs.current_session:\n            hs.current_session.write_to_history(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the empirical moment conditions for the given set of alternatives.", "response": "def calcMomentsMatlabEmpirical(params):\n    \"\"\"Top 3 alternatives 20 empirical moment conditions\"\"\"\n    alpha = params[0]\n    a = params[1:5]\n    b = params[5:]\n    p1 = alpha*a+(1-alpha)*b\n    p21 = alpha*a[0]*a[1:]/(1-a[0])+(1-alpha)*b[0]*b[1:]/(1-b[0])\n    p22 = alpha*a[1]*np.hstack((a[0],a[2:]))/(1-a[1])+(1-alpha)*b[1]*np.hstack((b[0],b[2:]))/(1-b[1])\n    p23 = alpha*a[2]*np.hstack((a[:2],a[3]))/(1-a[2])+(1-alpha)*b[2]*np.hstack((b[:2],b[3]))/(1-b[2])\n    p24 = alpha*a[3]*a[:3]/(1-a[3])+(1-alpha)*b[3]*b[:3]/(1-b[3])\n    p3 = np.array([\n        alpha*a[0]*a[2]*a[3]/(1-a[2])/(a[0]+a[1])+(1-alpha)*b[0]*b[2]*b[3]/(1-b[2])/(b[0]+b[1]),\n        alpha*a[0]*a[1]*a[3]/(1-a[3])/(a[1]+a[2])+(1-alpha)*b[0]*b[1]*b[3]/(1-b[3])/(b[1]+b[2]),\n        alpha*a[0]*a[1]*a[2]/(1-a[0])/(a[3]+a[2])+(1-alpha)*b[0]*b[1]*b[2]/(1-b[0])/(b[3]+b[2]),\n        alpha*a[2]*a[1]*a[3]/(1-a[1])/(a[0]+a[3])+(1-alpha)*b[2]*b[1]*b[3]/(1-b[1])/(b[0]+b[3])\n        ])\n    return np.concatenate((p1,p21,p22,p23,p24,p3))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aggregate(self, rankings, algorithm, epsilon, max_iters, approx_step, opto=\"scipy\", true_params=None):\n        t0 = None\n        t1 = None\n        t2 = None\n        t0 = time.perf_counter() ###################\n        if opto.startswith(\"matlab\") and self.matlabEng is None:\n            raise ValueError(\"invalid argument for opto: matlab engine not available\")\n\n        funcs = None\n        try:\n            funcs = GMMMixPLAggregator.mixPLalgorithms[algorithm]\n        except KeyError:\n            raise ValueError(\"invalid argument value for algorithm: '\" + str(algorithm) + \"'\")\n\n        # choose constraints for objective function\n        # Aeq = None\n        # beq = None\n        # if algorithm.endswith(\"uncons\"):\n        #     if opto == \"scipy\":\n        #         raise NotImplementedError(\"unconstrained optimization with scipy not implemented\")\n        #     else: # opto startswith \"matlab\"\n        #         Aeq = self.Aeq_uncons\n        #         beq = self.beq_uncons\n        # else: # opto is constrained\n        #     Aeq = self.Aeq\n        #     beq = self.beq\n\n        # compute moment condition values\n        moments = None\n        mixPLobjective_partial = None\n        if opto.startswith(\"matlab_emp\"):\n            if true_params is None:\n                raise ValueError(\"invalid value 'true_params=None' when 'opto=\" + opto + \"''\")\n            if algorithm.startswith(\"top3_full\"):\n                moments = calcMomentsMatlabEmpirical(true_params)\n            elif algorithm.startswith(\"top3_min\"):\n                moments = calcMomentsMatlabEmpirical_reduced(true_params)\n            else:\n                raise NotImplementedError(\"matlab empirical optimization not implemented for moments: '\" + algorithm + \"''\")\n        else: # opto is exactly \"scipy\" or \"matlab\"\n            moments = funcs.calcMoments(rankings)\n            # partial function only used for scipy\n            mixPLobjective_partial = functools.partial(funcs.mixPLobjective,\n                                                       moments=moments\n                                                      )\n\n        # generate an initial guess for the optimizer\n        params_t0 = np.empty(2*self.m + 1)\n        params_t0[0] = np.random.rand()\n        params_t0[1:self.m+1] = np.random.dirichlet(np.ones(self.m))\n        params_t0[self.m+1:] = np.random.dirichlet(np.ones(self.m))\n\n        # optimization\n        res = None\n        if opto == \"scipy\":\n            res = scipy.optimize.minimize(mixPLobjective_partial,\n                                          params_t0,\n                                          method=\"SLSQP\",\n                                          bounds=self.bounds_pairs,\n                                          constraints=self.cons,\n                                          options={\n                                            'disp': False,\n                                            'maxiter': max_iters,\n                                            'ftol': epsilon,\n                                            'eps': approx_step\n                                            }\n                                         )\n            res = res.x\n        elif opto.startswith(\"matlab\"):\n            tolfun = 1e-10\n            tolx = 1e-10\n            tolcon = 1e-8\n            if opto.endswith(\"_default\"): # default tolerances for interior-point\n                tolfun = 1e-6\n                tolx = 1e-10\n                tolcon = 1e-6\n            elif opto.endswith(\"_ultra\"): # \"optimized\" tolerances for interior-point\n                tolfun = 1e-13\n                tolx = 1e-13\n                tolcon = 1e-9\n\n            moments = matlab.double(moments.tolist())\n            params_t0 = matlab.double(params_t0.tolist())\n            t1 = time.perf_counter() ###################\n            res, val, fl = self.matlabEng.optimize(\"gmm_mixpl_objectives.\" + funcs.mixPLobjective.__name__,\n                                                   moments,\n                                                   params_t0,\n                                                   self.A,\n                                                   self.b,\n                                                   Aeq,\n                                                   beq,\n                                                   self.lb,\n                                                   self.ub,\n                                                   {\"Algorithm\": \"interior-point\",\n                                                    \"Display\": \"off\",\n                                                    \"TolFun\": tolfun,\n                                                    \"TolX\": tolx,\n                                                    \"TolCon\": tolcon},\n                                                   nargout=3\n                                                  )\n            t2 = time.perf_counter() ###################\n            print(\"t1\", t0)\n            res = np.array(res[0])\n\n        return (res, .5, .5)", "response": "This function computes the mixing proportions and parameters for a single language language."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting the dominance parameters for a single classical monitoring. km3net. de style.", "response": "def plot_dom_parameters(\n        data,\n        detector,\n        filename,\n        label,\n        title,\n        vmin=0.0,\n        vmax=10.0,\n        cmap='RdYlGn_r',\n        under='deepskyblue',\n        over='deeppink',\n        underfactor=1.0,\n        overfactor=1.0,\n        missing='lightgray',\n        hide_limits=False\n):\n    \"\"\"Creates a plot in the classical monitoring.km3net.de style.\n\n    Parameters\n    ----------\n    data: dict((du, floor) -> value)\n    detector: km3pipe.hardware.Detector() instance\n    filename: filename or filepath\n    label: str\n    title: str\n    underfactor: a scale factor for the points used for underflow values\n    overfactor: a scale factor for the points used for overflow values\n    hide_limits: do not show under/overflows in the plot\n\n    \"\"\"\n    x, y, _ = zip(*detector.doms.values())\n    fig, ax = plt.subplots(figsize=(10, 6))\n    cmap = plt.get_cmap(cmap)\n    cmap.set_over(over, 1.0)\n    cmap.set_under(under, 1.0)\n\n    m_size = 100\n    scatter_args = {\n        'edgecolors': 'None',\n        'vmin': vmin,\n        'vmax': vmax,\n    }\n    sc_inactive = ax.scatter(\n        x, y, c=missing, label='missing', s=m_size * 0.9, **scatter_args\n    )\n\n    xa, ya = map(np.array, zip(*data.keys()))\n    zs = np.array(list(data.values()))\n    in_range_idx = np.logical_and(zs >= vmin, zs <= vmax)\n    sc = ax.scatter(\n        xa[in_range_idx],\n        ya[in_range_idx],\n        c=zs[in_range_idx],\n        cmap=cmap,\n        s=m_size,\n        **scatter_args\n    )\n    if not hide_limits:\n        under_idx = zs < vmin\n        ax.scatter(\n            xa[under_idx],\n            ya[under_idx],\n            c=under,\n            label='< {0}'.format(vmin),\n            s=m_size * underfactor,\n            **scatter_args\n        )\n        over_idx = zs > vmax\n        ax.scatter(\n            xa[over_idx],\n            ya[over_idx],\n            c=over,\n            label='> {0}'.format(vmax),\n            s=m_size * overfactor,\n            **scatter_args\n        )\n\n    cb = plt.colorbar(sc)\n    cb.set_label(label)\n\n    ax.set_title(\n        \"{0}\\n{1} UTC\".format(title,\n                              datetime.utcnow().strftime(\"%c\"))\n    )\n    ax.set_xlabel(\"DU\")\n    ax.set_ylabel(\"DOM\")\n    ax.set_ylim(-2)\n    ax.set_yticks(range(1, 18 + 1))\n    major_locator = pylab.MaxNLocator(integer=True)\n    sc_inactive.axes.xaxis.set_major_locator(major_locator)\n\n    ax.legend(\n        bbox_to_anchor=(0., -.16, 1., .102),\n        loc=1,\n        ncol=2,\n        mode=\"expand\",\n        borderaxespad=0.\n    )\n\n    fig.tight_layout()\n\n    plt.savefig(filename, dpi=120, bbox_inches=\"tight\")\n    plt.close('all')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_dom_map(pmt_directions, values, nside=512, d=0.2, smoothing=0.1):\n    import healpy as hp\n    discs = [hp.query_disc(nside, dir, 0.2) for dir in pmt_directions]\n    npix = hp.nside2npix(nside)\n    pixels = np.zeros(npix)\n    for disc, value in zip(discs, values):\n        for d in disc:\n            pixels[d] = value\n    if smoothing > 0:\n        return hp.sphtfunc.smoothing(pixels, fwhm=smoothing, iter=1)\n    return pixels", "response": "Create a map of a DOM with given PMTs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the calculated intervals for the current channel", "response": "def calculated_intervals(self, value):\n        \"\"\"\n        Set the calculated intervals\n        This will be written to the stream_status collection if it's in the database channel\n\n        :param value: The calculated intervals\n        :type value: TimeIntervals, TimeInterval, list[TimeInterval]\n        \"\"\"\n        if not value:\n            self._calculated_intervals = TimeIntervals()\n            return\n\n        if isinstance(value, TimeInterval):\n            value = TimeIntervals([value])\n        elif isinstance(value, TimeIntervals):\n            pass\n        elif isinstance(value, list):\n            value = TimeIntervals(value)\n        else:\n            raise TypeError(\"Expected list/TimeInterval/TimeIntervals, got {}\".format(type(value)))\n\n        for interval in value:\n            if interval.end > utcnow():\n                raise ValueError(\"Calculated intervals should not be in the future\")\n\n        self._calculated_intervals = value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npurge the stream. This removes all data and clears the calculated intervals :return: None", "response": "def purge(self):\n        \"\"\"\n        Purge the stream. This removes all data and clears the calculated intervals\n\n        :return: None\n        \"\"\"\n        self.channel.purge_stream(self.stream_id, remove_definition=False, sandbox=None)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef window(self, time_interval=None, force_calculation=False):\n        if not time_interval:\n            if self.calculated_intervals:\n                time_interval = self.calculated_intervals[-1]\n            else:\n                raise ValueError(\"No calculations have been performed and no time interval was provided\")\n        elif isinstance(time_interval, TimeInterval):\n            time_interval = TimeInterval(time_interval.start, time_interval.end)\n        elif isinstance(time_interval, Iterable):\n            time_interval = parse_time_tuple(*time_interval)\n            if isinstance(time_interval, RelativeTimeInterval):\n                raise NotImplementedError\n        elif isinstance(time_interval, RelativeTimeInterval):\n            raise NotImplementedError\n        else:\n            raise TypeError(\"Expected TimeInterval or (start, end) tuple of type str or datetime, got {}\"\n                            .format(type(time_interval)))\n        return StreamView(stream=self, time_interval=time_interval, force_calculation=force_calculation)", "response": "Returns a StreamView object for the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(self):\n        with switch_db(StreamDefinitionModel, 'hyperstream'):\n            self.mongo_model = StreamDefinitionModel.objects.get(__raw__=self.stream_id.as_raw())\n            self._calculated_intervals = self.mongo_model.get_calculated_intervals()", "response": "Load the stream definition from the database"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the calculated intervals from the database", "response": "def calculated_intervals(self):\n        \"\"\"\n        Gets the calculated intervals from the database\n\n        :return: The calculated intervals\n        \"\"\"\n        if self._calculated_intervals is None:\n            logging.debug(\"get calculated intervals\")\n            self.load()\n            return self.mongo_model.get_calculated_intervals()\n        return self._calculated_intervals"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the calculated intervals in the database. Performs an upsert .", "response": "def calculated_intervals(self, intervals):\n        \"\"\"\n        Updates the calculated intervals in the database. Performs an upsert\n\n        :param intervals: The calculated intervals\n        :return: None\n        \"\"\"\n        logging.debug(\"set calculated intervals\")\n        self.mongo_model.set_calculated_intervals(intervals)\n        self.save()\n        self._calculated_intervals = TimeIntervals(intervals)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculated_intervals(self, intervals):\n        if len(intervals) > 1:\n            raise ValueError(\"Only single calculated interval valid for AssetStream\")\n        super(AssetStream, self.__class__).calculated_intervals.fset(self, intervals)", "response": "Updates the calculated intervals in the database. Performs an upsert\n\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ask(self, question, default=False):\n        choices = '[%s/%s]' % ('Y' if default else 'y', 'n' if default else 'N')\n        while True:\n            response = raw_input('%s %s' % (question, choices)).strip()\n            if not response:\n                return default\n            elif response in 'yYoO':\n                return True\n            elif response in 'nN':\n                return False", "response": "Ask a y or n question to the user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nedit a text using an external editor.", "response": "def edit(self, text):\n        \"\"\" Edit a text using an external editor.\n        \"\"\"\n        if isinstance(text, unicode):\n            text = text.encode(self._encoding)\n        if self._editor is None:\n            printer.p('Warning: no editor found, skipping edit')\n            return text\n        with tempfile.NamedTemporaryFile(mode='w+', suffix='kolekto-edit') as ftmp:\n            ftmp.write(text)\n            ftmp.flush()\n            subprocess.Popen([self._editor, ftmp.name]).wait()\n            ftmp.seek(0)\n            edited = ftmp.read()\n            return edited"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register(self, plugin):\n        self.needed_listeners -= plugin.listeners\n        self.needed_messengers -= plugin.messengers\n\n        if self.needed_messengers == self.needed_listeners == set():\n            self.valid = True\n\n        self.dispatcher.register(plugin)", "response": "Register a feather. plugin. Plugin and tell our dispatcher about it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        if not self.valid:\n            err = (\"\\nMessengers and listeners that still need set:\\n\\n\"\n                   \"messengers : %s\\n\\n\"\n                   \"listeners : %s\\n\") \n            raise InvalidApplication(err % (self.needed_messengers,\n                                            self.needed_listeners))\n        self.dispatcher.start()", "response": "Start the dispatcher if we have a set of needed messengers and the calendars that need to be set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef execute_condition(cond):\n\n    condition_method = 'rulengine.conditions.c_{0}_{1}'.format(\n        cond.data_type, cond.operator)\n    try:\n        func = import_class(condition_method)\n    except AttributeError:\n        condition_method = 'rulengine.conditions.c_{0}'.format(\n            cond.data_type)\n        func = import_class(condition_method)\n\n    executable_cond = convert_condition_to_executable(cond)\n    return func(executable_cond)", "response": "Execute a condition for the current language and return a function that returns the rule instance for the given operator and\n ArcGIS object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, instance):\n        '''\n        Insert a record to the table \n        emp = Employee(emp_id=1, name='Jane Doe', sex='Female')\n        inserted = Session().add(emp)\n        Returns a bool or raises mysql.Error exception\n        '''\n\n        table = instance.__class__.__name__.lower()\n        keys = \", \".join(vars(instance).keys())\n        values = tuple(vars(instance).values())\n\n        SQL = \"\"\"INSERT INTO %s (%s) VALUES(\"\"\" % (table, keys)\n        SQL += \"\"\" \"%s\",\"\"\" * len(values) % (values)\n        SQL = SQL[:-1] + \")\"\n\n        if self.DEBUG: print(SQL)\n\n        with Session(self.settings) as conn:\n            try:\n                c = conn.cursor()\n                c.execute(SQL)\n                conn.commit()\n                return True\n            except mysql.Error as e:\n                if e.errno==1062:\n                    print(\"Already saved to the database\")\n                    return True\n                elif e.errno == 1146:\n                    # Table does not exist\n                    return True\n                else:\n                    raise e\n\n                return True", "response": "Insert a record to the table \n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, instance):\n        '''\n        Update a record to the table \n        emp = Employee(emp_id=1, name='John Doe', sex='Female')\n        updated = Session().add(emp)\n        Returns a bool or raises mysql.Error exception\n        '''\n\n        if not self.exists(instance.__class__, instance.id):\n            print(\"Cant update a value that does not exist\")\n            return False\n\n        instance_dict = vars(instance)\n\n        table = instance.__class__.__name__.lower()\n        keys = \", \".join(instance_dict.keys())\n        values = tuple(instance_dict.values())\n\n        SQL = \"\"\"UPDATE {} SET \"\"\".format(table)\n\n        for key, value in instance_dict.items():\n            SQL += \"\"\" %s = \"%s\" ,\"\"\" % (key, value)\n\n        SQL = SQL[:-1] + \"\"\" WHERE id = \"{}\" \"\"\".format(instance.id)\n\n        if self.DEBUG: print(SQL)\n\n        try:\n            with Session(self.settings) as conn:\n                c = conn.cursor()\n                c.execute(SQL)\n                conn.commit()\n                return True\n        except Exception as e:\n            raise e", "response": "Update a record in the table \n"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes all records of the given model class.", "response": "def delete(self, model_class, **where):\n        '''\n        Session().delete(Employee, id=1, name='John Doe')\n        '''\n        assert hasattr(model_class, '_fields'), 'Not a valid class'\n        table = model_class.__name__.lower()\n\n        with Session() as conn:\n            SQL = f'DELETE FROM {table} WHERE'\n            if not where:\n                raise ValueError('Specify WHERE conditions as kwargs')\n\n            i = 1\n            for k, v in where.items():\n                SQL+= \" %s = '%s' \"%(k,v) if i ==1 else \"AND %s = '%s' \"%(k,v)\n                i +=1\n\n            c= conn.cursor()\n            c.execute(SQL)\n            conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndrops all records from the table model_class.", "response": "def delete_all(self, model_class):\n        '''Drop all records from the table model_class.__name__.lower()\n        '''\n        assert hasattr(model_class, '_fields'), 'Not a valid model class'\n\n        table = model_class.__name__.lower()\n        with Session() as conn:\n            SQL = f'DELETE FROM {table}'\n            conn.cursor().execute(SQL)\n            conn.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, model_class, strict=True, returnDict=False, fetchOne=False, **where):\n        '''params:\n        model_class: The queried model class\n        strict: bool -> If True, queries are run with EQUAL(=) operator.\n        If False: Queries are run with RLIKE keyword\n        returnDict: bool -> Return a list if dictionaries(field_names: values)\n        fetchOne: bool -> cursor.fetchone() else: cursor.fetchall()\n        where: **kwargs for quere WHERE condition.\n        if where in {}: Returns all results in the table\n\n        Usage:\n        print(Session().get(Employee, id=1, returnDict=True))\n        '''\n        self.typeassert(model_class, strict, returnDict, where)\n\n        table = model_class.__name__.lower()\n        with Session(self.settings) as conn:\n            if not where:\n                query = f'SELECT * FROM {table}'\n            else:\n                query = f'SELECT * FROM {table} WHERE'\n                \n            index= 1\n            operator = '=' if strict else 'RLIKE'\n\n            for key, value in where.items():\n                if index == 1:\n                    query+= \" %s %s '%s' \"%(key, operator, value)\n                else:\n                    query+= \" AND %s %s '%s' \"%(key, operator, value)\n                index += 1\n                \n            try:\n                cursor=conn.cursor()\n                cursor.execute(query)\n            except mysql.Error as e:\n                if e.errno == 1146:\n                    print(f\"The table {table} does not exist\")\n                    return []\n\n                else:\n                    raise e\n            else:\n                if fetchOne:\n                    colnames = [d[0] for d in cursor.description]\n                    results = cursor.fetchone()\n                    if returnDict:\n                        return {col: val for col, val in zip(colnames, results)}\\\n                        if results else {}\n                    return results\n\n                return self.handleResult(cursor, returnDict)", "response": "Get all the related entries of a specific model class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exists(self, model_class, ID):\n        '''Check if a record of id==ID exists in table model_class.__name__.lower()'''\n\n        assert hasattr(model_class, '_fields'), 'Not a valid model class'\n        res = self.get(model_class, id=ID, fetchOne=True)\n        if res:\n            return True\n        return False", "response": "Check if a record of id == ID exists in table model_class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing database migrations for the current version of the tables. Returns True if no exception else raises an unhandled exception.", "response": "def makemigrations(self):\n        ''' Do database migrations\n        1. Creates new tables from models\n        2. Updates columns and columns\n\n        Returns True if no exception else raises an unhandled exception\n        '''\n\n        UNCHANGED = []\n\n        with Session(self.settings) as conn:\n            cursor = conn.cursor()\n            for name, model in self.models.items():\n                print(\"Running migrations... on table: %s\"%model.__name__.lower())\n                columns = self.description(model)\n        \n                table = name.lower()\n                QUERY = \"CREATE TABLE IF NOT EXISTS %s (\"%table\n\n                for field, FieldType in model.columns.items():\n                    QUERY += \"%s %s, \" % (field, FieldType)\n                    # If no columns --> Table not created yet\n                    if columns:\n                        self.UpdateColums(cursor, field, FieldType, \n                            model, columns, UNCHANGED)\n\n                QUERY = QUERY[:-2] + \") ENGINE=InnoDB\"\n            \n                print(QUERY)\n                \n                try:\n                    cursor.execute(QUERY)\n                except mysql.Error as e:\n                    raise e\n\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the columns. Dont call directly", "response": "def UpdateColums(self, cursor, field, FieldType, model, columns, UNCHANGED):\n        '''Updates the columns. Dont call directly\n        '''\n        table = model.__name__.lower()\n        if field not in columns:\n            n = UNCHANGED.pop()\n            new_sql = f\"ALTER TABLE {table} ADD COLUMN {field} {FieldType} AFTER {n}\"\n            cursor.execute(new_sql)\n            print(\"\\n\\n\", new_sql)\n        else:\n            UNCHANGED.append(field)\n\n        # We drop the fields in the table not in models\n        TCOLS = set(columns)-set(model._fields)\n        for col in TCOLS:\n            columns.remove(col)\n            QRY = f\"ALTER TABLE {table} DROP COLUMN {col}\"\n            cursor.execute(QRY)\n            print(\"\\n\\n\", QRY)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a representation of this object that can be used with mongoengine Document. objects", "response": "def as_raw(self):\n        \"\"\"\n        Return a representation of this object that can be used with mongoengine Document.objects(__raw__=x)\n        Example:\n\n        >>> stream_id = StreamId(name='test', meta_data=((u'house', u'1'), (u'resident', u'1')))\n        >>> stream_id.as_raw()\n        {'stream_id.meta_data': [(u'house', u'1'), (u'resident', u'1')], 'stream_id.name': 'test'}\n\n        :return: The raw representation of this object.\n        \"\"\"\n        return dict(('stream_id.' + k, v) for k, v in self.as_dict().items())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef srv_event(token, hits, url=RBA_URL):\n\n    if url is None:\n        log.error(\"Please provide a valid RainbowAlga URL.\")\n        return\n\n    ws_url = url + '/message'\n\n    if isinstance(hits, pd.core.frame.DataFrame):\n        pos = [tuple(x) for x in hits[['x', 'y', 'z']].values]\n        time = list(hits['time'])\n        tot = list(hits['tot'])\n    elif isinstance(hits, Table):\n        pos = list(zip(hits.pos_x, hits.pos_y, hits.pos_z))\n        time = list(hits.time)\n        tot = list(hits.tot)\n    else:\n        log.error(\n            \"No calibration information found in hits (type: {0})\".format(\n                type(hits)\n            )\n        )\n        return\n\n    event = {\n        \"hits\": {\n            'pos': pos,\n            'time': time,\n            'tot': tot,\n        }\n    }\n\n    srv_data(ws_url, token, event, 'event')", "response": "Serve event to RainbowAlga"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserve data to RainbowAlga", "response": "def srv_data(url, token, data, kind):\n    \"\"\"Serve data to RainbowAlga\"\"\"\n    ws = websocket.create_connection(url)\n    message = {'token': token, 'data': data, 'kind': kind}\n    ws.send(pd.io.json.dumps(message))\n    ws.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts message to JSON and send it to the client with token", "response": "def raw_message_to(self, token, message):\n        \"\"\"Convert message to JSON and send it to the client with token\"\"\"\n        if token not in self._clients:\n            log.critical(\"Client with token '{0}' not found!\".format(token))\n            return\n        client = self._clients[token]\n        try:\n            client.write_message(message)\n        except (AttributeError, tornado.websocket.WebSocketClosedError):\n            log.error(\"Lost connection to client '{0}'\".format(client))\n        else:\n            print(\"Sent {0} bytes.\".format(len(message)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message(self, data, kind=\"info\"):\n        message = pd.io.json.dumps({'kind': kind, 'data': data})\n        print(\"Sent {0} bytes.\".format(len(message)))\n        self.write_message(message)", "response": "Convert a message to json and send it to the clients"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef schema(ds):\n    tables, ref_tables = {}, {}\n    table_lookup = {t.url.string: t for t in ds.tables if ds.get_tabletype(t)}\n    for table in table_lookup.values():\n        spec = TableSpec(ds.get_tabletype(table))\n        spec.primary_key = [\n            c for c in table.tableSchema.columns if\n            c.propertyUrl and c.propertyUrl.uri == term_uri('id')][0].name\n        # Map the column name to the default:\n        if spec.name in PROPERTY_URL_TO_COL:\n            spec.primary_key = PROPERTY_URL_TO_COL[spec.name][term_uri('id')]\n        for c in table.tableSchema.columns:\n            if c.propertyUrl and c.propertyUrl.uri == term_uri('source'):\n                # A column referencing sources is replaced by an association table.\n                otype = ds.get_tabletype(table).replace('Table', '')\n                ref_tables[ds.get_tabletype(table)] = TableSpec(\n                    '{0}Source'.format(otype),  # The name of the association table.\n                    [ColSpec(otype + '_ID'), ColSpec('Source_ID'), ColSpec('Context')],\n                    [\n                        (  # The foreign key to the referencing object:\n                            ['dataset_ID', otype + '_ID'],\n                            ds.get_tabletype(table),\n                            ['dataset_ID', spec.primary_key]),\n                        (  # The foreign key to the referenced source:\n                            ['dataset_ID', 'Source_ID'],\n                            'SourceTable',\n                            ['dataset_ID', 'ID']),\n                    ],\n                    c.name)\n            else:\n                cname = c.header\n                if c.propertyUrl and spec.name in PROPERTY_URL_TO_COL:\n                    if c.propertyUrl.uri in PROPERTY_URL_TO_COL[spec.name]:\n                        cname = PROPERTY_URL_TO_COL[spec.name][c.propertyUrl.uri]\n                spec.columns.append(ColSpec(\n                    cname,\n                    c.datatype.base if c.datatype else c.datatype,\n                    c.separator,\n                    cname == spec.primary_key,\n                    cldf_name=c.header))\n        for fk in table.tableSchema.foreignKeys:\n            if fk.reference.schemaReference:\n                # We only support Foreign Key references between tables!\n                continue  # pragma: no cover\n            ref = table_lookup[fk.reference.resource.string]\n            ref_type = ds.get_tabletype(ref)\n            if ref_type:\n                colRefs = sorted(fk.columnReference)\n                if spec.name in PROPERTY_URL_TO_COL:\n                    # Must map foreign keys\n                    colRefs = []\n                    for c in sorted(fk.columnReference):\n                        c = ds[spec.name, c]\n                        if c.propertyUrl and c.propertyUrl.uri in PROPERTY_URL_TO_COL[spec.name]:\n                            colRefs.append(PROPERTY_URL_TO_COL[spec.name][c.propertyUrl.uri])\n                        else:\n                            colRefs.append(c.header)\n                rcolRefs = sorted(fk.reference.columnReference)\n                if ref_type in PROPERTY_URL_TO_COL:\n                    # Must map foreign key targets!\n                    rcolRefs = []\n                    for c in sorted(fk.reference.columnReference):\n                        c = ds[ref_type, c]\n                        if c.propertyUrl and c.propertyUrl.uri in PROPERTY_URL_TO_COL[ref_type]:\n                            rcolRefs.append(PROPERTY_URL_TO_COL[ref_type][c.propertyUrl.uri])\n                        else:\n                            rcolRefs.append(c.header)\n                spec.foreign_keys.append((\n                    tuple(['dataset_ID'] + colRefs),\n                    ds.get_tabletype(table_lookup[fk.reference.resource.string]),\n                    tuple(['dataset_ID'] + rcolRefs)))\n        tables[spec.name] = spec\n\n    # must determine the order in which tables must be created!\n    ordered = OrderedDict()\n    i = 0\n    #\n    # We loop through the tables repeatedly, and whenever we find one, which has all\n    # referenced tables already in ordered, we move it from tables to ordered.\n    #\n    while tables and i < 100:\n        i += 1\n        for table in list(tables.keys()):\n            if all(ref[1] in ordered for ref in tables[table].foreign_keys):\n                # All referenced tables are already created.\n                ordered[table] = tables.pop(table)\n                break\n    if tables:  # pragma: no cover\n        raise ValueError('there seem to be cyclic dependencies between the tables')\n\n    return list(ordered.values()), ref_tables", "response": "Convert the table and column descriptions of a Dataset into a pair of tables and reference tables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new db file with the core schema.", "response": "def create(self, force=False, exists_ok=False):\n        \"\"\"\n        Creates a db file with the core schema.\n\n        :param force: If `True` an existing db file will be overwritten.\n        \"\"\"\n        if self.fname and self.fname.exists():\n            if force:\n                self.drop()\n            elif exists_ok:\n                return\n            else:\n                raise ValueError('db file already exists, use force=True to overwrite')\n        with self.connection() as db:\n            db.execute(\n                \"\"\"\\\nCREATE TABLE dataset (\n    ID TEXT PRIMARY KEY NOT NULL,\n    name TEXT,\n    version TEXT,\n    metadata_json TEXT\n)\"\"\")\n            db.execute(\"\"\"\\\nCREATE TABLE datasetmeta (\n    dataset_ID TEXT ,\n    key TEXT,\n    value TEXT,\n    PRIMARY KEY (dataset_ID, key),\n    FOREIGN KEY(dataset_ID) REFERENCES dataset(ID)\n)\"\"\")\n            db.execute(\"\"\"\\\nCREATE TABLE SourceTable (\n    dataset_ID TEXT ,\n    ID TEXT ,\n    bibtex_type TEXT,\n    {0}\n    extra TEXT,\n    PRIMARY KEY (dataset_ID, ID),\n    FOREIGN KEY(dataset_ID) REFERENCES dataset(ID)\n)\"\"\".format('\\n    '.join('`{0}` TEXT,'.format(f) for f in BIBTEX_FIELDS)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(self, ds, verbose=False):\n        try:\n            self.fetchone('select ID from dataset')\n        except sqlite3.OperationalError:\n            self.create(force=True)\n        self.unload(ds)\n        dataset = ds.cldf.wl\n        tables, ref_tables = schema(dataset)\n\n        # update the DB schema:\n        for t in tables:\n            if self._create_table_if_not_exists(t):\n                continue\n            db_cols = self.tables[t.name]\n            for col in t.columns:\n                if col.name not in db_cols:\n                    with self.connection() as conn:\n                        conn.execute(\n                            \"ALTER TABLE {0} ADD COLUMN `{1.name}` {1.db_type}\".format(\n                                t.name, col))\n                else:\n                    if db_cols[col.name] != col.db_type:\n                        raise ValueError(\n                            'column {0}:{1} {2} redefined with new type {3}'.format(\n                                t.name, col.name, db_cols[col.name], col.db_type))\n\n        for t in ref_tables.values():\n            self._create_table_if_not_exists(t)\n\n        self.update_schema()\n\n        # then load the data:\n        with self.connection() as db:\n            db.execute('PRAGMA foreign_keys = ON;')\n            insert(\n                db,\n                'dataset',\n                'ID,name,version,metadata_json',\n                (\n                    ds.id,\n                    '{0}'.format(dataset),\n                    git_hash(ds.dir),\n                    dumps(dataset.metadata_dict)))\n            insert(\n                db,\n                'datasetmeta',\n                'dataset_ID,key,value',\n                *[(ds.id, k, '{0}'.format(v)) for k, v in dataset.properties.items()])\n\n            # load sources:\n            rows = []\n            for src in dataset.sources.items():\n                values = [ds.id, src.id, src.genre] + [src.get(k) for k in BIBTEX_FIELDS]\n                values.append(\n                    dumps({k: v for k, v in src.items() if k not in BIBTEX_FIELDS}))\n                rows.append(tuple(values))\n            insert(\n                db,\n                'SourceTable',\n                ['dataset_ID', 'ID', 'bibtex_type'] + BIBTEX_FIELDS + ['extra'],\n                *rows)\n\n            # For regular tables, we extract and keep references to sources.\n            refs = defaultdict(list)\n\n            for t in tables:\n                # We want to lookup columns by the name used in the CLDF dataset.\n                cols = {col.cldf_name: col for col in t.columns}\n                # But we also want to look up primary keys by the database column name.\n                cols_by_name = {col.name: col for col in t.columns}\n\n                ref_table = ref_tables.get(t.name)\n                rows, keys = [], []\n                try:\n                    for row in dataset[t.name]:\n                        keys, values = ['dataset_ID'], [ds.id]\n                        for k, v in row.items():\n                            if ref_table and k == ref_table.consumes:\n                                col = cols_by_name[t.primary_key]\n                                refs[ref_table.name].append((row[col.cldf_name], v))\n                            else:\n                                col = cols[k]\n                                if isinstance(v, list):\n                                    v = (col.separator or ';').join(\n                                        nfilter(col.convert(vv) for vv in v))\n                                else:\n                                    v = col.convert(v)\n                                keys.append(\"`{0}`\".format(col.name))\n                                values.append(v)\n                        keys, values = self.update_row(t.name, keys, values)\n                        rows.append(tuple(values))\n                    insert(db, t.name, keys, *rows, **{'verbose': verbose})\n                except FileNotFoundError:\n                    if t.name != 'CognateTable':  # An empty CognateTable is allowed.\n                        raise  # pragma: no cover\n\n            # Now insert the references, i.e. the associations with sources:\n            for tname, items in refs.items():\n                rows = []\n                for oid, sources in items:\n                    for source in sources:\n                        sid, context = Sources.parse(source)\n                        rows.append([ds.id, oid, sid, context])\n                oid_col = '{0}_ID'.format(tname.replace('Source', ''))\n                insert(db, tname, ['dataset_ID', oid_col, 'Source_ID', 'Context'], *rows)\n            db.commit()", "response": "Load a CLDF dataset into the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a rule to the list of rules.", "response": "def add_rule(self, rule):\n        \"\"\"Supported rules: `a -> b` and `a => b` (terminal rule).\"\"\"\n        parsed_rule = None\n\n        if rule.count('->') == 1 and rule.count('=>') == 0:\n            parsed_rule = tuple(''.join(part.split()) for part in rule.split('->')) + (0,)\n        elif rule.count('->') == 0 and rule.count('=>') == 1:\n            parsed_rule = tuple(''.join(part.split()) for part in rule.split('=>')) + (1,)\n\n        if parsed_rule is None:\n            raise SyntaxError('Wrong format: ' + rule)\n        else:\n            self.rules.append(parsed_rule)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef execute_once(self, string):\n        for rule in self.rules:\n            if rule[0] in string:\n                pos = string.find(rule[0])\n                self.last_rule = rule\n                return string[:pos] + rule[1] + string[pos+len(rule[0]):]\n        self.last_rule = None\n        return string", "response": "Execute only one rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes algorithm (if max_times = None, there can be forever loop).", "response": "def execute(self, string, max_tacts=None):\n        \"\"\"Execute algorithm (if max_times = None, there can be forever loop).\"\"\"\n        counter = 0\n        self.last_rule = None\n\n        while True:\n            string = self.execute_once(string)\n            if self.last_rule is None or self.last_rule[2]:\n                break\n            counter += 1\n            if max_tacts is not None and counter >= max_tacts:\n                raise TimeoutError(\"algorithm hasn't been stopped\")\n\n        return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns python code for create and execute algo.", "response": "def compile(self):\n        \"\"\"Return python code for create and execute algo.\"\"\"\n        result = TEMPLATE\n\n        for rule in self.rules:\n            if rule[2]:\n                arrow = '=>'\n            else:\n                arrow = '->'\n            repr_rule = repr(rule[0] + arrow + rule[1])\n\n            result += \"algo.add_rule({repr_rule})\\n\".format(repr_rule=repr_rule)\n\n        result += \"for line in stdin:\\n\"\n        result += \"    print(algo.execute(''.join(line.split())))\"\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a logger with the given name.", "response": "def create_logger(name: str) -> Logger:\n    \"\"\"\n    Creates a logger with the given name.\n    :param name: name of the logger (gets prefixed with the package name)\n    :return: the created logger\n    \"\"\"\n    logger = logging.getLogger(f\"{PACKAGE_NAME}.{name}\")\n    logger.addHandler(StreamHandler())\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of keyword - only arguments for a MIT .", "response": "def kwonly_args(kws, required, withdefaults=(), leftovers=False):\n    \"\"\"\n    Based on the snippet by Eric Snow\n    http://code.activestate.com/recipes/577940\n\n    SPDX-License-Identifier: MIT\n    \"\"\"\n\n    if hasattr(withdefaults, 'items'):\n        # allows for OrderedDict to be passed\n        withdefaults = withdefaults.items()\n\n    kwonly = []\n\n    # extract the required keyword-only arguments\n    missing = []\n    for name in required:\n        if name not in kws:\n            missing.append(name)\n        else:\n            kwonly.append(kws.pop(name))\n\n    # validate required keyword-only arguments\n    if missing:\n        if len(missing) > 2:\n            end = 's: %s, and %s' % (', '.join(missing[:-1]), missing[-1])\n        elif len(missing) == 2:\n            end = 's: %s and %s' % tuple(missing)\n        else:\n            end = ': %s' % tuple(missing)\n\n        msg = 'missing %s required keyword-only argument%s'\n        raise TypeError(msg % (len(missing), end))\n\n    # handle the withdefaults\n    for name, value in withdefaults:\n        if name not in kws:\n            kwonly.append(value)\n        else:\n            kwonly.append(kws.pop(name))\n\n    # handle any leftovers\n    if not leftovers and kws:\n        msg = \"got an unexpected keyword argument '%s'\"\n        raise TypeError(msg % (kws.keys()[0]))\n\n    return [kws] + kwonly"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self, time_interval):\n        logging.info('{} with sink node {} running from {} to {}'.format(\n            self.tool.__class__.__name__, self.sink.node_id, time_interval.start, time_interval.end))\n\n        if self.plates:\n            if isinstance(self.tool, AggregateTool):\n                if len(self.sources) != 1:\n                    raise ValueError(\"Currently only a single source node is valid for an Aggregate Tool\")\n                if self.alignment_node:\n                    raise ValueError(\"Currently an alignment node cannot be used with an Aggregate Tool\")\n                \n                all_sources = self.sources[0]\n                \n                # Here we should loop through the plate values of the sink, and get the sources that are appropriate for\n                # that given plate value, and pass only those sources to the tool. This is cleaner than making the tool\n                # deal with all of the sources\n                for pv in self.sink.plate_values:\n                    sources = [all_sources.streams[s] for s in all_sources.streams if all([v in s for v in pv])]\n                    sink = self.sink.streams[pv]\n                    self.tool.execute(sources=sources, sink=sink, interval=time_interval, alignment_stream=None)\n            elif isinstance(self.tool, SelectorTool):\n                if len(self.sources) == 1:\n                    sources = self.sources[0].streams.values()\n                elif len(self.sources) == 2:\n                    selector_node = self.sources[0]\n                    if len(selector_node.streams) != 1:\n                        raise ValueError(\"Selector node should only have one stream\")\n                    sources = [self.sources[0].streams[None], self.sources[1].streams.values()]\n                else:\n                    raise ValueError(\"Currently only one or twos source nodes are valid for a Selector Tool\")\n                if self.alignment_node:\n                    raise ValueError(\"Currently an alignment node cannot be used with a Selector Tool\")\n                \n                diff, counts, is_sub_plate = self.sources[-1].difference(self.sink)\n                # TODO: This sub-plate selection is deprecated\n\n                if (counts == [1, 1] and is_sub_plate) or \\\n                    (len(self.sink.plates)==1 and counts == [1, 0] and is_sub_plate) or \\\n                    (next(p.is_root for p in self.sources[-1].plates)\n                     and len(self.sink.plates) == 1\n                     and self.sink.plates[0] in self.sources[-1].plates):\n                        # Special case of tools that are performing sub-selection\n                        self.tool.execute(sources=sources,\n                                          sinks=self.sink.streams.values(),\n                                          interval=time_interval)\n                else:\n                    raise ValueError(\"Source and sink plates do not match within a Selector Tool\")\n            else:\n                # TODO: This loop over plates is probably not correct:\n                # What we probably want is to take the cartesian product of plate values\n                if len(self.plates) == 1:\n                    plate = self.plates[0]\n                    for pv in plate.values:\n                        sources = self.get_sources(plate, pv)\n                        sink = self.sink.streams[pv]\n                        self.tool.execute(sources=sources, sink=sink, interval=time_interval,\n                                          alignment_stream=self.get_alignment_stream(None, None))\n                else:\n                    if len(self.sources) != 1 and not all(s.plates == self.plates for s in self.sources):\n                        source_plates = sorted(p.plate_id for s in self.sources for p in s.plates)\n                        self_plates = sorted(p.plate_id for p in self.plates)\n                        if source_plates == self_plates:\n                            # This is the case where the sources are all on separate plates and the sink is the\n                            # combination\n                            search = [[x[0] for x in p.values] for p in self.plates]\n                            _pv = sorted(itertools.product(*search))\n                            for pv in _pv:\n                                # Here we're selecting the streams that have the partial match of the plate value\n                                sources = [source.streams[s] for source in self.sources\n                                           for s in source.streams if (s[0] in pv)]\n                                try:\n                                    sink = self.sink.streams[pv]\n                                    self.tool.execute(sources=sources, sink=sink, interval=time_interval,\n                                                  alignment_stream=self.get_alignment_stream(None, None))\n                                except KeyError as e:\n                                    continue\n\n                        else:\n                            raise NotImplementedError\n                    for pv in Plate.get_overlapping_values(self.plates):\n                        sources = [source.streams[s] for source in self.sources for s in source.streams if pv == s]\n                        sink = self.sink.streams[pv]\n                        self.tool.execute(sources=sources, sink=sink, interval=time_interval,\n                                          alignment_stream=self.get_alignment_stream(None, None))\n        else:\n            if isinstance(self.tool, AggregateTool):\n                # raise ValueError(\"Cannot execute an AggregateTool if no plates are defined for the factor\")\n                # Here we're trying to aggregate off a plate. This is only allowed for a single non-overlapping plate.\n                if len(self.sources) != 1:\n                    raise ValueError(\"Currently only a single source node is valid for an Aggregate Tool\")\n                if self.alignment_node:\n                    raise ValueError(\"Currently an alignment node cannot be used with an Aggregate Tool\")\n\n                sources = self.sources[0].streams.values()\n                sink = self.sink.streams[None]\n                self.tool.execute(sources=sources, sink=sink, interval=time_interval, alignment_stream=None)\n\n            else:\n                # sources = [source.streams[None] for source in self.sources] if self.sources else None\n                sources = self.get_global_sources()\n                sink = self.sink.streams[None]\n                self.tool.execute(sources=sources, sink=sink, interval=time_interval,\n                                  alignment_stream=self.get_alignment_stream(None, None))\n        return self", "response": "Execute the factor over the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_sources(self, plate, plate_value, sources=None):\n        if sources is None:\n            sources = []\n        \n        if self.sources:\n            for si, source in enumerate(self.sources):\n                if len(source.streams) == 1 and None in source.streams:\n                    sources.append(source.streams[None])\n                elif plate_value in source.streams:\n                    sources.append(source.streams[plate_value])\n                else:\n                    # # TODO - determine whether this should raise an exception or not, or even log a warning\n                    # logging.warn(\"{} with value {} not valid for source {}\"\n                    #              .format(plate, plate_value, source))\n                    pass\n        \n        if not plate.is_root:\n            # Populate with sources defined on parent plate\n            parent_plate_value = tuple(pv for pv in plate_value if pv[0] != plate.meta_data_id)\n            sources = self.get_sources(plate.parent, parent_plate_value, sources)\n        \n        # sources.extend(self.get_global_sources())\n        \n        return sources", "response": "Gets the source streams for a given plate value on a given plate."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_global_sources(self):\n        sources = []\n        if self.sources:\n            for source in self.sources:\n                if None in source.streams:\n                    sources.append(source.streams[None])\n        return sources", "response": "Gets streams that live outside of the plates\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_alignment_stream(self, plate=None, plate_value=None):\n        if not self.alignment_node:\n            return None\n        if plate is not None or plate_value is not None:\n            # TODO: Need to implement alignment nodes that live inside plates\n            raise NotImplementedError(\"Currently only alignment nodes outside of plates are supported\")\n        return self.alignment_node.streams[plate]", "response": "Gets the alignment stream for a particular plate value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting the factor over the given time interval.", "response": "def execute(self, time_interval):\n        \"\"\"\n        Execute the factor over the given time interval. Note that this is normally done by the workflow,\n        but can also be done on the factor directly\n\n        :param time_interval: The time interval\n        :return: self (for chaining)\n        \"\"\"\n        logging.info('{} running from {} to {}'.format(\n            self.tool.__class__.__name__, time_interval.start, time_interval.end))\n\n        input_plate_values = self.input_plate.values if self.input_plate else [None]\n        output_plate_values = self.sink.plate_values\n        meta_data_ids = [p.meta_data_id for p in self.sink.plates]\n\n        def belongs(plate_value):\n            return all(ii in plate_value for ii in input_plate_value)\n\n        def meta_data_matches(plate_value):\n            return filter(lambda x: x[0] in meta_data_ids, plate_value)\n\n        for input_plate_value in input_plate_values:\n            if input_plate_value:\n                # Select only the valid output plate values based on the input plate value\n                # filtered = filter(belongs, output_plate_values)\n                filtered = filter(belongs, output_plate_values)\n            else:\n                filtered = output_plate_values\n\n            sinks = [self.sink.streams[s] for s in filtered]\n\n            sub_plate_values_only = map(meta_data_matches, filtered)\n\n            if not self.source:\n                source = None\n            elif input_plate_value in self.source.streams:\n                source = self.source.streams[input_plate_value]\n            else:\n                logging.warn(\"{} with value {} not valid for source {}\".format(\n                    self.input_plate, input_plate_value, self.source))\n                continue\n\n            if self.input_plate:\n                if len(self.output_plates) == 1:\n                    if self.output_plates[0].parent.plate_id != self.input_plate.plate_id:\n                        raise IncompatiblePlatesError(\"Parent plate of output plate does not match input plate\")\n            else:\n                if len(self.output_plates) != 1:\n                    raise ValueError(\"Should be a single output plate if there is no input plate\")\n\n            if len(self.output_plates) > 1:\n                raise NotImplementedError\n\n            splitting_stream = self.get_splitting_stream(input_plate_value)\n\n            self.tool.execute(\n                source=source,\n                sinks=sinks,\n                interval=time_interval,\n                splitting_stream=splitting_stream,\n                meta_data_id=self.output_plates[0].meta_data_id,\n                output_plate_values=sub_plate_values_only)\n            self.update_computed_intervals(sinks, time_interval)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_splitting_stream(self, input_plate_value):\n        if not self.splitting_node:\n            return None\n\n        if len(self.splitting_node.plates) == 0:\n            # Use global plate value\n            return self.splitting_node.streams[None]\n\n        if len(self.splitting_node.plates) > 1:\n            raise ValueError(\"Splitting node cannot live on multiple plates for factor {}\"\n                             .format(self.factor_id))\n\n        # now len(self.splitting_node.plates) == 1:\n        if not self.input_plate and len(self.splitting_node.plates) > 0:\n            raise ValueError(\"Splitting node cannot live on a plate if there is no input plate\")\n\n        splitting_plate = self.splitting_node.plates[0]\n        if self.input_plate == splitting_plate:\n            # Use matching plate value\n            splitting_stream = self.splitting_node.streams[input_plate_value]\n        else:\n            # First check if it's a direct child\n            if splitting_plate.is_child(self.input_plate):\n                ppv = filter(lambda x: all(p in input_plate_value for p in x), self.input_plate.parent.values)\n                if len(ppv) != 1:\n                    raise ValueError(\"Parent plate value not found\")\n                splitting_stream = self.splitting_node.streams[ppv[0]]\n            # Then more generally if it's a descendant\n            elif splitting_plate.is_descendant(self.input_plate):\n                # Here we need to find the splitting plate value that is valid for the\n                # current input plate value\n                # TODO: This needs checking - is the logic still the same as for the case above?\n                ppv = filter(lambda x: all(p in input_plate_value for p in x), self.input_plate.parent.values)\n                if len(ppv) != 1:\n                    raise ValueError(\"Parent plate value not found\")\n                splitting_stream = self.splitting_node.streams[ppv]\n            else:\n                raise IncompatiblePlatesError(\n                    \"Splitting node plate {} does not match input plate {} for factor {}\"\n                    .format(self.input_plate, self.splitting_node.plates[0], self.factor_id))\n        return splitting_stream", "response": "Get the splitting stream for the given input plate value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_computed_intervals(sinks, time_interval):\n        for sink in sinks:\n            sink.calculated_intervals += time_interval\n            required_intervals = TimeIntervals([time_interval]) - sink.calculated_intervals\n            if not required_intervals.is_empty:\n                raise RuntimeError('Tool execution did not cover the time interval {}'\n                                   .format(required_intervals))", "response": "Update the computed intervals of the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self, time_interval):\n        logging.info('{} running from {} to {}'.format(\n            self.tool.__class__.__name__, time_interval.start, time_interval.end))\n\n        # Execute the tool to produce the output plate values\n        output_plate_values = {}\n        if self.input_plate:\n            for ipv in self.input_plate.values:\n                if ipv in self.source.streams:\n                    source = self.source.streams[ipv]\n                else:\n                    logging.warn(\"{} with value {} not valid for source {}\".format(\n                        self.input_plate, ipv, self.source))\n                    continue\n\n                output_plate_values[ipv] = self.tool.execute(\n                    source=source, interval=time_interval, input_plate_value=ipv)\n        else:\n            source = self.source.streams[None] if self.source else None\n            if \"parent_plate\" in self.output_plate:\n                # Get the parent plate values\n                parent_plate = self._plate_manager.plates[self.output_plate[\"parent_plate\"]]\n                for ppv in parent_plate.values:\n                    output_plate_values[ppv] = self.tool.execute(\n                        source=source, interval=time_interval, input_plate_value=ppv)\n            else:\n                output_plate_values[None] = self.tool.execute(\n                    source=source, interval=time_interval, input_plate_value=None)\n\n        # Ensure that the output plate values exist\n        for ipv, opv in output_plate_values.items():\n            input_plate_value = \".\".join(\"_\".join(i) for i in ipv) if ipv else None\n            for pv in opv:\n                if ipv:\n                    identifier = input_plate_value + \".\" + self.output_plate[\"meta_data_id\"] + \"_\" + pv\n                else:\n                    identifier = self.output_plate[\"meta_data_id\"] + \"_\" + pv\n                if not self._meta_data_manager.contains(identifier):\n                    self._meta_data_manager.insert(\n                        tag=self.output_plate[\"meta_data_id\"],\n                        identifier=identifier,\n                        parent=input_plate_value if ipv else \"root\",\n                        data=pv\n                    )\n\n        if self.output_plate[\"use_provided_values\"]:\n            raise NotImplementedError(\"Currently only support using empty set and complement=True for the new plate\")\n\n        if self.output_plate[\"plate_id\"] not in self._plate_manager.plates:\n            # Create the output plate\n            if self.input_plate:\n                parent_plate = self.input_plate.plate_id\n            else:\n                if \"parent_plate\" in self.output_plate:\n                    parent_plate = self.output_plate['parent_plate']\n                else:\n                    parent_plate = None\n            self._plate_manager.create_plate(\n                plate_id=self.output_plate[\"plate_id\"],\n                description=self.output_plate[\"description\"],\n                meta_data_id=self.output_plate[\"meta_data_id\"],\n                values=[],\n                complement=True,\n                parent_plate=parent_plate\n            )\n\n            logging.info(\"Plate with ID {} created\".format(self.output_plate[\"plate_id\"]))\n\n        return self", "response": "Execute the tool over the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getEvoBibAsBibtex(*keys, **kw):\n    res = []\n    for key in keys:\n        bib = get_url(\n            \"http://bibliography.lingpy.org/raw.php?key=\" + key,\n            log=kw.get('log')).text\n        try:\n            res.append('@' + bib.split('@')[1].split('</pre>')[0])\n        except IndexError:  # pragma: no cover\n            res.append('@misc{' + key + ',\\nNote={missing source}\\n\\n}')\n    return '\\n\\n'.join(res)", "response": "Download bibtex format and parse it from EvoBib"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads a zipfile and immediately unpack selected content.", "response": "def download_and_unpack(self, url, *paths, **kw):\n        \"\"\"\n        Download a zipfile and immediately unpack selected content.\n\n        :param url:\n        :param paths:\n        :param kw:\n        :return:\n        \"\"\"\n        with self.temp_download(url, 'ds.zip', log=kw.pop('log', None)) as zipp:\n            with TemporaryDirectory() as tmpdir:\n                with zipfile.ZipFile(zipp.as_posix()) as zipf:\n                    for path in paths:\n                        zipf.extract(as_posix(path), path=tmpdir.as_posix())\n                        copy(tmpdir.joinpath(path), self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getWinners(self, profile, sampleFileName = None):\n\n        if sampleFileName != None:\n            candScores = self.getCandScoresMapFromSamplesFile(profile, sampleFileName)\n        else:\n            candScores = self.getCandScoresMap(profile)\n\n        # Check whether the winning candidate is the candidate that maximizes the score or \n        # minimizes it.\n        if self.maximizeCandScore == True:\n            bestScore = max(candScores.values())\n        else:\n            bestScore = min(candScores.values())\n        \n        # Create a list of all candidates with the winning score and return it.\n        winners = []\n        for cand in candScores.keys():\n            if candScores[cand] == bestScore:\n                winners.append(cand)\n        return winners", "response": "Returns a list of all winning candidates when we use MCMC approximation to compute Bayesian\n        utilities for an election profile."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getRanking(self, profile, sampleFileName = None):\n\n        if sampleFileName != None:\n            candScoresMap = self.getCandScoresMapFromSamplesFile(profile, sampleFileName)\n        else:\n            candScoresMap = self.getCandScoresMap(profile)\n\n        # We generate a map that associates each score with the candidates that have that acore.\n        reverseCandScoresMap = dict()\n        for key, value in candScoresMap.items():\n            if value not in reverseCandScoresMap.keys():\n                reverseCandScoresMap[value] = [key]\n            else:   \n                reverseCandScoresMap[value].append(key)\n        \n        # We sort the scores by either decreasing order or increasing order.\n        if self.maximizeCandScore == True:\n            sortedCandScores = sorted(reverseCandScoresMap.keys(), reverse=True)\n        else:\n            sortedCandScores = sorted(reverseCandScoresMap.keys())\n        \n        # We put the candidates into our ranking based on the order in which their score appears\n        ranking = []\n        for candScore in sortedCandScores:\n            for cand in reverseCandScoresMap[candScore]:\n                ranking.append(cand)\n\n        return ranking", "response": "Returns a list of lists that orders all candidates in tiers from best to worst when we use \n            MCMC approximation to compute Bayesian utilities for an election profile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictonary that associates the integer representation of each candidate with the integer representation of each candidate with the bayesian utilities we approximate from our sampling of the profile.", "response": "def getCandScoresMap(self, profile):\n        \"\"\"\n        Returns a dictonary that associates the integer representation of each candidate with the \n        Bayesian utilities we approximate from our sampling of the profile.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        wmg = profile.getWmg(True)\n        V = self.getInitialSample(wmg)\n\n        utilities = dict()\n        for cand in profile.candMap.keys():\n            utilities[cand] = 0.0\n\n        for i in range(0, self.burnIn):\n            V = self.sampleGenerator.getNextSample(V)\n\n        for i in range(0, self.n2):\n            for j in range(0, self.n1):\n                V = self.sampleGenerator.getNextSample(V)\n            for cand in profile.candMap.keys():\n                utilities[cand] += self.utilityFunction.getUtility([cand], V)\n\n        for cand in profile.candMap.keys():\n            utilities[cand] = utilities[cand]/self.n2\n\n        return utilities"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getWinnersBruteForce(self, profile):\n\n        candScoresMapBruteForce = self.getCandScoresMapBruteForce(profile) \n\n        # Check whether the winning candidate is the candidate that maximizes the score or \n        # minimizes it.\n        if self.maximizeCandScore == True:\n            bestScore = max(candScoresMapBruteForce.values())\n        else:\n            bestScore = min(candScoresMapBruteForce.values())\n        \n        # Create a list of all candidates with the winning score and return it.\n        winners = []\n        for cand in candScoresMapBruteForce.keys():\n            if candScoresMapBruteForce[cand] == bestScore:\n                winners.append(cand)\n        return winners", "response": "Returns a list of all winning candidates when we use brute force to compute Bayesian\n        utilities for an election profile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getRankingBruteForce(self, profile):\n\n        # We generate a map that associates each score with the candidates that have that score.\n        candScoresMapBruteForce = self.getCandScoresMapBruteForce(profile) \n        reverseCandScoresMap = dict()\n        for key, value in candScoresMapBruteForce.items():\n            if value not in reverseCandScoresMap.keys():\n                reverseCandScoresMap[value] = [key]\n            else:   \n                reverseCandScoresMap[value].append(key)\n        \n        # We sort the scores by either decreasing order or increasing order.\n        if self.maximizeCandScore == True:\n            sortedCandScores = sorted(reverseCandScoresMap.keys(), reverse=True)\n        else:\n            sortedCandScores = sorted(reverseCandScoresMap.keys())\n        \n        # We put the candidates into our ranking based on the order in which their score appears\n        ranking = []\n        for candScore in sortedCandScores:\n            for cand in reverseCandScoresMap[candScore]:\n                ranking.append(cand)\n\n        return ranking", "response": "Returns a list that orders all candidates from best to worst when we use brute force to maintain order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the number of samples to a file.", "response": "def printMcmcSamplesToFile(self, profile, numSamples, outFileName):\n        \"\"\"\n        Generate samples to a file.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        :ivar int numSamples: The number of samples to be generated.\n        :ivar str outFileName: The name of the file to be output.\n        \"\"\"\n\n        wmg = profile.getWmg(True)\n        V = self.getInitialSample(wmg)\n\n        # Print the number of candidates, phi, and the number of samples.\n        outFile = open(outFileName, 'w')\n        outFile.write(\"m,\" + str(profile.numCands) + '\\n')\n        outFile.write(\"phi,\" + str(self.phi) + '\\n')\n        outFile.write(\"numSamples,\" + str(numSamples))\n        \n        for i in range(0, numSamples):\n            V = self.sampleGenerator.getNextSample(V)\n            outFile.write(\"\\n\" + json.dumps(V))\n        outFile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the kendall - tau distance between a single vote and a wmg for the entire election.", "response": "def kendallTau(self, orderVector, wmgMap):\n        \"\"\"\n        Given a ranking for a single vote and a wmg for the entire election, calculate the kendall-tau\n        distance. a.k.a the number of discordant pairs between the wmg for the vote and the wmg for the\n        election. Currently, we expect the vote to be a strict complete ordering over the candidates.\n\n        :ivar list<int> rankList: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last.\n        :ivar dict<int,<dict,<int,int>>> wmgMap: A two-dimensional dictionary that associates integer\n            representations of each pair of candidates, cand1 and cand2, with the number of times\n            cand1 is ranked above cand2 minus the number of times cand2 is ranked above cand1. The\n            dictionary represents a weighted majority graph constructed from an entire election.\n        \"\"\"\n\n        discordantPairs = 0.0\n        for i in itertools.combinations(orderVector, 2):\n            discordantPairs = discordantPairs + max(0, wmgMap[i[1]][i[0]])\n        return discordantPairs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate an initial sample for the Markov chain. This function is called by MarkovChain. generateRandomSample.", "response": "def getInitialSample(self, wmg):\n        \"\"\"\n        Generate an initial sample for the Markov chain. This function will return a list \n        containing integer representations of each candidate in order of their rank in the current \n        vote, from first to last. The list will be a complete strict ordering over the candidates.\n        Initially, we rank the candidates in random order.\n\n        ivar: dict<int,<dict,<int,int>>> wmg: A two-dimensional dictionary that associates integer\n            representations of each pair of candidates, cand1 and cand2, with the number of times\n            cand1 is ranked above cand2 minus the number of times cand2 is ranked above cand1. The\n            dictionary represents a weighted majority graph for an election.\n        \"\"\"\n        \n        V = copy.deepcopy(wmg.keys())\n        random.shuffle(V)\n        return V"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictonary that associates the integer representation of each candidate with the integer representation of each candidate with the bayesian losses that we calculate using brute force.", "response": "def getCandScoresMapBruteForce(self, profile):\n        \"\"\"\n        Returns a dictonary that associates the integer representation of each candidate with the \n        bayesian losses that we calculate using brute force.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n        \n        wmg = profile.getWmg(True)\n        losses = dict()\n        for cand in wmg.keys():\n            losses[cand] = 0.0\n\n        # Calculate the denominator.\n        denom = 0.0\n        for permutation in itertools.permutations(wmg.keys()):\n            denom = denom + self.phi ** float(self.kendallTau(permutation, wmg))\n\n        for permutation in itertools.permutations(wmg.keys()):\n            prob = self.phi**float(self.kendallTau(permutation, wmg))/denom\n            for cand in wmg.keys():\n                losses[cand] += self.utilityFunction.getUtility([cand], permutation)* prob\n        return losses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef createBinaryRelation(self, m):\n\n        binaryRelation = []\n        for i in range(m):\n            binaryRelation.append(range(m))\n            binaryRelation[i][i] = 0\n        return binaryRelation", "response": "Create a two - dimensional array of size m by m."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getInitialSample(self, wmg):\n\n        cands = range(len(wmg))\n        allPairs = itertools.combinations(cands, 2)\n        V = self.createBinaryRelation(len(cands))\n        for pair in allPairs:\n            if wmg[pair[0]+1][pair[1]+1] > 0:\n                V[pair[0]][pair[1]] = 1\n                V[pair[1]][pair[0]] = 0\n            else:\n                V[pair[0]][pair[1]] = 0\n                V[pair[1]][pair[0]] = 1\n        return V", "response": "This function generates an initial sample for the Markov chain. This function returns a two - dimensional array of integers such that for each pair of candidates cand1 and cand2 cand1 and cand2 and 0 if more votes rank cand1 and cand2 and 1 if more votes rank cand1 and cand2 and 0 otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getCandScoresMapBruteForce(self, profile):\n\n        wmg = profile.getWmg(True)\n        m = len(wmg.keys())\n        cands = range(m)\n        V = self.createBinaryRelation(m)\n        gains = dict()\n        for cand in wmg.keys():\n            gains[cand] = 0\n        graphs = itertools.product(range(2), repeat=m*(m-1)/2)\n        for comb in graphs:\n            prob = 1\n            i = 0\n            for a, b in itertools.combinations(cands,2):\n                V[a][b] = comb[i]\n                V[b][a] = 1-comb[i]\n                if comb[i] > 0:\n                    prob *= 1/(1+self.phi ** float(wmg[a+1][b+1]))\n                else:\n                    prob *= 1/(1+self.phi ** float(wmg[b+1][a+1]))\n                i += 1\n                if i >= m*(m-1)/2:\n                    break\n            for cand in wmg.keys():\n                gains[cand] += self.utilityFunction.getUtility([cand], V)*prob\n        return gains", "response": "Returns a dictonary that associates the integer representation of each candidate with the \n        bayesian losses that we calculate using brute force."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_input(keys, raw):\n    if len(keys) == 1:\n        if keys[0] in UI.keys['up']:\n            keys[0] = 'up'\n        elif keys[0] in UI.keys['down']:\n            keys[0] = 'down'\n        elif len(keys[0]) == 4 and keys[0][0] == 'mouse press':\n            if keys[0][1] == 4:\n                keys[0] = 'up'\n            elif keys[0][1] == 5:\n                keys[0] = 'down'\n    return keys", "response": "Filter out the input keys that are not in the listbox."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wordlist2cognates(wordlist, source, expert='expert', ref='cogid'):\n    for k in wordlist:\n        yield dict(\n            Form_ID=wordlist[k, 'lid'],\n            ID=k,\n            Form=wordlist[k, 'ipa'],\n            Cognateset_ID='{0}-{1}'.format(\n                slug(wordlist[k, 'concept']), wordlist[k, ref]),\n            Cognate_Detection_Method=expert,\n            Source=source)", "response": "Turn a wordlist into a cognate set list using the cldf parameters."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cldf2wld(dataset):\n    header = [f for f in dataset.dataset.lexeme_class.fieldnames() if f != 'ID']\n    D = {0: ['lid'] + [h.lower() for h in header]}\n    for idx, row in enumerate(dataset.objects['FormTable']):\n        row = deepcopy(row)\n        row['Segments'] = ' '.join(row['Segments'])\n        D[idx + 1] = [row['ID']] + [row[h] for h in header]\n    return D", "response": "Make lingpy - compatible dictinary out of cldf main data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading LexStat object from cldf dataset.", "response": "def _cldf2lexstat(\n        dataset,\n        segments='segments',\n        transcription='value',\n        row='parameter_id',\n        col='language_id'):\n    \"\"\"Read LexStat object from cldf dataset.\"\"\"\n    D = _cldf2wld(dataset)\n    return lingpy.LexStat(D, segments=segments, transcription=transcription, row=row, col=col)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading worldist object from cldf dataset.", "response": "def _cldf2wordlist(dataset, row='parameter_id', col='language_id'):\n    \"\"\"Read worldist object from cldf dataset.\"\"\"\n    return lingpy.Wordlist(_cldf2wld(dataset), row=row, col=col)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_cognates(dataset, column='Segments', method='turchin', threshold=0.5, **kw):\n    if method == 'turchin':\n        for row in dataset.objects['FormTable']:\n            sounds = ''.join(lingpy.tokens2class(row[column], 'dolgo'))\n            if sounds.startswith('V'):\n                sounds = 'H' + sounds\n            sounds = '-'.join([s for s in sounds if s != 'V'][:2])\n            cogid = slug(row['Parameter_ID']) + '-' + sounds\n            if '0' not in sounds:\n                yield dict(\n                    Form_ID=row['ID'],\n                    Form=row['Value'],\n                    Cognateset_ID=cogid,\n                    Cognate_Detection_Method='CMM')\n\n    if method in ['sca', 'lexstat']:\n        lex = _cldf2lexstat(dataset)\n        if method == 'lexstat':\n            lex.get_scorer(**kw)\n        lex.cluster(method=method, threshold=threshold, ref='cogid')\n        for k in lex:\n            yield Cognate(\n                Form_ID=lex[k, 'lid'],\n                Form=lex[k, 'value'],\n                Cognateset_ID=lex[k, 'cogid'],\n                Cognate_Detection_Method=method + '-t{0:.2f}'.format(threshold))", "response": "Iterate over all cognates in a given dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning computes automatic alignments and writes them to file.", "response": "def iter_alignments(dataset, cognate_sets, column='Segments', method='library'):\n    \"\"\"\n    Function computes automatic alignments and writes them to file.\n    \"\"\"\n    if not isinstance(dataset, lingpy.basic.parser.QLCParser):\n        wordlist = _cldf2wordlist(dataset)\n        cognates = {r['Form_ID']: r for r in cognate_sets}\n        wordlist.add_entries(\n            'cogid',\n            'lid',\n            lambda x: cognates[x]['Cognateset_ID'] if x in cognates else 0)\n        alm = lingpy.Alignments(\n            wordlist,\n            ref='cogid',\n            row='parameter_id',\n            col='language_id',\n            segments=column.lower())\n        alm.align(method=method)\n        for k in alm:\n            if alm[k, 'lid'] in cognates:\n                cognate = cognates[alm[k, 'lid']]\n                cognate['Alignment'] = alm[k, 'alignment']\n                cognate['Alignment_Method'] = method\n    else:\n        alm = lingpy.Alignments(dataset, ref='cogid')\n        alm.align(method=method)\n\n        for cognate in cognate_sets:\n            idx = cognate['ID'] or cognate['Form_ID']\n            cognate['Alignment'] = alm[int(idx), 'alignment']\n            cognate['Alignment_Method'] = 'SCA-' + method"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts Any file to HDF5 file", "response": "def tohdf5(input_files, output_file, n_events, conv_times_to_jte, **kwargs):\n    \"\"\"Convert Any file to HDF5 file\"\"\"\n    if len(input_files) > 1:\n        cprint(\n            \"Preparing to convert {} files to HDF5.\".format(len(input_files))\n        )\n\n    from km3pipe import Pipeline    # noqa\n    from km3pipe.io import GenericPump, HDF5Sink, HDF5MetaData    # noqa\n\n    for input_file in input_files:\n        cprint(\"Converting '{}'...\".format(input_file))\n        if len(input_files) > 1:\n            output_file = input_file + '.h5'\n\n        meta_data = kwargs.copy()\n        meta_data['origin'] = input_file\n\n        pipe = Pipeline()\n        pipe.attach(HDF5MetaData, data=meta_data)\n        pipe.attach(GenericPump, filenames=input_file, **kwargs)\n        pipe.attach(StatusBar, every=250)\n        if conv_times_to_jte:\n            from km3modules.mc import MCTimeCorrector\n            pipe.attach(MCTimeCorrector)\n        pipe.attach(HDF5Sink, filename=output_file, **kwargs)\n        pipe.drain(n_events)\n        cprint(\"File '{}' was converted.\".format(input_file))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_channels(self):\n        logging.info(\"Updating channels\")\n        with switch_db(StreamDefinitionModel, 'hyperstream'):\n            for s in StreamDefinitionModel.objects():\n                try:\n                    stream_id = StreamId(name=s.stream_id.name, meta_data=s.stream_id.meta_data)\n                except AttributeError as e:\n                    raise e\n                logging.debug(\"Processing {}\".format(stream_id))\n\n                try:\n                    # This can fail if a plugin has been defined by a different instantiation of HyperStream on the same\n                    # database.\n                    channel = self.get_channel(s.channel_id)\n                except ChannelNotFoundError as e:\n                    logging.warn(e)\n                    continue\n\n                # calculated_intervals = TimeIntervals(map(lambda x: (x.start, x.end), s.calculated_intervals))\n                last_accessed = utcnow()\n                last_updated = s.last_updated if s.last_updated else utcnow()\n\n                if stream_id in channel.streams:\n                    if isinstance(channel, (AssetsChannel, AssetsFileChannel)):\n                        continue\n                    raise StreamAlreadyExistsError(stream_id)\n\n                from . import MemoryChannel, DatabaseChannel\n                if isinstance(channel, MemoryChannel):\n                    channel.create_stream(stream_id)\n                elif isinstance(channel, DatabaseChannel):\n                    if channel == self.assets:\n                        stream_type = AssetStream\n                    else:\n                        stream_type = DatabaseStream\n\n                    channel.streams[stream_id] = stream_type(\n                        channel=channel,\n                        stream_id=stream_id,\n                        calculated_intervals=None,  # Not required since it's initialised from mongo_model in __init__\n                        last_accessed=last_accessed,\n                        last_updated=last_updated,\n                        sandbox=s.sandbox,\n                        mongo_model=s\n                    )\n                else:\n                    logging.warn(\"Unable to parse stream {}\".format(stream_id))", "response": "Updates the channels with the stream references from the database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tool_class(self, tool):\n        if isinstance(tool, string_types):\n            tool_id = StreamId(tool)\n        elif isinstance(tool, StreamId):\n            tool_id = tool\n        else:\n            raise TypeError(tool)\n\n        tool_stream_view = None\n\n        # Look in the main tool channel first\n        if tool_id in self.tools:\n            tool_stream_view = self.tools[tool_id].window((MIN_DATE, self.tools.up_to_timestamp))\n        else:\n            # Otherwise look through all the channels in the order they were defined\n            for tool_channel in self.tool_channels:\n                if tool_channel == self.tools:\n                    continue\n                if tool_id in tool_channel:\n                    # noinspection PyTypeChecker\n                    tool_stream_view = tool_channel[tool_id].window((MIN_DATE, tool_channel.up_to_timestamp))\n\n        if tool_stream_view is None:\n            raise ToolNotFoundError(tool)\n\n        # TODO: Use tool versions - here we just take the latest one\n        last = tool_stream_view.last()\n        if last is None:\n            raise ToolNotFoundError(tool)\n\n        return tool_stream_view.last().value", "response": "Gets the actual class which can then be instantiated with its parameters\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the tool object from the tool channel and instantiates it using the parameters.", "response": "def get_tool(self, name, parameters, version=None):\n        \"\"\"\n        Gets the tool object from the tool channel(s), and instantiates it using the tool parameters\n\n        :param name: The name or stream id for the tool in the tool channel\n        :param parameters: The parameters for the tool\n        :param version: The string representation of the version\n        :return: The instantiated tool object\n        \"\"\"\n        # TODO: use the version\n        if version is not None:\n            logging.warn(\"Tool versions not yet supported\")\n\n        tool_class = self.get_tool_class(name)\n\n        # Check that the number of arguments is correct for this tool\n        arg_spec = inspect.getargspec(tool_class.__init__)\n        max_expected = len(arg_spec[0])\n        if arg_spec.defaults:\n            min_expected = max_expected - len(arg_spec.defaults)\n        else:\n            min_expected = max_expected\n        num_parameters = len(parameters) if parameters is not None else 0\n        if not (min_expected <= num_parameters + 1 <= max_expected):\n            message = \"Tool {} takes between {} and {} arguments ({} given)\".format(\n                tool_class.__name__, min_expected, max_expected, num_parameters + 1)\n            raise ToolInitialisationError(message)\n\n        # Instantiate tool\n        try:\n            tool = tool_class(**parameters) if parameters is not None else tool_class()\n        except TypeError:\n            raise ToolInitialisationError(name, parameters)\n\n        if not tool:\n            raise ToolNotFoundError(name, parameters)\n\n        tool.name = name\n\n        return tool"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_ancestors(self, current=None):\n        if not current:\n            current = []\n        current.insert(0, self)\n        if self.is_root:\n            if current:\n                return current\n        else:\n            return self.parent._get_ancestors(current)", "response": "Gets the ancestors of this node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef combine_values(parent_plate_value, plate_value):\n        if parent_plate_value:\n            if isinstance(plate_value[0], string_types):\n                combined_plate_value = parent_plate_value + (plate_value,)\n            elif isinstance(plate_value[0], tuple):\n                combined_plate_value = parent_plate_value + plate_value\n            else:\n                raise TypeError(\"Unknown plate value type\")\n        else:\n            combined_plate_value = plate_value\n\n        return tuple(sorted(combined_plate_value))", "response": "Combine the plate value with the parent plate value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nneeds to find where in the tree the two plates intersect, e.g. We are given as input plates D, E, whose positions in the tree are: root -> A -> B -> C -> D root -> A -> B -> E The results should then be the cartesian product between C, D, E looped over A and B If there's a shared plate in the hierarchy, we need to join on this shared plate, e.g.: [self.plates[p].values for p in plate_ids][0] = [(('house', '1'), ('location', 'hallway'), ('wearable', 'A')), (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'))] [self.plates[p].values for p in plate_ids][1] = [(('house', '1'), ('scripted', '15')), (('house', '1'), ('scripted', '13'))] Result should be one stream for each of: [(('house', '1'), ('location', 'hallway'), ('wearable', 'A'), ('scripted', '15)), (('house', '1'), ('location', 'hallway'), ('wearable', 'A'), ('scripted', '13)), (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'), ('scripted', '15)), (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'), ('scripted', '13))] :param plates: The input plates :return: The plate values :type plates: list[Plate] | list[Plate]", "response": "def get_overlapping_values(plates):\n        \"\"\"\n        Need to find where in the tree the two plates intersect, e.g.\n\n        We are given as input plates D, E, whose positions in the tree are:\n\n        root -> A -> B -> C -> D\n        root -> A -> B -> E\n\n        The results should then be the cartesian product between C, D, E looped over A and B\n\n        If there's a shared plate in the hierarchy, we need to join on this shared plate, e.g.:\n\n        [self.plates[p].values for p in plate_ids][0] =\n          [(('house', '1'), ('location', 'hallway'), ('wearable', 'A')),\n           (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'))]\n        [self.plates[p].values for p in plate_ids][1] =\n          [(('house', '1'), ('scripted', '15')),\n           (('house', '1'), ('scripted', '13'))]\n\n        Result should be one stream for each of:\n          [(('house', '1'), ('location', 'hallway'), ('wearable', 'A'), ('scripted', '15)),\n           (('house', '1'), ('location', 'hallway'), ('wearable', 'A'), ('scripted', '13)),\n           (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'), ('scripted', '15)),\n           (('house', '1'), ('location', 'kitchen'), ('wearable', 'A'), ('scripted', '13))]\n\n        :param plates: The input plates\n        :return: The plate values\n        :type plates: list[Plate] | list[Plate]\n        \"\"\"\n        if not plates:\n            return None\n\n        if len(plates) == 1:\n            return plates[0].values\n\n        if len(plates) > 2:\n            raise NotImplementedError\n\n        # First check for the simple case where one of the plates has no parent\n        # and does not share meta data with the other\n        plates_sorted = sorted(plates, key=lambda item: len(item.ancestor_plates))\n        if plates_sorted[0].is_root:\n            if plates_sorted[0].meta_data_id not in plates_sorted[1].ancestor_meta_data_ids:\n                return map(lambda x: tuple(itertools.chain(*x)), itertools.product(plates[0].values, plates[1].values))\n\n        # Get all of the ancestors zipped together, padded with None\n        ancestors = deque(itertools.izip_longest(*(p.ancestor_plates for p in plates)))\n\n        last_values = []\n        while len(ancestors) > 0:\n            current = ancestors.popleft()\n            if current[0] == current[1]:\n                # Plates are identical, take all values valid for matching parents\n                if last_values:\n                    raise NotImplementedError\n                else:\n                    last_values.extend(current[0].values)\n\n            elif current[0] is not None and current[1] is not None \\\n                    and current[0].meta_data_id == current[1].meta_data_id:\n                # Not identical, but same meta data id. Take all overlapping values valid for matching parents\n                if last_values:\n                    raise NotImplementedError\n                else:\n                    raise NotImplementedError\n            else:\n                # Different plates, take cartesian product of values with matching parents.\n                # Note that one of them may be none\n                if last_values:\n                    tmp = []\n                    for v in last_values:\n                        # Get the valid ones based on v\n                        # valid = [filter(lambda x: all(xx in v for xx in x[:-1]), c.values)\n                        #          for c in current if c is not None]\n                        valid = [filter(lambda x: all(vv in x for vv in v), c.values)\n                                 for c in current if c is not None]\n\n                        # Strip out v from the valid ones\n                        stripped = [map(lambda y: tuple(itertools.chain(*(yy for yy in y if yy not in v))), val)\n                                    for val in valid]\n\n                        # Get the cartesian product. Note that this still works if one of the current is None\n                        prod = list(itertools.product(*stripped))\n\n                        # Now update the last values be the product with v put back in\n                        new_values = [v + p for p in prod]\n                        if new_values:\n                            tmp.append(new_values)\n\n                    last_values = list(itertools.chain(*tmp))\n                    if not last_values:\n                        raise ValueError(\"Plate value computation failed - possibly there were no shared plate values\")\n                else:\n                    raise NotImplementedError\n\n        if not last_values:\n            raise ValueError(\"Plate value computation failed - possibly there were no shared plate values\")\n\n        return last_values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _config(self, args, config):\n        listings = dict((x.args, x) for x in config.subsections('listing'))\n        listing = listings.get(args.listing)\n        if listing is None:\n            if args.listing == u'default':\n                return {'pattern': self._profile.list_default_pattern,\n                        'order': self._profile.list_default_order}\n            else:\n                raise KolektoRuntimeError('Unknown listing %r' % args.listing)\n        else:\n            return {'pattern': listing.get('pattern'),\n                    'order': listing.get('order')}", "response": "Get the configuration for the current used listing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_points(self, pc1, pc2):\n        res = pc1[:]\n\n        for p in pc2:\n            for sp in res:\n                if sp.time == p.time and (\n                    sp.location is None or (sp.location.equals(p.location))\n                ):\n                    sp.members.extend(p.members)\n                    break\n            else:\n                res.append(p)\n\n        return res", "response": "Merge two sets of points based on time and location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_value(value):\n\n\tvalue = str(value)\n\tvalue = value.casefold()\n\n\tvalue = re.sub(r'\\/\\s*\\d+', '', value)  # Remove \"/<totaltracks>\" from track number.\n\tvalue = re.sub(r'^0+([0-9]+)', r'\\1', value)  # Remove leading zero(s) from track number.\n\tvalue = re.sub(r'^(\\d+)\\.+', r'\\1', value)  # Remove dots from track number.\n\tvalue = re.sub(r'[^\\w\\s]', '', value)  # Remove leading non-word characters.\n\tvalue = re.sub(r'^the\\s+', '', value)  # Remove leading \"the\".\n\tvalue = re.sub(r'^\\s+', '', value)  # Remove leading spaces.\n\tvalue = re.sub(r'\\s+$', '', value)  # Remove trailing spaces.\n\tvalue = re.sub(r'\\s+', ' ', value)  # Reduce multiple spaces to a single space.\n\n\treturn value", "response": "Normalize metadata value to improve match accuracy."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatten_element(p):\n    rd = {\"time\": p.time}\n    for member in p.members:\n        rd[member[\"standard\"]] = member[\"value\"]\n    return rd", "response": "Convenience function to flatten a station element into a record - style time series representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates detector from detx file.", "response": "def _init_from_file(self, filename):\n        \"\"\"Create detector from detx file.\"\"\"\n        if not filename.endswith(\"detx\"):\n            raise NotImplementedError('Only the detx format is supported.')\n        self._open_file(filename)\n        self._extract_comments()\n        self._parse_header()\n        self._parse_doms()\n        self._det_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _readline(self, ignore_comments=True):\n        while True:\n            line = self._det_file.readline()\n            if line == '':\n                return line    # To conform the EOF behaviour of .readline()\n            line = line.strip()\n            if line == '':\n                continue    # white-space-only line\n            if line.startswith('#'):\n                if not ignore_comments:\n                    return line\n            else:\n                return line", "response": "Returns the next line of the DETX file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve all comments from the file and add them to the object", "response": "def _extract_comments(self):\n        \"\"\"Retrieve all comments from the file\"\"\"\n        self._det_file.seek(0, 0)\n        for line in self._det_file.readlines():\n            line = line.strip()\n            if line.startswith('#'):\n                self.add_comment(line[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_header(self):\n        self.print(\"Parsing the DETX header\")\n        self._det_file.seek(0, 0)\n        first_line = self._readline()\n        try:\n            self.det_id, self.n_doms = split(first_line, int)\n            self.version = 'v1'\n        except ValueError:\n            det_id, self.version = first_line.split()\n            self.det_id = int(det_id)\n            validity = self._readline().strip()\n            self.valid_from, self.valid_until = split(validity, float)\n            raw_utm_info = self._readline().strip().split(' ')\n            try:\n                self.utm_info = UTMInfo(*raw_utm_info[1:])\n            except TypeError:\n                log.warning(\"Missing UTM information.\")\n            n_doms = self._readline()\n            self.n_doms = int(n_doms)", "response": "Parse the header of the detector file and set the attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_doms(self):\n        self.print(\"Reading PMT information...\")\n        self._det_file.seek(0, 0)\n        self._readline()\n        pmts = defaultdict(list)\n        pmt_index = 0\n        while True:\n            line = self._readline()\n\n            if line == '':\n                self.print(\"Done.\")\n                break\n\n            try:\n                dom_id, du, floor, n_pmts = split(line, int)\n            except ValueError:\n                continue\n\n            if du != self._current_du:\n                log.debug(\"Next DU, resetting floor to 1.\")\n                self._current_du = du\n                self.dus.append(du)\n                self._current_floor = 1\n                if du == 1 and floor == -1:\n                    log.warning(\n                        \"Floor ID is -1 (Jpp conversion bug), \"\n                        \"using our own floor ID!\"\n                    )\n            else:\n                self._current_floor += 1\n\n            if floor == -1:\n                log.debug(\"Setting floor ID to our own ID\")\n                floor = self._current_floor\n\n            self.doms[dom_id] = (du, floor, n_pmts)\n\n            if self.n_pmts_per_dom is None:\n                self.n_pmts_per_dom = n_pmts\n\n            if self.n_pmts_per_dom != n_pmts:\n                log.warning(\n                    \"DOMs with different number of PMTs are \"\n                    \"detected, this can cause some unexpected \"\n                    \"behaviour.\"\n                )\n\n            for i in range(n_pmts):\n                raw_pmt_info = self._readline()\n                pmt_info = raw_pmt_info.split()\n                pmt_id, x, y, z, rest = unpack_nfirst(pmt_info, 4)\n                dx, dy, dz, t0, rest = unpack_nfirst(rest, 4)\n                pmt_id = int(pmt_id)\n                omkey = (du, floor, i)\n                pmts['pmt_id'].append(int(pmt_id))\n                pmts['pos_x'].append(float(x))\n                pmts['pos_y'].append(float(y))\n                pmts['pos_z'].append(float(z))\n                pmts['dir_x'].append(float(dx))\n                pmts['dir_y'].append(float(dy))\n                pmts['dir_z'].append(float(dz))\n                pmts['t0'].append(float(t0))\n                pmts['du'].append(int(du))\n                pmts['floor'].append(int(floor))\n                pmts['channel_id'].append(int(i))\n                pmts['dom_id'].append(int(dom_id))\n                if self.version == 'v3' and rest:\n                    status, rest = unpack_nfirst(rest, 1)\n                    pmts['status'].append(int(status))\n                if rest:\n                    log.warning(\"Unexpected PMT values: {0}\".format(rest))\n                self._pmt_index_by_omkey[omkey] = pmt_index\n                self._pmt_index_by_pmt_id[pmt_id] = pmt_index\n                pmt_index += 1\n\n        self.pmts = Table(pmts, name='PMT')", "response": "Parses the DOMs from the detector file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncenter of mass calculated from the mean of the PMT positions", "response": "def com(self):\n        \"\"\"Center of mass, calculated from the mean of the PMT positions\"\"\"\n        if self._com is None:\n            self._com = np.mean(self.pmts.pos, axis=0)\n        return self._com"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef xy_positions(self):\n        if self._xy_positions is None or len(self._xy_positions) == 0:\n            xy_pos = []\n            for dom_id, pos in self.dom_positions.items():\n                if self.domid2floor(dom_id) == 1:\n                    xy_pos.append(np.array([pos[0], pos[1]]))\n            self._xy_positions = np.array(xy_pos)\n        return self._xy_positions", "response": "XY positions of the DUs given by the DOMs on floor 1."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef translate_detector(self, vector):\n        vector = np.array(vector, dtype=float)\n        self.pmts.pos_x += vector[0]\n        self.pmts.pos_y += vector[1]\n        self.pmts.pos_z += vector[2]\n        self.reset_caches()", "response": "Translate the detector by a given vector"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate_dom_by_yaw(self, dom_id, heading, centre_point=None):\n        pmts = self.pmts[self.pmts.dom_id == dom_id]\n        if centre_point is None:\n            centre_point = self.dom_positions[dom_id]\n\n        for pmt in pmts:\n            pmt_pos = np.array([pmt.pos_x, pmt.pos_y, pmt.pos_z])\n            pmt_dir = np.array([pmt.dir_x, pmt.dir_y, pmt.dir_z])\n            pmt_radius = np.linalg.norm(centre_point - pmt_pos)\n            index = self._pmt_index_by_pmt_id[pmt.pmt_id]\n            pmt_ref = self.pmts[index]\n\n            dir_rot = qrot_yaw([pmt.dir_x, pmt.dir_y, pmt.dir_z], heading)\n            pos_rot = pmt_pos - pmt_dir * pmt_radius + dir_rot * pmt_radius\n\n            pmt_ref.dir_x = dir_rot[0]\n            pmt_ref.dir_y = dir_rot[1]\n            pmt_ref.dir_z = dir_rot[2]\n            pmt_ref.pos_x = pos_rot[0]\n            pmt_ref.pos_y = pos_rot[1]\n            pmt_ref.pos_z = pos_rot[2]\n        self.reset_caches()", "response": "Rotate a DOM by a given yaw heading."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rotate_du_by_yaw(self, du, heading):\n        mask = (self.pmts.du == du)\n        dom_ids = np.unique(self.pmts.dom_id[mask])\n        for dom_id in dom_ids:\n            self.rotate_dom_by_yaw(dom_id, heading)\n        self.reset_caches()", "response": "Rotate all DOMs on a given DU by a given yaw heading."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rescale(self, factor, origin=(0, 0, 0)):\n        pmts = self.pmts\n        for dom_id in self.dom_ids:\n            mask = pmts.dom_id == dom_id\n            pos_x = pmts[mask].pos_x\n            pos_y = pmts[mask].pos_y\n            pos_z = pmts[mask].pos_z\n            pmts.pos_x[mask] = (pos_x - origin[0]) * factor\n            pmts.pos_y[mask] = (pos_y - origin[1]) * factor\n            pmts.pos_z[mask] = (pos_z - origin[2]) * factor\n        self.reset_caches()", "response": "Rescale detector positions by a given factor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pmt_with_id(self, pmt_id):\n        try:\n            return self.pmts[self._pmt_index_by_pmt_id[pmt_id]]\n        except KeyError:\n            raise KeyError(\"No PMT found for ID: {0}\".format(pmt_id))", "response": "Get PMT with global pmt_id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pmt(self, dom_id, channel_id):\n        du, floor, _ = self.doms[dom_id]\n        pmt = self.pmts[self._pmt_index_by_omkey[(du, floor, channel_id)]]\n        return pmt", "response": "Return the PMT with the given DOM ID and DAQ channel ID."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _config(self, args, config):\n        webexports = dict((x.args, x) for x in config.subsections('webexport'))\n        webexport = webexports.get(args.webexport)\n        if webexport is None:\n            if args.webexport == u'default':\n                raise NotImplementedError('Default webexport not implemented')  # FIXME\n            else:\n                raise KolektoRuntimeError('Unknown webexport %r' % args.webexport)\n        else:\n            return {'columns': list(webexport.subsections('column')),\n                    'page_title': webexport.get('page_title'),\n                    'page_credits': webexport.get('page_credits')}", "response": "Get configuration for the current used listing."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_mc_times_to_jte_times(times_mc, evt_timestamp_in_ns, evt_mc_time):\n    # needs to be cast to normal ndarray (not recarray), or else we\n    # would get invalid type promotion\n    times_mc = np.array(times_mc).astype(float)\n    times_jte = times_mc - evt_timestamp_in_ns + evt_mc_time\n    return times_jte", "response": "Function that converts MC times to JTE times."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute(self, source, splitting_stream, sinks, interval, meta_data_id, output_plate_values):\n        if not isinstance(interval, TimeInterval):\n            raise TypeError('Expected TimeInterval, got {}'.format(type(interval)))\n        # logging.info(self.message(interval))\n\n        calculated_intervals = None\n\n        for sink in sinks:\n            if interval.end > sink.channel.up_to_timestamp:\n                raise ValueError(\n                    'The stream is not available after {} and cannot be calculated'.format(\n                        sink.channel.up_to_timestamp))\n            if calculated_intervals is None:\n                calculated_intervals = sink.calculated_intervals\n                continue\n            if sink.calculated_intervals != calculated_intervals:\n                # TODO: What we actually want to do here is find any parts of the sinks that haven't been calculated,\n                # and recompute all of the sinks for that time period. This would only happen if computation of one of\n                # the sinks failed for some reason. For now we will just assume that all sinks have been computed the\n                # same amount, and we will raise an exception if this is not the case\n                raise RuntimeError(\"Partially executed sinks not yet supported\")\n\n        required_intervals = TimeIntervals([interval]) - calculated_intervals\n\n        if not required_intervals.is_empty:\n            document_count = 0\n\n            for interval in required_intervals:\n                for item in self._execute(\n                        source=source,\n                        splitting_stream=splitting_stream,\n                        interval=interval,\n                        meta_data_id=meta_data_id,\n                        output_plate_values=output_plate_values):\n                    # Join the output meta data with the parent plate meta data\n                    # meta_data = input_plate_value + (item.meta_data,) if input_plate_value else (item.meta_data, )\n                    meta_data = item.meta_data if isinstance(item.meta_data[0], tuple) else (item.meta_data,)\n                    try:\n                        # sink = next(s for s in sinks if set(s.stream_id.meta_data) == set(meta_data))\n                        sink = next(s for s in sinks if all(m in s.stream_id.meta_data for m in meta_data))\n                        sink.writer(item.stream_instance)\n                        document_count += 1\n                    except StopIteration:\n                        logging.warn(\"A multi-output tool has produced a value {} \"\n                                     \"which does not belong to the output plate\".format(meta_data))\n                        continue\n                    except TypeError:\n                        logging.error(\"A multi-output tool has produced a value {} \"\n                                      \"which cannot be hashed and does not belong to the output plate\"\n                                      .format(meta_data))\n            if not document_count:\n                logging.debug(\"{} did not produce any data for time interval {} on stream {}\".format(\n                    self.name, required_intervals, source))\n\n            self.write_to_history(\n                interval=interval,\n                tool=self.name,\n                document_count=document_count\n            )", "response": "Execute the tool over the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ifiles(irods_path):\n    raw_output = subprocess.check_output(\n        \"ils -r --bundle {0}\"\n        \"    | grep 'Bundle file:'\"\n        \"    | awk '{{print $3}}'\".format(irods_path),\n        shell=True\n    )\n    filenames = raw_output.decode('ascii').strip().split(\"\\n\")\n    return filenames", "response": "Return a list of filenames for given iRODS path ( recursively"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True of iRODS path exists otherwise False", "response": "def iexists(irods_path):\n    \"\"\"Returns True of iRODS path exists, otherwise False\"\"\"\n    try:\n        subprocess.check_output(\n            'ils {}'.format(irods_path),\n            shell=True,\n            stderr=subprocess.PIPE,\n        )\n        return True\n    except subprocess.CalledProcessError:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a random URL - safe text string in Base64 encoding.", "response": "def token_urlsafe(nbytes=32):\n    \"\"\"Return a random URL-safe text string, in Base64 encoding.\n\n    This is taken and slightly modified from the Python 3.6 stdlib.\n\n    The string has *nbytes* random bytes.  If *nbytes* is ``None``\n    or not supplied, a reasonable default is used.\n\n    >>> token_urlsafe(16)  #doctest:+SKIP\n    'Drmhze6EPcv0fN_81Bj-nA'\n\n    \"\"\"\n    tok = os.urandom(nbytes)\n    return base64.urlsafe_b64encode(tok).rstrip(b'=').decode('ascii')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prettyln(text, fill='-', align='^', prefix='[ ', suffix=' ]', length=69):\n    text = '{prefix}{0}{suffix}'.format(text, prefix=prefix, suffix=suffix)\n    print(\n        \"{0:{fill}{align}{length}}\".format(\n            text, fill=fill, align=align, length=length\n        )\n    )", "response": "Wrap text in a pretty line with maximum length."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef irods_filepath(det_id, run_id):\n    data_path = \"/in2p3/km3net/data/raw/sea\"\n    from km3pipe.db import DBManager\n    if not isinstance(det_id, int):\n        dts = DBManager().detectors\n        det_id = int(dts[dts.OID == det_id].SERIALNUMBER.values[0])\n    return data_path + \"/KM3NeT_{0:08}/{2}/KM3NeT_{0:08}_{1:08}.root\" \\\n           .format(det_id, run_id, run_id//1000)", "response": "Generate the iRODS filepath for given detector ID and run ID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unpack_nfirst(seq, nfirst):\n    iterator = iter(seq)\n    for _ in range(nfirst):\n        yield next(iterator, None)\n    yield tuple(iterator)", "response": "Unpack the nfrist items from the list and return the rest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split(string, callback=None, sep=None):\n    if callback is not None:\n        return [callback(i) for i in string.split(sep)]\n    else:\n        return string.split(sep)", "response": "Split the string and execute the callback function on each part."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a namedtuple with default values", "response": "def namedtuple_with_defaults(typename, field_names, default_values=[]):\n    \"\"\"Create a namedtuple with default values\n\n    >>> Node = namedtuple_with_defaults('Node', 'val left right')\n    >>> Node()\n    Node(val=None, left=None, right=None)\n    >>> Node = namedtuple_with_defaults('Node', 'val left right', [1, 2, 3])\n    >>> Node()\n    Node(val=1, left=2, right=3)\n    >>> Node = namedtuple_with_defaults('Node', 'val left right', {'right':7})\n    >>> Node()\n    Node(val=None, left=None, right=7)\n    >>> Node(4)\n    Node(val=4, left=None, right=7)\n    \"\"\"\n    the_tuple = collections.namedtuple(typename, field_names)\n    the_tuple.__new__.__defaults__ = (None, ) * len(the_tuple._fields)\n    if isinstance(default_values, collections.Mapping):\n        prototype = the_tuple(**default_values)\n    else:\n        prototype = the_tuple(*default_values)\n    the_tuple.__new__.__defaults__ = tuple(prototype)\n    return the_tuple"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remain_file_pointer(function):\n\n    def wrapper(*args, **kwargs):\n        \"\"\"Wrap the function and remain its parameters and return values\"\"\"\n        file_obj = args[-1]\n        old_position = file_obj.tell()\n        return_value = function(*args, **kwargs)\n        file_obj.seek(old_position, 0)\n        return return_value\n\n    return wrapper", "response": "Decorator that remains the file pointer position after calling the decorated function"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting CamelCase to lower_and_underscore.", "response": "def decamelise(text):\n    \"\"\"Convert CamelCase to lower_and_underscore.\"\"\"\n    s = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', text)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s).lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef camelise(text, capital_first=True):\n\n    def camelcase():\n        if not capital_first:\n            yield str.lower\n        while True:\n            yield str.capitalize\n\n    if istype(text, 'unicode'):\n        text = text.encode('utf8')\n    c = camelcase()\n    return \"\".join(next(c)(x) if x else '_' for x in text.split(\"_\"))", "response": "Convert lower_underscore to CamelCase."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npad a matrix with zeros on all sides.", "response": "def zero_pad(m, n=1):\n    \"\"\"Pad a matrix with zeros, on all sides.\"\"\"\n    return np.pad(m, (n, n), mode='constant', constant_values=[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the terminal supports color.", "response": "def supports_color():\n    \"\"\"Checks if the terminal supports color.\"\"\"\n    if isnotebook():\n        return True\n    supported_platform = sys.platform != 'win32' or 'ANSICON' in os.environ\n    is_a_tty = hasattr(sys.stdout, 'isatty') and sys.stdout.isatty()\n\n    if not supported_platform or not is_a_tty:\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the Jpp revision number", "response": "def get_jpp_revision(via_command='JPrint'):\n    \"\"\"Retrieves the Jpp revision number\"\"\"\n    try:\n        output = subprocess.check_output([via_command, '-v'],\n                                         stderr=subprocess.STDOUT)\n    except subprocess.CalledProcessError as e:\n        if e.returncode == 1:\n            output = e.output\n        else:\n            return None\n    except OSError:\n        return None\n    revision = output.decode().split('\\n')[0].split()[1].strip()\n    return revision"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_point(self, profile, point):\n        cur_points_z = [p.location.z for p in profile.elements]\n        try:\n            cur_idx = cur_points_z.index(point.z)\n            return profile.elements[cur_idx]\n        except ValueError:\n            new_idx = bisect_left(cur_points_z, point.z)\n            new_point = Point()\n            new_point.location = sPoint(point)\n            new_point.time = profile.time\n            profile.elements.insert(new_idx, new_point)\n            return new_point", "response": "Returns the given point in the given profile or adds it in sorted z order."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a general DataArray.", "response": "def _parse_data_array(self, data_array):\n        \"\"\"\n        Parses a general DataArray.\n        \"\"\"\n        # decimalSeparator = data_array.encoding.decimalSeparator\n        tokenSeparator = data_array.encoding.tokenSeparator\n        blockSeparator = data_array.encoding.blockSeparator\n        # collapseWhiteSpaces = data_array.encoding.collapseWhiteSpaces\n\n        data_values = data_array.values\n        lines = [x for x in data_values.split(blockSeparator) if x != \"\"]\n\n        ret_val = []\n\n        for row in lines:\n            values = row.split(tokenSeparator)\n            ret_val.append(\n                [\n                    float(v)\n                    if \" \" not in v.strip()\n                    else [float(vv) for vv in v.split()]\n                    for v in values\n                ]\n            )\n\n        # transpose into columns\n        return [list(x) for x in zip(*ret_val)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_sensor_data(self, obs_el, sensor_info):\n        data_array = obs_el.content\n\n        # get column defs\n        data_record = data_array.elementType.content\n        columns = []\n        for f in data_record.field:\n            columns.append(f)\n\n        # get more information on sensor cols\n        sensor_cols = defaultdict(list)\n        # sensor_vals = defaultdict(list)\n\n        sensor_rec = data_record.get_by_name(\"sensor\")\n        for sendata in sensor_rec.content.item:\n            if sendata.content is None:\n                continue\n\n            for f in sendata.content.field:\n                sensor_cols[sendata.name].append(f)\n\n        # @TODO deduplicate\n        # decimalSeparator = data_array.encoding.decimalSeparator\n        tokenSeparator = data_array.encoding.tokenSeparator\n        blockSeparator = data_array.encoding.blockSeparator\n        # collapseWhiteSpaces = data_array.encoding.collapseWhiteSpaces\n\n        data_values = data_array.values\n        lines = [x for x in data_values.split(blockSeparator) if x != \"\"]\n\n        # profile cacher!\n        profile_cache = ProfileCache()\n\n        for row in lines:\n            values = row.split(tokenSeparator)\n\n            # skey = None\n            i = 0\n            cur_time = None\n            # cur_qual = None\n\n            for c in columns:\n\n                if (\n                    isinstance(c.content, Time)\n                    and c.content.definition\n                    == \"http://www.opengis.net/def/property/OGC/0/SamplingTime\"\n                ):\n                    cur_time = parser.parse(values[i])\n                    i += 1\n\n                    if len(c.quality):\n                        # @TODO: do some quality constraint checks\n                        i += len(c.quality)\n                        # for qua in c.quality:\n\n                elif isinstance(c.content, DataChoice) and c.name == \"sensor\":\n                    sensor_key = values[i]\n                    i += 1\n\n                    sensor_dr = c.content.get_by_name(sensor_key).content\n                    sensor_info_ = sensor_info[sensor_key]\n                    parsed, nc = self._parse_sensor_record(\n                        sensor_dr, sensor_info_, values[i:]\n                    )\n\n                    # turn these into Points/Members\n                    for rec in parsed:\n                        # calc a Z value from rec/sensor and build point\n                        point, members = self._build_obs_point(\n                            sensor_info_, rec\n                        )\n\n                        # add to profile\n                        profile_cache.add_obs(\n                            sensor_info_, cur_time, point, members\n                        )\n\n                    i += nc\n\n        return profile_cache.get_collections()", "response": "Parse the sensor data from an OGC observation element."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses sensor data record and returns parsed values and how many items it consumed out of rem_values.", "response": "def _parse_sensor_record(self, sensor_data_rec, sensor_info, rem_values):\n        \"\"\"\n        Parses values via sensor data record passed in.\n        Returns parsed values AND how many items it consumed out of rem_values.\n        \"\"\"\n        val_idx = 0\n\n        # @TODO seems there is only a single field in each of these\n        assert len(sensor_data_rec.field) == 1\n        sensor_data_array = sensor_data_rec.field[0].content\n\n        # there is probably not going to be a count in the def, it'll be in the data\n        count = None\n        count_text = sensor_data_array.elementCount.text\n        if count_text:\n            count = int(count_text.strip())\n\n        if not count:\n            count = int(rem_values[val_idx])\n            val_idx += 1\n\n        parsed = []\n\n        for recnum in range(count):\n            cur = []\n\n            for f in sensor_data_array.elementType.field:\n                cur_val = rem_values[val_idx]\n                val_idx += 1\n\n                m = Member(name=f.name, standard=f.content.definition)\n\n                if hasattr(f.content, \"uom\"):\n                    m[\"units\"] = f.content.uom\n\n                try:\n                    m[\"value\"] = float(cur_val)\n                except ValueError:\n                    m[\"value\"] = cur_val\n\n                if len(f.quality):\n                    m[\"quality\"] = []\n                    for qual in f.quality:\n                        cur_qual = rem_values[val_idx]\n                        val_idx += 1\n\n                        # @TODO check this against constraints\n                        m[\"quality\"].append(cur_qual)\n\n                cur.append(m)\n\n            parsed.append(cur)\n\n        return parsed, val_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_obs_point(self, sensor_info, obs_recs):\n        cur_point = sensor_info[\"location\"][\"point\"]\n\n        keys = [m[\"name\"] for m in obs_recs]\n        if \"binIndex\" in keys:\n            zidx = keys.index(\"binIndex\")\n            bin_index = int(obs_recs[zidx][\"value\"])\n            z = sensor_info[\"profile_heights\"][\"values\"][bin_index]\n\n            point = sPoint(cur_point.x, cur_point.y, cur_point.z + z)\n\n        elif \"profileIndex\" in keys:\n            zidx = keys.index(\"profileIndex\")\n            bin_index = int(obs_recs[zidx][\"value\"])\n\n            # @TODO take into account orientation, may change x/y/z\n            # @TODO bin edges?\n            z = sensor_info[\"profile_bins\"][\"bin_center\"][\"values\"][bin_index]\n\n            point = sPoint(cur_point.x, cur_point.y, cur_point.z + z)\n\n        else:\n            raise ValueError(\"no binIndex or profileIndex in Member: %s\", keys)\n\n        # remove z related Member\n        obs_recs = obs_recs[:]\n        obs_recs.pop(zidx)\n\n        return point, obs_recs", "response": "Builds a 2 - tuple of sPoint and a list of obs_rec objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef populateCsv(self):\n    workingDirPath = createDir(self._workingDir)\n    csvPath = os.path.join(workingDirPath, \"data.csv\")\n    self.writeCsv(csvPath)\n    return csvPath, workingDirPath", "response": "Writes data from streams into CSV in working directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites data into one or many CSV files.", "response": "def writeCsv(self, path):\n    \"\"\"\n    Writes data one or many streams into one CSV file.\n    :param path: absolute or relative file path\n    \"\"\"\n    self._createConfluence()\n    with open(path, \"w\") as outputFile:\n      writer = csv.writer(outputFile)\n      headers = self.getStreamIds()\n      fieldNames = [\"timestamp\"] + headers\n      flags = [\"T\"] + [\"\" for h in headers]\n      types = [\"datetime\"] + self._confluence.getDataTypes()\n      writer.writerow(fieldNames)\n      writer.writerow(types)\n      writer.writerow(flags)\n      for row in self._confluence:\n        writer.writerow(row)\n    print \"Wrote CSV data to %s.\" % path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef writeSwarmDescription(self, csvPath, outPath, \n                    predictedField=None, swarmParams=None):\n    \"\"\"\n    Writes swarm description file (JSON).\n    :param csvPath: path to CSV data\n    :param outPath: absolute or relative file path to write swarm JSON file \n    :param predictedField: (string)\n    :param swarmParams: (dict) overrides any swarm params\n    \"\"\"\n    if self._confluence is None:\n      raise Exception(\"Missing Confluence! Cannot attempt operation requiring \"\n                      \"data without first loading the data.\")\n    if predictedField is None:\n      predictedField = self._predictedField\n    fields = self._createFieldDescription()\n    swarmDesc = createSwarmDescription(\n      fields, csvPath, predictedField, swarmParams=swarmParams\n    )\n    with open(outPath, \"w\") as swarmOut:\n      swarmOut.write(json.dumps(swarmDesc))", "response": "Writes swarm description file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepareSwarm(self, predictedField=None, swarmParams=None):\n    csvPath, workingDirPath = self.populateCsv()\n    swarmDescriptionPath = os.path.join(\n      workingDirPath, \"swarm_description.json\"\n    )\n    self.writeSwarmDescription(\n      csvPath, swarmDescriptionPath, \n      predictedField=predictedField, swarmParams=swarmParams\n    )", "response": "Populates the CSV file and creates a swarm_description. json file for it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a swarm with data within a working directory.", "response": "def runSwarm(self, workingDirPath):\n    \"\"\"\n    Runs a swarm with data within a working directory. This assumes that the \n    user has already run prepareSwarm().\n    :param workingDirPath: absolute or relative path to working directory\n    \"\"\"\n    if not os.path.exists(workingDirPath):\n      raise Exception(\"Working directory %s does not exist!\" % workingDirPath)\n    banner(\"RUNNING SWARM\")\n    self._modelParams = swarm(workingDirPath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef swarm(self, predictedField=None, swarmParams=None):\n    self.prepareSwarm(\n      predictedField=predictedField, swarmParams=swarmParams\n    )\n    self.runSwarm(self._workingDir)", "response": "Runs a swarm on the data and swarm description found within the given working directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching data from river streams and feeds them into the given function.", "response": "def stream(self, handler, whenDone=None):\n    \"\"\"\n    Fetches data from river streams and feeds them into the given function.\n    :param handler: (function) passed headers [list] and row [list] of the data\n                    for one time step, for every row of data\n    \"\"\"\n    self._createConfluence()\n    headers = [\"timestamp\"] + self.getStreamIds()\n    for row in self._confluence:\n      handler(headers, row)\n    \n    if whenDone is not None:\n      return whenDone()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, debug=False):\n\n        if debug:\n            # Set some default times for execution (debugging)\n            start_time = datetime(year=2016, month=10, day=19, hour=12, minute=28, tzinfo=UTC)\n            duration = timedelta(seconds=5)\n            end_time = start_time + duration\n\n            relative_interval = RelativeTimeInterval(0, 0)\n            time_interval = TimeInterval(start_time, end_time)\n            # workflow_id = \"lda_localisation_model_predict\"\n        else:\n            duration = 0  # not needed\n            relative_interval = self.hyperstream.config.online_engine.interval\n            time_interval = relative_interval.absolute(utcnow())\n\n        for _ in range(self.hyperstream.config.online_engine.iterations):\n            if not debug:\n                # if this takes more than x minutes, kill myself\n                signal.alarm(self.hyperstream.config.online_engine.alarm)\n\n            logging.info(\"Online engine starting up.\")\n\n            # self.hyperstream.workflow_manager.set_requested_intervals(workflow_id, TimeIntervals([time_interval]))\n            self.hyperstream.workflow_manager.set_all_requested_intervals(TimeIntervals([time_interval]))\n            self.hyperstream.workflow_manager.execute_all()\n\n            logging.info(\"Online engine shutting down.\")\n            logging.info(\"\")\n\n            sleep(self.hyperstream.config.online_engine.sleep)\n\n            if debug:\n                time_interval += duration\n            else:\n                time_interval = TimeInterval(time_interval.end, utcnow() + timedelta(seconds=relative_interval.end))", "response": "Execute the online engine."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the ipynb file to a gallery file.", "response": "def convert_ipynb_to_gallery(file_name):\n    \"\"\"\n    Blatantly stolen + adapted from\n    https://gist.github.com/wuhuikai/4a7ceb8bc52454e17a4eb8327d538d85\n\n    \"\"\"\n    python_file = \"\"\n\n    nb_dict = json.load(open(file_name))\n    cells = nb_dict['cells']\n\n    for i, cell in enumerate(cells):\n        if i == 0:\n            assert cell['cell_type'] == 'markdown', \\\n                'First cell has to be markdown'\n\n            md_source = ''.join(cell['source'])\n            rst_source = pdoc.convert_text(md_source, 'rst', 'md')\n            python_file = '\"\"\"\\n' + rst_source + '\\n\"\"\"'\n        else:\n            if cell['cell_type'] == 'markdown':\n                md_source = ''.join(cell['source'])\n                rst_source = pdoc.convert_text(md_source, 'rst', 'md')\n                commented_source = '\\n'.join([\n                    '# ' + x for x in rst_source.split('\\n')\n                ])\n                python_file = python_file + '\\n\\n\\n' + '#' * 70 + '\\n' + \\\n                    commented_source\n            elif cell['cell_type'] == 'code':\n                source = ''.join(cell['source'])\n                python_file = python_file + '\\n' * 2 + source\n\n    open(file_name.replace('.ipynb', '.py'), 'w').write(python_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_index_nested(x, i):\r\n    for ind in range(len(x)):\r\n        if i == x[ind]:\r\n            return ind\r\n    return -1", "response": "Returns the index of the first element in x containing the value i."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing the movie metadata.", "response": "def show(movie):\n    \"\"\" Show the movie metadata.\n    \"\"\"\n    for key, value in sorted(movie.iteritems(), cmp=metadata_sorter, key=lambda x: x[0]):\n        if isinstance(value, list):\n            if not value:\n                continue\n            other = value[1:]\n            value = value[0]\n        else:\n            other = []\n        printer.p('<b>{key}</b>: {value}', key=key, value=value)\n        for value in other:\n            printer.p('{pad}{value}', value=value, pad=' ' * (len(key) + 2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef metadata_sorter(x, y):\n    if x == y:\n        return 0\n    if x in METADATA_SORTER_FIRST and y in METADATA_SORTER_FIRST:\n        return -1 if METADATA_SORTER_FIRST.index(x) < METADATA_SORTER_FIRST.index(y) else 1\n    elif x in METADATA_SORTER_FIRST:\n        return -1\n    elif y in METADATA_SORTER_FIRST:\n        return 1\n    else:\n        if x.startswith('_') and y.startswith('_'):\n            return cmp(x[1:], y[1:])\n        elif x.startswith('_'):\n            return 1\n        elif y.startswith('_'):\n            return -1\n        else:\n            return cmp(x, y)", "response": "Sort metadata keys by priority."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a Consul client using the given configuration.", "response": "def create_consul_client(consul_configuration: ConsulConfiguration) -> Consul:\n    \"\"\"\n    Creates a Consul client using the given configuration.\n    :param consul_configuration: the configuration to use to create the client\n    :return: the created client\n    \"\"\"\n    consul_client = Consul(\n        host=consul_configuration.host,\n        port=consul_configuration.port,\n        token=consul_configuration.token,\n        scheme=consul_configuration.scheme,\n        dc=consul_configuration.datacentre,\n        verify=consul_configuration.verify,\n        cert=consul_configuration.certificate)\n\n    # Work around for https://github.com/cablehead/python-consul/issues/170\n    consul_client.http.session.headers.update({\"X-Consul-Token\": consul_configuration.token})\n\n    return consul_client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing lines from the fileinput and send them to the log_parsers", "response": "def parse_lines(log_parsers, fileinp):\n    \"\"\"parse lines from the fileinput and send them to the log_parsers\"\"\"\n    while 1:\n        logentry = fileinp.readline()\n        if not logentry:\n            break\n        elif not logentry.rstrip():\n            continue  # skip newlines\n\n        processed = False\n        for lp in log_parsers:\n            if lp.grok(logentry):\n                processed = True\n        if not processed:\n            # error: none of the logparsers worked on the line\n            logger = logging.getLogger('logparser')\n            logger.warning(\n                #'Could not parse line %s, in file %s >>>%s<<<',\n                #fileinp.lineno(), fileinp.filename(), line.rstrip())\n                'Could not parse line >>>%s<<<', logentry.rstrip())\n            print('Could not parse line >>>%s<<<' % logentry.rstrip())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_commands(self, parser):\n\n        entrypoints = self._get_entrypoints()\n\n        already_loaded = set()\n        for entrypoint in entrypoints:\n            if entrypoint.name not in already_loaded:\n                command_class = entrypoint.load()\n                command_class(entrypoint.name, self, parser).prepare()\n                already_loaded.add(entrypoint.name)", "response": "Load commands of this profile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(tree, source_filename):\n    #_, ext = os.path.splitext(source_filename)\n    filehash = sha1()\n    with printer.progress(os.path.getsize(source_filename)) as update:\n        with open(source_filename, 'rb') as fsource:\n            with NamedTemporaryFile(dir=os.path.join(tree, '.kolekto', 'movies'), delete=False) as fdestination:\n                # Copy the source into the temporary destination:\n                while True:\n                    buf = fsource.read(10 * 1024)\n                    if not buf:\n                        break\n                    filehash.update(buf)\n                    fdestination.write(buf)\n                    update(len(buf))\n                # Rename the file to its final name or raise an error if\n                # the file already exists:\n                dest = os.path.join(tree, '.kolekto', 'movies', filehash.hexdigest())\n                if os.path.exists(dest):\n                    raise IOError('This file already exists in tree (%s)' % filehash.hexdigest())\n                else:\n                    os.rename(fdestination.name, dest)\n    return filehash.hexdigest()", "response": "Copy a file in tree to a new file in tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all attachments for the specified fullname.", "response": "def list_attachments(fullname):\n    \"\"\" List attachment for the specified fullname.\n    \"\"\"\n    parent, filename = os.path.split(fullname)\n    filename_without_ext, ext = os.path.splitext(filename)\n    attachments = []\n    for found_filename in os.listdir(parent):\n        found_filename_without_ext, _ = os.path.splitext(found_filename)\n        if filename_without_ext == found_filename_without_ext and found_filename != filename:\n            attachments.append(os.path.join(parent, found_filename))\n    return attachments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _search(self, mdb, query, filename, year=None, auto=False):\n        choices = []\n        for datasource, movie in mdb.search(query, year=year):\n            if auto:\n                return datasource, movie\n            if movie.get('directors'):\n                directors = ' by '\n                if len(movie['directors']) > 1:\n                    directors += '%s and %s' % (', '.join(movie['directors'][0:-1]),\n                                                          movie['directors'][-1])\n                else:\n                    directors += movie['directors'][0]\n            else:\n                directors = ''\n            fmt = u'<b>{title}</b> ({year}){directors} [{datasource}]'\n            choices.append(option((datasource, movie), fmt, title=movie['title'],\n                                                            year=movie.get('year', 'Unknown'),\n                                                            directors=directors,\n                                                            datasource=datasource.name))\n\n        if not choices:\n            printer.p('No results to display for the file: {fn}', fn=filename)\n            return None, None\n\n        choices.append(option(('manual', None), 'Enter information manually'))\n        choices.append(option(('abort', None), 'None of these'))\n        printer.p('Please choose the relevant movie for the file: {fn}', fn=filename, end='\\n\\n')\n        return printer.choice(choices)", "response": "Search the movie using all available datasources and let the user choose a result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch the movie using all available datasources and let the user choose a result.", "response": "def _search(self, mdb, query, filename, season_num, episode_num, auto=False):\n        \"\"\" Search the movie using all available datasources and let the user\n            select a result. Return the choosen datasource and produced movie dict.\n\n        If auto is enabled, directly returns the first movie found.\n        \"\"\"\n        choices = []\n        for datasource, movie in mdb.search(query, season=season_num, episode=episode_num):\n            if auto:\n                return datasource, movie\n            fmt = u'<b>{title}</b> - <b>{ep}</b> S{season:02d}E{episode:02d} [{datasource}]'\n            choices.append(option((datasource, movie), fmt, title=movie['title'],\n                                                            ep=movie['episode_title'],\n                                                            season=movie['season'],\n                                                            episode=movie['episode'],\n                                                            datasource=datasource.name))\n\n        if not choices:\n            printer.p('No results to display for the file: {fn}', fn=filename)\n            return None, None\n\n        choices.append(option(('manual', None), 'Enter information manually'))\n        choices.append(option(('abort', None), 'None of these'))\n        printer.p('Please choose the relevant result for the file: {fn}', fn=filename, end='\\n\\n')\n        return printer.choice(choices)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef purge_stream(self, stream_id, remove_definition=False, sandbox=None):\n        super(AssetsChannel, self).purge_stream(\n            stream_id=stream_id, remove_definition=remove_definition, sandbox=sandbox)", "response": "Purge the stream with the given identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_to_stream(self, stream_id, data, sandbox=None):\n        if sandbox is not None:\n            raise NotImplementedError\n\n        if stream_id not in self.streams:\n            raise StreamNotFoundError(\"Stream with id '{}' does not exist\".format(stream_id))\n\n        writer = self.get_stream_writer(self.streams[stream_id])\n\n        if isinstance(data, StreamInstance):\n            data = [data]\n\n        for instance in data:\n            if not isinstance(instance, StreamInstance):\n                raise ValueError(\"Expected StreamInstance, got {}\".format(str(type(instance))))\n            writer(instance)", "response": "Writes the data to the specified stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _startXTVDNode(self, name, attrs):\n\n        schemaVersion = attrs.get('schemaVersion')\n        validFrom = self._parseDateTime(attrs.get('from'))\n        validTo = self._parseDateTime(attrs.get('to'))\n        self._progress.printMsg('Parsing version %s data from %s to %s' %\n                                    (schemaVersion,\n                                     validFrom.strftime('%Y/%m/%d'),\n                                     validTo.strftime('%Y/%m/%d')))", "response": "Process the start of the top - level xtvd node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the start of a stations node under xtvd / stations", "response": "def _startStationsNode(self, name, attrs):\n        \"\"\"Process the start of a node under xtvd/stations\"\"\"\n\n        if name == 'station':\n            self._stationId = attrs.get('id')\n            self._callSign = None\n            self._stationName = None\n            self._affiliate = None\n            self._fccChannelNumber = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the end of a stations node under xtvd / stations", "response": "def _endStationsNode(self, name, content):\n        \"\"\"Process the end of a node under xtvd/stations\"\"\"\n\n        if name == 'callSign':\n            self._callSign = content\n        elif name == 'name':\n            self._stationName = content\n        elif name == 'affiliate':\n            self._affiliate = content\n        elif name == 'fccChannelNumber':\n            self._fccChannelNumber = content\n        elif name == 'station':\n            if not self._error:\n                self._importer.new_station(self._stationId, self._callSign,\n                                          self._stationName, self._affiliate,\n                                          self._fccChannelNumber)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the start of a lineup node under xtvd / lineups", "response": "def _startLineupsNode(self, name, attrs):\n        \"\"\"Process the start of a node under xtvd/lineups\"\"\"\n\n        if name == 'lineup':\n            self._lineupName = attrs.get('name')\n            self._location = attrs.get('location')\n            self._device = attrs.get('device')\n            self._lineupType = attrs.get('type')\n            self._postalCode = attrs.get('postalCode')\n            self._lineupId = attrs.get('id')\n            self._importer.new_lineup(self._lineupName, self._location,\n                                     self._device, self._lineupType,\n                                     self._postalCode, self._lineupId)\n\n            self._progress.printMsg('Parsing lineup %s' % self._lineupName)\n        elif name == 'map':\n            self._stationId = attrs.get('station')\n            self._channel = attrs.get('channel')\n            self._channelMinor = attrs.get('channelMinor')\n            self._validFrom = self._parseDateTime(attrs.get('from'))\n            self._validTo = self._parseDateTime(attrs.get('to'))\n            self._onAirFrom = None\n            self._onAirTo = None\n        elif name == 'onAir':\n            self._onAirFrom = self._parseDateTime(attrs.get('from'))\n            self._onAirTo = self._parseDateTime(attrs.get('to'))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the end of a lineup node under xtvd / lineups", "response": "def _endLineupsNode(self, name, content):\n        \"\"\"Process the end of a node under xtvd/lineups\"\"\"\n\n        if name == 'map':\n            if not self._error:\n                self._importer.new_mapping(self._lineupId, self._stationId,\n                                          self._channel, self._channelMinor,\n                                          self._validFrom, self._validTo,\n                                          self._onAirFrom, self._onAirTo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _startSchedulesNode(self, name, attrs):\n\n        if name == 'schedule':\n            self._programId = attrs.get('program')\n            self._stationId = attrs.get('station')\n            self._time = self._parseDateTime(attrs.get('time'))\n            self._duration = self._parseDuration(attrs.get('duration'))\n            self._new = attrs.has_key('new')\n            self._stereo = attrs.has_key('stereo')\n            self._subtitled = attrs.has_key('subtitled')\n            self._hdtv = attrs.has_key('hdtv')\n            self._closeCaptioned = attrs.has_key('closeCaptioned')\n            self._ei = attrs.has_key('ei')\n            self._tvRating = attrs.get('tvRating')\n            self._dolby = attrs.get('dolby')\n            self._partNumber = None\n            self._partTotal = None\n        elif name == 'part':\n            self._partNumber = attrs.get('number')\n            self._partTotal = attrs.get('total')", "response": "Process the start of a node under xtvd / schedules"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the end of a schedule node under xtvd / schedules", "response": "def _endSchedulesNode(self, name, content):\n        \"\"\"Process the end of a node under xtvd/schedules\"\"\"\n\n        if name == 'schedule':\n            if not self._error:\n                self._importer.new_schedule(self._programId, self._stationId,\n                                           self._time, self._duration,\n                                           self._new, self._stereo,\n                                           self._subtitled, self._hdtv,\n                                           self._closeCaptioned, self._ei,\n                                           self._tvRating, self._dolby,\n                                           self._partNumber, self._partTotal)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the start of a program node under xtvd / programs", "response": "def _startProgramsNode(self, name, attrs):\n        \"\"\"Process the start of a node under xtvd/programs\"\"\"\n\n        if name == 'program':\n            self._programId = attrs.get('id')\n            self._series = None\n            self._title = None\n            self._subtitle = None\n            self._description = None\n            self._mpaaRating = None\n            self._starRating = None\n            self._runTime = None\n            self._year = None\n            self._showType = None\n            self._colorCode = None\n            self._originalAirDate = None\n            self._syndicatedEpisodeNumber = None\n            self._advisories = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the end of a program node under xtvd / programs", "response": "def _endProgramsNode(self, name, content):\n        \"\"\"Process the end of a node under xtvd/programs\"\"\"\n\n        if name == 'series':\n            self._series = content\n        elif name == 'title':\n            self._title = content\n        elif name == 'subtitle':\n            self._subtitle = content\n        elif name == 'description':\n            self._description = content\n        elif name == 'mpaaRating':\n            self._mpaaRating = content\n        elif name == 'starRating':\n            self._starRating = content\n        elif name == 'runTime':\n            self._runTime = self._parseDuration(content)\n        elif name == 'year':\n            self._year = content\n        elif name == 'showType':\n            self._showType = content\n        elif name == 'colorCode':\n            self._colorCode = content\n        elif name == 'originalAirDate':\n            self._originalAirDate = self._parseDate(content)\n        elif name == 'syndicatedEpisodeNumber':\n            self._syndicatedEpisodeNumber = content\n        elif name == 'advisory':\n            self._advisories.append(content)\n        elif name == 'program':\n            if not self._error:\n                self._importer.new_program(self._programId, self._series,\n                                          self._title, self._subtitle,\n                                          self._description, self._mpaaRating,\n                                          self._starRating, self._runTime,\n                                          self._year, self._showType,\n                                          self._colorCode,\n                                          self._originalAirDate,\n                                          self._syndicatedEpisodeNumber,\n                                          self._advisories)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _startProductionCrewNode(self, name, attrs):\n\n        if name == 'crew':\n            self._programId = attrs.get('program')\n        elif name == 'member':\n            self._role = None\n            self._givenname = None\n            self._surname = None", "response": "Process the start of a production crew node under xtvd / productionCrew"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the end of a production crew node under xtvd / productionCrew", "response": "def _endProductionCrewNode(self, name, content):\n        \"\"\"Process the end of a node under xtvd/productionCrew\"\"\"\n\n        if name == 'role':\n            self._role = content\n        elif name == 'givenname':\n            self._givenname = content\n        elif name == 'surname':\n            self._surname = content\n        elif name == 'member':\n            if not self._error:\n                if self._givenname:\n                    name = '%s %s' % (self._givenname, self._surname)\n                else:\n                    name = self._surname\n\n                self._importer.new_crew_member(self._programId, self._role,\n                                             name, self._givenname,\n                                             self._surname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the start of a genres node under xtvd / genres", "response": "def _startGenresNode(self, name, attrs):\n        \"\"\"Process the start of a node under xtvd/genres\"\"\"\n\n        if name == 'programGenre':\n            self._programId = attrs.get('program')\n        elif name == 'genre':\n            self._genre = None\n            self._relevance = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _endGenresNode(self, name, content):\n\n        if name == 'class':\n            self._genre = content\n        elif name == 'relevance':\n            self._relevance = content\n        elif name == 'genre':\n            if not self._error:\n                self._importer.new_genre(self._programId, self._genre,\n                                        self._relevance)", "response": "Process the end of a genres node under xtvd / genres"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef endElement(self, name):\n\n        content = ''.join(self._contentList)\n\n        if name == 'xtvd':\n            self._progress.endItems()\n        else:\n            try:\n                if self._context == 'stations':\n                    self._endStationsNode(name, content)\n                elif self._context == 'lineups':\n                    self._endLineupsNode(name, content)\n                elif self._context == 'schedules':\n                    self._endSchedulesNode(name, content)\n                elif self._context == 'programs':\n                    self._endProgramsNode(name, content)\n                elif self._context == 'productionCrew':\n                    self._endProductionCrewNode(name, content)\n                elif self._context == 'genres':\n                    self._endGenresNode(name, content)\n            except Exception, e:\n                self._error = True\n                self._progress.printMsg(str(e), error=True)\n\n        self._context = self._contextStack.pop()", "response": "Callback run at the end of each XML element"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef error(self, msg):\n\n        self._error = True\n        self._progress.printMsg('XML parse error: %s' % msg, error=True)", "response": "Callback run when an XML parse error occurs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_schedule(self, program, station, time, duration, new, stereo,\n                    subtitled, hdtv, closeCaptioned, ei, tvRating, dolby,\n                    partNumber, partTotal):\n        \"\"\"Callback run for each new schedule entry\"\"\"\n\n        raise NotImplementedError()", "response": "Callback run for each new schedule entry"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting the input string using each possible combination of lists in the provided environment. Returns a list of formated strings.", "response": "def format_all(format_string, env):\n    \"\"\" Format the input string using each possible combination of lists\n        in the provided environment. Returns a list of formated strings.\n    \"\"\"\n\n    prepared_env = parse_pattern(format_string, env, lambda x, y: [FormatWrapper(x, z) for z in y])\n    # Generate each possible combination, format the string with it and yield\n    # the resulting string:\n    for field_values in product(*prepared_env.itervalues()):\n        format_env = dict(izip(prepared_env.iterkeys(), field_values))\n        yield format_string.format(**format_env)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all links contained in directory (or any sub directory).", "response": "def walk_links(directory, prefix='', linkbase=None):\n    \"\"\" Return all links contained in directory (or any sub directory).\n    \"\"\"\n    links = {}\n    try:\n        for child in os.listdir(directory):\n            fullname = os.path.join(directory, child)\n            if os.path.islink(fullname):\n                link_path = os.path.normpath(os.path.join(directory, os.readlink(fullname)))\n                if linkbase:\n                    link_path = os.path.relpath(link_path, linkbase)\n                links[os.path.join(prefix, child)] = link_path\n            elif os.path.isdir(fullname):\n                links.update(walk_links(fullname,\n                                        prefix=os.path.join(prefix, child),\n                                        linkbase=linkbase))\n    except OSError as err:\n        if err.errno != 2:  # Ignore unknown directory error\n            raise\n    return links"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef preloop(self):\n        Cmd.preloop(self)    # sets up command completion\n        self._hist = []    # No history yet\n        self._locals = {}    # Initialize execution namespace for user\n        self._globals = {}", "response": "Initialize before prompting user for commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute_tool(self, stream, interval):\n        if interval.end > self.up_to_timestamp:\n            raise ValueError(\n                'The stream is not available after ' + str(self.up_to_timestamp) + ' and cannot be calculated')\n\n        required_intervals = TimeIntervals([interval]) - stream.calculated_intervals\n        if not required_intervals.is_empty:\n            for interval in required_intervals:\n                stream.tool.execute(stream.input_streams, stream, interval)\n                stream.calculated_intervals += interval\n\n            if not stream.required_intervals.is_empty:\n                raise RuntimeError('Tool execution did not cover the specified time interval.')", "response": "Executes the stream s tool over the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_create_stream(self, stream_id, try_create=True):\n        stream_id = get_stream_id(stream_id)\n        if stream_id in self.streams:\n            logging.debug(\"found {}\".format(stream_id))\n            return self.streams[stream_id]\n        elif try_create:\n            # Try to create the stream\n            logging.debug(\"creating {}\".format(stream_id))\n            return self.create_stream(stream_id=stream_id)", "response": "Helper function to get a stream or create one if it s not already defined."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind streams with the given meta data values. Useful for debugging purposes.", "response": "def find_streams(self, **kwargs):\n        \"\"\"\n        Finds streams with the given meta data values. Useful for debugging purposes.\n\n        :param kwargs: The meta data as keyword arguments\n        :return: The streams found\n        \"\"\"\n        found = {}\n\n        if 'name' in kwargs:\n            name = kwargs.pop('name')\n        else:\n            name = None\n\n        for stream_id, stream in self.streams.items():\n            if name is not None and stream_id.name != name:\n                continue\n\n            d = dict(stream_id.meta_data)\n            if all(k in d and d[k] == str(v) for k, v in kwargs.items()):\n                found[stream_id] = stream\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a single stream with the given meta data values. Useful for debugging purposes.", "response": "def find_stream(self, **kwargs):\n        \"\"\"\n        Finds a single stream with the given meta data values. Useful for debugging purposes.\n\n        :param kwargs: The meta data as keyword arguments\n        :return: The stream found\n        \"\"\"\n        found = list(self.find_streams(**kwargs).values())\n        if not found:\n            raise StreamNotFoundError(kwargs)\n        if len(found) > 1:\n            raise MultipleStreamsFoundError(kwargs)\n        return found[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npurges a node from the local cache", "response": "def purge_node(self, node_id, remove_definition=False, sandbox=None):\n        \"\"\"\n        Purges a node (collection of streams)\n\n        :param node_id: The node identifier\n        :param remove_definition: Whether to remove the stream definition as well\n        :param sandbox: The sandbox\n        :return: None\n        \"\"\"\n        for stream_id in list(self.streams):\n            stream = self.streams[stream_id]\n            if not stream.parent_node:\n                # This can happen if streams have been defined outside of nodes - generally nothing to worry about\n                logging.debug(\"cannot purge the stream with id {} because it has no parent node\".format(stream_id))\n                continue\n            if stream.parent_node.node_id == node_id:\n                self.purge_stream(stream_id, remove_definition=remove_definition, sandbox=sandbox)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(streamIds, **kwargs):\n  print \"Creating Confluence for the following RiverStreams:\" \\\n        \"\\n\\t%s\" % \",\\n\\t\".join([\":\".join(row) for row in streamIds])\n  confluence = Confluence(streamIds, **kwargs)\n  confluence.load()\n  return confluence", "response": "Creates and loads a Confluence object for the given list of RiverStreams."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef next_blob(self):\n        blob_file = self.blob_file\n        try:\n            preamble = DAQPreamble(file_obj=blob_file)\n        except struct.error:\n            raise StopIteration\n\n        try:\n            data_type = DATA_TYPES[preamble.data_type]\n        except KeyError:\n            log.error(\"Unkown datatype: {0}\".format(preamble.data_type))\n            data_type = 'Unknown'\n\n        blob = Blob()\n        blob[data_type] = None\n        blob['DAQPreamble'] = preamble\n\n        if data_type == 'DAQSummaryslice':\n            daq_frame = DAQSummaryslice(blob_file)\n            blob[data_type] = daq_frame\n            blob['DAQHeader'] = daq_frame.header\n        elif data_type == 'DAQEvent':\n            daq_frame = DAQEvent(blob_file)\n            blob[data_type] = daq_frame\n            blob['DAQHeader'] = daq_frame.header\n        else:\n            log.warning(\n                \"Skipping DAQ frame with data type code '{0}'.\".format(\n                    preamble.data_type\n                )\n            )\n            blob_file.seek(preamble.length - DAQPreamble.size, 1)\n\n        return blob", "response": "Get the next DAQ frame from the file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmove file pointer to the frame with given index.", "response": "def seek_to_frame(self, index):\n        \"\"\"Move file pointer to the frame with given index.\"\"\"\n        pointer_position = self.frame_positions[index]\n        self.blob_file.seek(pointer_position, 0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the file pointer positions of each frame in the archive.", "response": "def determine_frame_positions(self):\n        \"\"\"Record the file pointer position of each frame\"\"\"\n        self.rewind_file()\n        with ignored(struct.error):\n            while True:\n                pointer_position = self.blob_file.tell()\n                length = struct.unpack('<i', self.blob_file.read(4))[0]\n                self.blob_file.seek(length - 4, 1)\n                self.frame_positions.append(pointer_position)\n        self.rewind_file()\n        log.info(\"Found {0} frames.\".format(len(self.frame_positions)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts the values from the byte string.", "response": "def _parse_byte_data(self, byte_data):\n        \"\"\"Extract the values from byte string.\"\"\"\n        self.length, self.data_type = unpack('<ii', byte_data[:self.size])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract the values from the byte string.", "response": "def _parse_byte_data(self, byte_data):\n        \"\"\"Extract the values from byte string.\"\"\"\n        chunks = unpack('<iiiii', byte_data[:self.size])\n        det_id, run, time_slice, time_stamp, ticks = chunks\n        self.det_id = det_id\n        self.run = run\n        self.time_slice = time_slice\n        self.time_stamp = time_stamp\n        self.ticks = ticks"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_summary_frames(self, file_obj):\n        for _ in range(self.n_summary_frames):\n            dom_id = unpack('<i', file_obj.read(4))[0]\n            dq_status = file_obj.read(4)    # probably dom status? # noqa\n            dom_status = unpack('<iiii', file_obj.read(16))\n            raw_rates = unpack('b' * 31, file_obj.read(31))\n            pmt_rates = [self._get_rate(value) for value in raw_rates]\n            self.summary_frames[dom_id] = pmt_rates\n            self.dq_status[dom_id] = dq_status\n            self.dom_status[dom_id] = dom_status\n            self.dom_rates[dom_id] = np.sum(pmt_rates)", "response": "Iterate through the byte data and fill the summary_frames with the sum of the pmt rates and dom status."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the rate in Hz from the short int value", "response": "def _get_rate(self, value):\n        \"\"\"Return the rate in Hz from the short int value\"\"\"\n        if value == 0:\n            return 0\n        else:\n            return MINIMAL_RATE_HZ * math.exp(value * self._get_factor())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_triggered_hits(self, file_obj):\n        for _ in range(self.n_triggered_hits):\n            dom_id, pmt_id = unpack('<ib', file_obj.read(5))\n            tdc_time = unpack('>I', file_obj.read(4))[0]\n            tot = unpack('<b', file_obj.read(1))[0]\n            trigger_mask = unpack('<Q', file_obj.read(8))\n            self.triggered_hits.append(\n                (dom_id, pmt_id, tdc_time, tot, trigger_mask)\n            )", "response": "Parse and store triggered hits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse and store snapshot hits.", "response": "def _parse_snapshot_hits(self, file_obj):\n        \"\"\"Parse and store snapshot hits.\"\"\"\n        for _ in range(self.n_snapshot_hits):\n            dom_id, pmt_id = unpack('<ib', file_obj.read(5))\n            tdc_time = unpack('>I', file_obj.read(4))[0]\n            tot = unpack('<b', file_obj.read(1))[0]\n            self.snapshot_hits.append((dom_id, pmt_id, tdc_time, tot))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef runtable(det_id, n=5, run_range=None, compact=False, sep='\\t', regex=None):\n    db = kp.db.DBManager()\n    df = db.run_table(det_id)\n\n    if run_range is not None:\n        try:\n            from_run, to_run = [int(r) for r in run_range.split('-')]\n        except ValueError:\n            log.critical(\"Please specify a valid range (e.g. 3100-3200)!\")\n            raise SystemExit\n        else:\n            df = df[(df.RUN >= from_run) & (df.RUN <= to_run)]\n\n    if regex is not None:\n        try:\n            re.compile(regex)\n        except re.error:\n            log.error(\"Invalid regex!\")\n            return\n\n        df = df[df['RUNSETUPNAME'].str.contains(regex)\n                | df['RUNSETUPID'].str.contains(regex)]\n\n    if n is not None:\n        df = df.tail(n)\n\n    if compact:\n        df = df[['RUN', 'DATETIME', 'RUNSETUPNAME']]\n\n    df.to_csv(sys.stdout, sep=sep)", "response": "Print the run table of the last n runs for given detector"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits a single A and H calibration set to a single A and H.", "response": "def fit_ahrs(A, H, Aoff, Arot, Hoff, Hrot):\n    \"\"\"Calculate yaw, pitch and roll for given A/H and calibration set.\n\n    Author: Vladimir Kulikovsky\n\n    Parameters\n    ----------\n    A: list, tuple or numpy.array of shape (3,)\n    H: list, tuple or numpy.array of shape (3,)\n    Aoff: numpy.array of shape(3,)\n    Arot: numpy.array of shape(3, 3)\n    Hoff: numpy.array of shape(3,)\n    Hrot: numpy.array of shape(3, 3)\n\n    Returns\n    -------\n    yaw, pitch, roll\n\n    \"\"\"\n    Acal = np.dot(A - Aoff, Arot)\n    Hcal = np.dot(H - Hoff, Hrot)\n\n    # invert axis for DOM upside down\n    for i in (1, 2):\n        Acal[i] = -Acal[i]\n        Hcal[i] = -Hcal[i]\n\n    roll = arctan2(-Acal[1], -Acal[2])\n    pitch = arctan2(Acal[0], np.sqrt(Acal[1] * Acal[1] + Acal[2] * Acal[2]))\n    yaw = arctan2(\n        Hcal[2] * sin(roll) - Hcal[1] * cos(roll),\n        sum((\n            Hcal[0] * cos(pitch), Hcal[1] * sin(pitch) * sin(roll),\n            Hcal[2] * sin(pitch) * cos(roll)\n        ))\n    )\n\n    yaw = np.degrees(yaw)\n    while yaw < 0:\n        yaw += 360\n    # yaw = (yaw + magnetic_declination + 360 ) % 360\n    roll = np.degrees(roll)\n    pitch = np.degrees(pitch)\n    return yaw, pitch, roll"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_calibration(xroot):\n    names = [c.text for c in xroot.findall(\".//Name\")]\n    val = [[i.text for i in c] for c in xroot.findall(\".//Values\")]\n\n    # The fields has to be reindeced, these are the index mappings\n    col_ic = [int(v) for v in val[names.index(\"AHRS_Matrix_Column(-)\")]]\n    try:\n        row_ic = [int(v) for v in val[names.index(\"AHRS_Matrix_Row(-)\")]]\n    except ValueError:\n        row_ic = [2, 2, 2, 1, 1, 1, 0, 0, 0]\n    try:\n        vec_ic = [int(v) for v in val[names.index(\"AHRS_Vector_Index(-)\")]]\n    except ValueError:\n        vec_ic = [2, 1, 0]\n\n    Aoff_ix = names.index(\"AHRS_Acceleration_Offset(g/ms^2-)\")\n    Arot_ix = names.index(\"AHRS_Acceleration_Rotation(-)\")\n    Hrot_ix = names.index(\"AHRS_Magnetic_Rotation(-)\")\n\n    Aoff = np.array(val[Aoff_ix])[vec_ic].astype(float)\n    Arot = np.array(val[Arot_ix]).reshape(3, 3)[col_ic, row_ic] \\\n                                 .reshape(3, 3).astype(float)\n    Hrot = np.array(val[Hrot_ix]).reshape(3, 3)[col_ic, row_ic] \\\n                                 .reshape(3, 3).astype(float)\n\n    Hoff = []\n    for q in 'XYZ':\n        values = []\n        for t in ('Min', 'Max'):\n            ix = names.index(\"AHRS_Magnetic_{}{}(G-)\".format(q, t))\n            values.append(float(val[ix][0]))\n        Hoff.append(sum(values) / 2.)\n    Hoff = np.array(Hoff)\n\n    return Aoff, Arot, Hoff, Hrot", "response": "Extract AHRS calibration information from XML root."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calibrate(self):\n        now = time.time()\n        dom_ids = self.A.keys()\n        print(\n            \"Calibrating AHRS from median A and H for {} DOMs.\".format(\n                len(dom_ids)\n            )\n        )\n        calibrations = {}\n        for dom_id in dom_ids:\n            print(\"Calibrating DOM ID {}\".format(dom_id))\n            clb_upi = self.db.doms.via_dom_id(dom_id).clb_upi\n            ahrs_calib = get_latest_ahrs_calibration(clb_upi)\n            if ahrs_calib is None:\n                log.warning(\"AHRS calibration missing for '{}'\".format(dom_id))\n                continue\n            du, floor, _ = self.detector.doms[dom_id]\n            A = np.median(self.A[dom_id], axis=0)\n            H = np.median(self.H[dom_id], axis=0)\n\n            cyaw, cpitch, croll = fit_ahrs(A, H, *ahrs_calib)\n            calibrations[dom_id] = (now, du, floor, cyaw, cpitch, croll)\n        self.A = defaultdict(list)\n        self.H = defaultdict(list)\n        return calibrations", "response": "Calculate yaw pitch and roll from the median of A and H."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef humanize_filesize(value):\n\n    value = float(value)\n\n    if value == 1:\n        return '1 Byte'\n    elif value < 1024:\n        return '%d Bytes' % value\n    elif value < 1024:\n        return '%dB' % value\n\n    for i, s in enumerate(SUFFIXES):\n        unit = 1024 ** (i + 2)\n        if value < unit:\n            return '%.1f %s' % ((1024 * value / unit), s)\n    return '%.1f %s' % ((1024 * value / unit), s)", "response": "Return a humanized file size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_output_format(expected_formats):\n    def output_format_decorator(func):\n        def func_wrapper(*args, **kwargs):\n            self = args[0]\n            if self.output_format not in expected_formats:\n                raise ValueError(\"expected output format {}, got {}\".format('doc_gen', self.output_format))\n            return func(*args, **kwargs)\n        return func_wrapper\n    return output_format_decorator", "response": "Decorator for stream outputs that checks the format of the outputs after modifiers have been applied."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_tool_defined(func):\n    def func_wrapper(*args):\n        self = args[0]\n        # Deferred import to avoid circular dependency\n        from ..channels import ToolChannel\n        if isinstance(self.channel, ToolChannel) and not self.defined:\n            raise RuntimeError(\"Tool not yet defined\")\n        return func(*args)\n    return func_wrapper", "response": "Decorator to check whether a tool stream has been defined before execution\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_input_stream_count(expected_number_of_streams):\n\n    def stream_count_decorator(func):\n        def func_wrapper(*args, **kwargs):\n            self = args[0]\n            sources = kwargs['sources'] if 'sources' in kwargs else args[1]\n            if expected_number_of_streams == 0:\n                if sources:\n                    raise ValueError(\"No input streams expected\")\n            else:\n                given_number_of_streams = len(sources) if sources else 0\n                if given_number_of_streams != expected_number_of_streams:\n                    raise ValueError(\"{} tool takes {} stream(s) as input ({} given)\".format(\n                        self.__class__.__name__, expected_number_of_streams, given_number_of_streams))\n            return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return stream_count_decorator", "response": "Decorator for Tool. _execute that checks the number of input streams"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(filename):\n\n    with open(filename, \"rb\") as data:\n        header, v_major, v_minor, chunk_count = struct.unpack(\"!4sHHI\", data.read(12))\n\n        assert header == b\"ASEF\"\n        assert (v_major, v_minor) == (1, 0)\n\n        return [c for c in parser.parse_chunk(data)]", "response": "parses a. ase file and returns a list of colors and color groups and palettes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dumps(obj):\n    header = b'ASEF'\n    v_major, v_minor = 1, 0\n    chunk_count = writer.chunk_count(obj)\n\n    head = struct.pack('!4sHHI', header, v_major, v_minor, chunk_count)\n    body = b''.join([writer.chunk_for_object(c) for c in obj])\n    return head + body", "response": "converts a swatch to bytes suitable for writing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef isFullPreferenceOrder(self, candList):\n\n        # If a candidate is missing from the wmgMap or if there is a pair of candidates for which \n        # there is no value in the wmgMap, then the wmgMap cannot be a full preference order.\n        for cand1 in candList:            \n            if cand1 not in self.wmgMap.keys():\n                return False\n            for cand2 in candList:\n                if cand1 == cand2:\n                    continue\n                if cand2 not in self.wmgMap[cand1].keys():\n                    return False\n        return True", "response": "Returns True if the underlying weighted majority graph contains a comparision between every\n        pair of candidate and returns False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the underlying weighted majority graph contains a tie between any pair of candidates and returns False otherwise.", "response": "def containsTie(self):\n        \"\"\"\n        Returns True if the underlying weighted majority graph contains a tie between any pair of\n        candidates and returns False otherwise.\n        \"\"\"\n\n        # If a value of 0 is present in the wmgMap, we assume that it represents a tie.\n        for cand in self.wmgMap.keys():\n            if 0 in self.wmgMap[cand].values():\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getIncEdgesMap(self):\n\n        # We calculate the number of incoming edges for each candidate and store it into a dictionary \n        # that associates the number of incoming edges with the candidates with that number.\n        incEdgesMap = dict()\n        for cand1 in self.wmgMap.keys():\n            incEdgesSum = 0\n            for cand2 in self.wmgMap[cand1].keys():\n                if self.wmgMap[cand1][cand2] > 0:\n                    incEdgesSum += self.wmgMap[cand1][cand2]\n            \n            # Check if this is the first candidate associated with this number of associated edges.\n            if incEdgesSum in incEdgesMap.keys():\n                incEdgesMap[incEdgesSum].append(cand1)\n            else:\n                incEdgesMap[incEdgesSum] = [cand1]  \n\n        return incEdgesMap", "response": "Returns a dictionary that associates numbers of incoming edges with the number of incoming edges in the weighted majority graph with the candidates that have that number of incoming edges."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a dictionary that associates the integer representation of each candidate with its position in the ranking starting from 1.", "response": "def getRankMap(self):\n        \"\"\"\n        Returns a dictionary that associates the integer representation of each candidate with its\n        position in the ranking, starting from 1.\n        \"\"\"\n\n        # We sort the candidates based on the number of incoming edges they have in the graph. If \n        # two candidates have the same number, we assume that they are tied.\n        incEdgesMap = self.getIncEdgesMap()\n        sortedKeys = sorted(incEdgesMap.keys(), reverse = True)\n        rankMap = dict()\n        pos = 1\n        for key in sortedKeys:\n            cands = incEdgesMap[key]\n            for cand in cands:\n                rankMap[cand] = pos\n            pos += 1\n        return rankMap"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary that associates each position in the ranking with a list of integer representations of the candidates ranked at that position.", "response": "def getReverseRankMap(self):\n        \"\"\"\n        Returns a dictionary that associates each position in the ranking with a list of integer \n        representations of the candidates ranked at that position.\n        \"\"\"\n        \n        # We sort the candidates based on the number of incoming edges they have in the graph. If \n        # two candidates have the same number, we assume that they are tied.\n        incEdgesMap = self.getIncEdgesMap()\n        sortedKeys = sorted(incEdgesMap.keys(), reverse = True)\n        reverseRankMap = dict()\n        pos = 1\n        for key in sortedKeys:\n            cands = incEdgesMap[key]\n            reverseRankMap[pos] = cands\n            pos += 1\n        return reverseRankMap"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getOrderVector(self):\n\n        # We sort the candidates based on the number of incoming edges they have in the graph. If \n        # two candidates have the same number, we assume that they are tied.\n        incEdgesMap = self.getIncEdgesMap()\n        sortedKeys = sorted(incEdgesMap.keys(), reverse = True)\n        orderVector = []\n        print(sortedKeys)\n        # print(\"sortedKeys\",sortedKeys)\n        # print(\"incEdgesMap\", incEdgesMap)\n        for key in sortedKeys:\n            tier = []\n            cands = incEdgesMap[key]\n            # print(\"qq\",cands)\n            for cand in cands:\n                tier.append(cand)\n                # print(\"cand=\",cand)\n            # print(\"tier\", tier)\n            orderVector.append(tier[0])  # replace tier with tier[0]\n        return orderVector", "response": "Returns a list of lists. Each list represents tiers of candidates."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of lists that represents tiers of candidates in the same tier.", "response": "def getOrderVectorEGMM(self):\n        \"\"\"\n        Returns a list of lists. Each list represents tiers of candidates. candidates in earlier\n        tiers are preferred to candidates appearing in later tiers. Candidates in the same tier\n        are preferred equally. \n        \"\"\"\n\n        # We sort the candidates based on the number of incoming edges they have in the graph. If \n        # two candidates have the same number, we assume that they are tied.\n        incEdgesMap = self.getIncEdgesMap()\n        sortedKeys = sorted(incEdgesMap.keys())\n        orderVector = []\n        # print(\"sortedKeys\",sortedKeys)\n        # print(\"incEdgesMap\", incEdgesMap)\n        m = 0\n        for key in sortedKeys:\n            m += len(incEdgesMap[key])\n        result = [0] * m\n        for k in range(0, len(sortedKeys)):\n            key = sortedKeys[k]\n            cands = incEdgesMap[key]\n            # print(\"qq\",cands)\n            for cand in cands:\n                result[cand] = len(sortedKeys) - (k + 1)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the n - th discrete difference along a given axis.", "response": "def diff(a, n=1):\n    \"\"\"\n    Calculate the n-th discrete difference along given axis.\n    The first difference is given by ``out[n] = a[n+1] - a[n]`` along\n    the given axis, higher differences are calculated by using `diff`\n    recursively.\n\n    :param a: The list to calculate the diff on\n    :param n: The order of the difference\n    :type a: list | tuple\n    :type n: int\n    :return: THe array of nth order differences\n    \"\"\"\n    if n == 0:\n        return a\n    if n < 0:\n        raise ValueError(\"order must be non-negative but got \" + repr(n))\n\n    b = map(lambda x: x[1] - x[0], zip(a[:-1], a[1:]))\n\n    if n > 1:\n        return diff(b, n-1)\n    return b"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef histogram(a, bins):\n    if any(map(lambda x: x < 0, diff(bins))):\n        raise ValueError(\n            'bins must increase monotonically.')\n\n    try:\n        sa = sorted(a)\n    except TypeError:\n        # Perhaps just a single value? Treat as a list and carry on\n        sa = sorted([a])\n\n    # import numpy as np\n    # nl = np.searchsorted(sa, bins[:-1], 'left')\n    # nr = np.searchsorted(sa, bins[-1], 'right')\n    # nn = np.r_[nl, nr]\n    #\n    # # cl = list(accumulate(Counter(map(lambda x: bisect_left(bins[:-1], x), sa)))\n    # # print(\"cl\")\n    # # print([cl[i] for i in range(len(bins))])\n    # print(\"nl\")\n    # print(list(nl))\n    # # print(Counter(map(lambda x: bisect_right([bins[-1]], x), sa)))\n    # print(\"nr\")\n    # print([nr])\n    # print(\"nn\")\n    # print(list(nn))\n    # print(\"hist\")\n    # print(list(np.diff(nn)))\n    # print(list(np.histogram(a, bins)[0]))\n\n    nl = list(accumulate([Counter(map(lambda x: bisect_left(bins[:-1], x), sa))[i] for i in range(len(bins) - 1)]))\n    # print(\"nl\")\n    # print(nl)\n    nr = Counter(map(lambda x: bisect_right([bins[1]], x), sa))[1]\n    # print(nl)\n    # print(nr)\n    n = list(nl) + [nr]\n\n    return diff(n), bins", "response": "Compute the histogram of a set of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef deprecation(self, message, *args, **kws):\n    self._log(DEPRECATION, message, args, **kws)", "response": "Show a deprecation warning."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows a message only once determined by position in source or identifer.", "response": "def once(self, message, *args, **kws):\n    \"\"\"Show a message only once, determined by position in source or identifer.\n\n    This will not work in IPython or Jupyter notebooks if no identifier is\n    specified, since then the determined position in source contains the\n    execution number of the input (cell), which changes every time.\n    Set a unique identifier, otherwise the message will be printed every\n    time.\n    \"\"\"\n    # TODO: after py2 support drop, put this into\n    # function signature: identifier=None (between *args and **kws)\n    identifier = kws.pop('identifier', None)\n\n    if identifier is None:\n        caller = getframeinfo(stack()[1][0])\n        identifier = \"%s:%d\" % (caller.filename, caller.lineno)\n    if not hasattr(self, 'once_dict'):\n        self.once_dict = {}\n    if identifier in self.once_dict:\n        return\n    self.once_dict[identifier] = True\n    self._log(ONCE, message, args, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_logger(name):\n    if name in loggers:\n        return loggers[name]\n    logger = logging.getLogger(name)\n    logger.propagate = False\n    pre1, suf1 = hash_coloured_escapes(name) if supports_color() else ('', '')\n    pre2, suf2 = hash_coloured_escapes(name + 'salt')  \\\n                 if supports_color() else ('', '')\n    formatter = logging.Formatter(\n        '%(levelname)s {}+{}+{} '\n        '%(name)s: %(message)s'.format(pre1, pre2, suf1)\n    )\n    ch = logging.StreamHandler()\n    ch.setFormatter(formatter)\n    logger.addHandler(ch)\n    loggers[name] = logger\n\n    logger.once_dict = {}\n\n    return logger", "response": "Helper function to get a logger"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_printer(name, color=None, ansi_code=None, force_color=False):\n\n    if force_color or supports_color():\n        if color is None and ansi_code is None:\n            cpre_1, csuf_1 = hash_coloured_escapes(name)\n            cpre_2, csuf_2 = hash_coloured_escapes(name + 'salt')\n            name = cpre_1 + '+' + cpre_2 + '+' + csuf_1 + ' ' + name\n        else:\n            name = colored(name, color=color, ansi_code=ansi_code)\n\n    prefix = name + ': '\n\n    def printer(text):\n        print(prefix + str(text))\n\n    return printer", "response": "Return a function which prints a message with a coloured name prefix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a ANSI coloured text based on its hash", "response": "def hash_coloured(text):\n    \"\"\"Return a ANSI coloured text based on its hash\"\"\"\n    ansi_code = int(sha256(text.encode('utf-8')).hexdigest(), 16) % 230\n    return colored(text, ansi_code=ansi_code)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the ANSI hash colour prefix and suffix for a given text", "response": "def hash_coloured_escapes(text):\n    \"\"\"Return the ANSI hash colour prefix and suffix for a given text\"\"\"\n    ansi_code = int(sha256(text.encode('utf-8')).hexdigest(), 16) % 230\n    prefix, suffix = colored('SPLIT', ansi_code=ansi_code).split('SPLIT')\n    return prefix, suffix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns current TAI timestamp.", "response": "def tai_timestamp():\n    \"\"\"Return current TAI timestamp.\"\"\"\n    timestamp = time.time()\n    date = datetime.utcfromtimestamp(timestamp)\n    if date.year < 1972:\n        return timestamp\n    offset = 10 + timestamp\n    leap_seconds = [\n        (1972, 1, 1),\n        (1972, 7, 1),\n        (1973, 1, 1),\n        (1974, 1, 1),\n        (1975, 1, 1),\n        (1976, 1, 1),\n        (1977, 1, 1),\n        (1978, 1, 1),\n        (1979, 1, 1),\n        (1980, 1, 1),\n        (1981, 7, 1),\n        (1982, 7, 1),\n        (1983, 7, 1),\n        (1985, 7, 1),\n        (1988, 1, 1),\n        (1990, 1, 1),\n        (1991, 1, 1),\n        (1992, 7, 1),\n        (1993, 7, 1),\n        (1994, 7, 1),\n        (1996, 1, 1),\n        (1997, 7, 1),\n        (1999, 1, 1),\n        (2006, 1, 1),\n        (2009, 1, 1),\n        (2012, 7, 1),\n        (2015, 7, 1),\n        (2017, 1, 1),\n    ]\n    for idx, leap_date in enumerate(leap_seconds):\n        if leap_date >= (date.year, date.month, date.day):\n            return idx - 1 + offset\n    return len(leap_seconds) - 1 + offset"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef np_to_datetime(intime):\n    nptime = np.atleast_1d(intime)\n    np_corr = (nptime - np.datetime64('1970-01-01T00:00:00')) / \\\n        np.timedelta64(1, 's')\n    return [datetime.utcfromtimestamp(t) for t in np_corr]", "response": "Convert numpy datetime64 to list[datetime ]."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gather_field_values(\n\titem, *, fields=None, field_map=FIELD_MAP,\n\tnormalize_values=False, normalize_func=normalize_value):\n\t\"\"\"Create a tuple of normalized metadata field values.\n\n\tParameter:\n\t\titem (~collections.abc.Mapping, str, os.PathLike): Item dict or filepath.\n\t\tfields (list): A list of fields used to compare item dicts.\n\t\tfield_map (~collections.abc.Mapping): A mapping field name aliases.\n\t\t\tDefault: :data:`~google_music_utils.constants.FIELD_MAP`\n\t\tnormalize_values (bool): Normalize metadata values to remove common differences between sources.\n\t\t\tDefault: ``False``\n\t\tnormalize_func (function): Function to apply to metadata values if\n\t\t\t``normalize_values`` is ``True``.\n\t\t\tDefault: :func:`~google_music_utils.utils.normalize_value`\n\n\tReturns:\n\t\ttuple: Values from the given metadata fields.\n\t\"\"\"\n\n\tit = get_item_tags(item)\n\n\tif fields is None:\n\t\tfields = list(it.keys())\n\n\tnormalize = normalize_func if normalize_values else lambda x: str(x)\n\n\tfield_values = []\n\n\tfor field in fields:\n\t\tfield_values.append(\n\t\t\tnormalize(\n\t\t\t\tlist_to_single_value(\n\t\t\t\t\tget_field(it, field, field_map=field_map)\n\t\t\t\t)\n\t\t\t)\n\t\t)\n\n\treturn tuple(field_values)", "response": "Create a tuple of normalized metadata field values from the given item dict or filepath."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_existing_items(\n\tsrc, dst, *, fields=None, field_map=None,\n\tnormalize_values=False, normalize_func=normalize_value):\n\t\"\"\"Find items from an item collection that are in another item collection.\n\n\tParameters:\n\t\tsrc (list): A list of item dicts or filepaths.\n\t\tdst (list): A list of item dicts or filepaths.\n\t\tfields (list): A list of fields used to compare item dicts.\n\t\tfield_map (~collections.abc.Mapping): A mapping field name aliases.\n\t\t\tDefault: :data:`~google_music_utils.constants.FIELD_MAP`\n\t\tnormalize_values (bool): Normalize metadata values to remove common differences between sources.\n\t\t\tDefault: ``False``\n\t\tnormalize_func (function): Function to apply to metadata values if\n\t\t\t``normalize_values`` is ``True``.\n\t\t\tDefault: :func:`~google_music_utils.utils.normalize_value`\n\n\tYields:\n\t\tdict: The next item from ``src`` collection in ``dst`` collection.\n\t\"\"\"\n\n\tif field_map is None:\n\t\tfield_map = FIELD_MAP\n\n\tdst_keys = {\n\t\t_gather_field_values(\n\t\t\tdst_item, fields=fields, field_map=field_map,\n\t\t\tnormalize_values=normalize_values, normalize_func=normalize_func\n\t\t) for dst_item in dst\n\t}\n\n\tfor src_item in src:\n\t\tif _gather_field_values(\n\t\t\tsrc_item, fields=fields, field_map=field_map,\n\t\t\tnormalize_values=normalize_values, normalize_func=normalize_func\n\t\t) in dst_keys:\n\t\t\tyield src_item", "response": "Find items from an item collection that are in another item collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a monitoring logger to the base object that will be monitored.", "response": "def monitor(self, message, *args, **kws):\n    \"\"\"\n    Define a monitoring logger that will be added to Logger\n\n    :param self: The logging object\n    :param message: The logging message\n    :param args: Positional arguments\n    :param kws: Keyword arguments\n    :return:\n    \"\"\"\n    if self.isEnabledFor(MON):\n        # Yes, logger takes its '*args' as 'args'.\n        self._log(MON, message, args, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef monitor(msg, *args, **kwargs):\n    if len(logging.root.handlers) == 0:\n        logging.basicConfig()\n    logging.root.monitor(msg, *args, **kwargs)", "response": "Log a message with severity MON on the root logger."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _exception_converter(callable: Callable) -> Callable:\n    def wrapped(*args, **kwargs) -> Any:\n        try:\n            return callable(*args, **kwargs)\n        except ConsulLockBaseError as e:\n            raise e\n        except ACLPermissionDenied as e:\n            raise PermissionDeniedConsulError() from e\n        except ConsulException as e:\n            raise ConsulConnectionError() from e\n    return wrapped", "response": "Decorator that converts exceptions from underlying libraries to native exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _raise_if_teardown_called(callable: Callable) -> Callable:\n    def wrapper(consul_lock: \"ConsulLockManager\", *args, **kwargs):\n        if consul_lock._teardown_called:\n            raise UnusableStateError(\"Teardown has been called on the lock manager\")\n        return callable(consul_lock, *args, **kwargs)\n    return wrapper", "response": "Decorator that raises an exception if the consul lock manager has been torn down."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_session_ttl(session_timeout_in_seconds: float):\n        if session_timeout_in_seconds < MIN_LOCK_TIMEOUT_IN_SECONDS \\\n                or session_timeout_in_seconds > MAX_LOCK_TIMEOUT_IN_SECONDS:\n            raise InvalidSessionTtlValueError(\n                f\"Invalid session timeout: {session_timeout_in_seconds}. If defined, the timeout must be between \"\n                f\"{MIN_LOCK_TIMEOUT_IN_SECONDS} and {MAX_LOCK_TIMEOUT_IN_SECONDS} (inclusive).\")", "response": "Validates the given session timeout in seconds."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the given key.", "response": "def validate_key(key: str):\n        \"\"\"\n        Validates the given key.\n        :param key: the key to validate\n        :raises InvalidKeyError: raised if the given key is invalid\n        \"\"\"\n        if \"//\" in key:\n            raise DoubleSlashKeyError(key)\n        elif normpath(key) != key:\n            raise NonNormalisedKeyError(key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef acquire(self, key: str, blocking: bool=True, timeout: float=None, metadata: Any=None,\n                on_before_lock: LockEventListener=lambda key: None,\n                on_lock_already_locked: LockEventListener=lambda key: None,\n                lock_poll_interval_generator: Callable[[int], float]=DEFAULT_LOCK_POLL_INTERVAL_GENERATOR) \\\n            -> Optional[ConnectedConsulLockInformation]:\n        \"\"\"\n        Acquires a Consul lock.\n        :param key: the lock key\n        :param blocking: whether to block and wait for the lock\n        :param timeout: timeout in seconds\n        :param metadata: metadata to add to the lock information. Must be parsable by default JSON encode/decoder\n        :param on_before_lock: event listener to be called before attempt to acquire lock\n        :param on_lock_already_locked: event listener to be called when an attempt to acquire a lock has failed as the\n        lock is already locked\n        :param lock_poll_interval_generator: generator of the interval between Consul lock polls where the first\n        argument is the attempt number (starting at 1)\n        :return: information about the lock if acquired, else `None` if not acquired and not blocking\n        :raises InvalidKeyError: raised if the given key is not valid\n        :raises LockAcquireTimeoutError: raised if times out waiting for the lock\n        \"\"\"\n        ConsulLockManager.validate_key(key)\n\n        logger.debug(\"Creating Consul session...\")\n        session_id = self.consul_client.session.create(\n            lock_delay=0, ttl=self.session_ttl_in_seconds, behavior=\"delete\")\n        self._acquiring_session_ids.add(session_id)\n        logger.info(f\"Created session with ID: {session_id}\")\n        start_time = monotonic()\n\n        @timeout_decorator.timeout(timeout, timeout_exception=LockAcquireTimeoutError)\n        def _acquire() -> Optional[ConsulLockInformation]:\n            i = 1\n            while True:\n                logger.debug(\"Going to acquire lock\")\n                seconds_to_lock = monotonic() - start_time\n                on_before_lock(key)\n                lock_information = self._acquire_lock(key, session_id, seconds_to_lock, metadata)\n                if lock_information is not None:\n                    logger.debug(\"Acquired lock!\")\n                    return lock_information\n                else:\n                    on_lock_already_locked(key)\n                    if not blocking:\n                        logger.debug(\"Could not acquire lock (already locked) and not blocking\")\n                        return None\n                    else:\n                        logger.debug(\"Could not acquire lock (already locked)\")\n                interval = lock_poll_interval_generator(i)\n                logger.debug(f\"Sleeping for {interval}s\")\n                sleep(interval)\n                i += 1\n\n        lock_information = _acquire()\n        self._acquiring_session_ids.remove(session_id)\n\n        if lock_information is None:\n            self.consul_client.session.destroy(session_id=session_id)\n            logger.info(f\"Destroyed session (did not acquire the lock)\")\n\n        return lock_information", "response": "Acquires a Consul lock."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef execute(self, key: str, executable: str, *, capture_stdout: bool=False, capture_stderr: bool=False,\n                **acquire_kwargs) -> Tuple[Optional[int], Optional[bytes], Optional[bytes]]:\n        \"\"\"\n        Executes the given executable whilst holding the given lock.\n        :param key:\n        :param executable:\n        :param capture_stdout:\n        :param capture_stderr:\n        :param acquire_kwargs:\n        :return:\n        \"\"\"\n        lock = self.acquire(key, **acquire_kwargs)\n        if lock is None:\n            return None, None, None\n        return self.execute_with_lock(executable, lock, capture_stdout=capture_stdout, capture_stderr=capture_stderr)", "response": "Executes the given executable whilst holding the given lock."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef execute_with_lock(self, executable: str, lock: ConnectedConsulLockInformation, *, capture_stdout: bool=False,\n                          capture_stderr: bool=False) -> Tuple[int, Optional[bytes], Optional[bytes]]:\n        \"\"\"\n        TODO\n        :param executable:\n        :param lock:\n        :param capture_stdout:\n        :param capture_stderr:\n        :return:\n        \"\"\"\n        assert lock is not None\n        redirects = Munch(stdout=subprocess.PIPE if capture_stdout else sys.stdout,\n                         stderr=subprocess.PIPE if capture_stderr else sys.stderr)\n\n        # Patch for when sys.stdout and sys.stderr have been reassigned (e.g. in IDE test runners)\n        non_realtime_redirects: Dict[str, StringIO] = {}\n        for name, redirect in redirects.items():\n            if isinstance(redirect, StringIO):\n                logger.warning(f\"Cannot capture {name} in real-time as `sys.{name}` does not have a fileno\")\n                non_realtime_redirects[name] = redirect\n                redirects[name] = subprocess.PIPE\n\n        outputs = Munch(stdout=None, stderr=None)\n        with lock:\n            process = subprocess.Popen(executable, shell=True, stdout=redirects.stdout, stderr=redirects.stderr)\n            outputs.stdout, outputs.stderr = process.communicate()\n\n        # Second part of redirect reassignment patch\n        for name, original_redirect in non_realtime_redirects.items():\n            captured = outputs[name]\n            getattr(sys, name).write(captured.decode(\"utf-8\"))\n\n        return process.returncode, \\\n               outputs.stdout if capture_stdout else None, \\\n               outputs.stderr if capture_stderr else None", "response": "Execute a command with a lock."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef release(self, key: str) -> Optional[str]:\n        ConsulLockManager.validate_key(key)\n\n        key_value = self.consul_client.kv.get(key)[1]\n        if key_value is None:\n            logger.info(f\"No lock found\")\n            return None\n\n        lock_information = json.loads(key_value[\"Value\"].decode(\"utf-8\"), cls=ConsulLockInformationJSONDecoder)\n        logger.info(f\"Destroying the session {lock_information.session_id} that is holding the lock\")\n        unlocked = self.consul_client.session.destroy(session_id=lock_information.session_id)\n        # This instance might be managing the removed session\n        try:\n            self._acquiring_session_ids.remove(lock_information.session_id)\n        except KeyError:\n            pass\n\n        logger.info(\"Unlocked\" if unlocked else \"Went to unlock but was already released upon sending request\")\n        return key", "response": "Release the lock.\n\n        Noop if not locked.\n        :param key: the name of the lock to release\n        :return: the name of the released lock, or `None` if no lock was released\n        :raises InvalidKeyError: if the `key` is not valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreleases all locks with names that match the given regular expression.", "response": "def release_regex(self, key_regex: str) -> Set[str]:\n        \"\"\"\n        Releases all locks with names that that match the given regular expression.\n        :param key_regex: Python regular expression that captures the names of the locks that are to be released\n        :return: the names of the locks that were released\n        \"\"\"\n        keys = list(self.find_regex(key_regex).keys())\n        return self.release_all(keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreleases all of the given keys.", "response": "def release_all(self, keys: Sequence[str]) -> Set[str]:\n        \"\"\"\n        Releases all of the given keys.\n        :param keys: the keys to release\n        :return: the names of the keys that were released\n        \"\"\"\n        released: List[str] = []\n        for key in keys:\n            released.append(self.release(key))\n        return set(filter(None,released))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find(self, name: str) -> Optional[ConnectedConsulLockInformation]:\n        lock = self.consul_client.kv.get(name)[1]\n        if lock is None:\n            return None\n\n        lock_information = json.loads(lock[\"Value\"], cls=ConsulLockInformationJSONDecoder)\n        return ConnectedConsulLockInformation(\n            self, lock_information.key, lock_information.session_id, lock_information.created,\n            lock_information.seconds_to_lock, lock_information.metadata)", "response": "Finds the lock with the given name that matches that given."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the keys that match the given regular expression.", "response": "def find_regex(self, name_regex: str) -> Dict[str, Optional[ConnectedConsulLockInformation]]:\n        \"\"\"\n        Finds the locks with key names that match the given regex.\n        :param name_regex: key name regex\n        :return: keys that match\n        \"\"\"\n        # Gets prefix directory (must not include regex!)\n        escaped_name_regex = re.escape(name_regex)\n        directory_prefix = os.path.commonprefix(\n            (name_regex.replace(KEY_DIRECTORY_SEPARATOR, re.escape(KEY_DIRECTORY_SEPARATOR)), escaped_name_regex)) \\\n            .replace(\"\\\\\", \"\")\n\n        data = self.consul_client.kv.get(directory_prefix, recurse=True)[1]\n        if data is None:\n            return dict()\n        key_indexed_data = {key_data[\"Key\"]: key_data for key_data in data}\n\n        name_pattern = re.compile(name_regex)\n        matches = [value for key, value in key_indexed_data.items() if name_pattern.fullmatch(key) is not None]\n        matched_return: Dict[str, Optional[ConsulLockInformation]] = dict()\n        for match in matches:\n            try:\n                decoded_match = json.loads(match[\"Value\"], cls=ConsulLockInformationJSONDecoder)\n                matched_return[decoded_match.key] = decoded_match\n            except JSONDecodeError:\n                matched_return[match[\"Key\"]] = None\n        return matched_return"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntears down the instance removing any remaining sessions that have not been acquired.", "response": "def teardown(self):\n        \"\"\"\n        Tears down the instance, removing any remaining sessions that this instance has created.\n\n        The instance must not be used after this method has been called.\n        \"\"\"\n        with self._teardown_lock:\n            if not self._teardown_called:\n                self._teardown_called = True\n                if len(self._acquiring_session_ids) > 0:\n                    logger.info(\n                        f\"Destroying all sessions that have not acquired keys: {self._acquiring_session_ids}...\")\n                for session_id in self._acquiring_session_ids:\n                    try:\n                        self.consul_client.session.destroy(session_id=session_id)\n                        logger.debug(f\"Destroyed: {session_id}\")\n                    except requests.exceptions.ConnectionError as e:\n                        logger.debug(f\"Exception: {e}\")\n                        logger.warning(f\"Could not connect to Consul to clean up session {session_id}\")\n                atexit.unregister(self.teardown)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _acquire_lock(self, key: str, session_id: str, seconds_to_lock: float, metadata: Any) \\\n            -> Optional[ConnectedConsulLockInformation]:\n        \"\"\"\n        Attempts to get the lock using the given session.\n        :param key: name of the lock\n        :param session_id: the identifier of the Consul session that should try to hold the lock\n        :param seconds_to_lock: the number of seconds it took to acquire the lock\n        :param metadata: metadata to add to the lock information\n        :return: details about the lock if acquired, else `None`\n        :raises SessionLostConsulError: if the Consul session is lost\n        \"\"\"\n        lock_information = ConnectedConsulLockInformation(\n            self, key, session_id, datetime.utcnow(), seconds_to_lock, metadata)\n        value = json.dumps(lock_information, cls=ConsulLockInformationJSONEncoder, indent=4, sort_keys=True)\n        logger.debug(f\"Attempting to acquire lock with value: {value}\")\n        try:\n            success = self.consul_client.kv.put(key=key, value=value, acquire=session_id)\n        except ConsulException as e:\n            if \"invalid session\" in e.args[0]:\n                raise SessionLostConsulError() from e\n            raise e\n        return lock_information if success else None", "response": "Attempts to get the lock using the given session."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming an advanced query and return all matching entries.", "response": "def _aggregate(self, scroll_field, scroll_size=SEARCH_LIMIT):\n        \"\"\"Perform an advanced query, and return *all* matching results.\n        Will automatically perform multiple queries in order to retrieve all results.\n\n        Note: All ``aggregate`` queries run in advanced mode.\n\n        Arguments:\n            scroll_field (str): The field on which to scroll. This should be a field\n                    that counts/indexes the entries.\n            scroll_size (int): Maximum number of records returned per query. Must be\n                    between one and the ``SEARCH_LIMIT`` (inclusive).\n                    **Default:** ``SEARCH_LIMIT``.\n\n        Returns:\n            list of dict: All matching entries.\n        \"\"\"\n        # Make sure scroll_field is valid\n        if not scroll_field:\n            raise AttributeError(\"scroll_field is required.\")\n\n        # Make sure the query is set\n        if not self.initialized:\n            raise AttributeError('No query has been set.')\n\n        # Warn the user if we are changing the setting of advanced\n        if not self._SearchHelper__query[\"advanced\"]:\n            warnings.warn('This query will be run in advanced mode.', RuntimeWarning)\n            self._SearchHelper__query[\"advanced\"] = True\n\n        # Inform the user if they set an invalid value for the query size\n        if scroll_size <= 0:\n            raise AttributeError('Scroll size must greater than zero')\n\n        # Get the total number of records\n        total = self.search(limit=0, info=True, reset_query=False)[1][\"total_query_matches\"]\n\n        # If aggregate is unnecessary, use Search automatically instead\n        if total <= SEARCH_LIMIT:\n            return self.search(limit=SEARCH_LIMIT, reset_query=False)\n\n        # Scroll until all results are found\n        output = []\n\n        scroll_pos = 0\n        while len(output) < total:\n\n            # Scroll until the width is small enough to get all records\n            #   `scroll_id`s are unique to each dataset. If multiple datasets\n            #   match a certain query, the total number of matching records\n            #   may exceed the maximum that search will return - even if the\n            #   scroll width is much smaller than that maximum\n            scroll_width = scroll_size\n            while True:\n                query = \"({q}) AND ({field}:>={start} AND {field}:<{end})\".format(\n                        q=self._SearchHelper__query[\"q\"], field=scroll_field, start=scroll_pos,\n                        end=scroll_pos+scroll_width)\n\n                results, info = self.search(q=query, advanced=True, info=True)\n\n                # Check to make sure that all the matching records were returned\n                if info[\"total_query_matches\"] <= len(results):\n                    break\n\n                # If not, reduce the scroll width\n                # new_width is proportional with the proportion of results returned\n                new_width = scroll_width * (len(results) // info[\"total_query_matches\"])\n\n                # scroll_width should never be 0, and should only be 1 in rare circumstances\n                scroll_width = new_width if new_width > 1 else max(scroll_width//2, 1)\n\n            # Append the results to the output\n            output.extend(results)\n            scroll_pos += scroll_width\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming an aggregate query and return all matching results.", "response": "def aggregate(self, q=None, scroll_size=SEARCH_LIMIT, reset_query=True, **kwargs):\n        \"\"\"Perform an advanced query, and return *all* matching results.\n        Will automatically perform multiple queries in order to retrieve all results.\n\n        Note:\n            All ``aggregate`` queries run in advanced mode, and ``info`` is not available.\n\n        Arguments:\n            q (str): The query to execute. **Default:** The current helper-formed query, if any.\n                    There must be some query to execute.\n            scroll_size (int): Maximum number of records returned per query. Must be\n                    between one and the ``SEARCH_LIMIT`` (inclusive).\n                    **Default:** ``SEARCH_LIMIT``.\n            reset_query (bool): If ``True``, will destroy the current query after execution\n                    and start a fresh one.\n                    If ``False``, will keep the current query set.\n                    **Default:** ``True``.\n\n        Keyword Arguments:\n            scroll_field (str): The field on which to scroll. This should be a field\n                    that counts/indexes the entries.\n                    This should be set in ``self.scroll_field``, but if your application\n                    requires separate scroll fields for a single client,\n                    it can be set in this way as well.\n                    **Default**: ``self.scroll_field``.\n\n        Returns:\n            list of dict: All matching records.\n        \"\"\"\n        scroll_field = kwargs.get(\"scroll_field\", self.scroll_field)\n\n        # If q not specified, use internal, helper-built query\n        if q is None:\n            res = self._aggregate(scroll_field=scroll_field, scroll_size=scroll_size)\n            if reset_query:\n                self.reset_query()\n            return res\n        # Otherwise, run an independent query as SearchHelper.search() does.\n        else:\n            return self.__class__(index=self.index, q=q, advanced=True,\n                                  search_client=self._SearchHelper__search_client\n                                  ).aggregate(scroll_size=scroll_size, reset_query=reset_query)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if we are on a Lyon machine", "response": "def we_are_in_lyon():\n    \"\"\"Check if we are on a Lyon machine\"\"\"\n    import socket\n    try:\n        hostname = socket.gethostname()\n        ip = socket.gethostbyname(hostname)\n    except socket.gaierror:\n        return False\n    return ip.startswith(\"134.158.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a DataFrame from CSV text", "response": "def read_csv(text, sep=\"\\t\"):\n    \"\"\"Create a DataFrame from CSV text\"\"\"\n    import pandas as pd    # no top level load to make a faster import of db\n    return pd.read_csv(StringIO(text), sep=\"\\t\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_datetime(dataframe, timestamp_key='UNIXTIME'):\n\n    def convert_data(timestamp):\n        return datetime.fromtimestamp(float(timestamp) / 1e3, UTC_TZ)\n\n    try:\n        log.debug(\"Adding DATETIME column to the data\")\n        converted = dataframe[timestamp_key].apply(convert_data)\n        dataframe['DATETIME'] = converted\n    except KeyError:\n        log.warning(\"Could not add DATETIME column\")", "response": "Add an additional DATETIME column with standar datetime format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow AHRS calibration data for given clb_upi.", "response": "def show_ahrs_calibration(clb_upi, version='3'):\n    \"\"\"Show AHRS calibration data for given `clb_upi`.\"\"\"\n    db = DBManager()\n    ahrs_upi = clbupi2ahrsupi(clb_upi)\n    print(\"AHRS UPI: {}\".format(ahrs_upi))\n    content = db._get_content(\"show_product_test.htm?upi={0}&\"\n                              \"testtype=AHRS-CALIBRATION-v{1}&n=1&out=xml\"\n                              .format(ahrs_upi, version)) \\\n        .replace('\\n', '')\n\n    import xml.etree.ElementTree as ET\n\n    try:\n        root = ET.parse(io.StringIO(content)).getroot()\n    except ET.ParseError:\n        print(\"No calibration data found\")\n    else:\n        for child in root:\n            print(\"{}: {}\".format(child.tag, child.text))\n        names = [c.text for c in root.findall(\".//Name\")]\n        values = [[i.text for i in c] for c in root.findall(\".//Values\")]\n        for name, value in zip(names, values):\n            print(\"{}: {}\".format(name, value))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef datalog(self, parameter, run, maxrun=None, det_id='D_ARCA001'):\n        \"Retrieve datalogs for given parameter, run(s) and detector\"\n        parameter = parameter.lower()\n        if maxrun is None:\n            maxrun = run\n        with Timer('Database lookup'):\n            return self._datalog(parameter, run, maxrun, det_id)", "response": "Retrieve datalogs for given parameter run ( s ) and detector"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nextracting data from database", "response": "def _datalog(self, parameter, run, maxrun, det_id):\n        \"Extract data from database\"\n        values = {\n            'parameter_name': parameter,\n            'minrun': run,\n            'maxrun': maxrun,\n            'detid': det_id,\n        }\n        data = urlencode(values)\n        content = self._get_content('streamds/datalognumbers.txt?' + data)\n        if content.startswith('ERROR'):\n            log.error(content)\n            return None\n        try:\n            dataframe = read_csv(content)\n        except ValueError:\n            log.warning(\n                \"Empty dataset\"\n            )    # ...probably. Waiting for more info\n            return make_empty_dataset()\n        else:\n            add_datetime(dataframe)\n            try:\n                self._add_converted_units(dataframe, parameter)\n            except KeyError:\n                log.warning(\n                    \"Could not add converted units for {0}\".format(parameter)\n                )\n            return dataframe"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_converted_units(self, dataframe, parameter, key='VALUE'):\n        convert_unit = self.parameters.get_converter(parameter)\n        try:\n            log.debug(\"Adding unit converted DATA_VALUE to the data\")\n            dataframe[key] = dataframe['DATA_VALUE'].apply(convert_unit)\n        except KeyError:\n            log.warning(\"Missing 'VALUE': no unit conversion.\")\n        else:\n            dataframe.unit = self.parameters.unit(parameter)", "response": "Add an additional DATA_VALUE column with converted VALUEs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting detector string representation ( OID to serialnumber", "response": "def get_det_id(self, det_oid):\n        \"\"\"Convert detector string representation (OID) to serialnumber\"\"\"\n        try:\n            return self.detectors[self.detectors.OID == det_oid\n                                  ].SERIALNUMBER.iloc[0]\n        except IndexError:\n            log.critical(\"No det ID found for OID '{}'\".format(det_oid))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts detector serialnumber to string representation ( OID", "response": "def get_det_oid(self, det_id):\n        \"\"\"Convert detector serialnumber to string representation (OID)\"\"\"\n        try:\n            return self.detectors[self.detectors.SERIALNUMBER == det_id\n                                  ].OID.iloc[0]\n        except IndexError:\n            log.critical(\"No OID found for det ID '{}'\".format(det_id))\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting det ID or OID to det ID.", "response": "def to_det_id(self, det_id_or_det_oid):\n        \"\"\"Convert det ID or OID to det ID\"\"\"\n        try:\n            int(det_id_or_det_oid)\n        except ValueError:\n            return self.get_det_id(det_id_or_det_oid)\n        else:\n            return det_id_or_det_oid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting det OID or ID to det OID", "response": "def to_det_oid(self, det_id_or_det_oid):\n        \"\"\"Convert det OID or ID to det OID\"\"\"\n        try:\n            int(det_id_or_det_oid)\n        except ValueError:\n            return det_id_or_det_oid\n        else:\n            return self.get_det_oid(det_id_or_det_oid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_parameters(self):\n        \"Retrieve a list of available parameters from the database\"\n        parameters = self._get_json('allparam/s')\n        data = {}\n        for parameter in parameters:    # There is a case-chaos in the DB\n            data[parameter['Name'].lower()] = parameter\n        self._parameters = ParametersContainer(data)", "response": "Retrieve a list of available parameters from the database"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the trigger setup for a given runsetup OID", "response": "def trigger_setup(self, runsetup_oid):\n        \"Retrieve the trigger setup for a given runsetup OID\"\n        r = self._get_content(\n            \"jsonds/rslite/s?rs_oid={}&upifilter=1.1.2.2.3/*\".\n            format(runsetup_oid)\n        )\n        data = json.loads(r)['Data']\n        if not data:\n            log.error(\"Empty dataset.\")\n            return\n        raw_setup = data[0]\n        det_id = raw_setup['DetID']\n        name = raw_setup['Name']\n        description = raw_setup['Desc']\n\n        _optical_df = raw_setup['ConfGroups'][0]\n        optical_df = {'Name': _optical_df['Name'], 'Desc': _optical_df['Desc']}\n        for param in _optical_df['Params']:\n            pname = self.parameters.oid2name(param['OID']).replace('DAQ_', '')\n            try:\n                dtype = float if '.' in param['Val'] else int\n                val = dtype(param['Val'])\n            except ValueError:\n                val = param['Val']\n            optical_df[pname] = val\n\n        _acoustic_df = raw_setup['ConfGroups'][1]\n        acoustic_df = {\n            'Name': _acoustic_df['Name'],\n            'Desc': _acoustic_df['Desc']\n        }\n        for param in _acoustic_df['Params']:\n            pname = self.parameters.oid2name(param['OID']).replace('DAQ_', '')\n            try:\n                dtype = float if '.' in param['Val'] else int\n                val = dtype(param['Val'])\n            except ValueError:\n                val = param['Val']\n            acoustic_df[pname] = val\n\n        return TriggerSetup(\n            runsetup_oid, name, det_id, description, optical_df, acoustic_df\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef detx(self, det_id, t0set=None, calibration=None):\n        url = 'detx/{0}?'.format(det_id)    # '?' since it's ignored if no args\n        if t0set is not None:\n            url += '&t0set=' + t0set\n        if calibration is not None:\n            url += '&calibrid=' + calibration\n\n        detx = self._get_content(url)\n        return detx", "response": "Retrieve the detector file for given detector id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nretrieve AHRS values for given run ( s ) ( optionally CLBs and detector", "response": "def ahrs(self, run, maxrun=None, clbupi=None, det_id='D_ARCA001'):\n        \"Retrieve AHRS values for given run(s) (optionally CLBs) and detector\"\n        if maxrun is None:\n            maxrun = run\n        with Timer('Database lookup'):\n            return self._ahrs(run, maxrun, clbupi, det_id)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget JSON - type content", "response": "def _get_json(self, url):\n        \"Get JSON-type content\"\n        content = self._get_content('jsonds/' + url)\n        try:\n            json_content = json.loads(content.decode())\n        except AttributeError:\n            json_content = json.loads(content)\n        if json_content['Comment']:\n            log.warning(json_content['Comment'])\n        if json_content['Result'] != 'OK':\n            raise ValueError('Error while retrieving the parameter list.')\n        return json_content['Data']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef opener(self):\n        \"A reusable connection manager\"\n        if self._opener is None:\n            log.debug(\"Creating connection handler\")\n            opener = build_opener()\n            if self._cookies:\n                log.debug(\"Appending cookies\")\n            else:\n                log.debug(\"No cookies to append\")\n            for cookie in self._cookies:\n                cookie_str = cookie.name + '=' + cookie.value\n                opener.addheaders.append(('Cookie', cookie_str))\n            self._opener = opener\n        else:\n            log.debug(\"Reusing connection manager\")\n        return self._opener", "response": "A reusable connection manager"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request_sid_cookie(self, username, password):\n        log.debug(\"Requesting SID cookie\")\n        target_url = self._login_url + '?usr={0}&pwd={1}&persist=y'.format(\n            username, password\n        )\n        cookie = urlopen(target_url).read()\n        return cookie", "response": "Request a cookie for permanent session token."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restore_session(self, cookie):\n        log.debug(\"Restoring session from cookie: {}\".format(cookie))\n        opener = build_opener()\n        opener.addheaders.append(('Cookie', cookie))\n        self._opener = opener", "response": "Establish databse connection using permanent session cookie"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlogin to the database and store cookies for upcoming requests.", "response": "def login(self, username, password):\n        \"Login to the database and store cookies for upcoming requests.\"\n        log.debug(\"Logging in to the DB\")\n        opener = self._build_opener()\n        values = {'usr': username, 'pwd': password}\n        req = self._make_request(self._login_url, values)\n        try:\n            log.debug(\"Sending login request\")\n            f = opener.open(req)\n        except URLError as e:\n            log.error(\"Failed to connect to the database -> probably down!\")\n            log.error(\"Error from database server:\\n    {0}\".format(e))\n            return False\n        html = f.read()\n        failed_auth_message = 'Bad username or password'\n        if failed_auth_message in str(html):\n            log.error(failed_auth_message)\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the list of available straems", "response": "def _update_streams(self):\n        \"\"\"Update the list of available straems\"\"\"\n        content = self._db._get_content(\"streamds\")\n        self._stream_df = read_csv(content).sort_values(\"STREAM\")\n        self._streams = None\n        for stream in self.streams:\n            setattr(self, stream, self.__getattr__(stream))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nshows the help for a given stream.", "response": "def help(self, stream):\n        \"\"\"Show the help for a given stream.\"\"\"\n        if stream not in self.streams:\n            log.error(\"Stream '{}' not found in the database.\".format(stream))\n        params = self._stream_df[self._stream_df['STREAM'] == stream].values[0]\n        self._print_stream_parameters(params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _print_stream_parameters(self, values):\n        cprint(\"{0}\".format(*values), \"magenta\", attrs=[\"bold\"])\n        print(\"{4}\".format(*values))\n        cprint(\"  available formats:   {1}\".format(*values), \"blue\")\n        cprint(\"  mandatory selectors: {2}\".format(*values), \"red\")\n        cprint(\"  optional selectors:  {3}\".format(*values), \"green\")\n        print()", "response": "Print a coloured help for a given tuple of stream parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, stream, fmt='txt', **kwargs):\n        sel = ''.join([\"&{0}={1}\".format(k, v) for (k, v) in kwargs.items()])\n        url = \"streamds/{0}.{1}?{2}\".format(stream, fmt, sel[1:])\n        data = self._db._get_content(url)\n        if not data:\n            log.error(\"No data found at URL '%s'.\" % url)\n            return\n        if (data.startswith(\"ERROR\")):\n            log.error(data)\n            return\n        if fmt == \"txt\":\n            return read_csv(data)\n        return data", "response": "Get the data for a given stream manually"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict for given parameter", "response": "def get_parameter(self, parameter):\n        \"Return a dict for given parameter\"\n        parameter = self._get_parameter_name(parameter)\n        return self._parameters[parameter]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating unit conversion function for given parameter", "response": "def get_converter(self, parameter):\n        \"\"\"Generate unit conversion function for given parameter\"\"\"\n        if parameter not in self._converters:\n            param = self.get_parameter(parameter)\n            try:\n                scale = float(param['Scale'])\n            except KeyError:\n                scale = 1\n\n            def convert(value):\n                # easy_scale = float(param['EasyScale'])\n                # easy_scale_multiplier = float(param['EasyScaleMultiplier'])\n                return value * scale\n\n            return convert"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the unit for given parameter", "response": "def unit(self, parameter):\n        \"Get the unit for given parameter\"\n        parameter = self._get_parameter_name(parameter).lower()\n        return self._parameters[parameter]['Unit']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef oid2name(self, oid):\n        \"Look up the parameter name for a given OID\"\n        if not self._oid_lookup:\n            for name, data in self._parameters.items():\n                self._oid_lookup[data['OID']] = data['Name']\n        return self._oid_lookup[oid]", "response": "Look up the parameter name for a given OID"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef via_omkey(self, omkey, det_id):\n        du, floor = omkey\n        try:\n            return DOM.from_json([\n                d for d in self._json if d[\"DU\"] == du and d[\"Floor\"] == floor\n                and d[\"DetOID\"] == det_id\n            ][0])\n        except IndexError:\n            log.critical(\n                \"No DOM found for OMKey '{0}' and DetOID '{1}'.\".format(\n                    omkey, det_id\n                )\n            )", "response": "Return a DOM object for given OMKey and DetOID."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a DOM object for given dom_id and det_id", "response": "def via_dom_id(self, dom_id, det_id):\n        \"\"\"Return DOM for given dom_id\"\"\"\n        try:\n            return DOM.from_json([\n                d for d in self._json\n                if d[\"DOMId\"] == dom_id and d[\"DetOID\"] == det_id\n            ][0])\n        except IndexError:\n            log.critical(\"No DOM found for DOM ID '{0}'\".format(dom_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn DOM for given CLB UPI", "response": "def via_clb_upi(self, clb_upi, det_id):\n        \"\"\"return DOM for given CLB UPI\"\"\"\n        try:\n            return DOM.from_json([\n                d for d in self._json\n                if d[\"CLBUPI\"] == clb_upi and d[\"DetOID\"] == det_id\n            ][0])\n        except IndexError:\n            log.critical(\"No DOM found for CLB UPI '{0}'\".format(clb_upi))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef upi(self):\n        parameter = 'UPI'\n        if parameter not in self._by:\n            self._populate(by=parameter)\n        return self._by[parameter]", "response": "A dict of CLBs with UPI as key"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the base CLB for a given DU", "response": "def base(self, du):\n        \"\"\"Return the base CLB for a given DU\"\"\"\n        parameter = 'base'\n        if parameter not in self._by:\n            self._by[parameter] = {}\n            for clb in self.upi.values():\n                if clb.floor == 0:\n                    self._by[parameter][clb.du] = clb\n        return self._by[parameter][du]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the results for a given stream", "response": "def get_results(self, stream, time_interval):\n        \"\"\"\n        Get the results for a given stream\n\n        :param time_interval: The time interval\n        :param stream: The stream object\n        :return: A generator over stream instances\n        \"\"\"\n        query = stream.stream_id.as_raw()\n        query['datetime'] = {'$gt': time_interval.start, '$lte': time_interval.end}\n        with switch_db(StreamInstanceModel, 'hyperstream'):\n            for instance in StreamInstanceModel.objects(__raw__=query):\n                yield StreamInstance(timestamp=instance.datetime, value=instance.value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npurging the given stream.", "response": "def purge_stream(self, stream_id, remove_definition=False, sandbox=None):\n        \"\"\"\n        Purge the stream\n\n        :param stream_id: The stream identifier\n        :param remove_definition: Whether to remove the stream definition as well\n        :param sandbox: The sandbox for this stream\n        :return: None\n        :raises: NotImplementedError\n        \"\"\"\n        # TODO: Add time interval to this\n\n        if sandbox is not None:\n            raise NotImplementedError\n\n        if stream_id not in self.streams:\n            raise StreamNotFoundError(\"Stream with id '{}' not found\".format(stream_id))\n\n        stream = self.streams[stream_id]\n        query = stream_id.as_raw()\n        with switch_db(StreamInstanceModel, 'hyperstream'):\n            StreamInstanceModel.objects(__raw__=query).delete()\n\n        # Also update the stream status\n        stream.calculated_intervals = TimeIntervals([])\n\n        if remove_definition:\n            with switch_db(StreamDefinitionModel, 'hyperstream'):\n                StreamDefinitionModel.objects(__raw__=query).delete()\n\n        logging.info(\"Purged stream {}\".format(stream_id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_stream_writer(self, stream):\n\n        def writer(document_collection):\n            with switch_db(StreamInstanceModel, 'hyperstream'):\n                if isinstance(document_collection, StreamInstance):\n                    document_collection = [document_collection]\n\n                for t, doc in document_collection:\n                    instance = StreamInstanceModel(\n                        stream_id=stream.stream_id.as_dict(),\n                        datetime=t,\n                        value=doc)\n                    try:\n                        instance.save()\n                    except NotUniqueError as e:\n                        # Implies that this has already been written to the database\n                        # Raise an error if the value differs from that in the database\n                        logging.warn(\"Found duplicate document: {}\".format(e.message))\n                        existing = StreamInstanceModel.objects(stream_id=stream.stream_id.as_dict(), datetime=t)[0]\n                        if existing.value != doc:\n                            raise e\n                    except (InvalidDocumentError, InvalidDocument) as e:\n                        # Something wrong with the document - log the error\n                        logging.error(e)\n        return writer", "response": "Returns the function that creates a new stream instance with the given stream_id and datetime pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears all inactive and active sessions and their history edition of the current session.", "response": "def clear_sessions(hyperstream, inactive_only=True, clear_history=False):\n        \"\"\"\n        Clear all (inactive) sessions and (optionally) their history\n        \n        :param hyperstream: The hyperstream object\n        :param inactive_only: Whether to only clear inactive sessions (active sessions may be owned by another process)\n        :param clear_history: Whether to clear the history of the session. Note that this will only clear the history \n        if the creator is the same process: there could feasibly be a history stored in a file channel that is not \n         accessible by this process.\n        \n        \"\"\"\n        query = dict()\n        if inactive_only:\n            query['active'] = False\n        if hyperstream.current_session is not None:\n            query['session_id__ne'] = hyperstream.current_session.session_id\n\n        with switch_db(SessionModel, \"hyperstream\"):\n            for s in SessionModel.objects(**query):\n                if clear_history:\n                    channel = hyperstream.channel_manager[s.history_channel]\n                    stream_id = StreamId(\"session\", meta_data=(('uuid', str(s.session_id)),))\n                    try:\n                        channel.purge_stream(stream_id, remove_definition=True, sandbox=None)\n                    except StreamNotFoundError:\n                        pass\n                s.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef close(self):\n        self.active = False\n        self.end = utcnow()\n        self._model.save()", "response": "Close the current session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining the file pointer position of each frame of each CLB UDP packet.", "response": "def determine_packet_positions(self):\n        \"\"\"Record the file pointer position of each frame\"\"\"\n        print(\"Analysing file...\")\n        self.rewind_file()\n        with ignored(struct.error):\n            while True:\n                pointer_position = self.blob_file.tell()\n                length = struct.unpack('<i', self.blob_file.read(4))[0]\n                self.packet_positions.append(pointer_position)\n                self.blob_file.seek(length, 1)\n        self.rewind_file()\n        print(\"Found {0} CLB UDP packets.\".format(len(self.packet_positions)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves file pointer to the packet with given index.", "response": "def seek_to_packet(self, index):\n        \"\"\"Move file pointer to the packet with given index.\"\"\"\n        pointer_position = self.packet_positions[index]\n        self.blob_file.seek(pointer_position, 0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates next blob in file", "response": "def next_blob(self):\n        \"\"\"Generate next blob in file\"\"\"\n        try:\n            length = struct.unpack('<i', self.blob_file.read(4))[0]\n        except struct.error:\n            raise StopIteration\n        header = CLBHeader(file_obj=self.blob_file)\n        blob = {'CLBHeader': header}\n        remaining_length = length - header.size\n        pmt_data = []\n        pmt_raw_data = self.blob_file.read(remaining_length)\n        pmt_raw_data_io = BytesIO(pmt_raw_data)\n        for _ in range(int(remaining_length / 6)):\n            channel_id, time, tot = struct.unpack(\n                '>cic', pmt_raw_data_io.read(6)\n            )\n            pmt_data.append(PMTData(ord(channel_id), time, ord(tot)))\n        blob['PMTData'] = pmt_data\n        blob['PMTRawData'] = pmt_raw_data\n        return blob"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the values from the byte string.", "response": "def _parse_byte_data(self, byte_data):\n        \"\"\"Extract the values from byte string.\"\"\"\n        self.data_type = b''.join(unpack('cccc', byte_data[:4])).decode()\n        self.run = unpack('>i', byte_data[4:8])[0]\n        self.udp_sequence = unpack('>i', byte_data[8:12])[0]\n        self.timestamp, self.ns_ticks = unpack('>II', byte_data[12:20])\n        self.dom_id = unpack('>i', byte_data[20:24])[0]\n\n        dom_status_bits = unpack('>I', byte_data[24:28])[0]\n        self.dom_status = \"{0:032b}\".format(dom_status_bits)\n\n        self.human_readable_timestamp = datetime.datetime.fromtimestamp(\n            int(self.timestamp), UTC_TZ\n        ).strftime('%Y-%m-%d %H:%M:%S')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refresh(self, movie):\n        if '_tmdb_id' in movie:\n            tmdb_id = movie['_tmdb_id']\n            details = self._tmdb_get(tmdb_id)\n            cast = self._tmdb_cast(tmdb_id)\n            alternatives = self._tmdb_alt(tmdb_id)\n            refreshed = self.object_class({'title': details['original_title'],\n                                           'score': details['popularity'],\n                                           'directors': [x['name'] for x in cast['crew'] if x['department'] == 'Directing' and x['job'] == 'Director'],\n                                           'writers': [x['name'] for x in cast['crew'] if x['department'] == 'Writing'],\n                                           'cast': [x['name'] for x in cast['cast']],\n                                           'genres': [x['name'] for x in details['genres']],\n                                           'countries': [x['name'] for x in details['production_countries']],\n                                           'tmdb_votes': int(round(details.get('vote_average', 0) * 0.5)),\n                                           '_Datasource': self.name,\n                                           '_tmdb_id': tmdb_id})\n            if details.get('release_date'):\n                refreshed['year'] = datetime.strptime(details['release_date'], '%Y-%m-%d').year\n            if details.get('belongs_to_collection'):\n                refreshed['collection'] = details['belongs_to_collection']['name']\n            for alt in alternatives['titles']:\n                refreshed['title_%s' % alt['iso_3166_1'].lower()] = alt['title']\n            return refreshed", "response": "Refresh the metadata of the movie."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrefreshing the metadata of the movie.", "response": "def refresh(self, movie):\n        \"\"\" Try to refresh metadata of the movie through the datasource.\n        \"\"\"\n        if '_tmdbtv_id' in movie:\n            refreshed = {'_Datasource': self.name}\n            tvseries_id = movie['_tmdbtv_id']\n            series = self._tmdb_series(tvseries_id)\n            alternatives = self._tmdb_alt_titles(tvseries_id)\n            refreshed.update({'title': series['original_name'],\n                              'year': datetime.strptime(series['first_air_date'], '%Y-%m-%d').year,\n                              'genres': [x['name'] for x in series['genres']],\n                              'networks': [x['name'] for x in series['networks']],\n                              'countries': series['origin_country'],\n                              'tmdb_votes': int(round(series.get('vote_average', 0) * 0.5))})\n            for alt in alternatives['results']:\n                refreshed['title_%s' % alt['iso_3166_1'].lower()] = alt['title']\n            if 'season' in movie:\n                season_num = movie['season']\n                season = self._tmdb_season(tvseries_id, season_num)\n                refreshed.update({'season_title': season['name']})\n                if 'episode' in movie:\n                    episode_num = movie['episode']\n                    episode = self._tmdb_episode(tvseries_id, season_num, episode_num)\n                    credits = self._tmdb_credits(tvseries_id, season_num, episode_num)\n                    refreshed.update({'episode_title': episode['name'],\n                                      'directors': [x['name'] for x in credits['crew'] if x['department'] == 'Directing' and x['job'] == 'Director'],\n                                      'writers': [x['name'] for x in credits['crew'] if x['department'] == 'Writing'],\n                                      'cast': [x['name'] for x in credits['cast']],\n                                      'guests': [x['name'] for x in credits['guest_stars']]})\n            return refreshed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef refresh(self, movie):\n        if '_tmdb_id' in movie:\n            tmdb_id = movie['_tmdb_id']\n            movie = self._get('/1/movie/%s' % tmdb_id)\n            return movie['movie']", "response": "Refresh the metadata of the movie."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getKendallTauScore(myResponse, otherResponse):\n    # variables\n    kt = 0\n    list1 = myResponse.values()\n    list2 = otherResponse.values()\n\n    if len(list1) <= 1:\n        return kt\n\n    # runs through list1\n    for itr1 in range(0, len(list1) - 1):\n        # runs through list2\n        for itr2 in range(itr1 + 1, len(list2)):\n            # checks if there is a discrepancy. If so, adds\n            if ((list1[itr1] > list1[itr2]\n                 and list2[itr1] < list2[itr2])\n                or (list1[itr1] < list1[itr2]\n                    and list2[itr1] > list2[itr2])):\n                kt += 1\n    # normalizes between 0 and 1\n    kt = (kt * 2) / (len(list1) * (len(list1) - 1))\n\n    # returns found value\n    return kt", "response": "Returns the Kendall Tau Score for the given response and other response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getScoringVector(self, profile):\n\n        # Check to make sure that the scoring vector contains a score for every possible rank in a\n        # ranking.\n        if len(self.scoringVector) != profile.numCands:\n            print(\"ERROR: scoring vector is not the correct length\")\n            exit()\n\n        return self.scoringVector", "response": "Returns the scoring vector for the given election profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getMov(self, profile):\n        # from . import mov\n        import mov\n        return mov.MoVScoring(profile, self.getScoringVector(profile))", "response": "Returns an integer that is equal to the margin of victory of the election profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the scoring vector for a given profile.", "response": "def getScoringVector(self, profile):\n        \"\"\"\n        Returns the scoring vector [1,0,0,...,0]. This function is called by getCandScoresMap()\n        which is implemented in the parent class.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        scoringVector = []\n        scoringVector.append(1)\n        for i in range(1, profile.numCands):\n            scoringVector.append(0)\n        return scoringVector"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the scoring vector for the given profile.", "response": "def getScoringVector(self, profile):\n        \"\"\"\n        Returns the scoring vector [m-1,m-2,m-3,...,0] where m is the number of candidates in the\n        election profile. This function is called by getCandScoresMap() which is implemented in the\n        parent class.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        scoringVector = []\n        score = profile.numCands - 1\n        for i in range(0, profile.numCands):\n            scoringVector.append(score)\n            score -= 1\n        return scoringVector"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a scoring vector such that the first k candidates recieve 1 point and all others recieve 0.", "response": "def getScoringVector(self, profile):\n        \"\"\"\n        Returns a scoring vector such that the first k candidates recieve 1 point and all others\n        recive 0  This function is called by getCandScoresMap() which is implemented in the parent\n        class.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        if self.k > profile.numCands:\n            self.k = profile.numCands\n\n        scoringVector = []\n        for i in range(0, self.k):\n            scoringVector.append(1)\n        for i in range(self.k, profile.numCands):\n            scoringVector.append(0)\n        return scoringVector"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getCandScoresMap(self, profile):\n\n        # Currently, we expect the profile to contain complete ordering over candidates.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\":\n            print(\"ERROR: unsupported profile type\")\n            exit()\n\n        bucklinScores = dict()\n        rankMaps = profile.getRankMaps()\n        preferenceCounts = profile.getPreferenceCounts()\n        for cand in profile.candMap.keys():\n\n            # We keep track of the number of times a candidate is ranked in the first t positions.\n            numTimesRanked = 0\n\n            # We increase t in increments of 1 until we find t such that the candidate is ranked in the\n            # first t positions in at least half the votes.\n            for t in range(1, profile.numCands + 1):\n                for i in range(0, len(rankMaps)):\n                    if (rankMaps[i][cand] == t):\n                        numTimesRanked += preferenceCounts[i]\n                if numTimesRanked >= math.ceil(float(profile.numVoters) / 2):\n                    bucklinScores[cand] = t\n                    break\n\n        return bucklinScores", "response": "Returns a dictionary that associates integer representations of each candidate with their\n        Bucklin score."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary that associates integer representations of each candidate with their Copeland score.", "response": "def getCandScoresMap(self, profile):\n        \"\"\"\n        Returns a dictionary that associates integer representations of each candidate with their\n        Copeland score.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        # Initialize each Copeland score as 0.0.\n        copelandScores = dict()\n        for cand in profile.candMap.keys():\n            copelandScores[cand] = 0.0\n\n        preferenceCounts = profile.getPreferenceCounts()\n\n        # For each pair of candidates, calculate the number of votes in which one beat the other.\n        wmgMap = profile.getWmg()\n        for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n            if cand2 in wmgMap[cand1].keys():\n                if wmgMap[cand1][cand2] > 0:\n                    copelandScores[cand1] += 1.0\n                elif wmgMap[cand1][cand2] < 0:\n                    copelandScores[cand2] += 1.0\n\n                # If a pair of candidates is tied, we add alpha to their score for each vote.\n                else:\n                    copelandScores[cand1] += self.alpha\n                    copelandScores[cand2] += self.alpha\n\n        return copelandScores"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getCandScoresMap(self, profile):\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        wmg = profile.getWmg()\n\n        # Initialize the maximin score for each candidate as infinity.\n        maximinScores = dict()\n        for cand in wmg.keys():\n            maximinScores[cand] = float(\"inf\")\n\n        # For each pair of candidates, calculate the number of times each beats the other.\n        for cand1, cand2 in itertools.combinations(wmg.keys(), 2):\n            if cand2 in wmg[cand1].keys():\n                maximinScores[cand1] = min(maximinScores[cand1], wmg[cand1][cand2])\n                maximinScores[cand2] = min(maximinScores[cand2], wmg[cand2][cand1])\n\n        return maximinScores", "response": "Returns a dictionary that associates integer representations of each candidate with their maximin score."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the strongest paths between the candidates cand1 and cand2.", "response": "def computeStrongestPaths(self, profile, pairwisePreferences):\n        \"\"\"\n        Returns a two-dimensional dictionary that associates every pair of candidates, cand1 and\n        cand2, with the strongest path from cand1 to cand2.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        :ivar dict<int,dict<int,int>> pairwisePreferences: A two-dimensional dictionary that\n            associates every pair of candidates, cand1 and cand2, with number of voters who prefer\n            cand1 to cand2.\n        \"\"\"\n        cands = profile.candMap.keys()\n        numCands = len(cands)\n\n        # Initialize the two-dimensional dictionary that will hold our strongest paths.\n        strongestPaths = dict()\n        for cand in cands:\n            strongestPaths[cand] = dict()\n\n        for i in range(1, numCands + 1):\n            for j in range(1, numCands + 1):\n                if (i == j):\n                    continue\n                if pairwisePreferences[i][j] > pairwisePreferences[j][i]:\n                    strongestPaths[i][j] = pairwisePreferences[i][j]\n                else:\n                    strongestPaths[i][j] = 0\n\n        for i in range(1, numCands + 1):\n            for j in range(1, numCands + 1):\n                if (i == j):\n                    continue\n                for k in range(1, numCands + 1):\n                    if (i == k or j == k):\n                        continue\n                    strongestPaths[j][k] = max(strongestPaths[j][k], min(strongestPaths[j][i], strongestPaths[i][k]))\n\n        return strongestPaths"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the pairwise preferences for each pair of candidates cand1 and cand2.", "response": "def computePairwisePreferences(self, profile):\n        \"\"\"\n        Returns a two-dimensional dictionary that associates every pair of candidates, cand1 and\n        cand2, with number of voters who prefer cand1 to cand2.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        cands = profile.candMap.keys()\n\n        # Initialize the two-dimensional dictionary that will hold our pairwise preferences.\n        pairwisePreferences = dict()\n        for cand in cands:\n            pairwisePreferences[cand] = dict()\n        for cand1 in cands:\n            for cand2 in cands:\n                if cand1 != cand2:\n                    pairwisePreferences[cand1][cand2] = 0\n\n        for preference in profile.preferences:\n            wmgMap = preference.wmgMap\n            for cand1, cand2 in itertools.combinations(cands, 2):\n\n                # If either candidate was unranked, we assume that they are lower ranked than all\n                # ranked candidates.\n                if cand1 not in wmgMap.keys():\n                    if cand2 in wmgMap.keys():\n                        pairwisePreferences[cand2][cand1] += 1 * preference.count\n                elif cand2 not in wmgMap.keys():\n                    if cand1 in wmgMap.keys():\n                        pairwisePreferences[cand1][cand2] += 1 * preference.count\n\n                elif wmgMap[cand1][cand2] == 1:\n                    pairwisePreferences[cand1][cand2] += 1 * preference.count\n                elif wmgMap[cand1][cand2] == -1:\n                    pairwisePreferences[cand2][cand1] += 1 * preference.count\n\n        return pairwisePreferences"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary that associates integer representations of each candidate with the number of other candidates for which the strongest path to the other candidate is greater than the other candidate s stronget path to her.", "response": "def getCandScoresMap(self, profile):\n        \"\"\"\n        Returns a dictionary that associates integer representations of each candidate with the\n        number of other candidates for which her strongest path to the other candidate is greater\n        than the other candidate's stronget path to her.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        cands = profile.candMap.keys()\n        pairwisePreferences = self.computePairwisePreferences(profile)\n        strongestPaths = self.computeStrongestPaths(profile, pairwisePreferences)\n\n        # For each candidate, determine how many times p[E,X] >= p[X,E] using a variant of the\n        # Floyd-Warshall algorithm.\n        betterCount = dict()\n        for cand in cands:\n            betterCount[cand] = 0\n        for cand1 in cands:\n            for cand2 in cands:\n                if cand1 == cand2:\n                    continue\n                if strongestPaths[cand1][cand2] >= strongestPaths[cand2][cand1]:\n                    betterCount[cand1] += 1\n\n        return betterCount"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of all possible winners of a profile under STV rule.", "response": "def STVsocwinners(self, profile):\n        \"\"\"\n        Returns an integer list that represents all possible winners of a profile under STV rule.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n        ordering = profile.getOrderVectors()\n        prefcounts = profile.getPreferenceCounts()\n        m = profile.numCands\n\n        if min(ordering[0]) == 0:\n            startstate = set(range(m))\n        else:\n            startstate = set(range(1, m + 1))\n\n        ordering, startstate = self.preprocessing(ordering, prefcounts, m, startstate)\n        m_star = len(startstate)\n        known_winners = set()\n        # ----------Some statistics--------------\n        hashtable2 = set()\n\n        # push the node of start state into the priority queue\n        root = Node(value=startstate)\n        stackNode = []\n        stackNode.append(root)\n\n        while stackNode:\n            # ------------pop the current node-----------------\n            node = stackNode.pop()\n            # -------------------------------------------------\n            state = node.value.copy()\n\n            # use heuristic to delete all the candidates which satisfy the following condition\n\n            # goal state 1: if the state set contains only 1 candidate, then stop\n            if len(state) == 1 and list(state)[0] not in known_winners:\n                known_winners.add(list(state)[0])\n                continue\n            # goal state 2 (pruning): if the state set is subset of the known_winners set, then stop\n            if state <= known_winners:\n                continue\n            # ----------Compute plurality score for the current remaining candidates--------------\n            plural_score = self.get_plurality_scores3(prefcounts, ordering, state, m_star)\n            minscore = min(plural_score.values())\n            for to_be_deleted in state:\n                if plural_score[to_be_deleted] == minscore:\n                    child_state = state.copy()\n                    child_state.remove(to_be_deleted)\n                    tpc = tuple(sorted(child_state))\n                    if tpc in hashtable2:\n                        continue\n                    else:\n                        hashtable2.add(tpc)\n                        child_node = Node(value=child_state)\n                        stackNode.append(child_node)\n\n        return sorted(known_winners)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of all possible winners of a profile under baldwin rule.", "response": "def baldwinsoc_winners(self, profile):\n        \"\"\"\n        Returns an integer list that represents all possible winners of a profile under baldwin rule.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n        ordering = profile.getOrderVectors()\n        m = profile.numCands\n        prefcounts = profile.getPreferenceCounts()\n        if min(ordering[0]) == 0:\n            startstate = set(range(m))\n        else:\n            startstate = set(range(1, m + 1))\n        wmg = self.getWmg2(prefcounts, ordering, startstate, normalize=False)\n        known_winners = set()\n        # ----------Some statistics--------------\n        hashtable2 = set()\n\n        # push the node of start state into the priority queue\n        root = Node(value=startstate)\n        stackNode = []\n        stackNode.append(root)\n\n        while stackNode:\n            # ------------pop the current node-----------------\n            node = stackNode.pop()\n            # -------------------------------------------------\n            state = node.value.copy()\n\n            # goal state 1: if the state set contains only 1 candidate, then stop\n            if len(state) == 1 and list(state)[0] not in known_winners:\n                known_winners.add(list(state)[0])\n                continue\n            # goal state 2 (pruning): if the state set is subset of the known_winners set, then stop\n            if state <= known_winners:\n                continue\n            # ----------Compute plurality score for the current remaining candidates--------------\n            plural_score = dict()\n            for cand in state:\n                plural_score[cand] = 0\n            for cand1, cand2 in itertools.permutations(state, 2):\n                plural_score[cand1] += wmg[cand1][cand2]\n\n            # if current state satisfies one of the 3 goal state, continue to the next loop\n\n            # After using heuristics, generate children and push them into priority queue\n            # frontier = [val for val in known_winners if val in state] + list(set(state) - set(known_winners))\n\n            minscore = min(plural_score.values())\n            for to_be_deleted in state:\n                if plural_score[to_be_deleted] == minscore:\n                    child_state = state.copy()\n                    child_state.remove(to_be_deleted)\n                    tpc = tuple(sorted(child_state))\n                    if tpc in hashtable2:\n                        continue\n                    else:\n                        hashtable2.add(tpc)\n                        child_node = Node(value=child_state)\n                        stackNode.append(child_node)\n        return sorted(known_winners)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coombs_winners(self, profile):\n        elecType = profile.getElecType()\n        if elecType == \"soc\" or elecType == \"csv\":\n            return self.coombssoc_winners(profile)\n        elif elecType == \"toc\":\n            return self.coombstoc_winners(profile)\n        else:\n            print(\"ERROR: unsupported profile type\")\n            exit()", "response": "Returns an integer list that represents all possible winners of a profile under Coombs rule."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coombstoc_winners(self, profile):\n        ordering = profile.getOrderVectors()\n        m = profile.numCands\n        prefcounts = profile.getPreferenceCounts()\n        rankmaps = profile.getRankMaps()\n        if min(ordering[0]) == 0:\n            startstate = set(range(m))\n        else:\n            startstate = set(range(1, m + 1))\n        known_winners = set()\n        # half = math.floor(n / 2.0)\n        # ----------Some statistics--------------\n        hashtable2 = set()\n\n        # push the node of start state into the priority queue\n        root = Node(value=startstate)\n        stackNode = []\n        stackNode.append(root)\n\n        while stackNode:\n            # ------------pop the current node----------------\n            node = stackNode.pop()\n            # -------------------------------------------------\n            state = node.value.copy()\n            # use heuristic to delete all the candidates which satisfy the following condition\n\n            # goal state 1: if the state set contains only 1 candidate, then stop\n            if len(state) == 1 and list(state)[0] not in known_winners:\n                known_winners.add(list(state)[0])\n                continue\n            # goal state 2 (pruning): if the state set is subset of the known_winners set, then stop\n            if state <= known_winners:\n                continue\n            # ----------Compute plurality score for the current remaining candidates-------------\n            reverse_veto_score = self.get_reverse_veto_scores2(prefcounts, rankmaps, state)\n            # print(\"reverse_veto_score = \",reverse_veto_score)\n\n            # if current state satisfies one of the 3 goal state, continue to the next loop\n\n            # After using heuristics, generate children and push them into priority queue\n            # frontier = [val for val in known_winners if val in state] + list(set(state) - set(known_winners))\n\n            maxscore = max(reverse_veto_score.values())\n            for to_be_deleted in state:\n                if reverse_veto_score[to_be_deleted] == maxscore:\n                    child_state = state.copy()\n                    child_state.remove(to_be_deleted)\n                    tpc = tuple(sorted(child_state))\n                    if tpc in hashtable2:\n                        continue\n                    else:\n                        hashtable2.add(tpc)\n                        child_node = Node(value=child_state)\n                        stackNode.append(child_node)\n        return sorted(known_winners)", "response": "Returns a list of all possible winners of a profile under Coombs rule."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the winners of completed RP graph G I to known_winners.", "response": "def add_winners(self, G, I, known_winners, stats, possible_winners = None):\n        \"\"\"\n        Adds the winners of completed RP graph G\n        :param G: networkx graph, should be final resulting graph after running RP\n        :param I: list of all nodes\n        :param known_winners: list of winners found so far, will be updated\n        :param stats: Stats class storing run statistics\n        :param possible_winners: Can optionally pass in possible winners if already computed to avoid re-computing here\n        \"\"\"\n        if possible_winners is None:\n            G_in_degree = G.in_degree(I)\n            to_be_added = set([x[0] for x in G_in_degree if x[1] == 0])\n        else:\n            to_be_added = possible_winners\n        for c in to_be_added:\n            if c not in known_winners:\n                known_winners.add(c)\n                stats.discovery_states[c] = stats.num_nodes\n                stats.discovery_times[c] = time.perf_counter() - self.BEGIN\n                if self.debug_mode >= 2:\n                    print(\"Found new winner:\", c)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine if the stop condition met for the current state of the current edge G and E.", "response": "def stop_conditions(self, G, E, I, known_winners, stats):\n        \"\"\"\n        Determines if G, E state can be ended early\n        :param G: networkx DiGraph of the current representation of \"locked in\" edges in RP\n        :param E: networkx DiGraph of the remaining edges not yet considered\n        :param I: list of all nodes\n        :param known_winners: list of currently known PUT-winners\n        :param stats: Stats object containing runtime statistics\n        :return: -1 if no stop condition met, otherwise returns the int of the stop condition\n        \"\"\"\n\n        in_deg = G.in_degree(I)\n        possible_winners = [x[0] for x in in_deg if x[1] == 0]\n\n        # Stop Condition 2: Pruning. Possible winners are subset of known winners\n        if set(possible_winners) <= known_winners:\n            stats.stop_condition_hits[2] += 1\n            if self.debug_mode >= 2:\n                print(\"Stop Condition 2: pruned\")\n            return 2\n\n        # Stop Condition 3: Exactly one node has indegree 0\n        if len(possible_winners) == 1:\n            stats.stop_condition_hits[3] += 1\n            if self.debug_mode >= 2:\n                print(\"Stop Condition 3: one cand in degree 0\")\n            self.add_winners(G, I, known_winners, stats, possible_winners)\n            return 3\n\n        # Stop Condition 1: G U E is acyclic\n        temp_G = nx.compose(G, E)\n        if nx.is_directed_acyclic_graph(temp_G) is True:\n            stats.stop_condition_hits[1] += 1\n            if self.debug_mode >= 2:\n                print(\"Stop Condition 1: acyclic\")\n            self.add_winners(G, I, known_winners, stats)\n            return 1\n\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_max_children_scc_decomposition(self, G, tier, scc, bridges, I, known_winners, stats):\n        '''\n        Finds the maximal children of G when tier is added using SCC decomposition\n        :param G: Networkx DiGraph of edges \"locked in\" so far\n        :param tier: List of edges in the current tier to be added with equal weight\n        :param scc: List of the strongly connected components of G U tier, each being a list of edges\n        :param bridges: List of edges that are bridges between sccs of G U tier\n        :param I: List of all nodes\n        :param known_winners: Known PUT-winners computed by RP so far\n        :param stats: Stats object containing runtime statistics\n        :return: Array of Networkx DiGraphs that are the maximal children of G U T\n        '''\n\n        if len(scc) == 1:\n            children = self.explore_max_children_lp(G, tier, I, known_winners, stats)\n            return children\n\n        mc_list = []\n\n        for x in scc:\n            G_temp = nx.DiGraph(list(set(G.edges()).intersection(set(x))))\n            T_temp = list(set(tier).intersection(set(x)))\n            temp = self.explore_max_children_lp(G_temp, T_temp, I, known_winners, stats, f_scc = 1)\n            mc_list.append(temp)\n\n        Cartesian = itertools.product(*mc_list)\n        return [nx.DiGraph(list(set(itertools.chain(*[list(y.edges()) for y in x])).union(bridges))) for x in Cartesian]", "response": "Finds the maximal children of G when tier is added using SCC decomposition."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the maximal children of G when tier is added :param G: DiGraph, A directed graph :param tier: list of tuples which correspond to multiple edges with same max weight. e.g. edges = [x for x in wmg2.keys() if wmg2[x] == max_weight] :param I: all nodes in G :param known_winners: PUT-winners found so far by RP :param stats: Stats object :param f_scc: set to 1 if the G and tier being considered are an SCC of the full graph due to SCC decomposition :return: set of graphs which correspond to maximum children of given parent: G", "response": "def explore_max_children_lp(self, G, tier, I, known_winners, stats, f_scc = 0):\n        \"\"\"\n        Computes the maximal children of G when tier is added\n        :param G: DiGraph, A directed graph\n        :param tier: list of tuples which correspond to multiple edges with same max weight.\n                    e.g. edges = [x for x in wmg2.keys() if wmg2[x] == max_weight]\n        :param I: all nodes in G\n        :param known_winners: PUT-winners found so far by RP\n        :param stats: Stats object\n        :param f_scc: set to 1 if the G and tier being considered are an SCC of the full graph due to SCC decomposition\n        :return: set of graphs which correspond to maximum children of given parent: G\n        \"\"\"\n\n        # self.output_graph(G)\n        # self.output_graph(nx.DiGraph(tier))\n\n        max_children = []\n        cstack = []\n\n        # print(\"start mc:\", time.perf_counter() - self.BEGIN)\n\n        hashtable = set()\n\n        if self.debug_mode >= 1:\n            print(\"Exploring max children\")\n            print(\"G:\", G.edges())\n            print(\"Tier:\", tier)\n            print(\"Known winners:\", known_winners)\n            print(\"---------------------------\")\n\n        in_deg = G.in_degree()\n        nodes_with_no_incoming = set()\n        for x in in_deg:\n            if x[1] == 0:\n                nodes_with_no_incoming.add(x[0])\n        for x in I:\n            if x not in G.nodes():\n                nodes_with_no_incoming.add(x)\n\n        root = Node(value=(self.edges2string(G.edges(), I), self.edges2string(tier, I), nodes_with_no_incoming))\n        cstack.append(root)\n\n        END = self.BEGIN + self.TIMEOUT\n\n        while cstack:\n            node = cstack.pop()\n            (G_str, T_str, no_incoming) = node.value\n\n            if time.perf_counter() > END:\n                print(\"TIMEOUT\")\n                return max_children\n\n            # Check hash. Doesn't ever happen if the below hash is included\n            hash_G = hash(G_str)\n            if hash_G in hashtable:\n                stats.num_hashes += 1\n                print('hash')\n                if self.debug_mode >= 2:\n                    print(\"hashed in hashtable\")\n                continue\n            hashtable.add(hash_G)\n\n            stats.num_nodes += 1\n\n            G = nx.DiGraph(self.string2edges(G_str, I))\n            T = self.string2edges(T_str, I)\n            G.add_nodes_from(I)\n\n            if self.debug_mode == 3:\n                print(\"popped\")\n                print(\"G: \", G.edges())\n                print(\"T: \", T)\n\n            # goal state 2: if current G's possible winners is subset of known winners,\n            # then directly ignore it.\n            if no_incoming <= known_winners and not f_scc:\n                stats.stop_condition_hits[2] += 1\n                if self.debug_mode >= 3:\n                    print(\"MC goal state 2: pruned\")\n                continue\n\n            # goal state 1: if there are no edges to be added, then add the G_\n            if len(T) == 0:\n                max_children.append(G.copy())\n                if self.debug_mode >= 2:\n                    print(\"MC goal state 1: no more edges in tier\")\n                    print(\"max child: \", G.edges())\n                continue\n\n            # goal state 3: if current G has exactly one cand with in degree 0, it is a PUT-winner\n            if len(no_incoming) == 1 and not f_scc:\n                stats.stop_condition_hits[3] += 1\n                if self.debug_mode >= 2:\n                    print(\"MC goal state 3: only one cand in degree 0\")\n                    print(\"max child:\", G.edges())\n                self.add_winners(G, I, known_winners, stats, no_incoming)\n                continue\n\n            # goal state 4: if union of current G and edges is acyclic,\n            # then directly add it to the max_children_set\n            Gc = G.copy()\n            Gc.add_edges_from(T)\n            if nx.is_directed_acyclic_graph(Gc):\n                stats.stop_condition_hits[4] += 1\n\n                hash_temp_G = hash(self.edges2string(Gc.edges(), I))\n                if hash_temp_G not in hashtable:\n                    hashtable.add(hash_temp_G)\n                    max_children.append(Gc)\n\n                    if self.debug_mode >= 2:\n                        print(\"MC goal state 4: G U T is acyclic\")\n                        print(\"max child:\", Gc.edges())\n                else:\n                    stats.num_hashes += 1\n                continue\n\n            # Perform reductions every step:\n\n            # Compute \"bridge edges\" which are not in any cycle\n            Gc = G.copy()\n            Gc.add_edges_from(T)\n            scc = [list(g.edges()) for g in nx.strongly_connected_component_subgraphs(Gc, copy=True) if\n                   len(g.edges()) != 0]\n            bridges = set(Gc.edges()) - set(itertools.chain(*scc))\n            G.add_edges_from(bridges)\n            T = list(set(T) - bridges)\n\n            G_tc = nx.transitive_closure(G)\n\n            # Remove \"inconsistent edges\" that cannot be added to G without causing cycle\n            reverse_G = nx.DiGraph.reverse(G_tc)\n            T = list(set(T) - set(reverse_G.edges()))\n\n            # Remove \"redundant edges\": if there is already path from e[0] to e[1], can immediately add e\n            redundant_edges = set()\n            for e in T:\n                if G_tc.has_edge(e[0], e[1]):\n                    redundant_edges.add(e)\n                    G.add_edges_from([e])\n            stats.num_redundant_edges += len(redundant_edges)\n            T = list(set(T) - redundant_edges)\n\n            # Flag for whether adding any edge from T causes G to remain acyclic\n            f_isAcyclic = 0\n\n            children = dict()\n\n            # Used to break ties\n            index = 0\n            T = sorted(T)\n            for e in T:\n                G.add_edges_from([e])\n                Gc_str = self.edges2string(G.edges(), I)\n                if hash(Gc_str) in hashtable:\n                    f_isAcyclic = 1\n\n                    stats.num_hashes += 1\n                    G.remove_edges_from([e])\n                    continue\n\n                if not nx.has_path(G, source=e[1], target=e[0]):\n                    f_isAcyclic = 1\n\n                    Tc = copy.deepcopy(T)\n                    Tc.remove(e)\n\n                    # Remove the head of the edge if it had no incoming edges previously\n                    no_incoming_c = no_incoming.copy()\n                    no_incoming_c.discard(e[1])\n\n                    child = Node(value=(Gc_str, self.edges2string(Tc, I), no_incoming_c))\n\n                    priority = len(no_incoming_c - known_winners)\n\n                    children[child] = (priority, index)\n                    index = index + 1\n\n                    if self.debug_mode == 3:\n                        print(\"add new child with edge \", e, \" and priority \", priority)\n\n                G.remove_edges_from([e])\n\n            children_items = sorted(children.items(), key=lambda x: (x[1][0], x[1][1]))\n            sorted_children = [key for key, value in children_items]\n            cstack += sorted_children\n\n            # goal state 5: adding all edges in T individually cause G to be cyclic\n            if f_isAcyclic == 0:\n                max_children.append(G.copy())\n\n                if self.debug_mode >= 2:\n                    print(\"MC goal state 5 - found max child\")\n                    print(\"max child: \", G.edges())\n                continue\n\n        if self.debug_mode >= 1:\n            print(\"finished exploring max children\")\n            print(\"num max children:\", len(max_children))\n            print(\"PUT-winners:\", known_winners)\n\n        return max_children"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sample(self, E, I, known_winners, stats):\n        '''\n        Using random tie-breaking, run through one procedure of RP and add resulting winner to known_winners\n        :param E: DiGraph, All postive edges in the wmg\n        :param I: List of all nodes in E\n        :param known_winners: Set of already-discovered PUT-winners to be added to\n        :param stats: Stats object storing runtime statistics\n        :return: Nothing\n        '''\n        G = nx.DiGraph()\n        G.add_nodes_from(I)\n\n        Ec = E.copy()\n\n        while len(Ec.edges()) != 0:\n            max_weight = max([(d['weight']) for (u, v, d) in Ec.edges(data=True)])\n            tier = [(u, v) for (u, v, d) in Ec.edges(data=True) if d['weight'] == max_weight]\n\n            # e = tier[random.randint(0, len(tier) -1 )]\n            priorities = []\n            potential_winners = set([x[0] for x in G.in_degree(I) if x[1] == 0])\n            base_priority = len(potential_winners - known_winners)\n            for e in tier:\n                if G.in_degree(e[1]) == 0 and e[1] not in known_winners:\n                    priority = base_priority - 1\n                else:\n                    priority = base_priority\n                priorities.append(exp(priority / self.tau_for_testing))\n            q_sum = sum(priorities)\n            probs = []\n            for v in priorities:\n                probs.append(v / q_sum)\n            legal_actions_index = [i for i in range(len(tier))]\n            e = tier[np.random.choice(legal_actions_index, p=probs)]\n\n            if not nx.has_path(G, e[1], e[0]):\n                G.add_edges_from([e])\n                Ec.remove_edges_from([e])\n            else:\n                Ec.remove_edges_from([e])\n\n        self.add_winners(G, I, known_winners, stats)", "response": "This method is used to sample from the given set of known winners."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef black_winner(self, profile):\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        wmg = profile.getWmg()\n        m = profile.numCands\n        for cand1 in wmg.keys():\n            outgoing = 0\n            for cand2 in wmg[cand1].keys():\n                if wmg[cand1][cand2] > 0:\n                    outgoing += 1\n            if outgoing == m - 1:\n                return [cand1]\n\n        Borda_winner = MechanismBorda().getWinners(profile)\n        return Borda_winner", "response": "Returns a number or a list that associates the winner of a profile under black rule."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a number that associates the winner of a profile under Plurality with Runoff rule. :ivar Profile profile: A Profile object that represents an election profile.", "response": "def PluRunOff_single_winner(self, profile):\n        \"\"\"\n        Returns a number that associates the winner of a profile under Plurality with Runoff rule.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        # Initialization\n        prefcounts = profile.getPreferenceCounts()\n        len_prefcounts = len(prefcounts)\n        rankmaps = profile.getRankMaps()\n        ranking = MechanismPlurality().getRanking(profile)\n\n        # 1st round: find the top 2 candidates in plurality scores\n        # Compute the 1st-place candidate in plurality scores\n        # print(ranking)\n        max_cand = ranking[0][0][0]\n\n        # Compute the 2nd-place candidate in plurality scores\n        # Automatically using tie-breaking rule--numerically increasing order\n        if len(ranking[0][0]) > 1:\n            second_max_cand = ranking[0][0][1]\n        else:\n            second_max_cand = ranking[0][1][0]\n\n        top_2 = [max_cand, second_max_cand]\n        # 2nd round: find the candidate with maximum plurality score\n        dict_top2 = {max_cand: 0, second_max_cand: 0}\n        for i in range(len_prefcounts):\n            vote_top2 = {key: value for key, value in rankmaps[i].items() if key in top_2}\n            top_position = min(vote_top2.values())\n            keys = [x for x in vote_top2.keys() if vote_top2[x] == top_position]\n            for key in keys:\n                dict_top2[key] += prefcounts[i]\n\n        # print(dict_top2)\n        winner = max(dict_top2.items(), key=lambda x: x[1])[0]\n\n        return winner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PluRunOff_cowinners(self, profile):\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        # Initialization\n        prefcounts = profile.getPreferenceCounts()\n        len_prefcounts = len(prefcounts)\n        rankmaps = profile.getRankMaps()\n        ranking = MechanismPlurality().getRanking(profile)\n\n        known_winners = set()\n        # 1st round: find the top 2 candidates in plurality scores\n        top_2_combinations = []\n        if len(ranking[0][0]) > 1:\n            for cand1, cand2 in itertools.combinations(ranking[0][0], 2):\n                top_2_combinations.append([cand1, cand2])\n        else:\n            max_cand = ranking[0][0][0]\n            if len(ranking[0][1]) > 1:\n                for second_max_cand in ranking[0][1]:\n                    top_2_combinations.append([max_cand, second_max_cand])\n            else:\n                second_max_cand = ranking[0][1][0]\n                top_2_combinations.append([max_cand, second_max_cand])\n\n        # 2nd round: find the candidate with maximum plurality score\n        for top_2 in top_2_combinations:\n            dict_top2 = {top_2[0]: 0, top_2[1]: 0}\n            for i in range(len_prefcounts):\n                vote_top2 = {key: value for key, value in rankmaps[i].items() if key in top_2}\n                top_position = min(vote_top2.values())\n                keys = [x for x in vote_top2.keys() if vote_top2[x] == top_position]\n                for key in keys:\n                    dict_top2[key] += prefcounts[i]\n\n            max_value = max(dict_top2.values())\n            winners = [y for y in dict_top2.keys() if dict_top2[y] == max_value]\n            known_winners = known_winners | set(winners)\n\n        return sorted(known_winners)", "response": "Returns a list of all the winners of a profile under Plurality with Runoff rule."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef SNTV_winners(self, profile, K):\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        m = profile.numCands\n        candScoresMap = MechanismPlurality().getCandScoresMap(profile)\n        if K >= m:\n            return list(candScoresMap.keys())\n        # print(candScoresMap)\n        sorted_items = sorted(candScoresMap.items(), key=lambda x: x[1], reverse=True)\n        sorted_dict = {key: value for key, value in sorted_items}\n        winners = list(sorted_dict.keys())[0:K]\n        return winners", "response": "Returns a list that associates all the winners of a profile under Single non - transferable vote rule."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef single_peaked_winners(self, profile, d=1, K=3, funcType='Borda', scoringVector=[]):\n\n        # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n        # allowed however.\n        elecType = profile.getElecType()\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"csv\":\n            print(\"ERROR: unsupported election type\")\n            exit()\n\n        # ------------------1. INITIALIZATION-----------------------------\n        m = profile.numCands\n        n = profile.numVoters\n        cand = list(profile.candMap.keys())\n        cand.append(cand[m - 1] + 1)\n        theta = n - d\n        if funcType == 'Borda':\n            scoringVector = MechanismBorda().getScoringVector(profile)\n        z = dict()\n        for k in range(1, K + 2):  # k = 1,...,K + 1\n            z[k] = dict()\n            for j in range(1, m + 2):\n                z[k][j] = dict()\n\n        for j in range(1, m + 2):\n            for t in range(0, theta + 1):\n                z[1][j][t] = self.s(profile, 1, j, t, {cand[j - 1]}, scoringVector)\n                for k in range(1, K + 1):\n                    z[k + 1][j][t] = float(\"-inf\")\n\n        # ------------------2. MAIN LOOP-----------------------------\n        for k in range(1, K + 1):\n            # Predecessors loop:\n            for p in range(1, m + 1):\n                for u in range(0, theta + 1):\n                    if z[k][p][u] != float(\"-inf\"):\n                        # Successors sub-loop:\n                        for j in range(p + 1, m + 2):\n                            for t in range(u, theta + 1):\n                                z[k + 1][j][t] = max(z[k + 1][j][t], z[k][p][u]\n                                                     + self.s(profile, p + 1, j, t - u, {cand[p - 1], cand[j - 1]},\n                                                              scoringVector))\n\n        max_utility = z[K + 1][m + 1][theta]\n        print(\"max_utility=\", max_utility)\n        # --------------------3. OUTPUT WINNERS---------------------------\n        winners = []\n        temp_max = max_utility\n        j = m + 1\n        t = theta\n        for k in range(K + 1, 1, -1):\n            z_k_j_t = array(\n                [[z[k - 1][p][u] + self.s(profile, p + 1, j, t - u, {cand[p - 1], cand[j - 1]}, scoringVector)\n                  for u in range(0, theta + 1)] for p in range(1, m + 1)])\n            p_ind = where(temp_max == z_k_j_t)[0][0]\n            u_ind = where(temp_max == z_k_j_t)[0][0]\n            p0 = list(range(1, m + 1))[p_ind]\n            u0 = list(range(0, theta + 1))[u_ind]\n            winners.append(p0)\n            temp_max = z[k][p0][u0]\n            j = p0\n            t = u0\n\n        return sorted(winners)", "response": "Returns a list of all the winners of a profile under the Chamberlin\u2013Courant rule."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list that associates all the winners of a profile under The Borda - mean rule.", "response": "def Borda_mean_winners(self, profile):\n        \"\"\"\n        Returns a list that associates all the winners of a profile under The Borda-mean rule.\n\n        :ivar Profile profile: A Profile object that represents an election profile.\n        \"\"\"\n        n_candidates = profile.numCands\n        prefcounts = profile.getPreferenceCounts()\n        len_prefcounts = len(prefcounts)\n        rankmaps = profile.getRankMaps()\n        values = zeros([len_prefcounts, n_candidates], dtype=int)\n        if min(list(rankmaps[0].keys())) == 0:\n            delta = 0\n        else:\n            delta = 1\n        for i in range(len_prefcounts):\n            for j in range(delta, n_candidates + delta):\n                values[i][j - delta] = rankmaps[i][j]\n        # print(\"values=\", values)\n        mat0 = self._build_mat(values, n_candidates, prefcounts)\n        borda = [0 for i in range(n_candidates)]\n        for i in range(n_candidates):\n            borda[i] = sum([mat0[i, j] for j in range(n_candidates)])\n        borda_mean = mean(borda)\n        bin_winners_list = [int(borda[i] >= borda_mean) for i in range(n_candidates)]\n        return bin_winners_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_mat(self, ranks, n_candidates, prefcounts):\n\n        mat = zeros((n_candidates, n_candidates))\n        for i, j in itertools.combinations(range(n_candidates), 2):\n            preference = ranks[:, i] - ranks[:, j]\n            h_ij = dot((preference < 0), prefcounts)  # prefers i to j\n            h_ji = dot((preference > 0), prefcounts)  # prefers j to i\n            mat[i, j] = h_ij - h_ji\n            mat[j, i] = h_ji - h_ij\n        return mat", "response": "Builds the mxm matrix. Entry at i j has i > j - i < j"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_t0_nb(times, dom_ids, channel_ids, lookup_tables):\n    dom_id = 0\n    lookup = np.empty((31, 9))\n    for i in range(len(times)):\n        cur_dom_id = dom_ids[i]\n        if cur_dom_id != dom_id:\n            dom_id = cur_dom_id\n            for (d, m) in lookup_tables:\n                if d == dom_id:\n                    np.copyto(lookup, m)\n        t0 = lookup[channel_ids[i]][6]\n        times[i] += t0", "response": "Apply t0s using a lookup table of tuples ( dom_id calib"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_calibration_for_hits(hits, lookup):\n    n = len(hits)\n    cal = np.empty((n, 9))\n    for i in range(n):\n        calib = lookup[hits['dom_id'][i]][hits['channel_id'][i]]\n        cal[i] = calib\n    dir_x = cal[:, 3]\n    dir_y = cal[:, 4]\n    dir_z = cal[:, 5]\n    du = cal[:, 7]\n    floor = cal[:, 8]\n    pos_x = cal[:, 0]\n    pos_y = cal[:, 1]\n    pos_z = cal[:, 2]\n\n    t0 = cal[:, 6]\n\n    return [dir_x, dir_y, dir_z, du, floor, pos_x, pos_y, pos_z, t0]", "response": "Return the calibration for a given list of hits."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the calibration for the given list of hits.", "response": "def _get_calibration_for_mchits(hits, lookup):\n    \"\"\"Append the position, direction and t0 columns and add t0 to time\"\"\"\n    n_hits = len(hits)\n    cal = np.empty((n_hits, 9))\n    for i in range(n_hits):\n        cal[i] = lookup[hits['pmt_id'][i]]\n    dir_x = cal[:, 3]\n    dir_y = cal[:, 4]\n    dir_z = cal[:, 5]\n    du = cal[:, 7]\n    floor = cal[:, 8]\n    pos_x = cal[:, 0]\n    pos_y = cal[:, 1]\n    pos_z = cal[:, 2]\n    t0 = cal[:, 6]\n\n    return [dir_x, dir_y, dir_z, du, floor, pos_x, pos_y, pos_z, t0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the calibration to the given DataFrame.", "response": "def apply(self, hits, no_copy=False):\n        \"\"\"Add x, y, z, t0 (and du, floor if DataFrame) columns to the hits.\n\n        \"\"\"\n        if not no_copy:\n            hits = hits.copy()\n        if istype(hits, 'DataFrame'):\n            # do we ever see McHits here?\n            hits = Table.from_template(hits, 'Hits')\n        if hasattr(hits, 'dom_id') and hasattr(hits, 'channel_id'):\n            dir_x, dir_y, dir_z, du, floor, pos_x, pos_y, pos_z, t0 = _get_calibration_for_hits(\n                hits, self._calib_by_dom_and_channel\n            )\n\n            if hasattr(hits, 'time'):\n                if hits.time.dtype != t0.dtype:\n                    time = hits.time.astype('f4') + t0.astype('f4')\n                    hits = hits.drop_columns(['time'])\n                    hits = hits.append_columns(['time'], [time])\n                else:\n                    hits.time += t0\n\n            hits_data = {}\n            for colname in hits.dtype.names:\n                hits_data[colname] = hits[colname]\n            calib = {\n                'dir_x': dir_x,\n                'dir_y': dir_y,\n                'dir_z': dir_z,\n                'du': du.astype(np.uint8),\n                'floor': du.astype(np.uint8),\n                'pos_x': pos_x,\n                'pos_y': pos_y,\n                'pos_z': pos_z,\n                't0': t0,\n            }\n            hits_data.update(calib)\n            return Table(\n                hits_data,\n                h5loc=hits.h5loc,\n                split_h5=hits.split_h5,\n                name=hits.name\n            )\n\n        elif hasattr(hits, 'pmt_id'):\n            dir_x, dir_y, dir_z, du, floor, pos_x, pos_y, pos_z, t0 = _get_calibration_for_mchits(\n                hits, self._calib_by_pmt_id\n            )\n            if hasattr(hits, 'time'):\n                if hits.time.dtype != t0.dtype:\n                    time = hits.time.astype('f4') + t0.astype('f4')\n                    hits = hits.drop_columns(['time'])\n                    hits = hits.append_columns(['time'], [time])\n                else:\n                    hits.time += t0\n\n            hits_data = {}\n            for colname in hits.dtype.names:\n                hits_data[colname] = hits[colname]\n            calib = {\n                'dir_x': dir_x,\n                'dir_y': dir_y,\n                'dir_z': dir_z,\n                'du': du.astype(np.uint8),\n                'floor': du.astype(np.uint8),\n                'pos_x': pos_x,\n                'pos_y': pos_y,\n                'pos_z': pos_z,\n                't0': t0,\n            }\n            hits_data.update(calib)\n            return Table(\n                hits_data,\n                h5loc=hits.h5loc,\n                split_h5=hits.split_h5,\n                name=hits.name\n            )\n        else:\n            raise TypeError(\n                \"Don't know how to apply calibration to '{0}'. \"\n                \"We need at least 'dom_id' and 'channel_id', or \"\n                \"'pmt_id'.\".format(hits.name)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the corpus blob and returns a new blob entries for the given keys", "response": "def parse_corant(blob):\n    \"\"\"Creates new blob entries for the given blob keys\"\"\"\n\n    if 'track_seamuon' in blob.keys():\n\n        muon = blob['track_seamuon']\n\n        blob['Muon'] = Table({\n            'id': np.array(muon)[:, 0].astype(int),\n            'pos_x': np.array(muon)[:, 1],\n            'pos_y': np.array(muon)[:, 2],\n            'pos_z': np.array(muon)[:, 3],\n            'dir_x': np.array(muon)[:, 4],\n            'dir_y': np.array(muon)[:, 5],\n            'dir_z': np.array(muon)[:, 6],\n            'energy': np.array(muon)[:, 7],\n            'time': np.array(muon)[:, 8],\n            'particle_id': np.array(muon)[:, 9].astype(int),\n            'is_charm': np.array(muon)[:, 10].astype(int),\n            'mother_pid': np.array(muon)[:, 11].astype(int),\n            'grandmother_pid': np.array(muon)[:, 11].astype(int),\n        },\n                             h5loc='muon')\n\n        blob['MuonMultiplicity'] = Table({\n            'muon_multiplicity': len(np.array(muon)[:, 6])\n        },\n                                         h5loc='muon_multiplicity')\n\n    if 'track_seaneutrino' in blob.keys():\n\n        nu = blob['track_seaneutrino']\n\n        blob['Neutrino'] = Table({\n            'id': np.array(nu)[:, 0].astype(int),\n            'pos_x': np.array(nu)[:, 1],\n            'pos_y': np.array(nu)[:, 2],\n            'pos_z': np.array(nu)[:, 3],\n            'dir_x': np.array(nu)[:, 4],\n            'dir_y': np.array(nu)[:, 5],\n            'dir_z': np.array(nu)[:, 6],\n            'energy': np.array(nu)[:, 7],\n            'time': np.array(nu)[:, 8],\n            'particle_id': np.array(nu)[:, 9].astype(int),\n            'is_charm': np.array(nu)[:, 10].astype(int),\n            'mother_pid': np.array(nu)[:, 11].astype(int),\n            'grandmother_pid': np.array(nu)[:, 11].astype(int),\n        },\n                                 h5loc='nu')\n        blob['NeutrinoMultiplicity'] = Table({\n            'total': len(np.array(nu)[:, 6]),\n            'nue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 66]),\n            'anue': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 67]),\n            'numu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 68]),\n            'anumu': len(np.array(nu)[:, 6][np.array(nu)[:, 9] == 69]),\n        },\n                                             h5loc='nu_multiplicity')\n\n    if ('track_seamuon' or 'track_seaneutrino') in blob.keys():\n\n        blob['Weights'] = Table({\n            'w1': blob['weights'][0][0],\n            'w2': blob['weights'][0][1],\n            'w3': blob['weights'][0][2],\n        },\n                                h5loc='weights')\n\n    if 'track_primary' in blob.keys():\n\n        primary = blob['track_primary']\n\n        blob['Primary'] = Table({\n            'id': np.array(primary)[:, 0].astype(int),\n            'pos_x': np.array(primary)[:, 1],\n            'pos_y': np.array(primary)[:, 2],\n            'pos_z': np.array(primary)[:, 3],\n            'dir_x': np.array(primary)[:, 4],\n            'dir_y': np.array(primary)[:, 5],\n            'dir_z': np.array(primary)[:, 6],\n            'energy': np.array(primary)[:, 7],\n            'time': np.array(primary)[:, 8],\n            'particle_id': np.array(primary)[:, 9].astype(int)\n        },\n                                h5loc='primary')\n\n    return blob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_file_index_str(self):\n        file_index = str(self.file_index)\n        if self.n_digits is not None:\n            file_index = file_index.zfill(self.n_digits)\n        return file_index", "response": "Create a string out of the current file index"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_header(self):\n        self.log.info(\"Extracting the header\")\n        raw_header = self.raw_header = defaultdict(list)\n        first_line = self.blob_file.readline()\n        first_line = try_decode_string(first_line)\n        self.blob_file.seek(0, 0)\n        if not first_line.startswith(str('start_run')):\n            self.log.warning(\"No header found.\")\n            return raw_header\n        for line in iter(self.blob_file.readline, ''):\n            line = try_decode_string(line)\n            line = line.strip()\n            try:\n                tag, value = str(line).split(':')\n            except ValueError:\n                continue\n            raw_header[tag].append(str(value).split())\n            if line.startswith(str('end_event:')):\n                self._record_offset()\n                if self._auto_parse and 'physics' in raw_header:\n                    parsers = [p[0].lower() for p in raw_header['physics']]\n                    self._register_parsers(parsers)\n                return raw_header\n        raise ValueError(\"Incomplete header, no 'end_event' tag found!\")", "response": "Extracts the EVT header information from the file and returns the dictionary with the EVT header information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_blob(self, index):\n        self.log.info(\"Retrieving blob #{}\".format(index))\n        if index > len(self.event_offsets) - 1:\n            self.log.info(\"Index not in cache, caching offsets\")\n            self._cache_offsets(index, verbose=False)\n        self.blob_file.seek(self.event_offsets[index], 0)\n        blob = self._create_blob()\n        if blob is None:\n            self.log.info(\"Empty blob created...\")\n            raise IndexError\n        else:\n            self.log.debug(\"Applying parsers...\")\n            for parser in self.parsers:\n                parser(blob)\n            self.log.debug(\"Returning the blob\")\n            return blob", "response": "Return a blob with the event at the given index"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npump the next blob to the modules", "response": "def process(self, blob=None):\n        \"\"\"Pump the next blob to the modules\"\"\"\n        try:\n            blob = self.get_blob(self.index)\n\n        except IndexError:\n            self.log.info(\"Got an IndexError, trying the next file\")\n            if (self.basename\n                    or self.filenames) and self.file_index < self.index_stop:\n                self.file_index += 1\n                self.log.info(\"Now at file_index={}\".format(self.file_index))\n                self._reset()\n                self.blob_file.close()\n                self.log.info(\"Resetting blob index to 0\")\n                self.index = 0\n                file_index = self._get_file_index_str()\n                if self.filenames:\n                    self.filename = self.filenames[self.file_index - 1]\n                elif self.basename:\n                    self.filename = \"{}{}{}.evt\"  \\\n                                    .format(self.basename, file_index, self.suffix)\n                self.log.info(\"Next filename: {}\".format(self.filename))\n                self.print(\"Opening {0}\".format(self.filename))\n                self.open_file(self.filename)\n                self.prepare_blobs()\n                try:\n                    blob = self.get_blob(self.index)\n                except IndexError:\n                    self.log.warning(\n                        \"No blob found in file {}\".format(self.filename)\n                    )\n                else:\n                    return blob\n            self.log.info(\"No files left, terminating the pipeline\")\n            raise StopIteration\n\n        self.index += 1\n        return blob"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cache_offsets(self, up_to_index=None, verbose=True):\n        if not up_to_index:\n            if verbose:\n                self.print(\"Caching event file offsets, this may take a bit.\")\n            self.blob_file.seek(0, 0)\n            self.event_offsets = []\n            if not self.raw_header:\n                self.event_offsets.append(0)\n        else:\n            self.blob_file.seek(self.event_offsets[-1], 0)\n        for line in iter(self.blob_file.readline, ''):\n            line = try_decode_string(line)\n            if line.startswith('end_event:'):\n                self._record_offset()\n                if len(self.event_offsets) % 100 == 0:\n                    if verbose:\n                        print('.', end='')\n                    sys.stdout.flush()\n            if up_to_index and len(self.event_offsets) >= up_to_index + 1:\n                return\n        self.event_offsets.pop()    # get rid of the last entry\n        if not up_to_index:\n            self.whole_file_cached = True\n        self.print(\"\\n{0} events indexed.\".format(len(self.event_offsets)))", "response": "Cache all event offsets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _record_offset(self):\n        offset = self.blob_file.tell()\n        self.event_offsets.append(offset)", "response": "Stores the current file pointer position"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_blob(self):\n        blob = None\n        for line in self.blob_file:\n            line = try_decode_string(line)\n            line = line.strip()\n            if line == '':\n                self.log.info(\"Ignoring empty line...\")\n                continue\n            if line.startswith('end_event:') and blob:\n                blob['raw_header'] = self.raw_header\n                return blob\n            try:\n                tag, values = line.split(':')\n            except ValueError:\n                self.log.warning(\"Ignoring corrupt line: {}\".format(line))\n                continue\n            try:\n                values = tuple(split(values.strip(), callback=float))\n            except ValueError:\n                self.log.info(\"Empty value: {}\".format(values))\n            if line.startswith('start_event:'):\n                blob = Blob()\n                blob[tag] = tuple(int(v) for v in values)\n                continue\n            if tag not in blob:\n                blob[tag] = []\n            blob[tag].append(values)", "response": "Parse the next event from the current file position"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef runserver(project_name):\n\t'''\n\tRuns a python cgi server in a subprocess.\n\t'''\n\tDIR = os.listdir(project_name)\n\n\tif 'settings.py' not in DIR:\n\t\traise NotImplementedError('No file called: settings.py found in %s'%project_name)\n\n\tCGI_BIN_FOLDER = os.path.join(project_name, 'cgi', 'cgi-bin')\n\tCGI_FOLDER = os.path.join(project_name, 'cgi')\n\tif not os.path.exists(CGI_BIN_FOLDER):\n\t\tos.makedirs(CGI_BIN_FOLDER)\n\n\tos.chdir(CGI_FOLDER)\n\tsubprocess.Popen(\"python -m http.server --cgi 8000\")", "response": "Runs a python cgi server in a subprocess."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getUtility(self, decision, sample, aggregationMode = \"avg\"):\n\n        utilities = self.getUtilities(decision, sample)\n        if aggregationMode == \"avg\":\n            utility = numpy.mean(utilities)\n        elif aggregationMode == \"min\":\n            utility = min(utilities)\n        elif aggregationMode == \"max\":\n            utility = max(utilities)\n        else:\n            print(\"ERROR: aggregation mode not recognized\")\n            exit()\n        return utility", "response": "Get the utility function of a given decision and sample."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of floats that contains the utilities of every candidate in the current decision.", "response": "def getUtilities(self, decision, orderVector):\n        \"\"\"\n        Returns a floats that contains the utilities of every candidate in the decision.\n\n        :ivar list<int> decision: Contains a list of integer representations of candidates in the \n            current decision.\n        :ivar list<int> orderVector: A list of integer representations for each candidate ordered\n            from most preferred to least.\n        \"\"\"\n        \n        scoringVector = self.getScoringVector(orderVector)\n        utilities = []\n        for alt in decision:\n            altPosition = orderVector.index(alt)\n            utility = float(scoringVector[altPosition])\n            if self.isLoss == True:\n                utility = -1*utility\n            utilities.append(utility)\n        return utilities"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a scoring vector such that the first k candidates recieve 1 point and all others recive 0.", "response": "def getScoringVector(self, orderVector):\n        \"\"\"\n        Returns a scoring vector such that the first k candidates recieve 1 point and all others \n        recive 0  This function is called by getUtilities() which is implemented in the parent\n        class.\n\n        :ivar list<int> orderVector: A list of integer representations for each candidate ordered\n            from most preferred to least.\n        \"\"\"\n\n        scoringVector = []\n        for i in range(0, self.k):\n            scoringVector.append(1)\n        for i in range(self.k, len(orderVector)):\n            scoringVector.append(0)\n        return scoringVector"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the scoring vector for the given orderVector.", "response": "def getScoringVector(self, orderVector):\n        \"\"\"\n        Returns the scoring vector [1,0,0,...,0]. This function is called by getUtilities() \n        which is implemented in the parent class.\n\n        :ivar list<int> orderVector: A list of integer representations for each candidate ordered\n            from most preferred to least.\n        \"\"\"\n        \n        scoringVector = []\n        scoringVector.append(1)\n        for i in range(1, len(orderVector)):\n            scoringVector.append(0)\n        return scoringVector"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of floats that contains the utilities of every candidate in the current decision.", "response": "def getUtilities(self, decision, binaryRelations):\n        \"\"\"\n        Returns a floats that contains the utilities of every candidate in the decision. This was \n        adapted from code written by Lirong Xia.\n\n        :ivar list<int> decision: Contains a list of integer representations of candidates in the \n            current decision.\n        :ivar list<list,int> binaryRelations: A two-dimensional array whose number of rows and \n            colums is equal to the number of candidates. For each pair of candidates, cand1 and\n            cand2, binaryRelations[cand1-1][cand2-1] contains 1 if cand1 is ranked above cand2\n            and 0 otherwise.\n        \"\"\"\n\n        m = len(binaryRelations)\n        utilities = []\n        for cand in decision:\n            tops = [cand-1]\n            index = 0\n            while index < len(tops):\n                s = tops[index]\n                for j in range(m):\n                    if j == s:\n                        continue\n                    if binaryRelations[j][s] > 0:\n                        if j not in tops:\n                            tops.append(j)\n                index += 1\n            if len(tops) <= self.k:\n                if self.isLoss == False:\n                    utilities.append(1.0)\n                elif self.isLoss == True:\n                    utilities.append(-1.0)\n            else:\n                utilities.append(0.0)\n        return utilities"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef db_credentials(self):\n        try:\n            username = self.config.get('DB', 'username')\n            password = self.config.get('DB', 'password')\n        except Error:\n            username = input(\"Please enter your KM3NeT DB username: \")\n            password = getpass.getpass(\"Password: \")\n        return username, password", "response": "Return username and password for the KM3NeT WebDB."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_path(src):  # pragma: no cover\n    res = None\n    while not res:\n        if res is False:\n            print(colored('You must provide a path to an existing directory!', 'red'))\n        print('You need a local clone or release of (a fork of) '\n              'https://github.com/{0}'.format(src))\n        res = input(colored('Local path to {0}: '.format(src), 'green', attrs=['blink']))\n        if res and Path(res).exists():\n            return Path(res).resolve()\n        res = False", "response": "Prompts the user to input a local path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configure(cfgpath=None):\n    cfgpath = Path(cfgpath) \\\n        if cfgpath else Path(user_config_dir(pylexibank.__name__)) / 'config.ini'\n    if not cfgpath.exists():\n        print(\"\"\"\n{0}\n\nYou seem to be running lexibank for the first time.\nYour system configuration will now be written to a config file to be used\nwhenever lexibank is run lateron.\n\"\"\".format(\n            colored('Welcome to lexibank!', 'blue', attrs=['bold', 'reverse'])))\n        if not cfgpath.parent.exists():\n            cfgpath.parent.mkdir(parents=True)\n        cfg = Config()\n        cfg['paths'] = {k: get_path(src) for k, src in REPOS}\n        cfg.write(cfgpath)\n        print(\"\"\"\nConfiguration has been written to:\n{0}\nYou may edit this file to adapt to changes in your system or to reconfigure settings\nsuch as the logging level.\"\"\".format(cfgpath.resolve()))\n    else:\n        cfg = Config.from_file(cfgpath)\n\n    try:\n        cfg.glottolog\n    except (FileNotFoundError, ValueError):\n        raise ParserError('Misconfigured Glottolog path in {0}'.format(cfgpath))\n    if not Path(cfg['paths']['concepticon']).exists():\n        raise ParserError('Misconfigured Concepticon path in {0}'.format(cfgpath))\n\n    # Print the configuration directory for reference:\n    print(\"Using configuration file at:\")\n    print(str(cfgpath) + '\\n')\n    return cfg", "response": "Configure the lexibank with the given configuration file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit_delta_ts(data, time_s):\n\n    data = data / time_s\n\n    xs = np.arange(-20, 21)\n\n    def gaussian(x, mean, sigma, rate, offset):\n        return rate / np.sqrt(2 * np.pi) /  \\\n            sigma * np.exp(-(x - mean)**2 / sigma**2) + offset\n\n    rates = []\n    means = []\n    for combination in data:\n        try:\n            popt, _ = optimize.curve_fit(\n                gaussian, xs, combination, p0=[0, 2, 1000, 20]\n            )\n        except RuntimeError:\n            popt = (0, 0, 0, 0)\n        rates.append(popt[2])\n        means.append(popt[0])\n    return np.array(rates), np.array(means)", "response": "Fits gaussians to delta t for each PMT pair."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a workflow from the database and store in memory.", "response": "def load_workflow(self, workflow_id):\n        \"\"\"\n        Load workflow from the database and store in memory\n        :param workflow_id: The workflow id\n        :return: The workflow\n        \"\"\"\n        with switch_db(WorkflowDefinitionModel, db_alias='hyperstream'):\n            workflow_definition = WorkflowDefinitionModel.objects.get(workflow_id=workflow_id)\n            if not workflow_definition:\n                logging.warn(\"Attempted to load workflow with id {}, but not found\".format(workflow_id))\n\n            workflow = Workflow(\n                workflow_id=workflow_id,\n                name=workflow_definition.name,\n                description=workflow_definition.description,\n                owner=workflow_definition.owner,\n                online=workflow_definition.online,\n                monitor=workflow_definition.monitor\n            )\n\n            for n in workflow_definition.nodes:\n                workflow.create_node(\n                    stream_name=n.stream_name,\n                    channel=self.channel_manager.get_channel(n.channel_id),\n                    plates=[self.plate_manager.plates[p] for p in n.plate_ids])\n\n            for f in workflow_definition.factors:\n                source_nodes = [workflow.nodes[node_id] for node_id in f.sources] if f.sources else []\n                sink_nodes = [workflow.nodes[node_id] for node_id in f.sinks] if f.sinks else []\n                alignment_node = workflow.nodes[f.alignment_node] if f.alignment_node else None\n                splitting_node = workflow.nodes[f.splitting_node] if f.splitting_node else None\n                output_plate = f.output_plate\n\n                parameters = Tool.parameters_from_model(f.tool.parameters)\n                # tool = dict(name=f.tool.name, parameters=parameters)\n                tool = self.channel_manager.get_tool(f.tool.name, parameters, version=None)\n\n                if f.factor_type == \"Factor\":\n                    if len(sink_nodes) != 1:\n                        raise ValueError(\n                            \"Standard factors should have a single sink node, received {}\"\n                            .format(len(sink_nodes)))\n\n                    if splitting_node is not None:\n                        raise ValueError(\n                            \"Standard factors do not support splitting nodes\")\n\n                    if output_plate is not None:\n                        raise ValueError(\n                            \"Standard factors do not support output plates\")\n\n                    workflow.create_factor(\n                        tool=tool,\n                        sources=source_nodes,\n                        sink=sink_nodes[0],\n                        alignment_node=alignment_node\n                    )\n\n                elif f.factor_type == \"MultiOutputFactor\":\n                    if len(source_nodes) > 1:\n                        raise ValueError(\n                            \"MultiOutputFactor factors should have at most one source node, received {}\"\n                            .format(len(source_nodes)))\n\n                    if len(sink_nodes) != 1:\n                        raise ValueError(\n                            \"MultiOutputFactor factors should have a single sink node, received {}\"\n                            .format(len(sink_nodes)))\n\n                    if alignment_node is not None:\n                        raise ValueError(\n                            \"MultiOutputFactor does not support alignment nodes\")\n\n                    if output_plate is not None:\n                        raise ValueError(\n                            \"MultiOutputFactor does not support output plates\")\n\n                    workflow.create_multi_output_factor(\n                        tool=tool,\n                        source=source_nodes[0] if source_nodes else None,\n                        splitting_node=splitting_node,\n                        sink=sink_nodes[0]\n                    )\n\n                elif f.factor_type == \"NodeCreationFactor\":\n                    if len(source_nodes) > 1:\n                        raise ValueError(\n                            \"NodeCreationFactor factors should no more than one source node, received {}\"\n                            .format(len(source_nodes)))\n\n                    if len(sink_nodes) != 0:\n                        raise ValueError(\n                            \"NodeCreationFactor factors should not have sink nodes\"\n                            .format(len(sink_nodes)))\n\n                    if output_plate is None:\n                        raise ValueError(\n                            \"NodeCreationFactor requires an output plate definition\")\n\n                    workflow.create_node_creation_factor(\n                        tool=tool,\n                        source=source_nodes[0] if source_nodes else None,\n                        output_plate=output_plate.to_mongo().to_dict(),\n                        plate_manager=self.plate_manager\n                    )\n\n                else:\n                    raise NotImplementedError(\"Unsupported factor type {}\".format(f.factor_type))\n\n            self.add_workflow(workflow, False)\n            return workflow"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_workflow(self, workflow, commit=False):\n        if workflow.workflow_id in self.workflows:\n            raise KeyError(\"Workflow with id {} already exists\".format(workflow.workflow_id))\n\n        self.workflows[workflow.workflow_id] = workflow\n        logging.info(\"Added workflow {} to workflow manager\".format(workflow.workflow_id))\n\n        # Optionally also save the workflow to database\n        if commit:\n            self.commit_workflow(workflow.workflow_id)\n        else:\n            self.uncommitted_workflows.add(workflow.workflow_id)", "response": "Adds a new workflow to the database and optionally commits it to the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a workflow from the database.", "response": "def delete_workflow(self, workflow_id):\n        \"\"\"\n        Delete a workflow from the database\n        :param workflow_id:\n        :return: None\n        \"\"\"\n        deleted = False\n\n        with switch_db(WorkflowDefinitionModel, \"hyperstream\"):\n            workflows = WorkflowDefinitionModel.objects(workflow_id=workflow_id)\n            if len(workflows) == 1:\n                workflows[0].delete()\n                deleted = True\n            else:\n                logging.debug(\"Workflow with id {} does not exist\".format(workflow_id))\n\n        with switch_db(WorkflowStatusModel, \"hyperstream\"):\n            workflows = WorkflowStatusModel.objects(workflow_id=workflow_id)\n            if len(workflows) == 1:\n                workflows[0].delete()\n                deleted = True\n            else:\n                logging.debug(\"Workflow status with id {} does not exist\".format(workflow_id))\n\n        if workflow_id in self.workflows:\n            del self.workflows[workflow_id]\n            deleted = True\n\n        if deleted:\n            logging.info(\"Deleted workflow with id {}\".format(workflow_id))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommit the workflow to the database.", "response": "def commit_workflow(self, workflow_id):\n        \"\"\"\n        Commit the workflow to the database\n        :param workflow_id: The workflow id\n        :return: None\n        \"\"\"\n        # TODO: We should also be committing the Stream definitions if there are new ones\n\n        workflow = self.workflows[workflow_id]\n\n        with switch_db(WorkflowDefinitionModel, \"hyperstream\"):\n            workflows = WorkflowDefinitionModel.objects(workflow_id=workflow_id)\n            if len(workflows) > 0:\n                logging.warn(\"Workflow with id {} already exists in database\".format(workflow_id))\n                return\n\n            factors = []\n            for f in workflow.factors:\n                tool = f.tool.get_model()\n\n                if isinstance(f, Factor):\n                    sources = [s.node_id for s in f.sources] if f.sources else []\n                    sinks = [f.sink.node_id]\n                    alignment_node = f.alignment_node.node_id if f.alignment_node else None\n                    splitting_node = None\n                    output_plate = None\n\n                elif isinstance(f, MultiOutputFactor):\n                    sources = [f.source.node_id] if f.source else []\n                    sinks = [f.sink.node_id]\n                    alignment_node = None\n                    splitting_node = f.splitting_node.node_id if f.splitting_node else None\n                    output_plate = None\n\n                elif isinstance(f, NodeCreationFactor):\n                    sources = [f.source.node_id] if f.source else []\n                    sinks = []\n                    alignment_node = None\n                    splitting_node = None\n                    output_plate = f.output_plate\n\n                else:\n                    raise NotImplementedError(\"Unsupported factor type\")\n\n                if output_plate:\n                    output_plate_copy = deepcopy(output_plate)\n                    if 'parent_plate' in output_plate_copy:\n                        del output_plate_copy['parent_plate']\n                else:\n                    output_plate_copy = None\n\n                factor = FactorDefinitionModel(\n                    tool=tool,\n                    factor_type=f.__class__.__name__,\n                    sources=sources,\n                    sinks=sinks,\n                    alignment_node=alignment_node,\n                    splitting_node=splitting_node,\n                    output_plate=output_plate_copy\n                )\n\n                factors.append(factor)\n\n            nodes = []\n            for n in workflow.nodes.values():\n                nodes.append(NodeDefinitionModel(\n                    stream_name=n.node_id,\n                    plate_ids=n.plate_ids,\n                    channel_id=n._channel.channel_id\n                ))\n\n            workflow_definition = WorkflowDefinitionModel(\n                workflow_id=workflow.workflow_id,\n                name=workflow.name,\n                description=workflow.description,\n                nodes=nodes,\n                factors=factors,\n                owner=workflow.owner,\n                online=workflow.online,\n                monitor=workflow.monitor\n            )\n\n            workflow_definition.save()\n\n        with switch_db(WorkflowStatusModel, db_alias='hyperstream'):\n            workflow_status = WorkflowStatusModel(\n                workflow_id=workflow.workflow_id,\n                last_updated=utcnow(),\n                last_accessed=utcnow(),\n                requested_intervals=[]\n            )\n\n            workflow_status.save()\n\n        self.uncommitted_workflows.remove(workflow_id)\n        logging.info(\"Committed workflow {} to database\".format(workflow_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the requested intervals for a given workflow.", "response": "def set_requested_intervals(self, workflow_id, requested_intervals):\n        \"\"\"\n        Sets the requested intervals for a given workflow\n        :param workflow_id: The workflow id\n        :param requested_intervals: The requested intervals\n        :return: None\n        :type requested_intervals: TimeIntervals\n        \"\"\"\n        if workflow_id not in self.workflows:\n            raise ValueError(\"Workflow {} not found\".format(workflow_id))\n\n        self.workflows[workflow_id].requested_intervals = requested_intervals"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the requested intervals for all workflowCOOKIEs.", "response": "def set_all_requested_intervals(self, requested_intervals):\n        \"\"\"\n        Sets the requested intervals for all workflow\n        :param requested_intervals: The requested intervals\n        :return: None\n        :type requested_intervals: TimeIntervals\n        \"\"\"\n        for workflow_id in self.workflows:\n            if self.workflows[workflow_id].online:\n                self.workflows[workflow_id].requested_intervals = requested_intervals"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the tool over the given time interval.", "response": "def execute(self, sources, sink, interval, alignment_stream=None):\n        \"\"\"\n        Execute the tool over the given time interval.\n        If an alignment stream is given, the output instances will be aligned to this stream\n\n        :param sources: The source streams (possibly None)\n        :param sink: The sink stream\n        :param alignment_stream: The alignment stream\n        :param interval: The time interval\n        :type sources: list[Stream] | tuple[Stream] | None\n        :type sink: Stream\n        :type alignment_stream: Stream | None\n        :type interval: TimeInterval\n        :return: None\n        \"\"\"\n        if not isinstance(interval, TimeInterval):\n            raise TypeError('Expected TimeInterval, got {}'.format(type(interval)))\n        # logging.info(self.message(interval))\n\n        if interval.end > sink.channel.up_to_timestamp:\n            raise StreamNotAvailableError(sink.channel.up_to_timestamp)\n\n        required_intervals = TimeIntervals([interval]) - sink.calculated_intervals\n\n        if not required_intervals.is_empty:\n            document_count = 0\n\n            for interval in required_intervals:\n                for stream_instance in self._execute(\n                        sources=sources, alignment_stream=alignment_stream, interval=interval):\n                    sink.writer(stream_instance)\n                    document_count += 1\n                sink.calculated_intervals += interval\n\n            required_intervals = TimeIntervals([interval]) - sink.calculated_intervals\n            if not required_intervals.is_empty:\n                # raise ToolExecutionError(required_intervals)\n                logging.error(\"{} execution error for time interval {} on stream {}\".format(\n                    self.name, interval, sink))\n\n            if not document_count:\n                logging.debug(\"{} did not produce any data for time interval {} on stream {}\".format(\n                    self.name, interval, sink))\n\n            self.write_to_history(\n                interval=interval,\n                tool=self.name,\n                document_count=document_count\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_stream(self, stream_id, sandbox=None):\n        if stream_id in self.streams:\n            raise StreamAlreadyExistsError(\"Stream with id '{}' already exists\".format(stream_id))\n\n        if sandbox is not None:\n            raise ValueError(\"Cannot use sandboxes with memory streams\")\n\n        stream = Stream(channel=self, stream_id=stream_id, calculated_intervals=None, sandbox=None)\n        \n        self.streams[stream_id] = stream\n        self.data[stream_id] = StreamInstanceCollection()\n        return stream", "response": "Create a new stream with the given id and return its unique version of the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef purge_all(self, remove_definitions=False):\n        for stream_id in list(self.streams.keys()):\n            self.purge_stream(stream_id, remove_definition=remove_definitions)", "response": "Clears all streams in the channel - use with caution!"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef purge_stream(self, stream_id, remove_definition=False, sandbox=None):\n\n        if sandbox is not None:\n            raise NotImplementedError\n\n        if stream_id not in self.streams:\n            raise StreamNotFoundError(stream_id)\n\n        self.data[stream_id] = StreamInstanceCollection()\n        self.streams[stream_id].calculated_intervals = TimeIntervals()\n\n        if remove_definition:\n            del self.data[stream_id]\n            del self.streams[stream_id]", "response": "Clears all the data in a given stream and the calculated intervals in the given sandbox."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_results(self, stream, time_interval):\n        return [StreamInstance(t, self.data[stream.stream_id][t])\n                for t in sorted(self.data[stream.stream_id]) if t in time_interval]", "response": "Calculates the items in the time interval determined by the stream"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the state of the channel to up to date.", "response": "def update_state(self, up_to_timestamp):\n        \"\"\"\n        Call this function to ensure that the channel is up to date at the time of timestamp.\n        I.e., all the streams that have been created before or at that timestamp are calculated exactly until\n        up_to_timestamp.\n        \"\"\"\n        for stream_id in self.streams:\n            self.streams[stream_id].calculated_intervals = TimeIntervals([(MIN_DATE, up_to_timestamp)])\n        self.up_to_timestamp = up_to_timestamp"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncompiles regex from pattern and pattern_dict", "response": "def compile_regex(self, pattern, flags=0):\n        \"\"\"Compile regex from pattern and pattern_dict\"\"\"\n        pattern_re = regex.compile('(?P<substr>%\\{(?P<fullname>(?P<patname>\\w+)(?::(?P<subname>\\w+))?)\\})')\n        while 1:\n            matches = [md.groupdict() for md in pattern_re.finditer(pattern)]\n            if len(matches) == 0:\n                break\n            for md in matches:\n                if md['patname'] in self.pattern_dict:\n                    if md['subname']:\n                        # TODO error if more than one occurance\n                        if '(?P<' in self.pattern_dict[md['patname']]:\n                            # this is not part of the original logstash implementation \n                            # but it might be useful to be able to replace the\n                            # group name used in the pattern\n                            repl = regex.sub('\\(\\?P<(\\w+)>', '(?P<%s>' % md['subname'],\n                                self.pattern_dict[md['patname']], 1)\n                        else:\n                            repl = '(?P<%s>%s)' % (md['subname'], \n                                self.pattern_dict[md['patname']])\n                    else:\n                        repl = self.pattern_dict[md['patname']]\n                    # print \"Replacing %s with %s\"  %(md['substr'], repl)\n                    pattern = pattern.replace(md['substr'], repl)\n                else:\n                    # print('patname not found')\n                    # maybe missing path entry or missing pattern file?\n                    return\n        # print 'pattern: %s' % pattern\n        return regex.compile(pattern, flags)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _load_patterns(self, folders, pattern_dict=None):\n        if pattern_dict is None:\n            pattern_dict = {}\n        for folder in folders:\n            for file in os.listdir(folder):\n                if regex.match('^[\\w-]+$', file):\n                    self._load_pattern_file(os.path.join(folder, file), pattern_dict)\n        return pattern_dict", "response": "Load all pattern from all the files in folders"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_times():\n    if f.root.stopped:\n        return copy.deepcopy(f.root.times)\n    else:\n        t = timer()\n        times = collapse.collapse_times()\n        f.root.self_cut += timer() - t\n        return times", "response": "Produce a deepcopy of the current timing data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attach_subdivision(times):\n    t = timer()\n    if not isinstance(times, Times):\n        raise TypeError(\"Expected Times object for param 'times'.\")\n    assert times.total > 0., \"Attached subdivision has total time 0, appears empty.\"\n    name = times.name\n    f.r.self_agg += times.self_agg\n    if name not in f.t.subdvsn_awaiting:\n        times_copy = copy.deepcopy(times)\n        times_copy.parent = f.r\n        f.t.subdvsn_awaiting[name] = times_copy\n    else:\n        merge.merge_times(f.t.subdvsn_awaiting[name], times)\n    f.t.self_cut += timer() - t", "response": "This function is used to attach a new subdivision to a running n - times object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize and / or save a Times object using pickle.", "response": "def save_pkl(filename=None, times=None):\n    \"\"\"\n    Serialize and / or save a Times data object using pickle (cPickle).\n\n    Args:\n        filename (None, optional): Filename to dump to. If not provided,\n            returns serialized object.\n        times (None, optional): object to dump.  If non provided, uses\n            current root.\n\n    Returns:\n        pkl: Pickled Times data object, only if no filename provided.\n\n    Raises:\n        TypeError: If 'times' is not a Times object or a list of tuple of\n            them.\n    \"\"\"\n    if times is None:\n        if not f.root.stopped:\n            times = collapse.collapse_times()\n        else:\n            times = f.root.times\n    else:\n        if isinstance(times, (list, tuple)):\n            for t in times:\n                if not isinstance(t, Times):\n                    raise TypeError(\"Expected single Times instance or list/tuple of Times instances for param 'times'.\")\n        elif not isinstance(times, Times):\n            raise TypeError(\"Expected single Times instance or list/tuple of Times instances for param 'times'.\")\n    if filename is not None:\n        with open(str(filename), 'wb') as file:\n            pickle.dump(times, file)\n    else:\n        return pickle.dumps(times)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a single or multiple Times objects from a list of filenames.", "response": "def load_pkl(filenames):\n    \"\"\"\n    Unpickle file contents.\n\n    Args:\n        filenames (str): Can be one or a list or tuple of filenames to retrieve.\n\n    Returns:\n        Times: A single object, or from a collection of filenames, a list of Times objects.\n\n    Raises:\n        TypeError: If any loaded object is not a Times object.\n    \"\"\"\n    if not isinstance(filenames, (list, tuple)):\n        filenames = [filenames]\n    times = []\n    for name in filenames:\n        name = str(name)\n        with open(name, 'rb') as file:\n            loaded_obj = pickle.load(file)\n            if not isinstance(loaded_obj, Times):\n                raise TypeError(\"At least one loaded object is not a Times data object.\")\n            times.append(loaded_obj)\n    return times if len(times) > 1 else times[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts string to int or float.", "response": "def _to_number(cls, string):\n        \"\"\"Convert string to int or float.\"\"\"\n        try:\n            if float(string) - int(string) == 0:\n                return int(string)\n            return float(string)\n        except ValueError:\n            try:\n                return float(string)\n            except ValueError:\n                return string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving next 5 days forecast.", "response": "async def forecast(self, globalIdLocal):\n        \"\"\"Retrieve next 5 days forecast.\"\"\"\n\n        data = await self.retrieve(API_FORECAST + \"{globalIdLocal}.json\".\n                                   format(globalIdLocal=globalIdLocal))\n\n        if not self.weather_type:\n            await self.weather_type_classe()\n\n        if not self.wind_type:\n            await self.wind_type_classe()\n\n        _forecasts = []\n        for forecast in data['data']:\n            Forecast = namedtuple('Forecast', list(forecast.keys())+['description'])\n            _description = self.weather_type[forecast['idWeatherType']]\n            if forecast['classWindSpeed'] != -99.0:\n                _description += \", com vento \"+ self.wind_type[forecast['classWindSpeed']] +\\\n                                \" de \" + WIND_DIRECTION[forecast['predWindDir']]\n            vals = [self._to_number(v) for v in forecast.values()] + [_description]\n            _forecasts.append(Forecast(*vals))\n        return _forecasts"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving translation for weather type.", "response": "async def weather_type_classe(self):\n        \"\"\"Retrieve translation for weather type.\"\"\"\n\n        data = await self.retrieve(url=API_WEATHER_TYPE)\n\n        self.weather_type = dict()\n\n        for _type in data['data']:\n            self.weather_type[_type['idWeatherType']] = _type['descIdWeatherTypePT']\n\n        return self.weather_type"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def wind_type_classe(self):\n\n        data = await self.retrieve(url=API_WIND_TYPE)\n\n        self.wind_type = dict()\n\n        for _type in data['data']:\n            self.wind_type[int(_type['classWindSpeed'])] = _type['descClassWindSpeedDailyPT']\n\n        return self.wind_type", "response": "Retrieve translation for wind type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve current weather observation.", "response": "async def observations(self):\n        \"\"\"Retrieve current weather observation.\"\"\"\n        observations = []\n\n        raw_stations = await self.retrieve(url=API_OBSERVATION_STATIONS,\n                                           headers={'Referer': 'http://www.ipma.pt'})\n        if not raw_stations:\n            return observations \n\n        raw_observations = await self.retrieve(url=API_OBSERVATION_OBSERVATIONS,\n                                               headers={'Referer': 'http://www.ipma.pt'})\n        if not raw_observations:\n            return observations \n\n        Station = namedtuple('ObservationStation', ['latitude', 'longitude', 'stationID',\n                                                    'stationName', 'currentObs'])\n\n        Observation = namedtuple('Observation', ['temperature', 'humidity',\n                                                 'windspeed', 'winddirection',\n                                                 'precipitation', 'pressure',\n                                                 'description'])\n\n        last_observation = sorted(raw_observations.keys())[-1]\n\n        for station in raw_stations:\n            _station = raw_observations[last_observation][str(station.get('properties').get('idEstacao'))]\n\n            if _station is None:\n                continue\n\n            _observation = Observation(\n                _station['temperatura'],\n                _station['humidade'],\n                _station['intensidadeVentoKM'] if _station['intensidadeVentoKM'] != -99.0 else None,\n                WIND_DIRECTION[WIND_DIRECTION_ID[_station['idDireccVento']]],\n                _station['precAcumulada'] if _station['precAcumulada'] != -99.0 else None,\n                _station['pressao'] if _station['pressao'] != -99.0 else None,\n                \"{} @ {}\".format(station.get('properties').get('localEstacao'), last_observation),\n                )\n\n            _station = Station(\n                station.get('geometry').get('coordinates')[1],\n                station.get('geometry').get('coordinates')[0],\n                station.get('properties').get('idEstacao'),\n                station.get('properties').get('localEstacao'),\n                _observation)\n\n            observations.append(_station)\n        return observations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register(self, plugin):\n        for listener in plugin.listeners:\n            self.listeners[listener].add(plugin)\n        self.plugins.add(plugin)\n        plugin.messenger = self.messages\n        plugin.start()", "response": "Register a new plugin to our set of listeners and messages Queue for communication."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends APP_START to any plugins that listen for it and loop around the messages waiting for them to be sent to their listening plugins until the application is stopped.", "response": "def start(self):\n        \"\"\"Send 'APP_START' to any plugins that listen for it, and loop around\n        waiting for messages and sending them to their listening plugins until\n        it's time to shutdown.\n        \"\"\"\n        self.recieve('APP_START')\n        self.alive = True\n        while self.alive:\n            message, payload = self.messages.get()\n            if message == 'APP_STOP':\n                for plugin in self.plugins:\n                    plugin.recieve('SHUTDOWN')\n                self.alive = False\n            else:\n                self.recieve(message, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh_styles(self):\n        import matplotlib.pyplot as plt\n\n        self.colours = {}\n        for style in plt.style.available:\n            try:\n                style_colours = plt.style.library[style]['axes.prop_cycle']\n                self.colours[style] = [c['color'] for c in list(style_colours)]\n            except KeyError:\n                continue\n\n        self.colours['km3pipe'] = [\n            \"#ff7869\", \"#4babe1\", \"#96ad3e\", \"#e4823d\", \"#5d72b2\", \"#e2a3c2\",\n            \"#fd9844\", \"#e480e7\"\n        ]", "response": "Load all available styles and update colours"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating argument parser for the CLI.", "response": "def _create_parser() -> ArgumentParser:\n    \"\"\"\n    Creates argument parser for the CLI.\n    :return: the argument parser\n    \"\"\"\n    parser = ArgumentParser(prog=EXECUTABLE_NAME, description=f\"{DESCRIPTION} (v{VERSION})\")\n    parser.add_argument(\n        f\"-{VERBOSE_SHORT_PARAMETER}\", action=\"count\", default=0,\n        help=\"increase the level of log verbosity (add multiple increase further)\")\n    subparsers = parser.add_subparsers(dest=ACTION_CLI_PARAMETER_ACCESS, help=\"action\")\n\n    unlock_subparser = subparsers.add_parser(Action.UNLOCK.value, help=\"release a lock\")\n    unlock_subparser.add_argument(\n        f\"-{REGEX_KEY_ENABLED_SHORT_PARAMETER}\", action=\"store_true\", default=DEFAULT_REGEX_KEY_ENABLED,\n        help=\"whether the key should be treated as a regular expression and to release all matching locks\")\n\n    lock_subparser = subparsers.add_parser(Action.LOCK.value, help=\"acquire a lock\")\n    lock_and_execute_subparser = subparsers.add_parser(Action.EXECUTE.value, help=\"call executable whilst holding lock\")\n\n    for subparser in (lock_subparser, lock_and_execute_subparser):\n        subparser.add_argument(\n            f\"--{SESSION_TTL_LONG_PARAMETER}\", type=float, default=DEFAULT_SESSION_TTL,\n            help=f\"time to live (ttl) in seconds of the session that will be created to hold the lock. Must be between \"\n                 f\"{MIN_LOCK_TIMEOUT_IN_SECONDS}s and {MAX_LOCK_TIMEOUT_IN_SECONDS}s (inclusive). If set to \"\n                 f\"{NO_EXPIRY_SESSION_TTL_CLI_PARAMETER_VALUE}, the session will not expire\")\n        subparser.add_argument(\n            f\"--{NON_BLOCKING_LONG_PARAMETER}\", action=\"store_true\",\n            default=DEFAULT_NON_BLOCKING, help=\"do not block if cannot lock straight away\")\n        subparser.add_argument(\n            f\"--{TIMEOUT_LONG_PARAMETER}\", default=DEFAULT_TIMEOUT, type=float,\n            help=\"give up trying to acquire the key after this many seconds (where 0 is never)\")\n        subparser.add_argument(\n            f\"--{METADATA_LONG_PARAMETER}\", default=DEFAULT_METADATA, type=str, action=_ParseJsonAction,\n            help=\"additional metadata to add to the lock information (will be converted to JSON)\")\n        subparser.add_argument(\n            f\"--{ON_BEFORE_LOCK_LONG_PARAMETER}\", default=[], type=str, nargs=\"+\", action=\"append\",\n            help=\"path to executable that is to be called before an attempt is made to acquire a lock, where the lock \"\n                 \"key is passed as the first argument. Any failures of this executable are ignored\")\n        subparser.add_argument(\n            f\"--{ON_LOCK_ALREADY_LOCKED_LONG_PARAMETER}\", default=[], type=str, nargs=\"+\", action=\"append\",\n            help=\"path to executable that is to be called after an attempt has been made to acquire a lock but failed \"\n                 \"due to the lock already been taken, where the lock key is passed as the first argument. Any failures \"\n                 \"of this executable are ignored\")\n        subparser.add_argument(\n            f\"-{LOCK_POLL_INTERVAL_SHORT_PARAMETER}\", default=DEFAULT_LOCK_POLL_INTERVAL_GENERATOR(1), type=float,\n            help=\"number of seconds between polls to acquire a locked lock\")\n\n    # XXX: probably a better way of iterating subparsers on `subparsers`\n    for subparser in [unlock_subparser, lock_subparser, lock_and_execute_subparser]:\n        subparser.add_argument(\n            KEY_PARAMETER, type=str, help=\"the lock identifier\")\n\n    lock_and_execute_subparser.add_argument(\n        EXECUTABLE_PARAMETER, type=str, help=\"to execute in shell\")\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_cli_configuration(arguments: List[str]) -> CliConfiguration:\n    try:\n        parsed_arguments = {x.replace(\"_\", \"-\"): y for x, y in vars(_argument_parser.parse_args(arguments)).items()}\n    except SystemExit as e:\n        if e.code == SUCCESS_EXIT_CODE:\n            raise e\n        raise InvalidCliArgumentError() from e\n\n    parsed_action = parsed_arguments[ACTION_CLI_PARAMETER_ACCESS]\n    if parsed_action is None:\n        _argument_parser.print_help()\n        exit(INVALID_CLI_ARGUMENT_EXIT_CODE)\n    action = Action(parsed_action)\n\n    session_ttl = parsed_arguments.get(SESSION_TTL_LONG_PARAMETER, None)\n    if session_ttl == NO_EXPIRY_SESSION_TTL_CLI_PARAMETER_VALUE:\n        session_ttl = None\n\n    shared_parameters = dict(\n        key=parsed_arguments[KEY_PARAMETER],\n        log_verbosity=_get_verbosity(parsed_arguments), session_ttl=session_ttl)\n\n    if action == Action.UNLOCK:\n        return CliUnlockConfiguration(\n            **shared_parameters,\n            regex_key_enabled=parsed_arguments.get(REGEX_KEY_ENABLED_SHORT_PARAMETER, DEFAULT_REGEX_KEY_ENABLED))\n    else:\n        parameters = dict(\n            **shared_parameters,\n            non_blocking=parsed_arguments.get(NON_BLOCKING_LONG_PARAMETER, DEFAULT_NON_BLOCKING),\n            timeout=parsed_arguments.get(TIMEOUT_LONG_PARAMETER, DEFAULT_TIMEOUT),\n            metadata=parsed_arguments.get(METADATA_LONG_PARAMETER, DEFAULT_METADATA),\n            on_before_locked_executables=list(itertools.chain(*parsed_arguments.get(\n                ON_BEFORE_LOCK_LONG_PARAMETER, []))),\n            on_lock_already_locked_executables=list(itertools.chain(*parsed_arguments.get(\n                ON_LOCK_ALREADY_LOCKED_LONG_PARAMETER, []))),\n            lock_poll_interval=parsed_arguments.get(\n                LOCK_POLL_INTERVAL_SHORT_PARAMETER, DEFAULT_LOCK_POLL_INTERVAL_GENERATOR(1)))\n\n        if action == Action.LOCK:\n            return CliLockConfiguration(**parameters)\n        else:\n            return CliLockAndExecuteConfiguration(\n                **parameters,\n                executable=parsed_arguments[EXECUTABLE_PARAMETER])", "response": "Parses the CLI arguments and returns a CliConfiguration object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the verbosity level from the parsed arguments.", "response": "def _get_verbosity(parsed_arguments: Dict) -> int:\n    \"\"\"\n    Gets the verbosity level from the parsed arguments.\n    :param parsed_arguments: the parsed arguments\n    :return: the verbosity level implied\n    \"\"\"\n    verbosity = DEFAULT_LOG_VERBOSITY - (int(parsed_arguments[VERBOSE_SHORT_PARAMETER]) * 10)\n    if verbosity < 10:\n        raise InvalidCliArgumentError(\"Cannot provide any further logging - reduce log verbosity\")\n    assert verbosity <= logging.CRITICAL\n    return verbosity"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a function that will be called when a lock event is received.", "response": "def _generate_event_listener_caller(executables: List[str]) -> LockEventListener:\n    \"\"\"\n    TODO\n    :param executables:\n    :return:\n    \"\"\"\n    def event_listener_caller(key: str):\n        for executable in executables:\n            try:\n                process = subprocess.Popen([executable, key], stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n                output, stderr = process.communicate()\n                if len(stderr) > 0:\n                    logger.info(f\"stderr from executing \\\"{executable}\\\": {stderr.decode('utf-8').strip()}\")\n                if process.returncode != 0:\n                    logger.error(f\"Error when executing \\\"{executable}\\\": return code was {process.returncode}\")\n                # Not falling over if event listener does!\n            except OSError as e:\n                common_error_string = f\"Could not execute \\\"{executable}\\\":\"\n                if e.errno == errno.ENOEXEC:\n                    logger.warning(f\"{common_error_string} {e} (perhaps the executable needs a shebang?)\")\n                else:\n                    logger.warning(f\"{common_error_string} {e}\")\n\n    return event_listener_caller"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _acquire_lock(lock_manager: ConsulLockManager, configuration: CliLockConfiguration) \\\n        -> Optional[ConnectedConsulLockInformation]:\n    \"\"\"\n    TODO\n    :param lock_manager:\n    :param configuration:\n    :return:\n    \"\"\"\n    event_listeners: LockEventListener = {}\n    if configuration.on_before_locked_executables is not None:\n        event_listeners[\"on_before_lock\"] = _generate_event_listener_caller(\n            configuration.on_before_locked_executables)\n    if configuration.on_lock_already_locked_executables is not None:\n        event_listeners[\"on_lock_already_locked\"] = _generate_event_listener_caller(\n            configuration.on_lock_already_locked_executables)\n\n    try:\n        return lock_manager.acquire(\n            key=configuration.key, blocking=not configuration.non_blocking,\n            timeout=configuration.timeout, metadata=configuration.metadata, **event_listeners,\n            lock_poll_interval_generator=lambda i: configuration.lock_poll_interval)\n    except LockAcquireTimeoutError as e:\n        logger.debug(e)\n        logger.error(f\"Timed out whilst waiting to acquire lock: {configuration.key}\")\n        print(json.dumps(None))\n        exit(LOCK_ACQUIRE_TIMEOUT_EXIT_CODE)", "response": "TODO\n    :param lock_manager:\n    :param configuration:\n    :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nacquires a lock and executes it.", "response": "def _acquire_lock_and_execute(lock_manager: ConsulLockManager, configuration: CliLockAndExecuteConfiguration):\n    \"\"\"\n    Executes whilst holding a lock, exiting after the execute returns with the executables return code.\n    :param lock_manager: the lock manager\n    :param configuration: the configuration\n    \"\"\"\n    lock = _acquire_lock(lock_manager, configuration)\n    if lock is None:\n        exit(UNABLE_TO_ACQUIRE_LOCK_EXIT_CODE)\n    return_code, _, _ = lock_manager.execute_with_lock(configuration.executable, lock)\n    exit(return_code)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _acquire_lock_and_exit(lock_manager: ConsulLockManager, configuration: CliLockConfiguration):\n    lock = _acquire_lock(lock_manager, configuration)\n    print(json.dumps(lock, cls=ConsulLockInformationJSONEncoder, sort_keys=True))\n\n    if lock is None:\n        logger.error(f\"Unable to acquire lock: {configuration.key}\")\n        exit(UNABLE_TO_ACQUIRE_LOCK_EXIT_CODE)\n\n    exit(SUCCESS_EXIT_CODE)", "response": "Acquires a lock then exits."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _release_lock(lock_manager: ConsulLockManager, configuration: CliUnlockConfiguration):\n    if configuration.regex_key_enabled:\n        release_information = sorted(list(lock_manager.release_regex(key_regex=configuration.key)))\n    else:\n        release_information = lock_manager.release(key=configuration.key)\n    print(json.dumps(release_information))\n\n    exit(SUCCESS_EXIT_CODE)", "response": "Unlocks a lock.\n    :param lock_manager: the lock manager\n    :param configuration: the configuration required to unlock the lock"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_file_object(username, password, utc_start=None, utc_stop=None):\n\n    if not utc_start:\n        utc_start = datetime.now()\n\n    if not utc_stop:\n        utc_stop = utc_start + timedelta(days=1)\n\n    logging.info(\"Downloading schedules for username [%s] in range [%s] to \"\n                 \"[%s].\" % (username, utc_start, utc_stop))\n\n    replacements = {'start_time': utc_start.strftime('%Y-%m-%dT%H:%M:%SZ'), \n                    'stop_time':  utc_stop.strftime('%Y-%m-%dT%H:%M:%SZ')}\n    \n    soap_message_xml = (soap_message_xml_template % replacements)\n\n    authinfo = urllib2.HTTPDigestAuthHandler()\n    authinfo.add_password(realm, url, username, password)\n\n    try:\n        request = urllib2.Request(url, soap_message_xml, request_headers)\n        response = urllib2.build_opener(authinfo).open(request)\n\n        if response.headers['Content-Encoding'] == 'gzip':\n            response = GzipStream(response)\n    except:\n        logging.exception(\"Could not acquire connection to Schedules Direct.\")\n        raise\n\n    return response", "response": "Make the connection to the Schedules Direct service and return a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_file_object(file_obj, importer, progress):\n\n    logging.info(\"Processing schedule data.\")\n\n    try:\n        handler = XmlCallbacks(importer, progress)\n        parser = sax.make_parser()\n        parser.setContentHandler(handler)\n        parser.setErrorHandler(handler)\n        parser.parse(file_obj)\n    except:\n        logging.exception(\"Parse failed.\")\n        raise\n\n    logging.info(\"Schedule data processed.\")", "response": "Parse the data using the connected file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconcatenate KM3HDF5 files via pipeline.", "response": "def km3h5concat(input_files, output_file, n_events=None, **kwargs):\n    \"\"\"Concatenate KM3HDF5 files via pipeline.\"\"\"\n    from km3pipe import Pipeline    # noqa\n    from km3pipe.io import HDF5Pump, HDF5Sink    # noqa\n\n    pipe = Pipeline()\n    pipe.attach(HDF5Pump, filenames=input_files, **kwargs)\n    pipe.attach(StatusBar, every=250)\n    pipe.attach(HDF5Sink, filename=output_file, **kwargs)\n    pipe.drain(n_events)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef construct_experiment_id(time_interval):\n    # Construct id based on unix epoch timestamps\n    epoch = udatetime.utcfromtimestamp(0).replace(tzinfo=UTC)\n    start = int((time_interval.start - epoch).total_seconds() * 1000.0)\n    end = int((time_interval.end - epoch).total_seconds() * 1000.0)\n    return \"{}-{}\".format(start, end)", "response": "Construct an experiment id from a time interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reconstruct_interval(experiment_id):\n    start, end = map(lambda x: udatetime.utcfromtimestamp(x / 1000.0), map(float, experiment_id.split(\"-\")))\n    from ..time_interval import TimeInterval\n    return TimeInterval(start, end)", "response": "Reconstructs the interval from the given experiment id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json_serial(obj):\n\n    if isinstance(obj, (datetime, date)):\n        serial = obj.isoformat()\n        return serial\n\n    from ..time_interval import TimeInterval, TimeIntervals\n    if isinstance(obj, (TimeInterval, TimeIntervals)):\n        return obj.to_json()\n\n    from ..stream import StreamId\n    if isinstance(obj, StreamId):\n        return obj.to_json()\n\n    from ..channels import BaseChannel\n    if isinstance(obj, BaseChannel):\n        return json.dumps({'channel_id': obj.channel_id})\n\n    raise TypeError(\"Type %s not serializable\" % type(obj))", "response": "JSON serializer for objects not serializable by default json code"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mapper(self, meta_data, instance):\n        d = dict(meta_data)\n        v = d.pop(self.aggregation_meta_data)\n        instance = deepcopy(instance)\n        instance.value[self.aggregation_meta_data] = v\n        return instance", "response": "This method is used to map the meta data into the instance value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_data(stream, parameters, fmt):\n    sds = kp.db.StreamDS()\n    if stream not in sds.streams:\n        log.error(\"Stream '{}' not found in the database.\".format(stream))\n        return\n    params = {}\n    if parameters:\n        for parameter in parameters:\n            if '=' not in parameter:\n                log.error(\n                    \"Invalid parameter syntax '{}'\\n\"\n                    \"The correct syntax is 'parameter=value'\".\n                    format(parameter)\n                )\n                continue\n            key, value = parameter.split('=')\n            params[key] = value\n    data = sds.get(stream, fmt, **params)\n    if data is not None:\n        with pd.option_context('display.max_rows', None, 'display.max_columns',\n                               None):\n            print(data)\n    else:\n        sds.help(stream)", "response": "Retrieve data for given stream and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef available_streams():\n    sds = kp.db.StreamDS()\n    print(\"Available streams: \")\n    print(', '.join(sorted(sds.streams)))", "response": "Show a short list of available streams."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_runsummary(csv_filename, dryrun=False):\n    print(\"Checking '{}' for consistency.\".format(csv_filename))\n    if not os.path.exists(csv_filename):\n        log.critical(\"{} -> file not found.\".format(csv_filename))\n        return\n    try:\n        df = pd.read_csv(csv_filename, sep='\\t')\n    except pd.errors.EmptyDataError as e:\n        log.error(e)\n        return\n\n    cols = set(df.columns)\n\n    if not REQUIRED_COLUMNS.issubset(cols):\n        log.error(\n            \"Missing columns: {}.\".format(\n                ', '.join(str(c) for c in REQUIRED_COLUMNS - cols)\n            )\n        )\n        return\n\n    parameters = cols - REQUIRED_COLUMNS\n    if len(parameters) < 1:\n        log.error(\"No parameter columns found.\")\n        return\n\n    if len(df) == 0:\n        log.critical(\"Empty dataset.\")\n        return\n\n    print(\n        \"Found data for parameters: {}.\".format(\n            ', '.join(str(c) for c in parameters)\n        )\n    )\n    print(\"Converting CSV data into JSON\")\n    if dryrun:\n        log.warn(\"Dryrun: adding 'TEST_' prefix to parameter names\")\n        prefix = \"TEST_\"\n    else:\n        prefix = \"\"\n    data = convert_runsummary_to_json(df, prefix=prefix)\n    print(\"We have {:.3f} MB to upload.\".format(len(data) / 1024**2))\n\n    print(\"Requesting database session.\")\n    db = kp.db.DBManager()    # noqa\n    if kp.db.we_are_in_lyon():\n        session_cookie = \"sid=_kmcprod_134.158_lyo7783844001343100343mcprod1223user\"    # noqa\n    else:\n        session_cookie = kp.config.Config().get('DB', 'session_cookie')\n        if session_cookie is None:\n            raise SystemExit(\"Could not restore DB session.\")\n    log.debug(\"Using the session cookie: {}\".format(session_cookie))\n    cookie_key, sid = session_cookie.split('=')\n    print(\"Uploading the data to the database.\")\n    r = requests.post(\n        RUNSUMMARY_URL, cookies={cookie_key: sid}, files={'datafile': data}\n    )\n    if r.status_code == 200:\n        log.debug(\"POST request status code: {}\".format(r.status_code))\n        print(\"Database response:\")\n        db_answer = json.loads(r.text)\n        for key, value in db_answer.items():\n            print(\"  -> {}: {}\".format(key, value))\n        if db_answer['Result'] == 'OK':\n            print(\"Upload successful.\")\n        else:\n            log.critical(\"Something went wrong.\")\n    else:\n        log.error(\"POST request status code: {}\".format(r.status_code))\n        log.critical(\"Something went wrong...\")\n        return", "response": "Reads the CSV file and uploads its contents to the runsummary table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_runsummary_to_json(\n        df, comment='Uploaded via km3pipe.StreamDS', prefix='TEST_'\n):\n    \"\"\"Convert a Pandas DataFrame with runsummary to JSON for DB upload\"\"\"\n    data_field = []\n    comment += \", by {}\".format(getpass.getuser())\n    for det_id, det_data in df.groupby('det_id'):\n        runs_field = []\n        data_field.append({\"DetectorId\": det_id, \"Runs\": runs_field})\n\n        for run, run_data in det_data.groupby('run'):\n            parameters_field = []\n            runs_field.append({\n                \"Run\": int(run),\n                \"Parameters\": parameters_field\n            })\n\n            parameter_dict = {}\n            for row in run_data.itertuples():\n                for parameter_name in run_data.columns:\n                    if parameter_name in REQUIRED_COLUMNS:\n                        continue\n\n                    if parameter_name not in parameter_dict:\n                        entry = {'Name': prefix + parameter_name, 'Data': []}\n                        parameter_dict[parameter_name] = entry\n                    data_value = getattr(row, parameter_name)\n                    try:\n                        data_value = float(data_value)\n                    except ValueError as e:\n                        log.critical(\"Data values has to be floats!\")\n                        raise ValueError(e)\n                    value = {'S': str(getattr(row, 'source')), 'D': data_value}\n                    parameter_dict[parameter_name]['Data'].append(value)\n            for parameter_data in parameter_dict.values():\n                parameters_field.append(parameter_data)\n    data_to_upload = {\"Comment\": comment, \"Data\": data_field}\n    file_data_to_upload = json.dumps(data_to_upload)\n    return file_data_to_upload", "response": "Convert a Pandas DataFrame with runsummary to JSON for DB upload"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the acceptance ratio for a given order vector V and a proposed order vector W.", "response": "def calcAcceptanceRatio(self, V, W):\n        \"\"\"\n        Given a order vector V and a proposed order vector W, calculate the acceptance ratio for \n        changing to W when using MCMC.\n\n        ivar: dict<int,<dict,<int,int>>> wmg: A two-dimensional dictionary that associates integer\n            representations of each pair of candidates, cand1 and cand2, with the number of times\n            cand1 is ranked above cand2 minus the number of times cand2 is ranked above cand1. The\n            dictionary represents a weighted majority graph for an election.\n        :ivar float phi: A value for phi such that 0 <= phi <= 1.   \n        :ivar list<int> V: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last. This is the current sample.\n        :ivar list<int> W: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last. This is the proposed sample.\n        \"\"\"\n\n        acceptanceRatio = 1.0\n        for comb in itertools.combinations(V, 2):\n\n            #Check if comb[0] is ranked before comb[1] in V and W\n            vIOverJ = 1\n            wIOverJ = 1\n            if V.index(comb[0]) > V.index(comb[1]):\n                vIOverJ = 0\n            if W.index(comb[0]) > W.index(comb[1]):\n                wIOverJ = 0\n        \n            acceptanceRatio = acceptanceRatio * self.phi**(self.wmg[comb[0]][comb[1]]*(vIOverJ-wIOverJ))\n        return acceptanceRatio"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating the next sample by randomly flipping two adjacent candidates and selecting a random alternative in V.", "response": "def getNextSample(self, V):\n        \"\"\"\n        Generate the next sample by randomly flipping two adjacent candidates.\n\n        :ivar list<int> V: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last. This is the current sample.\n        \"\"\"\n\n        # Select a random alternative in V to switch with its adacent alternatives.\n        randPos = random.randint(0, len(V)-2)\n        W = copy.deepcopy(V)\n        d = V[randPos]\n        c = V[randPos+1]\n        W[randPos] = c\n        W[randPos+1] = d\n\n        # Check whether we should change to the new ranking.\n        prMW = 1\n        prMV = 1\n        prob = min(1.0,(prMW/prMV)*pow(self.phi, self.wmg[d][c]))/2\n        if random.random() <= prob:\n            V = W\n    \n        return V"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates the next sample by randomly shuffling candidates.", "response": "def getNextSample(self, V):\n        \"\"\"\n        Generate the next sample by randomly shuffling candidates.\n\n        :ivar list<int> V: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last. This is the current sample.\n        \"\"\"\n\n        positions = range(0, len(self.wmg))\n        randPoss = random.sample(positions, self.shuffleSize)\n        flipSet = copy.deepcopy(randPoss)\n        randPoss.sort()\n        W = copy.deepcopy(V)\n        for j in range(0, self.shuffleSize):\n            W[randPoss[j]] = V[flipSet[j]]\n\n        # Check whether we should change to the new ranking.\n        prMW = 1.0\n        prMV = 1.0\n        acceptanceRatio = self.calcAcceptanceRatio(V, W)\n        prob = min(1.0,(prMW/prMV)*acceptanceRatio)\n        if random.random() <= prob:\n            V = W\n        return V"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getNextSample(self, V):\n\n        phi = self.phi\n        wmg = self.wmg\n        W = []\n        W.append(V[0])\n        for j in range(2, len(V)+1):\n            randomSelect = random.random()\n            threshold = 0.0\n            denom = 1.0\n            for k in range(1, j):\n                denom = denom + phi**k\n            for k in range(1, j+1):\n                numerator = phi**(j - k)\n                threshold = threshold + numerator/denom\n                if randomSelect <= threshold:\n                    W.insert(k-1,V[j-1])\n                    break    \n\n        # Check whether we should change to the new ranking.\n        acceptanceRatio = self.calcAcceptanceRatio(V, W)\n        prob = min(1.0,acceptanceRatio)\n        if random.random() <= prob:\n            V = W\n\n        return V", "response": "This method generates a new ranking based on a Mallows - based jumping distribution."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a ranking over the candidates generate a new ranking by assigning each candidate at position i a Plakett - Luce weight of phi^i draw a new ranking.", "response": "def getNextSample(self, V):\n        \"\"\"\n        Given a ranking over the candidates, generate a new ranking by assigning each candidate at\n        position i a Plakett-Luce weight of phi^i and draw a new ranking.\n\n        :ivar list<int> V: Contains integer representations of each candidate in order of their\n            ranking in a vote, from first to last.\n        \"\"\"\n\n        W, WProb = self.drawRankingPlakettLuce(V)\n        VProb = self.calcProbOfVFromW(V, W)\n        acceptanceRatio = self.calcAcceptanceRatio(V, W)\n        prob = min(1.0, acceptanceRatio * (VProb/WProb))\n        if random.random() <= prob:\n            V = W\n        return V"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calcDrawingProbs(self):\n\n        wmg = self.wmg\n        phi = self.phi\n\n        # We say the weight of the candidate in position i is phi^i.\n        weights = []\n        for i in range(0, len(wmg.keys())):\n            weights.append(phi**i)\n\n        # Calculate the probabilty that an item at each weight is drawn.\n        totalWeight = sum(weights)\n        for i in range(0, len(wmg.keys())):\n            weights[i] = weights[i]/totalWeight\n\n        return weights", "response": "Calculates the probabilty of an item being from each position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drawRankingPlakettLuce(self, rankList):\n\n        probs = self.plakettLuceProbs\n        numCands = len(rankList)\n        newRanking = []\n        remainingCands = copy.deepcopy(rankList)\n        probsCopy = copy.deepcopy(self.plakettLuceProbs)\n        totalProb = sum(probs)\n\n        # We will use prob to iteratively calculate the probabilty that we draw the order vector\n        # that we end up drawing.\n        prob = 1.0\n        \n        while (len(newRanking) < numCands):\n\n            # We generate a random number from 0 to 1, and use it to select a candidate. \n            rand = random.random()\n            threshold = 0.0\n            for i in range(0, len(probsCopy)):\n                threshold = threshold + probsCopy[i]/totalProb\n                if rand <= threshold:\n                    prob = prob * probsCopy[i]/totalProb\n                    newRanking.append(remainingCands[i])\n                    remainingCands.pop(i)\n                    totalProb = totalProb - probsCopy[i]\n                    probsCopy.pop(i)\n                    break\n\n        return newRanking, prob", "response": "Given an order vector over the candidates draw candidates to generate a new order vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calcProbOfVFromW(self, V, W):\n\n        weights = range(0, len(V))\n        i = 0\n        for alt in W:\n            weights[alt-1] = self.phi ** i\n            i = i + 1\n        \n        # Calculate the probability that we draw V[0], V[1], and so on from W.\n        prob = 1.0\n        totalWeight = sum(weights)\n        for alt in V:\n            prob = prob * weights[alt-1]/totalWeight\n            totalWeight = totalWeight - weights[alt-1]\n    \n        return prob", "response": "Calculate the probability that we draw V as our next sample if our current sample was W."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the next sample for the condorcet model.", "response": "def getNextSample(self, V):\n        \"\"\"\n        Generate the next sample for the condorcet model. This algorithm is described in \"Computing\n        Optimal Bayesian Decisions for Rank Aggregation via MCMC Sampling,\" and is adapted from \n        code written by Lirong Xia.\n        \n        :ivar list<list<int> V: A two-dimensional list that for every pair of candidates cand1 and \n            cand2, V[cand1][cand2] contains 1 if cand1 is ranked above cand2 more times than cand2\n            is ranked above cand1 and 0 otherwise.\n        \"\"\"\n\n        cands = range(len(self.wmg))\n        W = copy.deepcopy(V)\n        allPairs = itertools.combinations(cands, 2)\n        for pair in allPairs:\n            a = pair[0]\n            b = pair[1]\n            if random.random() < 1.0/(1.0+pow(self.phi,self.wmg[a+1][b+1])):\n                W[a][b] = 1\n                W[b][a] = 0\n            else:\n                W[a][b] = 0\n                W[b][a] = 1\n        prMW = 1\n        prMV = 1\n        prob = min(1.0, prMW/prMV)\n        if random.random() <= prob:\n            V = W\n        return V"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_hist(rfile, histname, get_overflow=False):\n    import root_numpy as rnp\n\n    rfile = open_rfile(rfile)\n    hist = rfile[histname]\n    xlims = np.array(list(hist.xedges()))\n    bin_values = rnp.hist2array(hist, include_overflow=get_overflow)\n    rfile.close()\n    return bin_values, xlims", "response": "Read a 1D Histogram."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interpol_hist2d(h2d, oversamp_factor=10):\n    from rootpy import ROOTError\n\n    xlim = h2d.bins(axis=0)\n    ylim = h2d.bins(axis=1)\n    xn = h2d.nbins(0)\n    yn = h2d.nbins(1)\n    x = np.linspace(xlim[0], xlim[1], xn * oversamp_factor)\n    y = np.linspace(ylim[0], ylim[1], yn * oversamp_factor)\n    mat = np.zeros((xn, yn))\n    for xi in range(xn):\n        for yi in range(yn):\n            try:\n                mat[xi, yi] = h2d.interpolate(x[xi], y[yi])\n            except ROOTError:\n                continue\n    return mat, x, y", "response": "Sample the interpolator of a root 2d hist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_window(size=None, samples=16, *, fullscreen=False, title=None, threaded=True) -> Window:\n    '''\n        Create the main window.\n\n        Args:\n            size (tuple): The width and height of the window.\n            samples (int): The number of samples.\n\n        Keyword Args:\n            fullscreen (bool): Fullscreen?\n            title (bool): The title of the window.\n            threaded (bool): Threaded?\n\n        Returns:\n            Window: The main window.\n    '''\n\n    if size is None:\n        width, height = 1280, 720\n\n    else:\n        width, height = size\n\n    if samples < 0 or (samples & (samples - 1)) != 0:\n        raise Exception('Invalid number of samples: %d' % samples)\n\n    window = Window.__new__(Window)\n    window.wnd = glwnd.create_window(width, height, samples, fullscreen, title, threaded)\n    return window", "response": "Create the main window."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self, red=0.0, green=0.0, blue=0.0, alpha=0.0) -> None:\n        '''\n            Clear the window.\n        '''\n\n        self.wnd.clear(red, green, blue, alpha)", "response": "Clear the color of the current color."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef windowed(self, size) -> None:\n        '''\n            Set the window to windowed mode.\n        '''\n\n        width, height = size\n\n        self.wnd.windowed(width, height)", "response": "Set the window to windowed mode."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef viewport(self) -> Tuple[int, int, int, int]:\n        '''\n            tuple: The viewport of the window.\n        '''\n\n        return self.wnd.viewport", "response": "Returns the viewport of the window."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts metadata for a specific product", "response": "def product_metadata(product, dst_folder, counter=None, writers=[file_writer], geometry_check=None):\n    \"\"\" Extract metadata for a specific product \"\"\"\n\n    if not counter:\n        counter = {\n            'products': 0,\n            'saved_tiles': 0,\n            'skipped_tiles': 0,\n            'skipped_tiles_paths': []\n        }\n\n    s3_url = 'http://sentinel-s2-l1c.s3.amazonaws.com'\n\n    product_meta_link = '{0}/{1}'.format(s3_url, product['metadata'])\n    product_info = requests.get(product_meta_link, stream=True)\n    product_metadata = metadata_to_dict(product_info.raw)\n    product_metadata['product_meta_link'] = product_meta_link\n\n    counter['products'] += 1\n\n    for tile in product['tiles']:\n        tile_info = requests.get('{0}/{1}'.format(s3_url, tile))\n        try:\n            metadata = tile_metadata(tile_info.json(), copy(product_metadata), geometry_check)\n\n            for w in writers:\n                w(dst_folder, metadata)\n\n            logger.info('Saving to disk: %s' % metadata['tile_name'])\n            counter['saved_tiles'] += 1\n        except JSONDecodeError:\n            logger.warning('Tile: %s was not found and skipped' % tile)\n            counter['skipped_tiles'] += 1\n            counter['skipped_tiles_paths'].append(tile)\n\n    return counter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef daily_metadata(year, month, day, dst_folder, writers=[file_writer], geometry_check=None,\n                   num_worker_threads=1):\n    \"\"\" Extra metadata for all products in a specific date \"\"\"\n\n    threaded = False\n\n    counter = {\n        'products': 0,\n        'saved_tiles': 0,\n        'skipped_tiles': 0,\n        'skipped_tiles_paths': []\n    }\n\n    if num_worker_threads > 1:\n        threaded = True\n        queue = Queue()\n\n    # create folders\n    year_dir = os.path.join(dst_folder, str(year))\n    month_dir = os.path.join(year_dir, str(month))\n    day_dir = os.path.join(month_dir, str(day))\n\n    product_list = get_products_metadata_path(year, month, day)\n\n    logger.info('There are %s products in %s-%s-%s' % (len(list(iterkeys(product_list))),\n                                                       year, month, day))\n\n    for name, product in iteritems(product_list):\n        product_dir = os.path.join(day_dir, name)\n\n        if threaded:\n            queue.put([product, product_dir, counter, writers, geometry_check])\n        else:\n            counter = product_metadata(product, product_dir, counter, writers, geometry_check)\n\n    if threaded:\n        def worker():\n            while not queue.empty():\n                args = queue.get()\n                try:\n                    product_metadata(*args)\n                except Exception:\n                    exc = sys.exc_info()\n                    logger.error('%s tile skipped due to error: %s' % (threading.current_thread().name,\n                                                                       exc[1].__str__()))\n                    args[2]['skipped_tiles'] += 1\n                queue.task_done()\n\n        threads = []\n        for i in range(num_worker_threads):\n            t = threading.Thread(target=worker)\n            t.start()\n            threads.append(t)\n\n        queue.join()\n\n    return counter", "response": "Generate a single archive of all products in a specific date."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all metadata for all products in a date range.", "response": "def range_metadata(start, end, dst_folder, num_worker_threads=0, writers=[file_writer], geometry_check=None):\n    \"\"\" Extra metadata for all products in a date range \"\"\"\n\n    assert isinstance(start, date)\n    assert isinstance(end, date)\n\n    delta = end - start\n\n    dates = []\n\n    for i in range(delta.days + 1):\n        dates.append(start + timedelta(days=i))\n\n    days = len(dates)\n\n    total_counter = {\n        'days': days,\n        'products': 0,\n        'saved_tiles': 0,\n        'skipped_tiles': 0,\n        'skipped_tiles_paths': []\n    }\n\n    def update_counter(counter):\n        for key in iterkeys(total_counter):\n            if key in counter:\n                total_counter[key] += counter[key]\n\n    for d in dates:\n        logger.info('Getting metadata of {0}-{1}-{2}'.format(d.year, d.month, d.day))\n        update_counter(daily_metadata(d.year, d.month, d.day, dst_folder, writers, geometry_check,\n                                      num_worker_threads))\n\n    return total_counter"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a resource on TMDB.", "response": "def get_on_tmdb(uri, **kwargs):\n    \"\"\" Get a resource on TMDB.\n    \"\"\"\n    kwargs['api_key'] = app.config['TMDB_API_KEY']\n    response = requests_session.get((TMDB_API_URL + uri).encode('utf8'), params=kwargs)\n    response.raise_for_status()\n    return json.loads(response.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search():\n    redis_key = 's_%s' % request.args['query'].lower()\n    cached = redis_ro_conn.get(redis_key)\n    if cached:\n        return Response(cached)\n    else:\n        try:\n            found = get_on_tmdb(u'/search/movie', query=request.args['query'])\n            movies = []\n            for movie in found['results']:\n                cast = get_on_tmdb(u'/movie/%s/casts' % movie['id'])\n                year = datetime.strptime(movie['release_date'], '%Y-%m-%d').year if movie['release_date'] else None\n                movies.append({'title': movie['original_title'],\n                               'directors': [x['name'] for x in cast['crew'] if x['department'] == 'Directing' and x['job'] == 'Director'],\n                               'year': year,\n                               '_tmdb_id': movie['id']})\n        except requests.HTTPError as err:\n            return Response('TMDB API error: %s' % str(err), status=err.response.status_code)\n        json_response = json.dumps({'movies': movies})\n        redis_conn.setex(redis_key, app.config['CACHE_TTL'], json_response)\n        return Response(json_response)", "response": "Search a movie on TMDB."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_movie(tmdb_id):\n    redis_key = 'm_%s' % tmdb_id\n    cached = redis_ro_conn.get(redis_key)\n    if cached:\n        return Response(cached)\n    else:\n        try:\n            details = get_on_tmdb(u'/movie/%d' % tmdb_id)\n            cast = get_on_tmdb(u'/movie/%d/casts' % tmdb_id)\n            alternative = get_on_tmdb(u'/movie/%d/alternative_titles' % tmdb_id)\n        except requests.HTTPError as err:\n            return Response('TMDB API error: %s' % str(err), status=err.response.status_code)\n        movie = {'title': details['original_title'],\n                 'score': details['popularity'],\n                 'directors': [x['name'] for x in cast['crew'] if x['department'] == 'Directing' and x['job'] == 'Director'],\n                 'writers': [x['name'] for x in cast['crew'] if x['department'] == 'Writing'],\n                 'cast': [x['name'] for x in cast['cast']],\n                 'genres': [x['name'] for x in details['genres']],\n                 'countries': [x['name'] for x in details['production_countries']],\n                 'tmdb_votes': int(round(details.get('vote_average', 0) * 0.5)),\n                 '_tmdb_id': tmdb_id}\n        if details.get('release_date'):\n            movie['year'] = datetime.strptime(details['release_date'], '%Y-%m-%d').year\n        if details.get('belongs_to_collection'):\n            movie['collection'] = details['belongs_to_collection']['name']\n        for alt in alternative['titles']:\n            movie['title_%s' % alt['iso_3166_1'].lower()] = alt['title']\n        json_response = json.dumps({'movie': movie})\n        redis_conn.setex(redis_key, app.config['CACHE_TTL'], json_response)\n        return Response(json_response)", "response": "Get informations about a movie using its tmdb id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_response_error(self, response, retries, **kwargs):\n        error = self._convert_response_to_error(response)\n        if error is None:\n            return response\n\n        max_retries = self._max_retries_for_error(error)\n        if max_retries is None or retries >= max_retries:\n            return response\n\n        backoff = min(0.0625 * 2 ** retries, 1.0)\n        self.logger.warning(\"Sleeping for %r before retrying failed request...\", backoff)\n        time.sleep(backoff)\n\n        retries += 1\n        self.logger.warning(\"Retrying failed request. Attempt %d/%d.\", retries, max_retries)\n\n        return self.request(retries=retries, **kwargs)", "response": "Handles the response of a request and returns a new Response object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the response to an error object.", "response": "def _convert_response_to_error(self, response):\n        \"\"\"Subclasses may override this method in order to influence\n        how errors are parsed from the response.\n\n        Parameters:\n          response(Response): The response object.\n\n        Returns:\n          object or None: Any object for which a max retry count can\n          be retrieved or None if the error cannot be handled.\n        \"\"\"\n        content_type = response.headers.get(\"content-type\", \"\")\n        if \"application/x-protobuf\" in content_type:\n            self.logger.debug(\"Decoding protobuf response.\")\n            data = status_pb2.Status.FromString(response.content)\n            status = self._PB_ERROR_CODES.get(data.code)\n            error = {\"status\": status}\n            return error\n\n        elif \"application/json\" in content_type:\n            self.logger.debug(\"Decoding json response.\")\n            data = response.json()\n            error = data.get(\"error\")\n            if not error or not isinstance(error, dict):\n                self.logger.warning(\"Unexpected error response: %r\", data)\n                return None\n            return error\n\n        self.logger.warning(\"Unexpected response: %r\", response.text)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a stream instance based on the filters", "response": "def generate_instance(self, timestamp, value, meta_data):\n        \"\"\"\n        Generate a stream instance, checking if the filters are matched\n        :param timestamp: The timestamp\n        :param value: The value\n        :param meta_data: The meta data\n        :return: The stream instance (or None)\n        \"\"\"\n        if not self.filters or meta_data in self.filters:\n            if self.filters:\n                logging.debug('Filtered in  for {} from {}'.format(meta_data, self.filters))\n            return StreamMetaInstance(StreamInstance(timestamp=timestamp, value=value), meta_data)\n        if self.filters:\n            logging.debug('Filtered out for {} from {}'.format(meta_data, self.filters))\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a format string and return a prepared environment with the values found in the env.", "response": "def parse_pattern(format_string, env, wrapper=lambda x, y: y):\n    \"\"\" Parse the format_string and return prepared data according to the env.\n\n    Pick each field found in the format_string from the env(ironment), apply\n    the wrapper on each data and return a mapping between field-to-replace and\n    values for each.\n    \"\"\"\n\n    formatter = Formatter()\n    fields = [x[1] for x in formatter.parse(format_string) if x[1] is not None]\n\n    prepared_env = {}\n\n    # Create a prepared environment with only used fields, all as list:\n    for field in fields:\n        # Search for a movie attribute for each alternative field separated\n        # by a pipe sign:\n        for field_alt in (x.strip() for x in field.split('|')):\n            # Handle default values (enclosed by quotes):\n            if field_alt[0] in '\\'\"' and field_alt[-1] in '\\'\"':\n                field_values = field_alt[1:-1]\n            else:\n                field_values = env.get(field_alt)\n            if field_values is not None:\n                break\n        else:\n            field_values = []\n        if not isinstance(field_values, list):\n            field_values = [field_values]\n        prepared_env[field] = wrapper(field_alt, field_values)\n\n    return prepared_env"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_dataset(args):\n    if not args.args:\n        raise ParserError('you must specify an existing directory')\n    outdir = Path(args.args.pop(0))\n    if not outdir.exists():\n        raise ParserError('you must specify an existing directory')\n\n    id_pattern = re.compile('[a-z_0-9]+$')\n    md = {}\n    if args.args:\n        md['id'] = args.args.pop(0)\n    else:\n        md['id'] = input('Dataset ID: ')\n\n    while not id_pattern.match(md['id']):\n        print('dataset id must only consist of lowercase ascii letters, digits and _ (underscore)!')\n        md['id'] = input('Dataset ID: ')\n\n    outdir = outdir / md['id']\n    if not outdir.exists():\n        outdir.mkdir()\n\n    for key in ['title', 'url', 'license', 'conceptlist', 'citation']:\n        md[key] = input('Dataset {0}: '.format(key))\n\n    # check license!\n    # check conceptlist!\n\n    for path in Path(pylexibank.__file__).parent.joinpath('dataset_template').iterdir():\n        if path.is_file():\n            if path.suffix in ['.pyc']:\n                continue  # pragma: no cover\n            target = path.name\n            content = read_text(path)\n            if '+' in path.name:\n                target = re.sub(\n                    '\\+([a-z]+)\\+',\n                    lambda m: '{' + m.groups()[0] + '}',\n                    path.name\n                ).format(**md)\n            if target.endswith('_tmpl'):\n                target = target[:-5]\n                content = content.format(**md)\n            write_text(outdir / target, content)\n        else:\n            target = outdir / path.name\n            if target.exists():\n                shutil.rmtree(str(target))\n            shutil.copytree(str(path), str(target))\n    del md['id']\n    jsonlib.dump(md, outdir / 'metadata.json', indent=4)", "response": "lexibank new - dataset OUTDIR [ ID ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ls(args):\n    db = Database(args.db)\n    db.create(exists_ok=True)\n    in_db = {r[0]: r[1] for r in db.fetchall('select id, version from dataset')}\n    # FIXME: how to smartly choose columns?\n    table = Table('ID', 'Title')\n    cols = OrderedDict([\n        (col, {}) for col in args.args if col in [\n            'version',\n            'location',\n            'changes',\n            'license',\n            'all_lexemes',\n            'lexemes',\n            'concepts',\n            'languages',\n            'families',\n            'varieties',\n            'macroareas',\n        ]])\n    tl = 40\n    if cols:\n        tl = 25\n        table.columns.extend(col.capitalize() for col in cols)\n\n    for col, sql in [\n        ('languages', 'glottocodes_by_dataset'),\n        ('concepts', 'conceptsets_by_dataset'),\n        ('lexemes', 'mapped_lexemes_by_dataset'),\n        ('all_lexemes', 'lexemes_by_dataset'),\n        ('macroareas', 'macroareas_by_dataset'),\n        ('families', 'families_by_dataset'),\n    ]:\n        if col in cols:\n            cols[col] = {r[0]: r[1] for r in db.fetchall(sql)}\n    for ds in args.cfg.datasets:\n        row = [\n            colored(ds.id, 'green' if ds.id in in_db else 'red'),\n            truncate_with_ellipsis(ds.metadata.title or '', width=tl),\n        ]\n        for col in cols:\n            if col == 'version':\n                row.append(git_hash(ds.dir))\n            elif col == 'location':\n                row.append(colored(str(ds.dir), 'green'))\n            elif col == 'changes':\n                row.append(ds.git_repo.is_dirty())\n            elif col == 'license':\n                lic = licenses.find(ds.metadata.license or '')\n                row.append(lic.id if lic else ds.metadata.license)\n            elif col in ['languages', 'concepts', 'lexemes', 'all_lexemes', 'families']:\n                row.append(float(cols[col].get(ds.id, 0)))\n            elif col == 'macroareas':\n                row.append(', '.join(sorted((cols[col].get(ds.id) or '').split(','))))\n            else:\n                row.append('')\n\n        table.append(row)\n    totals = ['zztotal', len(args.cfg.datasets)]\n    for i, col in enumerate(cols):\n        if col in ['lexemes', 'all_lexemes']:\n            totals.append(sum([r[i + 2] for r in table]))\n        elif col == 'languages':\n            totals.append(float(db.fetchone(\n                \"SELECT count(distinct glottocode) FROM languagetable\")[0]))\n        elif col == 'concepts':\n            totals.append(float(db.fetchone(\n                \"SELECT count(distinct concepticon_id) FROM parametertable\")[0]))\n        elif col == 'families':\n            totals.append(float(db.fetchone(\n                \"SELECT count(distinct family) FROM languagetable\")[0]))\n        else:\n            totals.append('')\n    table.append(totals)\n    print(table.render(\n        tablefmt='simple', sortkey=lambda r: r[0], condensed=False, floatfmt=',.0f'))", "response": "ls - List all available lexical articles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mad(v):\n    return np.median(np.abs(v - np.median(v)))", "response": "MAD -- Median absolute deviation. More robust than standard deviation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting names of fit parameters from a scipy. rv_* distribution.", "response": "def param_names(scipy_dist):\n    \"\"\"Get names of fit parameters from a ``scipy.rv_*`` distribution.\"\"\"\n    if not isinstance(scipy_dist, rv_continuous):\n        raise TypeError\n    names = ['loc', 'scale']\n    if scipy_dist.shapes is not None:\n        names += scipy_dist.shapes.split()\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating symmetric percentiles with p coverage.", "response": "def perc(arr, p=95, **kwargs):\n    \"\"\"Create symmetric percentiles, with ``p`` coverage.\"\"\"\n    offset = (100 - p) / 2\n    return np.percentile(arr, (offset, 100 - offset), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resample_1d(arr, n_out=None, random_state=None):\n    if random_state is None:\n        random_state = np.random.RandomState()\n    arr = np.atleast_1d(arr)\n    n = len(arr)\n    if n_out is None:\n        n_out = n\n    idx = random_state.randint(0, n, size=n)\n    return arr[idx]", "response": "Resample an array with replacement."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bootstrap_params(rv_cont, data, n_iter=5, **kwargs):\n    fit_res = []\n    for _ in range(n_iter):\n        params = rv_cont.fit(resample_1d(data, **kwargs))\n        fit_res.append(params)\n    fit_res = np.array(fit_res)\n    return fit_res", "response": "Bootstrap the fit params of a distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets mean + quantile range from bootstrapped params.", "response": "def param_describe(params, quant=95, axis=0):\n    \"\"\"Get mean + quantile range from bootstrapped params.\"\"\"\n    par = np.mean(params, axis=axis)\n    lo, up = perc(quant)\n    p_up = np.percentile(params, up, axis=axis)\n    p_lo = np.percentile(params, lo, axis=axis)\n    return par, p_lo, p_up"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bootstrap_fit(\n        rv_cont, data, n_iter=10, quant=95, print_params=True, **kwargs\n):\n    \"\"\"Bootstrap a distribution fit + get confidence intervals for the params.\n\n    Parameters\n    ==========\n    rv_cont: scipy.stats.rv_continuous instance\n        The distribution which to fit.\n    data: array-like, 1d\n        The data on which to fit.\n    n_iter: int [default=10]\n        Number of bootstrap iterations.\n    quant: int [default=95]\n        percentile of the confidence limits (default is 95, i.e. 2.5%-97.5%)\n    print_params: bool [default=True]\n        Print a fit summary.\n    \"\"\"\n    fit_params = bootstrap_params(rv_cont, data, n_iter)\n    par, lo, up = param_describe(fit_params, quant=quant)\n    names = param_names(rv_cont)\n    maxlen = max([len(s) for s in names])\n    print(\"--------------\")\n    print(rv_cont.name)\n    print(\"--------------\")\n    for i, name in enumerate(names):\n        print(\n            \"{nam:>{fill}}: {mean:+.3f} \u2208 \"\n            \"[{lo:+.3f}, {up:+.3f}] ({q}%)\".format(\n                nam=name,\n                fill=maxlen,\n                mean=par[i],\n                lo=lo[i],\n                up=up[i],\n                q=quant\n            )\n        )\n    out = {\n        'mean': par,\n        'lower limit': lo,\n        'upper limit': up,\n    }\n    return out", "response": "Bootstrap a distribution fit + get confidence intervals for the params."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing Random Variates. Parameters ---------- size: int, optional (default=1) random_state_: optional (default=None)", "response": "def rvs(self, *args, **kwargs):\n        \"\"\"Draw Random Variates.\n\n        Parameters\n        ----------\n        size: int, optional (default=1)\n        random_state_: optional (default=None)\n        \"\"\"\n        # TODO REVERSE THIS FUCK PYTHON2\n        size = kwargs.pop('size', 1)\n        random_state = kwargs.pop('size', None)\n        # don't ask me why it uses `self._size`\n        return self._kde.sample(n_samples=size, random_state=random_state)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    from docopt import docopt\n    args = docopt(__doc__)\n    infile = args['INFILE']\n    outfile = args['OUTFILE']\n    i3extract(infile, outfile)", "response": "Entry point when running as script from commandline."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self, server_config):\n        if 'connection_string' in server_config:\n            self.client = pymongo.MongoClient(\n                    server_config['connection_string'])\n            self.db = self.client[server_config['db']]\n        else:\n            self.client = pymongo.MongoClient(\n                server_config['host'],\n                server_config['port'],\n                tz_aware=self.get_config_value('tz_aware', True))\n\n            self.db = self.client[server_config['db']]\n\n        if ('authentication_database' in server_config and\n                server_config['authentication_database']):\n            self.db.authenticate(\n                    server_config['username'], server_config['password'],\n                    source=server_config['authentication_database'])\n        else:\n            if 'username' in server_config:\n                if 'password' in server_config:\n                    self.db.authenticate(server_config['username'],\n                                         server_config['password'])\n                else:\n                    self.db.authenticate(server_config['username'])\n\n        # Mongo Engine connection\n        d = dict((k, v) for k, v in server_config.items()\n                 if k not in ['modalities', 'summaries'])\n        if 'authentication_database' in d:\n            d['authentication_source'] = d['authentication_database']\n            del d['authentication_database']\n\n        self.session = connect(alias=\"hyperstream\", **d)\n\n        # TODO: This sets the default connection of mongoengine, but seems to be a bit of a hack\n        if \"default\" not in connection._connections:\n            connection._connections[\"default\"] = connection._connections[\"hyperstream\"]\n            connection._connection_settings[\"default\"] = connection._connection_settings[\"hyperstream\"]", "response": "Connect to the database and store the ID of the current object in the connection object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalibrating intra DOM PMT time offsets efficiencies and sigmas for a given DOM.", "response": "def calibrate_dom(\n        dom_id,\n        data,\n        detector,\n        livetime=None,\n        fit_ang_dist=False,\n        scale_mc_to_data=True,\n        ad_fit_shape='pexp',\n        fit_background=True,\n        ctmin=-1.\n):\n    \"\"\"Calibrate intra DOM PMT time offsets, efficiencies and sigmas\n\n        Parameters\n        ----------\n        dom_id: DOM ID\n        data: dict of coincidences or root or hdf5 file\n        detector: instance of detector class\n        livetime: data-taking duration [s]\n        fixed_ang_dist: fixing angular distribution e.g. for data mc comparison\n        auto_scale: auto scales the fixed angular distribution to the data\n\n        Returns\n        -------\n        return_data: dictionary with fit results\n    \"\"\"\n\n    if isinstance(data, str):\n        filename = data\n        loaders = {\n            '.h5': load_k40_coincidences_from_hdf5,\n            '.root': load_k40_coincidences_from_rootfile\n        }\n        try:\n            loader = loaders[os.path.splitext(filename)[1]]\n        except KeyError:\n            log.critical('File format not supported.')\n            raise IOError\n        else:\n            data, livetime = loader(filename, dom_id)\n\n    combs = np.array(list(combinations(range(31), 2)))\n    angles = calculate_angles(detector, combs)\n    cos_angles = np.cos(angles)\n    angles = angles[cos_angles >= ctmin]\n    data = data[cos_angles >= ctmin]\n    combs = combs[cos_angles >= ctmin]\n\n    try:\n        fit_res = fit_delta_ts(data, livetime, fit_background=fit_background)\n        rates, means, sigmas, popts, pcovs = fit_res\n    except:\n        return 0\n\n    rate_errors = np.array([np.diag(pc)[2] for pc in pcovs])\n    # mean_errors = np.array([np.diag(pc)[0] for pc in pcovs])\n    scale_factor = None\n    if fit_ang_dist:\n        fit_res = fit_angular_distribution(\n            angles, rates, rate_errors, shape=ad_fit_shape\n        )\n        fitted_rates, exp_popts, exp_pcov = fit_res\n    else:\n        mc_fitted_rates = exponential_polinomial(np.cos(angles), *MC_ANG_DIST)\n        if scale_mc_to_data:\n            scale_factor = np.mean(rates[angles < 1.5]) /  \\\n                np.mean(mc_fitted_rates[angles < 1.5])\n        else:\n            scale_factor = 1.\n        fitted_rates = mc_fitted_rates * scale_factor\n        exp_popts = []\n        exp_pcov = []\n        print('Using angular distribution from Monte Carlo')\n\n    # t0_weights = np.array([0. if a>1. else 1. for a in angles])\n\n    if not fit_background:\n        minimize_weights = calculate_weights(fitted_rates, data)\n    else:\n        minimize_weights = fitted_rates\n\n    opt_t0s = minimize_t0s(means, minimize_weights, combs)\n    opt_sigmas = minimize_sigmas(sigmas, minimize_weights, combs)\n    opt_qes = minimize_qes(fitted_rates, rates, minimize_weights, combs)\n    corrected_means = correct_means(means, opt_t0s.x, combs)\n    corrected_rates = correct_rates(rates, opt_qes.x, combs)\n    rms_means, rms_corrected_means = calculate_rms_means(\n        means, corrected_means\n    )\n    rms_rates, rms_corrected_rates = calculate_rms_rates(\n        rates, fitted_rates, corrected_rates\n    )\n    cos_angles = np.cos(angles)\n    return_data = {\n        'opt_t0s': opt_t0s,\n        'opt_qes': opt_qes,\n        'data': data,\n        'means': means,\n        'rates': rates,\n        'fitted_rates': fitted_rates,\n        'angles': angles,\n        'corrected_means': corrected_means,\n        'corrected_rates': corrected_rates,\n        'rms_means': rms_means,\n        'rms_corrected_means': rms_corrected_means,\n        'rms_rates': rms_rates,\n        'rms_corrected_rates': rms_corrected_rates,\n        'gaussian_popts': popts,\n        'livetime': livetime,\n        'exp_popts': exp_popts,\n        'exp_pcov': exp_pcov,\n        'scale_factor': scale_factor,\n        'opt_sigmas': opt_sigmas,\n        'sigmas': sigmas,\n        'combs': combs\n    }\n    return return_data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_k40_coincidences_from_hdf5(filename, dom_id):\n\n    with h5py.File(filename, 'r') as h5f:\n        data = h5f['/k40counts/{0}'.format(dom_id)]\n        livetime = data.attrs['livetime']\n        data = np.array(data)\n\n    return data, livetime", "response": "Load k40 coincidences from hdf5 file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_k40_coincidences_from_rootfile(filename, dom_id):\n\n    from ROOT import TFile\n    root_file_monitor = TFile(filename, \"READ\")\n    dom_name = str(dom_id) + \".2S\"\n    histo_2d_monitor = root_file_monitor.Get(dom_name)\n    data = []\n    for c in range(1, histo_2d_monitor.GetNbinsX() + 1):\n        combination = []\n        for b in range(1, histo_2d_monitor.GetNbinsY() + 1):\n            combination.append(histo_2d_monitor.GetBinContent(c, b))\n        data.append(combination)\n\n    weights = {}\n    weights_histo = root_file_monitor.Get('weights_hist')\n    try:\n        for i in range(1, weights_histo.GetNbinsX() + 1):\n            # we have to read all the entries, unfortunately\n            weight = weights_histo.GetBinContent(i)\n            label = weights_histo.GetXaxis().GetBinLabel(i)\n            weights[label[3:]] = weight\n        dom_weight = weights[str(dom_id)]\n    except AttributeError:\n        log.info(\"Weights histogram broken or not found, setting weight to 1.\")\n        dom_weight = 1.\n    return np.array(data), dom_weight", "response": "Load k40 coincidences from a ROOT file produced by JMonitorK40."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits gaussians to delta t for each PMT pair.", "response": "def fit_delta_ts(data, livetime, fit_background=True):\n    \"\"\"Fits gaussians to delta t for each PMT pair.\n\n    Parameters\n    ----------\n    data: 2d np.array: x = PMT combinations (465), y = time, entry = frequency\n    livetime: length of data taking in seconds\n    fit_background: if True: fits gaussian with offset, else without offset\n\n    Returns\n    -------\n    numpy arrays with rates and means for all PMT combinations\n    \"\"\"\n\n    data = data / livetime\n    start = -(data.shape[1] - 1) / 2\n    end = -start + 1\n    xs = np.arange(start, end)\n\n    rates = []\n    sigmas = []\n    means = []\n    popts = []\n    pcovs = []\n    for combination in data:\n        mean0 = np.argmax(combination) + start\n        try:\n            if fit_background:\n                popt, pcov = optimize.curve_fit(\n                    gaussian,\n                    xs,\n                    combination,\n                    p0=[mean0, 4., 5., 0.1],\n                    bounds=([start, 0, 0, 0], [end, 10, 10, 1])\n                )\n            else:\n                popt, pcov = optimize.curve_fit(\n                    gaussian_wo_offset,\n                    xs,\n                    combination,\n                    p0=[mean0, 4., 5.],\n                    bounds=([start, 0, 0], [end, 10, 10])\n                )\n        except RuntimeError:\n            popt = (0, 0, 0, 0)\n        rates.append(popt[2])\n        means.append(popt[0])\n        sigmas.append(popt[1])\n        popts.append(popt)\n        pcovs.append(pcov)\n    return (\n        np.array(rates), np.array(means), np.array(sigmas), np.array(popts),\n        np.array(pcovs)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_angles(detector, combs):\n    angles = []\n    pmt_angles = detector.pmt_angles\n    for first, second in combs:\n        angles.append(\n            kp.math.angle_between(\n                np.array(pmt_angles[first]), np.array(pmt_angles[second])\n            )\n        )\n    return np.array(angles)", "response": "Calculates the angles between all PMT combinations according to positions in\n    detector_file\n    combs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit_angular_distribution(angles, rates, rate_errors, shape='pexp'):\n    if shape == 'exp':\n        fit_function = exponential\n        # p0 = [-0.91871169,  2.72224241, -1.19065965,  1.48054122]\n    if shape == 'pexp':\n        fit_function = exponential_polinomial\n        # p0 = [0.34921202, 2.8629577]\n\n    cos_angles = np.cos(angles)\n    popt, pcov = optimize.curve_fit(fit_function, cos_angles, rates)\n    fitted_rates = fit_function(cos_angles, *popt)\n    return fitted_rates, popt, pcov", "response": "Fits angular distribution of rates for all PMTs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef minimize_t0s(means, weights, combs):\n\n    def make_quality_function(means, weights, combs):\n        def quality_function(t0s):\n            sq_sum = 0\n            for mean, comb, weight in zip(means, combs, weights):\n                sq_sum += ((mean - (t0s[comb[1]] - t0s[comb[0]])) * weight)**2\n            return sq_sum\n\n        return quality_function\n\n    qfunc = make_quality_function(means, weights, combs)\n    # t0s = np.zeros(31)\n    t0s = np.random.rand(31)\n    bounds = [(0, 0)] + [(-10., 10.)] * 30\n    opt_t0s = optimize.minimize(qfunc, t0s, bounds=bounds)\n    return opt_t0s", "response": "Varies t0s to minimize the deviation of the gaussian means from zero."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvary sigmas to minimize gaussian sigma12 - sqrt ( sigma1 + sigma2 + sigma1\u00b2 + sigma2\u00b2.", "response": "def minimize_sigmas(sigmas, weights, combs):\n    \"\"\"Varies sigmas to minimize gaussian sigma12 - sqrt(sigma1\u00b2 + sigma2\u00b2).\n\n    Parameters\n    ----------\n    sigmas: numpy array of fitted sigmas of gaussians\n    weights: numpy array of weights for the squared sum\n    combs: pmt combinations to use for minimization\n\n    Returns\n    -------\n    opt_sigmas: optimal sigma values for all PMTs\n\n    \"\"\"\n\n    def make_quality_function(sigmas, weights, combs):\n        def quality_function(s):\n            sq_sum = 0\n            for sigma, comb, weight in zip(sigmas, combs, weights):\n                sigma_sqsum = np.sqrt(s[comb[1]]**2 + s[comb[0]]**2)\n                sq_sum += ((sigma - sigma_sqsum) * weight)**2\n            return sq_sum\n\n        return quality_function\n\n    qfunc = make_quality_function(sigmas, weights, combs)\n    s = np.ones(31) * 2.5\n    # s = np.random.rand(31)\n    bounds = [(0., 5.)] * 31\n    opt_sigmas = optimize.minimize(qfunc, s, bounds=bounds)\n    return opt_sigmas"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimize_qes(fitted_rates, rates, weights, combs):\n\n    def make_quality_function(fitted_rates, rates, weights, combs):\n        def quality_function(qes):\n            sq_sum = 0\n            for fitted_rate, comb, rate, weight  \\\n                    in zip(fitted_rates, combs, rates, weights):\n                sq_sum += ((rate / qes[comb[0]] / qes[comb[1]] - fitted_rate) *\n                           weight)**2\n            return sq_sum\n\n        return quality_function\n\n    qfunc = make_quality_function(fitted_rates, rates, weights, combs)\n    qes = np.ones(31)\n    bounds = [(0.1, 2.)] * 31\n    opt_qes = optimize.minimize(qfunc, qes, bounds=bounds)\n    return opt_qes", "response": "Varies QEs to minimize the deviation of the rates from the fitted_rates rates weights and combs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply optimal t0s to gaussians means.", "response": "def correct_means(means, opt_t0s, combs):\n    \"\"\"Applies optimal t0s to gaussians means.\n\n    Should be around zero afterwards.\n\n    Parameters\n    ----------\n    means: numpy array of means of gaussians of all PMT combinations\n    opt_t0s: numpy array of optimal t0 values for all PMTs\n    combs: pmt combinations used to correct\n\n    Returns\n    -------\n    corrected_means: numpy array of corrected gaussian means for all PMT combs\n\n    \"\"\"\n    corrected_means = np.array([(opt_t0s[comb[1]] - opt_t0s[comb[0]]) - mean\n                                for mean, comb in zip(means, combs)])\n    return corrected_means"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply optimal qes to rates.", "response": "def correct_rates(rates, opt_qes, combs):\n    \"\"\"Applies optimal qes to rates.\n\n    Should be closer to fitted_rates afterwards.\n\n    Parameters\n    ----------\n    rates: numpy array of rates of all PMT combinations\n    opt_qes: numpy array of optimal qe values for all PMTs\n    combs: pmt combinations used to correct\n\n    Returns\n    -------\n    corrected_rates: numpy array of corrected rates for all PMT combinations\n    \"\"\"\n\n    corrected_rates = np.array([\n        rate / opt_qes[comb[0]] / opt_qes[comb[1]]\n        for rate, comb in zip(rates, combs)\n    ])\n    return corrected_rates"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calculate_rms_means(means, corrected_means):\n    rms_means = np.sqrt(np.mean((means - 0)**2))\n    rms_corrected_means = np.sqrt(np.mean((corrected_means - 0)**2))\n    return rms_means, rms_corrected_means", "response": "Calculates the RMS of means from zero before and after corrections a PMT tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating RMS of rates from fitted_rates before and after correction", "response": "def calculate_rms_rates(rates, fitted_rates, corrected_rates):\n    \"\"\"Calculates RMS of rates from fitted_rates before and after correction\n\n    Parameters\n    ----------\n    rates: numpy array of rates of all PMT combinations\n    corrected_rates: numpy array of corrected rates for all PMT combinations\n\n    Returns\n    -------\n    rms_rates: RMS of rates from fitted_rates\n    rms_corrected_rates: RMS of corrected_ratesrates from fitted_rates\n    \"\"\"\n    rms_rates = np.sqrt(np.mean((rates - fitted_rates)**2))\n    rms_corrected_rates = np.sqrt(np.mean((corrected_rates - fitted_rates)**2))\n    return rms_rates, rms_corrected_rates"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd counts to twofold coincidences for a given tmax.", "response": "def add_to_twofold_matrix(times, tdcs, mat, tmax=10):\n    \"\"\"Add counts to twofold coincidences for a given `tmax`.\n\n    Parameters\n    ----------\n    times: np.ndarray of hit times (int32)\n    tdcs: np.ndarray of channel_ids (uint8)\n    mat: ref to a np.array((465, tmax * 2 + 1))\n    tmax: int (time window)\n\n    Returns\n    -------\n    mat: coincidence matrix (np.array((465, tmax * 2 + 1)))\n\n    \"\"\"\n    h_idx = 0    # index of initial hit\n    c_idx = 0    # index of coincident candidate hit\n    n_hits = len(times)\n    multiplicity = 0\n    while h_idx <= n_hits:\n        c_idx = h_idx + 1\n        if (c_idx < n_hits) and (times[c_idx] - times[h_idx] <= tmax):\n            multiplicity = 2\n            c_idx += 1\n            while (c_idx < n_hits) and (times[c_idx] - times[h_idx] <= tmax):\n                c_idx += 1\n                multiplicity += 1\n            if multiplicity != 2:\n                h_idx = c_idx\n                continue\n            c_idx -= 1\n            h_tdc = tdcs[h_idx]\n            c_tdc = tdcs[c_idx]\n            h_time = times[h_idx]\n            c_time = times[c_idx]\n            if h_tdc != c_tdc:\n                dt = int(c_time - h_time)\n                if h_tdc > c_tdc:\n                    mat[get_comb_index(c_tdc, h_tdc), -dt + tmax] += 1\n                else:\n                    mat[get_comb_index(h_tdc, c_tdc), dt + tmax] += 1\n        h_idx = c_idx"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reset(self):\n        self.counts = defaultdict(partial(np.zeros, (465, self.tmax * 2 + 1)))\n        self.n_timeslices = defaultdict(int)", "response": "Reset the coincidence counter to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self):\n        self.print(\"Dumping data to {}\".format(self.dump_filename))\n        pickle.dump({\n            'data': self.counts,\n            'livetime': self.get_livetime()\n        }, open(self.dump_filename, \"wb\"))", "response": "Write coincidence counts into a Python pickle file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_named_by_definition(cls, element_list, string_def):\n        try:\n            return next(\n                (\n                    st.value\n                    for st in element_list\n                    if st.definition == string_def\n                )\n            )\n        except Exception:\n            return None", "response": "Attempts to get an IOOS definition from a list of xml elements"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ioos_def(self, ident, elem_type, ont):\n        if elem_type == \"identifier\":\n            getter_fn = self.system.get_identifiers_by_name\n        elif elem_type == \"classifier\":\n            getter_fn = self.system.get_classifiers_by_name\n        else:\n            raise ValueError(\"Unknown element type '{}'\".format(elem_type))\n        return DescribeSensor.get_named_by_definition(\n            getter_fn(ident), urljoin(ont, ident)\n        )", "response": "Gets a definition given an identifier and where to search for it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sentence(start=None, depth=7):\n    ''' follow the grammatical patterns to generate a random sentence '''\n    if not GRAMMAR:\n        return 'Please set a GRAMMAR file'\n\n    start = start if start else GRAMMAR.start()\n\n    if isinstance(start, Nonterminal):\n        productions = GRAMMAR.productions(start)\n        if not depth:\n            # time to break the cycle\n            terminals = [p for p in productions if not isinstance(start, Nonterminal)]\n            if len(terminals):\n                production = terminals\n        production = random.choice(productions)\n\n        sentence = []\n        for piece in production.rhs():\n            sentence += get_sentence(start=piece, depth=depth-1)\n        return sentence\n    else:\n        return [start]", "response": "follow the grammatical patterns to generate a random sentence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfixing display formatting of a sentence array", "response": "def format_sentence(sentence):\n    ''' fix display formatting of a sentence array '''\n    for index, word in enumerate(sentence):\n        if word == 'a' and index + 1 < len(sentence) and \\\n                re.match(r'^[aeiou]', sentence[index + 1]) and not \\\n                re.match(r'^uni', sentence[index + 1]):\n            sentence[index] = 'an'\n    text = ' '.join(sentence)\n    text = '%s%s' % (text[0].upper(), text[1:])\n    text = text.replace(' ,', ',')\n    return '%s.' % text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_station(self, _id, callSign, name, affiliate, fccChannelNumber):\n\n        if self.__v_station:\n            # [Station: 11440, WFLX, WFLX, Fox Affiliate, 29]\n            # [Station: 11836, WSCV, WSCV, TELEMUNDO (HBC) Affiliate, 51]\n            # [Station: 11867, TBS, Turner Broadcasting System, Satellite, None]\n            # [Station: 11869, WTCE, WTCE, Independent, 21]\n            # [Station: 11924, WTVX, WTVX, CW Affiliate, 34]\n            # [Station: 11991, WXEL, WXEL, PBS Affiliate, 42]\n            # [Station: 12131, TOON, Cartoon Network, Satellite, None]\n            # [Station: 12444, ESPN2, ESPN2, Sports Satellite, None]\n            # [Station: 12471, WFGC, WFGC, Independent, 61]\n            # [Station: 16046, TVNI, TV Chile Internacional, Latin American Satellite, None]\n            # [Station: 22233, GOAC020, Government Access - GOAC020, Cablecast, None]\n            print(\"[Station: %s, %s, %s, %s, %s]\" % \n                  (_id, callSign, name, affiliate, fccChannelNumber))", "response": "Callback run for each new station"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_lineup(self, name, location, device, _type, postalCode, _id):\n\n        if self.__v_lineup:\n            # [Lineup: Comcast West Palm Beach /Palm Beach Co., West Palm Beach, Digital, CableDigital, 33436, FL09567:X]\n            print(\"[Lineup: %s, %s, %s, %s, %s, %s]\" % \n                  (name, location, device, _type, postalCode, _id))", "response": "Callback run for each new lineup"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new_mapping(self, lineup, station, channel, channelMinor,\n                   validFrom, validTo, onAirFrom, onAirTo):\n        \"\"\"Callback run for each new mapping within a lineup\"\"\"\n\n        if self.__v_mapping:\n            # [Mapping: FL09567:X, 11097, 45, None, 2010-06-29 00:00:00.00, None, None, None]\n            print(\"[Mapping: %s, %s, %s, %s, %s, %s, %s, %s]\" % \n                  (lineup, station, channel, channelMinor, validFrom, validTo, \n                   onAirFrom, onAirTo))", "response": "Callback run for each new mapping within a lineup"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_crew_member(self, program, role, fullname, givenname, surname):\n\n        if self.__v_crew_member:\n            # [Crew: EP000036710112, Actor, Estelle Parsons]\n            print(\"[Crew: %s, %s, %s]\" % (program, role, fullname))", "response": "Callback run for each new crew member entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qsub(script, job_name, dryrun=False, *args, **kwargs):\n    print(\"Preparing job script...\")\n    job_string = gen_job(script=script, job_name=job_name, *args, **kwargs)\n    env = os.environ.copy()\n    if dryrun:\n        print(\n            \"This is a dry run! Here is the generated job file, which will \"\n            \"not be submitted:\"\n        )\n        print(job_string)\n    else:\n        print(\"Calling qsub with the generated job script.\")\n        p = subprocess.Popen(\n            'qsub -V', stdin=subprocess.PIPE, env=env, shell=True\n        )\n        p.communicate(input=bytes(job_string.encode('ascii')))", "response": "Submit a job via qsub."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gen_job(\n        script,\n        job_name,\n        log_path='qlogs',\n        group='km3net',\n        platform='cl7',\n        walltime='00:10:00',\n        vmem='8G',\n        fsize='8G',\n        shell=None,\n        email=None,\n        send_mail='n',\n        job_array_start=1,\n        job_array_stop=None,\n        job_array_step=1,\n        irods=False,\n        sps=True,\n        hpss=False,\n        xrootd=False,\n        dcache=False,\n        oracle=False,\n        split_array_logs=False\n):\n    \"\"\"Generate a job script.\"\"\"\n    if shell is None:\n        shell = os.environ['SHELL']\n    if email is None:\n        email = os.environ['USER'] + '@km3net.de'\n    if isinstance(script, Script):\n        script = str(script)\n    log_path = os.path.join(os.getcwd(), log_path)\n    if job_array_stop is not None:\n        job_array_option = \"#$ -t {}-{}:{}\"  \\\n                           .format(job_array_start, job_array_stop,\n                                   job_array_step)\n    else:\n        job_array_option = \"#\"\n    if split_array_logs:\n        task_name = '_$TASK_ID'\n    else:\n        task_name = ''\n    job_string = JOB_TEMPLATE.format(\n        script=script,\n        email=email,\n        send_mail=send_mail,\n        log_path=log_path,\n        job_name=job_name,\n        group=group,\n        walltime=walltime,\n        vmem=vmem,\n        fsize=fsize,\n        irods=irods,\n        sps=sps,\n        hpss=hpss,\n        xrootd=xrootd,\n        dcache=dcache,\n        oracle=oracle,\n        shell=shell,\n        platform=platform,\n        job_array_option=job_array_option,\n        task_name=task_name\n    )\n    return job_string", "response": "Generate a job script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_jpp_env(jpp_dir):\n    env = {\n        v[0]: ''.join(v[1:])\n        for v in [\n            l.split('=') for l in os.popen(\n                \"source {0}/setenv.sh {0} && env\".format(jpp_dir)\n            ).read().split('\\n') if '=' in l\n        ]\n    }\n    return env", "response": "Return the environment dict of a loaded Jpp env."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an iget command to retrieve a file from iRODS.", "response": "def iget(self, irods_path, attempts=1, pause=15):\n        \"\"\"Add an iget command to retrieve a file from iRODS.\n\n        Parameters\n        ----------\n            irods_path: str\n                Filepath which should be fetched using iget\n            attempts: int (default: 1)\n                Number of retries, if iRODS access fails\n            pause: int (default: 15)\n                Pause between two access attempts in seconds\n        \"\"\"\n        if attempts > 1:\n            cmd = \"\"\"   for i in {{1..{0}}}; do\n                            ret=$(iget -v {1} 2>&1)\n                            echo $ret\n                            if [[ $ret == *\"ERROR\"* ]]; then\n                                echo \"Attempt $i failed\"\n                            else\n                                break\n                            fi\n                            sleep {2}s\n                        done \"\"\"\n            cmd = lstrip(cmd)\n            cmd = cmd.format(attempts, irods_path, pause)\n            self.add(cmd)\n        else:\n            self.add('iget -v \"{}\"'.format(irods_path))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_two_argument_command(self, command, arg1, arg2):\n        self.lines.append(\"{} {} {}\".format(command, arg1, arg2))", "response": "Helper function for two - argument commands"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlisting all garage door devices.", "response": "def get_devices(self):\n        \"\"\"List all garage door devices.\"\"\"\n        devices = self.make_request('[\"{username}\",\"{password}\",\"info\",\"\",\"\"]'.format(\n            username=self.username,\n            password=self.password))\n\n        if devices != False:\n            garage_doors = []\n\n            try:\n                self.apicode = devices.find('apicode').text\n                self._device_states = {}\n                for doorNum in range(1, 4):\n                    door = devices.find('door' + str(doorNum))\n                    doorName = door.find('name').text\n                    if doorName:\n                        dev = {'door': doorNum, 'name': doorName}\n                        for id in ['mode', 'sensor', 'status', 'sensorid', 'temperature', 'voltage',\n                                   'camera', 'events', 'permission']:\n                            item = door.find(id)\n                            if item is not None:\n                                dev[id] = item.text\n                        garage_state = door.find('status').text\n                        dev['status'] = self.DOOR_STATE[garage_state]\n                        self._device_states[doorNum] = self.DOOR_STATE[garage_state]\n                        garage_doors.append(dev)\n\n                return garage_doors\n            except TypeError as ex:\n                print(ex)\n                return False\n        else:\n            return False;"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist only MyQ garage door devices.", "response": "def get_status(self, device_id):\n        \"\"\"List only MyQ garage door devices.\"\"\"\n        devices = self.get_devices()\n\n        if devices != False:\n            for device in devices:\n                if device['door'] == device_id:\n                    return device['status']\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef analyze(segments, analysis, lookup=dict(bipa={}, dolgo={})):\n    # raise a ValueError in case of empty segments/strings\n    if not segments:\n        raise ValueError('Empty sequence.')\n\n    # test if at least one element in `segments` has information\n    # (helps to catch really badly formed input, such as ['\\n']\n    if not [segment for segment in segments if segment.strip()]:\n        raise ValueError('No information in the sequence.')\n\n    # build the phonologic and sound class analyses\n    try:\n        bipa_analysis, sc_analysis = [], []\n        for s in segments:\n            a = lookup['bipa'].get(s)\n            if a is None:\n                a = lookup['bipa'].setdefault(s, BIPA[s])\n            bipa_analysis.append(a)\n\n            sc = lookup['dolgo'].get(s)\n            if sc is None:\n                sc = lookup['dolgo'].setdefault(s, BIPA.translate(s, DOLGO))\n            sc_analysis.append(sc)\n    except:  # noqa\n        print(segments)\n        raise\n\n    # compute general errors; this loop must take place outside the\n    # following one because the code for computing single errors (either\n    # in `bipa_analysis` or in `soundclass_analysis`) is unnecessary\n    # complicated\n    for sound_bipa, sound_class in zip(bipa_analysis, sc_analysis):\n        if isinstance(sound_bipa, pyclts.models.UnknownSound) or sound_class == '?':\n            analysis.general_errors += 1\n\n    # iterate over the segments and analyses, updating counts of occurrences\n    # and specific errors\n    for segment, sound_bipa, sound_class in zip(segments, bipa_analysis, sc_analysis):\n        # update the segment count\n        analysis.segments.update([segment])\n\n        # add an error if we got an unknown sound, otherwise just append\n        # the `replacements` dictionary\n        if isinstance(sound_bipa, pyclts.models.UnknownSound):\n            analysis.bipa_errors.add(segment)\n        else:\n            analysis.replacements[sound_bipa.source].add(sound_bipa.__unicode__())\n\n        # update sound class errors, if any\n        if sound_class == '?':\n            analysis.sclass_errors.add(segment)\n\n    return segments, bipa_analysis, sc_analysis, analysis", "response": "Test a sequence for compatibility with CLPA and LingPy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef most_energetic(df):\n    idx = df.groupby(['event_id'])['energy'].transform(max) == df['energy']\n    return df[idx].reindex()", "response": "Grab most energetic particle from mc_tracks dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngrabbing leading particle from a dataframe", "response": "def leading_particle(df):\n    \"\"\"Grab leading particle (neutrino, most energetic bundle muon).\n\n    Note: selecting the most energetic mc particle does not always select\n    the neutrino! In some sub-percent cases, the post-interaction\n    secondaries can have more energy than the incoming neutrino!\n\n    aanet convention: mc_tracks[0] = neutrino\n    so grab the first row\n\n    if the first row is not unique (neutrinos are unique), it's a muon bundle\n    grab the most energetic then\n    \"\"\"\n    leading = df.groupby('event_id', as_index=False).first()\n    unique = leading.type.unique()\n\n    if len(unique) == 1 and unique[0] == 0:\n        leading = most_energetic(df)\n    return leading"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_stream(self, stream_id, sandbox=None):\n        if sandbox is not None:\n            raise NotImplementedError\n\n        logging.debug(\"Creating asset stream {}\".format(stream_id))\n\n        if stream_id in self.streams:\n            raise StreamAlreadyExistsError(\"Stream with id '{}' already exists\".format(stream_id))\n\n        stream = AssetStream(channel=self, stream_id=stream_id, calculated_intervals=None,\n                             last_accessed=utcnow(), last_updated=utcnow(), sandbox=sandbox)\n        self.streams[stream_id] = stream\n        return stream", "response": "Create the stream with the given identifier."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reconnect(self):\n        log.debug(\"Reconnecting to JLigier...\")\n        self._disconnect()\n        self._connect()\n        self._update_subscriptions()", "response": "Reconnect to JLigier and subscribe to the tags."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data(self, value):\n        if not value:\n            value = b''\n        if len(value) > self.SIZE:\n            raise ValueError(\"The maximum tag size is {0}\".format(self.SIZE))\n        self._data = value\n        while len(self._data) < self.SIZE:\n            self._data += b'\\x00'", "response": "Set the byte data and fill up the bytes to fit the size."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef execute(self, sources, sinks, interval):\n        if not isinstance(interval, TimeInterval):\n            raise TypeError('Expected TimeInterval, got {}'.format(type(interval)))\n        # logging.info(self.message(interval))\n\n        calculated_intervals = None\n\n        for sink in sinks:\n            if interval.end > sink.channel.up_to_timestamp:\n                raise ValueError('The stream is not available after {} and cannot be calculated'\n                                 .format(sink.channel.up_to_timestamp))\n            if calculated_intervals is None:\n                calculated_intervals = sink.calculated_intervals\n                continue\n            if sink.calculated_intervals != calculated_intervals:\n                # TODO: What we actually want to do here is find any parts of the sinks that haven't been calculated,\n                # and recompute all of the sinks for that time period. This would only happen if computation of one of\n                # the sinks failed for some reason. For now we will just assume that all sinks have been computed the\n                # same amount, and we will raise an exception if this is not the case\n                raise RuntimeError(\"Partially executed sinks not yet supported\")\n\n        required_intervals = TimeIntervals([interval]) - calculated_intervals\n\n        if not required_intervals.is_empty:\n            for interval in required_intervals:\n                produced_data = set()\n\n                for item in self._execute(sources=sources, interval=interval):\n                    # Join the output meta data with the parent plate meta data\n                    try:\n                        sink = next(s for s in sinks\n                                    if tuple(sorted(s.stream_id.meta_data)) == tuple(sorted(item.meta_data)))\n                    except StopIteration:\n                        # raise\n                        continue\n                    sink.writer(item.stream_instance)\n                    produced_data.add(sink)\n\n                for sink in sinks:\n                    if sink not in produced_data:\n                        logging.debug(\"{} did not produce any data for time interval {} on sink {}\".format(\n                            self.name, interval, sink))\n                    sink.calculated_intervals += interval\n\n                self.write_to_history(\n                    interval=interval,\n                    tool=self.name,\n                    document_count=len(produced_data)\n                )", "response": "Execute the tool over the given time interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets values in constant", "response": "def add(self, name, attr=None, value=None):\n        \"Set values in constant\"\n\n        if isinstance(name, tuple) or isinstance(name, list):\n            name, attr, value = self.__set_iter_value(name)\n\n        if attr is None:\n            attr = name\n\n        if value is None:\n            value = attr\n\n        self.__data += (self.get_const_string(name=name, value=value),)\n        # set attribute as slugfiy\n        self.__dict__[s_attr(attr)] = self.__data[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self, source, interval, input_plate_value):\n        if not isinstance(interval, TimeInterval):\n            raise TypeError('Expected TimeInterval, got {}'.format(type(interval)))\n        # logging.info(self.message(interval))\n\n        output_plate_values = set()\n\n        for item in self._execute(source=source, interval=interval):\n            # Join the output meta data with the parent plate meta data\n            # meta_data = input_plate_value + (item.meta_data,)\n            # sink.writer(item.stream_instance)\n            output_plate_values.add(item.meta_data, )\n\n        if not output_plate_values:\n            logging.debug(\"{} did not produce any data for time interval {} on stream {}\".format(\n                self.name, interval, source))\n\n        self.write_to_history(\n            interval=interval,\n            tool=self.name,\n            document_count=len(output_plate_values)\n        )\n\n        return output_plate_values", "response": "Execute the tool over the given time interval and return the set of meta data values that were written to the output stream."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting IOLoop in daemonized thread.", "response": "def start(self):\n        \"\"\"Start IOLoop in daemonized thread.\"\"\"\n        assert self._thread is None, 'thread already started'\n\n        # configure thread\n        self._thread = Thread(target=self._start_io_loop)\n        self._thread.daemon = True\n\n        # begin thread and block until ready\n        self._thread.start()\n        self._ready.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _start_io_loop(self):\n\n        def mark_as_ready():\n            self._ready.set()\n\n        if not self._io_loop:\n            self._io_loop = ioloop.IOLoop()\n\n        self._io_loop.add_callback(mark_as_ready)\n        self._io_loop.start()", "response": "Start IOLoop then set ready threading. Event."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_ready(self):\n        if not self._thread:\n            return False\n\n        if not self._ready.is_set():\n            return False\n\n        return True", "response": "Is thread & ioloop ready?"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef submit(self, fn, *args, **kwargs):\n        if not self.is_ready():\n            raise ThreadNotStartedError(\n                \"The thread has not been started yet, \"\n                \"make sure you call start() first\"\n            )\n\n        future = Future()\n\n        def execute():\n            \"\"\"Executes fn on the IOLoop.\"\"\"\n            try:\n                result = gen.maybe_future(fn(*args, **kwargs))\n            except Exception:\n                # The function we ran didn't return a future and instead raised\n                # an exception. Let's pretend that it returned this dummy\n                # future with our stack trace.\n                f = gen.Future()\n                f.set_exc_info(sys.exc_info())\n                on_done(f)\n            else:\n                result.add_done_callback(on_done)\n\n        def on_done(f):\n            \"\"\"Sets tornado.Future results to the concurrent.Future.\"\"\"\n\n            if not f.exception():\n                future.set_result(f.result())\n                return\n\n            # if f is a tornado future, then it has exc_info()\n            if hasattr(f, 'exc_info'):\n                exception, traceback = f.exc_info()[1:]\n\n            # else it's a concurrent.future\n            else:\n                # python2's concurrent.future has exception_info()\n                if hasattr(f, 'exception_info'):\n                    exception, traceback = f.exception_info()\n\n                # python3's concurrent.future just has exception()\n                else:\n                    exception = f.exception()\n                    traceback = None\n\n            # python2 needs exc_info set explicitly\n            if _FUTURE_HAS_EXC_INFO:\n                future.set_exception_info(exception, traceback)\n                return\n\n            # python3 just needs the exception, exc_info works fine\n            future.set_exception(exception)\n\n        self._io_loop.add_callback(execute)\n\n        return future", "response": "Submit a function on the IOLoop in daemonized thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning peak memory usage in MB", "response": "def peak_memory_usage():\n    \"\"\"Return peak memory usage in MB\"\"\"\n    if sys.platform.startswith('win'):\n        p = psutil.Process()\n        return p.memory_info().peak_wset / 1024 / 1024\n\n    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n    factor_mb = 1 / 1024\n    if sys.platform == 'darwin':\n        factor_mb = 1 / (1024 * 1024)\n    return mem * factor_mb"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new array of the highest rank of the sequence of alternatives in the sequence.", "response": "def _top(self, k):\r\n        \"\"\"\r\n        Description:\r\n            Top k breaking\r\n        Parameters:\r\n            k: the number of alternatives to break from highest rank\r\n        \"\"\"\r\n        if k > self.m:\r\n            raise ValueError(\"k larger than the number of alternatives\")\r\n        G = np.ones((self.m, self.m))\r\n        #np.fill_diagonal(G, 0)  # erroneous code from prefpy\r\n        for i in range(self.m):\r\n            for j in range(self.m):\r\n                if i == j:\r\n                    continue\r\n                if i > k and j > k:\r\n                    G[i][j] = 0\r\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bot(self, k):\r\n        if k < 2:\r\n            raise ValueError(\"k smaller than 2\")\r\n        G = np.ones((self.m, self.m))\r\n        np.fill_diagonal(G, 0)\r\n        for i in range(self.m):\r\n            for j in range(self.m):\r\n                if i == j:\r\n                    continue\r\n                if i <= k and j <= k:\r\n                    G[i][j] = 0\r\n        return G", "response": "Returns a numpy array of the most recent k alternatives from the lowest rank"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a numpy array where the first element in the array is the same size as the other element in the array.", "response": "def _adj(self, k):\r\n        \"\"\"\r\n        Description:\r\n            Adjacent breaking\r\n        Paramters:\r\n            k: not used\r\n        \"\"\"\r\n        G = np.zeros((self.m, self.m))\r\n        for i in range(self.m):\r\n            for j in range(self.m):\r\n                if i == j+1 or j == i+1:\r\n                    G[i][j] =  1\r\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a numpy array of the position k in the sequence of words in the sequence.", "response": "def _pos(self, k):\r\n        \"\"\"\r\n        Description:\r\n            Position k breaking\r\n        Parameters:\r\n            k: position k is used for the breaking\r\n        \"\"\"\r\n        if k < 2:\r\n            raise ValueError(\"k smaller than 2\")\r\n        G = np.zeros((self.m, self.m))\r\n        for i in range(self.m):\r\n            for j in range(self.m):\r\n                if i == j:\r\n                    continue\r\n                if i < k or j < k:\r\n                    continue\r\n                if i == k or j == k:\r\n                    G[i][j] = 1\r\n        return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the most recent ranking of the current state of the current state of the current state.", "response": "def aggregate(self, rankings, breaking=\"full\", k=None):\r\n        \"\"\"\r\n        Description:\r\n            Takes in a set of rankings and computes the\r\n            Plackett-Luce model aggregate ranking.\r\n        Parameters:\r\n            rankings: set of rankings to aggregate\r\n            breaking: type of breaking to use\r\n            k:        number to be used for top, bottom, and position breakings\r\n        \"\"\"\r\n\r\n        breakings = { \"full\":     self._full,\r\n                      \"top\":      self._top,\r\n                      \"bottom\":   self._bot,\r\n                      \"adjacent\": self._adj,\r\n                      \"position\": self._pos }\r\n\r\n        if (k == None and (breaking != \"full\" != breaking != \"position\")):\r\n            raise ValueError(\"k cannot be None for non-full or non-position breaking\")\r\n\r\n        break_mat = breakings[breaking](k)\r\n        P = np.zeros((self.m, self.m))\r\n        for ranking in rankings:\r\n            localP = np.zeros((self.m, self.m))\r\n            for ind1, alt1 in enumerate(self.alts):\r\n                for ind2, alt2 in enumerate(self.alts):\r\n                    if ind1 == ind2:\r\n                        continue\r\n                    alt1_rank = util.get_index_nested(ranking, alt1)\r\n                    alt2_rank = util.get_index_nested(ranking, alt2)\r\n                    if alt1_rank < alt2_rank: # alt 1 is ranked higher\r\n                        localP[ind1][ind2] = 1\r\n            for ind, alt in enumerate(self.alts):\r\n                localP[ind][ind] = -1*(np.sum(localP.T[ind][:ind]) +\r\n                                       np.sum(localP.T[ind][ind+1:]))\r\n            localP *= break_mat\r\n            P += localP/len(rankings)\r\n        #epsilon = 1e-7\r\n        #assert(np.linalg.matrix_rank(P) == self.m-1)\r\n        #assert(all(np.sum(P, axis=0) <= epsilon))\r\n        U, S, V = np.linalg.svd(P)\r\n        gamma = np.abs(V[-1])\r\n        gamma /= np.sum(gamma)\r\n        #assert(all(np.dot(P, gamma) < epsilon))\r\n        alt_scores = {cand: gamma[ind] for ind, cand in enumerate(self.alts)}\r\n        self.P = P\r\n        self.create_rank_dicts(alt_scores)\r\n        return gamma"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getElecType(self): \n\n        tiesPresent = False\n        incompletePresent = False\n\n        # Go through the list of Preferences and see if any contain a tie or are incomplete.\n        for preference in self.preferences:\n            if preference.containsTie() == True:\n                tiesPresent = True\n                break\n            if preference.isFullPreferenceOrder(self.candMap.keys()) == False:\n                incompletePresent = True\n                break\n\n        if tiesPresent == False and incompletePresent == False:\n            elecType = \"soc\"\n        elif tiesPresent == False and incompletePresent == True:\n            elecType = \"soi\"\n        elif tiesPresent == True and incompletePresent == False:\n            elecType = \"toc\"\n        elif tiesPresent == True and incompletePresent == True:\n            elecType = \"toi\"\n        return elecType", "response": "Determines whether the list of Preferences objects represents complete strict orderings over the given entry point."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getPreferenceCounts(self):\n\n        preferenceCounts = []\n        for preference in self.preferences:\n            preferenceCounts.append(preference.count)\n        return preferenceCounts", "response": "Returns a list of the number of times each preference is given."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getRankMaps(self):\n\n        rankMaps = []\n        for preference in self.preferences:\n            rankMaps.append(preference.getRankMap())\n        return rankMaps", "response": "Returns a list of dictionaries one for each preference that associates the integer \n        representation of each candidate with its position in the ranking starting from 1."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of dictionaries one for each preference that associates each position in that the ranking with a list of integer representations of the candidates ranked at that ArcGIS position.", "response": "def getReverseRankMaps(self):\n        \"\"\"\n        Returns a list of dictionaries, one for each preference, that associates each position in\n        the ranking with a list of integer representations of the candidates ranked at that \n        position and returns a list of the number of times each preference is given.\n        \"\"\"\n\n        reverseRankMaps = []\n        for preference in self.preferences:\n            reverseRankMaps.append(preference.getReverseRankMap())\n        return reverseRankMaps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getOrderVectors(self):\n\n        orderVectors = []\n        for preference in self.preferences:\n            orderVectors.append(preference.getOrderVector())\n        return orderVectors", "response": "Returns a list of lists one for each preference of candidates ordered from most preferred\n        to least preferred\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getOrderVectorsEGMM(self):\n\n        orderVectors = []\n        for preference in self.preferences:\n            orderVectors.append(preference.getOrderVectorEGMM())\n        return orderVectors", "response": "Returns a list of lists one for each preference of the ties ordered from most preferred\n        to least preferred\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a weighted majority graph that represents the whole profile.", "response": "def getWmg(self, normalize = False):\n        \"\"\"\n        Generate a weighted majority graph that represents the whole profile. The function will\n        return a two-dimensional dictionary that associates integer representations of each pair of\n        candidates, cand1 and cand2, with the number of times cand1 is ranked above cand2 minus the\n        number of times cand2 is ranked above cand1.\n                \n        :ivar bool normalize: If normalize is True, the function will return a normalized graph\n            where each edge has been divided by the value of the largest edge.\n        \"\"\"\n\n        # Initialize a new dictionary for our final weighted majority graph.\n        wmgMap = dict()\n        for cand in self.candMap.keys():\n            wmgMap[cand] = dict()\n        for cand1, cand2 in itertools.combinations(self.candMap.keys(), 2):\n            wmgMap[cand1][cand2] = 0\n            wmgMap[cand2][cand1] = 0\n\n        # Go through the wmgMaps and increment the value of each edge in our final graph with the \n        # edges in each of the wmgMaps. We take into account the number of times that the vote \n        # occured.\n        for i in range(0, len(self.preferences)):\n            preference = self.preferences[i]\n            preferenceWmgMap = preference.wmgMap\n            for cand1, cand2 in itertools.combinations(preferenceWmgMap.keys(), 2):\n                if cand2 in preferenceWmgMap[cand1].keys():\n                    wmgMap[cand1][cand2] += preferenceWmgMap[cand1][cand2]*preference.count\n                    wmgMap[cand2][cand1] += preferenceWmgMap[cand2][cand1]*preference.count\n\n        # By default, we assume that the weighted majority graph should not be normalized. If\n        # desired, we normalize by dividing each edge by the value of the largest edge. \n        if (normalize == True):\n            maxEdge = float('-inf')\n            for cand in wmgMap.keys():\n                maxEdge = max(maxEdge, max(wmgMap[cand].values()))\n            for cand1 in wmgMap.keys():\n                for cand2 in wmgMap[cand1].keys():\n                    wmgMap[cand1][cand2] = float(wmgMap[cand1][cand2])/maxEdge\n        \n        return wmgMap"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports a preflib format file that contains all the information of the current Profile.", "response": "def exportPreflibFile(self, fileName):\n        \"\"\"\n        Exports a preflib format file that contains all the information of the current Profile.\n\n        :ivar str fileName: The name of the output file to be exported.\n        \"\"\"\n\n        elecType = self.getElecType()\n\n        if elecType != \"soc\" and elecType != \"toc\" and elecType != \"soi\" and elecType != \"toi\":\n            print(\"ERROR: printing current type to preflib format is not supported\")\n            exit()\n\n        # Generate a list of reverse rankMaps, one for each vote. This will allow us to easiliy\n        # identify ties.\n        reverseRankMaps = self.getReverseRankMaps()\n\n        outfileObj = open(fileName, 'w')\n\n        # Print the number of candidates and the integer representation and name of each candidate.\n        outfileObj.write(str(self.numCands))\n        for candInt, cand in self.candMap.items():\n            outfileObj.write(\"\\n\" + str(candInt) + \",\" + cand)\n\n        # Sum up the number of preferences that are represented.\n        preferenceCount = 0\n        for preference in self.preferences:\n            preferenceCount += preference.count\n\n        # Print the number of voters, the sum of vote count, and the number of unique orders.\n        outfileObj.write(\"\\n\" + str(self.numVoters) + \",\" + str(preferenceCount) + \",\" + str(len(self.preferences)))\n\n        for i in range(0, len(reverseRankMaps)):\n\n            # First, print the number of times the preference appears.\n            outfileObj.write(\"\\n\" + str(self.preferences[i].count))\n            \n            reverseRankMap = reverseRankMaps[i]\n\n            # We sort the positions in increasing order and print the candidates at each position\n            # in order.\n            sortedKeys = sorted(reverseRankMap.keys())\n            for key in sortedKeys:\n\n                cands = reverseRankMap[key]\n\n                # If only one candidate is in a particular position, we assume there is no tie.\n                if len(cands) == 1:\n                    outfileObj.write(\",\" + str(cands[0]))\n\n                # If more than one candidate is in a particular position, they are tied. We print\n                # brackets around the candidates.\n                elif len(cands) > 1:\n                    outfileObj.write(\",{\" + str(cands[0]))\n                    for j in range(1, len(cands)):\n                        outfileObj.write(\",\" + str(cands[j]))\n                    outfileObj.write(\"}\")\n                    \n        outfileObj.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport a preflib format file that contains all the information of a Profile. This function will automatically create the Profile object for each member of the current Profile object.", "response": "def importPreflibFile(self, fileName):\n        \"\"\"\n        Imports a preflib format file that contains all the information of a Profile. This function\n        will completely override all members of the current Profile object. Currently, we assume \n        that in an election where incomplete ordering are allowed, if a voter ranks only one \n        candidate, then the voter did not prefer any candidates over another. This may lead to some\n        discrepancies when importing and exporting a .toi preflib file or a .soi preflib file.\n\n        :ivar str fileName: The name of the input file to be imported.\n        \"\"\"\n\n        # Use the functionality found in io to read the file.\n        elecFileObj = open(fileName, 'r')\n        self.candMap, rankMaps, wmgMapsCounts, self.numVoters = prefpy_io.read_election_file(elecFileObj)\n        elecFileObj.close()\n\n        self.numCands = len(self.candMap.keys())\n\n        # Go through the rankMaps and generate a wmgMap for each vote. Use the wmgMap to create a\n        # Preference object.\n        self.preferences = []\n        for i in range(0, len(rankMaps)):\n            wmgMap = self.genWmgMapFromRankMap(rankMaps[i])\n            self.preferences.append(Preference(wmgMap, wmgMapsCounts[i]))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexporting a json file that contains all the information of the current Profile.", "response": "def exportJsonFile(self, fileName):\n        \"\"\"\n        Exports a json file that contains all the information of the current Profile.\n\n        :ivar str fileName: The name of the output file to be exported.\n        \"\"\"\n\n        # Because our Profile class is not directly JSON serializable, we exporrt the underlying \n        # dictionary. \n        data = dict()\n        for key in self.__dict__.keys():\n            if key != \"preferences\":\n                data[key] = self.__dict__[key]\n        \n        # The Preference class is also not directly JSON serializable, so we export the underlying\n        # dictionary for each Preference object.\n        preferenceDicts = []\n        for preference in self.preferences:\n            preferenceDict = dict()\n            for key in preference.__dict__.keys():\n                preferenceDict[key] = preference.__dict__[key]\n            preferenceDicts.append(preferenceDict)\n        data[\"preferences\"] = preferenceDicts\n\n        outfile = open(fileName, 'w')\n        json.dump(data, outfile)\n        outfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports a json file that contains all the information of a Profile and creates a list of all the members of the current Profile object. This function will import all the information of a Profile and all the members of the Profile object.", "response": "def importJsonFile(self, fileName):\n        \"\"\"\n        Imports a json file that contains all the information of a Profile. This function will\n        completely override all members of the current Profile object.\n\n        :ivar str fileName: The name of the input file to be imported.\n        \"\"\"\n\n        infile = open(fileName)\n        data = json.load(infile)\n        infile.close()\n        \n        self.numCands = int(data[\"numCands\"])\n        self.numVoters = int(data[\"numVoters\"])\n\n        # Because the json.load function imports everything as unicode strings, we will go through\n        # the candMap dictionary and convert all the keys to integers and convert all the values to\n        # ascii strings.\n        candMap = dict()\n        for key in data[\"candMap\"].keys():\n            candMap[int(key)] = data[\"candMap\"][key].encode(\"ascii\")\n        self.candMap = candMap\n        \n        # The Preference class is also not directly JSON serializable, so we exported the \n        # underlying dictionary for each Preference object. When we import, we will create a \n        # Preference object from these dictionaries.\n        self.preferences = []\n        for preferenceMap in data[\"preferences\"]:\n            count = int(preferenceMap[\"count\"])\n\n            # Because json.load imports all the items in the wmgMap as unicode strings, we need to\n            # convert all the keys and values into integers.\n            preferenceWmgMap = preferenceMap[\"wmgMap\"]\n            wmgMap = dict()\n            for key in preferenceWmgMap.keys():\n                wmgMap[int(key)] = dict()\n                for key2 in preferenceWmgMap[key].keys():\n                    wmgMap[int(key)][int(key2)] = int(preferenceWmgMap[key][key2])\n\n            self.preferences.append(Preference(wmgMap, count))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef aggregate(self, rankings, epsilon, max_iters):\r\n\r\n        # compute the matrix w, the numbers of pairwise wins:\r\n        w = np.zeros((self.m, self.m))\r\n        for ranking in rankings:\r\n            localw = np.zeros((self.m, self.m))\r\n            for ind1, alt1 in enumerate(self.alts):\r\n                for ind2, alt2 in enumerate(self.alts):\r\n                    if ind1 == ind2:\r\n                        continue\r\n                    alt1_rank = util.get_index_nested(ranking, alt1)\r\n                    alt2_rank = util.get_index_nested(ranking, alt2)\r\n                    if alt1_rank < alt2_rank: # alt 1 is ranked higher\r\n                        localw[ind1][ind2] = 1\r\n            w += localw\r\n        W = w.sum(axis=1)\r\n\r\n        # gamma_t is the value of gamma at time = t\r\n        # gamma_t1 is the value of gamma at time t = t+1 (the next iteration)\r\n        # initial arbitrary value for gamma:\r\n        gamma_t = np.ones(self.m) / self.m\r\n        gamma_t1 = np.empty(self.m)\r\n\r\n        for f in range(max_iters):\r\n\r\n            for i in range(self.m):\r\n                s = 0 # sum of updating function\r\n                for j in range(self.m):\r\n                    if j != i:\r\n                        s += (w[j][i] + w[i][j]) / (gamma_t[i]+gamma_t[j])\r\n\r\n                gamma_t1[i] = W[i] / s\r\n\r\n            gamma_t1 /= np.sum(gamma_t1)\r\n\r\n            if epsilon != None and np.all(np.absolute(gamma_t1 - gamma_t) < epsilon):\r\n                alt_scores = {cand: gamma_t1[ind] for ind, cand in enumerate(self.alts)}\r\n                self.create_rank_dicts(alt_scores)\r\n                return gamma_t1 # convergence reached before max_iters\r\n\r\n            gamma_t = gamma_t1 # update gamma_t for the next iteration\r\n        alt_scores = {cand: gamma_t1[ind] for ind, cand in enumerate(self.alts)}\r\n        self.create_rank_dicts(alt_scores)\r\n        return gamma_t1", "response": "Returns an aggregate of the ground - truth parameters and the gamma for each entry in the given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_login_url(self, state=None):\n        payload = {'response_type': 'code',\n                   'client_id': self._client_id,\n                   'redirect_uri': self._redirect_uri,}\n        if state is not None:\n            payload['state'] = state\n        return \"%s?%s\" % (settings.API_AUTHORIZATION_URL,\n                          urllib.urlencode(payload))", "response": "Generates and returns URL for redirecting to the RunKeeper Login Page of Health Graph API."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns URL for image used for RunKeeper Login button.", "response": "def get_login_button_url(self, button_color=None, caption_color=None, button_size=None):\n        \"\"\"Return URL for image used for RunKeeper Login button.\n        \n        @param button_color:  Button color. Either 'blue', 'grey' or 'black'.\n                              Default: 'blue'.\n        @param caption_color: Button text color. Either 'white' or 'black'.\n                              Default: 'white'\n        @param button_size:   Button width in pixels. Either 200, 300 or 600.\n                              Default: 200\n        @return:              URL for Login Button Image.\n        \n        \"\"\"\n        if not button_color in settings.LOGIN_BUTTON_COLORS:\n            button_color = settings.LOGIN_BUTTON_COLORS[0]\n        if not caption_color in settings.LOGIN_BUTTON_CAPTION_COLORS:\n            caption_color = settings.LOGIN_BUTTON_CAPTION_COLORS[0]\n        if settings.LOGIN_BUTTON_SIZES.has_key(button_size):\n            button_size = settings.LOGIN_BUTTON_SIZES[button_size]\n        else:\n            button_size = settings.LOGIN_BUTTON_SIZES['None']\n        return settings.LOGIN_BUTTON_URL % (button_color, \n                                            caption_color, \n                                            button_size)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_access_token(self, code):\n        payload = {'grant_type': 'authorization_code',\n                   'code': code,\n                   'client_id': self._client_id,\n                   'client_secret': self._client_secret,\n                   'redirect_uri': self._redirect_uri,}\n        req = requests.post(settings.API_ACCESS_TOKEN_URL, data=payload)\n        data = req.json()\n        return data.get('access_token')", "response": "Returns the Access Token retrieved from the Health Graph API Token \n        Endpoint following the login to RunKeeper."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrevoke the Access Token by accessing the De - authorization Endpoint of Health Graph API.", "response": "def revoke_access_token(self, access_token):\n        \"\"\"Revokes the Access Token by accessing the De-authorization Endpoint\n        of Health Graph API.\n        \n        @param access_token: Access Token for querying Health Graph API.\n        \n        \"\"\"\n        payload = {'access_token': access_token,}\n        req = requests.post(settings.API_DEAUTHORIZATION_URL, data=payload)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a time tuple into a TimeInterval or RelativeTimeInterval object.", "response": "def parse_time_tuple(start, end):\n    \"\"\"\n    Parse a time tuple. These can be:\n      relative in seconds,       e.g. (-4, 0)\n      relative in timedelta,     e.g. (timedelta(seconds=-4), timedelta(0))\n      absolute in date/datetime, e.g. (datetime(2016, 4, 28, 20, 0, 0, 0, UTC), datetime(2016, 4, 28, 21, 0, 0, 0, UTC))\n      absolute in iso strings,   e.g. (\"2016-04-28T20:00:00.000Z\", \"2016-04-28T20:01:00.000Z\")\n      Mixtures of relative and absolute are not allowed\n\n    :param start: Start time\n    :param end: End time\n    :type start: int | timedelta | datetime | str\n    :type end: int | timedelta | datetime | str\n    :return: TimeInterval or RelativeTimeInterval object\n    \"\"\"\n    if isinstance(start, int):\n        start_time = timedelta(seconds=start)\n    elif isinstance(start, timedelta):\n        start_time = start\n    elif start is None:\n        start_time = MIN_DATE\n    elif isinstance(start, (date, datetime)):\n        start_time = start.replace(tzinfo=UTC)\n    else:\n        start_time = ciso8601.parse_datetime(start).replace(tzinfo=UTC)\n\n    if isinstance(end, int):\n        # TODO: add check for future (negative values) and ensure that start < end\n        if not isinstance(start_time, timedelta):\n            raise ValueError(\"Can't mix relative and absolute times\")\n        end_time = timedelta(seconds=end)\n    elif isinstance(end, timedelta):\n        if not isinstance(start_time, timedelta):\n            raise ValueError(\"Can't mix relative and absolute times\")\n        end_time = end\n    elif end is None:\n        end_time = utcnow()  # TODO: or MAX_DATE?\n    elif isinstance(end, datetime):\n        end_time = end.replace(tzinfo=UTC)\n    else:\n        end_time = ciso8601.parse_datetime(end).replace(tzinfo=UTC)\n\n    if isinstance(start_time, timedelta):\n        return RelativeTimeInterval(start=start_time, end=end_time)\n    else:\n        return TimeInterval(start=start_time, end=end_time)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split(self, points):\n        '''Splits the list of time intervals in the specified points\n\n        The function assumes that the time intervals do not overlap and ignores\n        points that are not inside of any interval.\n\n        Parameters\n        ==========\n        points: list of datetime\n        '''\n        for p in points:\n            for i in range(len(self.intervals)):\n                if (self.intervals[i].start < p) and (self.intervals[i].end > p):\n                    self.intervals = (self.intervals[:i]\n                                     + [TimeInterval(self.intervals[i].start, p),\n                                        TimeInterval(p, self.intervals[i].end)]\n                                     + self.intervals[(i + 1):])\n                    break", "response": "Splits the list of time intervals in the specified points into two lists of time intervals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_clear(self, arg, arguments):\n\n        sys.stdout.write(os.popen('clear').read())", "response": "Clear the current color screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves object to persistent storage.", "response": "def save(self, status=None, callback_pos=None, id_workflow=None):\n        \"\"\"Save object to persistent storage.\"\"\"\n        if self.model is None:\n            raise WorkflowsMissingModel()\n\n        with db.session.begin_nested():\n            workflow_object_before_save.send(self)\n\n            self.model.modified = datetime.now()\n            if status is not None:\n                self.model.status = status\n\n            if id_workflow is not None:\n                workflow = Workflow.query.filter_by(uuid=id_workflow).one()\n                self.model.workflow = workflow\n\n            # Special handling of JSON fields to mark update\n            if self.model.callback_pos is None:\n                self.model.callback_pos = list()\n            elif callback_pos is not None:\n                self.model.callback_pos = callback_pos\n            flag_modified(self.model, 'callback_pos')\n\n            if self.model.data is None:\n                self.model.data = dict()\n            flag_modified(self.model, 'data')\n\n            if self.model.extra_data is None:\n                self.model.extra_data = dict()\n            flag_modified(self.model, 'extra_data')\n\n            db.session.merge(self.model)\n\n            if self.id is not None:\n                self.log.debug(\"Saved object: {id} at {callback_pos}\".format(\n                    id=self.model.id or \"new\",\n                    callback_pos=self.model.callback_pos\n                ))\n        workflow_object_after_save.send(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new Workflow Object with given content.", "response": "def create(cls, data, **kwargs):\n        \"\"\"Create a new Workflow Object with given content.\"\"\"\n        with db.session.begin_nested():\n            model = cls.dbmodel(**kwargs)\n            model.data = data\n            obj = cls(model)\n            db.session.add(obj.model)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a workflow object from id.", "response": "def get(cls, id_):\n        \"\"\"Return a workflow object from id.\"\"\"\n        with db.session.no_autoflush:\n            query = cls.dbmodel.query.filter_by(id=id_)\n            try:\n                model = query.one()\n            except NoResultFound:\n                raise WorkflowsMissingObject(\"No object for for id {0}\".format(\n                    id_\n                ))\n            return cls(model)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap sqlalchemy query methods of sqlalchemy. BaseQuery.", "response": "def query(cls, *criteria, **filters):\n        \"\"\"Wrap sqlalchemy query methods.\n\n        A wrapper for the filter and filter_by functions of sqlalchemy.\n        Define a dict with which columns should be filtered by which values.\n\n        .. codeblock:: python\n\n            WorkflowObject.query(id=123)\n            WorkflowObject.query(status=ObjectStatus.COMPLETED)\n\n        The function supports also \"hybrid\" arguments using WorkflowObjectModel\n        indirectly.\n\n        .. codeblock:: python\n\n            WorkflowObject.query(\n                WorkflowObject.dbmodel.status == ObjectStatus.COMPLETED,\n                user_id=user_id\n            )\n\n        See also SQLAlchemy BaseQuery's filter and filter_by documentation.\n        \"\"\"\n        query = cls.dbmodel.query.filter(\n            *criteria).filter_by(**filters)\n        return [cls(obj) for obj in query.all()]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(self, force=False):\n        if self.model is None:\n            raise WorkflowsMissingModel()\n\n        with db.session.begin_nested():\n            db.session.delete(self.model)\n\n        return self", "response": "Delete a workflow object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nassigning an action to this object and a message to the user.", "response": "def set_action(self, action, message):\n        \"\"\"Set the action to be taken for this object.\n\n        Assign an special \"action\" to this object to be taken\n        in consideration in Holding Pen. The widget is referred to\n        by a string with the filename minus extension.\n\n        A message is also needed to tell the user the action\n        required in a textual way.\n\n        :param action: name of the action to add (i.e. \"approval\")\n        :type action: string\n\n        :param message: message to show to the user\n        :type message: string\n        \"\"\"\n        self.extra_data[\"_action\"] = action\n        self.extra_data[\"_message\"] = message"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef start_workflow(self, workflow_name, delayed=False, **kwargs):\n        from .tasks import start\n\n        if delayed:\n            self.save()\n            db.session.commit()\n            return start.delay(workflow_name, object_id=self.id, **kwargs)\n        else:\n            return start(workflow_name, data=[self], **kwargs)", "response": "Start the workflow specified on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncontinues the workflow for this object.", "response": "def continue_workflow(self, start_point=\"continue_next\",\n                          delayed=False, **kwargs):\n        \"\"\"Continue the workflow for this object.\n\n        The parameter `start_point` allows you to specify the point of where\n        the workflow shall continue:\n\n        * restart_prev: will restart from the previous task\n\n        * continue_next: will continue to the next task\n\n        * restart_task: will restart the current task\n\n        :param start_point: where should the workflow start from?\n        :type start_point: str\n\n        :param delayed: should the workflow run asynchronously?\n        :type delayed: bool\n\n        :return: UUID of WorkflowEngine (or AsyncResult).\n        \"\"\"\n        from .tasks import resume\n\n        self.save()\n        if not self.id_workflow:\n            raise WorkflowAPIError(\"No workflow associated with object: %r\"\n                                   % (repr(self),))\n        if delayed:\n            db.session.commit()\n            return resume.delay(self.id, start_point, **kwargs)\n        else:\n            return resume(self.id, start_point, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_current_task_info(self):\n        name = self.model.workflow.name\n        if not name:\n            return\n\n        current_task = workflows[name].workflow\n        for step in self.callback_pos:\n            current_task = current_task[step]\n            if callable(current_task):\n                return get_func_info(current_task)", "response": "Return dictionary of current task function info for this object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassemble a basic WSGI - compatible application providing functionality of git - http - backend.", "response": "def assemble_WSGI_git_app(*args, **kw):\r\n    '''\r\n    Assembles basic WSGI-compatible application providing functionality of git-http-backend.\r\n\r\n    content_path (Defaults to '.' = \"current\" directory)\r\n        The path to the folder that will be the root of served files. Accepts relative paths.\r\n\r\n    uri_marker (Defaults to '')\r\n        Acts as a \"virtual folder\" separator between decorative URI portion and\r\n        the actual (relative to content_path) path that will be appended to\r\n        content_path and used for pulling an actual file.\r\n\r\n        the URI does not have to start with contents of uri_marker. It can\r\n        be preceeded by any number of \"virtual\" folders. For --uri_marker 'my'\r\n        all of these will take you to the same repo:\r\n            http://localhost/my/HEAD\r\n            http://localhost/admysf/mylar/zxmy/my/HEAD\r\n        This WSGI hanlder will cut and rebase the URI when it's time to read from file system.\r\n\r\n        Default of '' means that no cutting marker is used, and whole URI after FQDN is\r\n        used to find file relative to content_path.\r\n\r\n    returns WSGI application instance.\r\n    '''\r\n\r\n    default_options = [\r\n        ['content_path','.'],\r\n        ['uri_marker','']\r\n    ]\r\n    args = list(args)\r\n    options = dict(default_options)\r\n    options.update(kw)\r\n    while default_options and args:\r\n        _d = default_options.pop(0)\r\n        _a = args.pop(0)\r\n        options[_d[0]] = _a\r\n    options['content_path'] = os.path.abspath(options['content_path'].decode('utf8'))\r\n    options['uri_marker'] = options['uri_marker'].decode('utf8')\r\n\r\n    selector = WSGIHandlerSelector()\r\n    generic_handler = StaticWSGIServer(**options)\r\n    git_inforefs_handler = GitHTTPBackendInfoRefs(**options)\r\n    git_rpc_handler = GitHTTPBackendSmartHTTP(**options)\r\n\r\n    if options['uri_marker']:\r\n        marker_regex = r'(?P<decorative_path>.*?)(?:/'+ options['uri_marker'] + ')'\r\n    else:\r\n        marker_regex = ''\r\n\r\n    selector.add(\r\n        marker_regex + r'(?P<working_path>.*?)/info/refs\\?.*?service=(?P<git_command>git-[^&]+).*$',\r\n        GET = git_inforefs_handler,\r\n        HEAD = git_inforefs_handler\r\n        )\r\n    selector.add(\r\n        marker_regex + r'(?P<working_path>.*)/(?P<git_command>git-[^/]+)$',\r\n        POST = git_rpc_handler\r\n        )\r\n    selector.add(\r\n        marker_regex + r'(?P<working_path>.*)$',\r\n        GET = generic_handler,\r\n        HEAD = generic_handler)\r\n\r\n    return selector"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef canned_handlers(self, environ, start_response, code = '200', headers = []):\r\n        '''\r\n        We convert an error code into\r\n        certain action over start_response and return a WSGI-compliant payload.\r\n        '''\r\n        headerbase = [('Content-Type', 'text/plain')]\r\n        if headers:\r\n            hObj = Headers(headerbase)\r\n            for header in headers:\r\n                hObj[header[0]] = '; '.join(header[1:])\r\n        start_response(self.canned_collection[code], headerbase)\r\n        return ['']", "response": "A list of handlers that can be used to handle the error code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a new selector mapping.", "response": "def add(self, path, default_handler = None, **http_methods):\r\n        \"\"\"\r\n        Add a selector mapping.\r\n\r\n        add(path, default_handler, **named_handlers)\r\n\r\n        Adding order is important. Firt added = first matched.\r\n        If you want to hand special case URI handled by one app and shorter\r\n        version of the same regex string by anoter app,\r\n        .add() special case first.\r\n\r\n        Inputs:\r\n         path - A regex string. We will compile it.\r\n          Highly recommend using grouping of type: \"(?P<groupname>.+)\"\r\n          These will be exposed to WSGI app through environment key\r\n          per http://www.wsgi.org/wsgi/Specifications/routing_args\r\n\r\n         default_handler - (optional) A pointer to the function / iterable\r\n          class instance that will handle ALL HTTP methods (verbs)\r\n\r\n         **named_handlers - (optional) An unlimited list of named args or\r\n          an unpacked dict of handlers allocated to handle specific HTTP\r\n          methods (HTTP verbs). See \"Examples\" below.\r\n\r\n        Matched named method handlers override default handler.\r\n\r\n        If neither default_handler nor named_handlers point to any methods,\r\n        \"Method not implemented\" is returned for the requests on this URI.\r\n\r\n        Examples:\r\n        selectorInstance.add('^(?P<working_path>.*)$',generic_handler,\r\n                              POST=post_handler, HEAD=head_handler)\r\n\r\n        custom_assembled_dict = {'GET':wsgi_app_a,'POST':wsgi_app_b}:\r\n        ## note the unpacking - \"**\" - of the dict in this case.\r\n        selectorInstance.add('^(?P<working_path>.*)$', **custom_assembled_dict)\r\n\r\n\r\n        If the string contains '\\?' (escaped ?, which translates to '?' in\r\n        non-regex strings) we understand that as \"do regex matching on\r\n        QUERY_PATH + '?' + QUERY_STRING\"\r\n\r\n        When lookup matches are met, results are injected into\r\n        environ['wsgiorg.routing_args'] per\r\n        http://www.wsgi.org/wsgi/Specifications/routing_args\r\n        \"\"\"\r\n        if default_handler:\r\n            methods = defaultdict(lambda: default_handler, http_methods.copy())\r\n        else:\r\n            methods = http_methods.copy()\r\n        self.mappings.append((re.compile(path.decode('utf8')), methods, (path.find(r'\\?')>-1) ))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_checks(self, dataObj, environ, start_response):\r\n        '''\r\n        This function is shared by GitInfoRefs and SmartHTTPRPCHandler WSGI classes.\r\n        It does the same basic steps - figure out working path, git command etc.\r\n\r\n        dataObj - dictionary\r\n        Because the dataObj passed in is mutable, it's a pointer. Once this function returns,\r\n        this object, as created by calling class, will have the free-form updated data.\r\n\r\n        Returns non-None object if an error was triggered (and already prepared in start_response).\r\n        '''\r\n        selector_matches = (environ.get('wsgiorg.routing_args') or ([],{}))[1]\r\n\r\n        # making sure we have a compatible git command\r\n        git_command = selector_matches.get('git_command') or ''\r\n        if git_command not in ['git-upload-pack', 'git-receive-pack']: # TODO: this is bad for future compatibility. There may be more commands supported then.\r\n            return self.canned_handlers(environ, start_response, 'bad_request')\r\n\r\n        # TODO: Add \"public\" to \"dynamic local\" path conversion hook ups here.\r\n\r\n        #############################################################\r\n        # making sure local path is a valid git repo folder\r\n        #\r\n        repo_path = os.path.abspath(\r\n            os.path.join(\r\n                self.content_path,\r\n                (selector_matches.get('working_path') or '').decode('utf8').strip('/').strip('\\\\')\r\n                )\r\n            )\r\n        _pp = os.path.abspath(self.content_path)\r\n\r\n        # this saves us from \"hackers\" putting relative paths after repo marker.\r\n        if not repo_path.startswith(_pp):\r\n            return self.canned_handlers(environ, start_response, 'forbidden')\r\n\r\n        if not self.has_access(\r\n            environ = environ,\r\n            repo_path = repo_path,\r\n            git_command = git_command\r\n            ):\r\n            return self.canned_handlers(environ, start_response, 'forbidden')\r\n\r\n        try:\r\n            files = os.listdir(repo_path)\r\n        except:\r\n            files = []\r\n        if not self.git_folder_signature.issubset([i.lower() for i in files]):\r\n            if not ( self.repo_auto_create and git_command == 'git-receive-pack' ):\r\n                return self.canned_handlers(environ, start_response, 'not_found')\r\n            else:\r\n                # 1. traverse entire post-prefix path and check that each segment\r\n                #    If it is ( a git folder OR a non-dir object ) forbid autocreate\r\n                # 2. Create folderS\r\n                # 3. Activate a bare git repo\r\n                _pf = _pp\r\n                _dirs = repo_path[len(_pp):].strip(os.sep).split(os.sep) or ['']\r\n                for _dir in _dirs:\r\n                    _pf = os.path.join(_pf,_dir)\r\n                    if not os.path.exists(_pf):\r\n                        try:\r\n                            os.makedirs(repo_path)\r\n                        except:\r\n                            return self.canned_handlers(environ, start_response, 'not_found')\r\n                        break\r\n                    elif not os.path.isdir(_pf) or self.git_folder_signature.issubset([i.lower() for i in os.listdir(_pf)]):\r\n                        return self.canned_handlers(environ, start_response, 'forbidden')\r\n                if subprocess.call('git init --quiet --bare \"%s\"' % repo_path, shell=True):\r\n                    return self.canned_handlers(environ, start_response, 'execution_failed')\r\n        #\r\n        #############################################################\r\n\r\n        dataObj['git_command'] = git_command\r\n        dataObj['repo_path'] = repo_path\r\n        return None", "response": "This function is used by the HTTPServer to check if the given dataObj is mutable and if so checks if the given dataObj has access to the given git command. Returns non - None if the request was not made."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints some information about the shell scope", "response": "def info_shell_scope(self):\n        \"\"\"prints some information about the shell scope\"\"\"\n        Console.ok(\"{:>20} = {:}\".format(\"ECHO\", self.echo))\n        Console.ok(\"{:>20} = {:}\".format(\"DEBUG\", self.debug))\n        Console.ok(\"{:>20} = {:}\".format(\"LOGLEVEL\", self.loglevel))\n        Console.ok(\"{:>20} = {:}\".format(\"SCOPE\", self.active_scope))\n        Console.ok(\"{:>20} = {:}\".format(\"SCOPES\", self.scopes))\n        Console.ok(\"{:>20} = {:}\".format(\"SCOPELESS\", self.scopeless))\n        Console.ok(\"{:>20} = {:}\".format(\"prompt\", self.prompt))\n        Console.ok(\"{:>20} = {:}\".format(\"scripts\", self.scripts))\n        Console.ok(\"{:>20} = {:}\".format(\"variables\", self.variables))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nactivate the shell scope", "response": "def activate_shell_scope(self):\n        \"\"\"activates the shell scope\"\"\"\n        self.variables = {}\n        self.prompt = 'cm> '\n        self.active_scope = \"\"\n        self.scopes = []\n        self.scopeless = ['load', 'info', 'var', 'use', 'quit', 'q', 'help']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_debug(self, args, arguments):\n        filename = path_expand(\"~/.cloudmesh/cmd3.yaml\")\n\n        config = ConfigDict(filename=filename)\n        if arguments['on']:\n            self.set_debug(True)\n        elif arguments['off']:\n            self.set_debug(False)", "response": "Set the debug level on or off."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_loglevel(self, args, arguments):\n        if arguments['debug']:\n            self.loglevel = \"DEBUG\"\n        elif arguments['error']:\n            self.loglevel = \"ERROR\"\n        elif arguments['warning']:\n            self.loglevel = \"WARNING\"\n        elif arguments['info']:\n            self.loglevel = \"INFO\"\n        elif arguments['critical']:\n            self.loglevel = \"CRITICAL\"\n        else:\n            Console.ok(\"Log level: {0}\".format(self.loglevel))\n            return\n        Console.ok (\"Log level: {0} is set\".format(self.loglevel))\n\n        filename = path_expand(\"~/.cloudmesh/cmd3.yaml\")\n        config = ConfigDict(filename=filename)\n        config[\"cmd3\"][\"properties\"][\"loglevel\"] = self.loglevel\n        config.write(filename=filename, output=\"yaml\", attribute_indent=\"    \")", "response": "Set the log level of the current application."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_verbose(self, args, arguments):\n        if args == '':\n            self.echo = not self.echo\n        else:\n            self.echo = arguments['True']", "response": "Usage:\n            verbose (True | False)\n            verbose\n\n        If it sets to True, a command will be printed before execution.\n        In the interactive mode, you may want to set it to False.\n        When you use scripts, we recommend to set it to True.\n\n        The default is set to False\n\n        If verbose is specified without parameter the flag is\n        toggled."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_var(self, arg, arguments):\n        if arguments['list'] or arg == '' or arg is None:\n            self._list_variables()\n            return\n\n        elif arguments['NAME=VALUE'] and \"=\" in arguments[\"NAME=VALUE\"]:\n            (variable, value) = arg.split('=', 1)\n            if value == \"time\" or value == \"now\":\n                value = datetime.datetime.now().strftime(\"%H:%M:%S\")\n            elif value == \"date\":\n                value = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n            self._add_variable(variable, value)\n            return\n        elif arguments['NAME=VALUE'] and \"=\" in arguments[\"NAME=VALUE\"]:\n            try:\n                v = arguments['NAME=VALUE']\n                Console.ok(str(self.variables[v]))\n            except:\n                Console.error('variable {:} not defined'.format(arguments['NAME=VALUE']))\n            \n        elif arg.startswith('delete'):\n            variable = arg.split(' ')[1]\n            self._delete_variable(variable)\n            return", "response": "This function is called by the command line to set the variable in the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate the stack of functions to call.", "response": "def _build_stack(self) -> List[Callable]:\n        \"\"\"\n        Generates the stack of functions to call. It looks at the ordered list\n        of all middlewares and only keeps those which have the method we're\n        trying to call.\n        \"\"\"\n\n        stack = []\n\n        for m in self.manager.middlewares:\n            try:\n                stack.append(getattr(m(self), self.name))\n            except AttributeError:\n                pass\n\n        return stack"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instance(cls) -> 'MiddlewareManager':\n\n        if cls._instance is None:\n            cls._instance = cls()\n            cls._instance.init()\n        return cls._instance", "response": "Returns a unique MiddlewareManager instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef health_check(cls):\n\n        try:\n            assert isinstance(settings.MIDDLEWARES, list)\n        except AssertionError:\n            yield HealthCheckFail(\n                '00005',\n                'The \"MIDDLEWARES\" configuration key should be assigned '\n                'to a list',\n            )\n            return\n\n        for m in settings.MIDDLEWARES:\n            try:\n                c = import_class(m)\n            except (TypeError, ValueError, AttributeError, ImportError):\n                yield HealthCheckFail(\n                    '00005',\n                    f'Cannot import middleware \"{m}\"',\n                )\n            else:\n                if not issubclass(c, BaseMiddleware):\n                    yield HealthCheckFail(\n                        '00005',\n                        f'Middleware \"{m}\" does not implement '\n                        f'\"BaseMiddleware\"',\n                    )", "response": "Checks that the configuration makes sense."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the function to call which will run all middlewares.", "response": "def get(self, name: Text, final: C) -> C:\n        \"\"\"\n        Get the function to call which will run all middlewares.\n\n        :param name: Name of the function to be called\n        :param final: Function to call at the bottom of the stack (that's the\n                      one provided by the implementer).\n        :return:\n        \"\"\"\n\n        # noinspection PyTypeChecker\n        return Caller(self, name, final)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a loci from the command line arguments.", "response": "def load_from_args(args):\n    \"\"\"\n    Return a Loci object giving the loci specified on the command line.\n\n    If no loci-related arguments are specified, return None. This makes it\n    possible to distinguish an empty set of loci, for example due to filters\n    removing all loci, from the case where the user didn't specify any\n    arguments.\n    \"\"\"\n    if not args.locus:\n        return None\n\n    loci_iterator = (Locus.parse(locus) for locus in args.locus)\n\n#   if args.neighbor_offsets:\n#       loci_iterator = expand_with_neighbors(\n#           loci_iterator, args.neighbor_offsets)\n\n    return Loci(loci_iterator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the home key presses.", "response": "def _do_home_key(self, event=None, select=False):\n        \"\"\" Performs home key action \"\"\"\n        # get nb char to first significative char\n        min_column = self.indenter_mode.min_column\n        text = api.TextHelper(self).current_line_text()[min_column:]\n        indent = len(text) - len(text.lstrip())\n        delta = (self.textCursor().positionInBlock() - indent - min_column)\n        cursor = self.textCursor()\n        move = QtGui.QTextCursor.MoveAnchor\n        if select:\n            move = QtGui.QTextCursor.KeepAnchor\n        if delta > 0:\n            cursor.movePosition(QtGui.QTextCursor.Left, move, delta)\n        else:\n            cursor.movePosition(QtGui.QTextCursor.StartOfBlock, move)\n            cursor.movePosition(QtGui.QTextCursor.Right, cursor.MoveAnchor, min_column)\n        self.setTextCursor(cursor)\n        if event:\n            event.accept()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef do_py(self, arg):\n        self.pystate['self'] = self\n        arg = arg.strip()\n        localvars = (self.locals_in_py and self.pystate) or {}\n        interp = InteractiveConsole(locals=localvars)\n        interp.runcode('import sys, os;sys.path.insert(0, os.getcwd())')\n        if arg:\n            interp.runcode(arg)\n        else:\n            def quit():\n                raise EmbeddedConsoleExit\n\n            def onecmd(arg):\n                return self.onecmd(arg + '\\n')\n\n            def run(arg):\n                try:\n                    f = open(arg)\n                    interp.runcode(f.read())\n                    f.close()\n                except IOError, e:\n                    self.perror(e)\n            self.pystate['quit'] = quit\n            self.pystate['exit'] = quit\n            self.pystate['cmd'] = onecmd\n            self.pystate['run'] = run\n            try:\n                cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n                keepstate = Statekeeper(sys, ('stdin', 'stdout'))\n                sys.stdout = self.stdout\n                sys.stdin = self.stdin\n                interp.interact(banner=\"Python %s on %s\\n%s\\n(%s)\\n%s\" %\n                                (sys.version, sys.platform, cprt, self.__class__.__name__, self.do_py.__doc__))\n            except EmbeddedConsoleExit:\n                pass\n            keepstate.restore()", "response": "A function that executes a python command."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_date(cls, timestamp):\n        if not timestamp:\n            raise DateTimeFormatterException('timestamp must a valid string {}'.format(timestamp))\n\n        return timestamp.strftime(cls.DATE_FORMAT)", "response": "Formats the date information provided by the\n        given a datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_datetime(cls, timestamp):\n        if not timestamp:\n            raise DateTimeFormatterException('timestamp must a valid string {}'.format(timestamp))\n\n        return timestamp.strftime(cls.DATETIME_FORMAT)", "response": "Formats the date and time information provided by the given timestamp object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to extract a datetime object from the given string expecting date information only. Raises a DateTimeFormatterException if the extraction fails.", "response": "def extract_date(cls, date_str):\n        \"\"\"\n        Tries to extract a `datetime` object from the given string, expecting\n        date information only.\n\n        Raises `DateTimeFormatterException` if the extraction fails.\n        \"\"\"\n        if not date_str:\n            raise DateTimeFormatterException('date_str must a valid string {}.'.format(date_str))\n\n        try:\n            return cls._extract_timestamp(date_str, cls.DATE_FORMAT)\n        except (TypeError, ValueError):\n            raise DateTimeFormatterException('Invalid date string {}.'.format(date_str))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_datetime(cls, datetime_str):\n        if not datetime_str:\n            raise DateTimeFormatterException('datetime_str must a valid string')\n\n        try:\n            return cls._extract_timestamp(datetime_str, cls.DATETIME_FORMAT)\n        except (TypeError, ValueError):\n            raise DateTimeFormatterException('Invalid datetime string {}.'.format(datetime_str))", "response": "Tries to extract a datetime object from the given string including\n            time information."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to extract a datetime object from the given string including only hours.", "response": "def extract_datetime_hour(cls, datetime_str):\n        \"\"\"\n        Tries to extract a `datetime` object from the given string, including only hours.\n\n        Raises `DateTimeFormatterException` if the extraction fails.\n        \"\"\"\n        if not datetime_str:\n            raise DateTimeFormatterException('datetime_str must a valid string')\n\n        try:\n            return cls._extract_timestamp(datetime_str, cls.DATETIME_HOUR_FORMAT)\n        except (TypeError, ValueError):\n            raise DateTimeFormatterException('Invalid datetime string {}.'.format(datetime_str))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntry to extract a datetime object from the given string.", "response": "def extract(cls, timestamp_str):\n        \"\"\"\n        Tries to extract a `datetime` object from the given string. First the\n        datetime format is tried, if it fails, the date format is used for\n        extraction.\n\n        Raises `DateTimeFormatterException` if the extraction fails.\n        \"\"\"\n        if not timestamp_str:\n            raise DateTimeFormatterException(\n                'timestamp_str must a valid string {}'.format(timestamp_str))\n\n        if isinstance(timestamp_str, (date, datetime)):\n            return timestamp_str\n\n        try:\n            return cls.extract_datetime(timestamp_str)\n        except DateTimeFormatterException:\n            pass\n\n        try:\n            return cls.extract_datetime_hour(timestamp_str)\n        except DateTimeFormatterException:\n            pass\n\n        try:\n            return cls.extract_date(timestamp_str)\n        except DateTimeFormatterException as e:\n            raise DateTimeFormatterException(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts a new workflow by given name for specified data.", "response": "def start(workflow_name, data=None, object_id=None, **kwargs):\n    \"\"\"Start a workflow by given name for specified data.\n\n    The name of the workflow to start is considered unique and it is\n    equal to the name of a file containing the workflow definition.\n\n    The data passed could be a list of Python standard data types such as\n    strings, dict, integers etc. to run through the workflow. Inside the\n    workflow tasks, this data is then available through ``obj.data``.\n\n    Or alternatively, pass the WorkflowObject to work on via\n    ``object_id`` parameter. NOTE: This will replace any value in ``data``.\n\n    This is also a Celery (http://celeryproject.org) task, so you can\n    access the ``start.delay`` function to enqueue the execution of the\n    workflow asynchronously.\n\n    :param workflow_name: the workflow name to run. Ex: \"my_workflow\".\n    :type workflow_name: str\n\n    :param data: the workflow name to run. Ex: \"my_workflow\" (optional if\n        ``object_id`` provided).\n    :type data: tuple\n\n    :param object_id: id of ``WorkflowObject`` to run (optional).\n    :type object_id: int\n\n    :return: UUID of the workflow engine that ran the workflow.\n    \"\"\"\n    from .proxies import workflow_object_class\n    from .worker_engine import run_worker\n\n    if data is None and object_id is None:\n        raise WorkflowsMissingData(\"No data or object_id passed to task.\u00df\")\n\n    if object_id is not None:\n        obj = workflow_object_class.get(object_id)\n        if not obj:\n            raise WorkflowsMissingObject(\n                \"Cannot find object: {0}\".format(object_id)\n            )\n        data = [obj]\n    else:\n        if not isinstance(data, (list, tuple)):\n            data = [data]\n\n    return text_type(run_worker(workflow_name, data, **kwargs).uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncontinue the workflow for given WorkflowObject id.", "response": "def resume(oid, restart_point=\"continue_next\", **kwargs):\n    \"\"\"Continue workflow for given WorkflowObject id (oid).\n\n    Depending on `start_point` it may start from previous, current or\n    next task.\n\n    Special custom keyword arguments can be given to the workflow engine\n    in order to pass certain variables to the tasks in the workflow execution,\n    such as a task-id from BibSched, the current user etc.\n\n    :param oid: id of WorkflowObject to run.\n    :type oid: str\n\n    :param start_point: where should the workflow start from? One of:\n        * restart_prev: will restart from the previous task\n        * continue_next: will continue to the next task\n        * restart_task: will restart the current task\n    :type start_point: str\n\n    :return: UUID of the workflow engine that ran the workflow.\n    \"\"\"\n    from .worker_engine import continue_worker\n    return text_type(continue_worker(oid, restart_point, **kwargs).uuid)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restart(uuid, **kwargs):\n    from .worker_engine import restart_worker\n    return text_type(restart_worker(uuid, **kwargs).uuid)", "response": "Restart the workflow from a given workflow engine UUID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimport all submodules and register them in the context namespace.", "response": "def import_submodules(context, root_module, path):\n    \"\"\"\n    Import all submodules and register them in the ``context`` namespace.\n\n    >>> import_submodules(locals(), __name__, __path__)\n    \"\"\"\n    for _, module_name, _ in pkgutil.walk_packages(path, root_module + '.'):\n        # this causes a Runtime error with model conflicts\n        # module = loader.find_module(module_name).load_module(module_name)\n        module = __import__(module_name, globals(), locals(), ['__name__'])\n        for k, v in vars(module).items():\n            if not k.startswith('_'):\n                context[k] = v\n        context[module_name] = module"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a cmd with the added plugins, :param name: TODO: :param plugins: list of plugins", "response": "def DynamicCmd(name, plugins):\n    \"\"\"\n    Returns a cmd with the added plugins,\n    \n    :param name: TODO:\n    :param plugins: list of plugins\n    \"\"\"\n    exec('class %s(cmd.Cmd):\\n    prompt=\"cm> \"' % name)\n    plugin_objects = []\n    for plugin in plugins:\n        classprefix = plugin['class']\n        plugin_list = plugin['plugins']\n        plugin_objects = plugin_objects + \\\n            load_plugins(classprefix, plugin_list)\n\n    exec_command = make_cmd_class(name, *plugin_objects)()\n    return (exec_command, plugin_objects)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of plugins from the specified directory", "response": "def get_plugins(directory):\n    \"\"\"\n    returns the list of plugins from the specified directory\n    :param directory: directory that contains the plugins. Files starting with _ will be ignored.\n    \"\"\"\n    # not just plugin_*.py\n    plugins = []\n    files = glob.glob(directory + \"/*.py\")\n    for p in files:\n        p = p.replace(directory + \"/\", \"\").replace(\".py\", \"\")\n        if not p.startswith('_'):\n            plugins.append(p)\n    # log.info(\"Loading Plugins from {0}\".format(dir))\n    # log.info(\"   {0}\".format(str(plugins)))\n    return plugins"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_plugins(classprefix, plugin_list):\n    # classprefix \"cmd3.plugins.\"\n    plugins = []\n    import_object = {}\n    # log.info(str(list))\n    for plugin in plugin_list:\n        if cygwin:\n            plugin = path_into_cygpath(plugin)\n        # print (\"PPPPP\", classprefix, plugin)\n\n        try:\n            import_object[plugin] = __import__(\n                classprefix + \".\" + plugin, globals(), locals(), [plugin], -1)\n            # print (\"TTT\", import_object[plugin], type(import_object[plugin]))\n            load_module = \"cls = import_object['{0}'].{1}\".format(plugin, plugin)\n            # print (\"LLL\", load_module)\n            exec(load_module)\n            plugins.append(cls)\n        except Exception, e:\n            # if echo:\n            Console.error(\"loading module {0} {1}\".format(str(plugin), str(classprefix)))\n            Console.error(70 * \"=\")\n            print(e)\n            Console.error(70 * \"=\")\n            print(traceback.format_exc())\n            Console.error(70 * \"-\")\n            print(sys.exc_info()[0])\n            Console.error(70 * \"-\")\n            \n    return plugins", "response": "loads the plugins specified in the list of plugins in the classprefix"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef command(func):\n    '''\n    A decorator to create a function with docopt arguments. It also generates a help function\n    \n    @command\n    def do_myfunc(self, args):\n        \"\"\" docopts text \"\"\"\n        pass\n        \n    will create\n    \n    def do_myfunc(self, args, arguments):\n        \"\"\" docopts text \"\"\"\n        ...\n        \n    def help_myfunc(self, args, arguments):\n        ... prints the docopt text ...\n    \n    :param func: the function for the decorator\n    '''\n    classname = inspect.getouterframes(inspect.currentframe())[1][3]\n    name = func.__name__\n    help_name = name.replace(\"do_\", \"help_\")\n    doc = textwrap.dedent(func.__doc__)\n\n    def new(instance, args):\n                # instance.new.__doc__ = doc\n        try:\n            argv = shlex.split(args)\n            arguments = docopt(doc, help=True, argv=argv)\n            func(instance, args, arguments)\n        except SystemExit:\n            if args not in ('-h', '--help'):\n                Console.error(\"Could not execute the command.\")\n            print(doc)\n    new.__doc__ = doc\n    return new", "response": "A command decorator to create a function with docopt arguments. It also generates a help function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new file if the file name does not exists", "response": "def create_file(filename):\n    \"\"\"\n    Creates a new file if the file name does not exists\n    :param filename: the name of the file\n    \"\"\"\n\n    expanded_filename = os.path.expanduser(os.path.expandvars(filename))\n    if not os.path.exists(expanded_filename):\n        open(expanded_filename, \"a\").close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the list of plugins from a directory.", "response": "def get_plugins_from_dir(dir_path, classbase):\n    \"\"\"dir_path/classbase/plugins\"\"\"\n\n    if dir_path == \"sys\":\n        dir_path = os.path.abspath(\n            os.path.join(os.path.dirname(__file__), 'plugins'))\n        dir_plugins = get_plugins(dir_path)\n        return {\"dir\": dir_path, \"plugins\": dir_plugins, \"class\": classbase}\n\n    if dir_path == \".\":\n        dir_path = os.path.expanduser(\n            os.path.expandvars(os.path.join(os.getcwd(), 'plugins')))\n        dir_plugins = get_plugins(dir_path)\n        return {\"dir\": dir_path, \"plugins\": dir_plugins, \"class\": classbase}\n    else:\n\n        dir_path = os.path.expanduser(os.path.expandvars(dir_path))\n        prefix = \"{0}/{1}\".format(dir_path, classbase)\n\n        user_path = \"{0}/plugins\".format(prefix)\n\n        create_dir(user_path)\n        create_file(\"{0}/__init__.py\".format(prefix))\n        create_file(\"{0}/plugins/__init__.py\".format(prefix))\n        sys.path.append(os.path.expanduser(dir_path))\n        dir_plugins = get_plugins(user_path)\n        # pprint({\"dir\": dir_path, \"plugins\": dir_plugins, \"class\": classbase})\n        return {\"dir\": dir_path, \"plugins\": dir_plugins, \"class\": classbase}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommand line interface for the CM.", "response": "def main():\n    \"\"\"cm.\n\n    Usage:\n      cm [-q] help\n      cm [-v] [-b] [--file=SCRIPT] [-i] [COMMAND ...]\n\n    Arguments:\n      COMMAND                  A command to be executed\n\n    Options:\n      --file=SCRIPT  -f  SCRIPT  Executes the script\n      -i                 After start keep the shell interactive,\n                         otherwise quit [default: False]\n      -b                 surpress the printing of the banner [default: False]\n    \"\"\"\n\n    echo = False\n    \n    try:\n        arguments = docopt(main.__doc__, help=True)\n        # fixing the help parameter parsing\n        if arguments['help']:\n            arguments['COMMAND'] = ['help']\n            arguments['help'] = 'False'\n\n        script_file = arguments['--file']\n        interactive = arguments['-i']\n        echo = arguments['-v']\n        if echo:\n            pprint(arguments)\n\n    except:\n        script_file = None\n        interactive = False\n\n        arguments = {'-b': True,\n                     'COMMAND': [' '.join(sys.argv[1:])]}\n\n    plugins = []\n\n    plugins.append(dict(get_plugins_from_dir(\"sys\", \"cmd3\")))\n    # plugins.append(dict(get_plugins_from_dir(\"~/.cloudmesh\", \"cmd3local\")))\n\n\n    # if not os.path.exists(path_expand( \"~/.cloudmesh/cmd3.yaml\")):\n    #     from cmd3.plugins.shell_core import create_cmd3_yaml_file\n    #     create_cmd3_yaml_file()\n\n\n    create_cmd3_yaml_file(force=False, verbose=False)\n    filename = path_expand(\"~/.cloudmesh/cmd3.yaml\")\n    try:\n        module_config = ConfigDict(filename=filename)\n        modules = module_config[\"cmd3\"][\"modules\"]\n        properties = module_config[\"cmd3\"][\"properties\"]\n    except:\n        modules = ['cloudmesh_cmd3.plugins']\n    for module_name in modules:\n        #print (\"INSTALL\", module_name)\n        try:\n            plugins.append(dict(get_plugins_from_module(module_name)))\n        except:\n            # print \"WARNING: could not find\", module_name\n            pass\n\n    # sys.exit()\n    # plugins.append(dict(get_plugins_from_dir(\"~/.cloudmesh\", \"cmd3local\")))\n    \n    # plugins.append(dict(get_plugins_from_dir (\".\", \"dot\")))\n\n    for plugin in plugins:\n        sys.path.append(os.path.expanduser(plugin['dir']))\n    sys.path.append(\"../..\")\n    sys.path.append(\".\")\n    sys.path.append(\"..\")\n\n    for plugin in plugins:\n        plugin['class'] += \".plugins\"\n\n    # pprint(plugins)\n    # pprint(sys.path)\n\n    # sys.exit()\n    name = \"CmCli\"\n\n    #\n    # not yet quite what i want, but falling back to a flatt array\n    #\n\n    (cmd, plugin_objects) = DynamicCmd(name, plugins)\n\n\n    cmd.set_verbose(echo)\n    cmd.activate()\n    cmd.set_verbose(echo)\n\n    cmd.set_debug(properties[\"debug\"])\n\n    if arguments['-b']:\n        cmd.set_banner(\"\")\n    if script_file is not None:\n        cmd.do_exec(script_file)\n\n    if len(arguments['COMMAND']) > 0:\n        try:            \n            user_cmd = \" \".join(arguments['COMMAND'])\n            if echo:\n                print(\">\", user_cmd)\n            cmd.onecmd(user_cmd)\n        except Exception, e:\n            Console.error(\"\")\n            Console.error(\"ERROR: executing command '{0}'\".format(user_cmd))\n            Console.error(\"\")\n            print (70 * \"=\")\n            print(e)\n            print (70 * \"=\")\n            print(traceback.format_exc())\n\n        if interactive:\n            cmd.cmdloop()\n            \n    elif not script_file or interactive:\n        cmd.cmdloop()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a file to the version", "response": "def addFile(self, path, msg=\"\"):\n        \"\"\"Adds a file to the version\"\"\"\n        item = Item.from_path(repo=self.repo, path=path)\n        self.addItem(item)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addItem(self, item):\n        try:\n            self.tree.addItem(item)\n        except AttributeError, e:\n            raise VersionError('Saved versions are immutable')", "response": "Adds an item to the tree if the tree is mutable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an item from the tree.", "response": "def removeItem(self, item):\n        \"\"\"Removes an item if the tree is mutable\"\"\"\n        try:\n            self.tree.removeItem(item)\n        except AttributeError, e:\n            raise VersionError('Saved versions are immutable')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iteritems(self):\n        if self.type in ['blob']:\n            raise StopIteration\n\n        for path, mode, sha in self.tree.iteritems():\n            item = Item(self, sha, path, mode)\n            yield item\n            for i in item.iteritems():\n                yield i", "response": "Generator that yields Items"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn set of items.", "response": "def items(self, path=None):\n        \"\"\"\n        Returns set of items.\n\n        :param path: Regex filter on item path.\n\n        :return: List of Item class objects.\n        \"\"\"\n        items = list(self.iteritems())\n        if path is not None:\n            path += '$'\n            regex = re.compile(path)\n            items = [i for i in items if regex.match(i.path)]\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_blob(self):\n        if not self.__blob:\n            self.__blob = self.repo.get_object(self.id)\n        return self.__blob", "response": "read blob on access only because get_object is slow"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_path(self, repo, path, name=None):\n        if name is None:\n            name = os.path.basename(path)\n        #FIXME: hack, there has to be a better way\n        return Item.from_string(repo=repo, name=name, string=open(path).read())", "response": "Create a new Item instance from a file path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new Item from a string.", "response": "def from_string(self, repo, name, string):\n        \"\"\"\n        Create a new Item from a data stream.\n\n        :param repo: Repo object.\n        :param name: Name of item.\n        :param data: Data stream.\n\n        :return: New Item class instance.\n        \"\"\"\n        try:\n            log.debug('Creating new item: %s' % name)\n            blob = Blob.from_string(string)\n            item = Item(parent=repo, sha=blob.sha, path=name)\n            item.blob = blob\n            return item\n        except AssertionError, e:\n            raise ItemError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save(self, msg=None):\n        if msg is None:\n            msg = 'Saving %s' % self.name\n        log.debug(msg)\n        self.repo.addItem(self, msg)", "response": "Modify item data and commit to repo. \n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking out the data of the item to the given path.", "response": "def checkout(self, path):\n        \"\"\"\n        Check out file data to path.\n\n        :param path: Filesystem path to check out item to.\n\n        :return: True if successful.\n        \"\"\"\n        if os.path.isdir(path):\n            path = os.path.join(path, self.name)\n        try:\n            log.debug('Checking out %s to %s' %(self.path, path))\n            f = open(path, 'w')\n            f.write(self.data())\n            f.close()\n            return True\n        except Exception, e:\n            raise ItemError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef versions(self, rev=None, index=None):\n        raise NotImplementedError\n        _revisions = [line.split()[0] for line in self.log.split('\\n') if line]\n        _versions = [Version(self.obj.repo.commit(r)) for r in _revisions if rev is None or r.startswith(rev)]\n        if index is not None and len(_versions) > index:\n            _versions = _versions[index]\n        return _versions", "response": "Return a list of Versions for this Item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, message):\n        self.commit.message = message\n        self.commit.tree = self.tree\n        #TODO: store new blobs only\n        for item in self.tree.items():\n            self.repo.object_store.add_object(item.blob)\n        self.repo.object_store.add_object(self.tree)\n\n        # set HEAD to new commit\n        self.repo.object_store.add_object(self.commit)\n        self.repo.refs['refs/heads/master'] = self.commit.id", "response": "Save the current tree and tree to the object store."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new version of a repo. Local object.", "response": "def new(self, repo):\n        \"\"\"\n        Create a new version of a repo.Local object.\n\n        :param repo: Instance of repo.Local.\n\n        :return: New Version instance.\n        \"\"\"\n        #TODO: subclass Commit, pass parent as init param\n        try:\n            # create new commit instance and set metadata\n            commit = Commit()\n            author = os.environ.get('USER')\n            commit.author = commit.committer = author\n            commit.commit_time = commit.author_time = int(time())\n            tz = parse_timezone('-0200')[0]\n            commit.commit_timezone = commit.author_timezone = tz\n            commit.encoding = \"UTF-8\"\n            commit.message = ''\n\n            # set previous version as parent to this one\n            parent = repo.versions(-1)\n            if parent:\n                commit.parents = [parent.id]\n\n            # create new tree, add entries from previous version\n            tree = Tree()\n            curr = repo.versions(-1)\n            if curr:\n                for item in curr.items():\n                    tree.addItem(item)\n            commit.tree = tree.id\n\n            # create new version, and add tree\n            version = Version(repo=repo, commit=commit, tree=tree)\n            return version\n\n        except Exception, e:\n            traceback.print_exc()\n            return VersionError(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprompting user for confirmation.", "response": "def confirm(prompt=None, resp=False):\n    \"\"\"\n    Prompts user for confirmation.\n\n    :param prompt: String to display to user.\n    :param resp: Default response value.\n\n    :return: Boolean response from user, or default value.\n    \"\"\"\n    if prompt is None:\n        prompt = 'Confirm'\n\n    if resp:\n        prompt = '%s [%s]|%s: ' % (prompt, 'y', 'n')\n    else:\n        prompt = '%s [%s]|%s: ' % (prompt, 'n', 'y')\n\n    while True:\n        ans = raw_input(prompt)\n        if not ans:\n            return resp\n        if ans not in ['y', 'Y', 'n', 'N']:\n            print 'please enter y or n.'\n            continue\n        if ans == 'y' or ans == 'Y':\n            return True\n        if ans == 'n' or ans == 'N':\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prompt(name, default):\n    value = raw_input('%s [%s]: ' %(name, default))\n    if not value:\n        value = default\n    return value", "response": "Prompts user for raw input."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(url):\n    from grit import Repo\n    return Repo.new(url=url, bare=True)", "response": "Creates a new Repo class instance at url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkout(url, version=None):\n    from grit import Repo\n    r = Repo(url)\n\n    def _write(item):\n        log.debug('writing: %s' % item.name)\n        if item.type != 'blob':\n            return\n        if r.type in ['repo', 'proxy', 'local']:\n            path = os.path.join(r.name, item.path)\n            pdir = os.path.dirname(path)\n            if not os.path.isdir(pdir):\n                os.makedirs(pdir)\n        else:\n            path = item.name\n\n        f = open(path, 'w')\n        f.write(item.data())\n        f.close()\n\n    if r.type == 'blob':\n        _write(r)\n    else:\n        items = r.items()\n        count = 1\n        total = len(items)\n        while count <= total:\n            print '[%s/%s] %0.2f%%' %(count, total, (float(count) / total) * 100), '*'*count, '\\r',\n            _write(items[count-1])\n            count += 1\n            sys.stdout.flush()\n        print", "response": "Checks out latest version of item or repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks in files to a repository.", "response": "def checkin(url, files, message=None):\n    \"\"\"\n    Check in files to a repository.\n\n    :param url: URL of repo to check files into.\n    :param message: Optional commit message.\n    \"\"\"\n    from grit import Repo, Item\n    r = Repo(url)\n\n    if not files:\n        raise GritError('No files')\n\n    def _write(path):\n        item = Item.from_path(repo=r, path=path)\n        if r.isLocal():\n            v.addItem(item=item)\n        else:\n            r.upload(filename=os.path.basename(path), filedata=open(path, 'r').read())\n\n    if r.isLocal():\n        v = r.addVersion()\n    count = 1\n    total = len(files) \n    while count <= total:\n        print '[%s/%s] %0.2f%%' %(count, total, (float(count) / total) * 100), '*'*count, '\\r',\n        _write(os.path.abspath(files[count-1]))\n        count += 1\n        sys.stdout.flush()\n    if message is None:\n        message = 'Publishing %s' % ', '.join(files)\n    if r.isLocal():\n        v.save(message=message)\n    print"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve an object from Transifex by making a GET request to Transifex.", "response": "def get(cls, **kwargs):\n        \"\"\"Retrieve an object by making a GET request to Transifex.\n\n        Each value in `kwargs` that corresponds to a field\n        defined in `self.url_fields` will be used in the URL path\n        of the request, so that a particular entry of this model\n        is identified and retrieved.\n\n        Raises:\n            AttributeError: if not all values for parameters in `url_fields`\n                are passed as kwargs\n            txlib.http.exceptions.NotFoundError: if the object with these\n                attributes is not found on the remote server\n            txlib.http.exceptions.ServerError subclass: depending on\n                the particular server response\n\n        Example:\n        # Note: also catch exceptions\n        >>> obj = MyModel.get(attr1=value1, attr2=value2)\n        \"\"\"\n        fields = {}\n        for field in cls.url_fields:\n            value = kwargs.pop(field, None)\n            if value is None:\n                cls._handle_wrong_field(field, ATTR_TYPE_URL)\n            fields[field] = value\n\n        # Create an instance of the model class and make the GET request\n        model = cls(**fields)\n        model._populate(**kwargs)\n        return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the instance to the remote Transifex server.", "response": "def save(self, **fields):\n        \"\"\"Save the instance to the remote Transifex server.\n\n        If it was pre-populated, it updates the instance on the server,\n        otherwise it creates a new object.\n\n        Any values given in `fields` will be attempted to be saved\n        on the object. The same goes for any other values already set\n        to the object by `model_instance.attr = value`.\n\n        Raises:\n            AttributeError: if a given field is not included in\n                `self.writable_fields`,\n        \"\"\"\n        for field in fields:\n            if field in self.writable_fields:\n                setattr(self, field, fields[field])\n            else:\n                self._handle_wrong_field(field, ATTR_TYPE_WRITE)\n\n        if self._populated_fields:\n            self._update(**self._modified_fields)\n        else:\n            self._create(**self._modified_fields)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get(self, **kwargs):\n        path = self._construct_path_to_item()\n        return self._http.get(path)", "response": "Get the resource from a remote Transifex server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a resource in the remote Transifex server.", "response": "def _create(self, **kwargs):\n        \"\"\"Create a resource in the remote Transifex server.\"\"\"\n        path = self._construct_path_to_collection()\n\n        # Use the fields for which we have values\n        for field in self.writable_fields:\n            try:\n                value = getattr(self, field)\n                kwargs[field] = value\n            except AttributeError:\n                pass\n        return self._http.post(path, json.dumps(kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating a resource in a remote Transifex server.", "response": "def _update(self, **kwargs):\n        \"\"\"Update a resource in a remote Transifex server.\"\"\"\n        path = self._construct_path_to_item()\n        if not kwargs:\n            return\n        return self._http.put(path, json.dumps(kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a resource from a remote Transifex server.", "response": "def _delete(self, **kwargs):\n        \"\"\"Delete a resource from a remote Transifex server.\"\"\"\n        path = self._construct_path_to_item()\n        return self._http.delete(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a dictionary of parameters used in URLs for this model.", "response": "def get_url_parameters(self):\n        \"\"\"Create a dictionary of parameters used in URLs for this model.\"\"\"\n        url_fields = {}\n        for field in self.url_fields:\n            url_fields[field] = getattr(self, field)\n        return url_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nraises an exception whenever an invalid attribute with the given name was attempted to be set to or retrieved from the base class.", "response": "def _handle_wrong_field(cls, field_name, field_type):\n        \"\"\"Raise an exception whenever an invalid attribute with\n        the given name was attempted to be set to or retrieved from\n        this model class.\n\n        Assumes that the given field is invalid, without making any checks.\n\n        Also adds an entry to the logs.\n        \"\"\"\n        if field_type == ATTR_TYPE_READ:\n            field_type = 'readable'\n        elif field_type == ATTR_TYPE_WRITE:\n            field_type = 'writable'\n        elif field_type == ATTR_TYPE_URL:\n            field_type = 'URL'\n        else:\n            raise AttributeError('Invalid attribute type: {}'.format(\n                field_type\n            ))\n\n        msg = '{} has no {} attribute \"{}\"'.format(\n            cls.__name__,\n            field_type,\n            field_name\n        )\n        _logger.error(msg)\n        raise AttributeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_http_rules(rules, content_type='text/plain'):\n    for kw in deepcopy(rules):\n\n        kw['url'] = re.compile(kw['url'])\n\n        # ensure headers dict for at least have a default content type\n        if 'Content-Type' not in kw.get('headers', {}):\n            kw['headers'] = dict(kw.get('headers', {}), **{\n                'Content-Type': content_type,\n            })\n\n        method = kw.pop('method')\n        url = kw.pop('url')\n\n        http_mock.register_uri(method, url, **kw)", "response": "Adds rules to global http mock."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the spec string into name args kwargs and return the value.", "response": "def _parseSpec(values):\n\t\"\"\"\n\tSplit the argument string. Example:\n\t\n\t--arg name:value0,value1,key2=value2,key3=value3\n\t\n\tgets split into\n\t\n\tname, args, kwargs = (\n\t    \"name\",\n\t    (value0, value1),\n\t    {\"key2\":value2, \"key3\":value3},\n\t)\n\t\"\"\"\n\tsplit  = values.split(\":\", 1)\n\tname   = split[0].strip()\n\trest   = split[1] if len(split) == 2 else \"\"\n\targs   = []\n\tkwargs = {}\n\t\n\tdef carveRest(s, sep):\n\t\tquotepairs = {\"'\": \"'\", \"\\\"\": \"\\\"\", \"{\":\"}\", \"[\":\"]\", \"(\":\")\"}\n\t\tval   = \"\"\n\t\tquote = \"\"\n\t\tprevC = \"\"\n\t\tfor i, c in enumerate(s):\n\t\t\tif quote:\n\t\t\t\tif   c == quote[-1]  and prevC != \"\\\\\":\n\t\t\t\t\tval    += c\n\t\t\t\t\tprevC   = \"\"\n\t\t\t\t\tquote   = quote[:-1]\n\t\t\t\telif c in quotepairs and prevC != \"\\\\\":\n\t\t\t\t\tval    += c\n\t\t\t\t\tprevC   = \"\"\n\t\t\t\t\tquote  += quotepairs[c]\n\t\t\t\telif prevC == \"\\\\\":\n\t\t\t\t\tval     = val[:-1]+c\n\t\t\t\t\tprevC   = \"\"\n\t\t\t\telse:\n\t\t\t\t\tval    += c\n\t\t\t\t\tprevC   = c\n\t\t\telse:\n\t\t\t\tif   c == sep:\n\t\t\t\t\tbreak\n\t\t\t\telif c in quotepairs and prevC != \"\\\\\":\n\t\t\t\t\tval    += c\n\t\t\t\t\tprevC   = \"\"\n\t\t\t\t\tquote  += quotepairs[c]\n\t\t\t\telif prevC == \"\\\\\":\n\t\t\t\t\tval     = val[:-1]+c\n\t\t\t\t\tprevC   = \"\"\n\t\t\t\telse:\n\t\t\t\t\tval    += c\n\t\t\t\t\tprevC   = c\n\t\t\t\n\t\treturn val, s[i+1:]\n\t\n\twhile rest:\n\t\tpositionalVal, positionalRest = carveRest(rest, \",\")\n\t\tkeywordKey,    keywordRest    = carveRest(rest, \"=\")\n\t\t\n\t\t#\n\t\t# If the distance to the first \"=\" (or end-of-string) is STRICTLY\n\t\t# shorter than the distance to the first \",\", we have found a\n\t\t# keyword argument.\n\t\t#\n\t\t\n\t\tif len(keywordKey)<len(positionalVal):\n\t\t\tkey       = re.sub(\"\\\\s+\", \"\", keywordKey)\n\t\t\tval, rest = carveRest(keywordRest, \",\")\n\t\t\ttry:    kwargs[key] = ast.literal_eval(val)\n\t\t\texcept: kwargs[key] = val\n\t\telse:\n\t\t\tif len(kwargs) > 0:\n\t\t\t\traise ValueError(\"Positional argument \"+repr(r)+\" found after first keyword argument!\")\n\t\t\tval, rest = positionalVal, positionalRest\n\t\t\ttry:    args += [ast.literal_eval(val)]\n\t\t\texcept: args += [val]\n\t\n\treturn name, args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_task_history(last_task):\n    if hasattr(last_task, 'branch') and last_task.branch:\n        return\n    elif hasattr(last_task, 'hide') and last_task.hide:\n        return\n    else:\n        return get_func_info(last_task)", "response": "Append last task to task history."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_func_info(func):\n    name = func.__name__\n    doc = func.__doc__ or \"\"\n    try:\n        nicename = func.description\n    except AttributeError:\n        if doc:\n            nicename = doc.split('\\n')[0]\n            if len(nicename) > 80:\n                nicename = name\n        else:\n            nicename = name\n    parameters = []\n    try:\n        closure = func.func_closure\n    except AttributeError:\n        closure = func.__closure__\n    try:\n        varnames = func.func_code.co_freevars\n    except AttributeError:\n        varnames = func.__code__.co_freevars\n\n    if closure:\n        for index, arg in enumerate(closure):\n            if not callable(arg.cell_contents):\n                parameters.append((varnames[index],\n                                   text_type(arg.cell_contents)))\n    return ({\n        \"nicename\": nicename,\n        \"doc\": doc,\n        \"parameters\": parameters,\n        \"name\": name,\n        \"time\": str(datetime.datetime.now()),\n        \"hostname\": socket.gethostname(),\n    })", "response": "Retrieve a function s information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn function info go through lists recursively.", "response": "def get_workflow_info(func_list):\n    \"\"\"Return function info, go through lists recursively.\"\"\"\n    funcs = []\n    for item in func_list:\n        if item is None:\n            continue\n        if isinstance(item, list):\n            funcs.append(get_workflow_info(item))\n        else:\n            funcs.append(get_func_info(item))\n    return funcs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_context_into_mutable(context):\n    def make_mutable(val):\n        if isinstance(val, Mapping):\n            return dict(val)\n        else:\n            return val\n\n    if not isinstance(context, (str, Mapping)):\n        try:\n            return [make_mutable(val) for val in context]\n        except TypeError:\n            pass\n    return make_mutable(context)", "response": "Copy a properly formatted context into a mutable data structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a properly formatted context immutable data structure.", "response": "def _make_context_immutable(context):\n    \"\"\"Best effort attempt at turning a properly formatted context\n    (either a string, dict, or array of strings and dicts) into an\n    immutable data structure.\n\n    If we get an array, make it immutable by creating a tuple; if we get\n    a dict, copy it into a MappingProxyType. Otherwise, return as-is.\n    \"\"\"\n    def make_immutable(val):\n        if isinstance(val, Mapping):\n            return MappingProxyType(val)\n        else:\n            return val\n\n    if not isinstance(context, (str, Mapping)):\n        try:\n            return tuple([make_immutable(val) for val in context])\n        except TypeError:\n            pass\n    return make_immutable(context)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve a value from the resolver_dict based on the data_format.", "response": "def _data_format_resolver(data_format, resolver_dict):\n    \"\"\"Resolve a value from :attr:`resolver_dict` based on the\n    :attr:`data_format`.\n\n    Args:\n        data_format (:class:`~.DataFormat` or str): The data format;\n            must be a member of :class:`~.DataFormat` or a string\n            equivalent.\n        resolver_dict (dict): the resolving dict. Can hold any value\n            for any of the valid :attr:`data_format` strings\n\n    Returns:\n        The value of the key in :attr:`resolver_dict` that matches\n        :attr:`data_format`\n    \"\"\"\n    try:\n        data_format = DataFormat(data_format)\n    except ValueError:\n        supported_formats = ', '.join(\n            [\"'{}'\".format(f.value) for f in DataFormat])\n        raise ValueError((\"'data_format' must be one of {formats}. Given \"\n                          \"'{value}'.\").format(formats=supported_formats,\n                                               value=data_format))\n    return (resolver_dict.get(data_format) or\n            resolver_dict.get(data_format.value))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_ld_data(data, data_format=None, **kwargs):\n    if not data_format:\n        data_format = _get_format_from_data(data)\n\n    extract_ld_data_fn = _data_format_resolver(data_format, {\n        'jsonld': _extract_ld_data_from_jsonld,\n        'json': _extract_ld_data_from_json,\n        'ipld': _extract_ld_data_from_ipld,\n    })\n    return extract_ld_data_fn(data, **kwargs)", "response": "Extracts the given data into a\n    with the resulting data\n    stripped of any Missing Linked Data specifics."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_types(schemas_and_tables):\n    '''normalize a list of desired annotation types\n    if passed None returns all types, otherwise checks that types exist\n    Parameters\n    ----------\n    types: list[str] or None\n\n    Returns\n    -------\n    list[str]\n        list of types\n\n    Raises\n    ------\n    UnknownAnnotationTypeException\n        If types contains an invalid type\n    '''\n\n    all_types = get_types()\n    if not (all(sn in all_types for sn, tn in schemas_and_tables)):\n        bad_types = [sn for sn,\n                     tn in schemas_and_tables if sn not in all_types]\n        msg = '{} are invalid types'.format(bad_types)\n        raise UnknownAnnotationTypeException(msg)", "response": "normalize a list of desired annotation types\n    if passed None returns all types otherwise checks that types exist\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_dataset_models(dataset, schemas_and_tables, metadata_dict = None, version: int = 1, include_contacts=False):\n    if metadata_dict is None:\n        metadata_dict={}\n    validate_types(schemas_and_tables)\n    dataset_dict = {}\n    cell_segment_model = make_cell_segment_model(dataset, version=version)\n    dataset_dict[root_model_name.lower()] = cell_segment_model\n    for schema_name, table_name in schemas_and_tables:\n        model_key = table_name\n        metadata = metadata_dict.get(table_name, None)\n        dataset_dict[model_key] = make_annotation_model(dataset,\n                                                        schema_name,\n                                                        table_name,\n                                                        table_metadata=metadata,\n                                                        version=version)\n    if include_contacts:\n        contact_model = make_annotation_model_from_schema(dataset,\n                                                          'contact',\n                                                          Contact,\n                                                          version=version)\n        dataset_dict['contact'] = contact_model\n    return dataset_dict", "response": "make all the models needed for a dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the key name of this object", "response": "def _key_name(self):  # type: () -> str\n        \"\"\"Return the key referring to this object\n\n        The default value is the lower case version of the class name\n\n        :rtype: str\n        \"\"\"\n        if self._key is not None:\n            return self._key\n\n        return self.__class__.__name__.lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _path(self):  # type: () -> str\n        if self._parent:\n            return '{}.{}'.format(self._parent._path(), self._key_name())\n\n        return self._key_name()", "response": "Return the dotted path representation of this object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_error(self, *args, **kwargs):  # type: () -> None\n        if kwargs.get('node', None):\n            # if node specified and not none\n            error = ConfigError.create_from_yaml_node(\n                *args,\n                **kwargs\n            )\n        elif self._value_node:\n            # default to using the node if we have one\n            error = ConfigError.create_from_yaml_node(\n                node=self._value_node,\n                *args,\n                **kwargs\n            )\n        else:\n            # no nodes or error_obj to attach\n            error = ConfigError(*args, **kwargs)\n\n        self._errors.append(error)", "response": "Convenience function to add an error to this object with line numbers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_descendants_errors(self):  # type: () -> List(ConfigError)\n        descendants_errors = []\n        if hasattr(self, '_children'):\n            if isinstance(self._children, (list, tuple)):\n                for c in self._children:\n                    descendants_errors += c._get_all_errors()\n            elif isinstance(self._children, dict):\n                for c in self._children.values():\n                    descendants_errors += c._get_all_errors()\n\n        return descendants_errors", "response": "Recursively get errors from descendants\n\nxid"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns validation on the object", "response": "def _validate(self):  # type: () -> None\n        \"\"\"Run validation, save errors to object in self._errors\"\"\"\n        # class can specify it's empty obj -- list would have empty of []\n        self._errors = []\n\n        self._validate_type()\n\n        if self.is_valid():\n            self._validate_value()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_type(self):  # type: () -> None\n        if not isinstance(self._value, self._type):\n            title = '{} has an invalid type'.format(self._key_name())\n            description = '{} must be a {}'.format(self._key_name(), self._type.__name__)\n\n            self._add_error(title=title, description=description)", "response": "Validation to ensure value is the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if we have at least one snapshot.", "response": "def haveSnapshots(self):\n\t\t\"\"\"Check if we have at least one snapshot.\"\"\"\n\t\treturn os.path.islink(self.latestLink) and os.path.isdir(self.latestLink)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart a fresh experiment from scratch.", "response": "def fromScratch(self):\n\t\t\"\"\"Start a fresh experiment, from scratch.\n\t\t\n\t\tReturns `self`.\"\"\"\n\t\t\n\t\tassert(not os.path.lexists(self.latestLink) or\n\t\t           os.path.islink (self.latestLink))\n\t\tself.rmR(self.latestLink)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef snapshot             (self):\n\t\tnextSnapshotNum  = self.nextSnapshotNum\n\t\tnextSnapshotPath = self.getFullPathToSnapshot(nextSnapshotNum)\n\t\t\n\t\tif os.path.lexists(nextSnapshotPath):\n\t\t\tself.rmR(nextSnapshotPath)\n\t\tself.mkdirp(os.path.join(nextSnapshotPath, \".experiment\"))\n\t\treturn self.dump(nextSnapshotPath).__markLatest(nextSnapshotNum)", "response": "Take a snapshot of the experiment.\n\tReturns self."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrolls back the experiment to the given snapshot number.", "response": "def rollback             (self, n=None):\n\t\t\"\"\"Roll back the experiment to the given snapshot number.\n\t\t\n\t\tReturns `self`.\"\"\"\n\t\t\n\t\tif n is None:\n\t\t\tif self.haveSnapshots: return self.fromSnapshot(self.latestLink)\n\t\t\telse:                  return self.fromScratch()\n\t\telif isinstance(n, int):\n\t\t\tloadSnapshotPath = self.getFullPathToSnapshot(n)\n\t\t\tassert(os.path.isdir(loadSnapshotPath))\n\t\t\treturn self.__markLatest(n).fromSnapshot(loadSnapshotPath)\n\t\telse:\n\t\t\traise ValueError(\"n must be int, or None!\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npurging all the directories in the archive according to some strategy.", "response": "def purge                (self,\n\t                          strategy           = \"klogn\",\n\t                          keep               = None,\n\t                          deleteNonSnapshots = False,\n\t                          **kwargs):\n\t\t\"\"\"Purge snapshot directory of snapshots according to some strategy,\n\t\tpreserving however a given \"keep\" list or set of snapshot numbers.\n\t\t\n\t\tAvailable strategies are:\n\t\t    \"lastk\":  Keep last k snapshots (Default: k=10)\n\t\t    \"klogn\":  Keep every snapshot in the last k, 2k snapshots in\n\t\t              the last k**2, 3k snapshots in the last k**3, ...\n\t\t              (Default: k=4. k must be > 1).\n\t\t\n\t\tReturns `self`.\"\"\"\n\t\t\n\t\tassert(isinstance(keep, (list, set))  or  keep is None)\n\t\tkeep = set(keep or [])\n\t\tif self.haveSnapshots:\n\t\t\tif   strategy == \"lastk\":\n\t\t\t\tkeep.update(self.strategyLastK(self.latestSnapshotNum, **kwargs))\n\t\t\telif strategy == \"klogn\":\n\t\t\t\tkeep.update(self.strategyKLogN(self.latestSnapshotNum, **kwargs))\n\t\t\telse:\n\t\t\t\traise ValueError(\"Unknown purge strategy \"+str(None)+\"!\")\n\t\t\tkeep.update([\"latest\", str(self.latestSnapshotNum)])\n\t\tkeep = set(map(str, keep))\n\t\t\n\t\tsnaps, nonSnaps    = self.listSnapshotDir(self.snapDir)\n\t\tdirEntriesToDelete = set()\n\t\tdirEntriesToDelete.update(snaps)\n\t\tdirEntriesToDelete.update(nonSnaps if deleteNonSnapshots else set())\n\t\tdirEntriesToDelete.difference_update(keep)\n\t\tfor dirEntry in dirEntriesToDelete:\n\t\t\tself.rmR(os.path.join(self.snapDir, dirEntry))\n\t\t\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getFullPathToSnapshot(self, n):\n\t\treturn os.path.join(self.snapDir, str(n))", "response": "Get the full path to snapshot n."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strategyLastK(kls, n, k=10):\n\t\treturn set(map(str, filter(lambda x:x>=0, range(n, n-k, -1))))", "response": "Return the directory names to preserve under the LastK purge strategy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strategyKLogN(kls, n, k=4):\n\t\tassert(k>1)\n\t\ts = set([n])\n\t\ti = 0\n\t\t\n\t\twhile k**i <= n:\n\t\t\ts.update(range(n, n-k*k**i, -k**i))\n\t\t\ti += 1\n\t\t\tn -= n % k**i\n\t\t\n\t\treturn set(map(str, filter(lambda x:x>=0, s)))", "response": "Return the directory names to preserve under the KLogN purge strategy."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mkdirp(kls, path, mode=0o755):\n\t\tos.makedirs(path, mode=mode, exist_ok=True)", "response": "Create a folder and all parent\n\tdirectories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the set of snapshot directories and non - snapshot directories under the given path.", "response": "def listSnapshotDir(kls, path):\n\t\t\"\"\"Return the set of snapshot directories and non-snapshot directories\n\t\tunder the given path.\"\"\"\n\t\tsnapshotSet    = set()\n\t\tnonsnapshotSet = set()\n\t\ttry:\n\t\t\tentryList = os.listdir(path)\n\t\t\tfor e in entryList:\n\t\t\t\tif kls.isFilenameInteger(e): snapshotSet   .add(e)\n\t\t\t\telse:                        nonsnapshotSet.add(e)\n\t\texcept FileNotFoundError: pass\n\t\tfinally:\n\t\t\treturn snapshotSet, nonsnapshotSet"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getFlag(self, flagName):\n        assert (len(flagName) > 0)\n\n        return Flag(os.path.join(self._variablesDirPath, flagName))", "response": "Returns a flag with the given name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncompensating temperature. Formula from datasheet Bosch BME280 Environmental sensor. 8.1 Compensation formulas in double precision floating point Edition BST-BME280-DS001-10 | Revision 1.1 | May 2015", "response": "def _compensate_temperature(self, adc_t):\n        \"\"\"Compensate temperature.\n\n        Formula from datasheet Bosch BME280 Environmental sensor.\n        8.1 Compensation formulas in double precision floating point\n        Edition BST-BME280-DS001-10 | Revision 1.1 | May 2015\n        \"\"\"\n        var_1 = ((adc_t / 16384.0 - self._calibration_t[0] / 1024.0)\n                 * self._calibration_t[1])\n        var_2 = ((adc_t / 131072.0 - self._calibration_t[0] / 8192.0)\n                 * (adc_t / 131072.0 - self._calibration_t[0] / 8192.0)\n                 * self._calibration_t[2])\n        self._temp_fine = var_1 + var_2\n        if self._delta_temp != 0.:  # temperature correction for self heating\n            temp = self._temp_fine / 5120.0 + self._delta_temp\n            self._temp_fine = temp * 5120.0\n        else:\n            temp = self._temp_fine / 5120.0\n        return temp"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compensate_pressure(self, adc_p):\n        var_1 = (self._temp_fine / 2.0) - 64000.0\n        var_2 = ((var_1 / 4.0) * (var_1 / 4.0)) / 2048\n        var_2 *= self._calibration_p[5]\n        var_2 += ((var_1 * self._calibration_p[4]) * 2.0)\n        var_2 = (var_2 / 4.0) + (self._calibration_p[3] * 65536.0)\n        var_1 = (((self._calibration_p[2]\n                   * (((var_1 / 4.0) * (var_1 / 4.0)) / 8192)) / 8)\n                 + ((self._calibration_p[1] * var_1) / 2.0))\n        var_1 /= 262144\n        var_1 = ((32768 + var_1) * self._calibration_p[0]) / 32768\n\n        if var_1 == 0:\n            return 0\n\n        pressure = ((1048576 - adc_p) - (var_2 / 4096)) * 3125\n        if pressure < 0x80000000:\n            pressure = (pressure * 2.0) / var_1\n        else:\n            pressure = (pressure / var_1) * 2\n\n        var_1 = (self._calibration_p[8]\n                 * (((pressure / 8.0) * (pressure / 8.0)) / 8192.0)) / 4096\n        var_2 = ((pressure / 4.0) * self._calibration_p[7]) / 8192.0\n        pressure += ((var_1 + var_2 + self._calibration_p[6]) / 16.0)\n\n        return pressure / 100", "response": "Compensate pressure.\n\n        Formula from datasheet Bosch BME280 Environmental sensor.\n        8.1 Compensation formulas in double precision floating point\n        Edition BST-BME280-DS001-10 | Revision 1.1 | May 2015."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compensate_humidity(self, adc_h):\n        var_h = self._temp_fine - 76800.0\n        if var_h == 0:\n            return 0\n\n        var_h = ((adc_h - (self._calibration_h[3] * 64.0 +\n                           self._calibration_h[4] / 16384.0 * var_h))\n                 * (self._calibration_h[1] / 65536.0\n                    * (1.0 + self._calibration_h[5] / 67108864.0 * var_h\n                       * (1.0 + self._calibration_h[2] / 67108864.0 * var_h))))\n        var_h *= 1.0 - self._calibration_h[0] * var_h / 524288.0\n\n        if var_h > 100.0:\n            var_h = 100.0\n        elif var_h < 0.0:\n            var_h = 0.0\n\n        return var_h", "response": "Compensate humidity.\n\n        Formula from datasheet Bosch BME280 Environmental sensor.\n        8.1 Compensation formulas in double precision floating point\n        Edition BST-BME280-DS001-10 | Revision 1.1 | May 2015."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npopulate the calibration data.", "response": "def _populate_calibration_data(self):\n        \"\"\"Populate calibration data.\n\n        From datasheet Bosch BME280 Environmental sensor.\n        \"\"\"\n        calibration_t = []\n        calibration_p = []\n        calibration_h = []\n        raw_data = []\n\n        try:\n            for i in range(0x88, 0x88 + 24):\n                raw_data.append(self._bus.read_byte_data(self._i2c_add, i))\n            raw_data.append(self._bus.read_byte_data(self._i2c_add, 0xA1))\n            for i in range(0xE1, 0xE1 + 7):\n                raw_data.append(self._bus.read_byte_data(self._i2c_add, i))\n        except OSError as exc:\n            self.log_error(\"Can't populate calibration data: %s\", exc)\n            return\n\n        calibration_t.append((raw_data[1] << 8) | raw_data[0])\n        calibration_t.append((raw_data[3] << 8) | raw_data[2])\n        calibration_t.append((raw_data[5] << 8) | raw_data[4])\n\n        if self._with_pressure:\n            calibration_p.append((raw_data[7] << 8) | raw_data[6])\n            calibration_p.append((raw_data[9] << 8) | raw_data[8])\n            calibration_p.append((raw_data[11] << 8) | raw_data[10])\n            calibration_p.append((raw_data[13] << 8) | raw_data[12])\n            calibration_p.append((raw_data[15] << 8) | raw_data[14])\n            calibration_p.append((raw_data[17] << 8) | raw_data[16])\n            calibration_p.append((raw_data[19] << 8) | raw_data[18])\n            calibration_p.append((raw_data[21] << 8) | raw_data[20])\n            calibration_p.append((raw_data[23] << 8) | raw_data[22])\n\n        if self._with_humidity:\n            calibration_h.append(raw_data[24])\n            calibration_h.append((raw_data[26] << 8) | raw_data[25])\n            calibration_h.append(raw_data[27])\n            calibration_h.append((raw_data[28] << 4) | (0x0F & raw_data[29]))\n            calibration_h.append(\n                (raw_data[30] << 4) | ((raw_data[29] >> 4) & 0x0F))\n            calibration_h.append(raw_data[31])\n\n        for i in range(1, 2):\n            if calibration_t[i] & 0x8000:\n                calibration_t[i] = (-calibration_t[i] ^ 0xFFFF) + 1\n\n        if self._with_pressure:\n            for i in range(1, 8):\n                if calibration_p[i] & 0x8000:\n                    calibration_p[i] = (-calibration_p[i] ^ 0xFFFF) + 1\n\n        if self._with_humidity:\n            for i in range(0, 6):\n                if calibration_h[i] & 0x8000:\n                    calibration_h[i] = (-calibration_h[i] ^ 0xFFFF) + 1\n\n        self._calibration_t = calibration_t\n        self._calibration_h = calibration_h\n        self._calibration_p = calibration_p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _take_forced_measurement(self):\n        # set to forced mode, i.e. \"take next measurement\"\n        self._bus.write_byte_data(self._i2c_add, 0xF4, self.ctrl_meas_reg)\n        while self._bus.read_byte_data(self._i2c_add, 0xF3) & 0x08:\n            sleep(0.005)", "response": "Take a forced measurement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread raw data and update compensated variables.", "response": "def update(self, first_reading=False):\n        \"\"\"Read raw data and update compensated variables.\"\"\"\n        try:\n            if first_reading or not self._ok:\n                self._bus.write_byte_data(self._i2c_add, 0xF2,\n                                          self.ctrl_hum_reg)\n                self._bus.write_byte_data(self._i2c_add, 0xF5, self.config_reg)\n                self._bus.write_byte_data(self._i2c_add, 0xF4,\n                                          self.ctrl_meas_reg)\n                self._populate_calibration_data()\n\n            if self.mode == 2:  # MODE_FORCED\n                self._take_forced_measurement()\n\n            data = []\n            for i in range(0xF7, 0xF7 + 8):\n                data.append(self._bus.read_byte_data(self._i2c_add, i))\n        except OSError as exc:\n            self.log_error(\"Bad update: %s\", exc)\n            self._ok = False\n            return\n\n        pres_raw = (data[0] << 12) | (data[1] << 4) | (data[2] >> 4)\n        temp_raw = (data[3] << 12) | (data[4] << 4) | (data[5] >> 4)\n        hum_raw = (data[6] << 8) | data[7]\n        self._ok = False\n\n        temperature = self._compensate_temperature(temp_raw)\n        if (temperature >= -20) and (temperature < 80):\n            self._temperature = temperature\n            self._ok = True\n        if self._with_humidity:\n            humidity = self._compensate_humidity(hum_raw)\n            if (humidity >= 0) and (humidity <= 100):\n                self._humidity = humidity\n            else:\n                self._ok = False\n        if self._with_pressure:\n            pressure = self._compensate_pressure(pres_raw)\n            if pressure > 100:\n                self._pressure = pressure\n            else:\n                self._ok = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending a PileupElement to this Pileup.", "response": "def append(self, element):\n        '''\n        Append a PileupElement to this Pileup. If an identical PileupElement is\n        already part of this Pileup, do nothing.\n        '''\n        assert element.locus == self.locus, (\n            \"Element locus (%s) != Pileup locus (%s)\"\n            % (element.locus, self.locus))\n        self.elements[element] = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the internal set of pileup elements from another.", "response": "def update(self, other):\n        '''\n        Add all pileup elements from other into self.\n        '''\n        assert self.locus == other.locus\n        self.elements.update(other.elements)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Pileup with the filtered elements removed.", "response": "def filter(self, filters):\n        '''\n        Apply filters to the pileup elements, and return a new Pileup with the\n        filtered elements removed.\n\n        Parameters\n        ----------\n        filters : list of PileupElement -> bool callables\n            A PileupUp element is retained if all filters return True when\n            called on it.\n        '''\n        new_elements = [\n            e for e in self.elements\n            if all(function(e) for function in filters)]\n        return Pileup(self.locus, new_elements)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the decorated function in a new task", "response": "def new_task(func):\n    \"\"\"\n    Runs the decorated function in a new task\n    \"\"\"\n\n    @wraps(func)\n    async def wrapper(self, *args, **kwargs):\n        loop = get_event_loop()\n        loop.create_task(func(self, *args, **kwargs))\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\niterates over all analytics providers in configuration", "response": "async def providers():\n    \"\"\"\n    Iterates over all instances of analytics provider found in configuration\n    \"\"\"\n\n    for provider in settings.ANALYTICS_PROVIDERS:\n        cls: BaseAnalytics = import_class(provider['class'])\n        yield await cls.instance(*provider['args'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntrack the view of a page", "response": "async def page_view(self,\n                        url: str,\n                        title: str,\n                        user_id: str,\n                        user_lang: str='') -> None:\n        \"\"\"\n        Track the view of a page\n        \"\"\"\n\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hash_user_id(self, user_id: str) -> str:\n\n        h = sha256()\n        h.update(user_id.encode())\n        return h.hexdigest()", "response": "Hash a user identifier."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_worker(wname, data, engine_uuid_hex=None, **kwargs):\n    if 'stop_on_halt' not in kwargs:\n        kwargs['stop_on_halt'] = False\n\n    if engine_uuid_hex:\n        engine_uuid = uuid.UUID(hex=engine_uuid_hex)\n        engine = WorkflowEngine.from_uuid(uuid=engine_uuid, **kwargs)\n    else:\n        engine = WorkflowEngine.with_name(wname, **kwargs)\n        engine.save()\n\n    objects = get_workflow_object_instances(data, engine)\n    db.session.commit()\n    engine.process(objects, **kwargs)\n    return engine", "response": "Run a workflow by name with list of data objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restart_worker(uuid, **kwargs):\n    if 'stop_on_halt' not in kwargs:\n        kwargs['stop_on_halt'] = False\n\n    engine = WorkflowEngine.from_uuid(uuid=uuid, **kwargs)\n\n    if \"data\" not in kwargs:\n        objects = workflow_object_class.query(id_workflow=uuid)\n    else:\n        data = kwargs.pop(\"data\")\n        if not isinstance(data, (list, tuple)):\n            data = [data]\n        objects = get_workflow_object_instances(data, engine)\n\n    db.session.commit()\n    engine.process(objects, **kwargs)\n    return engine", "response": "Restarts the workflow from beginning with given engine UUID and any data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrestarting the workflow with given id at given point.", "response": "def continue_worker(oid, restart_point=\"continue_next\", **kwargs):\n    \"\"\"Restart workflow with given id (uuid) at given point.\n\n    By providing the ``restart_point`` you can change the\n    point of which the workflow will continue from.\n\n    * restart_prev: will restart from the previous task\n    * continue_next: will continue to the next task (default)\n    * restart_task: will restart the current task\n\n    ``**kwargs`` can be used to pass custom arguments to the engine/object.\n\n    :param oid: object id of the object to process\n    :type oid: int\n\n    :param restart_point: point to continue from\n    :type restart_point: str\n\n    :return: WorkflowEngine instance\n    \"\"\"\n    if 'stop_on_halt' not in kwargs:\n        kwargs['stop_on_halt'] = False\n\n    workflow_object = workflow_object_class.get(oid)\n    workflow = Workflow.query.get(workflow_object.id_workflow)\n\n    engine = WorkflowEngine(workflow, **kwargs)\n    engine.save()\n\n    db.session.commit()\n    engine.continue_object(\n        workflow_object,\n        restart_point=restart_point,\n        **kwargs\n    )\n    return engine"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_workflow_object_instances(data, engine):\n    workflow_objects = []\n    data_type = engine.get_default_data_type()\n\n    for data_object in data:\n        if isinstance(\n            data_object, workflow_object_class._get_current_object()\n        ):\n            if not data_object.data_type:\n                data_object.data_type = data_type\n\n            if data_object.id:\n                data_object.log.debug(\"Existing workflow object found for \"\n                                      \"this object.\")\n                if data_object.status == data_object.known_statuses.COMPLETED:\n                    data_object.status = data_object.known_statuses.INITIAL\n\n            workflow_objects.append(data_object)\n        else:\n            # Data is not already a WorkflowObject, we then\n            # add the running object to run through the workflow.\n            current_obj = create_data_object_from_data(\n                data_object,\n                engine,\n                data_type\n            )\n            workflow_objects.append(current_obj)\n\n    return workflow_objects", "response": "Analyze data and create corresponding WorkflowObjects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_data_object_from_data(data_object, engine, data_type):\n    # Data is not already a WorkflowObject, we first\n    # create an initial object for each data object.\n    return workflow_object_class.create(\n        data=data_object,\n        id_workflow=engine.uuid,\n        status=workflow_object_class.known_statuses.INITIAL,\n        data_type=data_type,\n    )", "response": "Create a new WorkflowObject from given data and return it."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef h2e(string):\n    result = []\n    for c in string:\n        ccode = ord(c)  # Character code\n\n        if 0x3131 <= ccode <= 0x3163:  # Hangul Compatibility Jamo\n            result.append(H2E_MAPPING[c])\n        elif 0xAC00 <= ccode <= 0xD7A3:  # Hangul Syllables\n            ccode -= 0xAC00\n            # decompose hangul\n            lead = JA_LEAD[ccode // 588]\n            medi = MO[(ccode % 588) // 28]\n            tail = JA_TAIL[(ccode % 588) % 28]\n            result.append(H2E_MAPPING[lead])\n            result.append(H2E_MAPPING[medi])\n            result.append(H2E_MAPPING[tail])\n        else:  # Rest of all characters\n            result.append(c)\n    return ''.join(result)", "response": "Convert a string from Hangul to English."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert english to hangul.", "response": "def e2h(string):\n    \"\"\"\n    Convert english to hangul. ('gksdudxk' -> '\ud55c\uc601\ud0c0')\n    During processing, state interleaves INIT, LEAD_GIVEN, MEDI_GIVEN and TAIL_GIVEN\n\n    :param string: english that actually represent hangul string\n    :return: converted hangul string\n    \"\"\"\n    ctx = Context(string)\n    for char in ctx.input:\n        hchar = E2H_MAPPING.get(char)\n\n        if not char.isalpha() or not hchar:\n            ctx.do_final()\n            ctx.output.append(char)\n            ctx.state = State.INIT\n            continue\n\n        if ctx.state is State.INIT:\n            if hchar not in JA_LEAD:\n                ctx.output.append(hchar)\n            else:\n                ctx.state = State.LEAD_GIVEN\n                ctx.pending.append(hchar)\n\n        elif ctx.state is State.LEAD_GIVEN:\n            if hchar in MO:  # \u3147 + \u314f\n                ctx.state = State.MEDI_GIVEN\n                ctx.pending.append(hchar)\n            else:  # \u3147 + \u3147\n                ctx.state = State.INIT\n                ctx.pending.append(hchar)\n                ctx.flush()\n\n        elif ctx.state is State.MEDI_GIVEN:\n            if hchar in JA:  # \u3147 + \u314f + \u3147\n                ctx.state = State.TAIL_GIVEN\n                ctx.pending.append(hchar)\n            else:\n                compose_waiter = ctx.pending[-1]\n                compose_tuple = JAMO_COMPOSE_TABLE.get(compose_waiter)\n\n                if compose_tuple and hchar in compose_tuple:  # (\u3147 + \u3157 + \u314f)\n                    ctx.pending[-1] = chr((ord(compose_waiter) + compose_tuple.index(hchar) + 1))\n                else:  # (\u3147 + \u314f + \u314f)\n                    ctx.do_final()\n                    ctx.state = State.INIT\n                    ctx.output.append(hchar)\n\n        elif ctx.state is State.TAIL_GIVEN:\n            if hchar in MO:\n                if len(ctx.composing_tails) == 2:  # (\u3147 + \u3153 + \u3135 + \u3154 ==> \uc5b8\uc81c)\n                    ctx.pending.pop()  # drop '\u3135'\n                    ctx.do_final(ctx.composing_tails.pop(0))  # (\u3147 + \u3153 + \u3134)\n                    ctx.pending.append(ctx.composing_tails.pop())  # (\u3148)\n                    ctx.pending.append(hchar)  # (\u3148 + \u3154)\n                    ctx.state = State.MEDI_GIVEN\n                else:  # \u3131,\u314f,\u3145  + \u314f ==> \uac00,\uc0ac\n                    last_hchar = ctx.pending.pop()\n                    ctx.do_final()\n                    ctx.pending.append(last_hchar)\n                    ctx.pending.append(hchar)\n                    ctx.state = State.MEDI_GIVEN\n            else:\n                compose_waiter = ctx.pending[-1]\n                compose_tuple = JAMO_COMPOSE_TABLE.get(compose_waiter)\n\n                if compose_tuple and hchar in compose_tuple:  # (\u3137 + \u314f + \u3139 + \u3131)\n                    ctx.pending[-1] = chr((ord(compose_waiter) + compose_tuple.index(hchar) + 1))\n\n                    ctx.composing_tails.clear()\n                    ctx.composing_tails.append(compose_waiter)\n                    ctx.composing_tails.append(hchar)\n\n                else:  # (\u3137 + \u314f + \u3142 + \u3131) or (\u3137 + \u314f + \u3139 + \u3131 + \u3131)\n                    ctx.do_final()\n                    ctx.state = State.LEAD_GIVEN\n                    ctx.pending.append(hchar)\n                    ctx.composing_tails.clear()\n\n    ctx.do_final()  # finalize remain character if any\n    return ''.join(ctx.output)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomposing a single character from a given consonant and medial vowel.", "response": "def compose(lead, medi, tail=0):\n    \"\"\"\n    Compose hangul using given consonant and vowel\n    :param lead:  lead consonant\n    :param medi:  medial vowel\n    :param tail: tail consonant if any\n    :return: composed hangul character\n    \"\"\"\n    tail_remainder = None\n\n    lead = JA_LEAD.index(lead) * 588\n    medi = MO.index(medi) * 28\n    if tail:\n        try:\n            tail = JA_TAIL.index(tail)\n        except ValueError:\n            tail_remainder = tail\n            tail = 0\n\n    ccode = lead + medi + tail + 0xAC00\n    return [chr(ccode)] if not tail_remainder else [chr(ccode), tail_remainder]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints the rst page of the command what", "response": "def _print_rst(self, what):\n        \"\"\"\n        prints the rst page of the command what\n\n        :param what: the command\n        :type what: string\n\n        \"\"\"\n\n        print\n        print \"Command - %s::\" % what\n\n        exec (\"h = self.do_%s.__doc__\" % what)\n        # noinspection PyUnboundLocalVariable\n        h = textwrap.dedent(h).replace(\"::\\n\\n\", \"\")\n        h = textwrap.dedent(h).replace(\"\\n\", \"\\n    \")\n        print h"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallow loading of JSON rule data.", "response": "def load_json(cls, data, default_rule=None, raise_error=False):\n        \"\"\"Allow loading of JSON rule data.\"\"\"\n\n        rules = {k: _parser.parse_rule(v, raise_error)\n                 for k, v in json.loads(data).items()}\n\n        return cls(rules, default_rule)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_dict(cls, rules_dict: dict, default_rule=None, raise_error=False):\n\n        # Parse the rules stored in the dictionary\n        rules = {k: _parser.parse_rule(v, raise_error)\n                 for k, v in rules_dict.items()}\n\n        return cls(rules, default_rule)", "response": "Allow loading of rule data from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new Rules object based on the provided dict of rules.", "response": "def _set_rules(self, rules: dict, overwrite=True):\n        \"\"\"Created a new Rules object based on the provided dict of rules.\"\"\"\n\n        if not isinstance(rules, dict):\n            raise TypeError('rules must be an instance of dict or Rules,'\n                            'got %r instead' % type(rules))\n\n        if overwrite:\n            self.rules = Rules(rules, self.default_rule)\n        else:\n            self.rules.update(rules)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload rules from policy file or cache.", "response": "def load_rules(self, force_reload=False, overwrite=True):\n        \"\"\"Load rules from policy file or cache.\"\"\"\n\n        # double-checked locking\n        if self.load_once and self._policy_loaded:\n            return\n        with self._load_lock:\n            if self.load_once and self._policy_loaded:\n                return\n\n            reloaded, data = _cache.read_file(\n                self.policy_file, force_reload=force_reload)\n            self._policy_loaded = True\n            if reloaded or not self.rules:\n                rules = Rules.load_json(data, self.default_rule, self.raise_error)\n                self._set_rules(rules, overwrite=overwrite)\n                LOG.debug('Reload policy file: %s', self.policy_file)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef enforce(self, rule, target, creds, exc=None, *args, **kwargs):\n\n        self.load_rules()\n\n        if isinstance(rule, checks.BaseCheck):\n            result = rule(target, creds, self, rule)\n        elif not self.rules:\n            # No rules means we're going to fail closed.\n            result = False\n        else:\n            try:\n                # Evaluate the rule\n                result = self.rules[rule](target, creds, self, rule)\n            except KeyError:\n                LOG.debug('Rule [%s] does not exist', rule)\n                # If the rule doesn't exist, fail closed\n                result = False\n\n        if self.raise_error and not result:\n            if exc:\n                raise exc(*args, **kwargs)\n            else:\n                raise PolicyNotAuthorized(rule, target, creds)\n\n        return result", "response": "Enforces a rule against the target and credentials."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the flattened keys of BoundSpatialPoints in a schema", "response": "def get_flattened_bsp_keys_from_schema(schema):\n    \"\"\" Returns the flattened keys of BoundSpatialPoints in a schema\n\n    :param schema: schema\n    :return: list\n    \"\"\"\n    keys = []\n\n    for key in schema.declared_fields.keys():\n        field = schema.declared_fields[key]\n\n        if isinstance(field, mm.fields.Nested) and \\\n                isinstance(field.schema, BoundSpatialPoint):\n            keys.append(\"{}.{}\".format(key, \"position\"))\n\n    return keys"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning and generate if required the lock for this request.", "response": "def lock(self) -> asyncio.Lock:\n        \"\"\"\n        Return and generate if required the lock for this request.\n        \"\"\"\n\n        if self.lock_key not in self.request.custom_content:\n            self.request.custom_content[self.lock_key] = asyncio.Lock()\n\n        return self.request.custom_content[self.lock_key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the value from the API.", "response": "async def get_value(self):\n        \"\"\"\n        Get the value from the API. Make sure to use a lock in order not to\n        fetch the value twice at the same time.\n        \"\"\"\n\n        cc = self.request.custom_content\n\n        async with self.lock:\n            if self.content_key not in cc:\n                cc[self.content_key] = await self.call_api()\n\n        return cc[self.content_key]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the rank of the current text layer.", "response": "async def rank(self) -> Optional[float]:\n        \"\"\"\n        If there is a text layer inside the request, try to find a matching\n        text in the specified intent.\n        \"\"\"\n\n        if not self.request.has_layer(l.RawText):\n            return\n\n        tl = self.request.get_layer(l.RawText)\n        matcher = Matcher([\n            tuple(Trigram(y) for y in x)\n            for x in await self.intent.strings(self.request)\n        ])\n\n        return matcher % Trigram(tl.text)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nranking the QuickReply layer into available choices.", "response": "def _rank_qr(self, choices):\n        \"\"\"\n        Look for the QuickReply layer's slug into available choices.\n        \"\"\"\n        from bernard.platforms.facebook import layers as fbl\n\n        try:\n            qr = self.request.get_layer(fbl.QuickReply)\n            self.chosen = choices[qr.slug]\n            self.slug = qr.slug\n\n            if self.when is None or self.when == qr.slug:\n                return 1.0\n        except KeyError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _rank_text(self, choices):\n\n        tl = self.request.get_layer(l.RawText)\n        best = 0.0\n\n        for slug, params in choices.items():\n            strings = []\n\n            if params['intent']:\n                intent = getattr(intents, params['intent'])\n                strings += await intent.strings(self.request)\n\n            if params['text']:\n                strings.append((params['text'],))\n\n            matcher = Matcher([tuple(Trigram(y) for y in x) for x in strings])\n            score = matcher % Trigram(await render(tl.text, self.request))\n\n            if score > best:\n                self.chosen = params\n                self.slug = slug\n                best = score\n\n        if self.when is None or self.slug == self.when:\n            return best", "response": "Rank the text layer with the choices."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to find a choice in the user s list of available items.", "response": "async def rank(self):\n        \"\"\"\n        Try to find a choice in what the user did:\n\n        - If there is a quick reply, then use its payload as choice slug\n        - Otherwise, try to match each choice with its intent\n        \"\"\"\n        from bernard.platforms.facebook import layers as fbl\n\n        choices = self.request.get_trans_reg('choices')\n\n        if not choices:\n            return\n\n        if self.request.has_layer(fbl.QuickReply):\n            return self._rank_qr(choices)\n        elif self.request.has_layer(l.RawText):\n            return await self._rank_text(choices)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks that a variable exists for the time series id it has the appropriate attributes for the timeSeries variable.", "response": "def check_timeseries_id(self, dataset):\n        '''\n        Checks that if a variable exists for the time series id it has the appropriate attributes\n\n        :param netCDF4.Dataset dataset: An open netCDF dataset\n        '''\n        timeseries_ids = dataset.get_variables_by_attributes(cf_role='timeseries_id')\n        # No need to check\n        if not timeseries_ids:\n            return\n        test_ctx = TestCtx(BaseCheck.MEDIUM, 'Recommended attributes for the timeSeries variable')\n        timeseries_variable = timeseries_ids[0]\n        test_ctx.assert_true(\n            getattr(timeseries_variable, 'long_name', '') != \"\",\n            \"long_name attribute should exist and not be empty\"\n        )\n        return test_ctx.to_result()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_recommended_attributes(self, dataset):\n        '''\n        Feature type specific check of global recommended attributes.\n\n        :param netCDF4.Dataset dataset: An open netCDF dataset\n        '''\n        results = []\n        recommended_ctx = TestCtx(BaseCheck.MEDIUM, 'Recommended global attributes')\n        # Check time_coverage_duration and resolution\n        for attr in ['time_coverage_duration', 'time_coverage_resolution']:\n            attr_value = getattr(dataset, attr, '')\n            try:\n                parse_duration(attr_value)\n                recommended_ctx.assert_true(True, '')  # Score it True!\n            except Exception:\n                recommended_ctx.assert_true(False, '{} should exist and be ISO-8601 format (example: PT1M30S), currently: {}'.format(attr, attr_value))\n        results.append(recommended_ctx.to_result())\n        return results", "response": "Feature type specific check of global recommended attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_dimensions(self, dataset):\n        '''\n        Checks that the feature types of this dataset are consitent with a time series incomplete dataset\n\n        :param netCDF4.Dataset dataset: An open netCDF dataset\n        '''\n        required_ctx = TestCtx(BaseCheck.HIGH, 'All geophysical variables are time-series incomplete feature types')\n        message = '{} must be a valid timeseries feature type. It must have dimensions of (timeSeries, time).'\n        message += ' And all coordinates must have dimensions of (timeSeries)'\n        for variable in util.get_geophysical_variables(dataset):\n            is_valid = util.is_multi_timeseries_incomplete(dataset, variable)\n            required_ctx.assert_true(\n                is_valid,\n                message.format(variable)\n            )\n        return required_ctx.to_result()", "response": "Checks that the feature types of this dataset are consitent with a time series incomplete dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_timeseries_id(self, dataset):\n        '''\n        Checks that if a variable exists for the time series id it has the appropriate attributes\n\n        :param netCDF4.Dataset dataset: An open netCDF dataset\n        '''\n        results = []\n        required_ctx = TestCtx(BaseCheck.HIGH, 'Required variable for time series identifier')\n        recommended_ctx = TestCtx(BaseCheck.MEDIUM, 'Recommended attributes for the timeSeries variable')\n        # A variable with cf_role=\"timeseries_id\" MUST exist for this to be a valid timeseries incomplete\n        timeseries_ids = dataset.get_variables_by_attributes(cf_role='timeseries_id')\n        required_ctx.assert_true(timeseries_ids, 'a unique variable must define attribute cf_role=\"timeseries_id\"')\n        results.append(required_ctx.to_result())\n        if not timeseries_ids:\n            return results\n\n        timevar = util.get_time_variable(dataset)\n        nc_timevar = dataset.variables[timevar]\n        time_dimensions = nc_timevar.dimensions\n\n        timeseries_variable = timeseries_ids[0]\n        dims = timeseries_variable.dimensions\n        required_ctx.assert_true(\n            time_dimensions and time_dimensions[0] == dims[0],\n            '{} must have a dimension and that dimension must be shared by the time variable'.format(timeseries_variable.name)\n        )\n        recommended_ctx.assert_true(\n            getattr(timeseries_variable, 'long_name', '') != \"\",\n            \"long_name attribute should exist and not be empty\"\n        )\n        results.append(recommended_ctx.to_result())\n        return results", "response": "Checks that a variable exists for the time series identifier it has the appropriate attributes for the time series variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a file if it has been modified.", "response": "def read_file(filename: str, force_reload=False):\n    \"\"\"Read a file if it has been modified.\n\n    :param filename: File name which want to be read from.\n    :param force_reload: Whether to reload the file.\n    :returns: A tuple with a boolean specifying if the data is fresh or not.\n    \"\"\"\n\n    if force_reload:\n        _delete_cached_file(filename)\n\n    reloaded = False\n    mtime = os.path.getmtime(filename)\n    cache_info = CACHE.setdefault(filename, {})\n\n    if not cache_info or mtime > cache_info.get('mtime', 0):\n        LOG.debug('Reloading cached file %s', filename)\n        with open(filename) as fp:\n            cache_info['data'] = fp.read()\n        cache_info['mtime'] = mtime\n        reloaded = True\n\n    return reloaded, cache_info['data']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nraising a : exc : TypeError if the value is not a callable.", "response": "def is_callable(instance, attribute, value):\n    \"\"\"Raises a :exc:`TypeError` if the value is not a callable.\"\"\"\n\n    if not callable(value):\n        raise TypeError(\"'{}' must be callable\".format(attribute.name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef use_model_attr(attr):\n\n    def use_model_validator(instance, attribute, value):\n        getattr(instance, attr)(instance, attribute, value)\n    return use_model_validator", "response": "Use the validator set on a separate attribute on the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_creation_model(instance, attribute, value):\n\n    creation_name = value.get('name')\n    if not isinstance(creation_name, str):\n        instance_name = instance.__class__.__name__\n        err_str = (\"'name' must be given as a string in the '{attr}' \"\n                   \"parameter of a '{cls}'. Given \"\n                   \"'{value}'\").format(attr=attribute.name,\n                                       cls=instance_name,\n                                       value=creation_name)\n        raise ModelDataError(err_str)", "response": "Validate that the value is a valid creation model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a manifestation of the current object is a creation model.", "response": "def is_manifestation_model(instance, attribute, value):\n    \"\"\"Must include a ``manifestationOfWork`` key.\"\"\"\n\n    instance_name = instance.__class__.__name__\n    is_creation_model(instance, attribute, value)\n\n    manifestation_of = value.get('manifestationOfWork')\n    if not isinstance(manifestation_of, str):\n        err_str = (\"'manifestationOfWork' must be given as a string in the \"\n                   \"'{attr}' parameter of a '{cls}'. Given \"\n                   \"'{value}'\").format(attr=attribute.name,\n                                       cls=instance_name,\n                                       value=manifestation_of)\n        print(err_str)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_right_model(instance, attribute, value):\n\n    for key in ['source', 'license']:\n        key_value = value.get(key)\n        if not isinstance(key_value, str):\n            instance_name = instance.__class__.__name__\n            raise ModelDataError((\"'{key}' must be given as a string in \"\n                                  \"the '{attr}' parameter of a '{cls}'. Given \"\n                                  \"'{value}'\").format(key=key,\n                                                      attr=attribute.name,\n                                                      cls=instance_name,\n                                                      value=key_value))", "response": "Validate that the value is a dict of the right model parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the right is a Copyright model.", "response": "def is_copyright_model(instance, attribute, value):\n    \"\"\"Must include at least a ``rightsOf`` key, but not a ``source``\n    key (``rightsOf`` indicates that the Right contains full rights to\n    an existing Manifestation or Work; i.e. is a Copyright).\n    \"\"\"\n\n    rights_of = value.get('rightsOf')\n    if not isinstance(rights_of, str):\n        instance_name = instance.__class__.__name__\n        raise ModelDataError((\"'rightsOf' must be given as a string in \"\n                              \"the '{attr}' parameter of a '{cls}'. Given \"\n                              \"'{value}'\").format(attr=attribute.name,\n                                                  cls=instance_name,\n                                                  value=rights_of))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndefine an argument for the function when running in console script mode. The positional and keyword arguments are the same as for ArgumentParser.add_argument().", "response": "def add_argument(*args, **kwargs):\n    \"\"\"\n    Define an argument for the function when running in console script\n    mode.  The positional and keyword arguments are the same as for\n    ArgumentParser.add_argument().\n    \"\"\"\n\n    def decorator(func):\n        func = ScriptAdaptor._wrap(func)\n        func._add_argument(args, kwargs)\n        return func\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_preprocessor(preproc):\n\n    def decorator(func):\n        func = ScriptAdaptor._wrap(func)\n        func._add_preprocessor(preproc)\n        return func\n    return decorator", "response": "Decorator to add a preprocessor to run after the arguments are parsed and\n    before the function is executed."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_postprocessor(postproc):\n\n    def decorator(func):\n        func = ScriptAdaptor._wrap(func)\n        func._add_postprocessor(postproc)\n        return func\n    return decorator", "response": "Decorator to add a postprocessor to run after the function is executed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _setup_logging(args):\n\n    log_conf = getattr(args, 'logging', None)\n    if log_conf:\n        logging.config.fileConfig(log_conf)\n    else:\n        logging.basicConfig()", "response": "Setup logging for the script based on the logging configuration file specified by the logging attribute of the command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing an XML node describing a limit and return a Limit object.", "response": "def parse_limit_node(db, idx, limit):\n    \"\"\"\n    Given an XML node describing a limit, return a Limit object.\n\n    :param db: Handle for the Redis database.\n    :param idx: The index of the limit in the XML file; used for error\n                reporting.\n    :param limit: The XML node describing the limit.\n    \"\"\"\n\n    # First, try to import the class; this will raise ImportError if\n    # we can't import it\n    klass = utils.find_entrypoint('turnstile.limit', limit.get('class'),\n                                  required=True)\n\n    # Build the list of required attributes\n    required = set(k for k, v in klass.attrs.items()\n                   if 'default' not in v)\n\n    # Now, use introspection on the class to interpret the attributes\n    attrs = {}\n    for child in limit:\n        # Basic validation of child elements\n        if child.tag != 'attr':\n            warnings.warn(\"Unrecognized element %r while parsing limit at \"\n                          \"index %d; ignoring...\" % (child.tag, idx))\n            continue\n\n        # Get the attribute name\n        attr = child.get('name')\n\n        # Be liberal in what we accept--ignore unrecognized attributes\n        # (with a warning)\n        if attr not in klass.attrs:\n            warnings.warn(\"Limit at index %d does not accept an attribute \"\n                          \"%r; ignoring...\" % (idx, attr))\n            continue\n\n        # OK, get the attribute descriptor\n        desc = klass.attrs[attr]\n\n        # Grab the attribute type\n        attr_type = desc.get('type', str)\n\n        if attr_type == list:\n            # Lists are expressed as child elements; we ignore the\n            # child element names\n            subtype = desc.get('subtype', str)\n            value = []\n            try:\n                for j, grandchild in enumerate(child):\n                    if grandchild.tag != 'value':\n                        warnings.warn(\"Unrecognized element %r while parsing \"\n                                      \"%r attribute of limit at index %d; \"\n                                      \"ignoring element...\" %\n                                      (grandchild.tag, attr, idx))\n                        continue\n\n                    value.append(subtype(grandchild.text))\n            except ValueError:\n                warnings.warn(\"Invalid value %r while parsing element %d \"\n                              \"of %r attribute of limit at index %d; \"\n                              \"ignoring attribute...\" %\n                              (grandchild.text, j, attr, idx))\n                continue\n        elif attr_type == dict:\n            # Dicts are expressed as child elements, with the tags\n            # identifying the attribute name\n            subtype = desc.get('subtype', str)\n            value = {}\n            for grandchild in child:\n                if grandchild.tag != 'value':\n                    warnings.warn(\"Unrecognized element %r while parsing \"\n                                  \"%r attribute of limit at index %d; \"\n                                  \"ignoring element...\" %\n                                  (grandchild.tag, attr, idx))\n                    continue\n                elif 'key' not in grandchild.attrib:\n                    warnings.warn(\"Missing 'key' attribute of 'value' \"\n                                  \"element while parsing %r attribute of \"\n                                  \"limit at index %d; ignoring element...\" %\n                                  (attr, idx))\n                    continue\n\n                try:\n                    value[grandchild.get('key')] = subtype(grandchild.text)\n                except ValueError:\n                    warnings.warn(\"Invalid value %r while parsing %r element \"\n                                  \"of %r attribute of limit at index %d; \"\n                                  \"ignoring element...\" %\n                                  (grandchild.text, grandchild.get('key'),\n                                   attr, idx))\n                    continue\n        elif attr_type == bool:\n            try:\n                value = config.Config.to_bool(child.text)\n            except ValueError:\n                warnings.warn(\"Unrecognized boolean value %r while parsing \"\n                              \"%r attribute of limit at index %d; \"\n                              \"ignoring...\" % (child.text, attr, idx))\n                continue\n        else:\n            # Simple type conversion\n            try:\n                value = attr_type(child.text)\n            except ValueError:\n                warnings.warn(\"Invalid value %r while parsing %r attribute \"\n                              \"of limit at index %d; ignoring...\" %\n                              (child.text, attr, idx))\n                continue\n\n        # Save the attribute\n        attrs[attr] = value\n\n        # Remove from the required set\n        required.discard(attr)\n\n    # Did we get all required attributes?\n    if required:\n        raise TypeError(\"Missing required attributes %s\" %\n                        (', '.join(repr(a) for a in sorted(required))))\n\n    # OK, instantiate and return the class\n    return klass(db, **attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting up or update limits in the Redis database. :param conf_file: Name of the configuration file, for connecting to the Redis database. :param limits_file: Name of the XML file describing the limits to configure. :param do_reload: Controls reloading behavior. If True (the default), a reload command is issued. If False, no reload command is issued. String values result in a reload command of the given load type, and integer or float values result in a reload command of type 'spread' with the given spread interval. :param dry_run: If True, no changes are made to the database. Implies debug=True. :param debug: If True, debugging messages are emitted while loading the limits and updating the database.", "response": "def setup_limits(conf_file, limits_file, do_reload=True,\n                 dry_run=False, debug=False):\n    \"\"\"\n    Set up or update limits in the Redis database.\n\n    :param conf_file: Name of the configuration file, for connecting\n                      to the Redis database.\n    :param limits_file: Name of the XML file describing the limits to\n                        configure.\n    :param do_reload: Controls reloading behavior.  If True (the\n                      default), a reload command is issued.  If False,\n                      no reload command is issued.  String values\n                      result in a reload command of the given load\n                      type, and integer or float values result in a\n                      reload command of type 'spread' with the given\n                      spread interval.\n    :param dry_run: If True, no changes are made to the database.\n                    Implies debug=True.\n    :param debug: If True, debugging messages are emitted while\n                  loading the limits and updating the database.\n    \"\"\"\n\n    # If dry_run is set, default debug to True\n    if dry_run:\n        debug = True\n\n    # Connect to the database...\n    conf = config.Config(conf_file=conf_file)\n    db = conf.get_database()\n    limits_key = conf['control'].get('limits_key', 'limits')\n    control_channel = conf['control'].get('channel', 'control')\n\n    # Parse the limits file\n    limits_tree = etree.parse(limits_file)\n\n    # Now, we parse the limits XML file\n    lims = []\n    for idx, lim in enumerate(limits_tree.getroot()):\n        # Skip tags we don't recognize\n        if lim.tag != 'limit':\n            warnings.warn(\"Unrecognized tag %r in limits file at index %d\" %\n                          (lim.tag, idx))\n            continue\n\n        # Construct the limit and add it to the list of limits\n        try:\n            lims.append(parse_limit_node(db, idx, lim))\n        except Exception as exc:\n            warnings.warn(\"Couldn't understand limit at index %d: %s\" %\n                          (idx, exc))\n            continue\n\n    # Now that we have the limits, let's install them\n    if debug:\n        print >>sys.stderr, \"Installing the following limits:\"\n        for lim in lims:\n            print >>sys.stderr, \"  %r\" % lim\n    if not dry_run:\n        database.limit_update(db, limits_key, lims)\n\n    # Were we requested to reload the limits?\n    if do_reload is False:\n        return\n\n    # OK, figure out what kind of reload to do\n    params = []\n    if do_reload is True:\n        # Nothing to do; use default semantics\n        pass\n    elif (isinstance(do_reload, (int, long, float)) or\n          (isinstance(do_reload, basestring) and do_reload.isdigit())):\n        params = ['spread', do_reload]\n    else:\n        params = [str(do_reload)]\n\n    # Issue the reload command\n    if debug:\n        cmd = ['reload']\n        cmd.extend(params)\n        print >>sys.stderr, (\"Issuing command: %s\" %\n                             ' '.join(str(c) for c in cmd))\n    if not dry_run:\n        database.command(db, control_channel, 'reload', *params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_limit_node(root, limit):\n\n    # Build the base limit node\n    limit_node = etree.SubElement(root, 'limit',\n                                  {'class': limit._limit_full_name})\n\n    # Walk through all the recognized attributes\n    for attr in sorted(limit.attrs):\n        desc = limit.attrs[attr]\n        attr_type = desc.get('type', str)\n        value = getattr(limit, attr)\n\n        # Determine the default value, if we have one...\n        if 'default' in desc:\n            default = (desc['default']() if callable(desc['default']) else\n                       desc['default'])\n\n            # Skip attributes that have their default settings\n            if value == default:\n                continue\n\n        # Set up the attr node\n        attr_node = etree.SubElement(limit_node, 'attr', name=attr)\n\n        # Treat lists and dicts specially\n        if attr_type == list:\n            for val in value:\n                val_node = etree.SubElement(attr_node, 'value')\n                val_node.text = str(val)\n        elif attr_type == dict:\n            for key, val in sorted(value.items(), key=lambda x: x[0]):\n                val_node = etree.SubElement(attr_node, 'value', key=key)\n                val_node.text = str(val)\n        else:\n            attr_node.text = str(value)", "response": "Given a root node and a Limit object generate an XML node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_limits(conf_file, limits_file, debug=False):\n\n    # Connect to the database...\n    conf = config.Config(conf_file=conf_file)\n    db = conf.get_database()\n    limits_key = conf['control'].get('limits_key', 'limits')\n\n    # Now, grab all the limits\n    lims = [limits.Limit.hydrate(db, msgpack.loads(lim))\n            for lim in db.zrange(limits_key, 0, -1)]\n\n    # Build up the limits tree\n    root = etree.Element('limits')\n    limit_tree = etree.ElementTree(root)\n    for idx, lim in enumerate(lims):\n        if debug:\n            print >>sys.stderr, \"Dumping limit index %d: %r\" % (idx, lim)\n        make_limit_node(root, lim)\n\n    # Write out the limits file\n    if limits_file == '-':\n        limits_file = sys.stdout\n    if debug:\n        print >>sys.stderr, \"Dumping limits to file %r\" % limits_file\n    limit_tree.write(limits_file, xml_declaration=True, encoding='UTF-8',\n                     pretty_print=True)", "response": "Dump the current limits from the Redis database to the specified XML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the external control daemon.", "response": "def remote_daemon(conf_file):\n    \"\"\"\n    Run the external control daemon.\n\n    :param conf_file: Name of the configuration file.\n    \"\"\"\n\n    eventlet.monkey_patch()\n    conf = config.Config(conf_file=conf_file)\n    daemon = remote.RemoteControlDaemon(None, conf)\n    daemon.serve()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nissue a command to all running control daemons.", "response": "def turnstile_command(conf_file, command, arguments=[], channel=None,\n                      debug=False):\n    \"\"\"\n    Issue a command to all running control daemons.\n\n    :param conf_file: Name of the configuration file.\n    :param command: The command to execute.  Note that 'ping' is\n                    handled specially; in particular, the \"channel\"\n                    parameter is implied.  (A random value will be\n                    used for the channel to listen on.)\n    :param arguments: A list of arguments for the command.  Note that\n                      the colon character (':') cannot be used.\n    :param channel: If not None, specifies the name of a message\n                    channel to listen for responses on.  Will wait\n                    indefinitely; to terminate the listening loop, use\n                    the keyboard interrupt sequence.\n    :param debug: If True, debugging messages are emitted while\n                  sending the command.\n    \"\"\"\n\n    # Connect to the database...\n    conf = config.Config(conf_file=conf_file)\n    db = conf.get_database()\n    control_channel = conf['control'].get('channel', 'control')\n\n    # Now, set up the command\n    command = command.lower()\n    ts_conv = False\n    if command == 'ping':\n        # We handle 'ping' specially; first, figure out the channel\n        if arguments:\n            channel = arguments[0]\n        else:\n            channel = str(uuid.uuid4())\n            arguments = [channel]\n\n        # Next, add on a timestamp\n        if len(arguments) < 2:\n            arguments.append(time.time())\n            ts_conv = True\n\n        # Limit the argument list length\n        arguments = arguments[:2]\n\n    # OK, the command is all set up.  Let us now send the command...\n    if debug:\n        cmd = [command] + arguments\n        print >>sys.stderr, (\"Issuing command: %s\" %\n                             ' '.join(cmd))\n    database.command(db, control_channel, command, *arguments)\n\n    # Were we asked to listen on a channel?\n    if not channel:\n        return\n\n    # OK, let's subscribe to the channel...\n    pubsub = db.pubsub()\n    pubsub.subscribe(channel)\n\n    # Now we listen...\n    try:\n        count = 0\n        for msg in pubsub.listen():\n            # Make sure the message is one we're interested in\n            if debug:\n                formatted = pprint.pformat(msg)\n                print >>sys.stderr, \"Received message: %s\" % formatted\n            if (msg['type'] not in ('pmessage', 'message') or\n                    msg['channel'] != channel):\n                continue\n\n            count += 1\n\n            # Figure out the response\n            response = msg['data'].split(':')\n\n            # If this is a 'pong' and ts_conv is true, add an RTT to\n            # the response\n            if ts_conv and response[0] == 'pong':\n                try:\n                    rtt = (time.time() - float(response[2])) * 100\n                    response.append('(RTT %.2fms)' % rtt)\n                except Exception:\n                    # IndexError or ValueError, probably; ignore it\n                    pass\n\n            # Print out the response\n            print \"Response % 5d: %s\" % (count, ' '.join(response))\n    except KeyboardInterrupt:\n        # We want to break out of the loop, but not return any error\n        # to the caller...\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compactor_daemon(conf_file):\n\n    eventlet.monkey_patch()\n    conf = config.Config(conf_file=conf_file)\n    compactor.compactor(conf)", "response": "Run the compactor daemon."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _wrap(cls, func):\n\n        if isinstance(func, cls):\n            return func\n        return functools.update_wrapper(cls(func), func)", "response": "Wraps a function in a ScriptAdaptor object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_args(self, parser):\n\n        # Add all the arguments to the argument parser\n        for args, kwargs in self._arguments:\n            parser.add_argument(*args, **kwargs)", "response": "Setup the argparse. ArgumentParser object by adding all the arguments taken by the function."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a Namespace object drawn from argparse determines the keyword arguments that the underlying function needs to pass to the underlying function.", "response": "def get_kwargs(self, args):\n        \"\"\"\n        Given a Namespace object drawn from argparse, determines the\n        keyword arguments to pass to the underlying function.  Note\n        that, if the underlying function accepts all keyword\n        arguments, the dictionary returned will contain the entire\n        contents of the Namespace object.  Also note that an\n        AttributeError will be raised if any argument required by the\n        function is not set in the Namespace object.\n\n        :param args: A Namespace object from argparse.\n        \"\"\"\n\n        # Now we need to figure out which arguments the final function\n        # actually needs\n        kwargs = {}\n        argspec = inspect.getargspec(self._func)\n        required = set(argspec.args[:-len(argspec.defaults)]\n                       if argspec.defaults else argspec.args)\n        for arg_name in argspec.args:\n            try:\n                kwargs[arg_name] = getattr(args, arg_name)\n            except AttributeError:\n                if arg_name in required:\n                    # If this happens, that's a programming failure\n                    raise\n\n        # If the function accepts any keyword argument, add whatever\n        # remains\n        if argspec.keywords:\n            for key, value in args.__dict__.items():\n                if key in kwargs:\n                    # Already handled\n                    continue\n                kwargs[key] = value\n\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the underlying function safely given a set of keyword arguments and return the return value.", "response": "def safe_call(self, kwargs, args=None):\n        \"\"\"\n        Call the underlying function safely, given a set of keyword\n        arguments.  If successful, the function return value (likely\n        None) will be returned.  If the underlying function raises an\n        exception, the return value will be the exception message,\n        unless an argparse Namespace object defining a 'debug'\n        attribute of True is provided; in this case, the exception\n        will be re-raised.\n\n        :param kwargs: A dictionary of keyword arguments to pass to\n                       the underlying function.\n        :param args: If provided, this should be a Namespace object\n                     with a 'debug' attribute set to a boolean value.\n\n        :returns: The function return value, or the string value of\n                  the exception raised by the function.\n        \"\"\"\n\n        # Now let's call the function\n        try:\n            return self._func(**kwargs)\n        except Exception as exc:\n            if args and getattr(args, 'debug', False):\n                raise\n            return str(exc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef console(self):\n\n        # First, let's parse the arguments\n        parser = argparse.ArgumentParser(description=self.description)\n        self.setup_args(parser)\n        args = parser.parse_args()\n\n        # Next, let's run the preprocessors in order\n        for proc in self._preprocess:\n            try:\n                proc(args)\n            except Exception as exc:\n                if getattr(args, 'debug', False):\n                    raise\n                return str(exc)\n\n        # Finally, safely call the underlying function\n        result = self.safe_call(self.get_kwargs(args), args)\n\n        # Now, run the postprocessors in order\n        for proc in self._postprocess:\n            result = proc(args, result)\n\n        return result", "response": "Call the underlying function as a console script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimport a class based on its full name.", "response": "def import_class(name: Text) -> Type:\n    \"\"\"\n    Import a class based on its full name.\n\n    :param name: name of the class\n    \"\"\"\n\n    parts = name.split('.')\n    module_name = parts[:-1]\n    class_name = parts[-1]\n    module_ = importlib.import_module('.'.join(module_name))\n    return getattr(module_, class_name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_ro(obj: Any, forgive_type=False):\n\n    if isinstance(obj, (str, bytes, int, float, bool, RoDict, RoList)) \\\n            or obj is None:\n        return obj\n    elif isinstance(obj, Mapping):\n        return RoDict(obj, forgive_type)\n    elif isinstance(obj, Sequence):\n        return RoList(obj, forgive_type)\n    elif forgive_type:\n        return obj\n    else:\n        raise ValueError('Trying to make read-only an object of type \"{}\"'\n                         .format(obj.__class__.__name__))", "response": "Make a json - serializable type recursively read - only\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmake a RO object into a RW structure made with standard Python classes.", "response": "def make_rw(obj: Any):\n    \"\"\"\n    Copy a RO object into a RW structure made with standard Python classes.\n\n    WARNING there is no protection against recursion.\n    \"\"\"\n\n    if isinstance(obj, RoDict):\n        return {k: make_rw(v) for k, v in obj.items()}\n    elif isinstance(obj, RoList):\n        return [make_rw(x) for x in obj]\n    else:\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an URL and a dictionary of values that can be found in the query string of the URL change the query string to include the values specified in the dictionary.", "response": "def patch_qs(url: Text, data: Dict[Text, Text]) -> Text:\n    \"\"\"\n    Given an URL, change the query string to include the values specified in\n    the dictionary.\n\n    If the keys of the dictionary can be found in the query string of the URL,\n    then they will be removed.\n\n    It is guaranteed that all other values of the query string will keep their\n    order.\n    \"\"\"\n\n    qs_id = 4\n    p = list(urlparse(url))\n    qs = parse_qsl(p[qs_id])  # type: List[Tuple[Text, Text]]\n    patched_qs = list(chain(\n        filter(lambda x: x[0] not in data, qs),\n        data.items(),\n    ))\n\n    p[qs_id] = urlencode(patched_qs)\n\n    return urlunparse(p)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that all keys present in subset are present and have the same value in full_set.", "response": "def dict_is_subset(subset: Any, full_set: Any) -> bool:\n    \"\"\"\n    Checks that all keys present in `subset` are present and have the same\n    value in `full_set`. If a key is in `full_set` but not in `subset` then\n    True will be returned anyways.\n    \"\"\"\n\n    if not isinstance(subset, full_set.__class__):\n        return False\n    elif isinstance(subset, dict):\n        for k, v in subset.items():\n            if k not in full_set or not dict_is_subset(v, full_set[k]):\n                return False\n\n        return True\n    elif isinstance(subset, list):\n        if len(subset) != len(full_set):\n            return False\n\n        for a, b in zip(subset, full_set):\n            if not dict_is_subset(a, b):\n                return False\n\n        return True\n    else:\n        return subset == full_set"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compile(self, expression):\n\n        x = self.RE_PYTHON_VAR.sub('(?:\\\\1,)', expression)\n        x = self.RE_SPACES.sub('', x)\n        return re.compile(x)", "response": "Transform a class exp into a regex"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_string(self, objects: List[Any]) -> Text:\n\n        return ''.join(x.__class__.__name__ + ',' for x in objects)", "response": "Transforms a list of objects into a matchable string"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the list of objects matches the expression.", "response": "def match(self, objects: List[Any]) -> bool:\n        \"\"\"\n        Return True if the list of objects matches the expression.\n        \"\"\"\n\n        s = self._make_string(objects)\n        m = self._compiled_expression.match(s)\n        return m is not None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a config option from conf file under section sect.", "response": "def get_conf(conf, sect, opt):\n    \"\"\" Gets a config 'opt' from 'conf' file, under section 'sect'.\n\n    If no 'opt' exists under 'sect', it looks for option on the default_configs\n    dictionary\n\n    If there exists an environmental variable named MAMBUPY_{upper_case_opt},\n    it overrides whatever the conf files or default_configs dict says.\n\n    But if you send a command line argument named mambupy_{lower_case_opt},\n    it overrides anything else.\n\n    Args:\n     conf (ConfigParser): ConfigParser that reads from certain config file (INI\n                          format)\n     sect (string): section under the config file\n     opt (string): option to read\n\n    Returns:\n     string: configuration option. If not found on conf, returns a value from\n             default_configs dict. If environmental variable exists with name\n             MAMBUPY_{upper_case_opt} it overrides anything else\n    \"\"\"\n    argu = getattr(args, \"mambupy_\"+opt.lower())\n\n    if not argu:\n        envir = os.environ.get(\"MAMBUPY_\"+opt.upper())\n        if not envir:\n            try:\n                return conf.get(sect,opt)\n            except NoSectionError:\n                return default_configs[opt]\n        return envir\n    return argu"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iso8601timestamp(T=None, nanos=True, utc=False):\n\tT  = time.time() if T is None else T\n\tTi = math.floor(T)\n\tTn = round((T-Ti)*1e9)\n\tif Tn >= 1e9:\n\t\tTi += 1\n\t\tTn  = 0\n\t\n\ts  = time.gmtime(Ti)      if utc   else time.localtime(Ti)\n\tf  = time.strftime(\"%Y%m%dT%H%M%S\", s)\n\tn  = \".{:09d}\".format(Tn) if nanos else \"\"\n\ttz = \"Z\"                  if utc   else time.strftime(\"%z\", s)\n\treturn f+n+tz", "response": "Get ISO8601 - formatted timestamp string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createWorkDir(baseDir,\n                  projName,\n                  expUUID,\n                  expNames = [],\n                  nanos    = True,\n                  utc      = False):\n\t\"\"\"Create working directory for experiment if not existing already.\"\"\"\n\t\n\t#\n\t# First, ensure the project's top-level hierarchy, especially by-uuid/,\n\t# exists, so that the only possible failure is due to the creation of\n\t# one additional directory.\n\t#\n\tprojDir    = os.path.join(baseDir, projName)\n\tbyuuidDir  = os.path.join(projDir, \"by-uuid\")\n\tbytimeDir  = os.path.join(projDir, \"by-time\")\n\tbynameDir  = os.path.join(projDir, \"by-name\", *expNames)\n\tbyuuidPath = os.path.join(byuuidDir, expUUID)\n\tos.makedirs(byuuidDir, mode=0o755, exist_ok=True)\n\tos.makedirs(bytimeDir, mode=0o755, exist_ok=True)\n\tos.makedirs(bynameDir, mode=0o755, exist_ok=True)\n\t\n\t#\n\t# Attempt the creation of the experiment workDir by its UUID. Record\n\t# whether we were the original creators.\n\t#\n\ttry:\n\t\tpreexisting = False\n\t\tos.makedirs(byuuidPath,\n\t\t            mode     = 0o755,\n\t\t            exist_ok = False)\n\texcept FileExistsError:\n\t\tpreexisting = True\n\t\n\t#\n\t# If we were the first to create this working directory, additionally\n\t# make symlinks pointing to it from the auxiliary directories.\n\t#\n\tif not preexisting:\n\t\texpTime     = iso8601timestamp(nanos=nanos, utc=utc)\n\t\texpTimeUUID = expTime+\"-\"+expUUID\n\t\tbytimePath  = os.path.join(bytimeDir, expTimeUUID)\n\t\tbynamePath  = os.path.join(bynameDir, expUUID)\n\t\tos.symlink(os.path.relpath(byuuidPath, bytimeDir), bytimePath, True)\n\t\tos.symlink(os.path.relpath(byuuidPath, bynameDir), bynamePath, True)\n\t\n\t#\n\t# Create handy .rsync-filter files.\n\t#\n\twith contextlib.suppress(OSError):\n\t\twith open(os.path.join(baseDir, \".rsync-filter\"), \"x\") as f:\n\t\t\tf.write(\"#\\n\"\n\t\t\t        \"# rsync filter rules.\\n\"\n\t\t\t        \"#\\n\"\n\t\t\t        \"# When the argument -F is given to rsync, the rules within will be obeyed.\\n\"\n\t\t\t        \"#\\n\")\n\t\n\twith contextlib.suppress(OSError):\n\t\twith open(os.path.join(projDir, \".rsync-filter\"), \"x\") as f:\n\t\t\tf.write(\"#\\n\"\n\t\t\t        \"# rsync filter rules.\\n\"\n\t\t\t        \"#\\n\"\n\t\t\t        \"# When the argument -F is given to rsync, the rules within will be obeyed.\\n\"\n\t\t\t        \"#\\n\")\n\t\n\t#\n\t# Return the constructed workDir.\n\t#\n\treturn byuuidPath", "response": "Create a working directory for the given experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef humanize_timesince(start_time):  # pylint:disable=too-many-return-statements\n    if not start_time:\n        return start_time\n\n    delta = local_now() - start_time\n\n    # assumption: negative delta values originate from clock\n    #             differences on different app server machines\n    if delta.total_seconds() < 0:\n        return 'a few seconds ago'\n\n    num_years = delta.days // 365\n    if num_years > 0:\n        return '{} year{} ago'.format(\n            *((num_years, 's') if num_years > 1 else (num_years, '')))\n\n    num_weeks = delta.days // 7\n    if num_weeks > 0:\n        return '{} week{} ago'.format(\n            *((num_weeks, 's') if num_weeks > 1 else (num_weeks, '')))\n\n    num_days = delta.days\n    if num_days > 0:\n        return '{} day{} ago'.format(\n            *((num_days, 's') if num_days > 1 else (num_days, '')))\n\n    num_hours = delta.seconds // 3600\n    if num_hours > 0:\n        return '{} hour{} ago'.format(*((num_hours, 's') if num_hours > 1 else (num_hours, '')))\n\n    num_minutes = delta.seconds // 60\n    if num_minutes > 0:\n        return '{} minute{} ago'.format(\n            *((num_minutes, 's') if num_minutes > 1 else (num_minutes, '')))\n\n    return 'a few seconds ago'", "response": "Creates a string representation of time since the given start_time."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef humanize_timedelta(seconds):\n    hours, remainder = divmod(seconds, 3600)\n    days, hours = divmod(hours, 24)\n    minutes, seconds = divmod(remainder, 60)\n\n    if days:\n        result = '{}d'.format(days)\n        if hours:\n            result += ' {}h'.format(hours)\n        if minutes:\n            result += ' {}m'.format(minutes)\n        return result\n\n    if hours:\n        result = '{}h'.format(hours)\n        if minutes:\n            result += ' {}m'.format(minutes)\n        return result\n\n    if minutes:\n        result = '{}m'.format(minutes)\n        if seconds:\n            result += ' {}s'.format(seconds)\n        return result\n\n    return '{}s'.format(seconds)", "response": "Creates a string representation of timedelta."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding default start behaviour by raising ConnectionError instead of custom requests_mock. exceptions. NoMockAddress.", "response": "def start(self):\n        \"\"\"Overrides default start behaviour by raising ConnectionError instead\n        of custom requests_mock.exceptions.NoMockAddress.\n        \"\"\"\n        if self._http_last_send is not None:\n            raise RuntimeError('HttpMock has already been started')\n\n        # 1) save request.Session.send in self._last_send\n        # 2) replace request.Session.send with MockerCore send function\n        super(HttpMock, self).start()\n\n        # 3) save MockerCore send function in self._http_last_send\n        # 4) replace request.Session.send with HttpMock send function\n        self._patch_last_send()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a new handler for the specified time interval.", "response": "def handle(self, handler):\r\n        \"\"\" Registers a handler. The handler can be transmitted together \r\n        with two arguments as a list or dictionary. The arguments are:\r\n        \r\n        memoize \r\n            if True, the execution result will be cached in self.memoize\r\n        timeout \r\n            will allocate a predefined time interval for the execution\r\n            \r\n        If arguments are provided as a list, they are considered to have \r\n        this sequence: (handler, memoize, timeout)                \r\n        \r\n        Examples:\r\n            event += handler    \r\n            event += (handler, True, 1.5)\r\n            event += {'handler':handler, 'memoize':True, 'timeout':1.5}         \r\n        \"\"\"\r\n        handler_, memoize, timeout = self._extract(handler)\r\n        with self._hlock:\r\n            handlers = self.handlers.copy()  # Immutable as in .NET http://stackoverflow.com/a/786455/541420\r\n            handlers[hash(handler_)] = (handler_, memoize, timeout)\r\n            self.handlers = handlers\r\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unhandle(self, handler):\r\n        h, _, _ = self._extract(handler)\r\n        key = hash(h)\r\n        with self._hlock:\r\n            if key not in self.handlers:\r\n                raise ValueError('Handler \"%s\" was not found' % str(h))\r\n            handlers = self.handlers.copy()\r\n            del handlers[key]\r\n            self.handlers = handlers\r\n        return self", "response": "Unregisters a handler from the list of handlers."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes all handlers in a queue and returns a list of results.", "response": "def fire(self, *args, **kw):\r\n        \"\"\" Stores all registered handlers in a queue for processing \"\"\"\r\n        result = []\r\n\r\n        with self._hlock:\r\n            handlers = self.handlers\r\n\r\n        if self.threads == 0:  # same-thread execution - synchronized\r\n            for k in handlers:\r\n                # handler, memoize, timeout\r\n                h, m, t = handlers[k]\r\n                try:\r\n                    r = self._memoize(h, m, t, *args, **kw)\r\n                    result.append(tuple(r))\r\n                except:\r\n                    result.append((False, self._error(sys.exc_info()), h))\r\n\r\n        elif self.threads > 0:  # multi-thread execution - desynchronized if self.threads > 1\r\n            queue = Queue()\r\n\r\n            # result lock just in case [].append() is not  \r\n            # thread-safe in other Python implementations\r\n            rlock = RLock()\r\n\r\n            def _execute(*args, **kw):\r\n                \"\"\" Executes all handlers stored in the queue \"\"\"\r\n                while True:\r\n                    try:\r\n                        item = queue.get()\r\n                        if item is None:\r\n                            queue.task_done()\r\n                            break\r\n\r\n                        # handler, memoize, timeout\r\n                        h, m, t = handlers[item]  # call under active lock\r\n\r\n                        try:\r\n                            r = self._memoize(h, m, t, *args, **kw)\r\n                            if not self.asynch:\r\n                                with rlock:\r\n                                    result.append(tuple(r))\r\n                        except:\r\n                            if not self.asynch:\r\n                                with rlock:\r\n                                    result.append((False, self._error(sys.exc_info()), h))\r\n\r\n                        queue.task_done()\r\n\r\n                    except Empty:  # never triggered, just to be safe\r\n                        break\r\n\r\n            if handlers:\r\n                threads = self._threads(handlers=handlers)\r\n\r\n                for _ in range(threads):\r\n                    t = Thread(target=_execute, args=args, kwargs=kw)\r\n                    t.daemon = True\r\n                    t.start()\r\n\r\n                for k in handlers:\r\n                    queue.put(k)\r\n\r\n                    if self.asynch:  # main thread, no locking required\r\n                        h, _, _ = handlers[k]\r\n                        result.append((None, None, h))\r\n\r\n                for _ in range(threads):\r\n                    queue.put(None)  # stop each worker\r\n\r\n                if not self.asynch:\r\n                    queue.join()\r\n\r\n        return tuple(result) or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndiscarding all registered handlers and cached results", "response": "def clear(self):\r\n        \"\"\" Discards all registered handlers and cached results \"\"\"\r\n        with self._hlock:\r\n            self.handlers.clear()\r\n        with self._mlock:\r\n            self.memoize.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts a handler and handler s arguments that can be provided AttributeNames as list or dictionary.", "response": "def _extract(self, item):\r\n        \"\"\" Extracts a handler and handler's arguments that can be provided \r\n        as list or dictionary. If arguments are provided as list, they are \r\n        considered to have this sequence: (handler, memoize, timeout)                \r\n        Examples:\r\n            event += handler\r\n            event += (handler, True, 1.5)\r\n            event += {'handler':handler, 'memoize':True, 'timeout':1.5}\r\n        \"\"\"\r\n        if not item:\r\n            raise ValueError('Invalid arguments')\r\n\r\n        handler = None\r\n        memoize = False\r\n        timeout = 0\r\n\r\n        if not isinstance(item, (list, tuple, dict)):\r\n            handler = item\r\n        elif isinstance(item, (list, tuple)):\r\n            if len(item) == 3:\r\n                handler, memoize, timeout = item\r\n            elif len(item) == 2:\r\n                handler, memoize = item\r\n            elif len(item) == 1:\r\n                handler = item\r\n        elif isinstance(item, dict):\r\n            handler = item.get('handler')\r\n            memoize = item.get('memoize', False)\r\n            timeout = item.get('timeout', 0)\r\n        return handler, bool(memoize), float(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _memoize(self, handler, memoize, timeout, *args, **kw):\r\n        if not isinstance(handler, Event) and self.sender is not None:\r\n            args = list(args)[:]\r\n            args.insert(0, self.sender)\r\n\r\n        if not memoize:  # no caching\r\n\r\n            if timeout <= 0:  # no time restriction\r\n                return True, handler(*args, **kw), handler\r\n\r\n            result = self._timeout(timeout, handler, *args, **kw)\r\n            if isinstance(result, tuple) and len(result) == 3:\r\n                if isinstance(result[1], Exception):  # error occurred\r\n                    return False, self._error(result), handler\r\n            return True, result, handler\r\n\r\n        else:  # caching\r\n            with self._mlock:  # cache structure lock\r\n                hash_ = hash(handler)\r\n                if hash_ in self.memoize:\r\n                    for args_, kw_, result in self.memoize[hash_]:\r\n                        if args_ == args and kw_ == kw:  # shallow structure comparison only\r\n                            return True, result, handler\r\n\r\n                if timeout <= 0:  # no time restriction\r\n                    result = handler(*args, **kw)\r\n                else:\r\n                    result = self._timeout(timeout, handler, *args, **kw)\r\n                    if isinstance(result, tuple) and len(result) == 3:\r\n                        if isinstance(result[1], Exception):  # error occurred\r\n                            return False, self._error(result), handler\r\n\r\n                if hash_ not in self.memoize:\r\n                    self.memoize[hash_] = []\r\n                self.memoize[hash_].append((args, kw, result))\r\n                return True, result, handler", "response": "Memoizes the execution result of successful executionsudo\r\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhandle the timeout of the method execution", "response": "def _timeout(self, timeout, handler, *args, **kw):\r\n        \"\"\" Controls the time allocated for the execution of a method \"\"\"\r\n        t = spawn_thread(target=handler, args=args, kw=kw)\r\n        t.daemon = True\r\n        t.start()\r\n        t.join(timeout)\r\n\r\n        if not t.is_alive():\r\n            if t.exc_info:\r\n                return t.exc_info\r\n            return t.result\r\n        else:\r\n            try:\r\n                msg = '[%s] Execution was forcefully terminated'\r\n                raise RuntimeError(msg % t.name)\r\n            except:\r\n                return sys.exc_info()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _threads(self, handlers):\r\n        if self.threads < len(handlers):\r\n            return self.threads\r\n        return len(handlers)", "response": "Calculates the maximum number of threads that will be started"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the error info from the exception info.", "response": "def _error(self, exc_info):\r\n        \"\"\" Retrieves the error info \"\"\"\r\n        if self.exc_info:\r\n            if self.traceback:\r\n                return exc_info\r\n            return exc_info[:2]\r\n        return exc_info[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self):\n        if not self._ok:\n            self.log_error(\"Trying to restore OK mode w/ soft reset\")\n            self._ok = self._soft_reset()\n        try:\n            self._bus.write_byte(self._i2c_add, CMD_READ_TEMP_NOHOLD)\n            sleep(MEASUREMENT_WAIT_TIME)\n            buf_t = self._bus.read_i2c_block_data(\n                self._i2c_add, CMD_READ_TEMP_HOLD, 3)\n\n            self._bus.write_byte(self._i2c_add, CMD_READ_HUM_NOHOLD)\n            sleep(MEASUREMENT_WAIT_TIME)\n            buf_h = self._bus.read_i2c_block_data(\n                self._i2c_add, CMD_READ_HUM_HOLD, 3)\n        except OSError as exc:\n            self._ok = False\n            self.log_error(\"Bad reading: %s\", exc)\n            return\n\n        if self._crc8check(buf_t):\n            temp = (buf_t[0] << 8 | buf_t[1]) & 0xFFFC\n            self._temperature = self._calc_temp(temp)\n\n            if self._crc8check(buf_h):\n                humid = (buf_h[0] << 8 | buf_h[1]) & 0xFFFC\n                rh_actual = self._calc_humid(humid)\n                # For temperature coefficient compensation\n                rh_final = self._temp_coefficient(rh_actual, self._temperature)\n                rh_final = 100.0 if rh_final > 100 else rh_final  # Clamp > 100\n                rh_final = 0.0 if rh_final < 0 else rh_final  # Clamp < 0\n                self._humidity = rh_final\n            else:\n                self._humidity = -255\n                self._ok = False\n                self.log_error(\"Bad CRC error with humidity\")\n        else:\n            self._temperature = -255\n            self._ok = False\n            self.log_error(\"Bad CRC error with temperature\")", "response": "Read raw data and calculate temperature and humidity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_owner_access_token(self):\n        from .database import Session\n        db_session = Session.object_session(self)\n        owner = db_session.query(User).filter_by(\n            id_=self.owner_id).first()\n        return owner.access_token", "response": "Return workflow owner access token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_workflow_status(db_session, workflow_uuid, status,\n                               new_logs='', message=None):\n        \"\"\"Update database workflow status.\n\n        :param workflow_uuid: UUID which represents the workflow.\n        :param status: String that represents the workflow status.\n        :param new_logs: New logs from workflow execution.\n        :param message: Unused.\n        \"\"\"\n        try:\n            workflow = \\\n                db_session.query(Workflow).filter_by(id_=workflow_uuid).first()\n\n            if not workflow:\n                raise Exception('Workflow {0} doesn\\'t exist in database.'.\n                                format(workflow_uuid))\n            if status:\n                workflow.status = status\n            if new_logs:\n                workflow.logs = (workflow.logs or '') + new_logs + '\\n'\n            db_session.commit()\n        except Exception as e:\n            raise e", "response": "Update database workflow status."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse server address and return host and port", "response": "def parse_server_addr(str_addr, default_port=26000):\n    \"\"\"Parse address and returns host and port\n\n    Args:\n        str_addr --- string that contains server ip or hostname and optionaly\n        port\n\n    Returns: tuple (host, port)\n\n    Examples:\n\n    >>> parse_server_addr('127.0.0.1:26006')\n    ('127.0.0.1', 26006)\n    >>> parse_server_addr('[2001:db8:85a3:8d3:1319:8a2e:370:7348]:26006')\n    ('2001:db8:85a3:8d3:1319:8a2e:370:7348', 26006)\n    >>> parse_server_addr('[2001:db8:85a3:8d3:1319:8a2e:370:7348]')\n    ('2001:db8:85a3:8d3:1319:8a2e:370:7348', 26000)\n    >>> parse_server_addr('localhost:123')\n    ('localhost', 123)\n    >>> parse_server_addr('localhost:1d23')\n    Traceback (most recent call last):\n        ...\n    ValueError: Bad address string \"localhost:1d23\"\n    \"\"\"\n    m = ADDR_STR_RE.match(str_addr)\n    if m is None:\n        raise ValueError('Bad address string \"{0}\"'.format(str_addr))\n\n    dct = m.groupdict()\n    port = dct.get('port')\n    if port is None:\n        port = default_port\n    else:\n        port = int(port)  # Caution: could raise ValueEror or TypeError\n\n    if port == 0:\n        raise ValueError(\"Port can't be zero\")\n\n    host = dct['host'] if dct['host'] else dct['host6']\n    return host, port"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_state_changed(self, state):\n        super(GoToDefinitionMode, self).on_state_changed(state)\n        if state:\n            self.editor.mouse_moved.connect(self._on_mouse_moved)\n            self.editor.mouse_released.connect(self._on_mouse_released)\n            self.editor.add_action(self.action_goto, sub_menu='COBOL')\n            self.editor.mouse_double_clicked.connect(\n                self._timer.cancel_requests)\n        else:\n            self.editor.mouse_moved.disconnect(self._on_mouse_moved)\n            self.editor.mouse_released.disconnect(self._on_mouse_released)\n            self.editor.remove_action(self.action_goto, sub_menu='Python')\n            self.editor.mouse_double_clicked.disconnect(\n                self._timer.cancel_requests)", "response": "Connects and disconnects slots to and from signals when the mode state has changed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a decoration for the word under cursor.", "response": "def _add_decoration(self, cursor):\n        \"\"\"\n        Adds a decoration for the word under ``cursor``.\n        \"\"\"\n        if self.select_word(cursor):\n            self.editor.set_mouse_cursor(Qt.PointingHandCursor)\n        else:\n            self.editor.set_mouse_cursor(Qt.IBeamCursor)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrequests a go to assignment.", "response": "def request_goto(self, tc=None):\n        \"\"\"\n        Request a go to assignment.\n\n        :param tc: Text cursor which contains the text that we must look for\n                   its assignment. Can be None to go to the text that is under\n                   the text cursor.\n        :type tc: QtGui.QTextCursor\n        \"\"\"\n        if not tc:\n            tc = TextHelper(self.editor).word_under_cursor(\n                select_whole_word=True)\n        if not self._definition or isinstance(self.sender(), QAction):\n            self.select_word(tc)\n        if self._definition is not None:\n            QTimer.singleShot(100, self._goto_def)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_template(name):\n    path = os.path.join(base_dir, name)\n\n    if path not in templates:\n        try:\n            templates[path] = Template(path)\n        except IOError:\n            return None\n\n    return copy.deepcopy(templates[path])", "response": "Returns a copy of the template with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_element(self, elem, check_children=False, next_to_elem=None):\n        self.__add_element('eid', elem.attribs, self.__element_ids, elem, next_to_elem)\n        self.__add_element('aid', elem.attribs, self.__attrib_ids, elem, next_to_elem)\n        self.__add_element('rid', elem.attribs, self.__repeat_ids, elem, next_to_elem)\n        if check_children and elem.children:\n            for child in elem.children:\n                self.check_element(child, True, next_to_elem)", "response": "Checks the attributes of an entry - set entry for references to the three proton attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_value(self, eid, val, idx='*'):\n        if eid in self.__element_ids:\n            elems = self.__element_ids[eid]\n            if type(val) in SEQ_TYPES:\n                idx = 0\n            if idx == '*':\n                for elem in elems:\n                    self.__set_value(eid, elem, val, idx)\n            elif idx < len(elems):\n                self.__set_value(eid, elems[idx], val, idx)", "response": "Set the content of an xml element marked with the matching eid attribute."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_attribute(self, aid, attrib, val, idx='*'):\n        if aid in self.__attrib_ids:\n            elems = self.__attrib_ids[aid]\n            if idx == '*':\n                for elem in elems:\n                    self.__set_attribute(elem, attrib, val)\n            elif idx < len(elems):\n                elem = elems[idx]\n                self.__set_attribute(elem, attrib, val)", "response": "Set the value of an attribute in an xml element."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_properties(self, eid, value, idx='*'):\n        if value.__class__ not in Template.class_cache:\n            props = []\n            for name in dir(value.__class__):\n                prop = getattr(value.__class__, name)\n                if type(prop) == property and hasattr(prop, 'fget'):\n                    props.append((name, prop))\n            Template.class_cache[value.__class__] = props\n        for (name, prop) in Template.class_cache[value.__class__]:\n            new_eid = ''.join([eid, ':', name])\n            self.set_value(new_eid, prop.fget(value), idx)\n            self.set_attribute(eid, name, prop.fget(value), idx)", "response": "Set the value and attributes of an xml element marked with the matching eid attribute using the the\n        properties of the specified object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hide(self, eid, index=0):\n        elems = None\n        if eid in self.__element_ids:\n            elems = self.__element_ids[eid]\n        elif eid in self.__repeat_ids:\n            elems = self.__repeat_ids[eid]\n\n        if elems and index < len(elems):\n            elem = elems[index]\n            elem.parent.children.remove(elem)", "response": "Hide the element with the matching eid. If no match look for an element with the matching rid. If no match look for an element with the matching eid. If no match look for an element with the matching rid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef repeat(self, rid, count, index=0):\n        elems = None\n        if rid in self.__repeat_ids:\n            elems = self.__repeat_ids[rid]\n        elif rid in self.__element_ids:\n            elems = self.__element_ids\n\n        if elems and index < len(elems):\n            elem = elems[index]\n            self.__repeat(elem, count)", "response": "Repeat an xml element with the matching rid."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreplaces an xml element with the matching eid.", "response": "def replace(self, eid, replacement, index=0):\n        \"\"\"\n        Replace an xml element marked with the matching eid. If the replacement value is an Element or TextElement,\n        it's swapped in untouched. If it's a Template, the children of the root element in the template are used.\n        Otherwise the replacement value is wrapped with a TextElement.\n        \"\"\"\n        if eid in self.__element_ids:\n            elems = self.__element_ids[eid]\n        elif eid in self.__repeat_ids:\n            elems = self.__repeat_ids[eid]\n        else:\n            return\n\n        if index < len(elems):\n            elem = elems[index]\n            current_pos = elem.parent.children.index(elem)\n            elem.parent.children.remove(elem)\n            replacement_type = type(replacement)\n            if replacement_type in (Element, TextElement):\n                self.check_element(replacement, True)\n                elem.parent.children.insert(current_pos, replacement)\n                replacement.parent = elem.parent\n            elif replacement_type == Template:\n                for child in replacement.root.children:\n                    elem.parent.children.insert(current_pos, child)\n                    child.parent = elem.parent\n                    current_pos += 1\n                self.__merge_ids(self.__element_ids, replacement.__element_ids)\n                self.__merge_ids(self.__attrib_ids, replacement.__attrib_ids)\n                self.__merge_ids(self.__repeat_ids, replacement.__repeat_ids)\n            else:\n                elem.parent.children.insert(current_pos, TextElement(replacement))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the has algorithm and optionally the number of rounds to use.", "response": "def set_hasher(self, hash, rounds=None):\n        \"\"\"Updates the has algorithm and, optionally, the number of rounds\n        to use.\n\n        Raises:\n            `~WrongHashAlgorithm` if new algorithm isn't one of the three\n            recomended options.\n\n        \"\"\"\n        hash = hash.replace('-', '_')\n        if hash not in VALID_HASHERS:\n            raise WrongHashAlgorithm(WRONG_HASH_MESSAGE)\n        hasher = getattr(ph, hash)\n        utils.test_hasher(hasher)\n\n        default_rounds = getattr(hasher, 'default_rounds', 1)\n        min_rounds = getattr(hasher, 'min_rounds', 1)\n        max_rounds = getattr(hasher, 'max_rounds', float(\"inf\"))\n        rounds = min(max(rounds or default_rounds, min_rounds), max_rounds)\n        op = {\n            'schemes': VALID_HASHERS + DEPRECATED_HASHERS,\n            'deprecated': DEPRECATED_HASHERS,\n            'default': hash,\n            hash + '__default_rounds': rounds\n        }\n        self.hasher = CryptContext(**op)\n        self.hash = hash.replace('_', '-')  # For testing\n        self.rounds = rounds"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, key, default=None):\n\n        return self._config.get(None, {}).get(key, default)", "response": "Retrieves the given configuration option."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_database(self, override=None):\n\n        # Grab the database connection arguments\n        redis_args = self['redis']\n\n        # If we have an override, read some overrides from that\n        # section\n        if override:\n            redis_args = redis_args.copy()\n            for key, value in self[override].items():\n                if not key.startswith('redis.'):\n                    continue\n                key = key[len('redis.'):]\n                if value:\n                    redis_args[key] = value\n                else:\n                    redis_args.pop(key, None)\n\n        # Return the redis database connection\n        return database.initialize(redis_args)", "response": "Returns a redis database handle for the specified RedisCOOKIE."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a string to a boolean value.", "response": "def to_bool(value, do_raise=True):\n        \"\"\"Convert a string to a boolean value.\n\n        If the string consists of digits, the integer value of the string\n        is coerced to a boolean value.  Otherwise, any of the strings \"t\",\n        \"true\", \"on\", \"y\", and \"yes\" are considered True and any of the\n        strings \"f\", \"false\", \"off\", \"n\", and \"no\" are considered False.\n        A ValueError will be raised for any other value.\n        \"\"\"\n\n        value = value.lower()\n\n        # Try it as an integer\n        if value.isdigit():\n            return bool(int(value))\n\n        # OK, check it against the true/false values...\n        if value in _str_true:\n            return True\n        elif value in _str_false:\n            return False\n\n        # Not recognized\n        if do_raise:\n            raise ValueError(\"invalid literal for to_bool(): %r\" % value)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def become(self, layer_type: Type[L], request: 'Request') -> L:\n\n        raise ValueError('Cannot become \"{}\"'.format(layer_type.__name__))", "response": "Transform this layer into another layer type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def become(self, layer_type: Type[L], request: 'Request'):\n        if layer_type != RawText:\n            super(Text, self).become(layer_type, request)\n\n        return RawText(await render(self.text, request))", "response": "Transforms the translatable string into an actual string and put it into a RawText."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the health of the states in the states module.", "response": "async def health_check(self) -> Iterator[HealthCheckFail]:\n        \"\"\"\n        Perform the checks. So far:\n\n        - Make a list of the unique destination states from the transitions\n          list, then check the health of each of them.\n        \"\"\"\n\n        ds_class = getattr(settings, 'DEFAULT_STATE', '')\n        forbidden_defaults = [None, '', 'bernard.engine.state.DefaultState']\n\n        if ds_class in forbidden_defaults:\n            yield HealthCheckFail(\n                '00005',\n                f'Default state (`DEFAULT_STATE` in settings) is not set. '\n                f'You need to set it to your own implementation. Please refer '\n                f'yourself to the doc. See '\n                f'https://github.com/BernardFW/bernard/blob/develop/doc/'\n                f'get_started.md#statespy'\n            )\n\n        try:\n            import_class(ds_class)\n        except (ImportError, KeyError, AttributeError, TypeError):\n            yield HealthCheckFail(\n                '00005',\n                f'Cannot import \"{ds_class}\", which is the value'\n                f' of `DEFAULT_STATE` in the configuration. This means either'\n                f' that your `PYTHONPATH` is wrong or that the value you gave'\n                f' to `DEFAULT_STATE` is wrong. You need to provide a default'\n                f' state class for this framework to work. Please refer'\n                f' yourself to the documentation for more information. See'\n                f' https://github.com/BernardFW/bernard/blob/develop/doc/'\n                f'get_started.md#statespy'\n            )\n\n        states = set(t.dest for t in self.transitions)\n\n        for state in states:\n            async for check in state.health_check():\n                yield check"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes the register storage.", "response": "def _make_register(self) -> BaseRegisterStore:\n        \"\"\"\n        Make the register storage.\n        \"\"\"\n\n        s = settings.REGISTER_STORE\n        store_class = import_class(s['class'])\n        return store_class(**s['params'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_transitions(self) -> List[Transition]:\n\n        module_name = settings.TRANSITIONS_MODULE\n        module_ = importlib.import_module(module_name)\n        return module_.transitions", "response": "Load the transitions file and return the list of transitions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _make_allowed_states(self) -> Iterator[Text]:\n\n        for trans in self.transitions:\n            yield trans.dest.name()\n\n            if trans.origin:\n                yield trans.origin.name()", "response": "Generate the names of all allowed states."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _find_trigger(self,\n                            request: Request,\n                            origin: Optional[Text]=None,\n                            internal: bool=False) \\\n            -> Tuple[\n                Optional[BaseTrigger],\n                Optional[Type[BaseState]],\n                Optional[bool],\n            ]:\n        \"\"\"\n        Find the best trigger for this request, or go away.\n        \"\"\"\n\n        reg = request.register\n\n        if not origin:\n            origin = reg.get(Register.STATE)\n            logger.debug('From state: %s', origin)\n\n        results = await asyncio.gather(*(\n            x.rank(request, origin)\n            for x\n            in self.transitions\n            if x.internal == internal\n        ))\n\n        if len(results):\n            score, trigger, state, dnr = max(results, key=lambda x: x[0])\n\n            if score >= settings.MINIMAL_TRIGGER_SCORE:\n                return trigger, state, dnr\n\n        return None, None, None", "response": "Find the best trigger for this request or go away."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _confused_state(self, request: Request) -> Type[BaseState]:\n\n        origin = request.register.get(Register.STATE)\n\n        if origin in self._allowed_states:\n            try:\n                return import_class(origin)\n            except (AttributeError, ImportError):\n                pass\n\n        return import_class(settings.DEFAULT_STATE)", "response": "Find which state to call."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _build_state(self,\n                           request: Request,\n                           message: BaseMessage,\n                           responder: Responder) \\\n            -> Tuple[\n                Optional[BaseState],\n                Optional[BaseTrigger],\n                Optional[bool],\n            ]:\n        \"\"\"\n        Build the state for this request.\n        \"\"\"\n\n        trigger, state_class, dnr = await self._find_trigger(request)\n\n        if trigger is None:\n            if not message.should_confuse():\n                return None, None, None\n            state_class = self._confused_state(request)\n            logger.debug('Next state: %s (confused)', state_class.name())\n        else:\n            logger.debug('Next state: %s', state_class.name())\n\n        state = state_class(request, responder, trigger, trigger)\n        return state, trigger, dnr", "response": "Build the state for this request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting the state and handle it.", "response": "async def _run_state(self, responder, state, trigger, request) \\\n            -> BaseState:\n        \"\"\"\n        Execute the state, or if execution fails handle it.\n        \"\"\"\n\n        user_trigger = trigger\n\n        # noinspection PyBroadException\n        try:\n            if trigger:\n                await state.handle()\n            else:\n                await state.confused()\n\n            for i in range(0, settings.MAX_INTERNAL_JUMPS + 1):\n                if i == settings.MAX_INTERNAL_JUMPS:\n                    raise MaxInternalJump()\n\n                trigger, state_class, dnr = \\\n                    await self._find_trigger(request, state.name(), True)\n\n                if not trigger:\n                    break\n\n                logger.debug('Jumping to state: %s', state_class.name())\n                state = state_class(request, responder, trigger, user_trigger)\n                await state.handle()\n        except Exception:\n            logger.exception('Error while handling state \"%s\"', state.name())\n            responder.clear()\n            reporter.report(request, state.name())\n            await state.error()\n\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the next register to store.", "response": "async def _build_state_register(self,\n                                    state: BaseState,\n                                    request: Request,\n                                    responder: Responder) -> Dict:\n        \"\"\"\n        Build the next register to store.\n\n            - The state is the name of the current state\n            - The transition is made by all successive layers present in the\n              response.\n        \"\"\"\n\n        return {\n            Register.STATE: state.name(),\n            Register.TRANSITION:\n                await responder.make_transition_register(request),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle a message from the broker and runs it.", "response": "async def _handle_message(self,\n                              message: BaseMessage,\n                              responder: Responder) -> Optional[Dict]:\n        \"\"\"\n        Handles a message: find a state and run it.\n\n        :return: The register that was saved\n        \"\"\"\n\n        async def noop(request: Request, responder: Responder):\n            pass\n\n        mm = MiddlewareManager.instance()\n        reg_manager = self.register\\\n            .work_on_register(message.get_conversation().id)\n\n        async with reg_manager as reg:\n            request = Request(message, reg)\n            await request.transform()\n\n            if not request.stack.layers:\n                return\n\n            logger.debug('Incoming message: %s', request.stack)\n            await mm.get('pre_handle', noop)(request, responder)\n\n            # noinspection PyBroadException\n            try:\n                state, trigger, dnr = \\\n                    await self._build_state(request, message, responder)\n            except Exception:\n                logger.exception('Error while finding a transition from %s',\n                                 reg.get(Register.STATE))\n                reporter.report(request, None)\n                return\n\n            if state is None:\n                logger.debug(\n                    'No next state found but \"%s\" is not confusing, stopping',\n                    request.message,\n                )\n                return\n\n            state = await self._run_state(responder, state, trigger, request)\n\n            # noinspection PyBroadException\n            try:\n                await responder.flush(request)\n            except MissingTranslationError as e:\n                responder.clear()\n                responder.send([RawText(str(e))])\n                await responder.flush(request)\n\n                reporter.report(request, state.name())\n                logger.exception('Missing translation in state %s',\n                                 state.name())\n            except Exception:\n                reporter.report(request, state.name())\n                logger.exception('Could not flush content after %s',\n                                 state.name())\n            else:\n                if not dnr:\n                    reg.replacement = await self._build_state_register(\n                        state,\n                        request,\n                        responder,\n                    )\n                return reg.replacement"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef runGetResults(cmd, stdout=True, stderr=True, encoding=sys.getdefaultencoding()):\n        '''\n            runGetResults - Simple method to run a command and return the results of the execution as a dict.\n\n            @param cmd <str/list> - String of command and arguments, or list of command and arguments\n\n                If cmd is a string, the command will be executed as if ran exactly as written in a shell. This mode supports shell-isms like '&&' and '|'\n                If cmd is a list, the first element will be the executable, and further elements are arguments that will be passed to that executable.\n\n            @param stdout <True/False> - Default True, Whether to gather and include program's stdout data in results.\n\n                If False, that data the program prints to stdout will just be output to the current tty and not recorded.\n                If True, it will NOT be output to the tty, and will be recorded under the key \"stdout\" in the return dict.\n\n            @param stderr <True/False or \"stdout\"/subprocess.STDOUT> - Default True, Whether to gather and include program's stderr data in results, or to combine with \"stdout\" data.\n\n                If False, the data the program prints to stderr will just be output to the current tty and not recorded\n                If True, it will NOT be output to the tty, and will be recorded under the key \"stderr\" in the return dict.\n                If \"stdout\" or subprocess.STDOUT - stderr data will be blended with stdout data. This requires that stdout=True.\n\n\n            @param encoding <None/str> - Default sys.getdefaultencoding(), the program's output will automatically be decoded using the provided codec (e.x. \"utf-8\" or \"ascii\").\n\n                If None or False-ish, data will not be decoded (i.e. in python3 will be \"bytes\" type)\n\n                If unsure, leave this as it's default value, or provide \"utf-8\"\n\n            @return <dict> - Dict of results. Has following keys:\n\n                'returnCode' - <int> - Always present, included the integer return-code from the command.\n                'stdout'       <unciode/str/bytes (depending on #encoding)> - Present if stdout=True, contains data output by program to stdout, or stdout+stderr if stderr param is \"stdout\"/subprocess.STDOUT\n                'stderr'       <unicode/str/bytes (depending on #encoding)> - Present if stderr=True, contains data output by program to stderr.\n\n\n            @raises - SimpleCommandFailure if it cannot launch the given command, for reasons such as: cannot find the executable, or no permission to execute, etc\n        '''\n   \n        if stderr in ('stdout', subprocess.STDOUT):\n            stderr = subprocess.STDOUT\n        elif stderr == True or stderr == subprocess.PIPE:\n            stderr = subprocess.PIPE\n        else:\n            stderr = None\n\n        if stdout == True or stdout == subprocess.STDOUT:\n            stdout = subprocess.PIPE\n        else:\n            stdout = None\n            if stderr == subprocess.PIPE:\n                raise ValueError('Cannot redirect stderr to stdout if stdout is not captured.')\n\n        if issubclass(cmd.__class__, (list, tuple)):\n            shell = False\n        else:\n            shell = True\n\n        try:\n            pipe = subprocess.Popen(cmd, stdout=stdout, stderr=stderr, shell=shell)\n        except Exception as e:\n            try:\n                if shell is True:\n                    cmdStr = ' '.join(cmd)\n                else:\n                    cmdStr = cmd\n            except:\n                cmdStr = repr(cmd)\n\n            raise SimpleCommandFailure('Failed to execute \"%s\": %s' %(cmdStr, str(e)), returnCode=255)\n\n\n        streams = []\n        fileNoToKey = {}\n        ret = {}\n        if stdout == subprocess.PIPE:\n            streams.append(pipe.stdout)\n            fileNoToKey[pipe.stdout.fileno()] = 'stdout'\n            ret['stdout'] = []\n        if stderr == subprocess.PIPE:\n            streams.append(pipe.stderr)\n            fileNoToKey[pipe.stderr.fileno()] = 'stderr'\n            ret['stderr'] = []\n\n        returnCode = None\n\n\n        time.sleep(.02)\n        while returnCode is None or streams:\n            returnCode = pipe.poll()\n\n            while True:\n                (readyToRead, junk1, junk2) = select.select(streams, [], [], .005)\n                if not readyToRead:\n                    # Don't strangle CPU\n                    time.sleep(.01)\n                    break\n\n                for readyStream in readyToRead:\n\n                    retKey = fileNoToKey[readyStream.fileno()]\n                    curRead = readyStream.read()\n                    if curRead in (b'', ''):\n                        streams.remove(readyStream)\n                        continue\n                    ret[retKey].append(curRead)\n\n\n        for key in list(ret.keys()):\n            ret[key] = b''.join(ret[key])\n            if encoding:\n                ret[key] = ret[key].decode(encoding)\n\n        ret['returnCode'] = returnCode\n        \n        return ret", "response": "This method is used to run a command and return the results of the execution as a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef runGetOutput(cmd, raiseOnFailure=False, encoding=sys.getdefaultencoding()):\n        '''\n            runGetOutput - Simply runs a command and returns the output as a string. Use #runGetResults if you need something more complex.\n\n            @param cmd <str/list> - String of command and arguments, or list of command and arguments\n\n                If cmd is a string, the command will be executed as if ran exactly as written in a shell. This mode supports shell-isms like '&&' and '|'\n                If cmd is a list, the first element will be the executable, and further elements are arguments that will be passed to that executable.\n\n\n            @param raiseOnFailure <True/False> - Default False, if True a non-zero return from the command (failure) will raise a SimpleCommandFailure, which contains all gathered output and return code. @see #SimpleCommandFailure\n\n            \n            @param encoding <None/str> - Default sys.getdefaultencoding(), the program's output will automatically be decoded using the provided codec (e.x. \"utf-8\" or \"ascii\").\n\n                If None or False-ish, data will not be decoded (i.e. in python3 will be \"bytes\" type)\n\n                If unsure, leave this as it's default value, or provide \"utf-8\"\n\n\n            @return <str> - String of data output by the executed program. This combines stdout and stderr into one string. If you need them separate, use #runGetResults\n\n            @raises SimpleCommandFailure - \n\n                * If command cannot be executed (like program not found, insufficient permissions, etc)\n\n                * If #raiseOnFailure is set to True, and the program returns non-zero\n\n        '''\n\n\n        \n        results = Simple.runGetResults(cmd, stdout=True, stderr=subprocess.STDOUT, encoding=encoding)\n        if raiseOnFailure is True and results['returnCode'] != 0:\n            try:\n                if issubclass(cmd.__class__, (list, tuple)):\n                    cmdStr = ' '.join(cmd)\n                else:\n                    cmdStr = cmd\n            except:\n                cmdStr = repr(cmd)\n                \n            failMsg = \"Command '%s' failed with returnCode=%d\" %(cmdStr, results['returnCode'])\n            raise SimpleCommandFailure(failMsg, results['returnCode'], results.get('stdout', None), results.get('stderr', None))\n            \n\n        return results['stdout']", "response": "Runs a command and returns the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_context_store(name='default',\n                         ttl=settings.CONTEXT_DEFAULT_TTL,\n                         store=settings.CONTEXT_STORE) -> 'BaseContextStore':\n    \"\"\"\n    Create a context store. By default using the default configured context\n    store, but you can use a custom class if you want to using the `store`\n    setting.\n\n    The time to live of each store (aka there is one per conversation) is\n    defined by the `ttl` value, which is also inferred by default from the\n    configuration.\n\n    You can have several stores existing in parallel. To make the distinction\n    between them you need to give them different names, using the `name`\n    parameter.\n\n    The usage looks like:\n\n    >>> cs = create_context_store()\n    >>> class Hello(BaseTestState):\n    >>>     @cs.inject(['foo'])\n    >>>     async def handle(self, context):\n    >>>         logger.debug('foo is %s', context['foo'])\n    >>>\n    >>>     async def missing_context(self):\n    >>>         self.send(lyr.Text('`foo` is not in context'))\n\n    This requires that `foo` is present in the context in order to enter the\n    handler.\n\n    See `BaseContextStore.inject()` for more info.\n    \"\"\"\n\n    store_class = import_class(store['class'])\n    return store_class(name=name, ttl=ttl, **store['params'])", "response": "Create a new context store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inject(self,\n               require: Optional[List[Text]] = None,\n               fail: Text = 'missing_context',\n               var_name: Text = 'context'):\n        \"\"\"\n        This is a decorator intended to be used on states (and actually only\n        work on state handlers).\n\n        The `require` argument is a list of keys to be checked in the context.\n        If at least one of them is missing, then instead of calling the handler\n        another method will be called. By default the method is\n        `missing_context` but it can be configured using the `fail` argument.\n\n        The context will be injected into the handler as a keyword arg. By\n        default, the arg is expected to be named `context` but you can change\n        it to anything you'd like using `var_name`.\n\n        See `create_context_store()` for a full example.\n        \"\"\"\n\n        def decorator(func):\n            async def health_check(cls) -> Iterator[HealthCheckFail]:\n                if not callable(getattr(cls, fail, None)):\n                    yield HealthCheckFail(\n                        '00001',\n                        f'State \"{cls.__name__}\" has no method \"{fail}\" to '\n                        f'fall back to if required attributes are missing '\n                        f'from the context.'\n                    )\n\n            if require:\n                func.health_check = health_check\n\n            @wraps(func)\n            async def wrapper(state: Union[BaseState, BaseTrigger], **kwargs):\n                conv_id = state.request.conversation.id\n                key = f'context::{self.name}::{conv_id}'\n\n                x = self.open(key)\n                async with x as context:\n                    for item in (require or []):\n                        if item not in context:\n                            return await getattr(state, fail)(state, **kwargs)\n\n                    kwargs[var_name] = context\n                    return await func(state, **kwargs)\n\n            return wrapper\n        return decorator", "response": "Decorator for state handlers that checks the state s context and injects it into the handler."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef camelcase(text, acronyms=None):\n    words, _case, _sep = case_parse.parse_case(text, acronyms)\n    if words:\n        words[0] = words[0].lower()\n    return ''.join(words)", "response": "Return text in camelCase style."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pascalcase(text, acronyms=None):\n    words, _case, _sep = case_parse.parse_case(text, acronyms)\n    return ''.join(words)", "response": "Return text in PascalCase style."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns text in CONST_CASE style.", "response": "def constcase(text, acronyms=None):\n    \"\"\"Return text in CONST_CASE style (aka SCREAMING_SNAKE_CASE).\n\n    Args:\n        text: input string to convert case\n        detect_acronyms: should attempt to detect acronyms\n        acronyms: a list of acronyms to detect\n\n    >>> constcase(\"hello world\")\n    'HELLO_WORLD'\n    >>> constcase(\"helloHTMLWorld\", True, [\"HTML\"])\n    'HELLO_HTML_WORLD'\n    \"\"\"\n    words, _case, _sep = case_parse.parse_case(text, acronyms)\n    return '_'.join([w.upper() for w in words])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dotcase(text, acronyms=None):\n    words, _case, _sep = case_parse.parse_case(text, acronyms)\n    return '.'.join([w.lower() for w in words])", "response": "Return text in dot. case style."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef separate_words(text, acronyms=None):\n    words, _case, _sep = case_parse.parse_case(text, acronyms, preserve_case=True)\n    return ' '.join(words)", "response": "Return text in seperate words style."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the next token in the string.", "response": "def get_next_token(string):\n    '''\n    \"eats\" up the string until it hits an ending character to get valid leaf expressions.\n    For example, given \\\\Phi_{z}(L) = \\\\sum_{i=1}^{N} \\\\frac{1}{C_{i} \\\\times V_{\\\\rm max, i}},\n    this function would pull out \\\\Phi, stopping at _\n    @ string: str\n    returns a tuple of (expression [ex: \\\\Phi], remaining_chars [ex: _{z}(L) = \\\\sum_{i=1}^{N}...])\n    '''\n    STOP_CHARS = \"_ {}^ \\n ,()=\"\n    UNARY_CHARS = \"^_\"\n    # ^ and _ are valid leaf expressions--just ones that should be handled on their own\n    if string[0] in STOP_CHARS:\n        return string[0], string[1:]\n    \n    expression = []\n    for i, c in enumerate(string):\n        if c in STOP_CHARS:\n            break\n        else:\n            expression.append(c)\n    \n    return \"\".join(expression), string[i:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread in the significant pathways file as a pandas. DataFrame.", "response": "def _load_significant_pathways_file(path_to_file):\n    \"\"\"Read in the significant pathways file as a\n    pandas.DataFrame.\n    \"\"\"\n    feature_pathway_df = pd.read_table(\n        path_to_file, header=0,\n        usecols=[\"feature\", \"side\", \"pathway\"])\n    feature_pathway_df = feature_pathway_df.sort_values(\n        by=[\"feature\", \"side\"])\n    return feature_pathway_df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pathway_feature_permutation(pathway_feature_tuples,\n                                 permutation_max_iters):\n    \"\"\"Permute the pathways across features for one side in the\n    network. Used in `permute_pathways_across_features`\n\n    Parameters\n    -----------\n    pathway_feature_tuples : list(tup(str, int))\n      a tuple list [(pathway, feature)] where the pathway, feature pairing\n      indicates that a pathway was overrepresented in that feature\n    permutation_max_iters : int\n      specify the maximum number of iterations, limit the number of attempts\n      we have to generate a permutation\n\n    Returns\n    -----------\n    list(tup(str, int)), the list of pathway, feature pairings after the\n      permutation\n    \"\"\"\n\n    pathways, features = [list(elements_at_position)\n                          for elements_at_position in\n                          zip(*pathway_feature_tuples)]\n\n    original_pathways = pathways[:]\n\n    random.shuffle(pathways)\n    feature_block_locations = {}\n    i = 0\n    while i < len(pathways):\n        starting_index = i\n        current_feature = features[i]\n        pathway_set = set()\n        # input is grouped by feature, so we want to keep track of the start\n        # and end of a given \"block\" of the same feature--this corresponds\n        # to all the pathways overrepresented in that feature.\n        while i < len(pathways) and features[i] == current_feature:\n            # check the results of the permutation. if `pathway_set` does\n            # not contain the current pathway, we are maintaining the\n            # necessary invariants in our permutation thus far.\n            if pathways[i] not in pathway_set:\n                pathway_set.add(pathways[i])\n            else:\n                k = 0\n                random_pathway = None\n                while True:\n                    # select another random pathway from the list\n                    # and get the feature to which it belongs\n                    j = random.choice(range(0, len(pathways)))\n                    random_pathway = pathways[j]\n                    random_feature = features[j]\n                    if (random_pathway != pathways[i] and\n                            random_pathway not in pathway_set):\n                        # if this is a feature we have not already seen,\n                        # we are done.\n                        if random_feature not in feature_block_locations:\n                            break\n                        # otherwise, look at the indices that correspond\n                        # to that feature's block of pathways\n                        feature_block_start, feature_block_end = \\\n                            feature_block_locations[random_feature]\n                        pathway_block = pathways[feature_block_start:\n                                                 feature_block_end]\n                        # make sure that the current pathway is not in\n                        # that block--ensures that we maintain the invariant\n                        # after the swap\n                        if pathways[i] not in pathway_block:\n                            break\n                    k += 1\n                    if k > permutation_max_iters:\n                        print(\"Permutation step: reached the maximum \"\n                              \"number of iterations {0}.\".format(\n                                permutation_max_iters))\n                        return None\n                pathway_set.add(random_pathway)\n                pathways[j] = pathways[i]\n                pathways[i] = random_pathway\n            i += 1\n        ending_index = i\n        feature_block_locations[current_feature] = (\n            starting_index, ending_index)\n\n    if original_pathways == pathways:\n        return None\n    return list(zip(pathways, features))", "response": "Permute the pathways across features for one side in the system. Used in _permute_pathways_across_features."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether the generated permutation is a valid permutation.", "response": "def _permutation_correctness(pathway_feature_tuples, original):\n    \"\"\"Determine whether the generated permutation is a valid permutation.\n    Used in `permute_pathways_across_features`.\n    (Cannot be identical to the original significant pathways list,\n    and a feature should map to a distinct set of pathways.)\n    \"\"\"\n    if pathway_feature_tuples:\n        if set(pathway_feature_tuples) == set(original):\n            return False\n        pathways_in_feature = {}\n        for (pathway, feature) in pathway_feature_tuples:\n            if feature not in pathways_in_feature:\n                pathways_in_feature[feature] = set()\n            if pathway in pathways_in_feature[feature]:\n                return False\n            else:\n                pathways_in_feature[feature].add(pathway)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connected_to(self, vertex_id):\n        if vertex_id not in self.edge:\n            return None\n        connected_to = (self.edge[1] if self.edge[0] == vertex_id\n                        else self.edge[0])\n        return connected_to", "response": "Return the entry point id that is connected to the given vertex_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_network_file(self, path_to_network_file):\n        network_df = pd.read_table(path_to_network_file)\n        network_edges = {}\n        for _, row in network_df.iterrows():\n            vertex0_id = self.add_pathway(row[\"pw0\"])\n            vertex1_id = self.add_pathway(row[\"pw1\"])\n            edge_id = self.edge_tuple(vertex0_id, vertex1_id)\n            if \"features\" in row:\n                network_edges[edge_id] = \\\n                  [int(float(f)) for f in row[\"features\"].split(\" \")]\n            else:\n                network_edges[edge_id] = []\n        self._augment_network(network_edges)", "response": "Reads a network file and adds edges to the species."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _construct_from_dataframe(self):\n        direct_edges = {}\n        feature_side_grouping = self.feature_pathway_df.groupby(\n            [\"feature\", \"side\"])\n        for (feature, _), group in feature_side_grouping:\n            co_occurring_pathways = group[\"pathway\"].tolist()\n            pairings = list(itertools.combinations(co_occurring_pathways, 2))\n            if not pairings:\n                continue\n            self.features.add(feature)\n            for pathway0, pathway1 in pairings:\n                vertex0_id = self.add_pathway(pathway0)\n                vertex1_id = self.add_pathway(pathway1)\n                new_edge = self.edge_tuple(vertex0_id, vertex1_id)\n                if new_edge not in direct_edges:\n                    direct_edges[new_edge] = []\n                direct_edges[new_edge].append(feature)\n        self._augment_network(direct_edges)", "response": "Build the network from the significant pathways dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding a CoNetwork from a dictionary of significant pathways and their edges.", "response": "def _construct_from_permutation(self, significant_pathways):\n        \"\"\"Build the network from a dictionary of (side -> tuple lists),\n        where the side is specified as \"pos\" and/or \"neg\" (from the feature\n        gene signature(s)) and mapped to a tuple list of [(pathway, feature)].\n        Used during the PathCORE-T permutation test by applying the method\n        `permute_pathways_across_features` to an existing CoNetwork.\n        \"\"\"\n        for side, pathway_feature_tuples in significant_pathways.items():\n            feature_pathway_dict = self._collect_pathways_by_feature(\n                pathway_feature_tuples)\n            self._edges_from_permutation(feature_pathway_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef permute_pathways_across_features(self):\n        side_labels = self.feature_pathway_df.side.unique()\n        pathway_features = {}\n        for side in side_labels:\n            pathway_features[side] = []\n\n        feature_grouping = self.feature_pathway_df.groupby(\n            [\"side\", \"feature\"])\n\n        for (side, feature), group in feature_grouping:\n            pathways = group[\"pathway\"].tolist()\n            for pathway in pathways:\n                pathway_features[side].append((pathway, feature))\n\n        permuted_network = {}\n        for side in side_labels:\n            pathway_side_permutation = _pathway_feature_permutation(\n                pathway_features[side], self.permutation_max_iters)\n            while pathway_side_permutation is None:\n                pathway_side_permutation = _pathway_feature_permutation(\n                    pathway_features[side], self.permutation_max_iters)\n            assert _permutation_correctness(\n                pathway_side_permutation, pathway_features[side]), \\\n                (\"Permutation on side {0} did not preserve the \"\n                 \"expected invariants\").format(side)\n            permuted_network[side] = pathway_side_permutation\n        return CoNetwork(self.n_features,\n                         significant_pathways=permuted_network)", "response": "Return a permutation of the network. Requires that the significant pathways file has been specified during the CoNetwork."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the edge weights by odds ratios.", "response": "def weight_by_edge_odds_ratios(self,\n                                   edges_expected_weight,\n                                   flag_as_significant):\n        \"\"\"Applied during the permutation test. Update the edges in the\n        network to be weighted by their odds ratios. The odds ratio measures\n        how unexpected the observed edge weight is based on the expected\n        weight.\n\n        Parameters\n        -----------\n        edges_expected_weight : list(tup(int, int), float)\n          A tuple list of (edge id, edge expected weight) generated from the\n          permutation test step.\n        flag_as_significant : [set|list](tup(int, int))\n          A set or list of edge ids that are considered significant against\n          the null model of random associations generated in the permutation\n          test\n        \"\"\"\n        for edge_id, expected_weight in edges_expected_weight:\n            edge_obj = self.edges[edge_id]\n            edge_obj.weight /= expected_weight\n            if edge_id in flag_as_significant:\n                edge_obj.significant = True\n            else:\n                edge_obj.significant = False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef aggregate(self, merge):\n        self.features = set()\n        self.n_features += merge.n_features\n\n        vertex_id_conversion = self.convert_pathway_mapping(merge.pathways)\n        for edge_id, edge in merge.edges.items():\n            edge_key = self.remapped_edge(\n                vertex_id_conversion, edge_id)\n            if edge_key in self.edges:\n                if self.edges[edge_key].which_features:\n                    self.edges[edge_key].which_features = []\n                self.edges[edge_key].weight += edge.weight\n            else:\n                vertex0_id, vertex1_id = edge_key\n                new_edge_obj = Edge(vertex0_id, vertex1_id, [])\n                new_edge_obj.weight = edge.weight\n                self.edges[edge_key] = new_edge_obj\n                self._add_edge_to_vertex(vertex0_id, new_edge_obj)\n                self._add_edge_to_vertex(vertex1_id, new_edge_obj)", "response": "Combine this CoNetwork object with another CoNetwork object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse to convert the pathway-to-vertex id mapping in one CoNetwork to the one used in the current CoNetwork (`self`). The following tasks are carried out in the remapping: (1) If `self.pathways` contains the pathway to be merged, map the vertex id in `other_pathway_mapping` to the vertex id in `self.pathways`. (2) If not, create a vertex in `self` and then add the key-value pair to `self.pathways` accordingly. Parameters ----------- other_pathway_mapping : dict(str -> int) the `pathways` field in the CoNetwork class. This is a (pathway -> vertex id) map. Returns ----------- dict(int -> int), the (other vertex id -> `self` vertex id) conversion map", "response": "def convert_pathway_mapping(self, other_pathway_mapping):\n        \"\"\"Used to convert the pathway-to-vertex id mapping in one CoNetwork\n        to the one used in the current CoNetwork (`self`).\n        The following tasks are carried out in the remapping:\n        (1) If `self.pathways` contains the pathway to be merged, map the\n            vertex id in `other_pathway_mapping` to the vertex id\n            in `self.pathways`.\n        (2) If not, create a vertex in `self` and then add the key-value pair\n            to `self.pathways` accordingly.\n\n        Parameters\n        -----------\n        other_pathway_mapping : dict(str -> int)\n          the `pathways` field in the CoNetwork class. This is a\n          (pathway -> vertex id) map.\n\n        Returns\n        -----------\n        dict(int -> int), the (other vertex id -> `self` vertex id)\n          conversion map\n        \"\"\"\n        vertex_id_conversion = {}\n        for pathway, vertex_id in other_pathway_mapping.items():\n            if pathway in self.pathways:\n                vertex_id_conversion[vertex_id] = self.pathways[pathway]\n            else:\n                self_vertex_id = self.add_pathway(pathway)\n                self.vertices[self_vertex_id] = Vertex(self_vertex_id)\n                vertex_id_conversion[vertex_id] = self_vertex_id\n        return vertex_id_conversion"}
