{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls the specified method property or attribute of the object that is used", "response": "def call_member(obj, f, *args, **kwargs):\n    \"\"\" Calls the specified method, property or attribute of the given object\n\n    Parameters\n    ----------\n    obj : object\n        The object that will be used\n    f : str or function\n        Name of or reference to method, property or attribute\n    failfast : bool\n        If True, will raise an exception when trying a method that doesn't exist. If False, will simply return None\n        in that case\n    \"\"\"\n    # get function name\n    if not isinstance(f, str):\n        fname = f.__func__.__name__\n    else:\n        fname = f\n    # get the method ref\n    method = getattr(obj, fname)\n    # handle cases\n    if inspect.ismethod(method):\n        return method(*args, **kwargs)\n\n    # attribute or property\n    return method"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_args(func):\n    args, varargs, keywords, defaults = getargspec_no_self(func)\n    return dict(zip(args[-len(defaults):], defaults))", "response": "returns a dictionary of arg_name = > default_values for the input function\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef blocksplit_dtrajs(dtrajs, lag=1, sliding=True, shift=None):\n    dtrajs_new = []\n    for dtraj in dtrajs:\n        if len(dtraj) <= lag:\n            continue\n        if shift is None:\n            s = np.random.randint(min(lag, dtraj.size-lag))\n        else:\n            s = shift\n        if sliding:\n            if s > 0:\n                dtrajs_new.append(dtraj[0:lag+s])\n            for t0 in range(s, dtraj.size-lag, lag):\n                dtrajs_new.append(dtraj[t0:t0+2*lag])\n        else:\n            for t0 in range(s, dtraj.size-lag, lag):\n                dtrajs_new.append(dtraj[t0:t0+lag+1])\n    return dtrajs_new", "response": "Splits the discrete trajectories into approximately uncorrelated fragments at given lag or longer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cvsplit_dtrajs(dtrajs):\n    if len(dtrajs) == 1:\n        raise ValueError('Only have a single trajectory. Cannot be split into train and test set')\n    I0 = np.random.choice(len(dtrajs), int(len(dtrajs)/2), replace=False)\n    I1 = np.array(list(set(list(np.arange(len(dtrajs)))) - set(list(I0))))\n    dtrajs_train = [dtrajs[i] for i in I0]\n    dtrajs_test = [dtrajs[i] for i in I1]\n    return dtrajs_train, dtrajs_test", "response": "Splits the trajectories into training and test sets with approximately equal number of trajectories\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the random seed of numpy within the context", "response": "def numpy_random_seed(seed=42):\n    \"\"\" sets the random seed of numpy within the context.\n\n    Example\n    -------\n    >>> import numpy as np\n    >>> with numpy_random_seed(seed=0):\n    ...    np.random.randint(1000)\n    684\n    \"\"\"\n    old_state = np.random.get_state()\n    np.random.seed(seed)\n    try:\n        yield\n    finally:\n        np.random.set_state(old_state)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the random seed of Python within the context", "response": "def random_seed(seed=42):\n    \"\"\" sets the random seed of Python within the context.\n\n    Example\n    -------\n    >>> import random\n    >>> with random_seed(seed=0):\n    ...    random.randint(0, 1000) # doctest: +SKIP\n    864\n    \"\"\"\n    old_state = random.getstate()\n    random.seed(seed)\n    try:\n        yield\n    finally:\n        random.setstate(old_state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\napplying given PyEMMA config values temporarily within the given context", "response": "def settings(**kwargs):\n    \"\"\" apply given PyEMMA config values temporarily within the given context.\"\"\"\n    from pyemma import config\n\n    old_settings = {}\n    try:\n        # remember old setting, set new one. May raise ValueError, if invalid setting is given.\n        for k, v in kwargs.items():\n            old_settings[k] = getattr(config, k)\n            setattr(config, k, v)\n        yield\n    finally:\n        # restore old settings\n        for k, v in old_settings.items():\n            setattr(config, k, v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all metastable sets in PCCA.", "response": "def metastable_sets(self):\n        \"\"\"\n        Crisp clustering using PCCA. This is only recommended for visualization purposes. You *cannot* compute any\n        actual quantity of the coarse-grained kinetics without employing the fuzzy memberships!\n        Returns\n        -------\n        A list of length equal to metastable states. Each element is an array with microstate indexes contained in it\n        \"\"\"\n        res = []\n        assignment = self.metastable_assignment\n        for i in range(self.m):\n            res.append(np.where(assignment == i)[0])\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scatter_contour(\n        x, y, z, ncontours=50, colorbar=True, fig=None,\n        ax=None, cmap=None, outfile=None):\n    \"\"\"Contour plot on scattered data (x,y,z) and\n    plots the positions of the points (x,y) on top.\n\n    Parameters\n    ----------\n    x : ndarray(T)\n        x-coordinates\n    y : ndarray(T)\n        y-coordinates\n    z : ndarray(T)\n        z-coordinates\n    ncontours : int, optional, default=50\n        number of contour levels\n    fig : matplotlib Figure object, optional, default=None\n        the figure to plot into. When set to None the default\n        Figure object will be used\n    ax : matplotlib Axes object, optional, default=None\n        the axes to plot to. When set to None the default Axes\n        object will be used.\n    cmap : matplotlib colormap, optional, default=None\n        the color map to use. None will use pylab.cm.jet.\n    outfile : str, optional, default=None\n        output file to write the figure to. When not given,\n        the plot will be displayed\n\n    Returns\n    -------\n    ax : Axes object containing the plot\n\n    \"\"\"\n    _warn(\n        'scatter_contour is deprected; use plot_contour instead'\n        ' and manually add a scatter plot on top.',\n        DeprecationWarning)\n    ax = contour(\n        x, y, z, ncontours=ncontours, colorbar=colorbar,\n        fig=fig, ax=ax, cmap=cmap)\n    # scatter points\n    ax.scatter(x , y, marker='o', c='b', s=5)\n    # show or save\n    if outfile is not None:\n        ax.get_figure().savefig(outfile)\n    return ax", "response": "Plots a scatter plot on the data x y z and the positions of the points y z on top."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a two - dimensional histogram of the given set of samples.", "response": "def get_histogram(\n        xall, yall, nbins=100,\n        weights=None, avoid_zero_count=False):\n    \"\"\"Compute a two-dimensional histogram.\n\n    Parameters\n    ----------\n    xall : ndarray(T)\n        Sample x-coordinates.\n    yall : ndarray(T)\n        Sample y-coordinates.\n    nbins : int, optional, default=100\n        Number of histogram bins used in each dimension.\n    weights : ndarray(T), optional, default=None\n        Sample weights; by default all samples have the same weight.\n    avoid_zero_count : bool, optional, default=True\n        Avoid zero counts by lifting all histogram elements to the\n        minimum value before computing the free energy. If False,\n        zero histogram counts would yield infinity in the free energy.\n\n    Returns\n    -------\n    x : ndarray(nbins, nbins)\n        The bins' x-coordinates in meshgrid format.\n    y : ndarray(nbins, nbins)\n        The bins' y-coordinates in meshgrid format.\n    z : ndarray(nbins, nbins)\n        Histogram counts in meshgrid format.\n\n    \"\"\"\n    z, xedge, yedge = _np.histogram2d(\n        xall, yall, bins=nbins, weights=weights)\n    x = 0.5 * (xedge[:-1] + xedge[1:])\n    y = 0.5 * (yedge[:-1] + yedge[1:])\n    if avoid_zero_count:\n        z = _np.maximum(z, _np.min(z[z.nonzero()]))\n    return x, y, z.T"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_grid_data(xall, yall, zall, nbins=100, method='nearest'):\n    from scipy.interpolate import griddata\n    x, y = _np.meshgrid(\n        _np.linspace(xall.min(), xall.max(), nbins),\n        _np.linspace(yall.min(), yall.max(), nbins),\n        indexing='ij')\n    z = griddata(\n        _np.hstack([xall[:,None], yall[:,None]]),\n        zall, (x, y), method=method)\n    return x, y, z", "response": "Interpolate unstructured two - dimensional data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_free_energy(z, minener_zero=False):\n    pi = _to_density(z)\n    free_energy = _np.inf * _np.ones(shape=z.shape)\n    nonzero = pi.nonzero()\n    free_energy[nonzero] = -_np.log(pi[nonzero])\n    if minener_zero:\n        free_energy[nonzero] -= _np.min(free_energy[nonzero])\n    return free_energy", "response": "Compute free energies from histogram counts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _prune_kwargs(kwargs):\n    allowed_keys = [\n        'corner_mask', 'alpha', 'locator', 'extend', 'xunits',\n        'yunits', 'antialiased', 'nchunk', 'hatches', 'zorder']\n    ignored = [key for key in kwargs.keys() if key not in allowed_keys]\n    for key in ignored:\n        _warn(\n            '{}={} is not an allowed optional parameter and will'\n            ' be ignored'.format(key, kwargs[key]))\n        kwargs.pop(key, None)\n    return kwargs", "response": "Remove non - allowed keys from a kwargs dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_map(\n        x, y, z, ax=None, cmap=None,\n        ncontours=100, vmin=None, vmax=None, levels=None,\n        cbar=True, cax=None, cbar_label=None,\n        cbar_orientation='vertical', norm=None,\n        **kwargs):\n    \"\"\"Plot a two-dimensional map from data on a grid.\n\n    Parameters\n    ----------\n    x : ndarray(T)\n        Binned x-coordinates.\n    y : ndarray(T)\n        Binned y-coordinates.\n    z : ndarray(T)\n        Binned z-coordinates.\n    ax : matplotlib.Axes object, optional, default=None\n        The ax to plot to; if ax=None, a new ax (and fig) is created.\n    cmap : matplotlib colormap, optional, default=None\n        The color map to use.\n    ncontours : int, optional, default=100\n        Number of contour levels.\n    vmin : float, optional, default=None\n        Lowest z-value to be plotted.\n    vmax : float, optional, default=None\n        Highest z-value to be plotted.\n    levels : iterable of float, optional, default=None\n        Contour levels to plot.\n    cbar : boolean, optional, default=True\n        Plot a color bar.\n    cax : matplotlib.Axes object, optional, default=None\n        Plot the colorbar into a custom axes object instead of\n        stealing space from ax.\n    cbar_label : str, optional, default=None\n        Colorbar label string; use None to suppress it.\n    cbar_orientation : str, optional, default='vertical'\n        Colorbar orientation; choose 'vertical' or 'horizontal'.\n    norm : matplotlib norm, optional, default=None\n        Use a norm when coloring the contour plot.\n\n    Optional parameters for contourf (**kwargs)\n    -------------------------------------------\n    corner_mask : boolean, optional\n        Enable/disable corner masking, which only has an effect if\n        z is a masked array. If False, any quad touching a masked\n        point is masked out. If True, only the triangular corners\n        of quads nearest those points are always masked out, other\n        triangular corners comprising three unmasked points are\n        contoured as usual.\n        Defaults to rcParams['contour.corner_mask'], which\n        defaults to True.\n    alpha : float\n        The alpha blending value.\n    locator : [ None | ticker.Locator subclass ]\n        If locator is None, the default MaxNLocator is used. The\n        locator is used to determine the contour levels if they are\n        not given explicitly via the levels argument.\n    extend : [ \u2018neither\u2019 | \u2018both\u2019 | \u2018min\u2019 | \u2018max\u2019 ]\n        Unless this is \u2018neither\u2019, contour levels are automatically\n        added to one or both ends of the range so that all data are\n        included. These added ranges are then mapped to the special\n        colormap values which default to the ends of the\n        colormap range, but can be set via\n        matplotlib.colors.Colormap.set_under() and\n        matplotlib.colors.Colormap.set_over() methods.\n    xunits, yunits : [ None | registered units ]\n        Override axis units by specifying an instance of a\n        matplotlib.units.ConversionInterface.\n    antialiased : boolean, optional\n        Enable antialiasing, overriding the defaults. For filled\n        contours, the default is True. For line contours, it is\n        taken from rcParams[\u2018lines.antialiased\u2019].\n    nchunk : [ 0 | integer ]\n        If 0, no subdivision of the domain. Specify a positive\n        integer to divide the domain into subdomains of nchunk by\n        nchunk quads. Chunking reduces the maximum length of polygons\n        generated by the contouring algorithm which reduces the\n        rendering workload passed on to the backend and also requires\n        slightly less RAM. It can however introduce rendering\n        artifacts at chunk boundaries depending on the backend, the\n        antialiased flag and value of alpha.\n    hatches :\n        A list of cross hatch patterns to use on the filled areas.\n        If None, no hatching will be added to the contour. Hatching\n        is supported in the PostScript, PDF, SVG and Agg backends\n        only.\n    zorder : float\n        Set the zorder for the artist. Artists with lower zorder\n        values are drawn first.\n\n    Returns\n    -------\n    fig : matplotlib.Figure object\n        The figure in which the used ax resides.\n    ax : matplotlib.Axes object\n        The ax in which the map was plotted.\n    misc : dict\n        Contains a matplotlib.contour.QuadContourSet 'mappable' and,\n        if requested, a matplotlib.Colorbar object 'cbar'.\n\n    \"\"\"\n    import matplotlib.pyplot as _plt\n    if ax is None:\n        fig, ax = _plt.subplots()\n    else:\n        fig = ax.get_figure()\n    mappable = ax.contourf(\n        x, y, z, ncontours, norm=norm,\n        vmin=vmin, vmax=vmax, cmap=cmap,\n        levels=levels, **_prune_kwargs(kwargs))\n    misc = dict(mappable=mappable)\n    if cbar_orientation not in ('horizontal', 'vertical'):\n        raise ValueError(\n            'cbar_orientation must be \"horizontal\" or \"vertical\"')\n    if cbar:\n        if cax is None:\n            cbar_ = fig.colorbar(\n                mappable, ax=ax, orientation=cbar_orientation)\n        else:\n            cbar_ = fig.colorbar(\n                mappable, cax=cax, orientation=cbar_orientation)\n        if cbar_label is not None:\n            cbar_.set_label(cbar_label)\n        misc.update(cbar=cbar_)\n    return fig, ax, misc", "response": "Plots a two - dimensional contour map from data on a grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_density(\n        xall, yall, ax=None, cmap=None,\n        ncontours=100, vmin=None, vmax=None, levels=None,\n        cbar=True, cax=None, cbar_label='sample density',\n        cbar_orientation='vertical', logscale=False, nbins=100,\n        weights=None, avoid_zero_count=False, **kwargs):\n    \"\"\"Plot a two-dimensional density map using a histogram of\n    scattered data.\n\n    Parameters\n    ----------\n    xall : ndarray(T)\n        Sample x-coordinates.\n    yall : ndarray(T)\n        Sample y-coordinates.\n    ax : matplotlib.Axes object, optional, default=None\n        The ax to plot to; if ax=None, a new ax (and fig) is created.\n    cmap : matplotlib colormap, optional, default=None\n        The color map to use.\n    ncontours : int, optional, default=100\n        Number of contour levels.\n    vmin : float, optional, default=None\n        Lowest z-value to be plotted.\n    vmax : float, optional, default=None\n        Highest z-value to be plotted.\n    levels : iterable of float, optional, default=None\n        Contour levels to plot.\n    cbar : boolean, optional, default=True\n        Plot a color bar.\n    cax : matplotlib.Axes object, optional, default=None\n        Plot the colorbar into a custom axes object instead of\n        stealing space from ax.\n    cbar_label : str, optional, default='sample density'\n        Colorbar label string; use None to suppress it.\n    cbar_orientation : str, optional, default='vertical'\n        Colorbar orientation; choose 'vertical' or 'horizontal'.\n    logscale : boolean, optional, default=False\n        Plot the z-values in logscale.\n    nbins : int, optional, default=100\n        Number of histogram bins used in each dimension.\n    weights : ndarray(T), optional, default=None\n        Sample weights; by default all samples have the same weight.\n    avoid_zero_count : bool, optional, default=True\n        Avoid zero counts by lifting all histogram elements to the\n        minimum value before computing the free energy. If False,\n        zero histogram counts would yield infinity in the free energy.\n\n    Optional parameters for contourf (**kwargs)\n    -------------------------------------------\n    corner_mask : boolean, optional\n        Enable/disable corner masking, which only has an effect if\n        z is a masked array. If False, any quad touching a masked\n        point is masked out. If True, only the triangular corners\n        of quads nearest those points are always masked out, other\n        triangular corners comprising three unmasked points are\n        contoured as usual.\n        Defaults to rcParams['contour.corner_mask'], which\n        defaults to True.\n    alpha : float\n        The alpha blending value.\n    locator : [ None | ticker.Locator subclass ]\n        If locator is None, the default MaxNLocator is used. The\n        locator is used to determine the contour levels if they are\n        not given explicitly via the levels argument.\n    extend : [ \u2018neither\u2019 | \u2018both\u2019 | \u2018min\u2019 | \u2018max\u2019 ]\n        Unless this is \u2018neither\u2019, contour levels are automatically\n        added to one or both ends of the range so that all data are\n        included. These added ranges are then mapped to the special\n        colormap values which default to the ends of the\n        colormap range, but can be set via\n        matplotlib.colors.Colormap.set_under() and\n        matplotlib.colors.Colormap.set_over() methods.\n    xunits, yunits : [ None | registered units ]\n        Override axis units by specifying an instance of a\n        matplotlib.units.ConversionInterface.\n    antialiased : boolean, optional\n        Enable antialiasing, overriding the defaults. For filled\n        contours, the default is True. For line contours, it is\n        taken from rcParams[\u2018lines.antialiased\u2019].\n    nchunk : [ 0 | integer ]\n        If 0, no subdivision of the domain. Specify a positive\n        integer to divide the domain into subdomains of nchunk by\n        nchunk quads. Chunking reduces the maximum length of polygons\n        generated by the contouring algorithm which reduces the\n        rendering workload passed on to the backend and also requires\n        slightly less RAM. It can however introduce rendering\n        artifacts at chunk boundaries depending on the backend, the\n        antialiased flag and value of alpha.\n    hatches :\n        A list of cross hatch patterns to use on the filled areas.\n        If None, no hatching will be added to the contour. Hatching\n        is supported in the PostScript, PDF, SVG and Agg backends\n        only.\n    zorder : float\n        Set the zorder for the artist. Artists with lower zorder\n        values are drawn first.\n\n    Returns\n    -------\n    fig : matplotlib.Figure object\n        The figure in which the used ax resides.\n    ax : matplotlib.Axes object\n        The ax in which the map was plotted.\n    misc : dict\n        Contains a matplotlib.contour.QuadContourSet 'mappable' and,\n        if requested, a matplotlib.Colorbar object 'cbar'.\n\n    \"\"\"\n    x, y, z = get_histogram(\n        xall, yall, nbins=nbins, weights=weights,\n        avoid_zero_count=avoid_zero_count)\n    pi = _to_density(z)\n    pi = _np.ma.masked_where(pi <= 0, pi)\n    if logscale:\n        from matplotlib.colors import LogNorm\n        norm = LogNorm(vmin=vmin, vmax=vmax)\n        if levels is None:\n            levels = _np.logspace(\n                _np.floor(_np.log10(pi.min())),\n                _np.ceil(_np.log10(pi.max())),\n                ncontours + 1)\n    else:\n        norm = None\n    fig, ax, misc = plot_map(\n        x, y, pi, ax=ax, cmap=cmap,\n        ncontours=ncontours, vmin=vmin, vmax=vmax, levels=levels,\n        cbar=cbar, cax=cax, cbar_label=cbar_label,\n        cbar_orientation=cbar_orientation, norm=norm,\n        **kwargs)\n    if cbar and logscale:\n        from matplotlib.ticker import LogLocator\n        misc['cbar'].set_ticks(LogLocator(base=10.0, subs=range(10)))\n    return fig, ax, misc", "response": "Plot a two - dimensional density map of the given contour data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_free_energy(\n        xall, yall, weights=None, ax=None, nbins=100, ncontours=100,\n        offset=-1, avoid_zero_count=False, minener_zero=True, kT=1.0,\n        vmin=None, vmax=None, cmap='nipy_spectral', cbar=True,\n        cbar_label='free energy / kT', cax=None, levels=None,\n        legacy=True, ncountours=None, cbar_orientation='vertical',\n        **kwargs):\n    \"\"\"Plot a two-dimensional free energy map using a histogram of\n    scattered data.\n\n    Parameters\n    ----------\n    xall : ndarray(T)\n        Sample x-coordinates.\n    yall : ndarray(T)\n        Sample y-coordinates.\n    weights : ndarray(T), optional, default=None\n        Sample weights; by default all samples have the same weight.\n    ax : matplotlib.Axes object, optional, default=None\n        The ax to plot to; if ax=None, a new ax (and fig) is created.\n        Number of contour levels.\n    nbins : int, optional, default=100\n        Number of histogram bins used in each dimension.\n    ncontours : int, optional, default=100\n        Number of contour levels.\n    offset : float, optional, default=-1\n        Deprecated and ineffective; raises a ValueError\n        outside legacy mode.\n    avoid_zero_count : bool, optional, default=False\n        Avoid zero counts by lifting all histogram elements to the\n        minimum value before computing the free energy. If False,\n        zero histogram counts would yield infinity in the free energy.\n    minener_zero : boolean, optional, default=True\n        Shifts the energy minimum to zero.\n    kT : float, optional, default=1.0\n        The value of kT in the desired energy unit. By default,\n        energies are computed in kT (setting 1.0). If you want to\n        measure the energy in kJ/mol at 298 K, use kT=2.479 and\n        change the cbar_label accordingly.\n    vmin : float, optional, default=None\n        Lowest free energy value to be plotted.\n        (default=0.0 in legacy mode)\n    vmax : float, optional, default=None\n        Highest free energy value to be plotted.\n    cmap : matplotlib colormap, optional, default='nipy_spectral'\n        The color map to use.\n    cbar : boolean, optional, default=True\n        Plot a color bar.\n    cbar_label : str, optional, default='free energy / kT'\n        Colorbar label string; use None to suppress it.\n    cax : matplotlib.Axes object, optional, default=None\n        Plot the colorbar into a custom axes object instead of\n        stealing space from ax.\n    levels : iterable of float, optional, default=None\n        Contour levels to plot.\n    legacy : boolean, optional, default=True\n        Switch to use the function in legacy mode (deprecated).\n    ncountours : int, optional, default=None\n        Legacy parameter (typo) for number of contour levels.\n    cbar_orientation : str, optional, default='vertical'\n        Colorbar orientation; choose 'vertical' or 'horizontal'.\n\n    Optional parameters for contourf (**kwargs)\n    -------------------------------------------\n    corner_mask : boolean, optional\n        Enable/disable corner masking, which only has an effect if\n        z is a masked array. If False, any quad touching a masked\n        point is masked out. If True, only the triangular corners\n        of quads nearest those points are always masked out, other\n        triangular corners comprising three unmasked points are\n        contoured as usual.\n        Defaults to rcParams['contour.corner_mask'], which\n        defaults to True.\n    alpha : float\n        The alpha blending value.\n    locator : [ None | ticker.Locator subclass ]\n        If locator is None, the default MaxNLocator is used. The\n        locator is used to determine the contour levels if they are\n        not given explicitly via the levels argument.\n    extend : [ \u2018neither\u2019 | \u2018both\u2019 | \u2018min\u2019 | \u2018max\u2019 ]\n        Unless this is \u2018neither\u2019, contour levels are automatically\n        added to one or both ends of the range so that all data are\n        included. These added ranges are then mapped to the special\n        colormap values which default to the ends of the\n        colormap range, but can be set via\n        matplotlib.colors.Colormap.set_under() and\n        matplotlib.colors.Colormap.set_over() methods.\n    xunits, yunits : [ None | registered units ]\n        Override axis units by specifying an instance of a\n        matplotlib.units.ConversionInterface.\n    antialiased : boolean, optional\n        Enable antialiasing, overriding the defaults. For filled\n        contours, the default is True. For line contours, it is\n        taken from rcParams[\u2018lines.antialiased\u2019].\n    nchunk : [ 0 | integer ]\n        If 0, no subdivision of the domain. Specify a positive\n        integer to divide the domain into subdomains of nchunk by\n        nchunk quads. Chunking reduces the maximum length of polygons\n        generated by the contouring algorithm which reduces the\n        rendering workload passed on to the backend and also requires\n        slightly less RAM. It can however introduce rendering\n        artifacts at chunk boundaries depending on the backend, the\n        antialiased flag and value of alpha.\n    hatches :\n        A list of cross hatch patterns to use on the filled areas.\n        If None, no hatching will be added to the contour. Hatching\n        is supported in the PostScript, PDF, SVG and Agg backends\n        only.\n    zorder : float\n        Set the zorder for the artist. Artists with lower zorder\n        values are drawn first.\n\n    Returns\n    -------\n    fig : matplotlib.Figure object\n        The figure in which the used ax resides.\n    ax : matplotlib.Axes object\n        The ax in which the map was plotted.\n    misc : dict\n        Contains a matplotlib.contour.QuadContourSet 'mappable' and,\n        if requested, a matplotlib.Colorbar object 'cbar'.\n\n    \"\"\"\n    if legacy:\n        _warn(\n            'Legacy mode is deprecated is will be removed in the'\n            ' next major release. Until then use legacy=False',\n            DeprecationWarning)\n        cmap = _get_cmap(cmap)\n        if offset != -1:\n            _warn(\n                'Parameter offset is deprecated and will be ignored',\n                DeprecationWarning)\n        if ncountours is not None:\n            _warn(\n                'Parameter ncountours is deprecated;'\n                ' use ncontours instead',\n                DeprecationWarning)\n            ncontours = ncountours\n        if vmin is None:\n            vmin = 0.0\n    else:\n        if offset != -1:\n            raise ValueError(\n                'Parameter offset is not allowed outside legacy mode')\n        if ncountours is not None:\n            raise ValueError(\n                'Parameter ncountours is not allowed outside'\n                ' legacy mode; use ncontours instead')\n    x, y, z = get_histogram(\n        xall, yall, nbins=nbins, weights=weights,\n        avoid_zero_count=avoid_zero_count)\n    f = _to_free_energy(z, minener_zero=minener_zero) * kT\n    fig, ax, misc = plot_map(\n        x, y, f, ax=ax, cmap=cmap,\n        ncontours=ncontours, vmin=vmin, vmax=vmax, levels=levels,\n        cbar=cbar, cax=cax, cbar_label=cbar_label,\n        cbar_orientation=cbar_orientation, norm=None,\n        **kwargs)\n    if legacy:\n        return fig, ax\n    return fig, ax, misc", "response": "Plot a two - dimensional free energy map for the given contour levels."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a two - dimensional contour map by interpolating the data of the contour.", "response": "def plot_contour(\n        xall, yall, zall, ax=None, cmap=None,\n        ncontours=100, vmin=None, vmax=None, levels=None,\n        cbar=True, cax=None, cbar_label=None,\n        cbar_orientation='vertical', norm=None, nbins=100,\n        method='nearest', mask=False, **kwargs):\n    \"\"\"Plot a two-dimensional contour map by interpolating\n    scattered data on a grid.\n\n    Parameters\n    ----------\n    xall : ndarray(T)\n        Sample x-coordinates.\n    yall : ndarray(T)\n        Sample y-coordinates.\n    zall : ndarray(T)\n        Sample z-coordinates.\n    ax : matplotlib.Axes object, optional, default=None\n        The ax to plot to; if ax=None, a new ax (and fig) is created.\n    cmap : matplotlib colormap, optional, default=None\n        The color map to use.\n    ncontours : int, optional, default=100\n        Number of contour levels.\n    vmin : float, optional, default=None\n        Lowest z-value to be plotted.\n    vmax : float, optional, default=None\n        Highest z-value to be plotted.\n    levels : iterable of float, optional, default=None\n        Contour levels to plot; use legacy style calculation\n        if 'legacy'.\n    cbar : boolean, optional, default=True\n        Plot a color bar.\n    cax : matplotlib.Axes object, optional, default=None\n        Plot the colorbar into a custom axes object instead of\n        stealing space from ax.\n    cbar_label : str, optional, default=None\n        Colorbar label string; use None to suppress it.\n    cbar_orientation : str, optional, default='vertical'\n        Colorbar orientation; choose 'vertical' or 'horizontal'.\n    norm : matplotlib norm, optional, default=None\n        Use a norm when coloring the contour plot.\n    nbins : int, optional, default=100\n        Number of grid points used in each dimension.\n    method : str, optional, default='nearest'\n        Assignment method; scipy.interpolate.griddata supports the\n        methods 'nearest', 'linear', and 'cubic'.\n    mask : boolean, optional, default=False\n        Hide unsampled areas is True.\n\n    Optional parameters for contourf (**kwargs)\n    -------------------------------------------\n    corner_mask : boolean, optional\n        Enable/disable corner masking, which only has an effect if\n        z is a masked array. If False, any quad touching a masked\n        point is masked out. If True, only the triangular corners\n        of quads nearest those points are always masked out, other\n        triangular corners comprising three unmasked points are\n        contoured as usual.\n        Defaults to rcParams['contour.corner_mask'], which\n        defaults to True.\n    alpha : float\n        The alpha blending value.\n    locator : [ None | ticker.Locator subclass ]\n        If locator is None, the default MaxNLocator is used. The\n        locator is used to determine the contour levels if they are\n        not given explicitly via the levels argument.\n    extend : [ \u2018neither\u2019 | \u2018both\u2019 | \u2018min\u2019 | \u2018max\u2019 ]\n        Unless this is \u2018neither\u2019, contour levels are automatically\n        added to one or both ends of the range so that all data are\n        included. These added ranges are then mapped to the special\n        colormap values which default to the ends of the\n        colormap range, but can be set via\n        matplotlib.colors.Colormap.set_under() and\n        matplotlib.colors.Colormap.set_over() methods.\n    xunits, yunits : [ None | registered units ]\n        Override axis units by specifying an instance of a\n        matplotlib.units.ConversionInterface.\n    antialiased : boolean, optional\n        Enable antialiasing, overriding the defaults. For filled\n        contours, the default is True. For line contours, it is\n        taken from rcParams[\u2018lines.antialiased\u2019].\n    nchunk : [ 0 | integer ]\n        If 0, no subdivision of the domain. Specify a positive\n        integer to divide the domain into subdomains of nchunk by\n        nchunk quads. Chunking reduces the maximum length of polygons\n        generated by the contouring algorithm which reduces the\n        rendering workload passed on to the backend and also requires\n        slightly less RAM. It can however introduce rendering\n        artifacts at chunk boundaries depending on the backend, the\n        antialiased flag and value of alpha.\n    hatches :\n        A list of cross hatch patterns to use on the filled areas.\n        If None, no hatching will be added to the contour. Hatching\n        is supported in the PostScript, PDF, SVG and Agg backends\n        only.\n    zorder : float\n        Set the zorder for the artist. Artists with lower zorder\n        values are drawn first.\n\n    Returns\n    -------\n    fig : matplotlib.Figure object\n        The figure in which the used ax resides.\n    ax : matplotlib.Axes object\n        The ax in which the map was plotted.\n    misc : dict\n        Contains a matplotlib.contour.QuadContourSet 'mappable' and,\n        if requested, a matplotlib.Colorbar object 'cbar'.\n\n    \"\"\"\n    x, y, z = get_grid_data(\n        xall, yall, zall, nbins=nbins, method=method)\n    if vmin is None:\n        vmin = _np.min(zall[zall > -_np.inf])\n    if vmax is None:\n        vmax = _np.max(zall[zall < _np.inf])\n    if levels == 'legacy':\n        eps = (vmax - vmin) / float(ncontours)\n        levels = _np.linspace(vmin - eps, vmax + eps)\n    if mask:\n        _, _, counts = get_histogram(\n            xall, yall, nbins=nbins, weights=None,\n            avoid_zero_count=None)\n        z = _np.ma.masked_where(counts.T <= 0, z)\n    return plot_map(\n        x, y, z, ax=ax, cmap=cmap,\n        ncontours=ncontours, vmin=vmin, vmax=vmax, levels=levels,\n        cbar=cbar, cax=cax, cbar_label=cbar_label,\n        cbar_orientation=cbar_orientation, norm=norm,\n        **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots a two - dimensional contour map of states on a grid.", "response": "def plot_state_map(\n        xall, yall, states, ax=None, ncontours=100, cmap=None,\n        cbar=True, cax=None, cbar_label='state',\n        cbar_orientation='vertical', nbins=100, mask=True,\n        **kwargs):\n    \"\"\"Plot a two-dimensional contour map of states by interpolating\n    labels of scattered data on a grid.\n\n    Parameters\n    ----------\n    xall : ndarray(T)\n        Sample x-coordinates.\n    yall : ndarray(T)\n        Sample y-coordinates.\n    states : ndarray(T)\n        Sample state labels.\n    ax : matplotlib.Axes object, optional, default=None\n        The ax to plot to; if ax=None, a new ax (and fig) is created.\n    cmap : matplotlib colormap, optional, default=None\n        The color map to use.\n    ncontours : int, optional, default=100\n        Number of contour levels.\n    cbar : boolean, optional, default=True\n        Plot a color bar.\n    cax : matplotlib.Axes object, optional, default=None\n        Plot the colorbar into a custom axes object instead of\n        stealing space from ax.\n    cbar_label : str, optional, default='state'\n        Colorbar label string; use None to suppress it.\n    cbar_orientation : str, optional, default='vertical'\n        Colorbar orientation; choose 'vertical' or 'horizontal'.\n    nbins : int, optional, default=100\n        Number of grid points used in each dimension.\n    mask : boolean, optional, default=False\n        Hide unsampled areas is True.\n\n    Optional parameters for contourf (**kwargs)\n    -------------------------------------------\n    corner_mask : boolean, optional\n        Enable/disable corner masking, which only has an effect if\n        z is a masked array. If False, any quad touching a masked\n        point is masked out. If True, only the triangular corners\n        of quads nearest those points are always masked out, other\n        triangular corners comprising three unmasked points are\n        contoured as usual.\n        Defaults to rcParams['contour.corner_mask'], which\n        defaults to True.\n    alpha : float\n        The alpha blending value.\n    locator : [ None | ticker.Locator subclass ]\n        If locator is None, the default MaxNLocator is used. The\n        locator is used to determine the contour levels if they are\n        not given explicitly via the levels argument.\n    extend : [ \u2018neither\u2019 | \u2018both\u2019 | \u2018min\u2019 | \u2018max\u2019 ]\n        Unless this is \u2018neither\u2019, contour levels are automatically\n        added to one or both ends of the range so that all data are\n        included. These added ranges are then mapped to the special\n        colormap values which default to the ends of the\n        colormap range, but can be set via\n        matplotlib.colors.Colormap.set_under() and\n        matplotlib.colors.Colormap.set_over() methods.\n    xunits, yunits : [ None | registered units ]\n        Override axis units by specifying an instance of a\n        matplotlib.units.ConversionInterface.\n    antialiased : boolean, optional\n        Enable antialiasing, overriding the defaults. For filled\n        contours, the default is True. For line contours, it is\n        taken from rcParams[\u2018lines.antialiased\u2019].\n    nchunk : [ 0 | integer ]\n        If 0, no subdivision of the domain. Specify a positive\n        integer to divide the domain into subdomains of nchunk by\n        nchunk quads. Chunking reduces the maximum length of polygons\n        generated by the contouring algorithm which reduces the\n        rendering workload passed on to the backend and also requires\n        slightly less RAM. It can however introduce rendering\n        artifacts at chunk boundaries depending on the backend, the\n        antialiased flag and value of alpha.\n    hatches :\n        A list of cross hatch patterns to use on the filled areas.\n        If None, no hatching will be added to the contour. Hatching\n        is supported in the PostScript, PDF, SVG and Agg backends\n        only.\n    zorder : float\n        Set the zorder for the artist. Artists with lower zorder\n        values are drawn first.\n\n    Returns\n    -------\n    fig : matplotlib.Figure object\n        The figure in which the used ax resides.\n    ax : matplotlib.Axes object\n        The ax in which the map was plotted.\n    misc : dict\n        Contains a matplotlib.contour.QuadContourSet 'mappable' and,\n        if requested, a matplotlib.Colorbar object 'cbar'.\n\n    Notes\n    -----\n    Please note that this plot is an approximative visualization:\n    the underlying matplotlib.countourf function smoothes transitions\n    between different values and, thus, coloring at state boundaries\n    might be imprecise.\n\n    \"\"\"\n    from matplotlib.cm import get_cmap\n    nstates = int(_np.max(states) + 1)\n    cmap_ = get_cmap(cmap, nstates)\n    fig, ax, misc = plot_contour(\n        xall, yall, states, ax=ax, cmap=cmap_,\n        ncontours=ncontours, vmin=None, vmax=None, levels=None,\n        cbar=cbar, cax=cax, cbar_label=cbar_label,\n        cbar_orientation=cbar_orientation, norm=None, nbins=nbins,\n        method='nearest', mask=mask, **kwargs)\n    if cbar:\n        cmin, cmax = misc['mappable'].get_clim()\n        f = (cmax - cmin) / float(nstates)\n        n = _np.arange(nstates)\n        misc['cbar'].set_ticks((n + 0.5) * f)\n        misc['cbar'].set_ticklabels(n)\n    return fig, ax, misc"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the autocorrelation function of multiple trajectories.", "response": "def acf(trajs, stride=1, max_lag=None, subtract_mean=True, normalize=True, mean=None):\n    '''Computes the (combined) autocorrelation function of multiple trajectories.\n\n       Parameters\n       ----------\n       trajs : list of (*,N) ndarrays\n         the observable trajectories, N is the number of observables\n       stride : int (default = 1)\n         only take every n'th frame from trajs\n       max_lag : int (default = maximum trajectory length / stride)\n         only compute acf up to this lag time\n       subtract_mean : bool (default = True)\n         subtract trajectory mean before computing acfs\n       normalize : bool (default = True)\n         divide acf be the variance such that acf[0,:]==1\n       mean : (N) ndarray (optional)\n         if subtract_mean is True, you can give the trajectory mean\n         so this functions doesn't have to compute it again\n\n       Returns\n       -------\n       acf : (max_lag,N) ndarray\n           autocorrelation functions for all observables\n\n       Note\n       ----\n       The computation uses FFT (with zero-padding) and is done im memory (RAM).\n    '''\n    if not isinstance(trajs, list):\n        trajs = [trajs]\n\n    mytrajs = [None] * len(trajs)\n    for i in range(len(trajs)):\n        if trajs[i].ndim == 1:\n            mytrajs[i] = trajs[i].reshape((trajs[i].shape[0], 1))\n        elif trajs[i].ndim == 2:\n            mytrajs[i] = trajs[i]\n        else:\n            raise Exception(\n                'Unexpected number of dimensions in trajectory number %d' % i)\n    trajs = mytrajs\n\n    assert stride > 0, 'stride must be > 0'\n    assert max_lag is None or max_lag > 0, 'max_lag must be > 0'\n\n    if subtract_mean and mean is None:\n        # compute mean over all trajectories\n        mean = trajs[0].sum(axis=0)\n        n_samples = trajs[0].shape[0]\n        for i, traj in enumerate(trajs[1:]):\n            if traj.shape[1] != mean.shape[0]:\n                raise Exception(('number of order parameters in trajectory number %d differs' +\n                                 'from the number found in previous trajectories.') % (i + 1))\n            mean += traj.sum(axis=0)\n            n_samples += traj.shape[0]\n        mean /= n_samples\n\n    res = np.array([[]])\n    # number of samples for every tau\n    N = np.array([])\n\n    for i, traj in enumerate(trajs):\n        data = traj[::stride]\n        if subtract_mean:\n            data -= mean\n        # calc acfs\n        l = data.shape[0]\n        fft = np.fft.fft(data, n=2 ** int(np.ceil(np.log2(l * 2 - 1))), axis=0)\n        acftraj = np.fft.ifft(fft * np.conjugate(fft), axis=0).real\n        # throw away acf data for long lag times (and negative lag times)\n        if max_lag and max_lag < l:\n            acftraj = acftraj[:max_lag, :]\n        else:\n            acftraj = acftraj[:l, :]\n            if max_lag:\n                sys.stderr.write(\n                    'Warning: trajectory number %d is shorter than maximum lag.\\n' % i)\n        # find number of samples used for every lag\n        Ntraj = np.linspace(l, l - acftraj.shape[0] + 1, acftraj.shape[0])\n        # adapt shape of acf: resize temporal dimension, additionally set\n        # number of order parameters of acf in the first step\n        if res.shape[1] < acftraj.shape[1] and res.shape[1] > 0:\n            raise Exception(('number of order parameters in trajectory number %d differs ' +\n                             'from the number found in previous trajectories.') % i)\n        if res.shape[1] < acftraj.shape[1] or res.shape[0] < acftraj.shape[0]:\n            res = np.resize(res, acftraj.shape)\n            N = np.resize(N, acftraj.shape[0])\n        # update acf and number of samples\n        res[0:acftraj.shape[0], :] += acftraj\n        N[0:acftraj.shape[0]] += Ntraj\n\n    # divide by number of samples\n    res = np.transpose(np.transpose(res) / N)\n\n    # normalize acfs\n    if normalize:\n        res /= res[0, :].copy()\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_file_reader(input_files, topology, featurizer, chunksize=None, **kw):\n    from pyemma.coordinates.data.numpy_filereader import NumPyFileReader\n    from pyemma.coordinates.data.py_csv_reader import PyCSVReader\n    from pyemma.coordinates.data import FeatureReader\n    from pyemma.coordinates.data.fragmented_trajectory_reader import FragmentedTrajectoryReader\n\n    # fragmented trajectories\n    if (isinstance(input_files, (list, tuple)) and len(input_files) > 0 and\n            any(isinstance(item, (list, tuple)) for item in input_files)):\n        return FragmentedTrajectoryReader(input_files, topology, chunksize, featurizer)\n\n    # normal trajectories\n    if (isinstance(input_files, str)\n            or (isinstance(input_files, (list, tuple))\n                and (any(isinstance(item, str) for item in input_files)\n                     or len(input_files) is 0))):\n        reader = None\n        # check: if single string create a one-element list\n        if isinstance(input_files, str):\n            input_list = [input_files]\n        elif len(input_files) > 0 and all(isinstance(item, str) for item in input_files):\n            input_list = input_files\n        else:\n            if len(input_files) is 0:\n                raise ValueError(\"The passed input list should not be empty.\")\n            else:\n                raise ValueError(\"The passed list did not exclusively contain strings or was a list of lists \"\n                                 \"(fragmented trajectory).\")\n\n        # TODO: this does not handle suffixes like .xyz.gz (rare)\n        _, suffix = os.path.splitext(input_list[0])\n\n        suffix = str(suffix)\n\n        # check: do all files have the same file type? If not: raise ValueError.\n        if all(item.endswith(suffix) for item in input_list):\n\n            # do all the files exist? If not: Raise value error\n            all_exist = True\n            from six import StringIO\n            err_msg = StringIO()\n            for item in input_list:\n                if not os.path.isfile(item):\n                    err_msg.write('\\n' if err_msg.tell() > 0 else \"\")\n                    err_msg.write('File %s did not exist or was no file' % item)\n                    all_exist = False\n            if not all_exist:\n                raise ValueError('Some of the given input files were directories'\n                                 ' or did not exist:\\n%s' % err_msg.getvalue())\n            featurizer_or_top_provided = featurizer is not None or topology is not None\n            # we need to check for h5 first, because of mdtraj custom HDF5 traj format (which is deprecated).\n            if suffix in ('.h5', '.hdf5') and not featurizer_or_top_provided:\n                # This check is potentially expensive for lots of files, we also re-open the file twice (causing atime updates etc.)\n                # So we simply require that no featurizer option is given.\n                # and not all((_is_mdtraj_hdf5_file(f) for f in input_files)):\n                from pyemma.coordinates.data.h5_reader import H5Reader\n                reader = H5Reader(filenames=input_files, chunk_size=chunksize, **kw)\n            # CASE 1.1: file types are MD files\n            elif FeatureReader.supports_format(suffix):\n                # check: do we either have a featurizer or a topology file name? If not: raise ValueError.\n                # create a MD reader with file names and topology\n                if not featurizer_or_top_provided:\n                    raise ValueError('The input files were MD files which makes it mandatory to have either a '\n                                     'Featurizer or a topology file.')\n\n                reader = FeatureReader(input_list, featurizer=featurizer, topologyfile=topology,\n                                       chunksize=chunksize)\n            elif suffix in ('.npy', '.npz'):\n                reader = NumPyFileReader(input_list, chunksize=chunksize)\n            # otherwise we assume that given files are ascii tabulated data\n            else:\n                reader = PyCSVReader(input_list, chunksize=chunksize, **kw)\n        else:\n            raise ValueError('Not all elements in the input list were of the type %s!' % suffix)\n    else:\n        raise ValueError('Input \"{}\" was no string or list of strings.'.format(input_files))\n    return reader", "response": "r Creates a possibly featured file reader by a number of input files and a topology file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a single trajectory object from a list of files.", "response": "def single_traj_from_n_files(file_list, top):\n    \"\"\" Creates a single trajectory object from a list of files\n\n    \"\"\"\n    traj = None\n    for ff in file_list:\n        if traj is None:\n            traj = md.load(ff, top=top)\n        else:\n            traj = traj.join(md.load(ff, top=top))\n\n    return traj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_traj_attributes(target, origin, start):\n\n    # The list of copied attributes can be extended here with time\n    # Or perhaps ask the mdtraj guys to implement something similar?\n    stop = start+origin.n_frames\n\n    target.xyz[start:stop] = origin.xyz\n    target.unitcell_lengths[start:stop] = origin.unitcell_lengths\n    target.unitcell_angles[start:stop] = origin.unitcell_angles\n    target.time[start:stop] = origin.time\n\n    return target", "response": "Copy the attributes of origin into target"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_element(self, e):\n        if not isinstance(e, Iterable):\n            raise TypeError(\"given element {} is not iterable in terms of \"\n                            \"PyEMMAs coordinate pipeline.\".format(e))\n\n        # only if we have more than one element\n        if not e.is_reader and len(self._chain) >= 1:\n            data_producer = self._chain[-1]\n            # avoid calling the setter of StreamingTransformer.data_producer, since this\n            # triggers a re-parametrization even on readers (where it makes not sense)\n            e._data_producer = data_producer\n        e.chunksize = self.chunksize\n        self._chain.append(e)", "response": "r Appends the given element to the end of the current pipeline stage."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_element(self, index, e):\n        if index > len(self._chain):\n            raise IndexError(\"tried to access element %i, but chain has only %i\"\n                             \" elements\" % (index, len(self._chain)))\n\n        if type(index) is not int:\n            raise ValueError(\n                \"index is not a integer but '%s'\" % str(type(index)))\n        # if e is already in chain, we're finished\n        if self._chain[index] is e:\n            return\n\n        # remove current index and its data producer\n        replaced = self._chain.pop(index)\n        if not replaced.is_reader:\n            replaced.data_producer = None\n\n        self._chain.insert(index, e)\n\n        if index == 0:\n            e.data_producer = e\n        else:\n            # rewire data_producers\n            e.data_producer = self._chain[index - 1]\n\n        # if e has a successive element, need to set data_producer\n        try:\n            successor = self._chain[index + 1]\n            successor.data_producer = e\n        except IndexError:\n            pass\n\n        # set data_producer for predecessor of e\n        # self._chain[max(0, index - 1)].data_producer = self._chain[index]\n\n        # since data producer of element after insertion changed, reset its status\n        # TODO: make parameterized a property?\n        self._chain[index]._estimated = False\n\n        return replaced", "response": "r Replaces an element in the pipeline stage and return replaced element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parametrize(self):\n        for element in self._chain:\n            if not element.is_reader and not element._estimated:\n                element.estimate(element.data_producer, stride=self.param_stride, chunksize=self.chunksize)\n\n        self._estimated = True", "response": "r Reads all data and discretizes it into discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_estimated(self):\n        result = self._estimated\n        for el in self._chain:\n            if not el.is_reader:\n                result &= el._estimated\n        return result", "response": "r Returns True if all the elements in the pipeline are parametrized."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_dtrajs(self, prefix='', output_dir='.',\n                    output_format='ascii', extension='.dtraj'):\n        r\"\"\"Saves calculated discrete trajectories. Filenames are taken from\n        given reader. If data comes from memory dtrajs are written to a default\n        filename.\n\n\n        Parameters\n        ----------\n        prefix : str\n            prepend prefix to filenames.\n        output_dir : str (optional)\n            save files to this directory. Defaults to current working directory.\n        output_format : str\n            if format is 'ascii' dtrajs will be written as csv files, otherwise\n            they will be written as NumPy .npy files.\n        extension : str\n            file extension to append (eg. '.itraj')\n\n        \"\"\"\n\n        clustering = self._chain[-1]\n        reader = self._chain[0]\n        from pyemma.coordinates.clustering.interface import AbstractClustering\n        assert isinstance(clustering, AbstractClustering)\n\n        trajfiles = None\n        if isinstance(reader, FeatureReader):\n            trajfiles = reader.filenames\n\n        clustering.save_dtrajs(\n            trajfiles, prefix, output_dir, output_format, extension)", "response": "r Saves calculated discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply(modifications, state):\n        count = 0\n        for a in modifications:\n            if _debug:\n                assert a[0] in ('set', 'mv', 'map', 'rm')\n                logger.debug(\"processing rule: %s\", str(a))\n            if len(a) == 3:\n                operation, name, value = a\n                if operation == 'set':\n                    state[name] = value\n                    count += 1\n                elif operation == 'mv':\n                    try:\n                        arg = state.pop(name)\n                        state[value] = arg\n                        count += 1\n                    except KeyError:\n                        raise ClassVersionException(\"the previous version didn't \"\n                                                    \"store an attribute named '{}'\".format(a[1]))\n                elif operation == 'map':\n                    func = value\n                    if hasattr(func, '__func__'):\n                        func = func.__func__\n                    assert callable(func)\n                    state[name] = func(state[name])\n                    count += 1\n            elif len(a) == 2:\n                operation, value = a\n                if operation == 'rm':\n                    state.pop(value, None)\n                    count += 1\n                elif operation == 'transform':\n                    assert callable(value)\n                    value(state)\n                    count += 1\n\n        assert count == len(modifications), 'was not able to process all modifications on state'", "response": "Applies modifications to given state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading a previously saved PyEMMA object from disk.", "response": "def load(cls, file_name, model_name='default'):\n        \"\"\" Loads a previously saved PyEMMA object from disk.\n\n        Parameters\n        ----------\n        file_name : str or file like object (has to provide read method).\n            The file like object tried to be read for a serialized object.\n        model_name: str, default='default'\n            if multiple models are contained in the file, these can be accessed by\n            their name. Use :func:`pyemma.list_models` to get a representation of all stored models.\n\n        Returns\n        -------\n        obj : the de-serialized object\n        \"\"\"\n        from .h5file import H5File\n        with H5File(file_name, model_name=model_name, mode='r') as f:\n            return f.model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of the state of the serialized fields of the class.", "response": "def _get_state_of_serializeable_fields(self, klass, state):\n        \"\"\" :return a dictionary {k:v} for k in self.serialize_fields and v=getattr(self, k)\"\"\"\n        for field in SerializableMixIn._get_serialize_fields(klass):\n            # only try to get fields, we actually have.\n            if hasattr(self, field):\n                if _debug and field in state:\n                    logger.debug('field \"%s\" already in state!', field)\n                state[field] = getattr(self, field)\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the version of the current klass from the state mapping from old locations to new locations.", "response": "def _get_version_for_class_from_state(state, klass):\n        \"\"\" retrieves the version of the current klass from the state mapping from old locations to new ones. \"\"\"\n        # klass may have been renamed, so we have to look this up in the class rename registry.\n        names = [_importable_name(klass)]\n        # lookup old names, handled by current klass.\n        from .util import class_rename_registry\n        names.extend(class_rename_registry.old_handled_by(klass))\n        for n in names:\n            try:\n                return state['class_tree_versions'][n]\n            except KeyError:\n                continue\n        # if we did not find a suitable version number return infinity.\n        if _debug:\n            logger.debug('unable to obtain a __serialize_version for class %s', klass)\n        return float('inf')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset only fields from state which are present in klass. __serialize_fields", "response": "def _set_state_from_serializeable_fields_and_state(self, state, klass):\n        \"\"\" set only fields from state, which are present in klass.__serialize_fields \"\"\"\n        if _debug:\n            logger.debug(\"restoring state for class %s\", klass)\n\n        for field in SerializableMixIn._get_serialize_fields(klass):\n            if field in state:\n                # ensure we can set attributes. Log culprits.\n                try:\n                    setattr(self, field, state.get(field))\n                except AttributeError:\n                    logger.debug('field: %s', field, exc_info=True)\n            else:\n                if _debug:\n                    logger.debug(\"skipped %s, because it is not contained in state\", field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_classes_to_inspect(self):\n        return tuple(filter(lambda c:\n                      SerializableMixIn._get_version(c, require=False) or\n                      (SerializableMixIn._get_serialize_fields(c) or\n                       SerializableMixIn._get_interpolation_map(c)),\n                      self.__class__.__mro__))", "response": "get classes to inspect from which\n         is a tuple of tuples"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef partial_fit(self, X):\n        from pyemma.coordinates import source\n\n        self._estimate(source(X), partial_fit=True)\n        self._estimated = True\n\n        return self", "response": "incrementally update the estimates with the source data X"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef C0t_(self):\n        self._check_estimated()\n        return self._rc.cov_XY(bessel=self.bessel)", "response": "Time - lagged covariance matrix"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Ctt_(self):\n        self._check_estimated()\n        return self._rc.cov_YY(bessel=self.bessel)", "response": "Covariance matrix of the time shifted data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dtram(\n    ttrajs, dtrajs, bias, lag, unbiased_state=None,\n    count_mode='sliding', connectivity='reversible_pathways',\n    maxiter=10000, maxerr=1.0E-15, save_convergence_info=0, dt_traj='1 step',\n    init=None, init_maxiter=10000, init_maxerr=1.0E-8):\n    r\"\"\"\n    Discrete transition-based reweighting analysis method\n\n    Parameters\n    ----------\n    ttrajs : numpy.ndarray(T) of int, or list of numpy.ndarray(T_i) of int\n        A single discrete trajectory or a list of discrete trajectories. The integers are\n        indexes in 0,...,num_therm_states-1 enumerating the thermodynamic states the trajectory is\n        in at any time.\n    dtrajs : numpy.ndarray(T) of int, or list of numpy.ndarray(T_i) of int\n        A single discrete trajectory or a list of discrete trajectories. The integers are indexes\n        in 0,...,num_conf_states-1 enumerating the num_conf_states Markov states or the bins the\n        trajectory is in at any time.\n    bias : numpy.ndarray(shape=(num_therm_states, num_conf_states)) object\n        bias_energies_full[j, i] is the bias energy in units of kT for each discrete state i\n        at thermodynamic state j.\n    lag : int or list of int, optional, default=1\n        Integer lag time at which transitions are counted. Providing a list of lag times will\n        trigger one estimation per lag time.\n    unbiased_state : int, optional, default=None\n        Index of the unbiased thermodynamic state or None if there is no unbiased data available.\n    count_mode : str, optional, default='sliding'\n        Mode to obtain count matrices from discrete trajectories. Should be one of:\n\n        * 'sliding' : a trajectory of length T will have :math:`T-\\tau` counts at time indexes\n            .. math::\n                 (0 \\rightarrow \\tau), (1 \\rightarrow \\tau+1), ..., (T-\\tau-1 \\rightarrow T-1)\n        * 'sample' : a trajectory of length T will have :math:`T/\\tau` counts at time indexes\n            .. math::\n                    (0 \\rightarrow \\tau), (\\tau \\rightarrow 2 \\tau), ..., ((T/\\tau-1) \\tau \\rightarrow T)\n\n        Currently only 'sliding' is supported.\n    connectivity : str, optional, default='reversible_pathways'\n        One of 'reversible_pathways', 'summed_count_matrix' or None.\n        Defines what should be considered a connected set in the joint (product)\n        space of conformations and thermodynamic ensembles.\n        * 'reversible_pathways' : requires that every state in the connected set\n          can be reached by following a pathway of reversible transitions. A\n          reversible transition between two Markov states (within the same\n          thermodynamic state k) is a pair of Markov states that belong to the\n          same strongly connected component of the count matrix (from\n          thermodynamic state k). A pathway of reversible transitions is a list of\n          reversible transitions [(i_1, i_2), (i_2, i_3),..., (i_(N-2), i_(N-1)),\n          (i_(N-1), i_N)]. The thermodynamic state where the reversible\n          transitions happen, is ignored in constructing the reversible pathways.\n          This is equivalent to assuming that two ensembles overlap at some Markov\n          state whenever there exist frames from both ensembles in that Markov\n          state.\n        * 'summed_count_matrix' : all thermodynamic states are assumed to overlap.\n          The connected set is then computed by summing the count matrices over\n          all thermodynamic states and taking it's largest strongly connected set.\n          Not recommended!\n        * None : assume that everything is connected. For debugging.\n        For more details see :func:`pyemma.thermo.extensions.cset.compute_csets_dTRAM`.\n    maxiter : int, optional, default=10000\n        The maximum number of dTRAM iterations before the estimator exits unsuccessfully.\n    maxerr : float, optional, default=1e-15\n        Convergence criterion based on the maximal free energy change in a self-consistent\n        iteration step.\n    save_convergence_info : int, optional, default=0\n        Every save_convergence_info iteration steps, store the actual increment\n        and the actual loglikelihood; 0 means no storage.\n    dt_traj : str, optional, default='1 step'\n        Description of the physical time corresponding to the lag. May be used by analysis\n        algorithms such as plotting tools to pretty-print the axes. By default '1 step', i.e.\n        there is no physical time unit.  Specify by a number, whitespace and unit. Permitted\n        units are (* is an arbitrary string):\n\n        |  'fs',   'femtosecond*'\n        |  'ps',   'picosecond*'\n        |  'ns',   'nanosecond*'\n        |  'us',   'microsecond*'\n        |  'ms',   'millisecond*'\n        |  's',    'second*'\n\n    init : str, optional, default=None\n        Use a specific initialization for self-consistent iteration:\n\n        | None:    use a hard-coded guess for free energies and Lagrangian multipliers\n        | 'wham':  perform a short WHAM estimate to initialize the free energies\n\n    init_maxiter : int, optional, default=10000\n        The maximum number of self-consistent iterations during the initialization.\n    init_maxerr : float, optional, default=1.0E-8\n        Convergence criterion for the initialization.\n\n    Returns\n    -------\n    A :class:`MEMM <pyemma.thermo.models.memm.MEMM>` object or list thereof\n        A multi-ensemble Markov state model (for each given lag time) which consists of stationary\n        and kinetic quantities at all temperatures/thermodynamic states.\n\n    Example\n    -------\n    **Umbrella sampling**: Suppose we simulate in K umbrellas, centered at\n    positions :math:`y_0,...,y_{K-1}` with bias energies\n\n    .. math::\n        b_k(x) = \\frac{c_k}{2 \\textrm{kT}} \\cdot (x - y_k)^2\n\n    Suppose we have one simulation of length T in each umbrella, and they are ordered from 0 to K-1.\n    We have discretized the x-coordinate into 100 bins.\n    Then dtrajs and ttrajs should each be a list of :math:`K` arrays.\n    dtrajs would look for example like this::\n\n    [ (0, 0, 0, 0, 1, 1, 1, 0, 0, 0, ...),  (0, 1, 0, 1, 0, 1, 1, 0, 0, 1, ...), ... ]\n\n    where each array has length T, and is the sequence of bins (in the range 0 to 99) visited along\n    the trajectory. ttrajs would look like this::\n\n    [ (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...),  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...), ... ]\n\n    Because trajectory 1 stays in umbrella 1 (index 0), trajectory 2 stays in umbrella 2 (index 1),\n    and so forth. bias is a :math:`K \\times n` matrix with all reduced bias energies evaluated at\n    all centers:\n\n    .. math::\n        \\left(\\begin{array}{cccc}\n            b_0(y_0) &  b_0(y_1) &  ... &  b_0(y_{n-1}) \\\\\n            b_1(y_0) &  b_1(y_1) &  ... &  b_1(y_{n-1}) \\\\\n            ... \\\\\n            b_{K-1}(y_0) &  b_{K-1}(y_1) &  ... &  b_{K-1}(y_{n-1})\n        \\end{array}\\right)\n\n    Let us try the above example:\n\n    >>> from pyemma.thermo import dtram\n    >>> import numpy as np\n    >>> ttrajs = [np.array([0,0,0,0,0,0,0,0,0,0]), np.array([1,1,1,1,1,1,1,1,1,1])]\n    >>> dtrajs = [np.array([0,0,0,0,1,1,1,0,0,0]), np.array([0,1,0,1,0,1,1,0,0,1])]\n    >>> bias = np.array([[0.0, 0.0], [0.5, 1.0]])\n    >>> dtram_obj = dtram(ttrajs, dtrajs, bias, 1)\n    >>> dtram_obj.log_likelihood() # doctest: +ELLIPSIS\n    -9.805...\n    >>> dtram_obj.count_matrices # doctest: +SKIP\n    array([[[5, 1],\n            [1, 2]],\n           [[1, 4],\n            [3, 1]]], dtype=int32)\n    >>> dtram_obj.stationary_distribution # doctest: +ELLIPSIS\n    array([ 0.38...,  0.61...])\n\n    See :class:`MEMM <pyemma.thermo.models.memm.MEMM>` for a full documentation.\n\n    .. autoclass:: pyemma.thermo.models.memm.MEMM\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.thermo.models.memm.MEMM\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.thermo.models.memm.MEMM\n            :attributes:\n\n    References\n    ----------\n\n    .. [1] Wu, H. et al 2014\n        Statistically optimal analysis of state-discretized trajectory data from multiple thermodynamic states\n        J. Chem. Phys. 141, 214106\n\n    \"\"\"\n    # prepare trajectories\n    ttrajs = _types.ensure_dtraj_list(ttrajs)\n    dtrajs = _types.ensure_dtraj_list(dtrajs)\n    if len(ttrajs) != len(dtrajs):\n        raise ValueError(\"Unmatching number of dtraj/ttraj elements: %d!=%d\" % (\n            len(dtrajs), len(ttrajs)) )\n    for ttraj, dtraj in zip(ttrajs, dtrajs):\n        if len(ttraj) != len(dtraj):\n            raise ValueError(\"Unmatching number of data points in ttraj/dtraj: %d!=%d\" % (\n                len(ttraj), len(dtraj)))\n    # check lag time(s)\n    lags = _np.asarray(lag, dtype=_np.intc).reshape((-1,)).tolist()\n    # build DTRAM and run estimation\n    from pyemma.thermo import DTRAM\n    from pyemma._base.progress import ProgressReporter\n    pg = ProgressReporter()\n    pg.register(len(lags), description='Estimating DTRAM for lags')\n    dtram_estimators = []\n    with pg.context():\n        for _lag in lags:\n            d = DTRAM(\n                bias, _lag,\n                count_mode=count_mode, connectivity=connectivity,\n                maxiter=maxiter, maxerr=maxerr, save_convergence_info=save_convergence_info,\n                dt_traj=dt_traj, init=init, init_maxiter=init_maxiter,\n                init_maxerr=init_maxerr).estimate((ttrajs, dtrajs))\n            dtram_estimators.append(d)\n            pg.update(1)\n    _assign_unbiased_state_label(dtram_estimators, unbiased_state)\n    # return\n    if len(dtram_estimators) == 1:\n        return dtram_estimators[0]\n    return dtram_estimators", "response": "r Returns a new object containing the count matrices for the given set of discrete trajectories."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wham(\n    ttrajs, dtrajs, bias,\n    maxiter=100000, maxerr=1.0E-15, save_convergence_info=0, dt_traj='1 step'):\n    r\"\"\"\n    Weighted histogram analysis method\n\n    Parameters\n    ----------\n    ttrajs : numpy.ndarray(T) of int, or list of numpy.ndarray(T_i) of int\n        A single discrete trajectory or a list of discrete trajectories. The integers are\n        indexes in 0,...,num_therm_states-1 enumerating the thermodynamic states the trajectory is\n        in at any time.\n    dtrajs : numpy.ndarray(T) of int, or list of numpy.ndarray(T_i) of int\n        A single discrete trajectory or a list of discrete trajectories. The integers are indexes\n        in 0,...,num_conf_states-1 enumerating the num_conf_states Markov states or the bins the\n        trajectory is in at any time.\n    bias : numpy.ndarray(shape=(num_therm_states, num_conf_states)) object\n        bias_energies_full[j, i] is the bias energy in units of kT for each discrete state i\n        at thermodynamic state j.\n    maxiter : int, optional, default=10000\n        The maximum number of dTRAM iterations before the estimator exits unsuccessfully.\n    maxerr : float, optional, default=1e-15\n        Convergence criterion based on the maximal free energy change in a self-consistent\n        iteration step.\n    save_convergence_info : int, optional, default=0\n        Every save_convergence_info iteration steps, store the actual increment\n        and the actual loglikelihood; 0 means no storage.\n    dt_traj : str, optional, default='1 step'\n        Description of the physical time corresponding to the lag. May be used by analysis\n        algorithms such as plotting tools to pretty-print the axes. By default '1 step', i.e.\n        there is no physical time unit.  Specify by a number, whitespace and unit. Permitted\n        units are (* is an arbitrary string):\n\n        |  'fs',   'femtosecond*'\n        |  'ps',   'picosecond*'\n        |  'ns',   'nanosecond*'\n        |  'us',   'microsecond*'\n        |  'ms',   'millisecond*'\n        |  's',    'second*'\n\n    Returns\n    -------\n    A :class:`MultiThermModel <pyemma.thermo.models.multi_therm.MultiThermModel>` object\n        A stationary model which consists of thermodynamic quantities at all\n        temperatures/thermodynamic states.\n\n    Example\n    -------\n    **Umbrella sampling**: Suppose we simulate in K umbrellas, centered at\n    positions :math:`y_0,...,y_{K-1}` with bias energies\n\n    .. math::\n        b_k(x) = \\frac{c_k}{2 \\textrm{kT}} \\cdot (x - y_k)^2\n\n    Suppose we have one simulation of length T in each umbrella, and they are ordered from 0 to K-1.\n    We have discretized the x-coordinate into 100 bins.\n    Then dtrajs and ttrajs should each be a list of :math:`K` arrays.\n    dtrajs would look for example like this::\n\n    [ (0, 0, 0, 0, 1, 1, 1, 0, 0, 0, ...),  (0, 1, 0, 1, 0, 1, 1, 0, 0, 1, ...), ... ]\n\n    where each array has length T, and is the sequence of bins (in the range 0 to 99) visited along\n    the trajectory. ttrajs would look like this::\n\n    [ (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...),  (1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...), ... ]\n\n    Because trajectory 1 stays in umbrella 1 (index 0), trajectory 2 stays in umbrella 2 (index 1),\n    and so forth. bias is a :math:`K \\times n` matrix with all reduced bias energies evaluated at\n    all centers:\n\n    .. math::\n        \\left(\\begin{array}{cccc}\n            b_0(y_0) &  b_0(y_1) &  ... &  b_0(y_{n-1}) \\\\\n            b_1(y_0) &  b_1(y_1) &  ... &  b_1(y_{n-1}) \\\\\n            ... \\\\\n            b_{K-1}(y_0) &  b_{K-1}(y_1) &  ... &  b_{K-1}(y_{n-1})\n        \\end{array}\\right)\n\n    Let us try the above example:\n\n    >>> from pyemma.thermo import wham\n    >>> import numpy as np\n    >>> ttrajs = [np.array([0,0,0,0,0,0,0,0,0,0]), np.array([1,1,1,1,1,1,1,1,1,1])]\n    >>> dtrajs = [np.array([0,0,0,0,1,1,1,0,0,0]), np.array([0,1,0,1,0,1,1,0,0,1])]\n    >>> bias = np.array([[0.0, 0.0], [0.5, 1.0]])\n    >>> wham_obj = wham(ttrajs, dtrajs, bias)\n    >>> wham_obj.log_likelihood() # doctest: +ELLIPSIS\n    -6.6...\n    >>> wham_obj.state_counts # doctest: +SKIP\n    array([[7, 3],\n           [5, 5]])\n    >>> wham_obj.stationary_distribution # doctest: +ELLIPSIS +REPORT_NDIFF\n    array([ 0.5...,  0.4...])\n\n    See :class:`MultiThermModel <pyemma.thermo.models.multi_therm.MultiThermModel>` for a full\n    documentation.\n\n\n    .. autoclass:: pyemma.thermo.models.multi_therm.MultiThermModel\n        :members:\n        :undoc-members:\n\n        .. rubric:: Methods\n\n        .. autoautosummary:: pyemma.thermo.models.multi_therm.MultiThermModel\n           :methods:\n\n        .. rubric:: Attributes\n\n        .. autoautosummary:: pyemma.thermo.models.multi_therm.MultiThermModel\n            :attributes:\n\n    References\n    ----------\n\n    .. [1] Ferrenberg, A.M. and Swensen, R.H. 1988.\n        New Monte Carlo Technique for Studying Phase Transitions.\n        Phys. Rev. Lett. 23, 2635--2638\n\n    .. [2] Kumar, S. et al 1992.\n        The Weighted Histogram Analysis Method for Free-Energy Calculations on Biomolecules. I. The Method.\n        J. Comp. Chem. 13, 1011--1021\n\n    \"\"\"\n    # check trajectories\n    ttrajs = _types.ensure_dtraj_list(ttrajs)\n    dtrajs = _types.ensure_dtraj_list(dtrajs)\n    if len(ttrajs) != len(dtrajs):\n        raise ValueError(\"Unmatching number of dtraj/ttraj elements: %d!=%d\" % (\n            len(dtrajs), len(ttrajs)) )\n    for ttraj, dtraj in zip(ttrajs, dtrajs):\n        if len(ttraj) != len(dtraj):\n            raise ValueError(\"Unmatching number of data points in ttraj/dtraj: %d!=%d\" % (\n                len(ttraj), len(dtraj)))\n    # build WHAM\n    from pyemma.thermo import WHAM\n    wham_estimator = WHAM(\n        bias,\n        maxiter=maxiter, maxerr=maxerr,\n        save_convergence_info=save_convergence_info, dt_traj=dt_traj)\n    # run estimation\n    return wham_estimator.estimate((ttrajs, dtrajs))", "response": "r Weighted histogram analysis method for the given set of ttrajs and dtrajs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef default_chunksize(self):\n        if self._default_chunksize is None:\n            try:\n                # TODO: if dimension is not yet fixed (eg tica var cutoff, use dim of data_producer.\n                self.dimension()\n                self.output_type()\n            except:\n                self._default_chunksize = Iterable._FALLBACK_CHUNKSIZE\n            else:\n                self._default_chunksize = Iterable._compute_default_cs(self.dimension(),\n                                                                       self.output_type().itemsize, self.logger)\n        return self._default_chunksize", "response": "How much data will be processed at once in case no chunksize has been provided."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an iterator over the data.", "response": "def iterator(self, stride=1, lag=0, chunk=None, return_trajindex=True, cols=None, skip=0):\n        \"\"\" creates an iterator to stream over the (transformed) data.\n\n        If your data is too large to fit into memory and you want to incrementally compute\n        some quantities on it, you can create an iterator on a reader or transformer (eg. TICA)\n        to avoid memory overflows.\n\n        Parameters\n        ----------\n\n        stride : int, default=1\n            Take only every stride'th frame.\n        lag: int, default=0\n            how many frame to omit for each file.\n        chunk: int, default=None\n            How many frames to process at once. If not given obtain the chunk size\n            from the source.\n        return_trajindex: boolean, default=True\n            a chunk of data if return_trajindex is False, otherwise a tuple of (trajindex, data).\n        cols: array like, default=None\n            return only the given columns.\n        skip: int, default=0\n            skip 'n' first frames of each trajectory.\n\n        Returns\n        -------\n        iter : instance of DataSourceIterator\n            a implementation of a DataSourceIterator to stream over the data\n\n        Examples\n        --------\n\n        >>> from pyemma.coordinates import source; import numpy as np\n        >>> data = [np.arange(3), np.arange(4, 7)]\n        >>> reader = source(data)\n        >>> iterator = reader.iterator(chunk=1)\n        >>> for array_index, chunk in iterator:\n        ...     print(array_index, chunk)\n        0 [[0]]\n        0 [[1]]\n        0 [[2]]\n        1 [[4]]\n        1 [[5]]\n        1 [[6]]\n        \"\"\"\n        if self.in_memory:\n            from pyemma.coordinates.data.data_in_memory import DataInMemory\n            return DataInMemory(self._Y).iterator(\n                lag=lag, chunk=chunk, stride=stride, return_trajindex=return_trajindex, skip=skip\n            )\n        chunk = chunk if chunk is not None else self.chunksize\n        if lag > 0:\n            if chunk == 0 or lag <= chunk:\n                it = self._create_iterator(skip=skip, chunk=chunk, stride=1,\n                                           return_trajindex=return_trajindex, cols=cols)\n                it.return_traj_index = True\n                return _LaggedIterator(it, lag, return_trajindex, stride)\n            else:\n                it = self._create_iterator(skip=skip, chunk=chunk, stride=stride,\n                                           return_trajindex=return_trajindex, cols=cols)\n                it.return_traj_index = True\n                it_lagged = self._create_iterator(skip=skip + lag, chunk=chunk, stride=stride,\n                                                  return_trajindex=True, cols=cols)\n                return _LegacyLaggedIterator(it, it_lagged, return_trajindex)\n        return self._create_iterator(skip=skip, chunk=chunk, stride=stride,\n                                     return_trajindex=return_trajindex, cols=cols)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, X, y, lengths):\n\n        alpha = self.alpha\n        if alpha <= 0:\n            raise ValueError(\"alpha should be >0, got {0!r}\".format(alpha))\n\n        X = atleast2d_or_csr(X)\n        classes, y = np.unique(y, return_inverse=True)\n        lengths = np.asarray(lengths)\n        Y = y.reshape(-1, 1) == np.arange(len(classes))\n\n        end = np.cumsum(lengths)\n        start = end - lengths\n\n        init_prob = np.log(Y[start].sum(axis=0) + alpha)\n        init_prob -= logsumexp(init_prob)\n        final_prob = np.log(Y[start].sum(axis=0) + alpha)\n        final_prob -= logsumexp(final_prob)\n\n        feature_prob = np.log(safe_sparse_dot(Y.T, X) + alpha)\n        feature_prob -= logsumexp(feature_prob, axis=0)\n\n        trans_prob = np.log(count_trans(y, len(classes)) + alpha)\n        trans_prob -= logsumexp(trans_prob, axis=0)\n\n        self.coef_ = feature_prob\n        self.intercept_init_ = init_prob\n        self.intercept_final_ = final_prob\n        self.intercept_trans_ = trans_prob\n\n        self.classes_ = classes\n\n        return self", "response": "Fit Multinomial HMM model to data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lstree(files, dirs):\n    for f, sha1 in files:\n        yield \"100644 blob {}\\t{}\\0\".format(sha1, f)\n\n    for d, sha1 in dirs:\n        yield \"040000 tree {}\\t{}\\0\".format(sha1, d)", "response": "Make git ls - tree like output."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hash_dir(path):\n    dir_hash = {}\n\n    for root, dirs, files in os.walk(path, topdown=False):\n        f_hash = ((f, hash_file(join(root, f))) for f in files)\n        d_hash = ((d, dir_hash[join(root, d)]) for d in dirs)\n        # split+join normalizes paths on Windows (note the imports)\n        dir_hash[join(*split(root))] = _mktree(f_hash, d_hash)\n\n    return dir_hash[path]", "response": "Write directory at path to Git index return its SHA1 as a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding the features for i th token in sentence.", "response": "def features(sentence, i):\n    \"\"\"Features for i'th token in sentence.\n\n    Currently baseline named-entity recognition features, but these can\n    easily be changed to do POS tagging or chunking.\n    \"\"\"\n\n    word = sentence[i]\n\n    yield \"word:{}\" + word.lower()\n\n    if word[0].isupper():\n        yield \"CAP\"\n\n    if i > 0:\n        yield \"word-1:{}\" + sentence[i - 1].lower()\n        if i > 1:\n            yield \"word-2:{}\" + sentence[i - 2].lower()\n    if i + 1 < len(sentence):\n        yield \"word+1:{}\" + sentence[i + 1].lower()\n        if i + 2 < len(sentence):\n            yield \"word+2:{}\" + sentence[i + 2].lower()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the F - score of a BIO - tagging scheme.", "response": "def bio_f_score(y_true, y_pred):\n    \"\"\"F-score for BIO-tagging scheme, as used by CoNLL.\n\n    This F-score variant is used for evaluating named-entity recognition and\n    related problems, where the goal is to predict segments of interest within\n    sequences and mark these as a \"B\" (begin) tag followed by zero or more \"I\"\n    (inside) tags. A true positive is then defined as a BI* segment in both\n    y_true and y_pred, with false positives and false negatives defined\n    similarly.\n\n    Support for tags schemes with classes (e.g. \"B-NP\") are limited: reported\n    scores may be too high for inconsistent labelings.\n\n    Parameters\n    ----------\n    y_true : array-like of strings, shape (n_samples,)\n        Ground truth labeling.\n\n    y_pred : array-like of strings, shape (n_samples,)\n        Sequence classifier's predictions.\n\n    Returns\n    -------\n    f : float\n        F-score.\n    \"\"\"\n\n    if len(y_true) != len(y_pred):\n        msg = \"Sequences not of the same length ({} != {}).\"\"\"\n        raise ValueError(msg.format(len(y_true), len(y_pred)))\n\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n\n    is_b = partial(np.char.startswith, prefix=\"B\")\n\n    where = np.where\n    t_starts = where(is_b(y_true))[0]\n    p_starts = where(is_b(y_pred))[0]\n\n    # These lengths are off-by-one because we skip the \"B\", but that's ok.\n    # http://stackoverflow.com/q/17929499/166749\n    t_lengths = np.diff(where(is_b(np.r_[y_true[y_true != 'O'], ['B']]))[0])\n    p_lengths = np.diff(where(is_b(np.r_[y_pred[y_pred != 'O'], ['B']]))[0])\n\n    t_segments = set(zip(t_starts, t_lengths, y_true[t_starts]))\n    p_segments = set(zip(p_starts, p_lengths, y_pred[p_starts]))\n\n    # tp = len(t_segments & p_segments)\n    # fn = len(t_segments - p_segments)\n    # fp = len(p_segments - t_segments)\n    tp = sum(x in t_segments for x in p_segments)\n    fn = sum(x not in p_segments for x in t_segments)\n    fp = sum(x not in t_segments for x in p_segments)\n\n    if tp == 0:\n        # special-cased like this in CoNLL evaluation\n        return 0.\n\n    precision = tp / float(tp + fp)\n    recall = tp / float(tp + fn)\n\n    return 2. * precision * recall / (precision + recall)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\naverage accuracy measured on whole sequences.", "response": "def whole_sequence_accuracy(y_true, y_pred, lengths):\n    \"\"\"Average accuracy measured on whole sequences.\n\n    Returns the fraction of sequences in y_true that occur in y_pred without a\n    single error.\n    \"\"\"\n    lengths = np.asarray(lengths)\n    end = np.cumsum(lengths)\n    start = end - lengths\n    bounds = np.vstack([start, end]).T\n\n    errors = sum(1. for i, j in bounds\n                 if np.any(y_true[i:j] != y_pred[i:j]))\n    return 1 - errors / len(lengths)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a CoNLL file and extract features on the tokens and vectorize them.", "response": "def load_conll(f, features, n_features=(2 ** 16), split=False):\n    \"\"\"Load CoNLL file, extract features on the tokens and vectorize them.\n\n    The ConLL file format is a line-oriented text format that describes\n    sequences in a space-separated format, separating the sequences with\n    blank lines. Typically, the last space-separated part is a label.\n\n    Since the tab-separated parts are usually tokens (and maybe things like\n    part-of-speech tags) rather than feature vectors, a function must be\n    supplied that does the actual feature extraction. This function has access\n    to the entire sequence, so that it can extract context features.\n\n    A ``sklearn.feature_extraction.FeatureHasher`` (the \"hashing trick\")\n    is used to map symbolic input feature names to columns, so this function\n    dos not remember the actual input feature names.\n\n    Parameters\n    ----------\n    f : {string, file-like}\n        Input file.\n    features : callable\n        Feature extraction function. Must take a list of tokens l that\n        represent a single sequence and an index i into this list, and must\n        return an iterator over strings that represent the features of l[i].\n    n_features : integer, optional\n        Number of columns in the output.\n    split : boolean, default=False\n        Whether to split lines on whitespace beyond what is needed to parse\n        out the labels. This is useful for CoNLL files that have extra columns\n        containing information like part of speech tags.\n\n    Returns\n    -------\n    X : scipy.sparse matrix, shape (n_samples, n_features)\n        Samples (feature vectors), as a single sparse matrix.\n    y : np.ndarray, dtype np.string, shape n_samples\n        Per-sample labels.\n    lengths : np.ndarray, dtype np.int32, shape n_sequences\n        Lengths of sequences within (X, y). The sum of these is equal to\n        n_samples.\n    \"\"\"\n    fh = FeatureHasher(n_features=n_features, input_type=\"string\")\n    labels = []\n    lengths = []\n\n    with _open(f) as f:\n        raw_X = _conll_sequences(f, features, labels, lengths, split)\n        X = fh.transform(raw_X)\n\n    return X, np.asarray(labels), np.asarray(lengths, dtype=np.int32)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, X, lengths=None):\n        X = atleast2d_or_csr(X)\n        scores = safe_sparse_dot(X, self.coef_.T)\n        if hasattr(self, \"coef_trans_\"):\n            n_classes = len(self.classes_)\n            coef_t = self.coef_trans_.T.reshape(-1, self.coef_trans_.shape[-1])\n            trans_scores = safe_sparse_dot(X, coef_t.T)\n            trans_scores = trans_scores.reshape(-1, n_classes, n_classes)\n        else:\n            trans_scores = None\n\n        decode = self._get_decoder()\n\n        if lengths is None:\n            y = decode(scores, trans_scores, self.intercept_trans_,\n                       self.intercept_init_, self.intercept_final_)\n        else:\n            start, end = validate_lengths(X.shape[0], lengths)\n\n            y = [decode(scores[start[i]:end[i]], trans_scores,\n                        self.intercept_trans_, self.intercept_init_,\n                        self.intercept_final_)\n                 for i in six.moves.xrange(len(lengths))]\n            y = np.hstack(y)\n\n        return self.classes_[y]", "response": "Predict labels and tags for samples X."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef score(self, X, y, lengths=None):\n        return accuracy_score(y, self.predict(X, lengths))", "response": "Returns the mean accuracy on the given test data and labels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(self, X, y, lengths):\n\n        decode = self._get_decoder()\n\n        X = atleast2d_or_csr(X)\n\n        classes, y = np.unique(y, return_inverse=True)\n        n_classes = len(classes)\n\n        if self.trans_features:\n            trans_true = make_trans_matrix(y, n_classes)\n        class_range = np.arange(n_classes)\n        Y_true = y.reshape(-1, 1) == class_range\n\n        lengths = np.asarray(lengths)\n        n_samples, n_features = X.shape\n\n        end = np.cumsum(lengths)\n        start = end - lengths\n\n        w = np.zeros((n_classes, n_features), order='F')\n        b_trans = np.zeros((n_classes, n_classes))\n        b_init = np.zeros(n_classes)\n        b_final = np.zeros(n_classes)\n\n        w_avg = np.zeros_like(w)\n        b_trans_avg = np.zeros_like(b_trans)\n        b_init_avg = np.zeros_like(b_init)\n        b_final_avg = np.zeros_like(b_final)\n\n        if self.trans_features:\n            w_trans = np.zeros((n_classes, n_classes, n_features), order='F')\n            w_t_flat = w_trans.reshape(-1, n_features)\n            w_trans_avg = np.zeros_like(w_trans)\n            w_t_avg_flat = w_trans_avg.reshape(-1, n_features)\n\n        sequence_ids = np.arange(lengths.shape[0])\n        rng = check_random_state(self.random_state)\n\n        avg_count = 1.\n        lr_exponent = self.lr_exponent\n\n        for it in six.moves.xrange(1, self.max_iter + 1):\n            lr = 1. / (it ** lr_exponent)\n\n            if self.verbose:\n                print(\"Iteration {0:2d}\".format(it), end=\"... \")\n                sys.stdout.flush()\n\n            rng.shuffle(sequence_ids)\n\n            sum_loss = 0\n\n            for i in sequence_ids:\n                X_i = X[start[i]:end[i]]\n                score = safe_sparse_dot(X_i, w.T)\n                if self.trans_features:\n                    trans_score = safe_sparse_dot(X_i, w_t_flat.T)\n                    trans_score = trans_score.reshape(-1, n_classes, n_classes)\n                else:\n                    trans_score = None\n                y_pred = decode(score, trans_score, b_trans, b_init, b_final)\n                y_t_i = y[start[i]:end[i]]\n                loss = (y_pred != y_t_i).sum()\n\n                if loss:\n                    sum_loss += loss\n\n                    Y_t_i = Y_true[start[i]:end[i]]\n                    Y_pred = y_pred.reshape(-1, 1) == class_range\n                    Y_pred = Y_pred.astype(np.float64)\n\n                    Y_diff = csc_matrix(Y_pred - Y_t_i)\n                    Y_diff *= -lr\n                    w_update = safe_sparse_dot(Y_diff.T, X_i)\n\n                    if self.trans_features:\n                        trans_pred = make_trans_matrix(y_pred, n_classes)\n\n                        trans_diff = trans_true[start[i]:end[i]].toarray()\n                        safe_add(trans_diff, -trans_pred)\n                        trans_diff = csc_matrix(trans_diff)\n                        trans_diff *= -lr\n\n                        w_trans_update = safe_sparse_dot(trans_diff.T, X_i)\n                        safe_add(w_t_flat, w_trans_update)\n\n                        w_trans_update *= avg_count\n                        safe_add(w_t_avg_flat, w_trans_update)\n\n                    t_trans = count_trans(y_t_i, n_classes)\n                    p_trans = count_trans(y_pred, n_classes)\n                    b_trans_update = lr * (p_trans - t_trans)\n                    b_init_update = lr * (Y_pred[0] - Y_true[start[i]])\n                    b_final_update = lr * (Y_pred[-1] - Y_true[end[i] - 1])\n\n                    safe_add(w, w_update)\n                    b_trans -= b_trans_update\n                    b_init -= b_init_update\n                    b_final -= b_final_update\n\n                    w_update *= avg_count\n                    b_trans_update *= avg_count\n                    b_init_update *= avg_count\n                    b_final_update *= avg_count\n\n                    safe_add(w_avg, w_update)\n                    b_trans_avg -= b_trans_update\n                    b_init_avg -= b_init_update\n                    b_final_avg -= b_final_update\n\n            if self.verbose:\n                # XXX the loss reported is that for w, but the one for\n                # w_avg is what matters for early stopping.\n                print(\"loss = {0:.4f}\".format(sum_loss / n_samples))\n\n            avg_count += 1.\n\n        w -= w_avg / avg_count\n        if self.trans_features:\n            w_trans -= w_trans_avg / avg_count\n        b_init -= b_init_avg / avg_count\n        b_trans -= b_trans_avg / avg_count\n        b_final -= b_final_avg / avg_count\n\n        self.coef_ = w\n        if self.trans_features:\n            self.coef_trans_ = w_trans\n        self.intercept_init_ = b_init\n        self.intercept_trans_ = b_trans\n        self.intercept_final_ = b_final\n\n        self.classes_ = classes\n\n        return self", "response": "Fit to a set of sequences."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef array2d(X, dtype=None, order=None, copy=False):\n    if sp.issparse(X):\n        raise TypeError('A sparse matrix was passed, but dense data '\n                        'is required. Use X.toarray() to convert to dense.')\n    X_2d = np.asarray(np.atleast_2d(X), dtype=dtype, order=order)\n    _assert_all_finite(X_2d)\n    if X is X_2d and copy:\n        X_2d = safe_copy(X_2d)\n    return X_2d", "response": "Returns at least 2 - d array with data from X"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlikes numpy. atleast_2d but converts sparse matrices to CSR format", "response": "def atleast2d_or_csr(X, dtype=None, order=None, copy=False):\n    \"\"\"Like numpy.atleast_2d, but converts sparse matrices to CSR format\n\n    Also, converts np.matrix to np.ndarray.\n    \"\"\"\n    return _atleast2d_or_sparse(X, dtype, order, copy, sp.csr_matrix,\n                                \"tocsr\", sp.isspmatrix_csr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating lengths array against n_samples.", "response": "def validate_lengths(n_samples, lengths):\n    \"\"\"Validate lengths array against n_samples.\n\n    Parameters\n    ----------\n    n_samples : integer\n        Total number of samples.\n\n    lengths : array-like of integers, shape (n_sequences,), optional\n        Lengths of individual sequences in the input.\n\n    Returns\n    -------\n    start : array of integers, shape (n_sequences,)\n        Start indices of sequences.\n\n    end : array of integers, shape (n_sequences,)\n        One-past-the-end indices of sequences.\n    \"\"\"\n    if lengths is None:\n        lengths = [n_samples]\n    lengths = np.asarray(lengths, dtype=np.int32)\n    if lengths.sum() > n_samples:\n        msg = \"More than {0:d} samples in lengths array {1!s}\"\n        raise ValueError(msg.format(n_samples, lengths))\n\n    end = np.cumsum(lengths)\n    start = end - lengths\n\n    return start, end"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_trans_matrix(y, n_classes, dtype=np.float64):\n    indices = np.empty(len(y), dtype=np.int32)\n\n    for i in six.moves.xrange(len(y) - 1):\n        indices[i] = y[i] * i + y[i + 1]\n\n    indptr = np.arange(len(y) + 1)\n    indptr[-1] = indptr[-2]\n\n    return csr_matrix((np.ones(len(y), dtype=dtype), indices, indptr),\n                      shape=(len(y), n_classes ** 2))", "response": "Make a sparse transition matrix for y."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake non - mocked network request", "response": "async def passthrough(self, request):\n        \"\"\"Make non-mocked network request\"\"\"\n        connector = TCPConnector()\n        connector._resolve_host = partial(self._old_resolver_mock, connector)\n\n        new_is_ssl = ClientRequest.is_ssl\n        ClientRequest.is_ssl = self._old_is_ssl\n        try:\n            original_request = request.clone(scheme=\"https\" if request.headers[\"AResponsesIsSSL\"] else \"http\")\n\n            headers = {k: v for k, v in request.headers.items() if k != \"AResponsesIsSSL\"}\n\n            async with ClientSession(connector=connector) as session:\n                async with getattr(session, request.method.lower())(original_request.url, headers=headers, data=(await request.read())) as r:\n                    headers = {k: v for k, v in r.headers.items() if k.lower() == \"content-type\"}\n                    text = await r.text()\n                    response = self.Response(text=text, status=r.status, headers=headers)\n                    return response\n        finally:\n            ClientRequest.is_ssl = new_is_ssl"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calculate_scores(self, han, fu, config, is_yakuman=False):\n\n        # kazoe hand\n        if han >= 13 and not is_yakuman:\n            # Hands over 26+ han don't count as double yakuman\n            if config.options.kazoe_limit == HandConfig.KAZOE_LIMITED:\n                han = 13\n            # Hands over 13+ is a sanbaiman\n            elif config.options.kazoe_limit == HandConfig.KAZOE_SANBAIMAN:\n                han = 12\n\n        if han >= 5:\n            if han >= 78:\n                rounded = 48000\n            elif han >= 65:\n                rounded = 40000\n            elif han >= 52:\n                rounded = 32000\n            elif han >= 39:\n                rounded = 24000\n            # double yakuman\n            elif han >= 26:\n                rounded = 16000\n            # yakuman\n            elif han >= 13:\n                rounded = 8000\n            # sanbaiman\n            elif han >= 11:\n                rounded = 6000\n            # baiman\n            elif han >= 8:\n                rounded = 4000\n            # haneman\n            elif han >= 6:\n                rounded = 3000\n            else:\n                rounded = 2000\n\n            double_rounded = rounded * 2\n            four_rounded = double_rounded * 2\n            six_rounded = double_rounded * 3\n        else:\n            base_points = fu * pow(2, 2 + han)\n            rounded = (base_points + 99) // 100 * 100\n            double_rounded = (2 * base_points + 99) // 100 * 100\n            four_rounded = (4 * base_points + 99) // 100 * 100\n            six_rounded = (6 * base_points + 99) // 100 * 100\n\n            is_kiriage = False\n            if config.options.kiriage:\n                if han == 4 and fu == 30:\n                    is_kiriage = True\n                if han == 3 and fu == 60:\n                    is_kiriage = True\n\n            # mangan\n            if rounded > 2000 or is_kiriage:\n                rounded = 2000\n                double_rounded = rounded * 2\n                four_rounded = double_rounded * 2\n                six_rounded = double_rounded * 3\n\n        if config.is_tsumo:\n            return {'main': double_rounded, 'additional': config.is_dealer and double_rounded or rounded}\n        else:\n            return {'main': config.is_dealer and six_rounded or four_rounded, 'additional': 0}", "response": "Calculates how much scores cost a hand with given han and fu."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_fu(self,\n                     hand,\n                     win_tile,\n                     win_group,\n                     config,\n                     valued_tiles=None,\n                     melds=None,):\n        \"\"\"\n        Calculate hand fu with explanations\n        :param hand:\n        :param win_tile: 136 tile format\n        :param win_group: one set where win tile exists\n        :param is_tsumo:\n        :param config: HandConfig object\n        :param valued_tiles: dragons, player wind, round wind\n        :param melds: opened sets\n        :return:\n        \"\"\"\n\n        win_tile_34 = win_tile // 4\n\n        if not valued_tiles:\n            valued_tiles = []\n\n        if not melds:\n            melds = []\n\n        fu_details = []\n\n        if len(hand) == 7:\n            return [{'fu': 25, 'reason': FuCalculator.BASE}], 25\n\n        pair = [x for x in hand if is_pair(x)][0]\n        pon_sets = [x for x in hand if is_pon(x)]\n\n        copied_opened_melds = [x.tiles_34 for x in melds if x.type == Meld.CHI]\n        closed_chi_sets = []\n        for x in hand:\n            if x not in copied_opened_melds:\n                closed_chi_sets.append(x)\n            else:\n                copied_opened_melds.remove(x)\n\n        is_open_hand = any([x.opened for x in melds])\n\n        if win_group in closed_chi_sets:\n            tile_index = simplify(win_tile_34)\n\n            # penchan\n            if contains_terminals(win_group):\n                # 1-2-... wait\n                if tile_index == 2 and win_group.index(win_tile_34) == 2:\n                    fu_details.append({'fu': 2, 'reason': FuCalculator.PENCHAN})\n                # 8-9-... wait\n                elif tile_index == 6 and win_group.index(win_tile_34) == 0:\n                    fu_details.append({'fu': 2, 'reason': FuCalculator.PENCHAN})\n\n            # kanchan waiting 5-...-7\n            if win_group.index(win_tile_34) == 1:\n                fu_details.append({'fu': 2, 'reason': FuCalculator.KANCHAN})\n\n        # valued pair\n        count_of_valued_pairs = valued_tiles.count(pair[0])\n        if count_of_valued_pairs == 1:\n            fu_details.append({'fu': 2, 'reason': FuCalculator.VALUED_PAIR})\n\n        # east-east pair when you are on east gave double fu\n        if count_of_valued_pairs == 2:\n            fu_details.append({'fu': 2, 'reason': FuCalculator.VALUED_PAIR})\n            fu_details.append({'fu': 2, 'reason': FuCalculator.VALUED_PAIR})\n\n        # pair wait\n        if is_pair(win_group):\n            fu_details.append({'fu': 2, 'reason': FuCalculator.PAIR_WAIT})\n\n        for set_item in pon_sets:\n            open_meld = [x for x in melds if set_item == x.tiles_34]\n            open_meld = open_meld and open_meld[0] or None\n\n            set_was_open = open_meld and open_meld.opened or False\n            is_kan = (open_meld and (open_meld.type == Meld.KAN or open_meld.type == Meld.CHANKAN)) or False\n            is_honor = set_item[0] in TERMINAL_INDICES + HONOR_INDICES\n\n            # we win by ron on the third pon tile, our pon will be count as open\n            if not config.is_tsumo and set_item == win_group:\n                set_was_open = True\n\n            if is_honor:\n                if is_kan:\n                    if set_was_open:\n                        fu_details.append({'fu': 16, 'reason': FuCalculator.OPEN_TERMINAL_KAN})\n                    else:\n                        fu_details.append({'fu': 32, 'reason': FuCalculator.CLOSED_TERMINAL_KAN})\n                else:\n                    if set_was_open:\n                        fu_details.append({'fu': 4, 'reason': FuCalculator.OPEN_TERMINAL_PON})\n                    else:\n                        fu_details.append({'fu': 8, 'reason': FuCalculator.CLOSED_TERMINAL_PON})\n            else:\n                if is_kan:\n                    if set_was_open:\n                        fu_details.append({'fu': 8, 'reason': FuCalculator.OPEN_KAN})\n                    else:\n                        fu_details.append({'fu': 16, 'reason': FuCalculator.CLOSED_KAN})\n                else:\n                    if set_was_open:\n                        fu_details.append({'fu': 2, 'reason': FuCalculator.OPEN_PON})\n                    else:\n                        fu_details.append({'fu': 4, 'reason': FuCalculator.CLOSED_PON})\n\n        add_tsumo_fu = len(fu_details) > 0 or config.options.fu_for_pinfu_tsumo\n\n        if config.is_tsumo and add_tsumo_fu:\n            # 2 additional fu for tsumo (but not for pinfu)\n            fu_details.append({'fu': 2, 'reason': FuCalculator.TSUMO})\n\n        if is_open_hand and not len(fu_details) and config.options.fu_for_open_pinfu:\n            # there is no 1-20 hands, so we had to add additional fu\n            fu_details.append({'fu': 2, 'reason': FuCalculator.HAND_WITHOUT_FU})\n\n        if is_open_hand or config.is_tsumo:\n            fu_details.append({'fu': 20, 'reason': FuCalculator.BASE})\n        else:\n            fu_details.append({'fu': 30, 'reason': FuCalculator.BASE})\n\n        return fu_details, self.round_fu(fu_details)", "response": "Calculate the FU of a hand with explanations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_shanten(self, tiles_34, open_sets_34=None, chiitoitsu=True, kokushi=True):\n        # we will modify them later, so we need to use a copy\n        tiles_34 = copy.deepcopy(tiles_34)\n\n        self._init(tiles_34)\n\n        count_of_tiles = sum(tiles_34)\n\n        if count_of_tiles > 14:\n            return -2\n\n        # With open hand we need to remove open sets from hand and replace them with isolated pon sets\n        # it will allow to calculate count of shanten correctly\n        if open_sets_34:\n            isolated_tiles = find_isolated_tile_indices(tiles_34)\n            for meld in open_sets_34:\n                if not isolated_tiles:\n                    break\n\n                isolated_tile = isolated_tiles.pop()\n\n                tiles_34[meld[0]] -= 1\n                tiles_34[meld[1]] -= 1\n                tiles_34[meld[2]] -= 1\n                tiles_34[isolated_tile] = 3\n\n        if not open_sets_34:\n            self.min_shanten = self._scan_chiitoitsu_and_kokushi(chiitoitsu, kokushi)\n\n        self._remove_character_tiles(count_of_tiles)\n\n        init_mentsu = math.floor((14 - count_of_tiles) / 3)\n        self._scan(init_mentsu)\n\n        return self.min_shanten", "response": "Calculate the minimum number of shanten for a set of 34 tiles."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_condition_met(self, hand, win_tile, melds, is_tsumo):\n        win_tile //= 4\n\n        open_sets = [x.tiles_34 for x in melds if x.opened]\n\n        chi_sets = [x for x in hand if (is_chi(x) and win_tile in x and x not in open_sets)]\n        pon_sets = [x for x in hand if is_pon(x)]\n\n        closed_pon_sets = []\n        for item in pon_sets:\n            if item in open_sets:\n                continue\n\n            # if we do the ron on syanpon wait our pon will be consider as open\n            # and it is not 789999 set\n            if win_tile in item and not is_tsumo and not len(chi_sets):\n                continue\n\n            closed_pon_sets.append(item)\n\n        return len(closed_pon_sets) == 3", "response": "Check if condition met in the hand."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_condition_met(self, hand, *args):\n        pon_sets = [x for x in hand if is_pon(x)]\n        if len(pon_sets) != 4:\n            return False\n\n        count_wind_sets = 0\n        winds = [EAST, SOUTH, WEST, NORTH]\n        for item in pon_sets:\n            if is_pon(item) and item[0] in winds:\n                count_wind_sets += 1\n\n        return count_wind_sets == 4", "response": "Check if the condition is met."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef estimate_hand_value(self, tiles, win_tile, melds=None, dora_indicators=None, config=None):\n\n        if not melds:\n            melds = []\n\n        if not dora_indicators:\n            dora_indicators = []\n\n        self.config = config or HandConfig()\n\n        agari = Agari()\n        hand_yaku = []\n        scores_calculator = ScoresCalculator()\n        tiles_34 = TilesConverter.to_34_array(tiles)\n        divider = HandDivider()\n        fu_calculator = FuCalculator()\n\n        opened_melds = [x.tiles_34 for x in melds if x.opened]\n        all_melds = [x.tiles_34 for x in melds]\n        is_open_hand = len(opened_melds) > 0\n\n        # special situation\n        if self.config.is_nagashi_mangan:\n            hand_yaku.append(self.config.yaku.nagashi_mangan)\n            fu = 30\n            han = self.config.yaku.nagashi_mangan.han_closed\n            cost = scores_calculator.calculate_scores(han, fu, self.config, False)\n            return HandResponse(cost, han, fu, hand_yaku)\n\n        if win_tile not in tiles:\n            return HandResponse(error=\"Win tile not in the hand\")\n\n        if self.config.is_riichi and is_open_hand:\n            return HandResponse(error=\"Riichi can't be declared with open hand\")\n\n        if self.config.is_ippatsu and is_open_hand:\n            return HandResponse(error=\"Ippatsu can't be declared with open hand\")\n\n        if self.config.is_ippatsu and not self.config.is_riichi and not self.config.is_daburu_riichi:\n            return HandResponse(error=\"Ippatsu can't be declared without riichi\")\n\n        if not agari.is_agari(tiles_34, all_melds):\n            return HandResponse(error='Hand is not winning')\n\n        if not self.config.options.has_double_yakuman:\n            self.config.yaku.daburu_kokushi.han_closed = 13\n            self.config.yaku.suuankou_tanki.han_closed = 13\n            self.config.yaku.daburu_chuuren_poutou.han_closed = 13\n            self.config.yaku.daisuushi.han_closed = 13\n            self.config.yaku.daisuushi.han_open = 13\n\n        hand_options = divider.divide_hand(tiles_34, melds)\n\n        calculated_hands = []\n        for hand in hand_options:\n            is_chiitoitsu = self.config.yaku.chiitoitsu.is_condition_met(hand)\n            valued_tiles = [HAKU, HATSU, CHUN, self.config.player_wind, self.config.round_wind]\n\n            win_groups = self._find_win_groups(win_tile, hand, opened_melds)\n            for win_group in win_groups:\n                cost = None\n                error = None\n                hand_yaku = []\n                han = 0\n\n                fu_details, fu = fu_calculator.calculate_fu(\n                    hand,\n                    win_tile,\n                    win_group,\n                    self.config,\n                    valued_tiles,\n                    melds\n                )\n\n                is_pinfu = len(fu_details) == 1 and not is_chiitoitsu and not is_open_hand\n\n                pon_sets = [x for x in hand if is_pon(x)]\n                chi_sets = [x for x in hand if is_chi(x)]\n\n                if self.config.is_tsumo:\n                    if not is_open_hand:\n                        hand_yaku.append(self.config.yaku.tsumo)\n\n                if is_pinfu:\n                    hand_yaku.append(self.config.yaku.pinfu)\n\n                # let's skip hand that looks like chitoitsu, but it contains open sets\n                if is_chiitoitsu and is_open_hand:\n                    continue\n\n                if is_chiitoitsu:\n                    hand_yaku.append(self.config.yaku.chiitoitsu)\n\n                if self.config.options.has_daisharin and self.config.yaku.daisharin.is_condition_met(hand, self.config.options.has_daisharin_other_suits):\n                    self.config.yaku.daisharin.rename(hand)\n                    hand_yaku.append(self.config.yaku.daisharin)\n\n                is_tanyao = self.config.yaku.tanyao.is_condition_met(hand)\n                if is_open_hand and not self.config.options.has_open_tanyao:\n                    is_tanyao = False\n\n                if is_tanyao:\n                    hand_yaku.append(self.config.yaku.tanyao)\n\n                if self.config.is_riichi and not self.config.is_daburu_riichi:\n                    hand_yaku.append(self.config.yaku.riichi)\n\n                if self.config.is_daburu_riichi:\n                    hand_yaku.append(self.config.yaku.daburu_riichi)\n\n                if self.config.is_ippatsu:\n                    hand_yaku.append(self.config.yaku.ippatsu)\n\n                if self.config.is_rinshan:\n                    hand_yaku.append(self.config.yaku.rinshan)\n\n                if self.config.is_chankan:\n                    hand_yaku.append(self.config.yaku.chankan)\n\n                if self.config.is_haitei:\n                    hand_yaku.append(self.config.yaku.haitei)\n\n                if self.config.is_houtei:\n                    hand_yaku.append(self.config.yaku.houtei)\n\n                if self.config.is_renhou:\n                    if self.config.options.renhou_as_yakuman:\n                        hand_yaku.append(self.config.yaku.renhou_yakuman)\n                    else:\n                        hand_yaku.append(self.config.yaku.renhou)\n\n                if self.config.is_tenhou:\n                    hand_yaku.append(self.config.yaku.tenhou)\n\n                if self.config.is_chiihou:\n                    hand_yaku.append(self.config.yaku.chiihou)\n\n                if self.config.yaku.honitsu.is_condition_met(hand):\n                    hand_yaku.append(self.config.yaku.honitsu)\n\n                if self.config.yaku.chinitsu.is_condition_met(hand):\n                    hand_yaku.append(self.config.yaku.chinitsu)\n\n                if self.config.yaku.tsuisou.is_condition_met(hand):\n                    hand_yaku.append(self.config.yaku.tsuisou)\n\n                if self.config.yaku.honroto.is_condition_met(hand):\n                    hand_yaku.append(self.config.yaku.honroto)\n\n                if self.config.yaku.chinroto.is_condition_met(hand):\n                    hand_yaku.append(self.config.yaku.chinroto)\n\n                # small optimization, try to detect yaku with chi required sets only if we have chi sets in hand\n                if len(chi_sets):\n                    if self.config.yaku.chanta.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.chanta)\n\n                    if self.config.yaku.junchan.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.junchan)\n\n                    if self.config.yaku.ittsu.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.ittsu)\n\n                    if not is_open_hand:\n                        if self.config.yaku.ryanpeiko.is_condition_met(hand):\n                            hand_yaku.append(self.config.yaku.ryanpeiko)\n                        elif self.config.yaku.iipeiko.is_condition_met(hand):\n                            hand_yaku.append(self.config.yaku.iipeiko)\n\n                    if self.config.yaku.sanshoku.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.sanshoku)\n\n                # small optimization, try to detect yaku with pon required sets only if we have pon sets in hand\n                if len(pon_sets):\n                    if self.config.yaku.toitoi.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.toitoi)\n\n                    if self.config.yaku.sanankou.is_condition_met(hand, win_tile, melds, self.config.is_tsumo):\n                        hand_yaku.append(self.config.yaku.sanankou)\n\n                    if self.config.yaku.sanshoku_douko.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.sanshoku_douko)\n\n                    if self.config.yaku.shosangen.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.shosangen)\n\n                    if self.config.yaku.haku.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.haku)\n\n                    if self.config.yaku.hatsu.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.hatsu)\n\n                    if self.config.yaku.chun.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.chun)\n\n                    if self.config.yaku.east.is_condition_met(hand, self.config.player_wind, self.config.round_wind):\n                        if self.config.player_wind == EAST:\n                            hand_yaku.append(self.config.yaku.yakuhai_place)\n\n                        if self.config.round_wind == EAST:\n                            hand_yaku.append(self.config.yaku.yakuhai_round)\n\n                    if self.config.yaku.south.is_condition_met(hand, self.config.player_wind, self.config.round_wind):\n                        if self.config.player_wind == SOUTH:\n                            hand_yaku.append(self.config.yaku.yakuhai_place)\n\n                        if self.config.round_wind == SOUTH:\n                            hand_yaku.append(self.config.yaku.yakuhai_round)\n\n                    if self.config.yaku.west.is_condition_met(hand, self.config.player_wind, self.config.round_wind):\n                        if self.config.player_wind == WEST:\n                            hand_yaku.append(self.config.yaku.yakuhai_place)\n\n                        if self.config.round_wind == WEST:\n                            hand_yaku.append(self.config.yaku.yakuhai_round)\n\n                    if self.config.yaku.north.is_condition_met(hand, self.config.player_wind, self.config.round_wind):\n                        if self.config.player_wind == NORTH:\n                            hand_yaku.append(self.config.yaku.yakuhai_place)\n\n                        if self.config.round_wind == NORTH:\n                            hand_yaku.append(self.config.yaku.yakuhai_round)\n\n                    if self.config.yaku.daisangen.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.daisangen)\n\n                    if self.config.yaku.shosuushi.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.shosuushi)\n\n                    if self.config.yaku.daisuushi.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.daisuushi)\n\n                    if self.config.yaku.ryuisou.is_condition_met(hand):\n                        hand_yaku.append(self.config.yaku.ryuisou)\n\n                    # closed kan can't be used in chuuren_poutou\n                    if not len(melds) and self.config.yaku.chuuren_poutou.is_condition_met(hand):\n                        if tiles_34[win_tile // 4] == 2:\n                            hand_yaku.append(self.config.yaku.daburu_chuuren_poutou)\n                        else:\n                            hand_yaku.append(self.config.yaku.chuuren_poutou)\n\n                    if not is_open_hand and self.config.yaku.suuankou.is_condition_met(hand, win_tile,\n                                                                                       self.config.is_tsumo):\n                        if tiles_34[win_tile // 4] == 2:\n                            hand_yaku.append(self.config.yaku.suuankou_tanki)\n                        else:\n                            hand_yaku.append(self.config.yaku.suuankou)\n\n                    if self.config.yaku.sankantsu.is_condition_met(hand, melds):\n                        hand_yaku.append(self.config.yaku.sankantsu)\n\n                    if self.config.yaku.suukantsu.is_condition_met(hand, melds):\n                        hand_yaku.append(self.config.yaku.suukantsu)\n\n                # yakuman is not connected with other yaku\n                yakuman_list = [x for x in hand_yaku if x.is_yakuman]\n                if yakuman_list:\n                    hand_yaku = yakuman_list\n\n                # calculate han\n                for item in hand_yaku:\n                    if is_open_hand and item.han_open:\n                        han += item.han_open\n                    else:\n                        han += item.han_closed\n\n                if han == 0:\n                    error = 'There are no yaku in the hand'\n                    cost = None\n\n                # we don't need to add dora to yakuman\n                if not yakuman_list:\n                    tiles_for_dora = tiles[:]\n\n                    # we had to search for dora in kan fourth tiles as well\n                    for meld in melds:\n                        if meld.type == Meld.KAN or meld.type == Meld.CHANKAN:\n                            tiles_for_dora.append(meld.tiles[3])\n\n                    count_of_dora = 0\n                    count_of_aka_dora = 0\n\n                    for tile in tiles_for_dora:\n                        count_of_dora += plus_dora(tile, dora_indicators)\n\n                    for tile in tiles_for_dora:\n                        if is_aka_dora(tile, self.config.options.has_aka_dora):\n                            count_of_aka_dora += 1\n\n                    if count_of_dora:\n                        self.config.yaku.dora.han_open = count_of_dora\n                        self.config.yaku.dora.han_closed = count_of_dora\n                        hand_yaku.append(self.config.yaku.dora)\n                        han += count_of_dora\n\n                    if count_of_aka_dora:\n                        self.config.yaku.aka_dora.han_open = count_of_aka_dora\n                        self.config.yaku.aka_dora.han_closed = count_of_aka_dora\n                        hand_yaku.append(self.config.yaku.aka_dora)\n                        han += count_of_aka_dora\n\n                if not error:\n                    cost = scores_calculator.calculate_scores(han, fu, self.config, len(yakuman_list) > 0)\n\n                calculated_hand = {\n                    'cost': cost,\n                    'error': error,\n                    'hand_yaku': hand_yaku,\n                    'han': han,\n                    'fu': fu,\n                    'fu_details': fu_details\n                }\n\n                calculated_hands.append(calculated_hand)\n\n        # exception hand\n        if not is_open_hand and self.config.yaku.kokushi.is_condition_met(None, tiles_34):\n            if tiles_34[win_tile // 4] == 2:\n                hand_yaku.append(self.config.yaku.daburu_kokushi)\n            else:\n                hand_yaku.append(self.config.yaku.kokushi)\n\n            if self.config.is_renhou and self.config.options.renhou_as_yakuman:\n                hand_yaku.append(self.config.yaku.renhou_yakuman)\n\n            if self.config.is_tenhou:\n                hand_yaku.append(self.config.yaku.tenhou)\n\n            if self.config.is_chiihou:\n                hand_yaku.append(self.config.yaku.chiihou)\n\n            # calculate han\n            han = 0\n            for item in hand_yaku:\n                if is_open_hand and item.han_open:\n                    han += item.han_open\n                else:\n                    han += item.han_closed\n\n            fu = 0\n            cost = scores_calculator.calculate_scores(han, fu, self.config, len(hand_yaku) > 0)\n            calculated_hands.append({\n                'cost': cost,\n                'error': None,\n                'hand_yaku': hand_yaku,\n                'han': han,\n                'fu': fu,\n                'fu_details': []\n            })\n\n        # let's use cost for most expensive hand\n        calculated_hands = sorted(calculated_hands, key=lambda x: (x['han'], x['fu']), reverse=True)\n        calculated_hand = calculated_hands[0]\n\n        cost = calculated_hand['cost']\n        error = calculated_hand['error']\n        hand_yaku = calculated_hand['hand_yaku']\n        han = calculated_hand['han']\n        fu = calculated_hand['fu']\n        fu_details = calculated_hand['fu_details']\n\n        return HandResponse(cost, han, fu, hand_yaku, error, fu_details)", "response": "estimate the value of a hand based on the given tiles win_tile melds and dora_indicators."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_aka_dora(tile, aka_enabled):\n\n    if not aka_enabled:\n        return False\n\n    if tile in [FIVE_RED_MAN, FIVE_RED_PIN, FIVE_RED_SOU]:\n        return True\n\n    return False", "response": "Checks if the tile is aka Dora or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plus_dora(tile, dora_indicators):\n    tile_index = tile // 4\n    dora_count = 0\n\n    for dora in dora_indicators:\n        dora //= 4\n\n        # sou, pin, man\n        if tile_index < EAST:\n\n            # with indicator 9, dora will be 1\n            if dora == 8:\n                dora = -1\n            elif dora == 17:\n                dora = 8\n            elif dora == 26:\n                dora = 17\n\n            if tile_index == dora + 1:\n                dora_count += 1\n        else:\n            if dora < EAST:\n                continue\n\n            dora -= 9 * 3\n            tile_index_temp = tile_index - 9 * 3\n\n            # dora indicator is north\n            if dora == 3:\n                dora = -1\n\n            # dora indicator is hatsu\n            if dora == 6:\n                dora = 3\n\n            if tile_index_temp == dora + 1:\n                dora_count += 1\n\n    return dora_count", "response": "returns the count of dora in a single 136 tile"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind isolated tile indices in the 34 tile format.", "response": "def find_isolated_tile_indices(hand_34):\n    \"\"\"\n    Tiles that don't have -1, 0 and +1 neighbors\n    :param hand_34: array of tiles in 34 tile format\n    :return: array of isolated tiles indices\n    \"\"\"\n    isolated_indices = []\n\n    for x in range(0, CHUN + 1):\n        # for honor tiles we don't need to check nearby tiles\n        if is_honor(x) and hand_34[x] == 0:\n            isolated_indices.append(x)\n        else:\n            simplified = simplify(x)\n\n            # 1 suit tile\n            if simplified == 0:\n                if hand_34[x] == 0 and hand_34[x + 1] == 0:\n                    isolated_indices.append(x)\n            # 9 suit tile\n            elif simplified == 8:\n                if hand_34[x] == 0 and hand_34[x - 1] == 0:\n                    isolated_indices.append(x)\n            # 2-8 tiles tiles\n            else:\n                if hand_34[x] == 0 and hand_34[x - 1] == 0 and hand_34[x + 1] == 0:\n                    isolated_indices.append(x)\n\n    return isolated_indices"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_tile_strictly_isolated(hand_34, tile_34):\n    hand_34 = copy.copy(hand_34)\n    # we don't need to count target tile in the hand\n    hand_34[tile_34] -= 1\n    if hand_34[tile_34] < 0:\n        hand_34[tile_34] = 0\n\n    indices = []\n    if is_honor(tile_34):\n        return hand_34[tile_34] == 0\n    else:\n        simplified = simplify(tile_34)\n\n        # 1 suit tile\n        if simplified == 0:\n            indices = [tile_34, tile_34 + 1, tile_34 + 2]\n        # 2 suit tile\n        elif simplified == 1:\n            indices = [tile_34 - 1, tile_34, tile_34 + 1, tile_34 + 2]\n        # 8 suit tile\n        elif simplified == 7:\n            indices = [tile_34 - 2, tile_34 - 1, tile_34, tile_34 + 1]\n        # 9 suit tile\n        elif simplified == 8:\n            indices = [tile_34 - 2, tile_34 - 1, tile_34]\n        # 3-7 tiles tiles\n        else:\n            indices = [tile_34 - 2, tile_34 - 1, tile_34, tile_34 + 1, tile_34 + 2]\n\n    return all([hand_34[x] == 0 for x in indices])", "response": "Returns true if the given tile is strictly isolated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count_tiles_by_suits(tiles_34):\n    suits = [\n        {'count': 0, 'name': 'sou',   'function': is_sou},\n        {'count': 0, 'name': 'man',   'function': is_man},\n        {'count': 0, 'name': 'pin',   'function': is_pin},\n        {'count': 0, 'name': 'honor', 'function': is_honor}\n    ]\n\n    for x in range(0, 34):\n        tile = tiles_34[x]\n        if not tile:\n            continue\n\n        for item in suits:\n            if item['function'](x):\n                item['count'] += tile\n\n    return suits", "response": "Separate tiles by suits and count them\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the condition is met.", "response": "def is_condition_met(self, hand, *args):\n        \"\"\"\n        Hand composed entirely of honour tiles.\n        :param hand: list of hand's sets\n        :return: boolean\n        \"\"\"\n        indices = reduce(lambda z, y: z + y, hand)\n        return all(x in HONOR_INDICES for x in indices)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of possible hands.", "response": "def divide_hand(self, tiles_34, melds=None):\n        \"\"\"\n        Return a list of possible hands.\n        :param tiles_34:\n        :param melds: list of Meld objects\n        :return:\n        \"\"\"\n        if not melds:\n            melds = []\n\n        closed_hand_tiles_34 = tiles_34[:]\n\n        # small optimization, we can't have a pair in open part of the hand,\n        # so we don't need to try find pairs in open sets\n        open_tile_indices = melds and reduce(lambda x, y: x + y, [x.tiles_34 for x in melds]) or []\n        for open_item in open_tile_indices:\n            closed_hand_tiles_34[open_item] -= 1\n\n        pair_indices = self.find_pairs(closed_hand_tiles_34)\n\n        # let's try to find all possible hand options\n        hands = []\n        for pair_index in pair_indices:\n            local_tiles_34 = tiles_34[:]\n\n            # we don't need to combine already open sets\n            for open_item in open_tile_indices:\n                local_tiles_34[open_item] -= 1\n\n            local_tiles_34[pair_index] -= 2\n\n            # 0 - 8 man tiles\n            man = self.find_valid_combinations(local_tiles_34, 0, 8)\n\n            # 9 - 17 pin tiles\n            pin = self.find_valid_combinations(local_tiles_34, 9, 17)\n\n            # 18 - 26 sou tiles\n            sou = self.find_valid_combinations(local_tiles_34, 18, 26)\n\n            honor = []\n            for x in HONOR_INDICES:\n                if local_tiles_34[x] == 3:\n                    honor.append([x] * 3)\n\n            if honor:\n                honor = [honor]\n\n            arrays = [[[pair_index] * 2]]\n            if sou:\n                arrays.append(sou)\n            if man:\n                arrays.append(man)\n            if pin:\n                arrays.append(pin)\n            if honor:\n                arrays.append(honor)\n\n            for meld in melds:\n                arrays.append([meld.tiles_34])\n\n            # let's find all possible hand from our valid sets\n            for s in itertools.product(*arrays):\n                hand = []\n                for item in list(s):\n                    if isinstance(item[0], list):\n                        for x in item:\n                            hand.append(x)\n                    else:\n                        hand.append(item)\n\n                hand = sorted(hand, key=lambda a: a[0])\n                if len(hand) == 5:\n                    hands.append(hand)\n\n        # small optimization, let's remove hand duplicates\n        unique_hands = []\n        for hand in hands:\n            hand = sorted(hand, key=lambda x: (x[0], x[1]))\n            if hand not in unique_hands:\n                unique_hands.append(hand)\n\n        hands = unique_hands\n\n        if len(pair_indices) == 7:\n            hand = []\n            for index in pair_indices:\n                hand.append([index] * 2)\n            hands.append(hand)\n\n        return hands"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all possible pairs in the hand and return their indices", "response": "def find_pairs(self, tiles_34, first_index=0, second_index=33):\n        \"\"\"\n        Find all possible pairs in the hand and return their indices\n        :return: array of pair indices\n        \"\"\"\n        pair_indices = []\n        for x in range(first_index, second_index + 1):\n            # ignore pon of honor tiles, because it can't be a part of pair\n            if x in HONOR_INDICES and tiles_34[x] != 2:\n                continue\n\n            if tiles_34[x] >= 2:\n                pair_indices.append(x)\n\n        return pair_indices"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_valid_combinations(self, tiles_34, first_index, second_index, hand_not_completed=False):\n        indices = []\n        for x in range(first_index, second_index + 1):\n            if tiles_34[x] > 0:\n                indices.extend([x] * tiles_34[x])\n\n        if not indices:\n            return []\n\n        all_possible_combinations = list(itertools.permutations(indices, 3))\n\n        def is_valid_combination(possible_set):\n            if is_chi(possible_set):\n                return True\n\n            if is_pon(possible_set):\n                return True\n\n            return False\n\n        valid_combinations = []\n        for combination in all_possible_combinations:\n            if is_valid_combination(combination):\n                valid_combinations.append(list(combination))\n\n        if not valid_combinations:\n            return []\n\n        count_of_needed_combinations = int(len(indices) / 3)\n\n        # simple case, we have count of sets == count of tiles\n        if count_of_needed_combinations == len(valid_combinations) and \\\n                reduce(lambda z, y: z + y, valid_combinations) == indices:\n            return [valid_combinations]\n\n        # filter and remove not possible pon sets\n        for item in valid_combinations:\n            if is_pon(item):\n                count_of_sets = 1\n                count_of_tiles = 0\n                while count_of_sets > count_of_tiles:\n                    count_of_tiles = len([x for x in indices if x == item[0]]) / 3\n                    count_of_sets = len([x for x in valid_combinations\n                                         if x[0] == item[0] and x[1] == item[1] and x[2] == item[2]])\n\n                    if count_of_sets > count_of_tiles:\n                        valid_combinations.remove(item)\n\n        # filter and remove not possible chi sets\n        for item in valid_combinations:\n            if is_chi(item):\n                count_of_sets = 5\n                # TODO calculate real count of possible sets\n                count_of_possible_sets = 4\n                while count_of_sets > count_of_possible_sets:\n                    count_of_sets = len([x for x in valid_combinations\n                                         if x[0] == item[0] and x[1] == item[1] and x[2] == item[2]])\n\n                    if count_of_sets > count_of_possible_sets:\n                        valid_combinations.remove(item)\n\n        # lit of chi\\pon sets for not completed hand\n        if hand_not_completed:\n            return [valid_combinations]\n\n        # hard case - we can build a lot of sets from our tiles\n        # for example we have 123456 tiles and we can build sets:\n        # [1, 2, 3] [4, 5, 6] [2, 3, 4] [3, 4, 5]\n        # and only two of them valid in the same time [1, 2, 3] [4, 5, 6]\n\n        possible_combinations = set(itertools.permutations(\n            range(0, len(valid_combinations)), count_of_needed_combinations\n        ))\n\n        combinations_results = []\n        for combination in possible_combinations:\n            result = []\n            for item in combination:\n                result += valid_combinations[item]\n            result = sorted(result)\n\n            if result == indices:\n                results = []\n                for item in combination:\n                    results.append(valid_combinations[item])\n                results = sorted(results, key=lambda z: z[0])\n                if results not in combinations_results:\n                    combinations_results.append(results)\n\n        return combinations_results", "response": "Find and return all valid set combinations in given suit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting 136 tiles array to one line string", "response": "def to_one_line_string(tiles):\n        \"\"\"\n        Convert 136 tiles array to the one line string\n        Example of output 123s123p123m33z\n        \"\"\"\n        tiles = sorted(tiles)\n\n        man = [t for t in tiles if t < 36]\n\n        pin = [t for t in tiles if 36 <= t < 72]\n        pin = [t - 36 for t in pin]\n\n        sou = [t for t in tiles if 72 <= t < 108]\n        sou = [t - 72 for t in sou]\n\n        honors = [t for t in tiles if t >= 108]\n        honors = [t - 108 for t in honors]\n\n        sou = sou and ''.join([str((i // 4) + 1) for i in sou]) + 's' or ''\n        pin = pin and ''.join([str((i // 4) + 1) for i in pin]) + 'p' or ''\n        man = man and ''.join([str((i // 4) + 1) for i in man]) + 'm' or ''\n        honors = honors and ''.join([str((i // 4) + 1) for i in honors]) + 'z' or ''\n\n        return man + pin + sou + honors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_136_array(tiles):\n        temp = []\n        results = []\n        for x in range(0, 34):\n            if tiles[x]:\n                temp_value = [x * 4] * tiles[x]\n                for tile in temp_value:\n                    if tile in results:\n                        count_of_tiles = len([x for x in temp if x == tile])\n                        new_tile = tile + count_of_tiles\n                        results.append(new_tile)\n\n                        temp.append(tile)\n                    else:\n                        results.append(tile)\n                        temp.append(tile)\n        return results", "response": "Convert 34 array to the 136 array"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef string_to_136_array(sou=None, pin=None, man=None, honors=None, has_aka_dora=False):\n        def _split_string(string, offset, red=None):\n            data = []\n            temp = []\n\n            if not string:\n                return []\n\n            for i in string:\n                if i == 'r' and has_aka_dora:\n                    temp.append(red)\n                    data.append(red)\n                else:\n                    tile = offset + (int(i) - 1) * 4\n                    if tile == red and has_aka_dora:\n                        # prevent non reds to become red\n                        tile += 1\n                    if tile in data:\n                        count_of_tiles = len([x for x in temp if x == tile])\n                        new_tile = tile + count_of_tiles\n                        data.append(new_tile)\n\n                        temp.append(tile)\n                    else:\n                        data.append(tile)\n                        temp.append(tile)\n\n            return data\n\n        results = _split_string(man, 0, FIVE_RED_MAN)\n        results += _split_string(pin, 36, FIVE_RED_PIN)\n        results += _split_string(sou, 72, FIVE_RED_SOU)\n        results += _split_string(honors, 108)\n\n        return results", "response": "Convert one line string tiles format to a 136 array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef string_to_34_array(sou=None, pin=None, man=None, honors=None):\n        results = TilesConverter.string_to_136_array(sou, pin, man, honors)\n        results = TilesConverter.to_34_array(results)\n        return results", "response": "Method to convert one line string tiles format to 34 array"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_34_tile_in_136_array(tile34, tiles):\n        if tile34 is None or tile34 > 33:\n            return None\n\n        tile = tile34 * 4\n\n        possible_tiles = [tile] + [tile + i for i in range(1, 4)]\n\n        found_tile = None\n        for possible_tile in possible_tiles:\n            if possible_tile in tiles:\n                found_tile = possible_tile\n                break\n\n        return found_tile", "response": "Find 34 tile in 136 array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if a set of tiles is agari or not.", "response": "def is_agari(self, tiles_34, open_sets_34=None):\n        \"\"\"\n        Determine was it win or not\n        :param tiles_34: 34 tiles format array\n        :param open_sets_34: array of array of 34 tiles format\n        :return: boolean\n        \"\"\"\n        # we will modify them later, so we need to use a copy\n        tiles = copy.deepcopy(tiles_34)\n\n        # With open hand we need to remove open sets from hand and replace them with isolated pon sets\n        # it will allow to determine agari state correctly\n        if open_sets_34:\n            isolated_tiles = find_isolated_tile_indices(tiles)\n            for meld in open_sets_34:\n                if not isolated_tiles:\n                    break\n\n                isolated_tile = isolated_tiles.pop()\n\n                tiles[meld[0]] -= 1\n                tiles[meld[1]] -= 1\n                tiles[meld[2]] -= 1\n                tiles[isolated_tile] = 3\n\n        j = (1 << tiles[27]) | (1 << tiles[28]) | (1 << tiles[29]) | (1 << tiles[30]) | \\\n            (1 << tiles[31]) | (1 << tiles[32]) | (1 << tiles[33])\n\n        if j >= 0x10:\n            return False\n\n        # 13 orphans\n        if ((j & 3) == 2) and (tiles[0] * tiles[8] * tiles[9] * tiles[17] * tiles[18] *\n                               tiles[26] * tiles[27] * tiles[28] * tiles[29] * tiles[30] *\n                               tiles[31] * tiles[32] * tiles[33] == 2):\n            return True\n\n        # seven pairs\n        if not (j & 10) and sum([tiles[i] == 2 for i in range(0, 34)]) == 7:\n            return True\n\n        if j & 2:\n            return False\n\n        n00 = tiles[0] + tiles[3] + tiles[6]\n        n01 = tiles[1] + tiles[4] + tiles[7]\n        n02 = tiles[2] + tiles[5] + tiles[8]\n\n        n10 = tiles[9] + tiles[12] + tiles[15]\n        n11 = tiles[10] + tiles[13] + tiles[16]\n        n12 = tiles[11] + tiles[14] + tiles[17]\n\n        n20 = tiles[18] + tiles[21] + tiles[24]\n        n21 = tiles[19] + tiles[22] + tiles[25]\n        n22 = tiles[20] + tiles[23] + tiles[26]\n\n        n0 = (n00 + n01 + n02) % 3\n        if n0 == 1:\n            return False\n\n        n1 = (n10 + n11 + n12) % 3\n        if n1 == 1:\n            return False\n\n        n2 = (n20 + n21 + n22) % 3\n        if n2 == 1:\n            return False\n\n        if ((n0 == 2) + (n1 == 2) + (n2 == 2) + (tiles[27] == 2) + (tiles[28] == 2) +\n                (tiles[29] == 2) + (tiles[30] == 2) + (tiles[31] == 2) + (tiles[32] == 2) +\n                (tiles[33] == 2) != 1):\n            return False\n\n        nn0 = (n00 * 1 + n01 * 2) % 3\n        m0 = self._to_meld(tiles, 0)\n        nn1 = (n10 * 1 + n11 * 2) % 3\n        m1 = self._to_meld(tiles, 9)\n        nn2 = (n20 * 1 + n21 * 2) % 3\n        m2 = self._to_meld(tiles, 18)\n\n        if j & 4:\n            return not (n0 | nn0 | n1 | nn1 | n2 | nn2) and self._is_mentsu(m0) \\\n                   and self._is_mentsu(m1) and self._is_mentsu(m2)\n\n        if n0 == 2:\n            return not (n1 | nn1 | n2 | nn2) and self._is_mentsu(m1) and self._is_mentsu(m2) \\\n                   and self._is_atama_mentsu(nn0, m0)\n\n        if n1 == 2:\n            return not (n2 | nn2 | n0 | nn0) and self._is_mentsu(m2) and self._is_mentsu(m0) \\\n                   and self._is_atama_mentsu(nn1, m1)\n\n        if n2 == 2:\n            return not (n0 | nn0 | n1 | nn1) and self._is_mentsu(m0) and self._is_mentsu(m1) \\\n                   and self._is_atama_mentsu(nn2, m2)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value(self):\r\n        return self._sign[1] * self.S0 * norm.cdf(\r\n            self._sign[1] * self.d1, 0.0, 1.0\r\n        ) - self._sign[1] * self.K * np.exp(-self.r * self.T) * norm.cdf(\r\n            self._sign[1] * self.d2, 0.0, 1.0\r\n        )", "response": "Compute option value according to BSM model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef implied_vol(self, value, precision=1.0e-5, iters=100):\r\n\r\n        vol = np.sqrt(2.0 * np.pi / self.T) * (value / self.S0)\r\n        for _ in itertools.repeat(None, iters):  # Faster than range\r\n            opt = BSM(\r\n                S0=self.S0,\r\n                K=self.K,\r\n                T=self.T,\r\n                r=self.r,\r\n                sigma=vol,\r\n                kind=self.kind,\r\n            )\r\n            diff = value - opt.value()\r\n            if abs(diff) < precision:\r\n                return vol\r\n            vol = vol + diff / opt.vega()\r\n        return vol", "response": "Get the implied vol at the specified price using an iterative approach."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an option to the object s options container.", "response": "def add_option(self, K=None, price=None, St=None, kind=\"call\", pos=\"long\"):\r\n        \"\"\"Add an option to the object's `options` container.\"\"\"\r\n        kinds = {\r\n            \"call\": Call,\r\n            \"Call\": Call,\r\n            \"c\": Call,\r\n            \"C\": Call,\r\n            \"put\": Put,\r\n            \"Put\": Put,\r\n            \"p\": Put,\r\n            \"P\": Put,\r\n        }\r\n        St = self.St if St is None else St\r\n        option = kinds[kind](St=St, K=K, price=price, pos=pos)\r\n        self.options.append(option)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef grid(self, start=None, stop=None, St=None, **kwargs):\r\n\r\n        lb = 0.75\r\n        rb = 1.25\r\n        if not any((start, stop, St)) and self.St is None:\r\n            St = np.mean([op.K for op in self.options], axis=0)\r\n            start = St * lb\r\n            stop = St * rb\r\n        elif not any((start, stop)):\r\n            St = self.St if St is None else St\r\n            start = np.max(St) * lb\r\n            stop = np.max(St) * rb\r\n        St = np.linspace(start, stop, **kwargs)\r\n        payoffs = self.payoff(St=St)\r\n        profits = self.profit(St=St)\r\n        return St, payoffs, profits", "response": "Returns a grid of payoff and profit times for a given set of time items."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the solution for the rolling case. Matrix formulation is performed on two dimensional arrays x and y.", "response": "def _rolling_lstsq(x, y):\r\n    \"\"\"Finds solution for the rolling case.  Matrix formulation.\"\"\"\r\n    if x.ndim == 2:\r\n        # Treat everything as 3d and avoid AxisError on .swapaxes(1, 2) below\r\n        # This means an original input of:\r\n        #     array([0., 1., 2., 3., 4., 5., 6.])\r\n        # becomes:\r\n        # array([[[0.],\r\n        #         [1.],\r\n        #         [2.],\r\n        #         [3.]],\r\n        #\r\n        #        [[1.],\r\n        #         [2.],\r\n        #         ...\r\n        x = x[:, :, None]\r\n    elif x.ndim <= 1:\r\n        raise np.AxisError(\"x should have ndmi >= 2\")\r\n    return np.squeeze(\r\n        np.matmul(\r\n            np.linalg.inv(np.matmul(x.swapaxes(1, 2), x)),\r\n            np.matmul(x.swapaxes(1, 2), np.atleast_3d(y)),\r\n        )\r\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _confirm_constant(a):\r\n    a = np.asanyarray(a)\r\n    return np.isclose(a, 1.0).all(axis=0).any()", "response": "Confirm a has volumn vector of 1s."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_constant_params(\r\n    a, has_const=False, use_const=True, rtol=1e-05, atol=1e-08\r\n):\r\n    \"\"\"Helper func to interaction between has_const and use_const params.\r\n\r\n    has_const   use_const   outcome\r\n    ---------   ---------   -------\r\n    True        True        Confirm that a has constant; return a\r\n    False       False       Confirm that a doesn't have constant; return a\r\n    False       True        Confirm that a doesn't have constant; add constant\r\n    True        False       ValueError\r\n    \"\"\"\r\n\r\n    if all((has_const, use_const)):\r\n        if not _confirm_constant(a):\r\n            raise ValueError(\r\n                \"Data does not contain a constant; specify\" \" has_const=False\"\r\n            )\r\n        k = a.shape[-1] - 1\r\n    elif not any((has_const, use_const)):\r\n        if _confirm_constant(a):\r\n            raise ValueError(\r\n                \"Data already contains a constant; specify\" \" has_const=True\"\r\n            )\r\n        k = a.shape[-1]\r\n    elif not has_const and use_const:\r\n        # Also run a quick check to confirm that `a` is *not* ~N(0,1).\r\n        #     In this case, constant should be zero. (exclude it entirely)\r\n        c1 = np.allclose(a.mean(axis=0), b=0.0, rtol=rtol, atol=atol)\r\n        c2 = np.allclose(a.std(axis=0), b=1.0, rtol=rtol, atol=atol)\r\n        if c1 and c2:\r\n            # TODO: maybe we want to just warn here?\r\n            raise ValueError(\r\n                \"Data appears to be ~N(0,1).  Specify\" \" use_constant=False.\"\r\n            )\r\n        # `has_constant` does checking on its own and raises VE if True\r\n        try:\r\n            a = add_constant(a, has_constant=\"raise\")\r\n        except ValueError as e:\r\n            raise ValueError(\r\n                \"X data already contains a constant; please specify\"\r\n                \" has_const=True\"\r\n            ) from e\r\n        k = a.shape[-1] - 1\r\n    else:\r\n        raise ValueError(\"`use_const` == False implies has_const is False.\")\r\n\r\n    return k, a", "response": "Helper function to check that the constant parameters of a list of items are in the correct order."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef condition_number(self):\r\n        # Mimic x = np.matrix(self.x) (deprecated)\r\n        x = np.atleast_2d(self.x)\r\n        ev = np.linalg.eig(x.T @ x)[0]\r\n        return np.sqrt(ev.max() / ev.min())", "response": "Condition number of x ; ratio of largest to smallest eigenvalue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fstat_sig(self):\r\n        return 1.0 - scs.f.cdf(self.fstat, self.df_reg, self.df_err)", "response": "p - value of the F - statistic."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pvalues_all(self):\r\n        return 2.0 * (1.0 - scs.t.cdf(np.abs(self._tstat_all), self.df_err))", "response": "Two - tailed p values for t - stats of all parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rsq_adj(self):\r\n        n = self.n\r\n        k = self.k\r\n        return 1.0 - ((1.0 - self.rsq) * (n - 1.0) / (n - k - 1.0))", "response": "Adjusted R - squared."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _se_all(self):\r\n        x = np.atleast_2d(self.x)\r\n        err = np.atleast_1d(self.ms_err)\r\n        se = np.sqrt(np.diagonal(np.linalg.inv(x.T @ x)) * err[:, None])\r\n        return np.squeeze(se)", "response": "Standard errors for all parameters including the intercept."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ss_tot(self):\r\n        return np.sum(np.square(self.y - self.ybar), axis=0)", "response": "Total sum of squares."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsums of squares of the regression.", "response": "def ss_reg(self):\r\n        \"\"\"Sum of squares of the regression.\"\"\"\r\n        return np.sum(np.square(self.predicted - self.ybar), axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _std_err(self):\r\n        return np.sqrt(np.sum(np.square(self._resids), axis=1) / self._df_err)", "response": "Standard error of the estimate of the resource set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _predicted(self):\r\n        return np.squeeze(\r\n            np.matmul(self.xwins, np.expand_dims(self.solution, axis=-1))\r\n        )", "response": "The predicted values of y."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntotals sum of squares.", "response": "def _ss_tot(self):\r\n        \"\"\"Total sum of squares.\"\"\"\r\n        return np.sum(\r\n            np.square(self.ywins - np.expand_dims(self._ybar, axis=-1)), axis=1\r\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ss_reg(self):\r\n        return np.sum(\r\n            np.square(self._predicted - np.expand_dims(self._ybar, axis=1)),\r\n            axis=1,\r\n        )", "response": "Sum of squares of the regression."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadjusting R - squared.", "response": "def _rsq_adj(self):\r\n        \"\"\"Adjusted R-squared.\"\"\"\r\n        n = self.n\r\n        k = self.k\r\n        return 1.0 - ((1.0 - self._rsq) * (n - 1.0) / (n - k - 1.0))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fstat_sig(self):\r\n        return 1.0 - scs.f.cdf(self._fstat, self._df_reg, self._df_err)", "response": "p - value of the F - statistic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _se_all(self):\r\n        err = np.expand_dims(self._ms_err, axis=1)\r\n        t1 = np.diagonal(\r\n            np.linalg.inv(np.matmul(self.xwins.swapaxes(1, 2), self.xwins)),\r\n            axis1=1,\r\n            axis2=2,\r\n        )\r\n        return np.squeeze(np.sqrt(t1 * err))", "response": "Standard errors ( SE ) for all parameters including the intercept."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _condition_number(self):\r\n        ev = np.linalg.eig(np.matmul(self.xwins.swapaxes(1, 2), self.xwins))[0]\r\n        return np.sqrt(ev.max(axis=1) / ev.min(axis=1))", "response": "Condition number of x ; ratio of largest to smallest eigenvalue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef activeshare(fund, idx, in_format=\"num\"):\r\n\r\n    if not (fund.index.is_unique) and (idx.index.is_unique):\r\n        raise ValueError(\"Inputs must have unique indices.\")\r\n    if isinstance(fund, pd.DataFrame):\r\n        cols = fund.columns\r\n    fund = fund * NUMTODEC[in_format]\r\n    idx = idx * NUMTODEC[in_format]\r\n\r\n    union = fund.index.union(idx.index)\r\n    fund = fund.reindex(union, fill_value=0.0).values\r\n    idx = idx.reindex(union, fill_value=0.0).values\r\n\r\n    if fund.ndim == 1:\r\n        # Resulting active share will be a scalar\r\n        diff = fund - idx\r\n    else:\r\n        diff = fund - idx[:, None]\r\n    act_sh = np.sum(np.abs(diff) * 0.5, axis=0)\r\n    if isinstance(act_sh, np.ndarray):\r\n        act_sh = pd.Series(act_sh, index=cols)\r\n    return act_sh", "response": "Returns the active ahare of a fund versus an index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef amortize(rate, nper, pv, freq=\"M\"):\r\n\r\n    freq = utils.get_anlz_factor(freq)\r\n    rate = rate / freq\r\n    nper = nper * freq\r\n\r\n    periods = np.arange(1, nper + 1, dtype=int)\r\n\r\n    principal = np.ppmt(rate, periods, nper, pv)\r\n    interest = np.ipmt(rate, periods, nper, pv)\r\n    pmt = np.pmt(rate, nper, pv)\r\n\r\n    def balance(pv, rate, nper, pmt):\r\n        dfac = (1 + rate) ** nper\r\n        return pv * dfac - pmt * (dfac - 1) / rate\r\n\r\n    res = pd.DataFrame(\r\n        {\r\n            \"beg_bal\": balance(pv, rate, periods - 1, -pmt),\r\n            \"prin\": principal,\r\n            \"interest\": interest,\r\n            \"end_bal\": balance(pv, rate, periods, -pmt),\r\n        },\r\n        index=periods,\r\n    )[\"beg_bal\", \"prin\", \"interest\", \"end_bal\"]\r\n    return res", "response": "Construct an annualized amortization schedule for a fixed - rate loan."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef corr_heatmap(\r\n    x,\r\n    mask_half=True,\r\n    cmap=\"RdYlGn_r\",\r\n    vmin=-1,\r\n    vmax=1,\r\n    linewidths=0.5,\r\n    square=True,\r\n    figsize=(10, 10),\r\n    **kwargs\r\n):\r\n    \"\"\"Wrapper around seaborn.heatmap for visualizing correlation matrix.\r\n\r\n    Parameters\r\n    ----------\r\n    x : DataFrame\r\n        Underlying data (not a correlation matrix)\r\n    mask_half : bool, default True\r\n        If True, mask (whiteout) the upper right triangle of the matrix\r\n    All other parameters passed to seaborn.heatmap:\r\n    https://seaborn.pydata.org/generated/seaborn.heatmap.html\r\n\r\n    Example\r\n    -------\r\n    # Generate some correlated data\r\n    >>> import numpy as np\r\n    >>> import pandas as pd\r\n    >>> k = 10\r\n    >>> size = 400\r\n    >>> mu = np.random.randint(0, 10, k).astype(float)\r\n    >>> r = np.random.ranf(k ** 2).reshape((k, k)) * 5\r\n    >>> df = pd.DataFrame(np.random.multivariate_normal(mu, r, size=size))\r\n    >>> corr_heatmap(df, figsize=(6, 6))\r\n    \"\"\"\r\n\r\n    if mask_half:\r\n        mask = np.zeros_like(x.corr().values)\r\n        mask[np.triu_indices_from(mask)] = True\r\n    else:\r\n        mask = None\r\n    with sns.axes_style(\"white\"):\r\n        return sns.heatmap(\r\n            x.corr(),\r\n            cmap=cmap,\r\n            vmin=vmin,\r\n            vmax=vmax,\r\n            linewidths=linewidths,\r\n            square=square,\r\n            mask=mask,\r\n            **kwargs\r\n        )", "response": "Returns a seaborn. heatmap that correlates to the given data frame."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of parameters corresponding to the given parameter value.", "response": "def ewm_params(param, param_value):\r\n    \"\"\"Corresponding parameter values for exponentially weighted functions.\r\n\r\n    Parameters\r\n    ----------\r\n    param : {'alpha', 'com', 'span', 'halflife'}\r\n    param_value : float or int\r\n        The parameter value.\r\n\r\n    Returns\r\n    -------\r\n    result : dict\r\n        Layout/index of corresponding parameters.\r\n    \"\"\"\r\n\r\n    if param not in [\"alpha\", \"com\", \"span\", \"halflife\"]:\r\n        raise NameError(\"`param` must be one of {alpha, com, span, halflife}\")\r\n\r\n    def input_alpha(a):\r\n        com = 1.0 / a - 1.0\r\n        span = 2.0 / a - 1.0\r\n        halflife = np.log(0.5) / np.log(1.0 - a)\r\n        return {\"com\": com, \"span\": span, \"halflife\": halflife}\r\n\r\n    def output_alpha(param, p):\r\n        eqs = {\r\n            \"com\": 1.0 / (1.0 + p),\r\n            \"span\": 2.0 / (p + 1.0),\r\n            \"halflife\": 1.0 - np.exp(np.log(0.5) / p),\r\n        }\r\n        return eqs[param]\r\n\r\n    if param == \"alpha\":\r\n        dct = input_alpha(a=param_value)\r\n        alpha = param_value\r\n    else:\r\n        alpha = output_alpha(param=param, p=param_value)\r\n        dct = input_alpha(a=alpha)\r\n    dct.update({\"alpha\": alpha})\r\n    return dct"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ewm_bootstrap(\r\n    a, size=None, com=None, span=None, halflife=None, alpha=None\r\n):\r\n    \"\"\"Bootstrap a new distribution through exponential weighting.\r\n\r\n    Parameters\r\n    ----------\r\n    a : 1-D array-like\r\n        Array from which to generate random sample of elements\r\n    size : int or tuple of ints, default None\r\n        Output shape.  If None, a single value is returned\r\n    com : float, default None\r\n        Center of mass; alpha = 1 / (1 + com), for com \u2265 0\r\n    span : float, default None\r\n        Span parameter; a = 2 / (span + 1), for span \u2265 1\r\n    halflife : float, default None\r\n        Halflife parameter; alpha = 1 \u2212 exp(log(0.5) / halflife),\r\n        for halflife > 0\r\n    alpha : float, default None\r\n        Smoothing factor\r\n\r\n    Example\r\n    -------\r\n    >>> import pandas as pd\r\n    >>> np.random.seed(123)\r\n\r\n    # Our bootstrapped histogram should approximate these freqs\r\n    >>> ewm_weights(10, alpha=.10)\r\n    [ 0.05948221  0.06609135  0.07343483  0.08159426  0.09066029  0.10073365\r\n      0.11192628  0.12436253  0.13818059  0.15353399]\r\n\r\n    >>> res = ewm_bootstrap(np.arange(10), size=int(1e6), alpha=.10)\r\n    >>> res = pd.Series(res).value_counts()\r\n    >>> (res / res.sum()).head()\r\n    9    0.15323\r\n    8    0.13834\r\n    7    0.12424\r\n    6    0.11189\r\n    5    0.10113\r\n    dtype: float64\r\n    \"\"\"\r\n\r\n    if not any((com, span, halflife, alpha)):\r\n        raise ValueError(\"Specify one of `com`, `span`, `halflife`, `alpha`.\")\r\n    p = ewm_weights(\r\n        i=len(a), com=com, span=span, halflife=halflife, alpha=alpha\r\n    )\r\n    res = np.random.choice(a=a, size=size, p=p)\r\n    return res", "response": "Bootstrap a new histogram through exponential weighting."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of the factor exposures generated through OLS regression.", "response": "def factor_loadings(\r\n    r, factors=None, scale=False, pickle_from=None, pickle_to=None\r\n):\r\n    \"\"\"Security factor exposures generated through OLS regression.\r\n\r\n    Incorporates a handful of well-known factors models.\r\n\r\n    Parameters\r\n    ----------\r\n    r : Series or DataFrame\r\n        The left-hand-side variable(s).  If `r` is nx1 shape (a Series or\r\n        single-column DataFrame), the result will be a DataFrame.  If `r` is\r\n        an nxm DataFrame, the result will be a dictionary of DataFrames.\r\n    factors : DataFrame or None, default None\r\n        Factor returns (right-hand-side variables).  If None, factor returns\r\n        are loaded from `pyfinance.datasets.load_factors`\r\n    scale : bool, default False\r\n        If True, cale up/down the volatilities of all factors besides MKT & RF,\r\n        to the vol of MKT.  Both means and the standard deviations are\r\n        multiplied by the scale factor (ratio of MKT.std() to other stdevs)\r\n    pickle_from : str or None, default None\r\n        Passed to `pyfinance.datasets.load_factors` if factors is not None\r\n    pickle_to : str or None, default None\r\n        Passed to `pyfinance.datasets.load_factors` if factors is not None\r\n\r\n    Example\r\n    -------\r\n    # TODO\r\n    \"\"\"\r\n\r\n    # TODO:\r\n    # - Might be appropriate for `returns`, will require higher dimensionality\r\n    # - Option to subtract or not subtract RF (Jensen alpha)\r\n    # - Annualized alpha\r\n    # - Add variance inflation factor to output (method of `ols.OLS`)\r\n    # - Add 'missing=drop' functionality (see statsmodels.OLS)\r\n    # - Take all combinations of factors; which has highest explanatory power\r\n    #       or lowest SSE/MSE?\r\n\r\n    if factors is None:\r\n        factors = datasets.load_factors(\r\n            pickle_from=pickle_from, pickle_to=pickle_to\r\n        )\r\n\r\n    r, factors = utils.constrain(r, factors)\r\n\r\n    if isinstance(r, pd.Series):\r\n        n = 1\r\n        r = r.subtract(factors[\"RF\"])\r\n    elif isinstance(r, pd.DataFrame):\r\n        n = r.shape[1]\r\n        r = r.subtract(np.tile(factors[\"RF\"], (n, 1)).T)\r\n        # r = r.subtract(factors['RF'].values.reshape(-1,1))\r\n    else:\r\n        raise ValueError(\"`r` must be one of (Series, DataFrame)\")\r\n\r\n    if scale:\r\n        # Scale up the volatilities of all factors besides MKT & RF, to the\r\n        # vol of MKT.  Both means and the standard deviations are multiplied\r\n        # by the scale factor (ratio of MKT.std() to other stdevs)\r\n        tgtvol = factors[\"MKT\"].std()\r\n        diff = factors.columns.difference([\"MKT\", \"RF\"])  # don't scale these\r\n        vols = factors[diff].std()\r\n        factors.loc[:, diff] = factors[diff] * tgtvol / vols\r\n\r\n    # Right-hand-side dict of models\r\n    rhs = OrderedDict(\r\n        [\r\n            (\"Capital Asset Pricing Model (CAPM)\", [\"MKT\"]),\r\n            (\"Fama-French 3-Factor Model\", [\"MKT\", \"SMB\", \"HML\"]),\r\n            (\"Carhart 4-Factor Model\", [\"MKT\", \"SMB\", \"HMLD\", \"UMD\"]),\r\n            (\r\n                \"Fama-French 5-Factor Model\",\r\n                [\"MKT\", \"SMB\", \"HMLD\", \"RMW\", \"CMA\"],\r\n            ),\r\n            (\r\n                \"AQR 6-Factor Model\",\r\n                [\"MKT\", \"SMB\", \"HMLD\", \"RMW\", \"CMA\", \"UMD\"],\r\n            ),\r\n            (\"Price-Signal Model\", [\"MKT\", \"UMD\", \"STR\", \"LTR\"])(\r\n                \"Fung-Hsieh Trend-Following Model\",\r\n                [\"BDLB\", \"FXLB\", \"CMLB\", \"STLB\", \"SILB\"],\r\n            ),\r\n        ]\r\n    )\r\n\r\n    # Get union of keys and sort them according to `factors.columns`' order;\r\n    # used later as columns in result\r\n    cols = set(itertools.chain(*rhs.values()))\r\n    cols = [o for o in factors.columns if o in cols] + [\"alpha\", \"rsq_adj\"]\r\n\r\n    # Empty DataFrame to be populated with each regression's attributes\r\n    stats = [\"coef\", \"tstat\"]\r\n    idx = pd.MultiIndex.from_product([rhs.keys(), stats])\r\n    res = pd.DataFrame(columns=cols, index=idx)\r\n\r\n    # Regression calls\r\n    if n > 1:\r\n        # Dict of DataFrames\r\n        d = {}\r\n        for col in r:\r\n            for k, v in rhs.items():\r\n                res = res.copy()\r\n                model = ols.OLS(y=r[col], x=factors[v], hasconst=False)\r\n                res.loc[(k, \"coef\"), factors[v].columns] = model.beta()\r\n                res.loc[(k, \"tstat\"), factors[v].columns] = model.tstat_beta()\r\n                res.loc[(k, \"coef\"), \"alpha\"] = model.alpha()\r\n                res.loc[(k, \"tstat\"), \"alpha\"] = model.tstat_alpha()\r\n                res.loc[(k, \"coef\"), \"rsq_adj\"] = model.rsq_adj()\r\n            d[col] = res\r\n        res = d\r\n\r\n    else:\r\n        # Single DataFrame\r\n        for k, v in rhs.items():\r\n            model = ols.OLS(y=r, x=factors[v], hasconst=False)\r\n            res.loc[(k, \"coef\"), factors[v].columns] = model.beta()\r\n            res.loc[(k, \"tstat\"), factors[v].columns] = model.tstat_beta()\r\n            res.loc[(k, \"coef\"), \"alpha\"] = model.alpha()\r\n            res.loc[(k, \"tstat\"), \"alpha\"] = model.tstat_alpha()\r\n            res.loc[(k, \"coef\"), \"rsq_adj\"] = model.rsq_adj()\r\n\r\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the variance inflation factor for each set of regressors.", "response": "def variance_inflation_factor(regressors, hasconst=False):\r\n    \"\"\"Calculate variance inflation factor (VIF) for each all `regressors`.\r\n\r\n    A wrapper/modification of statsmodels:\r\n    statsmodels.stats.outliers_influence.variance_inflation_factor\r\n\r\n    One recommendation is that if VIF is greater than 5, then the explanatory\r\n    variable `x` is highly collinear with the other explanatory\r\n    variables, and the parameter estimates will have large standard errors\r\n    because of this. [source: StatsModels]\r\n\r\n    Parameters\r\n    ----------\r\n    regressors: DataFrame\r\n        DataFrame containing the entire set of regressors\r\n    hasconst : bool, default False\r\n        If False, a column vector will be added to `regressors` for use in\r\n        OLS\r\n\r\n    Example\r\n    -------\r\n    # Generate some data\r\n    from datetime import date\r\n    from pandas_datareader.data import DataReader as dr\r\n\r\n    syms = {'TWEXBMTH' : 'usd',\r\n            'T10Y2YM' : 'term_spread',\r\n            'PCOPPUSDM' : 'copper'\r\n           }\r\n    start = date(2000, 1, 1)\r\n    data = (dr(syms.keys(), 'fred', start)\r\n            .pct_change()\r\n            .dropna())\r\n    data = data.rename(columns = syms)\r\n\r\n    print(variance_inflation_factor(data))\r\n    usd            1.31609\r\n    term_spread    1.03793\r\n    copper         1.37055\r\n    dtype: float64\r\n    \"\"\"\r\n\r\n    if not hasconst:\r\n        regressors = add_constant(regressors, prepend=False)\r\n    k = regressors.shape[1]\r\n\r\n    def vif_sub(x, regressors):\r\n        x_i = regressors.iloc[:, x]\r\n        mask = np.arange(k) != x\r\n        x_not_i = regressors.iloc[:, mask]\r\n        rsq = linear_model.OLS(x_i, x_not_i, missing=\"drop\").fit().rsquared_adj\r\n        vif = 1.0 / (1.0 - rsq)\r\n        return vif\r\n\r\n    vifs = pd.Series(np.arange(k), index=regressors.columns)\r\n    vifs = vifs.apply(vif_sub, args=(regressors,))\r\n\r\n    # Find the constant column (probably called 'const', but not necessarily\r\n    # and drop it. `is_nonzero_const` borrowed from statsmodels.add_constant\r\n    is_nonzero_const = np.ptp(regressors.values, axis=0) == 0\r\n    is_nonzero_const &= np.all(regressors != 0.0, axis=0)\r\n    vifs.drop(vifs.index[is_nonzero_const], inplace=True)\r\n    return vifs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self):\r\n\r\n        # Defaults/anchors\r\n        best_sse = np.inf\r\n        best_param = (0.0, 1.0)\r\n        best_dist = scs.norm\r\n\r\n        # Compute the histogram of `x`.  density=True gives a probability\r\n        # density function at each bin, normalized such that the integral over\r\n        # the range is 1.0\r\n        hist, bin_edges = np.histogram(self.x, bins=self.bins, density=True)\r\n\r\n        # The results of np.histogram will have len(bin_edges) = len(hist) + 1\r\n        # Find the midpoint at each bin to reduce the size of bin_edges by 1\r\n        bin_edges = (bin_edges + np.roll(bin_edges, -1))[:-1] / 2.0\r\n\r\n        with warnings.catch_warnings():\r\n            warnings.filterwarnings(\"ignore\")\r\n\r\n            sses = []\r\n            params = []\r\n\r\n            for dist in self.distributions:\r\n\r\n                dist = getattr(scs, dist)\r\n\r\n                try:\r\n                    # The generic rv_continuous.fit() returns `mle_tuple`:\r\n                    #    'MLEs for any shape parameters (if applicable),\r\n                    #    followed by those for location and scale.'\r\n                    param = *shape, loc, scale = dist.fit(self.x)\r\n\r\n                    pdf = dist.pdf(bin_edges, loc=loc, scale=scale, *shape)\r\n                    sse = np.sum(np.power(hist - pdf, 2.0))\r\n\r\n                    sses.append(sse)\r\n                    params.append(param)\r\n\r\n                    if best_sse > sse > 0.0:\r\n                        best_dist = dist\r\n                        best_param = param\r\n                        best_sse = sse\r\n                        best_pdf = pdf\r\n\r\n                except (NotImplementedError, AttributeError):\r\n                    sses.append(np.nan)\r\n                    params.append(np.nan)\r\n\r\n        self.best_dist = best_dist\r\n        self.best_param = best_param\r\n        self.best_sse = best_sse\r\n        self.best_pdf = best_pdf\r\n\r\n        self.sses = sses\r\n        self.params = params\r\n        self.hist = hist\r\n        self.bin_edges = bin_edges\r\n\r\n        return self", "response": "Fit each distribution to data and calculate an SSE."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef best(self):\r\n        return pd.Series(\r\n            {\r\n                \"name\": self.best_dist.name,\r\n                \"params\": self.best_param,\r\n                \"sse\": self.best_sse,\r\n            }\r\n        )", "response": "The resulting best - fit distribution its parameters and SSE."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a DataFrame containing all the tested distributions parameters and SSEs.", "response": "def all(self, by=\"name\", ascending=True):\r\n        \"\"\"All tested distributions, their parameters, and SSEs.\"\"\"\r\n        res = pd.DataFrame(\r\n            {\r\n                \"name\": self.distributions,\r\n                \"params\": self.params,\r\n                \"sse\": self.sses,\r\n            }\r\n        )[[\"name\", \"sse\", \"params\"]]\r\n        res.sort_values(by=by, ascending=ascending, inplace=True)\r\n\r\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot(self):\r\n        plt.plot(self.bin_edges, self.hist, self.bin_edges, self.best_pdf)", "response": "Plot the empirical histogram versus best - fit distribution s PDF."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self):\r\n\r\n        self.n_samples, self.n_features = self.ms.shape\r\n        self.u, self.s, self.vt = np.linalg.svd(self.ms, full_matrices=False)\r\n        self.v = self.vt.T\r\n\r\n        # sklearn's implementation is to guarantee that the left and right\r\n        # singular vectors (U and V) are always the same, by imposing the\r\n        # that the largest coefficient of U in absolute value is positive\r\n        # This implementation uses u_based_decision=False rather than the\r\n        # default True to flip that logic and ensure the resulting\r\n        # components and loadings have high positive coefficients\r\n        self.u, self.vt = svd_flip(\r\n            self.u, self.v, u_based_decision=self.u_based_decision\r\n        )\r\n        self.v = self.vt.T\r\n\r\n        # Drop eigenvalues with value > threshold\r\n        # *keep* is number of components retained\r\n        self.eigenvalues = self.s ** 2 / self.n_samples\r\n        self.keep = np.count_nonzero(self.eigenvalues > self.threshold)\r\n\r\n        self.inertia = (self.eigenvalues / self.eigenvalues.sum())[: self.keep]\r\n        self.cumulative_inertia = self.inertia.cumsum()[: self.keep]\r\n        self.eigenvalues = self.eigenvalues[: self.keep]\r\n\r\n        return self", "response": "Fit the model to the given set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef eigen_table(self):\r\n        idx = [\"Eigenvalue\", \"Variability (%)\", \"Cumulative (%)\"]\r\n        table = pd.DataFrame(\r\n            np.array(\r\n                [self.eigenvalues, self.inertia, self.cumulative_inertia]\r\n            ),\r\n            columns=[\"F%s\" % i for i in range(1, self.keep + 1)],\r\n            index=idx,\r\n        )\r\n\r\n        return table", "response": "Returns a table of the entries in the Eigenvalues expl. variance and cumulative expl. variance."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a DataFrame of loadings for each feature.", "response": "def loadings(self):\r\n        \"\"\"Loadings = eigenvectors times sqrt(eigenvalues).\"\"\"\r\n        loadings = self.v[:, : self.keep] * np.sqrt(self.eigenvalues)\r\n        cols = [\"PC%s\" % i for i in range(1, self.keep + 1)]\r\n        loadings = pd.DataFrame(\r\n            loadings, columns=cols, index=self.feature_names\r\n        )\r\n        return loadings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef optimize(self):\r\n\r\n        def te(weights, r, proxies):\r\n            \"\"\"Helper func.  `pyfinance.tracking_error` doesn't work here.\"\"\"\r\n            if isinstance(weights, list):\r\n                weights = np.array(weights)\r\n            proxy = np.sum(proxies * weights, axis=1)\r\n            te = np.std(proxy - r)  # not anlzd...\r\n            return te\r\n\r\n        ew = utils.equal_weights(n=self.n, sumto=self.sumto)\r\n        bnds = tuple((0, 1) for x in range(self.n))\r\n        cons = {\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - self.sumto}\r\n\r\n        xs = []\r\n        funs = []\r\n        for i, j in zip(self._r, self._proxies):\r\n            opt = sco.minimize(\r\n                te,\r\n                x0=ew,\r\n                args=(i, j),\r\n                method=\"SLSQP\",\r\n                bounds=bnds,\r\n                constraints=cons,\r\n            )\r\n            x, fun = opt[\"x\"], opt[\"fun\"]\r\n            xs.append(x)\r\n            funs.append(fun)\r\n        self._xs = np.array(xs)\r\n        self._funs = np.array(funs)\r\n\r\n        return self", "response": "Analogous to sklearn s fit. Returns self to enable chaining."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nforwards - month returns of the replicating portfolio.", "response": "def replicate(self):\r\n        \"\"\"Forward-month returns of the replicating portfolio.\"\"\"\r\n        return np.sum(\r\n            self.proxies[self.window :] * self._xs[:-1], axis=1\r\n        ).reindex(self.r.index)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_to_squeeze(obj, raise_=False):\n\n    if isinstance(obj, pd.Series):\n        return obj\n    elif isinstance(obj, pd.DataFrame) and obj.shape[-1] == 1:\n        return obj.squeeze()\n    else:\n        if raise_:\n            raise ValueError(\"Input cannot be squeezed.\")\n        return obj", "response": "Attempt to squeeze to 1d Series."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an annualized time - relative time for a given object.", "response": "def anlzd_ret(self, freq=None):\n        \"\"\"Annualized (geometric) return.\n\n        Parameters\n        ----------\n        freq : str or None, default None\n            A frequency string used to create an annualization factor.\n            If None, `self.freq` will be used.  If that is also None,\n            a frequency will be inferred.  If none can be inferred,\n            an exception is raised.\n\n            It may be any frequency string or anchored offset string\n            recognized by Pandas, such as 'D', '5D', 'Q', 'Q-DEC', or\n            'BQS-APR'.\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        if self.index.is_all_dates:\n            # TODO: Could be more granular here,\n            #       for cases with < day frequency.\n            td = self.index[-1] - self.index[0]\n            n = td.total_seconds() / SECS_PER_CAL_YEAR\n        else:\n            # We don't have a datetime-like Index, so assume\n            # periods/dates are consecutive and simply count them.\n            # We do, however, need an explicit frequency.\n            freq = freq if freq is not None else self.freq\n            if freq is None:\n                raise FrequencyError(\n                    \"Must specify a `freq` when a\"\n                    \" datetime-like index is not used.\"\n                )\n\n            n = len(self) / utils.get_anlz_factor(freq)\n        return nanprod(self.ret_rels()) ** (1.0 / n) - 1.0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef beta_adj(self, benchmark, adj_factor=2 / 3, **kwargs):\n\n        beta = self.beta(benchmark=benchmark, **kwargs)\n        return adj_factor * beta + (1 - adj_factor)", "response": "Adjusted beta.\n\n        Beta that is adjusted to reflect the tendency of beta to\n        be mean reverting.\n        [Source: CFA Institute]\n\n        Formula:\n        adj_factor * raw_beta + (1 - adj_factor)\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, pd.DataFrame, np.ndarray}\n            The benchmark securitie(s) to which `self` is compared.\n\n        Returns\n        -------\n        float or np.ndarray\n            If `benchmark` is 1d, returns a scalar.\n            If `benchmark` is 2d, returns a 1d ndarray.\n\n        Reference\n        ---------\n        .. _Blume, Marshall.  \"Betas and Their Regression Tendencies.\"\n            http://www.stat.ucla.edu/~nchristo/statistics417/blume_betas.pdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the ratio of upside to downside capture.", "response": "def capture_ratio(self, benchmark, threshold=0.0, compare_op=(\"ge\", \"lt\")):\n        \"\"\"Capture ratio--ratio of upside to downside capture.\n\n        Upside capture ratio divided by the downside capture ratio.\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, 1d np.ndarray}\n            The benchmark security to which `self` is compared.\n        threshold : float, default 0.\n            The threshold at which the comparison should be done.\n            `self` and `benchmark` are \"filtered\" to periods where\n            `benchmark` is greater than/less than `threshold`.\n        compare_op : {tuple, str, list}, default ('ge', 'lt')\n            Comparison operator used to compare to `threshold`.\n            If a sequence, the two elements are passed to\n            `self.up_capture()` and `self.down_capture()`, respectively.\n            If `str`, indicates the comparison operater used in\n            both method calls.\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        if isinstance(compare_op(tuple, list)):\n            op1, op2 = compare_op\n        else:\n            op1, op2 = compare_op, compare_op\n        uc = self.up_capture(\n            benchmark=benchmark, threshold=threshold, compare_op=op1\n        )\n        dc = self.down_capture(\n            benchmark=benchmark, threshold=threshold, compare_op=op2\n        )\n        return uc / dc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new TSeries containing only the samples from self that are below threshold.", "response": "def downmarket_filter(\n        self,\n        benchmark,\n        threshold=0.0,\n        compare_op=\"lt\",\n        include_benchmark=False,\n    ):\n        \"\"\"Drop elementwise samples where `benchmark` > `threshold`.\n\n        Filters `self` (and optionally, `benchmark`) to periods\n        where `benchmark` < `threshold`.  (Or <= `threshold`.)\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, 1d np.ndarray}\n            The benchmark security to which `self` is compared.\n        threshold : float, default 0.0\n            The threshold at which the comparison should be done.\n            `self` and `benchmark` are \"filtered\" to periods where\n            `benchmark` is lt/le `threshold`.\n        compare_op : {'lt', 'le'}\n            Comparison operator used to compare to `threshold`.\n            'lt' is less-than; 'le' is less-than-or-equal.\n        include_benchmark : bool, default False\n            If True, return tuple of (`self`, `benchmark`) both\n            filtered.  If False, return only `self` filtered.\n\n        Returns\n        -------\n        TSeries or tuple of TSeries\n            TSeries if `include_benchmark=False`, otherwise, tuple.\n        \"\"\"\n\n        return self._mkt_filter(\n            benchmark=benchmark,\n            threshold=threshold,\n            compare_op=compare_op,\n            include_benchmark=include_benchmark,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef drawdown_length(self, return_int=False):\n\n        td = self.drawdown_end() - self.drawdown_start()\n        if return_int:\n            return td.days\n        return td", "response": "Returns the length of the drawdown in days."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the length of drawdown recovery in days.", "response": "def drawdown_recov(self, return_int=False):\n        \"\"\"Length of drawdown recovery in days.\n\n        This is the duration from trough to recovery date.\n\n        Parameters\n        ----------\n        return_int : bool, default False\n            If True, return the number of days as an int.\n            If False, return a Pandas Timedelta object.\n\n        Returns\n        -------\n        int or pandas._libs.tslib.Timedelta\n        \"\"\"\n\n        td = self.recov_date() - self.drawdown_end()\n        if return_int:\n            return td.days\n        return td"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef excess_drawdown_idx(self, benchmark, method=\"caer\"):\n\n        # TODO: plot these (compared) in docs.\n        if isinstance(method, (int, float)):\n            method = [\"caer\", \"cger\", \"ecr\", \"ecrr\"][method]\n        method = method.lower()\n\n        if method == \"caer\":\n            er = self.excess_ret(benchmark=benchmark, method=\"arithmetic\")\n            return er.drawdown_idx()\n\n        elif method == \"cger\":\n            er = self.excess_ret(benchmark=benchmark, method=\"geometric\")\n            return er.drawdown_idx()\n\n        elif method == \"ecr\":\n            er = self.ret_idx() - benchmark.ret_idx() + 1\n            if er.isnull().any():\n                return er / er.cummax() - 1.0\n            else:\n                return er / np.maximum.accumulate(er) - 1.0\n\n        elif method == \"ecrr\":\n            # Credit to: SO @piRSquared\n            # https://stackoverflow.com/a/36848867/7954504\n            p = self.ret_idx().values\n            b = benchmark.ret_idx().values\n            er = p - b\n            if er.isnull().any():\n                # The slower route but NaN-friendly.\n                cam = self.expanding(min_periods=1).apply(lambda x: x.argmax())\n            else:\n                cam = utils.cumargmax(er)\n            p0 = p[cam]\n            b0 = b[cam]\n            return (p * b0 - b * p0) / (p0 * b0)\n\n        else:\n            raise ValueError(\n                \"`method` must be one of\"\n                \" ('caer', 'cger', 'ecr', 'ecrr'),\"\n                \" case-insensitive, or\"\n                \" an integer mapping to these methods\"\n                \" (1 thru 4).\"\n            )", "response": "Excess drawdown index ; TSeries of excess drawdowns."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef excess_ret(self, benchmark, method=\"arithmetic\"):\n\n        if method.startswith(\"arith\"):\n            return self - _try_to_squeeze(benchmark)\n        elif method.startswith(\"geo\"):\n            # Geometric excess return,\n            # (1 + `self`) / (1 + `benchmark`) - 1.\n            return (\n                self.ret_rels() / _try_to_squeeze(benchmark).ret_rels() - 1.0\n            )", "response": "Excess return.\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, 1d np.ndarray}\n            The benchmark security to which `self` is compared.\n        method : {{'arith', 'arithmetic'}, {'geo', 'geometric'}}\n            The methodology used.  An arithmetic excess return is a\n            straightforward subtraction.  A geometric excess return\n            is the ratio of return-relatives of `self` to `benchmark`,\n            minus one.\n\n        Also known as: active return.\n\n        Reference\n        ---------\n        .. _Essex River Analytics - A Case for Arithmetic Attribution\n            http://www.northinfo.com/documents/563.pdf\n\n        .. _Bacon, Carl.  Excess Returns - Arithmetic or Geometric?\n            https://www.cfapubs.org/doi/full/10.2469/dig.v33.n1.1235"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the gain - to - loss ratio of positive to negative returns.", "response": "def gain_to_loss_ratio(self):\n        \"\"\"Gain-to-loss ratio, ratio of positive to negative returns.\n\n        Formula:\n        (n pos. / n neg.) * (avg. up-month return / avg. down-month return)\n        [Source: CFA Institute]\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        gt = self > 0\n        lt = self < 0\n        return (nansum(gt) / nansum(lt)) * (self[gt].mean() / self[lt].mean())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pct_negative(self, threshold=0.0):\n\n        return np.count_nonzero(self[self < threshold]) / self.count()", "response": "Return the percentage of periods in which self is less than threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the percentage of periods in which self is greater than threshold.", "response": "def pct_positive(self, threshold=0.0):\n        \"\"\"Pct. of periods in which `self` is greater than `threshold.`\n\n        Parameters\n        ----------\n        threshold : {float, TSeries, pd.Series}, default 0.\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        return np.count_nonzero(self[self > threshold]) / self.count()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef semi_stdev(self, threshold=0.0, ddof=0, freq=None):\n\n        if freq is None:\n            freq = self._try_get_freq()\n            if freq is None:\n                raise FrequencyError(msg)\n        n = self.count() - ddof\n        ss = (nansum(np.minimum(self - threshold, 0.0) ** 2) ** 0.5) / n\n        return ss * freq ** 0.5", "response": "Semi - standard deviation of downside returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns over rf per unit of total risk.", "response": "def sharpe_ratio(self, rf=0.02, ddof=0):\n        \"\"\"Return over `rf` per unit of total risk.\n\n        The average return in excess of the risk-free rate divided\n        by the standard deviation of return; a measure of the average\n        excess return earned per unit of standard deviation of return.\n        [Source: CFA Institute]\n\n        Parameters\n        ----------\n        rf : {float, TSeries, pd.Series}, default 0.02\n            If float, this represents an *compounded annualized*\n            risk-free rate; 2.0% is the default.\n            If a TSeries or pd.Series, this represents a time series\n            of periodic returns to a risk-free security.\n\n            To download a risk-free rate return series using\n            3-month US T-bill yields, see:`pyfinance.datasets.load_rf`.\n        ddof : int, default 0\n            Degrees of freedom, passed to pd.Series.std().\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        rf = self._validate_rf(rf)\n        stdev = self.anlzd_stdev(ddof=ddof)\n        return (self.anlzd_ret() - rf) / stdev"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sortino_ratio(self, threshold=0.0, ddof=0, freq=None):\n\n        stdev = self.semi_stdev(threshold=threshold, ddof=ddof, freq=freq)\n        return (self.anlzd_ret() - threshold) / stdev", "response": "Return over a threshold per unit of downside deviation."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tracking_error(self, benchmark, ddof=0):\n\n        er = self.excess_ret(benchmark=benchmark)\n        return er.anlzd_stdev(ddof=ddof)", "response": "Returns the tracking error of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn over rf per unit of systematic risk.", "response": "def treynor_ratio(self, benchmark, rf=0.02):\n        \"\"\"Return over `rf` per unit of systematic risk.\n\n        A measure of risk-adjusted performance that relates a\n        portfolio's excess returns to the portfolio's beta.\n        [Source: CFA Institute]\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, 1d np.ndarray}\n            The benchmark security to which `self` is compared.\n        rf : {float, TSeries, pd.Series}, default 0.02\n            If float, this represents an *compounded annualized*\n            risk-free rate; 2.0% is the default.\n            If a TSeries or pd.Series, this represents a time series\n            of periodic returns to a risk-free security.\n\n            To download a risk-free rate return series using\n            3-month US T-bill yields, see:`pyfinance.datasets.load_rf`.\n\n        Returns\n        -------\n        float\n        \"\"\"\n\n        benchmark = _try_to_squeeze(benchmark)\n        if benchmark.ndim > 1:\n            raise ValueError(\"Treynor ratio requires a single benchmark\")\n        rf = self._validate_rf(rf)\n        beta = self.beta(benchmark)\n        return (self.anlzd_ret() - rf) / beta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef up_capture(self, benchmark, threshold=0.0, compare_op=\"ge\"):\n\n        slf, bm = self.upmarket_filter(\n            benchmark=benchmark,\n            threshold=threshold,\n            compare_op=compare_op,\n            include_benchmark=True,\n        )\n        return slf.geomean() / bm.geomean()", "response": "Return the up - market capture ratio of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering the internal data to obtain a subset of the internal data for the given time series.", "response": "def upmarket_filter(\n        self,\n        benchmark,\n        threshold=0.0,\n        compare_op=\"ge\",\n        include_benchmark=False,\n    ):\n        \"\"\"Drop elementwise samples where `benchmark` < `threshold`.\n\n        Filters `self` (and optionally, `benchmark`) to periods\n        where `benchmark` > `threshold`.  (Or >= `threshold`.)\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, 1d np.ndarray}\n            The benchmark security to which `self` is compared.\n        threshold : float, default 0.0\n            The threshold at which the comparison should be done.\n            `self` and `benchmark` are \"filtered\" to periods where\n            `benchmark` is gt/ge `threshold`.\n        compare_op : {'ge', 'gt'}\n            Comparison operator used to compare to `threshold`.\n            'gt' is greater-than; 'ge' is greater-than-or-equal.\n        include_benchmark : bool, default False\n            If True, return tuple of (`self`, `benchmark`) both\n            filtered.  If False, return only `self` filtered.\n\n        Returns\n        -------\n        TSeries or tuple of TSeries\n            TSeries if `include_benchmark=False`, otherwise, tuple.\n        \"\"\"\n\n        return self._mkt_filter(\n            benchmark=benchmark,\n            threshold=threshold,\n            compare_op=compare_op,\n            include_benchmark=include_benchmark,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninterfacing to OLS regression against benchmark.", "response": "def CAPM(self, benchmark, has_const=False, use_const=True):\n        \"\"\"Interface to OLS regression against `benchmark`.\n\n        `self.alpha()`, `self.beta()` and several other methods\n        stem from here.  For the full method set, see\n        `pyfinance.ols.OLS`.\n\n        Parameters\n        ----------\n        benchmark : {pd.Series, TSeries, pd.DataFrame, np.ndarray}\n            The benchmark securitie(s) to which `self` is compared.\n        has_const : bool, default False\n            Specifies whether `benchmark` includes a user-supplied constant\n            (a column vector).  If False, it is added at instantiation.\n        use_const : bool, default True\n            Whether to include an intercept term in the model output.  Note the\n            difference between `has_const` and `use_const`: the former\n            specifies whether a column vector of 1s is included in the\n            input; the latter specifies whether the model itself\n            should include a constant (intercept) term.  Exogenous\n            data that is ~N(0,1) would have a constant equal to zero;\n            specify use_const=False in this situation.\n\n        Returns\n        -------\n        pyfinance.ols.OLS\n        \"\"\"\n\n        return ols.OLS(\n            y=self, x=benchmark, has_const=has_const, use_const=use_const\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_13f(url):\r\n\r\n    resp = requests.get(url).text\r\n    data = xmltodict.parse(resp)[\"informationTable\"][\"infoTable\"]\r\n    df = pd.DataFrame(data).drop(\r\n        [\"shrsOrPrnAmt\", \"investmentDiscretion\"], axis=1\r\n    )\r\n    df[\"votingAuthority\"] = df[\"votingAuthority\"].apply(lambda d: d[\"Sole\"])\r\n    df.loc[:, \"value\"] = pd.to_numeric(df[\"value\"], errors=\"coerce\")\r\n    df.loc[:, \"votingAuthority\"] = pd.to_numeric(\r\n        df[\"votingAuthority\"], errors=\"coerce\"\r\n    )\r\n    return df", "response": "Load and parse an SEC. gov - hosted 13F XML file to Pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_factors():\r\n\r\n    # TODO: factors elegible for addition\r\n    #   VIIX, VIIZ, XIV, ZIV, CRP (AQR)\r\n    #   http://www.cboe.com/micro/buywrite/monthendpricehistory.xls ends 2016\r\n    #   could use:\r\n    #   http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/putdailyprice.csv\r\n\r\n    # Warning: slow, kludgy data retrieval follows\r\n    # ------------------------------------------------------------------------\r\n\r\n    # `tgt` will become a list of DataFrames and eventually concatenated\r\n    tgt = []\r\n\r\n    # MKT, SMB, HML, RMW, CMA, RF, UMD, STR, LTR\r\n    facs = [\r\n        \"F-F_Research_Data_5_Factors_2x3\",\r\n        \"F-F_Momentum_Factor\",\r\n        \"F-F_ST_Reversal_Factor\",\r\n        \"F-F_LT_Reversal_Factor\",\r\n    ]\r\n\r\n    for fac in facs:\r\n        tgt.append(pdr.DataReader(fac, \"famafrench\", DSTART)[0])\r\n\r\n    # BETA, ACC, VAR, IVAR require some manipulation to compute returns\r\n    # in the dual-sort method of Fama-French\r\n    for i in [\"BETA\", \"AC\", \"VAR\", \"RESVAR\"]:\r\n        ser = pdr.DataReader(\r\n            \"25_Portfolios_ME_\" + i + \"_5x5\", \"famafrench\", DSTART\r\n        )[0]\r\n        ser = ser.iloc[:, [0, 5, 10, 15, 20]].mean(axis=1) - ser.iloc[\r\n            :, [4, 9, 14, 19, 24]\r\n        ].mean(axis=1)\r\n        ser = ser.rename(i)\r\n        tgt.append(ser)\r\n\r\n    # E/P, CF/P, D/P (univariate sorts, quintile spreads)\r\n    for i in [\"E-P\", \"CF-P\", \"D-P\"]:\r\n        ser = pdr.DataReader(\r\n            \"Portfolios_Formed_on_\" + i, \"famafrench\", DSTART\r\n        )[0]\r\n        ser = ser.loc[:, \"Hi 20\"] - ser.loc[:, \"Lo 20\"]\r\n        ser = ser.rename(i)\r\n        tgt.append(ser)\r\n\r\n    tgt = [df.to_timestamp(how=\"end\") for df in tgt]\r\n\r\n    # BAB, QMJ, HMLD\r\n    # TODO: performance is poor here, runtime is eaten up by these 3\r\n    links = {\r\n        \"BAB\": \"http://bit.ly/2hWyaG8\",\r\n        \"QMJ\": \"http://bit.ly/2hUBSgF\",\r\n        \"HMLD\": \"http://bit.ly/2hdVb7G\",\r\n    }\r\n    for key, value in links.items():\r\n        ser = pd.read_excel(value, header=18, index_col=0)[\"USA\"] * 100\r\n        ser = ser.rename(key)\r\n        tgt.append(ser)\r\n\r\n    # Lookback straddles\r\n    link = \"http://faculty.fuqua.duke.edu/~dah7/DataLibrary/TF-Fac.xls\"\r\n    straddles = pd.read_excel(link, header=14, index_col=0)\r\n    straddles.index = pd.DatetimeIndex(\r\n        straddles.index.astype(str) + \"01\"\r\n    ) + offsets.MonthEnd(1)\r\n    straddles = straddles * 100.0\r\n    tgt.append(straddles)\r\n\r\n    # LIQ\r\n    link = \"http://bit.ly/2pn2oBK\"\r\n    liq = pd.read_csv(\r\n        link,\r\n        skiprows=14,\r\n        delim_whitespace=True,\r\n        header=None,\r\n        usecols=[0, 3],\r\n        index_col=0,\r\n        names=[\"date\", \"LIQ\"],\r\n    )\r\n    liq.index = pd.DatetimeIndex(\r\n        liq.index.astype(str) + \"01\"\r\n    ) + offsets.MonthEnd(1)\r\n    liq = liq.replace(-99, np.nan) * 100.0\r\n    tgt.append(liq)\r\n\r\n    # USD, HY\r\n    fred = pdr.DataReader([\"DTWEXB\", \"BAMLH0A0HYM2\"], \"fred\", DSTART)\r\n    fred = fred.asfreq(\"D\", method=\"ffill\").fillna(method=\"ffill\").asfreq(\"M\")\r\n    fred.loc[:, \"DTWEXB\"] = fred[\"DTWEXB\"].pct_change() * 100.0\r\n    fred.loc[:, \"BAMLH0A0HYM2\"] = fred[\"BAMLH0A0HYM2\"].diff()\r\n    tgt.append(fred)\r\n\r\n    # PUT, BXM, RXM (CBOE options strategy indices)\r\n    link1 = \"http://www.cboe.com/micro/put/put_86-06.xls\"\r\n    link2 = \"http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/putdailyprice.csv\"  # noqa\r\n\r\n    put1 = pd.read_excel(\r\n        link1, index_col=0, skiprows=6, header=None\r\n    ).rename_axis(\"DATE\")\r\n    put2 = pd.read_csv(\r\n        link2, index_col=0, parse_dates=True, skiprows=7, header=None\r\n    ).rename_axis(\"DATE\")\r\n    put = (\r\n        pd.concat((put1, put2))\r\n        .rename(columns={1: \"PUT\"})\r\n        .iloc[:, 0]\r\n        .asfreq(\"D\", method=\"ffill\")\r\n        .fillna(method=\"ffill\")\r\n        .asfreq(\"M\")\r\n        .pct_change()\r\n        * 100.0\r\n    )\r\n    tgt.append(put)\r\n\r\n    link1 = \"http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/bxmarchive.csv\"  # noqa\r\n    link2 = \"http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/bxmcurrent.csv\"  # noqa\r\n\r\n    bxm1 = pd.read_csv(\r\n        link1, index_col=0, parse_dates=True, skiprows=5, header=None\r\n    ).rename_axis(\"DATE\")\r\n    bxm2 = pd.read_csv(\r\n        link2, index_col=0, parse_dates=True, skiprows=4, header=None\r\n    ).rename_axis(\"DATE\")\r\n    bxm = (\r\n        pd.concat((bxm1, bxm2))\r\n        .rename(columns={1: \"BXM\"})\r\n        .iloc[:, 0]\r\n        .asfreq(\"D\", method=\"ffill\")\r\n        .fillna(method=\"ffill\")\r\n        .asfreq(\"M\")\r\n        .pct_change()\r\n        * 100.0\r\n    )\r\n    tgt.append(bxm)\r\n\r\n    link = \"http://www.cboe.com/publish/scheduledtask/mktdata/datahouse/rxm_historical.csv\"  # noqa\r\n    rxm = (\r\n        pd.read_csv(\r\n            link, index_col=0, parse_dates=True, skiprows=2, header=None\r\n        )\r\n        .rename(columns={1: \"RXM\"})\r\n        .rename_axis(\"DATE\")\r\n        .iloc[:, 0]\r\n        .asfreq(\"D\", method=\"ffill\")\r\n        .fillna(method=\"ffill\")\r\n        .asfreq(\"M\")\r\n        .pct_change()\r\n        * 100.0\r\n    )\r\n    tgt.append(rxm)\r\n\r\n    # Clean up data retrieved above\r\n    # -----------------------------------------------------------------\r\n\r\n    factors = pd.concat(tgt, axis=1).round(2)\r\n    newnames = {\r\n        \"Mkt-RF\": \"MKT\",\r\n        \"Mom   \": \"UMD\",\r\n        \"ST_Rev\": \"STR\",\r\n        \"LT_Rev\": \"LTR\",\r\n        \"RESVAR\": \"IVAR\",\r\n        \"AC\": \"ACC\",\r\n        \"PTFSBD\": \"BDLB\",\r\n        \"PTFSFX\": \"FXLB\",\r\n        \"PTFSCOM\": \"CMLB\",\r\n        \"PTFSIR\": \"IRLB\",\r\n        \"PTFSSTK\": \"STLB\",\r\n        \"DTWEXB\": \"USD\",\r\n        \"BAMLH0A0HYM2\": \"HY\",\r\n    }\r\n    factors.rename(columns=newnames, inplace=True)\r\n\r\n    # Get last valid RF date; returns will be constrained to this date\r\n    factors = factors[: factors[\"RF\"].last_valid_index()]\r\n\r\n    # Subtract RF for long-only factors\r\n    subtract = [\"HY\", \"PUT\", \"BXM\", \"RXM\"]\r\n\r\n    for i in subtract:\r\n        factors.loc[:, i] = factors[i] - factors[\"RF\"]\r\n\r\n    return factors", "response": "Returns a new base base"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading interest rates from fred. stlouisfed. org.", "response": "def load_rates(freq=\"D\"):\r\n    \"\"\"Load interest rates from https://fred.stlouisfed.org/.\r\n\r\n    Parameters\r\n    ----------\r\n    reload : bool, default True\r\n        If True, download the data from source rather than loading pickled data\r\n    freq : str {'D', 'W', 'M'}, default 'D'\r\n        Frequency of time series; daily, weekly, or monthly\r\n    start : str or datetime, default '1963', optional\r\n        Start date of time series\r\n    dropna : bool, default True\r\n        If True, drop NaN along rows in resulting DataFrame\r\n    how : str, default 'any'\r\n        Passed to dropna()\r\n\r\n    Original source\r\n    ---------------\r\n    Board of Governors of the Federal Reserve System\r\n    H.15 Selected Interest Rates\r\n    https://www.federalreserve.gov/releases/h15/\r\n    \"\"\"\r\n\r\n    months = [1, 3, 6]\r\n    years = [1, 2, 3, 5, 7, 10, 20, 30]\r\n\r\n    # Nested dictionaries of symbols from fred.stlouisfed.org\r\n    nom = {\r\n        \"D\": [\"DGS%sMO\" % m for m in months] + [\"DGS%s\" % y for y in years],\r\n        \"W\": [\"WGS%sMO\" % m for m in months] + [\"WGS%sYR\" % y for y in years],\r\n        \"M\": [\"GS%sM\" % m for m in months] + [\"GS%s\" % y for y in years],\r\n    }\r\n\r\n    tips = {\r\n        \"D\": [\"DFII%s\" % y for y in years[3:7]],\r\n        \"W\": [\"WFII%s\" % y for y in years[3:7]],\r\n        \"M\": [\"FII%s\" % y for y in years[3:7]],\r\n    }\r\n\r\n    fcp = {\r\n        \"D\": [\"DCPF1M\", \"DCPF2M\", \"DCPF3M\"],\r\n        \"W\": [\"WCPF1M\", \"WCPF2M\", \"WCPF3M\"],\r\n        \"M\": [\"CPF1M\", \"CPF2M\", \"CPF3M\"],\r\n    }\r\n\r\n    nfcp = {\r\n        \"D\": [\"DCPN30\", \"DCPN2M\", \"DCPN3M\"],\r\n        \"W\": [\"WCPN1M\", \"WCPN2M\", \"WCPN3M\"],\r\n        \"M\": [\"CPN1M\", \"CPN2M\", \"CPN3M\"],\r\n    }\r\n\r\n    short = {\r\n        \"D\": [\"DFF\", \"DPRIME\", \"DPCREDIT\"],\r\n        \"W\": [\"FF\", \"WPRIME\", \"WPCREDIT\"],\r\n        \"M\": [\"FEDFUNDS\", \"MPRIME\", \"MPCREDIT\"],\r\n    }\r\n\r\n    rates = list(\r\n        itertools.chain.from_iterable(\r\n            [d[freq] for d in [nom, tips, fcp, nfcp, short]]\r\n        )\r\n    )\r\n    rates = pdr.DataReader(rates, \"fred\", start=DSTART)\r\n\r\n    l1 = (\r\n        [\"Nominal\"] * 11\r\n        + [\"TIPS\"] * 4\r\n        + [\"Fncl CP\"] * 3\r\n        + [\"Non-Fncl CP\"] * 3\r\n        + [\"Short Rates\"] * 3\r\n    )\r\n\r\n    l2 = (\r\n        [\"%sm\" % m for m in months]\r\n        + [\"%sy\" % y for y in years]\r\n        + [\"%sy\" % y for y in years[3:7]]\r\n        + 2 * [\"%sm\" % m for m in range(1, 4)]\r\n        + [\"Fed Funds\", \"Prime Rate\", \"Primary Credit\"]\r\n    )\r\n\r\n    rates.columns = pd.MultiIndex.from_arrays([l1, l2])\r\n\r\n    return rates"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_rf(freq=\"M\"):\r\n\r\n    freqs = \"DWMQA\"\r\n    freq = freq.upper()\r\n    if freq not in freqs:\r\n        raise ValueError(\r\n            \"`freq` must be either a single element or subset\"\r\n            \" from %s, case-insensitive\" % freqs\r\n        )\r\n\r\n    # Load daily 3-Month Treasury Bill: Secondary Market Rate.\r\n    # Note that this is on discount basis and will be converted to BEY.\r\n    # Periodicity is daily.\r\n    rates = (\r\n        pdr.DataReader(\"DTB3\", \"fred\", DSTART)\r\n        .mul(0.01)\r\n        .asfreq(\"D\", method=\"ffill\")\r\n        .fillna(method=\"ffill\")\r\n        .squeeze()\r\n    )\r\n\r\n    # Algebra doesn't 'work' on DateOffsets, don't simplify here!\r\n    minus_one_month = offsets.MonthEnd(-1)\r\n    plus_three_months = offsets.MonthEnd(3)\r\n    trigger = rates.index.is_month_end\r\n    dtm_old = rates.index + minus_one_month + plus_three_months - rates.index\r\n    dtm_new = (\r\n        rates.index.where(trigger, rates.index + minus_one_month)\r\n        + plus_three_months\r\n        - rates.index\r\n    )\r\n\r\n    # This does 2 things in one step:\r\n    # (1) convert discount yield to BEY\r\n    # (2) get the price at that BEY and days to maturity\r\n    # The two equations are simplified\r\n    # See https://people.ucsc.edu/~lbaum/econ80h/LS-Chap009.pdf\r\n    p_old = (100 / 360) * (360 - rates * dtm_old.days)\r\n    p_new = (100 / 360) * (360 - rates * dtm_new.days)\r\n\r\n    res = p_old.pct_change().where(trigger, p_new.pct_change()).dropna()\r\n    # TODO: For purpose of using in TSeries, we should drop upsampled\r\n    #       periods where we don't have the full period constituents.\r\n    return res.add(1.0).resample(freq).prod().sub(1.0)", "response": "Build a risk - free rate return series using 3 - month US T - bill yields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new table with the most recent S&P 500 and interest rate variables.", "response": "def load_shiller():\r\n    \"\"\"Load market & macroeconomic data from Robert Shiller's website.\r\n\r\n    Returns\r\n    -------\r\n    iedata : pd.DataFrame\r\n        Time series of S&P 500 and interest rate variables.\r\n\r\n    Example\r\n    -------\r\n    >>> from pyfinance import datasets\r\n    >>> shiller = datasets.load_shiller()\r\n    >>> shiller.iloc[:7, :5]\r\n                sp50p  sp50d  sp50e      cpi  real_rate\r\n    date\r\n    1871-01-31   4.44   0.26    0.4  12.4641     5.3200\r\n    1871-02-28   4.50   0.26    0.4  12.8446     5.3233\r\n    1871-03-31   4.61   0.26    0.4  13.0350     5.3267\r\n    1871-04-30   4.74   0.26    0.4  12.5592     5.3300\r\n    1871-05-31   4.86   0.26    0.4  12.2738     5.3333\r\n    1871-06-30   4.82   0.26    0.4  12.0835     5.3367\r\n    1871-07-31   4.73   0.26    0.4  12.0835     5.3400\r\n\r\n    .. _ONLINE DATA ROBERT SHILLER:\r\n        http://www.econ.yale.edu/~shiller/data.htm\r\n    \"\"\"\r\n\r\n    xls = \"http://www.econ.yale.edu/~shiller/data/ie_data.xls\"\r\n    cols = [\r\n        \"date\",\r\n        \"sp50p\",\r\n        \"sp50d\",\r\n        \"sp50e\",\r\n        \"cpi\",\r\n        \"frac\",\r\n        \"real_rate\",\r\n        \"real_sp50p\",\r\n        \"real_sp50d\",\r\n        \"real_sp50e\",\r\n        \"cape\",\r\n    ]\r\n    iedata = pd.read_excel(\r\n        xls, sheet_name=\"Data\", skiprows=7, skip_footer=1, names=cols\r\n    ).drop(\"frac\", axis=1)\r\n    dt = iedata[\"date\"].astype(str).str.replace(\".\", \"\") + \"01\"\r\n    iedata[\"date\"] = pd.to_datetime(dt, format=\"%Y%m%d\") + offsets.MonthEnd()\r\n    return iedata.set_index(\"date\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the monthly retail trade data from census. gov.", "response": "def load_retaildata():\r\n    \"\"\"Monthly retail trade data from census.gov.\"\"\"\r\n    # full = 'https://www.census.gov/retail/mrts/www/mrtssales92-present.xls'\r\n    # indiv = 'https://www.census.gov/retail/marts/www/timeseries.html'\r\n\r\n    db = {\r\n        \"Auto, other Motor Vehicle\": \"https://www.census.gov/retail/marts/www/adv441x0.txt\",\r\n        \"Building Material and Garden Equipment and Supplies Dealers\": \"https://www.census.gov/retail/marts/www/adv44400.txt\",\r\n        \"Clothing and Clothing Accessories Stores\": \"https://www.census.gov/retail/marts/www/adv44800.txt\",\r\n        \"Dept. Stores (ex. leased depts)\": \"https://www.census.gov/retail/marts/www/adv45210.txt\",\r\n        \"Electronics and Appliance Stores\": \"https://www.census.gov/retail/marts/www/adv44300.txt\",\r\n        \"Food Services and Drinking Places\": \"https://www.census.gov/retail/marts/www/adv72200.txt\",\r\n        \"Food and Beverage Stores\": \"https://www.census.gov/retail/marts/www/adv44500.txt\",\r\n        \"Furniture and Home Furnishings Stores\": \"https://www.census.gov/retail/marts/www/adv44200.txt\",\r\n        \"Gasoline Stations\": \"https://www.census.gov/retail/marts/www/adv44700.txt\",\r\n        \"General Merchandise Stores\": \"https://www.census.gov/retail/marts/www/adv45200.txt\",\r\n        \"Grocery Stores\": \"https://www.census.gov/retail/marts/www/adv44510.txt\",\r\n        \"Health and Personal Care Stores\": \"https://www.census.gov/retail/marts/www/adv44600.txt\",\r\n        \"Miscellaneous Store Retailers\": \"https://www.census.gov/retail/marts/www/adv45300.txt\",\r\n        \"Motor Vehicle and Parts Dealers\": \"https://www.census.gov/retail/marts/www/adv44100.txt\",\r\n        \"Nonstore Retailers\": \"https://www.census.gov/retail/marts/www/adv45400.txt\",\r\n        \"Retail and Food Services, total\": \"https://www.census.gov/retail/marts/www/adv44x72.txt\",\r\n        \"Retail, total\": \"https://www.census.gov/retail/marts/www/adv44000.txt\",\r\n        \"Sporting Goods, Hobby, Book, and Music Stores\": \"https://www.census.gov/retail/marts/www/adv45100.txt\",\r\n        \"Total (excl. Motor Vehicle)\": \"https://www.census.gov/retail/marts/www/adv44y72.txt\",\r\n        \"Retail (excl. Motor Vehicle and Parts Dealers)\": \"https://www.census.gov/retail/marts/www/adv4400a.txt\",\r\n    }\r\n\r\n    dct = {}\r\n    for key, value in db.items():\r\n        data = pd.read_csv(\r\n            value,\r\n            skiprows=5,\r\n            skip_blank_lines=True,\r\n            header=None,\r\n            sep=\"\\s+\",\r\n            index_col=0,\r\n        )\r\n        try:\r\n            cut = data.index.get_loc(\"SEASONAL\")\r\n        except KeyError:\r\n            cut = data.index.get_loc(\"NO\")\r\n        data = data.iloc[:cut]\r\n        data = data.apply(lambda col: pd.to_numeric(col, downcast=\"float\"))\r\n        data = data.stack()\r\n        year = data.index.get_level_values(0)\r\n        month = data.index.get_level_values(1)\r\n        idx = pd.to_datetime(\r\n            {\"year\": year, \"month\": month, \"day\": 1}\r\n        ) + offsets.MonthEnd(1)\r\n        data.index = idx\r\n        data.name = key\r\n        dct[key] = data\r\n\r\n    sales = pd.DataFrame(dct)\r\n    sales = sales.reindex(\r\n        pd.date_range(sales.index[0], sales.index[-1], freq=\"M\")\r\n    )\r\n    # TODO: account for any skipped months; could specify a DateOffset to\r\n    # `freq` param of `pandas.DataFrame.shift`\r\n    yoy = sales.pct_change(periods=12)\r\n\r\n    return sales, yoy"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef avail(df):\r\n    avail = DataFrame(\r\n        {\r\n            \"start\": df.apply(lambda col: col.first_valid_index()),\r\n            \"end\": df.apply(lambda col: col.last_valid_index()),\r\n        }\r\n    )\r\n    return avail[[\"start\", \"end\"]]", "response": "Return start & end availability for each column in a DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstrain group of DataFrames & Series to intersection of indices.", "response": "def constrain(*objs):\r\n    \"\"\"Constrain group of DataFrames & Series to intersection of indices.\r\n\r\n    Parameters\r\n    ----------\r\n    objs : iterable\r\n        DataFrames and/or Series to constrain\r\n\r\n    Returns\r\n    -------\r\n    new_dfs : list of DataFrames, copied rather than inplace\r\n    \"\"\"\r\n\r\n    # TODO: build in the options to first dropna on each index before finding\r\n    #     intersection, AND to use `dropcol` from this module.  Note that this\r\n    #     would require filtering out Series to which dropcol isn't applicable.\r\n\r\n    # A little bit of set magic below.\r\n    # Note that pd.Index.intersection only applies to 2 Index objects\r\n    common_idx = pd.Index(set.intersection(*[set(o.index) for o in objs]))\r\n    new_dfs = (o.reindex(common_idx) for o in objs)\r\n\r\n    return tuple(new_dfs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstrain a Series or DataFrame to a specified lookback period.", "response": "def constrain_horizon(\r\n    r,\r\n    strict=False,\r\n    cust=None,\r\n    years=0,\r\n    quarters=0,\r\n    months=0,\r\n    days=0,\r\n    weeks=0,\r\n    year=None,\r\n    month=None,\r\n    day=None,\r\n):\r\n\r\n    \"\"\"Constrain a Series/DataFrame to a specified lookback period.\r\n\r\n    See the documentation for dateutil.relativedelta:\r\n    dateutil.readthedocs.io/en/stable/relativedelta.html\r\n\r\n    Parameters\r\n    ----------\r\n    r : DataFrame or Series\r\n        The target pandas object to constrain\r\n    strict : bool, default False\r\n        If True, raise Error if the implied start date on the horizon predates\r\n        the actual start date of `r`.  If False, just return `r` in this\r\n        situation\r\n    years, months, weeks, days : int, default 0\r\n        Relative information; specify as positive to subtract periods.  Adding\r\n        or subtracting a relativedelta with relative information performs\r\n        the corresponding aritmetic operation on the original datetime value\r\n        with the information in the relativedelta\r\n    quarters : int, default 0\r\n        Similar to the other plural relative info periods above, but note that\r\n        this param is custom here.  (It is not a standard relativedelta param)\r\n    year, month, day : int, default None\r\n        Absolute information; specify as positive to subtract periods.  Adding\r\n        relativedelta with absolute information does not perform an aritmetic\r\n        operation, but rather REPLACES the corresponding value in the\r\n        original datetime with the value(s) in relativedelta\r\n    \"\"\"\r\n\r\n    textnum = {\r\n        \"zero\": 0,\r\n        \"one\": 1,\r\n        \"two\": 2,\r\n        \"three\": 3,\r\n        \"four\": 4,\r\n        \"five\": 5,\r\n        \"six\": 6,\r\n        \"seven\": 7,\r\n        \"eight\": 8,\r\n        \"nine\": 9,\r\n        \"ten\": 10,\r\n        \"eleven\": 11,\r\n        \"twelve\": 12,\r\n        \"thirteen\": 13,\r\n        \"fourteen\": 14,\r\n        \"fifteen\": 15,\r\n        \"sixteen\": 16,\r\n        \"seventeen\": 17,\r\n        \"eighteen\": 18,\r\n        \"nineteen\": 19,\r\n        \"twenty\": 20,\r\n        \"twenty four\": 24,\r\n        \"thirty six\": 36,\r\n    }\r\n\r\n    relativedeltas = years, quarters, months, days, weeks, year, month, day\r\n    if cust is not None and any(relativedeltas):\r\n        raise ValueError(\r\n            \"Cannot specify competing (nonzero) values for both\"\r\n            \" `cust` and other parameters.\"\r\n        )\r\n    if cust is not None:\r\n        cust = cust.lower()\r\n\r\n        if cust.endswith(\"y\"):\r\n            years = int(re.search(r\"\\d+\", cust).group(0))\r\n\r\n        elif cust.endswith(\"m\"):\r\n            months = int(re.search(r\"\\d+\", cust).group(0))\r\n\r\n        elif cust.endswith((\"years ago\", \"year ago\", \"year\", \"years\")):\r\n            pos = cust.find(\" year\")\r\n            years = textnum[cust[:pos].replace(\"-\", \"\")]\r\n\r\n        elif cust.endswith((\"months ago\", \"month ago\", \"month\", \"months\")):\r\n            pos = cust.find(\" month\")\r\n            months = textnum[cust[:pos].replace(\"-\", \"\")]\r\n\r\n        else:\r\n            raise ValueError(\"`cust` not recognized.\")\r\n\r\n    # Convert quarters to months & combine for MonthOffset\r\n    months += quarters * 3\r\n\r\n    # Start date will be computed relative to `end`\r\n    end = r.index[-1]\r\n\r\n    # Establish some funky date conventions assumed in finance.  If the end\r\n    # date is 6/30, the date *3 months prior* is 3/31, not 3/30 as would be\r\n    # produced by dateutil.relativedelta.\r\n\r\n    if end.is_month_end and days == 0 and weeks == 0:\r\n        if years != 0:\r\n            years *= 12\r\n            months += years\r\n        start = end - offsets.MonthBegin(months)\r\n    else:\r\n        start = end - offsets.DateOffset(\r\n            years=years,\r\n            months=months,\r\n            days=days - 1,\r\n            weeks=weeks,\r\n            year=year,\r\n            month=month,\r\n            day=day,\r\n        )\r\n    if strict and start < r.index[0]:\r\n        raise ValueError(\r\n            \"`start` pre-dates first element of the Index, %s\" % r.index[0]\r\n        )\r\n    return r[start:end]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cumargmax(a):\r\n\r\n    # Thank you @Alex Riley\r\n    # https://stackoverflow.com/a/40675969/7954504\r\n\r\n    m = np.asarray(np.maximum.accumulate(a))\r\n    if a.ndim == 1:\r\n        x = np.arange(a.shape[0])\r\n    else:\r\n        x = np.repeat(np.arange(a.shape[0])[:, None], a.shape[1], axis=1)\r\n\r\n    x[1:] *= m[:-1] < m[1:]\r\n    np.maximum.accumulate(x, axis=0, out=x)\r\n    return x", "response": "Cumulative argmax.\r\n\r\n    Parameters\r\n    ----------\r\n    a : np.ndarray\r\n\r\n    Returns\r\n    -------\r\n    np.ndarray"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndrop columns that contain NaN within [start, end] inclusive. A wrapper around DataFrame.dropna() that builds an easier *subset* syntax for tseries-indexed DataFrames. Parameters ---------- df : DataFrame start : str or datetime, default None start cutoff date, inclusive end : str or datetime, default None end cutoff date, inclusive Example ------- df = DataFrame(np.random.randn(10,3), index=pd.date_range('2017', periods=10)) # Drop in some NaN df.set_value('2017-01-04', 0, np.nan) df.set_value('2017-01-02', 2, np.nan) df.loc['2017-01-05':, 1] = np.nan # only col2 will be kept--its NaN value falls before `start` print(dropcols(df, start='2017-01-03')) 2 2017-01-01 0.12939 2017-01-02 NaN 2017-01-03 0.16596 2017-01-04 1.06442 2017-01-05 -1.87040 2017-01-06 -0.17160 2017-01-07 0.94588 2017-01-08 1.49246 2017-01-09 0.02042 2017-01-10 0.75094", "response": "def dropcols(df, start=None, end=None):\r\n    \"\"\"Drop columns that contain NaN within [start, end] inclusive.\r\n\r\n    A wrapper around DataFrame.dropna() that builds an easier *subset*\r\n    syntax for tseries-indexed DataFrames.\r\n\r\n    Parameters\r\n    ----------\r\n    df : DataFrame\r\n    start : str or datetime, default None\r\n        start cutoff date, inclusive\r\n    end : str or datetime, default None\r\n        end cutoff date, inclusive\r\n\r\n    Example\r\n    -------\r\n    df = DataFrame(np.random.randn(10,3),\r\n                   index=pd.date_range('2017', periods=10))\r\n\r\n    # Drop in some NaN\r\n    df.set_value('2017-01-04', 0, np.nan)\r\n    df.set_value('2017-01-02', 2, np.nan)\r\n    df.loc['2017-01-05':, 1] = np.nan\r\n\r\n    # only col2 will be kept--its NaN value falls before `start`\r\n    print(dropcols(df, start='2017-01-03'))\r\n                      2\r\n    2017-01-01  0.12939\r\n    2017-01-02      NaN\r\n    2017-01-03  0.16596\r\n    2017-01-04  1.06442\r\n    2017-01-05 -1.87040\r\n    2017-01-06 -0.17160\r\n    2017-01-07  0.94588\r\n    2017-01-08  1.49246\r\n    2017-01-09  0.02042\r\n    2017-01-10  0.75094\r\n\r\n    \"\"\"\r\n\r\n    if isinstance(df, Series):\r\n        raise ValueError(\"func only applies to `pd.DataFrame`\")\r\n    if start is None:\r\n        start = df.index[0]\r\n    if end is None:\r\n        end = df.index[-1]\r\n    subset = df.index[(df.index >= start) & (df.index <= end)]\r\n    return df.dropna(axis=1, subset=subset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dropout(a, p=0.5, inplace=False):\r\n\r\n    dt = a.dtype\r\n    if p == 0.5:\r\n        # Can't pass float dtype to `randint` directly.\r\n        rand = np.random.randint(0, high=2, size=a.shape).astype(dtype=dt)\r\n    else:\r\n        rand = np.random.choice([0, 1], p=[p, 1 - p], size=a.shape).astype(dt)\r\n    if inplace:\r\n        a *= rand\r\n    else:\r\n        return a * rand", "response": "Returns a copy of a with equal size p with proportion p."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _uniquewords(*args):\r\n    words = {}\r\n    n = 0\r\n    for word in itertools.chain(*args):\r\n        if word not in words:\r\n            words[word] = n\r\n            n += 1\r\n    return words", "response": "Dictionary of words to their indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expanding_stdize(obj, **kwargs):\r\n\r\n    return (obj - obj.expanding(**kwargs).mean()) / (\r\n        obj.expanding(**kwargs).std()\r\n    )", "response": "Standardize a pandas object column - wise on expanding window."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the number of periods per year given a frequency.", "response": "def get_anlz_factor(freq):\r\n    \"\"\"Find the number of periods per year given a frequency.\r\n\r\n    Parameters\r\n    ----------\r\n    freq : str\r\n        Any frequency str or anchored offset str recognized by Pandas.\r\n\r\n    Returns\r\n    -------\r\n    float\r\n\r\n    Example\r\n    -------\r\n    >>> get_anlz_factor('D')\r\n    252.0\r\n    >>> get_anlz_factor('5D')  # 5-business-day periods per year\r\n    50.4\r\n\r\n    >>> get_anlz_factor('Q')\r\n    4.0\r\n    >>> get_anlz_factor('Q-DEC')\r\n    4.0\r\n    >>> get_anlz_factor('BQS-APR')\r\n    4.0\r\n    \"\"\"\r\n\r\n    # 'Q-NOV' would give us (2001, 1); we just want (2000, 1).\r\n    try:\r\n        base, mult = get_freq_code(freq)\r\n    except ValueError:\r\n        # The above will fail for a bunch of irregular frequencies, such\r\n        # as 'Q-NOV' or 'BQS-APR'\r\n        freq = freq.upper()\r\n        if freq.startswith((\"A-\", \"BA-\", \"AS-\", \"BAS-\")):\r\n            freq = \"A\"\r\n        elif freq.startswith((\"Q-\", \"BQ-\", \"QS-\", \"BQS-\")):\r\n            freq = \"Q\"\r\n        elif freq in {\"MS\", \"BMS\"}:\r\n            freq = \"M\"\r\n        else:\r\n            raise ValueError(\"Invalid frequency: %s\" % freq)\r\n        base, mult = get_freq_code(freq)\r\n    return PERIODS_PER_YEAR[(base // 1000) * 1000] / mult"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef public_dir(obj, max_underscores=0, type_=None):\r\n\r\n    if max_underscores > 0:\r\n        cond1 = lambda i: not i.startswith(\"_\" * max_underscores)  # noqa\r\n    else:\r\n        cond1 = lambda i: True  # noqa\r\n    if type_:\r\n        if isinstance(type_, str):\r\n            if type_ in [\"callable\", \"func\", \"function\"]:\r\n                type_ = Callable\r\n            elif \"callable\" in type_ or \"func\" in type_:\r\n                type_ = [\r\n                    i\r\n                    if i not in [\"callable\", \"func\", \"function\"]\r\n                    else Callable\r\n                    for i in type_\r\n                ]\r\n        if isinstance(type_, str):\r\n            # 'str' --> str (class `type`)\r\n            type_ = eval(type_)\r\n        elif not isinstance(type_, type):\r\n            type_ = [eval(i) if isinstance(i, str) else i for i in type_]\r\n        # else: we have isinstance(type_, type)\r\n        cond2 = lambda i: isinstance(getattr(obj, i), type_)  # noqa\r\n    else:\r\n        cond2 = lambda i: True  # noqa\r\n    return [i for i in dir(obj) if cond1(i) and cond2(i)]", "response": "Like dir but returns a new object with the same attributes as the object passed in."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate a length - n_tickers list of unique random ticker symbols.", "response": "def random_tickers(\r\n    length, n_tickers, endswith=None, letters=None, slicer=itertools.islice\r\n):\r\n    \"\"\"Generate a length-n_tickers list of unique random ticker symbols.\r\n\r\n    Parameters\r\n    ----------\r\n    length : int\r\n        The length of each ticker string.\r\n    n_tickers : int\r\n        Number of tickers to generate.\r\n    endswith : str, default None\r\n        Specify the ending element(s) of each ticker (for example, 'X').\r\n    letters : sequence, default None\r\n        Sequence of possible letters to choose from.  If None, defaults to\r\n        `string.ascii_uppercase`.\r\n\r\n    Returns\r\n    -------\r\n    list of str\r\n\r\n    Examples\r\n    --------\r\n    >>> from pyfinance import utils\r\n    >>> utils.random_tickers(length=5, n_tickers=4, endswith='X')\r\n    ['UZTFX', 'ROYAX', 'ZBVIX', 'IUWYX']\r\n\r\n    >>> utils.random_tickers(3, 8)\r\n    ['SBW', 'GDF', 'FOG', 'PWO', 'QDH', 'MJJ', 'YZD', 'QST']\r\n    \"\"\"\r\n\r\n    # The trick here is that we need uniqueness.  That defeats the\r\n    #     purpose of using NumPy because we need to generate 1x1.\r\n    #     (Although the alternative is just to generate a \"large\r\n    #     enough\" duplicated sequence and prune from it.)\r\n\r\n    if letters is None:\r\n        letters = string.ascii_uppercase\r\n    if endswith:\r\n        # Only generate substrings up to `endswith`\r\n        length = length - len(endswith)\r\n    join = \"\".join\r\n\r\n    def yield_ticker(rand=random.choices):\r\n        if endswith:\r\n            while True:\r\n                yield join(rand(letters, k=length)) + endswith\r\n        else:\r\n            while True:\r\n                yield join(rand(letters, k=length))\r\n\r\n    tickers = itertools.islice(unique_everseen(yield_ticker()), n_tickers)\r\n    return list(tickers)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef random_weights(size, sumto=1.0):\r\n\r\n    w = np.random.random(size)\r\n    if w.ndim == 2:\r\n        if isinstance(sumto, (np.ndarray, list, tuple)):\r\n            sumto = np.asarray(sumto)[:, None]\r\n        w = sumto * w / w.sum(axis=-1)[:, None]\r\n    elif w.ndim == 1:\r\n        w = sumto * w / w.sum()\r\n    else:\r\n        raise ValueError(\"`w.ndim` must be 1 or 2, not %s\" % w.ndim)\r\n    return w", "response": "Generate an array of random weights that sum to sumto."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new array with the given length as a rolling - window.", "response": "def rolling_windows(a, window):\r\n    \"\"\"Creates rolling-window 'blocks' of length `window` from `a`.\r\n\r\n    Note that the orientation of rows/columns follows that of pandas.\r\n\r\n    Example\r\n    -------\r\n    import numpy as np\r\n    onedim = np.arange(20)\r\n    twodim = onedim.reshape((5,4))\r\n\r\n    print(twodim)\r\n    [[ 0  1  2  3]\r\n     [ 4  5  6  7]\r\n     [ 8  9 10 11]\r\n     [12 13 14 15]\r\n     [16 17 18 19]]\r\n\r\n    print(rwindows(onedim, 3)[:5])\r\n    [[0 1 2]\r\n     [1 2 3]\r\n     [2 3 4]\r\n     [3 4 5]\r\n     [4 5 6]]\r\n\r\n    print(rwindows(twodim, 3)[:5])\r\n    [[[ 0  1  2  3]\r\n      [ 4  5  6  7]\r\n      [ 8  9 10 11]]\r\n\r\n     [[ 4  5  6  7]\r\n      [ 8  9 10 11]\r\n      [12 13 14 15]]\r\n\r\n     [[ 8  9 10 11]\r\n      [12 13 14 15]\r\n      [16 17 18 19]]]\r\n    \"\"\"\r\n\r\n    if window > a.shape[0]:\r\n        raise ValueError(\r\n            \"Specified `window` length of {0} exceeds length of\"\r\n            \" `a`, {1}.\".format(window, a.shape[0])\r\n        )\r\n    if isinstance(a, (Series, DataFrame)):\r\n        a = a.values\r\n    if a.ndim == 1:\r\n        a = a.reshape(-1, 1)\r\n    shape = (a.shape[0] - window + 1, window) + a.shape[1:]\r\n    strides = (a.strides[0],) + a.strides\r\n    windows = np.squeeze(\r\n        np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\r\n    )\r\n    # In cases where window == len(a), we actually want to \"unsqueeze\" to 2d.\r\n    #     I.e., we still want a \"windowed\" structure with 1 window.\r\n    if windows.ndim == 1:\r\n        windows = np.atleast_2d(windows)\r\n    return windows"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unique_everseen(iterable, filterfalse_=itertools.filterfalse):\r\n    # Itertools recipes:\r\n    # https://docs.python.org/3/library/itertools.html#itertools-recipes\r\n    seen = set()\r\n    seen_add = seen.add\r\n    for element in filterfalse_(seen.__contains__, iterable):\r\n        seen_add(element)\r\n        yield element", "response": "Yield unique elements preserving order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uniqify(seq):\r\n\r\n    if PY37:\r\n        # Credit: Raymond Hettinger\r\n        return list(dict.fromkeys(seq))\r\n    else:\r\n        # Credit: Dave Kirby\r\n        # https://www.peterbe.com/plog/uniqifiers-benchmark\r\n        seen = set()\r\n        # We don't care about truth value of `not seen.add(x)`;\r\n        # just there to add to `seen` inplace.\r\n        return [x for x in seq if x not in seen and not seen.add(x)]", "response": "Returns a list of unique elements in a sequence preserving order."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, request):\n        self.app_list = site.get_app_list(request)\n        self.apps_dict = self.create_app_list_dict()\n        # no menu provided\n        items = get_config('MENU')\n        if not items:\n            voices = self.get_default_voices()\n        else:\n            voices = []\n            for item in items:\n                self.add_voice(voices, item)\n\n        return JsonResponse(voices, safe=False)", "response": "Returns a json representing the menu voices\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a voice to the list", "response": "def add_voice(self, voices, item):\n        \"\"\" Adds a voice to the list\n        \"\"\"\n        voice = None\n        if item.get('type') == 'title':\n            voice = self.get_title_voice(item)\n        elif item.get('type') == 'app':\n            voice = self.get_app_voice(item)\n        elif item.get('type') == 'model':\n            voice = self.get_app_model_voice(item)\n        elif item.get('type') == 'free':\n            voice = self.get_free_voice(item)\n        if voice:\n            voices.append(voice)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the js menu compatible voice dict if the user can see it None otherwise", "response": "def get_title_voice(self, item):\n        \"\"\" Title voice\n            Returns the js menu compatible voice dict if the user\n            can see it, None otherwise\n        \"\"\"\n        view = True\n        if item.get('perms', None):\n            view = self.check_user_permission(item.get('perms', []))\n        elif item.get('apps', None):\n            view = self.check_apps_permission(item.get('apps', []))\n        if view:\n            return {\n                'type': 'title',\n                'label': item.get('label', ''),\n                'icon': item.get('icon', None)\n            }\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the js menu compatible voice dict if the user can see it None otherwise", "response": "def get_free_voice(self, item):\n        \"\"\" Free voice\n            Returns the js menu compatible voice dict if the user\n            can see it, None otherwise\n        \"\"\"\n        view = True\n        if item.get('perms', None):\n            view = self.check_user_permission(item.get('perms', []))\n        elif item.get('apps', None):\n            view = self.check_apps_permission(item.get('apps', []))\n\n        if view:\n            return {\n                'type': 'free',\n                'label': item.get('label', ''),\n                'icon': item.get('icon', None),\n                'url': item.get('url', None)\n            }\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the js menu compatible voice dict if the user can see it None otherwise", "response": "def get_app_voice(self, item):\n        \"\"\" App voice\n            Returns the js menu compatible voice dict if the user\n            can see it, None otherwise\n        \"\"\"\n        if item.get('name', None) is None:\n            raise ImproperlyConfigured('App menu voices must have a name key')\n        if self.check_apps_permission([item.get('name', None)]):\n            children = []\n            if item.get('models', None) is None:\n                for name, model in self.apps_dict[item.get('name')]['models'].items(): # noqa\n                    children.append({\n                        'type': 'model',\n                        'label': model.get('name', ''),\n                        'url': model.get('admin_url', '')\n                    })\n            else:\n                for model_item in item.get('models', []):\n                    voice = self.get_model_voice(item.get('name'), model_item)\n                    if voice:\n                        children.append(voice)\n\n            return {\n                'type': 'app',\n                'label': item.get('label', ''),\n                'icon': item.get('icon', None),\n                'children': children\n            }\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_app_model_voice(self, app_model_item):\n        if app_model_item.get('name', None) is None:\n            raise ImproperlyConfigured('Model menu voices must have a name key') # noqa\n\n        if app_model_item.get('app', None) is None:\n            raise ImproperlyConfigured('Model menu voices must have an app key') # noqa\n\n        return self.get_model_voice(app_model_item.get('app'), app_model_item)", "response": "Returns the js menu compatible voice dict for the user s model item"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_model_voice(self, app, model_item):\n        if model_item.get('name', None) is None:\n            raise ImproperlyConfigured('Model menu voices must have a name key') # noqa\n\n        if self.check_model_permission(app, model_item.get('name', None)):\n            return {\n                'type': 'model',\n                'label': model_item.get('label', ''),\n                'icon': model_item.get('icon', None),\n                'url': self.apps_dict[app]['models'][model_item.get('name')]['admin_url'], # noqa\n            }\n\n        return None", "response": "Returns the js menu compatible voice dict if the user can see it None otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a more efficient dictionary from the app_list list obtained from django admin", "response": "def create_app_list_dict(self):\n        \"\"\" Creates a more efficient to check dictionary from\n            the app_list list obtained from django admin\n        \"\"\"\n        d = {}\n        for app in self.app_list:\n            models = {}\n            for model in app.get('models', []):\n                models[model.get('object_name').lower()] = model\n            d[app.get('app_label').lower()] = {\n                'app_url': app.get('app_url', ''),\n                'app_label': app.get('app_label'),\n                'models': models\n            }\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if one of apps is listed in apps_dict", "response": "def check_apps_permission(self, apps):\n        \"\"\" Checks if one of apps is listed in apps_dict\n            Since apps_dict is derived from the app_list\n            given by django admin, it lists only the apps\n            the user can view\n        \"\"\"\n        for app in apps:\n            if app in self.apps_dict:\n                return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if the user can view the app_list", "response": "def check_model_permission(self, app, model):\n        \"\"\" Checks if model is listed in apps_dict\n            Since apps_dict is derived from the app_list\n            given by django admin, it lists only the apps\n            and models the user can view\n        \"\"\"\n        if self.apps_dict.get(app, False) and model in self.apps_dict[app]['models']:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a js menu ready dict from django admin app list", "response": "def get_default_voices(self):\n        \"\"\" When no custom menu is defined in settings\n            Retrieves a js menu ready dict from the django admin app list\n        \"\"\"\n        voices = []\n        for app in self.app_list:\n            children = []\n            for model in app.get('models', []):\n                child = {\n                    'type': 'model',\n                    'label': model.get('name', ''),\n                    'url': model.get('admin_url', '')\n                }\n                children.append(child)\n            voice = {\n                'type': 'app',\n                'label': app.get('name', ''),\n                'url': app.get('app_url', ''),\n                'children': children\n            }\n            voices.append(voice)\n\n        return voices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef startDrag(self, index):\n\n        if not index.isValid():\n            return\n        \n        dataFrame = self.model().dataFrame()\n\n        # get all infos from dataFrame\n        dfindex = dataFrame.iloc[[index.row()]].index\n        columnName = dataFrame.columns[index.column()]\n        dtype = dataFrame[columnName].dtype\n        value = dataFrame[columnName][dfindex]\n\n        # create the mime data\n        mimePayload = PandasCellPayload(\n            dfindex,\n            columnName,\n            value,\n            dtype,\n            hex(id(self.model()))\n        )\n        mimeData = MimeData()\n        mimeData.setData(mimePayload)\n                \n        # create the drag icon and start drag operation\n        drag = QtGui.QDrag(self)\n        drag.setMimeData(mimeData)\n        pixmap = QtGui.QPixmap(\":/icons/insert-table.png\")\n        drag.setHotSpot(QtCore.QPoint(pixmap.width()/3, pixmap.height()/3))\n        drag.setPixmap(pixmap)\n        result = drag.start(Qt.MoveAction)", "response": "start a drag operation with a PandasCellPayload on defined index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the Uuser Interface with all sub widgets.", "response": "def initUi(self):\n        \"\"\"Initalizes the Uuser Interface with all sub widgets.\n\n        \"\"\"\n        self.gridLayout = QtGui.QGridLayout(self)\n        self.gridLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.buttonFrame = QtGui.QFrame(self)\n        #self.buttonFrame.setMinimumSize(QtCore.QSize(250, 50))\n        #self.buttonFrame.setMaximumSize(QtCore.QSize(250, 50))\n        self.buttonFrame.setFrameShape(QtGui.QFrame.NoFrame)\n        spacerItemButton = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)\n\n        self.buttonFrameLayout = QtGui.QGridLayout(self.buttonFrame)\n        self.buttonFrameLayout.setContentsMargins(0, 0, 0, 0)\n\n        self.editButton = QtGui.QToolButton(self.buttonFrame)\n        self.editButton.setObjectName('editbutton')\n        self.editButton.setText(self.tr('edit'))\n        self.editButton.setToolTip(self.tr('toggle editing mode'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/document-edit.png')))\n\n        self.editButton.setIcon(icon)\n\n        self.addColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.addColumnButton.setObjectName('addcolumnbutton')\n        self.addColumnButton.setText(self.tr('+col'))\n        self.addColumnButton.setToolTip(self.tr('add new column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-column-right.png')))\n\n        self.addColumnButton.setIcon(icon)\n\n        self.addRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.addRowButton.setObjectName('addrowbutton')\n        self.addRowButton.setText(self.tr('+row'))\n        self.addRowButton.setToolTip(self.tr('add new row'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-insert-row-below.png')))\n\n        self.addRowButton.setIcon(icon)\n\n        self.removeColumnButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeColumnButton.setObjectName('removecolumnbutton')\n        self.removeColumnButton.setText(self.tr('-col'))\n        self.removeColumnButton.setToolTip(self.tr('remove a column'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-column.png')))\n\n        self.removeColumnButton.setIcon(icon)\n\n        self.removeRowButton = QtGui.QToolButton(self.buttonFrame)\n        self.removeRowButton.setObjectName('removerowbutton')\n        self.removeRowButton.setText(self.tr('-row'))\n        self.removeRowButton.setToolTip(self.tr('remove selected rows'))\n        icon = QtGui.QIcon(QtGui.QPixmap(_fromUtf8(':/icons/edit-table-delete-row.png')))\n\n        self.removeRowButton.setIcon(icon)\n\n        self.buttons = [self.editButton, self.addColumnButton, self.addRowButton, self.removeColumnButton, self.removeRowButton]\n\n        for index, button in enumerate(self.buttons):\n            button.setMinimumSize(self._iconSize)\n            button.setMaximumSize(self._iconSize)\n            button.setIconSize(self._iconSize)\n            button.setCheckable(True)\n            self.buttonFrameLayout.addWidget(button, 0, index, 1, 1)\n        self.buttonFrameLayout.addItem(spacerItemButton, 0, index+1, 1, 1)\n\n        for button in self.buttons[1:]:\n            button.setEnabled(False)\n\n        #self.tableView = QtGui.QTableView(self)\n        self.tableView = DragTable(self)\n        self.tableView.setAlternatingRowColors(True)\n        self.tableView.setSortingEnabled(True)\n        \n        self.gridLayout.addWidget(self.buttonFrame, 0, 0, 1, 1)\n        self.gridLayout.addWidget(self.tableView, 1, 0, 1, 1)\n\n        self.editButton.toggled.connect(self.enableEditing)\n        self.addColumnButton.toggled.connect(self.showAddColumnDialog)\n        self.addRowButton.toggled.connect(self.addRow)\n        self.removeRowButton.toggled.connect(self.removeRow)\n        self.removeColumnButton.toggled.connect(self.showRemoveColumnDialog)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enableEditing(self, enabled):\n        for button in self.buttons[1:]:\n            button.setEnabled(enabled)\n            if button.isChecked():\n                button.setChecked(False)\n\n        model = self.tableView.model()\n\n        if model is not None:\n            model.enableEditing(enabled)", "response": "Enable the editing buttons to add rows and columns and to edit the data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef uncheckButton(self):\n        #for button in self.buttons[1:]:\n        for button in self.buttons:\n            # supress editButtons toggled event\n            button.blockSignals(True)\n            if button.isChecked():\n                button.setChecked(False)\n            button.blockSignals(False)", "response": "Removes the checked stated of all buttons in this widget."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a column with the given parameters to the underlying model.", "response": "def addColumn(self, columnName, dtype, defaultValue):\n        \"\"\"Adds a column with the given parameters to the underlying model\n\n        This method is also a slot.\n        If no model is set, nothing happens.\n\n        Args:\n            columnName (str): The name of the new column.\n            dtype (numpy.dtype): The datatype of the new column.\n            defaultValue (object): Fill the column with this value.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.addDataFrameColumn(columnName, dtype, defaultValue)\n\n        self.addColumnButton.setChecked(False)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndisplay the dialog to add a column to the model.", "response": "def showAddColumnDialog(self, triggered):\n        \"\"\"Display the dialog to add a column to the model.\n\n        This method is also a slot.\n\n        Args:\n            triggered (bool): If the corresponding button was\n                activated, the dialog will be created and shown.\n\n        \"\"\"\n        if triggered:\n            dialog = AddAttributesDialog(self)\n            dialog.accepted.connect(self.addColumn)\n            dialog.rejected.connect(self.uncheckButton)\n            dialog.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addRow(self, triggered):\n        if triggered:\n            model = self.tableView.model()\n            model.addDataFrameRows()\n            self.sender().setChecked(False)", "response": "Adds a row to the model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef removeRow(self, triggered):\n        if triggered:\n            model = self.tableView.model()\n            selection = self.tableView.selectedIndexes()\n\n            rows = [index.row() for index in selection]\n            model.removeDataFrameRows(set(rows))\n            self.sender().setChecked(False)", "response": "Removes a row from the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove one or multiple columns from the model.", "response": "def removeColumns(self, columnNames):\n        \"\"\"Removes one or multiple columns from the model.\n\n        This method is also a slot.\n\n        Args:\n            columnNames (list): A list of columns, which shall\n                be removed from the model.\n\n        \"\"\"\n        model = self.tableView.model()\n\n        if model is not None:\n            model.removeDataFrameColumns(columnNames)\n\n        self.removeColumnButton.setChecked(False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef showRemoveColumnDialog(self, triggered):\n        if triggered:\n            model = self.tableView.model()\n            if model is not None:\n                columns = model.dataFrameColumns()\n                dialog = RemoveAttributesDialog(columns, self)\n                dialog.accepted.connect(self.removeColumns)\n                dialog.rejected.connect(self.uncheckButton)\n                dialog.show()", "response": "Display the dialog to remove column from the model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the model for the enclosed TableView in this widget.", "response": "def setViewModel(self, model):\n        \"\"\"Sets the model for the enclosed TableView in this widget.\n\n        Args:\n            model (DataFrameModel): The model to be displayed by\n                the Table View.\n\n        \"\"\"\n        if isinstance(model, DataFrameModel):\n            self.enableEditing(False)\n            self.uncheckButton()\n            \n            selectionModel = self.tableView.selectionModel()\n            self.tableView.setModel(model)\n            model.dtypeChanged.connect(self.updateDelegate)\n            model.dataChanged.connect(self.updateDelegates)\n            del selectionModel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef createThread(parent, worker, deleteWorkerLater=False):\n    thread = QtCore.QThread(parent)\n    thread.started.connect(worker.doWork)\n    worker.finished.connect(thread.quit)\n    if deleteWorkerLater:\n        thread.finished.connect(worker.deleteLater)\n\n    worker.moveToThread(thread)\n    worker.setParent(parent)\n    return thread", "response": "Create a new thread for given worker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef description(self, value):\n        # lists, tuples, dicts refer to numpy.object types and\n        # return a 'text' description - working as intended or bug?\n        try:\n            value = np.dtype(value)\n        except TypeError as e:\n            return None\n        for (dtype, string) in self._all:\n            if dtype == value:\n                return string\n\n        # no match found return given value\n        return None", "response": "Fetches the translated description for the given datatype."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the datatype for the given text description.", "response": "def dtype(self, value):\n        \"\"\"Gets the datatype for the given `value` (description).\n\n        Args:\n            value (str): A text description for any datatype.\n\n        Returns:\n            numpy.dtype: The matching datatype for the given text.\n            None: If no match can be found, `None` will be returned.\n\n        \"\"\"\n        for (dtype, string) in self._all:\n            if string == value:\n                return dtype\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfill all NaN values of a column with an empty string", "response": "def fillNoneValues(column):\n    \"\"\"Fill all NaN/NaT values of a column with an empty string\n\n    Args:\n        column (pandas.Series): A Series object with all rows.\n\n    Returns:\n        column: Series with filled NaN values.\n    \"\"\"\n    if column.dtype == object:\n        column.fillna('', inplace=True)\n    return column"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convertTimestamps(column):\n    tempColumn = column\n\n    try:\n        # Try to convert the first row and a random row instead of the complete\n        # column, might be faster\n        # tempValue = np.datetime64(column[0])\n        tempValue = np.datetime64(column[randint(0, len(column.index) - 1)])\n        tempColumn = column.apply(to_datetime)\n    except Exception:\n        pass\n    return tempColumn", "response": "Convert a dtype of a given column to a datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef superReadCSV(filepath, first_codec='UTF_8', usecols=None,\n                 low_memory=False, dtype=None, parse_dates=True,\n                 sep=',', chunksize=None, verbose=False, **kwargs):\n\n        \"\"\"\n        A wrap to pandas read_csv with mods to accept a dataframe or\n        filepath. returns dataframe untouched, reads filepath and returns\n        dataframe based on arguments.\n        \"\"\"\n\n        if isinstance(filepath, pd.DataFrame):\n            return filepath\n\n        assert isinstance(first_codec, str), \"first_codec must be a string\"\n\n        codecs = ['UTF_8', 'ISO-8859-1', 'ASCII', 'UTF_16', 'UTF_32']\n        try:\n            codecs.remove(first_codec)\n        except ValueError as not_in_list:\n            pass\n        codecs.insert(0, first_codec)\n        errors = []\n        for c in codecs:\n            try:\n\n                return pd.read_csv(filepath,\n                                   usecols=usecols,\n                                   low_memory=low_memory,\n                                   encoding=c,\n                                   dtype=dtype,\n                                   parse_dates=parse_dates,\n                                   sep=sep,\n                                   chunksize=chunksize,\n                                   **kwargs)\n            # Need to catch `UnicodeError` here, not just `UnicodeDecodeError`,\n            # because pandas 0.23.1 raises it when decoding with UTF_16 and the\n            # file is not in that format:\n            except (UnicodeError, UnboundLocalError) as e:\n                errors.append(e)\n            except Exception as e:\n                errors.append(e)\n                if 'tokenizing' in str(e):\n                    pass\n                else:\n                    raise\n        if verbose:\n            [print(e) for e in errors]\n\n        raise UnicodeDecodeError(\"Tried {} codecs and failed on all: \\n CODECS: {} \\n FILENAME: {}\".format(\n                        len(codecs), codecs, os.path.basename(filepath)))", "response": "A wrapper to pandas. read_csv that wraps pandas. read_csv with mods to accept a dataframe or\n        filepath. returns dataframe untouched reads filepath and returns dataframe untouched reads filepath and returns dataframe based on arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef identify_sep(filepath):\n    ext = os.path.splitext(filepath)[1].lower()\n    allowed_exts = ['.csv', '.txt', '.tsv']\n    assert ext in ['.csv', '.txt'], \"Unexpected file extension {}. \\\n                                    Supported extensions {}\\n filename: {}\".format(\n                                    ext, allowed_exts, os.path.basename(filepath))\n    maybe_seps = ['|',\n                  ';',\n                  ',',\n                  '\\t',\n                  ':']\n\n    with open(filepath,'r') as fp:\n        header = fp.__next__()\n\n    count_seps_header = {sep: _count(sep, header) for sep in maybe_seps}\n    count_seps_header = {sep: count for sep,\n                         count in count_seps_header.items() if count > 0}\n\n    if count_seps_header:\n        return max(count_seps_header.__iter__(),\n                   key=(lambda key: count_seps_header[key]))\n    else:\n        raise Exception(\"Couldn't identify the sep from the header... here's the information:\\n HEADER: {}\\n SEPS SEARCHED: {}\".format(header, maybe_seps))", "response": "Identifies the separator of data in a filepath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread a file into a dataframe.", "response": "def superReadFile(filepath, **kwargs):\n    \"\"\"\n    Uses pandas.read_excel (on excel files) and returns a dataframe of the\n    first sheet (unless sheet is specified in kwargs)\n    Uses superReadText (on .txt,.tsv, or .csv files) and returns a dataframe of\n    the data. One function to read almost all types of data files.\n    \"\"\"\n    if isinstance(filepath, pd.DataFrame):\n        return filepath\n\n    ext = os.path.splitext(filepath)[1].lower()\n\n    if ext in ['.xlsx', '.xls']:\n        df = pd.read_excel(filepath, **kwargs)\n\n    elif ext in ['.pkl', '.p', '.pickle', '.pk']:\n        df = pd.read_pickle(filepath)\n\n    else:\n        # Assume it's a text-like file and try to read it.\n        try:\n            df = superReadText(filepath, **kwargs)\n        except Exception as e:\n            # TODO: Make this trace back better? Custom Exception? Raise original?\n            raise Exception(\"Error reading file: {}\".format(e))\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dedupe_cols(frame):\n\n    cols = list(frame.columns)\n    for i, item in enumerate(frame.columns):\n        if item in frame.columns[:i]:\n            cols[i] = \"toDROP\"\n    frame.columns = cols\n    return frame.drop(\"toDROP\", 1, errors='ignore')", "response": "Remove all columns that have the same name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a list of strings and appends 2 3 4 etc to duplicates. Never append a 0 or 1.", "response": "def rename_dupe_cols(cols):\n    \"\"\"\n    Takes a list of strings and appends 2,3,4 etc to duplicates. Never\n    appends a 0 or 1. Appended #s are not always in order...but if you wrap\n    this in a dataframe.to_sql function you're guaranteed to not have dupe\n    column name errors importing data to SQL...you'll just have to check\n    yourself to see which fields were renamed.\n    \"\"\"\n    counts = {}\n    positions = {pos: fld for pos, fld in enumerate(cols)}\n\n    for c in cols:\n        if c in counts.keys():\n            counts[c] += 1\n        else:\n            counts[c] = 1\n\n    fixed_cols = {}\n\n    for pos, col in positions.items():\n        if counts[col] > 1:\n            fix_cols = {pos: fld for pos, fld in positions.items() if fld == col}\n            keys = [p for p in fix_cols.keys()]\n            min_pos = min(keys)\n            cnt = 1\n            for p, c in fix_cols.items():\n                if not p == min_pos:\n                    cnt += 1\n                    c = c + str(cnt)\n                    fixed_cols.update({p: c})\n\n    positions.update(fixed_cols)\n\n    cols = [x for x in positions.values()]\n\n    return cols"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a widget used to edit the item specified by index for editing.", "response": "def createEditor(self, parent, option, index):\n        \"\"\"Returns the widget used to edit the item specified by index for editing. The parent widget and style option are used to control how the editor widget appears.\n\n        Args:\n            parent (QWidget): parent widget.\n            option (QStyleOptionViewItem): controls how editor widget appears.\n            index (QModelIndex): model data index.\n        \"\"\"\n        editor = BigIntSpinbox(parent)\n        try:\n            editor.setMinimum(self.minimum)\n            editor.setMaximum(self.maximum)\n            editor.setSingleStep(self.singleStep)\n        except TypeError as err:\n            # initiate the editor with default values\n            pass\n        return editor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setEditorData(self, spinBox, index):\n        if index.isValid():\n            value = index.model().data(index, QtCore.Qt.EditRole)\n            spinBox.setValue(value)", "response": "Sets the data to be displayed and edited by the editor widget from the model item specified by the index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new editor widget for the item specified by index.", "response": "def createEditor(self, parent, option, index):\n        \"\"\"Returns the widget used to edit the item specified by index for editing. The parent widget and style option are used to control how the editor widget appears.\n\n        Args:\n            parent (QWidget): parent widget.\n            option (QStyleOptionViewItem): controls how editor widget appears.\n            index (QModelIndex): model data index.\n        \"\"\"\n        editor = QtGui.QDoubleSpinBox(parent)\n        try:\n            editor.setMinimum(self.minimum)\n            editor.setMaximum(self.maximum)\n            editor.setSingleStep(self.singleStep)\n            editor.setDecimals(self.decimals)\n        except TypeError as err:\n            # initiate the spinbox with default values.\n            pass\n        return editor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setModelData(self, spinBox, model, index):\n        spinBox.interpretText()\n        value = spinBox.value()\n        model.setData(index, value, QtCore.Qt.EditRole)", "response": "Sets the data from the editor widget and stores it in the specified model at the item index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the widget used to edit the item specified by index for editing.", "response": "def createEditor(self, parent, option, index):\n        \"\"\"Returns the widget used to edit the item specified by index for editing. The parent widget and style option are used to control how the editor widget appears.\n\n        Args:\n            parent (QWidget): parent widget.\n            option (QStyleOptionViewItem): controls how editor widget appears.\n            index (QModelIndex): model data index.\n        \"\"\"\n        editor = QtGui.QLineEdit(parent)\n        return editor"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the data from the editor widget and stores it in the specified model at the item index.", "response": "def setModelData(self, editor, model, index):\n        \"\"\"Gets data from the editor widget and stores it in the specified model at the item index.\n\n        Args:\n            editor (QtGui.QLineEdit): editor widget.\n            model (QAbstractItemModel): parent model.\n            index (QModelIndex): model data index.\n        \"\"\"\n        if index.isValid():\n            value = editor.text()\n            model.setData(index, value, QtCore.Qt.EditRole)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an Editor Widget for the given index.", "response": "def createEditor(self, parent, option, index):\n        \"\"\"Creates an Editor Widget for the given index.\n\n        Enables the user to manipulate the displayed data in place. An editor\n        is created, which performs the change.\n        The widget used will be a `QComboBox` with all available datatypes in the\n        `pandas` project.\n\n        Args:\n            parent (QtCore.QWidget): Defines the parent for the created editor.\n            option (QtGui.QStyleOptionViewItem): contains all the information\n                that QStyle functions need to draw the items.\n            index (QtCore.QModelIndex): The item/index which shall be edited.\n\n        Returns:\n            QtGui.QWidget: he widget used to edit the item specified by index\n                for editing.\n\n        \"\"\"\n        combo = QtGui.QComboBox(parent)\n        combo.addItems(SupportedDtypes.names())\n        combo.currentIndexChanged.connect(self.currentIndexChanged)\n        return combo"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setEditorData(self, editor, index):\n        editor.blockSignals(True)\n        data = index.data()\n        dataIndex = editor.findData(data)\n        # dataIndex = editor.findData(data, role=Qt.EditRole)\n        editor.setCurrentIndex(dataIndex)\n        editor.blockSignals(False)", "response": "Sets the current data for the editor for the given index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setModelData(self, editor, model, index):\n        model.setData(index, editor.itemText(editor.currentIndex()))", "response": "Updates the model after changing the data in the editor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setData(self, data):\n        try:\n            bytestream = pickle.dumps(data)\n            super(MimeData, self).setData(self._mimeType, bytestream)\n        except TypeError:\n            raise TypeError(self.tr(\"can not pickle added data\"))\n        except:\n            raise", "response": "Add some data.\n        \n        Args:\n            data (object): Object to add as data. This object has to be pickable. \n                Qt objects don't work!\n        \n        Raises:\n            TypeError if data is not pickable"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn stored data as pickle object", "response": "def data(self):\n        \"\"\"return stored data\n        \n        Returns:\n            unpickled data\n        \"\"\"\n        try:\n            bytestream = super(MimeData, self).data(self._mimeType).data()\n            return pickle.loads(bytestream)\n        except:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accepted(self):\n        #return super(DataFrameExportDialog, self).accepted\n        try:\n            self._saveModel()\n        except Exception as err:\n            self._statusBar.showMessage(str(err))\n            raise\n        else:\n            self._resetWidgets()\n            self.exported.emit(True)\n            self.accept()", "response": "This method is called when the user is accepted the DataFrame export dialog."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rejected(self):\n        self._resetWidgets()\n        self.exported.emit(False)\n        self.reject()", "response": "Close the widget and reset its inital state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stepBy(self, steps):\n        self.setValue(self.value() + steps*self.singleStep())", "response": "Set the value of the record by a given number of steps."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stepEnabled(self):\n        if self.value() > self.minimum() and self.value() < self.maximum():\n            return self.StepUpEnabled | self.StepDownEnabled\n        elif self.value() <= self.minimum():\n            return self.StepUpEnabled\n        elif self.value() >= self.maximum():\n            return self.StepDownEnabled", "response": "Virtual function that determines whether stepping up and down is legal at any given time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setMaximum(self, maximum):\n        if not isinstance(maximum, int):\n            raise TypeError(\"Argument is not of type int or long\")\n        self._maximum = maximum", "response": "setter to _maximum.\n\n        Args:\n            maximum (int or long): new _maximum value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef save_file(self, filepath, save_as=None, keep_orig=False, **kwargs):\n        df = self._models[filepath].dataFrame()\n        kwargs['index'] = kwargs.get('index', False)\n\n        if save_as is not None:\n            to_path = save_as\n        else:\n            to_path = filepath\n\n        ext = os.path.splitext(to_path)[1].lower()\n\n        if ext == \".xlsx\":\n            kwargs.pop('sep', None)\n            df.to_excel(to_path, **kwargs)\n\n        elif ext in ['.csv','.txt']:\n            df.to_csv(to_path, **kwargs)\n\n        else:\n            raise NotImplementedError(\"Cannot save file of type {}\".format(ext))\n\n        if save_as is not None:\n            if  keep_orig is False:\n                # Re-purpose the original model\n                # Todo - capture the DataFrameModelManager._updates too\n                model = self._models.pop(filepath)\n                model._filePath = to_path\n            else:\n                # Create a new model.\n                model = DataFrameModel()\n                model.setDataFrame(df, copyDataFrame=True, filePath=to_path)\n\n            self._models[to_path] = model", "response": "Saves a DataFrameModel to a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a DataFrameModel and registers it to the given file_path.", "response": "def set_model(self, df_model, file_path):\n        \"\"\"\n        Sets a DataFrameModel and registers it to the given file_path.\n        :param df_model: (DataFrameModel)\n            The DataFrameModel to register.\n        :param file_path:\n            The file path to associate with the DataFrameModel.\n            *Overrides the current filePath on the DataFrameModel (if any)\n        :return: None\n        \"\"\"\n        assert isinstance(df_model, DataFrameModel), \"df_model argument must be a DataFrameModel!\"\n        df_model._filePath = file_path\n\n        try:\n            self._models[file_path]\n        except KeyError:\n            self.signalNewModelRead.emit(file_path)\n\n        self._models[file_path] = df_model"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_file(self, filepath, df, notes=None):\n        assert isinstance(df, pd.DataFrame), \"Cannot update file with type '{}'\".format(type(df))\n\n        self._models[filepath].setDataFrame(df, copyDataFrame=False)\n\n        if notes:\n            update = dict(date=pd.Timestamp(datetime.datetime.now()),\n                                                     notes=notes)\n\n            self._updates[filepath].append(update)\n        self._paths_updated.append(filepath)", "response": "Updates the DataFrame for the specified filepath with the given DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_file(self, filepath):\n        self._models.pop(filepath)\n        self._updates.pop(filepath, default=None)\n        self.signalModelDestroyed.emit(filepath)", "response": "Removes the DataFrameModel from being registered."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a filepath into a DataFrameModel and registers it in the self. _models dict.", "response": "def read_file(self, filepath, **kwargs):\n        \"\"\"\n        Reads a filepath into a DataFrameModel and registers\n        it.\n        Example use:\n            dfmm = DataFrameModelManger()\n            dfmm.read_file(path_to_file)\n            dfm = dfmm.get_model(path_to_file)\n            df = dfm.get_frame(path_to_file)\n\n        :param filepath: (str)\n            The filepath to read\n        :param kwargs:\n            .xlsx files: pandas.read_excel(**kwargs)\n            .csv files: pandas.read_csv(**kwargs)\n        :return: DataFrameModel\n        \"\"\"\n        try:\n            model = self._models[filepath]\n        except KeyError:\n            model = read_file(filepath, **kwargs)\n            self._models[filepath] = model\n            self.signalNewModelRead.emit(filepath)\n        finally:\n            self._paths_read.append(filepath)\n\n        return self._models[filepath]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef excepthook(excType, excValue, tracebackobj):\n    separator = '-' * 80\n\n    logFile = os.path.join(tempfile.gettempdir(), \"error.log\")\n\n    notice = \"An unhandled exception occurred. Please report the problem.\\n\"\n\n    notice += \"\"\"A log has been written to \"{}\".\\n\\nError information:\"\"\".format(logFile)\n\n    timeString = time.strftime(\"%Y-%m-%d, %H:%M:%S\")\n\n    tbinfofile = io.StringIO()\n    traceback.print_tb(tracebackobj, None, tbinfofile)\n    tbinfofile.seek(0)\n    tbinfo = tbinfofile.read()\n    if python_version < 3:\n        # Python3 has no str().decode()\n        tbinfo = tbinfo.decode('utf-8')\n    else:\n        pass\n\n    try:\n        if python_version < 3:\n            # Python3 has no str().decode()\n            excValueStr = str(excValue).decode('utf-8')\n        else:\n            excValueStr = str(excValue)\n\n    except UnicodeEncodeError:\n        excValueStr = str(excValue)\n\n    errmsg = '{0}: \\n{1}'.format(excType, excValueStr)\n\n    sections = ['\\n', separator, timeString, separator,\n                errmsg, separator, tbinfo]\n    try:\n        msg = '\\n'.join(sections)\n\n    except TypeError:\n        # Remove all things not string.\n        sections = [item for item in sections if type(item) == str]\n        msg = '\\n'.join(sections)\n\n    try:\n        f = codecs.open(logFile, \"a+\", encoding='utf-8')\n        f.write(msg)\n        f.close()\n    except IOError:\n        msgbox(\"unable to write to {0}\".format(logFile), \"Writing error\")\n\n    # always show an error message\n    try:\n        if not _isQAppRunning():\n            app = QtGui.QApplication([])\n        _showMessageBox(str(notice) + str(msg))\n    except Exception:\n        msgbox(str(notice) + str(msg), \"Error\")", "response": "This function is used to catch unhandled exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exception_format():\n    return \"\".join(traceback.format_exception(\n        sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\n    ))", "response": "Convert exception info into a string suitable for display."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure that every string within input_list is unique.", "response": "def uniquify_list_of_strings(input_list):\n    \"\"\"\n    Ensure that every string within input_list is unique.\n    :param list input_list: List of strings\n    :return: New list with unique names as needed.\n    \"\"\"\n    output_list = list()\n    for i, item in enumerate(input_list):\n        tempList = input_list[:i] + input_list[i + 1:]\n        if item not in tempList:\n            output_list.append(item)\n        else:\n            output_list.append('{0}_{1}'.format(item, i))\n    return output_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_hotkey(text):\n\n    ret_val = [text, None, None]  # Default return values\n    if text is None:\n        return ret_val\n\n    # Single character, remain visible\n    res = re.search('(?<=\\[).(?=\\])', text)\n    if res:\n        start = res.start(0)\n        end = res.end(0)\n        caption = text[:start - 1] + text[start:end] + text[end + 1:]\n        ret_val = [caption, text[start:end], start - 1]\n\n    # Single character, hide it\n    res = re.search('(?<=\\[\\[).(?=\\]\\])', text)\n    if res:\n        start = res.start(0)\n        end = res.end(0)\n        caption = text[:start - 2] + text[end + 2:]\n        ret_val = [caption, text[start:end], None]\n\n    # a Keysym.  Always hide it\n    res = re.search('(?<=\\[\\<).+(?=\\>\\])', text)\n    if res:\n        start = res.start(0)\n        end = res.end(0)\n        caption = text[:start - 2] + text[end + 2:]\n        ret_val = [caption, '<{}>'.format(text[start:end]), None]\n\n    return ret_val", "response": "Extract a desired hotkey from the text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload in an image file and return as a tk Image.", "response": "def load_tk_image(filename):\n    \"\"\"\n    Load in an image file and return as a tk Image.\n\n    :param filename: image filename to load\n    :return: tk Image object\n    \"\"\"\n\n    if filename is None:\n        return None\n\n    if not os.path.isfile(filename):\n        raise ValueError('Image file {} does not exist.'.format(filename))\n\n    tk_image = None\n\n    filename = os.path.normpath(filename)\n    _, ext = os.path.splitext(filename)\n\n    try:\n        pil_image = PILImage.open(filename)\n        tk_image = PILImageTk.PhotoImage(pil_image)\n    except:\n        try:\n            # Fallback if PIL isn't available\n            tk_image = tk.PhotoImage(file=filename)\n        except:\n            msg = \"Cannot load {}.  Check to make sure it is an image file.\".format(filename)\n            try:\n                _ = PILImage\n            except:\n                msg += \"\\nPIL library isn't installed.  If it isn't installed, only .gif files can be used.\"\n            raise ValueError(msg)\n    return tk_image"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setDataFrame(self, dataFrame):\n        if not isinstance(dataFrame, pandas.core.frame.DataFrame):\n            raise TypeError('Argument is not of type pandas.core.frame.DataFrame')\n\n        self.layoutAboutToBeChanged.emit()\n        self._dataFrame = dataFrame\n        self.layoutChanged.emit()", "response": "setter function to _dataFrame. Holds all the data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setEditable(self, editable):\n        if not isinstance(editable, bool):\n            raise TypeError('Argument is not of type bool')\n        self._editable = editable", "response": "setter to _editable. apply changes while changing dtype.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the data stored in the model at the given index.", "response": "def data(self, index, role=Qt.DisplayRole):\n        \"\"\"Retrieve the data stored in the model at the given `index`.\n\n        Args:\n            index (QtCore.QModelIndex): The model index, which points at a\n                data object.\n            role (Qt.ItemDataRole, optional): Defaults to `Qt.DisplayRole`. You\n                have to use different roles to retrieve different data for an\n                `index`. Accepted roles are `Qt.DisplayRole`, `Qt.EditRole` and\n                `DTYPE_ROLE`.\n\n        Returns:\n            None if an invalid index is given, the role is not accepted by the\n            model or the column is greater than `1`.\n            The column name will be returned if the given column number equals `0`\n            and the role is either `Qt.DisplayRole` or `Qt.EditRole`.\n            The datatype will be returned, if the column number equals `1`. The\n            `Qt.DisplayRole` or `Qt.EditRole` return a human readable, translated\n            string, whereas the `DTYPE_ROLE` returns the raw data type.\n\n        \"\"\"\n\n        # an index is invalid, if a row or column does not exist or extends\n        # the bounds of self.columnCount() or self.rowCount()\n        # therefor a check for col>1 is unnecessary.\n        if not index.isValid():\n            return None\n\n        col = index.column()\n\n        #row = self._dataFrame.columns[index.column()]\n        columnName = self._dataFrame.columns[index.row()]\n        columnDtype = self._dataFrame[columnName].dtype\n\n        if role == Qt.DisplayRole or role == Qt.EditRole:\n            if col == 0:\n                if columnName == index.row():\n                    return index.row()\n                return columnName\n            elif col == 1:\n                return SupportedDtypes.description(columnDtype)\n        elif role == DTYPE_ROLE:\n            if col == 1:\n                return columnDtype\n            else:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setData(self, index, value, role=DTYPE_CHANGE_ROLE):\n        if role != DTYPE_CHANGE_ROLE or not index.isValid():\n            return False\n\n        if not self.editable():\n            return False\n\n        self.layoutAboutToBeChanged.emit()\n\n        dtype = SupportedDtypes.dtype(value)\n        currentDtype = np.dtype(index.data(role=DTYPE_ROLE))\n\n        if dtype is not None:\n            if dtype != currentDtype:\n                # col = index.column()\n                # row = self._dataFrame.columns[index.column()]\n                columnName = self._dataFrame.columns[index.row()]\n\n                try:\n                    if dtype == np.dtype('<M8[ns]'):\n                        if currentDtype in SupportedDtypes.boolTypes():\n                            raise Exception(\"Can't convert a boolean value into a datetime value.\")\n                        self._dataFrame[columnName] = self._dataFrame[columnName].apply(pandas.to_datetime)\n                    else:\n                        self._dataFrame[columnName] = self._dataFrame[columnName].astype(dtype)\n                    self.dtypeChanged.emit(index.row(), dtype)\n                    self.layoutChanged.emit()\n\n                    return True\n                except Exception:\n                    message = 'Could not change datatype %s of column %s to datatype %s' % (currentDtype, columnName, dtype)\n                    self.changeFailed.emit(message, index, dtype)\n                    raise\n                    # self._dataFrame[columnName] = self._dataFrame[columnName].astype(currentDtype)\n                    # self.layoutChanged.emit()\n                    # self.dtypeChanged.emit(columnName)\n                    #raise NotImplementedError, \"dtype changing not fully working, original error:\\n{}\".format(e)\n        return False", "response": "Updates the datatype of a column."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the item flags for the given index as ored value e. x.", "response": "def flags(self, index):\n        \"\"\"Returns the item flags for the given index as ored value, e.x.: Qt.ItemIsUserCheckable | Qt.ItemIsEditable\n\n        Args:\n            index (QtCore.QModelIndex): Index to define column and row\n\n        Returns:\n            for column 'column': Qt.ItemIsSelectable | Qt.ItemIsEnabled\n            for column 'data type': Qt.ItemIsSelectable | Qt.ItemIsEnabled | Qt.ItemIsEditable\n\n        \"\"\"\n        if not index.isValid():\n            return Qt.NoItemFlags\n\n        col = index.column()\n\n        flags = Qt.ItemIsEnabled | Qt.ItemIsSelectable\n\n        if col > 0 and self.editable():\n            flags = Qt.ItemIsSelectable | Qt.ItemIsEnabled | Qt.ItemIsEditable\n\n        return flags"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying the filter to the stored dataframe and returns the index of the objects that match the given string.", "response": "def search(self):\n        \"\"\"Applies the filter to the stored dataframe.\n\n        A safe environment dictionary will be created, which stores all allowed\n        functions and attributes, which may be used for the filter.\n        If any object in the given `filterString` could not be found in the\n        dictionary, the filter does not apply and returns `False`.\n\n        Returns:\n            tuple: A (indexes, success)-tuple, which indicates identified objects\n                by applying the filter and if the operation was successful in\n                general.\n\n        \"\"\"\n        # there should be a grammar defined and some lexer/parser.\n        # instead of this quick-and-dirty implementation.\n\n        safeEnvDict = {\n            'freeSearch': self.freeSearch,\n            'extentSearch': self.extentSearch,\n            'indexSearch': self.indexSearch\n\n        }\n        for col in self._dataFrame.columns:\n            safeEnvDict[col] = self._dataFrame[col]\n\n        try:\n            searchIndex = eval(self._filterString, {\n                               '__builtins__': None}, safeEnvDict)\n        except NameError:\n            return [], False\n        except SyntaxError:\n            return [], False\n        except ValueError:\n            # the use of 'and'/'or' is not valid, need to use binary operators.\n            return [], False\n        except TypeError:\n            # argument must be string or compiled pattern\n            return [], False\n        return searchIndex, True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a free text search for all columns in the dataframe.", "response": "def freeSearch(self, searchString):\n        \"\"\"Execute a free text search for all columns in the dataframe.\n\n        Parameters\n        ----------\n            searchString (str): Any string which may be contained in a column.\n\n        Returns\n        -------\n            list: A list containing all indexes with filtered data. Matches\n                will be `True`, the remaining items will be `False`. If the\n                dataFrame is empty, an empty list will be returned.\n        \"\"\"\n\n        if not self._dataFrame.empty:\n            # set question to the indexes of data and set everything to false.\n            question = self._dataFrame.index == -9999\n            for column in self._dataFrame.columns:\n                dfColumn = self._dataFrame[column]\n                dfColumn = dfColumn.apply(str)\n\n                question2 = dfColumn.str.contains(searchString,\n                                                  flags=re.IGNORECASE,\n                                                  regex=True, na=False)\n                question = np.logical_or(question, question2)\n\n            return question\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter the data by a geographical bounding box.", "response": "def extentSearch(self, xmin, ymin, xmax, ymax):\n        \"\"\"Filters the data by a geographical bounding box.\n\n        The bounding box is given as lower left point coordinates and upper\n        right point coordinates.\n\n        Note:\n            It's necessary that the dataframe has a `lat` and `lng` column\n            in order to apply the filter.\n\n            Check if the method could be removed in the future. (could be done\n            via freeSearch)\n\n        Returns\n        -------\n            list: A list containing all indexes with filtered data. Matches\n                will be `True`, the remaining items will be `False`. If the\n                dataFrame is empty, an empty list will be returned.\n\n        \"\"\"\n        if not self._dataFrame.empty:\n            try:\n                questionMin = (self._dataFrame.lat >= xmin) & (\n                    self._dataFrame.lng >= ymin)\n                questionMax = (self._dataFrame.lat <= xmax) & (\n                    self._dataFrame.lng <= ymax)\n                return np.logical_and(questionMin, questionMax)\n            except AttributeError:\n                return []\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfilters the data by a list of indexes.", "response": "def indexSearch(self, indexes):\n        \"\"\"Filters the data by a list of indexes.\n\n        Args:\n            indexes (list of int): List of index numbers to return.\n\n        Returns:\n            list: A list containing all indexes with filtered data. Matches\n            will be `True`, the remaining items will be `False`. If the\n            dataFrame is empty, an empty list will be returned.\n\n        \"\"\"\n\n        if not self._dataFrame.empty:\n            filter0 = self._dataFrame.index == -9999\n            for index in indexes:\n                filter1 = self._dataFrame.index == index\n                filter0 = np.logical_or(filter0, filter1)\n\n            return filter0\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a data file into a DataFrameModel.", "response": "def read_file(filepath, **kwargs):\n    \"\"\"\n    Read a data file into a DataFrameModel.\n\n    :param filepath: The rows/columns filepath to read.\n    :param kwargs:\n            xls/x files - see pandas.read_excel(**kwargs)\n            .csv/.txt/etc - see pandas.read_csv(**kwargs)\n    :return: DataFrameModel\n    \"\"\"\n    return DataFrameModel(dataFrame=superReadFile(filepath, **kwargs),\n                          filePath=filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread SQL query or database table into a DataFrameModel.", "response": "def read_sql(sql, con, filePath, index_col=None, coerce_float=True,\n             params=None, parse_dates=None, columns=None, chunksize=None):\n    \"\"\"\n    Read SQL query or database table into a DataFrameModel.\n    Provide a filePath argument in addition to the *args/**kwargs from\n    pandas.read_sql and get a DataFrameModel.\n\n    NOTE: The chunksize option is overridden to None always (for now).\n\n    Reference:\n    http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.read_sql.html\n    pandas.read_sql(sql, con, index_col=None, coerce_float=True,\n                    params=None, parse_dates=None, columns=None, chunksize=None)\n\n\n\n    :return: DataFrameModel\n    \"\"\"\n\n    # TODO: Decide if chunksize is worth keeping and how to handle?\n    df = pandas.read_sql(sql, con, index_col, coerce_float,\n                    params, parse_dates, columns, chunksize=None)\n    return DataFrameModel(df, filePath=filePath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the model s dataFrame by reading a file.", "response": "def setDataFrameFromFile(self, filepath, **kwargs):\n        \"\"\"\n        Sets the model's dataFrame by reading a file.\n        Accepted file formats:\n            - .xlsx (sheet1 is read unless specified in kwargs)\n            - .csv (comma separated unless specified in kwargs)\n            - .txt (any separator)\n\n        :param filepath: (str)\n            The path to the file to be read.\n        :param kwargs:\n            pandas.read_csv(**kwargs) or pandas.read_excel(**kwargs)\n        :return: None\n        \"\"\"\n        df = superReadFile(filepath, **kwargs)\n        self.setDataFrame(df, filePath=filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the _dataFrame property of the base class.", "response": "def setDataFrame(self, dataFrame, copyDataFrame=False, filePath=None):\n        \"\"\"\n        Setter function to _dataFrame. Holds all data.\n\n        Note:\n            It's not implemented with python properties to keep Qt conventions.\n\n        Raises:\n            TypeError: if dataFrame is not of type pandas.core.frame.DataFrame.\n\n        Args:\n            dataFrame (pandas.core.frame.DataFrame): assign dataFrame to _dataFrame. Holds all the data displayed.\n            copyDataFrame (bool, optional): create a copy of dataFrame or use it as is. defaults to False.\n                If you use it as is, you can change it from outside otherwise you have to reset the dataFrame\n                after external changes.\n\n        \"\"\"\n        if not isinstance(dataFrame, pandas.core.frame.DataFrame):\n            raise TypeError(\"not of type pandas.core.frame.DataFrame\")\n\n        self.layoutAboutToBeChanged.emit()\n        if copyDataFrame:\n            self._dataFrame = dataFrame.copy()\n        else:\n            self._dataFrame = dataFrame\n\n        self._columnDtypeModel = ColumnDtypeModel(dataFrame)\n        self._columnDtypeModel.dtypeChanged.connect(self.propagateDtypeChanges)\n        self._columnDtypeModel.changeFailed.connect(\n            lambda columnName, index, dtype: self.changingDtypeFailed.emit(columnName, index, dtype)\n        )\n        if filePath is not None:\n            self._filePath = filePath\n        self.layoutChanged.emit()\n        self.dataChanged.emit()\n        self.dataFrameChanged.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef timestampFormat(self, timestampFormat):\n        if not isinstance(timestampFormat, str):\n            raise TypeError('not of type unicode')\n        #assert isinstance(timestampFormat, unicode) or timestampFormat.__class__.__name__ == \"DateFormat\", \"not of type unicode\"\n        self._timestampFormat = timestampFormat", "response": "Setter to _timestampFormat. Formatting string for conversion of timestamps to QtCore. QDateTime. Used in data method."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rename(self, index=None, columns=None, **kwargs):\n        kwargs['inplace'] = True\n        self.layoutAboutToBeChanged.emit()\n        self._dataFrame.rename(index, columns, **kwargs)\n        self.layoutChanged.emit()\n        self.dataChanged.emit()\n        self.dataFrameChanged.emit()", "response": "Renames the dataframe in - place calling appropriate signals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef applyFunction(self, func):\n        assert callable(func), \"function {} is not callable\".format(func)\n        self.layoutAboutToBeChanged.emit()\n        df = func(self._dataFrame)\n        assert isinstance(df, pandas.DataFrame), \"function {} did not return a DataFrame.\".format(func.__name__)\n        self._dataFrame = df\n        self.layoutChanged.emit()\n        self.dataChanged.emit()\n        self.dataFrameChanged.emit()", "response": "Applies a function to the dataFrame with appropriate signals."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the header depending on section orientation and role", "response": "def headerData(self, section, orientation, role=Qt.DisplayRole):\n        \"\"\"\n        Return the header depending on section, orientation and Qt::ItemDataRole\n\n        Args:\n            section (int): For horizontal headers, the section number corresponds to the column number.\n                Similarly, for vertical headers, the section number corresponds to the row number.\n            orientation (Qt::Orientations):\n            role (Qt::ItemDataRole):\n\n        Returns:\n            None if not Qt.DisplayRole\n            _dataFrame.columns.tolist()[section] if orientation == Qt.Horizontal\n            section if orientation == Qt.Vertical\n            None if horizontal orientation and section raises IndexError\n        \"\"\"\n        if role != Qt.DisplayRole:\n            return None\n\n        if orientation == Qt.Horizontal:\n            try:\n                label = self._dataFrame.columns.tolist()[section]\n                if label == section:\n                    label = section\n                return label\n            except (IndexError, ):\n                return None\n        elif orientation == Qt.Vertical:\n            return section"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data of the item at the given index and role.", "response": "def data(self, index, role=Qt.DisplayRole):\n        \"\"\"return data depending on index, Qt::ItemDataRole and data type of the column.\n\n        Args:\n            index (QtCore.QModelIndex): Index to define column and row you want to return\n            role (Qt::ItemDataRole): Define which data you want to return.\n\n        Returns:\n            None if index is invalid\n            None if role is none of: DisplayRole, EditRole, CheckStateRole, DATAFRAME_ROLE\n\n            if role DisplayRole:\n                unmodified _dataFrame value if column dtype is object (string or unicode).\n                _dataFrame value as int or long if column dtype is in _intDtypes.\n                _dataFrame value as float if column dtype is in _floatDtypes. Rounds to defined precision (look at: _float16_precision, _float32_precision).\n                None if column dtype is in _boolDtypes.\n                QDateTime if column dtype is numpy.timestamp64[ns]. Uses timestampFormat as conversion template.\n\n            if role EditRole:\n                unmodified _dataFrame value if column dtype is object (string or unicode).\n                _dataFrame value as int or long if column dtype is in _intDtypes.\n                _dataFrame value as float if column dtype is in _floatDtypes. Rounds to defined precision (look at: _float16_precision, _float32_precision).\n                _dataFrame value as bool if column dtype is in _boolDtypes.\n                QDateTime if column dtype is numpy.timestamp64[ns]. Uses timestampFormat as conversion template.\n\n            if role CheckStateRole:\n                Qt.Checked or Qt.Unchecked if dtype is numpy.bool_ otherwise None for all other dtypes.\n\n            if role DATAFRAME_ROLE:\n                unmodified _dataFrame value.\n\n            raises TypeError if an unhandled dtype is found in column.\n        \"\"\"\n\n        if not index.isValid():\n            return None\n\n        def convertValue(row, col, columnDtype):\n            value = None\n            if columnDtype == object:\n                value = self._dataFrame.ix[row, col]\n            elif columnDtype in self._floatDtypes:\n                value = round(float(self._dataFrame.ix[row, col]), self._float_precisions[str(columnDtype)])\n            elif columnDtype in self._intDtypes:\n                value = int(self._dataFrame.ix[row, col])\n            elif columnDtype in self._boolDtypes:\n                # TODO this will most likely always be true\n                # See: http://stackoverflow.com/a/715455\n                # well no: I am mistaken here, the data is already in the dataframe\n                # so its already converted to a bool\n                value = bool(self._dataFrame.ix[row, col])\n\n            elif columnDtype in self._dateDtypes:\n                #print numpy.datetime64(self._dataFrame.ix[row, col])\n                value = pandas.Timestamp(self._dataFrame.ix[row, col])\n                value = QtCore.QDateTime.fromString(str(value), self.timestampFormat)\n                #print value\n            # else:\n            #     raise TypeError, \"returning unhandled data type\"\n            return value\n\n        row = self._dataFrame.index[index.row()]\n        col = self._dataFrame.columns[index.column()]\n        columnDtype = self._dataFrame[col].dtype\n\n        if role == Qt.DisplayRole:\n            # return the value if you wanne show True/False as text\n            if columnDtype == numpy.bool:\n                result = self._dataFrame.ix[row, col]\n            else:\n                result = convertValue(row, col, columnDtype)\n        elif role  == Qt.EditRole:\n            result = convertValue(row, col, columnDtype)\n        elif role  == Qt.CheckStateRole:\n            if columnDtype == numpy.bool_:\n                if convertValue(row, col, columnDtype):\n                    result = Qt.Checked\n                else:\n                    result = Qt.Unchecked\n            else:\n                result = None\n        elif role == DATAFRAME_ROLE:\n            result = self._dataFrame.ix[row, col]\n        else:\n            result = None\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the item flags for the given index as ored value e. x..: Qt. ItemIsEditable | Qt. ItemIsSelectable | Qt. ItemIsEnabled | Qt. ItemIsEditable | Qt. ItemIsEditable", "response": "def flags(self, index):\n        \"\"\"Returns the item flags for the given index as ored value, e.x.: Qt.ItemIsUserCheckable | Qt.ItemIsEditable\n\n        If a combobox for bool values should pop up ItemIsEditable have to set for bool columns too.\n\n        Args:\n            index (QtCore.QModelIndex): Index to define column and row\n\n        Returns:\n            if column dtype is not boolean Qt.ItemIsSelectable | Qt.ItemIsEnabled | Qt.ItemIsEditable\n            if column dtype is boolean Qt.ItemIsSelectable | Qt.ItemIsEnabled | Qt.ItemIsUserCheckable\n        \"\"\"\n        flags = super(DataFrameModel, self).flags(index)\n\n        if not self.editable:\n            return flags\n\n        col = self._dataFrame.columns[index.column()]\n        if self._dataFrame[col].dtype == numpy.bool:\n            flags |= Qt.ItemIsUserCheckable\n        else:\n            # if you want to have a combobox for bool columns set this\n            flags |= Qt.ItemIsEditable\n\n        return flags"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value to the index position depending on the data type of the column and row.", "response": "def setData(self, index, value, role=Qt.DisplayRole):\n        \"\"\"Set the value to the index position depending on Qt::ItemDataRole and data type of the column\n\n        Args:\n            index (QtCore.QModelIndex): Index to define column and row.\n            value (object): new value.\n            role (Qt::ItemDataRole): Use this role to specify what you want to do.\n\n        Raises:\n            TypeError: If the value could not be converted to a known datatype.\n\n        Returns:\n            True if value is changed. Calls layoutChanged after update.\n            False if value is not different from original value.\n\n        \"\"\"\n        if not index.isValid() or not self.editable:\n            return False\n\n        if value != index.data(role):\n\n            self.layoutAboutToBeChanged.emit()\n\n            row = self._dataFrame.index[index.row()]\n            col = self._dataFrame.columns[index.column()]\n            #print 'before change: ', index.data().toUTC(), self._dataFrame.iloc[row][col]\n            columnDtype = self._dataFrame[col].dtype\n\n            if columnDtype == object:\n                pass\n\n            elif columnDtype in self._intDtypes:\n                dtypeInfo = numpy.iinfo(columnDtype)\n                if value < dtypeInfo.min:\n                    value = dtypeInfo.min\n                elif value > dtypeInfo.max:\n                    value = dtypeInfo.max\n\n            elif columnDtype in self._floatDtypes:\n                value = numpy.float64(value).astype(columnDtype)\n\n            elif columnDtype in self._boolDtypes:\n                value = numpy.bool_(value)\n\n            elif columnDtype in self._dateDtypes:\n                # convert the given value to a compatible datetime object.\n                # if the conversation could not be done, keep the original\n                # value.\n                if isinstance(value, QtCore.QDateTime):\n                    value = value.toString(self.timestampFormat)\n                try:\n                    value = pandas.Timestamp(value)\n                except Exception:\n                    raise Exception(\"Can't convert '{0}' into a datetime\".format(value))\n                    # return False\n            else:\n                raise TypeError(\"try to set unhandled data type\")\n\n            self._dataFrame.set_value(row, col, value)\n\n            #print 'after change: ', value, self._dataFrame.iloc[row][col]\n            self.layoutChanged.emit()\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort(self, columnId, order=Qt.AscendingOrder):\n        self.layoutAboutToBeChanged.emit()\n        self.sortingAboutToStart.emit()\n        column = self._dataFrame.columns[columnId]\n        self._dataFrame.sort_values(column, ascending=not bool(order), inplace=True)\n        self.layoutChanged.emit()\n        self.sortingFinished.emit()", "response": "Sort the model column\n            by the given columnId."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setFilter(self, search):\n        if not isinstance(search, DataSearch):\n            raise TypeError('The given parameter must an `qtpandas.DataSearch` object')\n\n        self._search = search\n\n        self.layoutAboutToBeChanged.emit()\n\n        if self._dataFrameOriginal is not None:\n            self._dataFrame = self._dataFrameOriginal\n        self._dataFrameOriginal = self._dataFrame.copy()\n\n        self._search.setDataFrame(self._dataFrame)\n        searchIndex, valid = self._search.search()\n\n        if valid:\n            self._dataFrame = self._dataFrame[searchIndex]\n            self.layoutChanged.emit()\n        else:\n            self.clearFilter()\n            self.layoutChanged.emit()\n\n        self.dataFrameChanged.emit()", "response": "Apply a filter and hide rows."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenable or disables most editing methods.", "response": "def enableEditing(self, editable=True):\n        \"\"\"\n        Sets the DataFrameModel and columnDtypeModel's\n        editable properties.\n        :param editable: bool\n            defaults to True,\n            False disables most editing methods.\n        :return:\n            None\n        \"\"\"\n        self.editable = editable\n        self._columnDtypeModel.setEditable(self.editable)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addDataFrameColumn(self, columnName, dtype=str, defaultValue=None):\n        if not self.editable or dtype not in SupportedDtypes.allTypes():\n            return False\n\n        elements = self.rowCount()\n        columnPosition = self.columnCount()\n\n        newColumn = pandas.Series([defaultValue]*elements, index=self._dataFrame.index, dtype=dtype)\n\n        self.beginInsertColumns(QtCore.QModelIndex(), columnPosition - 1, columnPosition - 1)\n        try:\n            self._dataFrame.insert(columnPosition, columnName, newColumn, allow_duplicates=False)\n        except ValueError as e:\n            # columnName does already exist\n            return False\n\n        self.endInsertColumns()\n\n        self.propagateDtypeChanges(columnPosition, newColumn.dtype)\n\n        return True", "response": "Adds a column to the dataframe as long as the model s editable property is True and the dtype is supported."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef removeDataFrameColumns(self, columns):\n        if not self.editable:\n            return False\n\n        if columns:\n            deleted = 0\n            errored = False\n            for (position, name) in columns:\n                position = position - deleted\n                if position < 0:\n                    position = 0\n                self.beginRemoveColumns(QtCore.QModelIndex(), position, position)\n                try:\n                    self._dataFrame.drop(name, axis=1, inplace=True)\n                except ValueError as e:\n                    errored = True\n                    continue\n                self.endRemoveColumns()\n                deleted += 1\n            self.dataChanged.emit()\n\n            if errored:\n                return False\n            else:\n                return True\n        return False", "response": "Removes columns from the dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef removeDataFrameRows(self, rows):\n        if not self.editable:\n            return False\n\n        if rows:\n            position = min(rows)\n            count = len(rows)\n            self.beginRemoveRows(QtCore.QModelIndex(), position, position + count - 1)\n\n            removedAny = False\n            for idx, line in self._dataFrame.iterrows():\n                if idx in rows:\n                    removedAny = True\n                    self._dataFrame.drop(idx, inplace=True)\n\n            if not removedAny:\n                return False\n\n            self._dataFrame.reset_index(inplace=True, drop=True)\n\n            self.endRemoveRows()\n            return True\n        return False", "response": "Removes rows from the dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef restore(self):\n        if not os.path.exists(self.filename):\n            return self\n        if not os.path.isfile(self.filename):\n            return self\n\n        try:\n            with open(self.filename, \"rb\") as f:\n                unpickledObject = pickle.load(f)\n\n            for key in list(self.__dict__.keys()):\n                default = self.__dict__[key]\n                self.__dict__[key] = unpickledObject.__dict__.get(key, default)\n        except:\n            pass\n\n        return self", "response": "Restore the values of the attributes of the object from the pickled file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the attributes of the EgStore object to a pickle file.", "response": "def store(self):\n        \"\"\"\n        Save the attributes of the EgStore object to a pickle file.\n        Note that if the directory for the pickle file does not already exist,\n        the store operation will fail.\n        \"\"\"\n        with open(self.filename, \"wb\") as f:\n            pickle.dump(self, f)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete my persistent file.", "response": "def kill(self):\n        \"\"\"\n        Delete my persistent file (i.e. pickle file), if it exists.\n        \"\"\"\n        if os.path.isfile(self.filename):\n            os.remove(self.filename)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay a message a title an image and a set of buttons.", "response": "def buttonbox(msg=\"\", title=\" \", choices=(\"Button[1]\", \"Button[2]\", \"Button[3]\"), image=None, root=None, default_choice=None, cancel_choice=None):\n    \"\"\"\n    Display a msg, a title, an image, and a set of buttons.\n    The buttons are defined by the members of the choices list.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param list choices: a list or tuple of the choices to be displayed\n    :param str image: Filename of image to display\n    :param str default_choice: The choice you want highlighted when the gui appears\n    :param str cancel_choice: If the user presses the 'X' close, which button should be pressed\n    :return: the text of the button that the user selected\n    \"\"\"\n    global boxRoot, __replyButtonText, buttonsFrame\n\n    # If default is not specified, select the first button.  This matches old\n    # behavior.\n    if default_choice is None:\n        default_choice = choices[0]\n\n    # Initialize __replyButtonText to the first choice.\n    # This is what will be used if the window is closed by the close button.\n    __replyButtonText = choices[0]\n\n    if root:\n        root.withdraw()\n        boxRoot = Toplevel(master=root)\n        boxRoot.withdraw()\n    else:\n        boxRoot = Tk()\n        boxRoot.withdraw()\n\n    boxRoot.title(title)\n    boxRoot.iconname('Dialog')\n    boxRoot.geometry(st.rootWindowPosition)\n    boxRoot.minsize(400, 100)\n\n    # ------------- define the messageFrame ---------------------------------\n    messageFrame = Frame(master=boxRoot)\n    messageFrame.pack(side=TOP, fill=BOTH)\n\n    # ------------- define the imageFrame ---------------------------------\n    if image:\n        tk_Image = None\n        try:\n            tk_Image = ut.load_tk_image(image)\n        except Exception as inst:\n            print(inst)\n        if tk_Image:\n            imageFrame = Frame(master=boxRoot)\n            imageFrame.pack(side=TOP, fill=BOTH)\n            label = Label(imageFrame, image=tk_Image)\n            label.image = tk_Image  # keep a reference!\n            label.pack(side=TOP, expand=YES, fill=X, padx='1m', pady='1m')\n\n    # ------------- define the buttonsFrame ---------------------------------\n    buttonsFrame = Frame(master=boxRoot)\n    buttonsFrame.pack(side=TOP, fill=BOTH)\n\n    # -------------------- place the widgets in the frames -------------------\n    messageWidget = Message(messageFrame, text=msg, width=400)\n    messageWidget.configure(\n        font=(st.PROPORTIONAL_FONT_FAMILY, st.PROPORTIONAL_FONT_SIZE))\n    messageWidget.pack(side=TOP, expand=YES, fill=X, padx='3m', pady='3m')\n\n    __put_buttons_in_buttonframe(choices, default_choice, cancel_choice)\n\n    # -------------- the action begins -----------\n    boxRoot.deiconify()\n    boxRoot.mainloop()\n    boxRoot.destroy()\n    if root:\n        root.deiconify()\n    return __replyButtonText"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __fillablebox(msg, title=\"\", default=\"\", mask=None, image=None, root=None):\n\n    global boxRoot, __enterboxText, __enterboxDefaultText\n    global cancelButton, entryWidget, okButton\n\n    if title is None:\n        title == \"\"\n    if default is None:\n        default = \"\"\n    __enterboxDefaultText = default\n    __enterboxText = __enterboxDefaultText\n\n    if root:\n        root.withdraw()\n        boxRoot = Toplevel(master=root)\n        boxRoot.withdraw()\n    else:\n        boxRoot = Tk()\n        boxRoot.withdraw()\n\n    boxRoot.protocol('WM_DELETE_WINDOW', __enterboxQuit)\n    boxRoot.title(title)\n    boxRoot.iconname('Dialog')\n    boxRoot.geometry(st.rootWindowPosition)\n    boxRoot.bind(\"<Escape>\", __enterboxCancel)\n\n    # ------------- define the messageFrame ---------------------------------\n    messageFrame = Frame(master=boxRoot)\n    messageFrame.pack(side=TOP, fill=BOTH)\n\n    # ------------- define the imageFrame ---------------------------------\n    try:\n        tk_Image = ut.load_tk_image(image)\n    except Exception as inst:\n        print(inst)\n        tk_Image = None\n    if tk_Image:\n        imageFrame = Frame(master=boxRoot)\n        imageFrame.pack(side=TOP, fill=BOTH)\n        label = Label(imageFrame, image=tk_Image)\n        label.image = tk_Image  # keep a reference!\n        label.pack(side=TOP, expand=YES, fill=X, padx='1m', pady='1m')\n\n    # ------------- define the buttonsFrame ---------------------------------\n    buttonsFrame = Frame(master=boxRoot)\n    buttonsFrame.pack(side=TOP, fill=BOTH)\n\n    # ------------- define the entryFrame ---------------------------------\n    entryFrame = Frame(master=boxRoot)\n    entryFrame.pack(side=TOP, fill=BOTH)\n\n    # ------------- define the buttonsFrame ---------------------------------\n    buttonsFrame = Frame(master=boxRoot)\n    buttonsFrame.pack(side=TOP, fill=BOTH)\n\n    # -------------------- the msg widget ----------------------------\n    messageWidget = Message(messageFrame, width=\"4.5i\", text=msg)\n    messageWidget.configure(\n        font=(st.PROPORTIONAL_FONT_FAMILY, st.PROPORTIONAL_FONT_SIZE))\n    messageWidget.pack(side=RIGHT, expand=1, fill=BOTH, padx='3m', pady='3m')\n\n    # --------- entryWidget ----------------------------------------------\n    entryWidget = Entry(entryFrame, width=40)\n    bindArrows(entryWidget)\n    entryWidget.configure(\n        font=(st.PROPORTIONAL_FONT_FAMILY, st.TEXT_ENTRY_FONT_SIZE))\n    if mask:\n        entryWidget.configure(show=mask)\n    entryWidget.pack(side=LEFT, padx=\"3m\")\n    entryWidget.bind(\"<Return>\", __enterboxGetText)\n    entryWidget.bind(\"<Escape>\", __enterboxCancel)\n    # put text into the entryWidget\n    entryWidget.insert(0, __enterboxDefaultText)\n\n    # ------------------ ok button -------------------------------\n    okButton = Button(buttonsFrame, takefocus=1, text=\"OK\")\n    bindArrows(okButton)\n    okButton.pack(\n        expand=1, side=LEFT, padx='3m', pady='3m', ipadx='2m', ipady='1m')\n\n    # for the commandButton, bind activation events to the activation event\n    # handler\n    commandButton = okButton\n    handler = __enterboxGetText\n    for selectionEvent in st.STANDARD_SELECTION_EVENTS:\n        commandButton.bind(\"<{}>\".format(selectionEvent), handler)\n\n    # ------------------ cancel button -------------------------------\n    cancelButton = Button(buttonsFrame, takefocus=1, text=\"Cancel\")\n    bindArrows(cancelButton)\n    cancelButton.pack(\n        expand=1, side=RIGHT, padx='3m', pady='3m', ipadx='2m', ipady='1m')\n\n    # for the commandButton, bind activation events to the activation event\n    # handler\n    commandButton = cancelButton\n    handler = __enterboxCancel\n    for selectionEvent in st.STANDARD_SELECTION_EVENTS:\n        commandButton.bind(\"<{}>\".format(selectionEvent), handler)\n\n    # ------------------- time for action! -----------------\n    entryWidget.focus_force()  # put the focus on the entryWidget\n    boxRoot.deiconify()\n    boxRoot.mainloop()  # run it!\n\n    # -------- after the run has completed ----------------------------------\n    if root:\n        root.deiconify()\n    boxRoot.destroy()  # button_click didn't destroy boxRoot, so we do it now\n    return __enterboxText", "response": "Fill a box with some text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getFileDialogTitle(msg, title):\n    if msg and title:\n        return \"%s - %s\" % (title, msg)\n    if msg and not title:\n        return str(msg)\n    if title and not msg:\n        return str(title)\n    return None", "response": "Create nicely - formatted string based on arguments msg and title"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __buttonEvent(event=None, buttons=None, virtual_event=None):\n    # TODO: Replace globals with tkinter variables\n    global boxRoot, __replyButtonText\n\n    # Determine window location and save to global\n    m = re.match(\"(\\d+)x(\\d+)([-+]\\d+)([-+]\\d+)\", boxRoot.geometry())\n    if not m:\n        raise ValueError(\n            \"failed to parse geometry string: {}\".format(boxRoot.geometry()))\n    width, height, xoffset, yoffset = [int(s) for s in m.groups()]\n    st.rootWindowPosition = '{0:+g}{1:+g}'.format(xoffset, yoffset)\n\n    # print('{0}:{1}:{2}'.format(event, buttons, virtual_event))\n    if virtual_event == 'cancel':\n        for button_name, button in list(buttons.items()):\n            if 'cancel_choice' in button:\n                __replyButtonText = button['original_text']\n        __replyButtonText = None\n        boxRoot.quit()\n        return\n\n    if virtual_event == 'select':\n        text = event.widget.config('text')[-1]\n        if not isinstance(text, ut.str):\n            text = ' '.join(text)\n        for button_name, button in list(buttons.items()):\n            if button['clean_text'] == text:\n                __replyButtonText = button['original_text']\n                boxRoot.quit()\n                return\n\n    # Hotkeys\n    if buttons:\n        for button_name, button in list(buttons.items()):\n            hotkey_pressed = event.keysym\n            if event.keysym != event.char:  # A special character\n                hotkey_pressed = '<{}>'.format(event.keysym)\n            if button['hotkey'] == hotkey_pressed:\n                __replyButtonText = button_name\n                boxRoot.quit()\n                return\n\n    print(\"Event not understood\")", "response": "Handles an event that is generated by a person interacting with a button."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __put_buttons_in_buttonframe(choices, default_choice, cancel_choice):\n    global buttonsFrame, cancel_invoke\n\n    # TODO: I'm using a dict to hold buttons, but this could all be cleaned up if I subclass Button to hold\n    #      all the event bindings, etc\n    # TODO: Break __buttonEvent out into three: regular keyboard, default\n    # select, and cancel select.\n    unique_choices = ut.uniquify_list_of_strings(choices)\n    # Create buttons dictionary and Tkinter widgets\n    buttons = dict()\n    for button_text, unique_button_text in zip(choices, unique_choices):\n        this_button = dict()\n        this_button['original_text'] = button_text\n        this_button['clean_text'], this_button[\n            'hotkey'], hotkey_position = ut.parse_hotkey(button_text)\n        this_button['widget'] = Button(buttonsFrame,\n                                       takefocus=1,\n                                       text=this_button['clean_text'],\n                                       underline=hotkey_position)\n        this_button['widget'].pack(\n            expand=YES, side=LEFT, padx='1m', pady='1m', ipadx='2m', ipady='1m')\n        buttons[unique_button_text] = this_button\n    # Bind arrows, Enter, Escape\n    for this_button in list(buttons.values()):\n        bindArrows(this_button['widget'])\n        for selectionEvent in st.STANDARD_SELECTION_EVENTS:\n            this_button['widget'].bind(\"<{}>\".format(selectionEvent),\n                                       lambda e: __buttonEvent(\n                                           e, buttons, virtual_event='select'),\n                                       add=True)\n\n    # Assign default and cancel buttons\n    if cancel_choice in buttons:\n        buttons[cancel_choice]['cancel_choice'] = True\n    boxRoot.bind_all('<Escape>', lambda e: __buttonEvent(\n        e, buttons, virtual_event='cancel'), add=True)\n    boxRoot.protocol('WM_DELETE_WINDOW', lambda: __buttonEvent(\n        None, buttons, virtual_event='cancel'))\n    if default_choice in buttons:\n        buttons[default_choice]['default_choice'] = True\n        buttons[default_choice]['widget'].focus_force()\n    # Bind hotkeys\n    for hk in [button['hotkey'] for button in list(buttons.values()) if button['hotkey']]:\n        boxRoot.bind_all(hk, lambda e: __buttonEvent(e, buttons), add=True)\n\n    return", "response": "Put the buttons in the buttons frame"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay a msgbox with choices of Yes and No.", "response": "def ynbox(msg=\"Shall I continue?\", title=\" \",\n          choices=(\"[<F1>]Yes\", \"[<F2>]No\"), image=None,\n          default_choice='[<F1>]Yes', cancel_choice='[<F2>]No'):\n    \"\"\"\n    Display a msgbox with choices of Yes and No.\n\n    The returned value is calculated this way::\n\n        if the first choice (\"Yes\") is chosen, or if the dialog is cancelled:\n            return True\n        else:\n            return False\n\n    If invoked without a msg argument, displays a generic\n    request for a confirmation\n    that the user wishes to continue.  So it can be used this way::\n\n        if ynbox():\n            pass # continue\n        else:\n            sys.exit(0)  # exit the program\n\n    :param msg: the msg to be displayed\n    :type msg: str\n    :param str title: the window title\n    :param list choices: a list or tuple of the choices to be displayed\n    :param str image: Filename of image to display\n    :param str default_choice: The choice you want highlighted\n        when the gui appears\n    :param str cancel_choice: If the user presses the 'X' close, which\n      button should be pressed\n\n    :return: True if 'Yes' or dialog is cancelled, False if 'No'\n    \"\"\"\n    return boolbox(msg=msg,\n                   title=title,\n                   choices=choices,\n                   image=image,\n                   default_choice=default_choice,\n                   cancel_choice=cancel_choice)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ccbox(msg=\"Shall I continue?\", title=\" \",\n          choices=(\"C[o]ntinue\", \"C[a]ncel\"), image=None,\n          default_choice='Continue', cancel_choice='Cancel'):\n    \"\"\"\n    Display a msgbox with choices of Continue and Cancel.\n\n    The returned value is calculated this way::\n\n        if the first choice (\"Continue\") is chosen,\n          or if the dialog is cancelled:\n            return True\n        else:\n            return False\n\n    If invoked without a msg argument, displays a generic\n        request for a confirmation\n    that the user wishes to continue.  So it can be used this way::\n\n        if ccbox():\n            pass # continue\n        else:\n            sys.exit(0)  # exit the program\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param list choices: a list or tuple of the choices to be displayed\n    :param str image: Filename of image to display\n    :param str default_choice: The choice you want highlighted\n      when the gui appears\n    :param str cancel_choice: If the user presses the 'X' close,\n      which button should be pressed\n\n    :return: True if 'Continue' or dialog is cancelled, False if 'Cancel'\n    \"\"\"\n    return boolbox(msg=msg,\n                   title=title,\n                   choices=choices,\n                   image=image,\n                   default_choice=default_choice,\n                   cancel_choice=cancel_choice)", "response": "Display a msgbox with choices of Continue and Cancel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef boolbox(msg=\"Shall I continue?\", title=\" \",\n            choices=(\"[Y]es\", \"[N]o\"), image=None,\n            default_choice='Yes', cancel_choice='No'):\n    \"\"\"\n    Display a boolean msgbox.\n\n    The returned value is calculated this way::\n\n        if the first choice is chosen, or if the dialog is cancelled:\n            returns True\n        else:\n            returns False\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param list choices: a list or tuple of the choices to be displayed\n    :param str image: Filename of image to display\n    :param str default_choice: The choice you want highlighted\n      when the gui appears\n    :param str cancel_choice: If the user presses the 'X' close, which button\n      should be pressed\n    :return: True if first button pressed or dialog is cancelled, False if\n      second button is pressed\n    \"\"\"\n    if len(choices) != 2:\n        raise AssertionError(\n            'boolbox takes exactly 2 choices!  Consider using indexbox instead'\n        )\n\n    reply = bb.buttonbox(msg=msg,\n                         title=title,\n                         choices=choices,\n                         image=image,\n                         default_choice=default_choice,\n                         cancel_choice=cancel_choice)\n    if reply == choices[0]:\n        return True\n    else:\n        return False", "response": "Display a boolean msgbox."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef indexbox(msg=\"Shall I continue?\", title=\" \",\n             choices=(\"Yes\", \"No\"), image=None,\n             default_choice='Yes', cancel_choice='No'):\n    \"\"\"\n    Display a buttonbox with the specified choices.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param list choices: a list or tuple of the choices to be displayed\n    :param str image: Filename of image to display\n    :param str default_choice: The choice you want highlighted\n      when the gui appears\n    :param str cancel_choice: If the user presses the 'X' close,\n      which button should be pressed\n    :return: the index of the choice selected, starting from 0\n    \"\"\"\n    reply = bb.buttonbox(msg=msg,\n                         title=title,\n                         choices=choices,\n                         image=image,\n                         default_choice=default_choice,\n                         cancel_choice=cancel_choice)\n    if reply is None:\n        return None\n    for i, choice in enumerate(choices):\n        if reply == choice:\n            return i\n    msg = (\"There is a program logic error in the EasyGui code \"\n           \"for indexbox.\\nreply={0}, choices={1}\".format(\n               reply, choices))\n    raise AssertionError(msg)", "response": "Display a buttonbox with the specified choices."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef msgbox(msg=\"(Your message goes here)\", title=\" \",\n           ok_button=\"OK\", image=None, root=None):\n    \"\"\"\n    Display a message box\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param str ok_button: text to show in the button\n    :param str image: Filename of image to display\n    :param tk_widget root: Top-level Tk widget\n    :return: the text of the ok_button\n    \"\"\"\n    if not isinstance(ok_button, ut.str):\n        raise AssertionError(\n            \"The 'ok_button' argument to msgbox must be a string.\")\n\n    return bb.buttonbox(msg=msg,\n                        title=title,\n                        choices=[ok_button],\n                        image=image,\n                        root=root,\n                        default_choice=ok_button,\n                        cancel_choice=ok_button)", "response": "Display a message box in a Tk - widget"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef integerbox(msg=\"\", title=\" \", default=\"\",\n               lowerbound=0, upperbound=99, image=None, root=None):\n    \"\"\"\n    Show a box in which a user can enter an integer.\n\n    In addition to arguments for msg and title, this function accepts\n    integer arguments for \"default\", \"lowerbound\", and \"upperbound\".\n\n    The default argument may be None.\n\n    When the user enters some text, the text is checked to verify that it\n    can be converted to an integer between the lowerbound and upperbound.\n\n    If it can be, the integer (not the text) is returned.\n\n    If it cannot, then an error msg is displayed, and the integerbox is\n    redisplayed.\n\n    If the user cancels the operation, None is returned.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param str default: The default value to return\n    :param int lowerbound: The lower-most value allowed\n    :param int upperbound: The upper-most value allowed\n    :param str image: Filename of image to display\n    :param tk_widget root: Top-level Tk widget\n    :return: the integer value entered by the user\n\n    \"\"\"\n\n    if not msg:\n        msg = \"Enter an integer between {0} and {1}\".format(\n            lowerbound, upperbound)\n\n    # Validate the arguments for default, lowerbound and upperbound and\n    # convert to integers\n    exception_string = (\n        'integerbox \"{0}\" must be an integer.  It is >{1}< of type {2}')\n    if default:\n        try:\n            default = int(default)\n        except ValueError:\n            raise ValueError(\n                exception_string.format('default', default, type(default)))\n    try:\n        lowerbound = int(lowerbound)\n    except ValueError:\n        raise ValueError(\n            exception_string.format('lowerbound',\n                                    lowerbound, type(lowerbound)))\n    try:\n        upperbound = int(upperbound)\n    except ValueError:\n        raise ValueError(\n            exception_string.format('upperbound',\n                                    upperbound, type(upperbound)))\n\n    while True:\n        reply = enterbox(msg, title, str(default), image=image, root=root)\n        if reply is None:\n            return None\n        try:\n            reply = int(reply)\n        except:\n            msgbox('The value that you entered:\\n\\t\"{}\"\\nis not an integer.'\n                   .format(reply), \"Error\")\n            continue\n        if reply < lowerbound:\n            msgbox('The value that you entered is less than the lower '\n                   'bound of {}.'.format(lowerbound), \"Error\")\n            continue\n        if reply > upperbound:\n            msgbox('The value that you entered is greater than the upper bound'\n                   ' of {}.'.format(\n                       upperbound), \"Error\")\n            continue\n        # reply has passed all validation checks.\n        # It is an integer between the specified bounds.\n        return reply", "response": "Display an integer box in which a user can enter an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef multenterbox(msg=\"Fill in values for the fields.\", title=\" \",\n                 fields=(), values=()):\n    r\"\"\"\n    Show screen with multiple data entry fields.\n\n    If there are fewer values than names, the list of values is padded with\n    empty strings until the number of values is the same as the number\n    of names.\n\n    If there are more values than names, the list of values\n    is truncated so that there are as many values as names.\n\n    Returns a list of the values of the fields,\n    or None if the user cancels the operation.\n\n    Here is some example code, that shows how values returned from\n    multenterbox can be checked for validity before they are accepted::\n\n        msg = \"Enter your personal information\"\n        title = \"Credit Card Application\"\n        fieldNames = [\"Name\",\"Street Address\",\"City\",\"State\",\"ZipCode\"]\n        fieldValues = []  # we start with blanks for the values\n        fieldValues = multenterbox(msg,title, fieldNames)\n\n        # make sure that none of the fields was left blank\n        while 1:\n            if fieldValues is None: break\n            errmsg = \"\"\n            for i in range(len(fieldNames)):\n                if fieldValues[i].strip() == \"\":\n                    errmsg += ('\"%s\" is a required field.\\n\\n' % fieldNames[i])\n            if errmsg == \"\":\n                break # no problems found\n            fieldValues = multenterbox(errmsg, title, fieldNames, fieldValues)\n\n        print(\"Reply was: %s\" % str(fieldValues))\n\n    :param str msg: the msg to be displayed.\n    :param str title: the window title\n    :param list fields: a list of fieldnames.\n    :param list values: a list of field values\n    :return: String\n    \"\"\"\n    return bb.__multfillablebox(msg, title, fields, values, None)", "response": "r Shows a multenterbox with multiple data entry fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay an exception box in the current window.", "response": "def exceptionbox(msg=None, title=None):\n    \"\"\"\n    Display a box that gives information about\n    an exception that has just been raised.\n\n    The caller may optionally pass in a title for the window, or a\n    msg to accompany the error information.\n\n    Note that you do not need to (and cannot) pass an exception object\n    as an argument.  The latest exception will automatically be used.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :return: None\n\n    \"\"\"\n    if title is None:\n        title = \"Error Report\"\n    if msg is None:\n        msg = \"An error (exception) has occurred in the program.\"\n\n    codebox(msg, title, ut.exception_format())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay some text in a monospaced font with no line wrapping.", "response": "def codebox(msg=\"\", title=\" \", text=\"\"):\n    \"\"\"\n    Display some text in a monospaced font, with no line wrapping.\n    This function is suitable for displaying code and text that is\n    formatted using spaces.\n\n    The text parameter should be a string, or a list or tuple of lines to be\n    displayed in the textbox.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param str text: what to display in the textbox\n    \"\"\"\n    return tb.textbox(msg, title, text, codebox=1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calculateEncodingKey(comparator):\n    encodingName = None\n    for k, v in list(_encodings.items()):\n        if v == comparator:\n            encodingName = k\n            break\n    return encodingName", "response": "Calculates the first key of all available encodings where the corresponding\n    value matches the comparator."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the inital layout with all subwidgets.", "response": "def _initUI(self):\n        \"\"\"Creates the inital layout with all subwidgets.\n\n        The layout is a `QHBoxLayout`. Each time a radio button is\n        selected or unselected, a slot\n        `DelimiterSelectionWidget._delimiter` is called.\n        Furthermore the `QLineEdit` widget has a custom regex validator\n        `DelimiterValidator` enabled.\n\n        \"\"\"\n        #layout = QtGui.QHBoxLayout(self)\n\n        self.semicolonRadioButton = QtGui.QRadioButton('Semicolon')\n        self.commaRadioButton = QtGui.QRadioButton('Comma')\n        self.tabRadioButton = QtGui.QRadioButton('Tab')\n        self.otherRadioButton = QtGui.QRadioButton('Other')\n        self.commaRadioButton.setChecked(True)\n\n        self.otherSeparatorLineEdit = QtGui.QLineEdit(self)\n        #TODO: Enable this or add BAR radio and option.\n        self.otherSeparatorLineEdit.setEnabled(False)\n\n        self.semicolonRadioButton.toggled.connect(self._delimiter)\n        self.commaRadioButton.toggled.connect(self._delimiter)\n        self.tabRadioButton.toggled.connect(self._delimiter)\n\n        self.otherRadioButton.toggled.connect(self._enableLine)\n        self.otherSeparatorLineEdit.textChanged.connect(lambda: self._delimiter(True))\n        self.otherSeparatorLineEdit.setValidator(DelimiterValidator(self))\n\n        currentLayout = self.layout()\n        # unset and delete the current layout in order to set a new one\n        if currentLayout is not None:\n            del currentLayout\n\n        layout = QtGui.QHBoxLayout()\n        layout.addWidget(self.semicolonRadioButton)\n        layout.addWidget(self.commaRadioButton)\n        layout.addWidget(self.tabRadioButton)\n        layout.addWidget(self.otherRadioButton)\n        layout.addWidget(self.otherSeparatorLineEdit)\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the currently selected delimiter character.", "response": "def currentSelected(self):\n        \"\"\"Returns the currently selected delimiter character.\n\n        Returns:\n            str: One of `,`, `;`, `\\t`, `*other*`.\n\n        \"\"\"\n        if self.commaRadioButton.isChecked():\n            return ','\n        elif self.semicolonRadioButton.isChecked():\n            return ';'\n        elif self.tabRadioButton.isChecked():\n            return '\\t'\n        elif self.otherRadioButton.isChecked():\n            return self.otherSeparatorLineEdit.text()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the user interface with a grid layout and several widgets.", "response": "def _initUI(self):\n        \"\"\"Initiates the user interface with a grid layout and several widgets.\n\n        \"\"\"\n        self.setModal(self._modal)\n        self.setWindowTitle(self._windowTitle)\n\n        layout = QtGui.QGridLayout()\n\n        self._filenameLabel = QtGui.QLabel('Choose File', self)\n        self._filenameLineEdit = QtGui.QLineEdit(self)\n        self._filenameLineEdit.textEdited.connect(self._updateFilename)\n        chooseFileButtonIcon = QtGui.QIcon(QtGui.QPixmap(':/icons/document-open.png'))\n        self._chooseFileAction = QtGui.QAction(self)\n        self._chooseFileAction.setIcon(chooseFileButtonIcon)\n        self._chooseFileAction.triggered.connect(self._openFile)\n\n        self._chooseFileButton = QtGui.QToolButton(self)\n        self._chooseFileButton.setDefaultAction(self._chooseFileAction)\n\n        layout.addWidget(self._filenameLabel, 0, 0)\n        layout.addWidget(self._filenameLineEdit, 0, 1, 1, 2)\n        layout.addWidget(self._chooseFileButton, 0, 3)\n\n        self._encodingLabel = QtGui.QLabel('File Encoding', self)\n\n        encoding_names = list([x.upper() for x in sorted(list(set(_encodings.values())))])\n        self._encodingComboBox = QtGui.QComboBox(self)\n        self._encodingComboBox.addItems(encoding_names)\n        self._encodingComboBox.activated.connect(self._updateEncoding)\n\n        layout.addWidget(self._encodingLabel, 1, 0)\n        layout.addWidget(self._encodingComboBox, 1, 1, 1, 1)\n\n        self._hasHeaderLabel = QtGui.QLabel('Header Available?', self)\n        self._headerCheckBox = QtGui.QCheckBox(self)\n        self._headerCheckBox.toggled.connect(self._updateHeader)\n\n\n        layout.addWidget(self._hasHeaderLabel, 2, 0)\n        layout.addWidget(self._headerCheckBox, 2, 1)\n\n        self._delimiterLabel = QtGui.QLabel('Column Delimiter', self)\n        self._delimiterBox = DelimiterSelectionWidget(self)\n        self._delimiter = self._delimiterBox.currentSelected()\n        self._delimiterBox.delimiter.connect(self._updateDelimiter)\n\n        layout.addWidget(self._delimiterLabel, 3, 0)\n        layout.addWidget(self._delimiterBox, 3, 1, 1, 3)\n\n        self._tabWidget = QtGui.QTabWidget(self)\n        self._previewTableView = QtGui.QTableView(self)\n        self._datatypeTableView = QtGui.QTableView(self)\n        self._tabWidget.addTab(self._previewTableView, 'Preview')\n        self._tabWidget.addTab(self._datatypeTableView, 'Change Column Types')\n        layout.addWidget(self._tabWidget, 4, 0, 3, 4)\n\n        self._datatypeTableView.horizontalHeader().setDefaultSectionSize(200)\n        self._datatypeTableView.setItemDelegateForColumn(1, DtypeComboDelegate(self._datatypeTableView))\n\n\n        self._loadButton = QtGui.QPushButton('Load Data', self)\n        #self.loadButton.setAutoDefault(False)\n\n        self._cancelButton = QtGui.QPushButton('Cancel', self)\n        # self.cancelButton.setDefault(False)\n        # self.cancelButton.setAutoDefault(True)\n\n        self._buttonBox = QtGui.QDialogButtonBox(self)\n        self._buttonBox.addButton(self._loadButton, QtGui.QDialogButtonBox.AcceptRole)\n        self._buttonBox.addButton(self._cancelButton, QtGui.QDialogButtonBox.RejectRole)\n        self._buttonBox.accepted.connect(self.accepted)\n        self._buttonBox.rejected.connect(self.rejected)\n        layout.addWidget(self._buttonBox, 9, 2, 1, 2)\n        self._loadButton.setDefault(False)\n        self._filenameLineEdit.setFocus()\n\n        self._statusBar = QtGui.QStatusBar(self)\n        self._statusBar.setSizeGripEnabled(False)\n        self._headerCheckBox.setChecked(True)\n        layout.addWidget(self._statusBar, 8, 0, 1, 4)\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _openFile(self):\n\n        file_types = \"Comma Separated Values (*.csv);;Text files (*.txt);;All Files (*)\"\n        ret = QtGui.QFileDialog.getOpenFileName(self,\n                                                self.tr('open file'),\n                                                filter=file_types)\n\n        if isinstance(ret, tuple):\n            ret = ret[0] #PySide compatibility maybe?\n\n        if ret:\n            self._filenameLineEdit.setText(ret)\n            self._updateFilename()", "response": "Opens a file dialog and sets a value for the filenameLineEdit widget."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _updateFilename(self):\n        self._filename = self._filenameLineEdit.text()\n        self._guessEncoding(self._filename)\n        self._previewFile()", "response": "Calls several methods after the filename changed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens a file from the given path and checks the file encoding.", "response": "def _guessEncoding(self, path):\n        \"\"\"Opens a file from the given `path` and checks the file encoding.\n\n        The file must exists on the file system and end with the extension\n        `.csv`. The file is read line by line until the encoding could be\n        guessed.\n        On a successfull identification, the widgets of this dialog will be\n        updated.\n\n        Args:\n            path (string): Path to a csv file on the file system.\n\n        \"\"\"\n        if os.path.exists(path) and path.lower().endswith('csv'):\n            # encoding = self._detector.detect(path)\n            encoding = None\n\n            if encoding is not None:\n                if encoding.startswith('utf'):\n                    encoding = encoding.replace('-', '')\n                encoding = encoding.replace('-','_')\n\n                viewValue = _encodings.get(encoding)\n\n                self._encodingKey = encoding\n\n                index = self._encodingComboBox.findText(viewValue.upper())\n                self._encodingComboBox.setCurrentIndex(index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _updateEncoding(self, index):\n        encoding = self._encodingComboBox.itemText(index)\n        encoding = encoding.lower()\n\n        self._encodingKey = _calculateEncodingKey(encoding)\n        self._previewFile()", "response": "Updates the encoding key and preview file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the preview widgets with new models for both tab panes.", "response": "def _previewFile(self):\n        \"\"\"Updates the preview widgets with new models for both tab panes.\n\n        \"\"\"\n        dataFrame = self._loadCSVDataFrame()\n        dataFrameModel = DataFrameModel(dataFrame, filePath=self._filename)\n        dataFrameModel.enableEditing(True)\n        self._previewTableView.setModel(dataFrameModel)\n        columnModel = dataFrameModel.columnDtypeModel()\n        columnModel.changeFailed.connect(self.updateStatusBar)\n        self._datatypeTableView.setModel(columnModel)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _loadCSVDataFrame(self):\n        if self._filename and os.path.exists(self._filename):\n            # default fallback if no encoding was found/selected\n            encoding = self._encodingKey or 'UTF_8'\n\n            try:\n                dataFrame = superReadFile(self._filename,\n                    sep=self._delimiter, first_codec=encoding,\n                    header=self._header)\n                dataFrame = dataFrame.apply(fillNoneValues)\n                dataFrame = dataFrame.apply(convertTimestamps)\n            except Exception as err:\n                self.updateStatusBar(str(err))\n                print(err)\n                return pandas.DataFrame()\n            self.updateStatusBar('Preview generated.')\n            return dataFrame\n        self.updateStatusBar('File could not be read.')\n        return pandas.DataFrame()", "response": "Loads the given csv file with pandas and generate a new dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _resetWidgets(self):\n        self._filenameLineEdit.setText('')\n        self._encodingComboBox.setCurrentIndex(0)\n        self._delimiterBox.reset()\n        self._headerCheckBox.setChecked(False)\n        self._statusBar.showMessage('')\n        self._previewTableView.setModel(None)\n        self._datatypeTableView.setModel(None)", "response": "Resets all widgets of this dialog to its inital state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accepted(self):\n        model = self._previewTableView.model()\n        if model is not None:\n            df = model.dataFrame().copy()\n            dfModel = DataFrameModel(df)\n            self.load.emit(dfModel, self._filename)\n            print((\"Emitted model for {}\".format(self._filename)))\n        self._resetWidgets()\n        self.accept()", "response": "Successfully close the widget and return the loaded model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the user interface with a grid layout and several widgets.", "response": "def _initUI(self):\n        \"\"\"Initiates the user interface with a grid layout and several widgets.\n\n        \"\"\"\n        self.setModal(self._modal)\n        self.setWindowTitle(self._windowTitle)\n\n        layout = QtGui.QGridLayout()\n\n        self._filenameLabel = QtGui.QLabel('Output File', self)\n        self._filenameLineEdit = QtGui.QLineEdit(self)\n        chooseFileButtonIcon = QtGui.QIcon(QtGui.QPixmap(':/icons/document-save-as.png'))\n        self._chooseFileAction = QtGui.QAction(self)\n        self._chooseFileAction.setIcon(chooseFileButtonIcon)\n        self._chooseFileAction.triggered.connect(self._createFile)\n\n        self._chooseFileButton = QtGui.QToolButton(self)\n        self._chooseFileButton.setDefaultAction(self._chooseFileAction)\n\n        layout.addWidget(self._filenameLabel, 0, 0)\n        layout.addWidget(self._filenameLineEdit, 0, 1, 1, 2)\n        layout.addWidget(self._chooseFileButton, 0, 3)\n\n        self._encodingLabel = QtGui.QLabel('File Encoding', self)\n\n\n        encoding_names = list(map(lambda x: x.upper(), sorted(list(set(_encodings.values())))))\n\n        self._encodingComboBox = QtGui.QComboBox(self)\n        self._encodingComboBox.addItems(encoding_names)\n        self._idx = encoding_names.index('UTF_8')\n        self._encodingComboBox.setCurrentIndex(self._idx)\n        #self._encodingComboBox.activated.connect(self._updateEncoding)\n\n        layout.addWidget(self._encodingLabel, 1, 0)\n        layout.addWidget(self._encodingComboBox, 1, 1, 1, 1)\n\n        self._hasHeaderLabel = QtGui.QLabel('Header Available?', self)\n        self._headerCheckBox = QtGui.QCheckBox(self)\n        #self._headerCheckBox.toggled.connect(self._updateHeader)\n\n        layout.addWidget(self._hasHeaderLabel, 2, 0)\n        layout.addWidget(self._headerCheckBox, 2, 1)\n\n        self._delimiterLabel = QtGui.QLabel('Column Delimiter', self)\n        self._delimiterBox = DelimiterSelectionWidget(self)\n\n        layout.addWidget(self._delimiterLabel, 3, 0)\n        layout.addWidget(self._delimiterBox, 3, 1, 1, 3)\n\n        self._exportButton = QtGui.QPushButton('Export Data', self)\n        self._cancelButton = QtGui.QPushButton('Cancel', self)\n\n        self._buttonBox = QtGui.QDialogButtonBox(self)\n        self._buttonBox.addButton(self._exportButton, QtGui.QDialogButtonBox.AcceptRole)\n        self._buttonBox.addButton(self._cancelButton, QtGui.QDialogButtonBox.RejectRole)\n\n        self._buttonBox.accepted.connect(self.accepted)\n        self._buttonBox.rejected.connect(self.rejected)\n\n        layout.addWidget(self._buttonBox, 5, 2, 1, 2)\n        self._exportButton.setDefault(False)\n        self._filenameLineEdit.setFocus()\n\n        self._statusBar = QtGui.QStatusBar(self)\n        self._statusBar.setSizeGripEnabled(False)\n        layout.addWidget(self._statusBar, 4, 0, 1, 4)\n        self.setLayout(layout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _resetWidgets(self):\n        self._filenameLineEdit.setText('')\n        self._encodingComboBox.setCurrentIndex(self._idx)\n        self._delimiterBox.reset()\n        self._headerCheckBox.setChecked(False)\n        self._statusBar.showMessage('')", "response": "Resets all widgets of this dialog to its inital state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndisplaying some text in a proportional font with line wrapping at word breaks at word breaks.", "response": "def textbox(msg=\"\", title=\" \", text=\"\", codebox=0):\n    \"\"\"\n    Display some text in a proportional font with line wrapping at word breaks.\n    This function is suitable for displaying general written text.\n\n    The text parameter should be a string, or a list or tuple of lines to be\n    displayed in the textbox.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :param str text: what to display in the textbox\n    :param str codebox: if 1, act as a codebox\n    \"\"\"\n\n    if msg is None:\n        msg = \"\"\n    if title is None:\n        title = \"\"\n\n    boxRoot = tk.Tk()\n\n    def __textboxOK(event):\n        boxRoot.quit()\n\n    # Quit when x button pressed\n    boxRoot.protocol('WM_DELETE_WINDOW', boxRoot.quit)\n\n    screen_width = boxRoot.winfo_screenwidth()\n    screen_height = boxRoot.winfo_screenheight()\n    root_width = int((screen_width * 0.8))\n    root_height = int((screen_height * 0.5))\n    root_xpos = int((screen_width * 0.1))\n    root_ypos = int((screen_height * 0.05))\n\n    boxRoot.title(title)\n    boxRoot.iconname('Dialog')\n    st.rootWindowPosition = \"+0+0\"\n    boxRoot.geometry(st.rootWindowPosition)\n    boxRoot.expand = tk.NO\n    boxRoot.minsize(root_width, root_height)\n    st.rootWindowPosition = '+{0}+{1}'.format(root_xpos, root_ypos)\n    boxRoot.geometry(st.rootWindowPosition)\n\n    mainframe = tk.Frame(master=boxRoot)\n    mainframe.pack(side=tk.TOP, fill=tk.BOTH, expand=tk.YES)\n\n    # ----  put frames in the window -----------------------------------\n    # we pack the textboxFrame first, so it will expand first\n    textboxFrame = tk.Frame(mainframe, borderwidth=3)\n    textboxFrame.pack(side=tk.BOTTOM, fill=tk.BOTH, expand=tk.YES)\n\n    message_and_buttonsFrame = tk.Frame(mainframe)\n    message_and_buttonsFrame.pack(side=tk.TOP, fill=tk.X, expand=tk.NO)\n\n    messageFrame = tk.Frame(message_and_buttonsFrame)\n    messageFrame.pack(side=tk.LEFT, fill=tk.X, expand=tk.YES)\n\n    buttonsFrame = tk.Frame(message_and_buttonsFrame)\n    buttonsFrame.pack(side=tk.RIGHT, expand=tk.NO)\n\n    # -------------------- put widgets in the frames --------------------\n\n    # put a textArea in the top frame\n    if codebox:\n        character_width = int((root_width * 0.6) / st.MONOSPACE_FONT_SIZE)\n        textArea = tk.Text(\n            textboxFrame, height=25, width=character_width, padx=\"2m\",\n            pady=\"1m\")\n        textArea.configure(wrap=tk.NONE)\n        textArea.configure(font=(st.MONOSPACE_FONT_FAMILY,\n                                 st.MONOSPACE_FONT_SIZE))\n\n    else:\n        character_width = int((root_width * 0.6) / st.MONOSPACE_FONT_SIZE)\n        textArea = tk.Text(\n            textboxFrame, height=25, width=character_width, padx=\"2m\",\n            pady=\"1m\"\n        )\n        textArea.configure(wrap=tk.WORD)\n        textArea.configure(\n            font=(st.PROPORTIONAL_FONT_FAMILY, st.PROPORTIONAL_FONT_SIZE))\n\n    # some simple keybindings for scrolling\n    mainframe.bind(\"<Next>\", textArea.yview_scroll(1, tk.PAGES))\n    mainframe.bind(\"<Prior>\", textArea.yview_scroll(-1, tk.PAGES))\n\n    mainframe.bind(\"<Right>\", textArea.xview_scroll(1, tk.PAGES))\n    mainframe.bind(\"<Left>\", textArea.xview_scroll(-1, tk.PAGES))\n\n    mainframe.bind(\"<Down>\", textArea.yview_scroll(1, tk.UNITS))\n    mainframe.bind(\"<Up>\", textArea.yview_scroll(-1, tk.UNITS))\n\n    # add a vertical scrollbar to the frame\n    rightScrollbar = tk.Scrollbar(\n        textboxFrame, orient=tk.VERTICAL, command=textArea.yview)\n    textArea.configure(yscrollcommand=rightScrollbar.set)\n\n    # add a horizontal scrollbar to the frame\n    bottomScrollbar = tk.Scrollbar(\n        textboxFrame, orient=tk.HORIZONTAL, command=textArea.xview)\n    textArea.configure(xscrollcommand=bottomScrollbar.set)\n\n    # pack the textArea and the scrollbars.  Note that although we must define\n    # the textArea first, we must pack it last,\n    # so that the bottomScrollbar will be located properly.\n\n    # Note that we need a bottom scrollbar only for code.\n    # Text will be displayed with wordwrap, so we don't need to have\n    # a horizontal scroll for it.\n    if codebox:\n        bottomScrollbar.pack(side=tk.BOTTOM, fill=tk.X)\n    rightScrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n\n    textArea.pack(side=tk.LEFT, fill=tk.BOTH, expand=tk.YES)\n\n    # ---------- put a msg widget in the msg frame-------------------\n    messageWidget = tk.Message(\n        messageFrame, anchor=tk.NW, text=msg, width=int(root_width * 0.9))\n    messageWidget.configure(\n        font=(st.PROPORTIONAL_FONT_FAMILY, st.PROPORTIONAL_FONT_SIZE))\n    messageWidget.pack(side=tk.LEFT, expand=tk.YES, fill=tk.BOTH,\n                       padx='1m', pady='1m')\n\n    # put the buttons in the buttonsFrame\n    okButton = tk.Button(\n        buttonsFrame, takefocus=tk.YES, text=\"OK\", height=1, width=6)\n    okButton.pack(\n        expand=tk.NO, side=tk.TOP, padx='2m', pady='1m', ipady=\"1m\",\n        ipadx=\"2m\")\n\n    # for the commandButton, bind activation events to the activation event\n    # handler\n    commandButton = okButton\n    handler = __textboxOK\n    for selectionEvent in [\"Return\", \"Button-1\", \"Escape\"]:\n        commandButton.bind(\"<%s>\" % selectionEvent, handler)\n\n    # ----------------- the action begins ------------------------------------\n    text = to_string(text)\n    try:\n        textArea.insert('end', text, \"normal\")\n    except Exception:\n        msgbox(\"Exception when trying to load the textArea.\")\n        sys.exit(16)\n\n    try:\n        okButton.focus_force()\n    except Exception:\n        msgbox(\"Exception when trying to put focus on okButton.\")\n        sys.exit(16)\n\n    boxRoot.mainloop()\n\n    # this line MUST go before the line that destroys boxRoot\n    areaText = textArea.get(0.0, 'end-1c')\n    boxRoot.destroy()\n    return areaText"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_pin(self, pin, matrix=None):\n        if matrix is None:\n            _, matrix = self.read_pin()\n        return \"\".join([str(matrix.index(p) + 1) for p in pin])", "response": "Transform correct PIN according to the displayed matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the base32 - encoded version of pk_bytes ( G...)", "response": "def address_from_public_key(pk_bytes):\n    \"\"\"Returns the base32-encoded version of pk_bytes (G...)\n    \"\"\"\n    final_bytes = bytearray()\n\n    # version\n    final_bytes.append(6 << 3)\n    # public key\n    final_bytes.extend(pk_bytes)\n    # checksum\n    final_bytes.extend(struct.pack(\"<H\", _crc16_checksum(final_bytes)))\n\n    return base64.b32encode(final_bytes).decode()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the base64data of a stellar sign transaction into a base64 - encoded dictionary.", "response": "def parse_transaction_bytes(tx_bytes):\n    \"\"\"Parses base64data into a map with the following keys:\n        tx - a StellarSignTx describing the transaction header\n        operations - an array of protobuf message objects for each operation\n    \"\"\"\n    tx = messages.StellarSignTx()\n    unpacker = xdrlib.Unpacker(tx_bytes)\n\n    tx.source_account = _xdr_read_address(unpacker)\n    tx.fee = unpacker.unpack_uint()\n    tx.sequence_number = unpacker.unpack_uhyper()\n\n    # Timebounds is an optional field\n    if unpacker.unpack_bool():\n        max_timebound = 2 ** 32 - 1  # max unsigned 32-bit int\n        # (trezor does not support the full 64-bit time value)\n\n        tx.timebounds_start = unpacker.unpack_uhyper()\n        tx.timebounds_end = unpacker.unpack_uhyper()\n\n        if tx.timebounds_start > max_timebound or tx.timebounds_start < 0:\n            raise ValueError(\n                \"Starting timebound out of range (must be between 0 and \"\n                + max_timebound\n            )\n        if tx.timebounds_end > max_timebound or tx.timebounds_end < 0:\n            raise ValueError(\n                \"Ending timebound out of range (must be between 0 and \" + max_timebound\n            )\n\n    # memo type determines what optional fields are set\n    tx.memo_type = unpacker.unpack_uint()\n\n    # text\n    if tx.memo_type == MEMO_TYPE_TEXT:\n        tx.memo_text = unpacker.unpack_string()\n    # id (64-bit uint)\n    if tx.memo_type == MEMO_TYPE_ID:\n        tx.memo_id = unpacker.unpack_uhyper()\n    # hash / return are the same structure (32 bytes representing a hash)\n    if tx.memo_type == MEMO_TYPE_HASH or tx.memo_type == MEMO_TYPE_RETURN:\n        tx.memo_hash = unpacker.unpack_fopaque(32)\n\n    tx.num_operations = unpacker.unpack_uint()\n\n    operations = []\n    for _ in range(tx.num_operations):\n        operations.append(_parse_operation_bytes(unpacker))\n\n    return tx, operations"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the next operation from the byte stream unpacker and returns a protobuf message representing the next operation as read from unpacker", "response": "def _parse_operation_bytes(unpacker):\n    \"\"\"Returns a protobuf message representing the next operation as read from\n    the byte stream in unpacker\n    \"\"\"\n\n    # Check for and parse optional source account field\n    source_account = None\n    if unpacker.unpack_bool():\n        source_account = unpacker.unpack_fopaque(32)\n\n    # Operation type (See OP_ constants)\n    type = unpacker.unpack_uint()\n\n    if type == OP_CREATE_ACCOUNT:\n        return messages.StellarCreateAccountOp(\n            source_account=source_account,\n            new_account=_xdr_read_address(unpacker),\n            starting_balance=unpacker.unpack_hyper(),\n        )\n\n    if type == OP_PAYMENT:\n        return messages.StellarPaymentOp(\n            source_account=source_account,\n            destination_account=_xdr_read_address(unpacker),\n            asset=_xdr_read_asset(unpacker),\n            amount=unpacker.unpack_hyper(),\n        )\n\n    if type == OP_PATH_PAYMENT:\n        op = messages.StellarPathPaymentOp(\n            source_account=source_account,\n            send_asset=_xdr_read_asset(unpacker),\n            send_max=unpacker.unpack_hyper(),\n            destination_account=_xdr_read_address(unpacker),\n            destination_asset=_xdr_read_asset(unpacker),\n            destination_amount=unpacker.unpack_hyper(),\n            paths=[],\n        )\n\n        num_paths = unpacker.unpack_uint()\n        for _ in range(num_paths):\n            op.paths.append(_xdr_read_asset(unpacker))\n\n        return op\n\n    if type == OP_MANAGE_OFFER:\n        return messages.StellarManageOfferOp(\n            source_account=source_account,\n            selling_asset=_xdr_read_asset(unpacker),\n            buying_asset=_xdr_read_asset(unpacker),\n            amount=unpacker.unpack_hyper(),\n            price_n=unpacker.unpack_uint(),\n            price_d=unpacker.unpack_uint(),\n            offer_id=unpacker.unpack_uhyper(),\n        )\n\n    if type == OP_CREATE_PASSIVE_OFFER:\n        return messages.StellarCreatePassiveOfferOp(\n            source_account=source_account,\n            selling_asset=_xdr_read_asset(unpacker),\n            buying_asset=_xdr_read_asset(unpacker),\n            amount=unpacker.unpack_hyper(),\n            price_n=unpacker.unpack_uint(),\n            price_d=unpacker.unpack_uint(),\n        )\n\n    if type == OP_SET_OPTIONS:\n        op = messages.StellarSetOptionsOp(source_account=source_account)\n\n        # Inflation destination\n        if unpacker.unpack_bool():\n            op.inflation_destination_account = _xdr_read_address(unpacker)\n\n        # clear flags\n        if unpacker.unpack_bool():\n            op.clear_flags = unpacker.unpack_uint()\n\n        # set flags\n        if unpacker.unpack_bool():\n            op.set_flags = unpacker.unpack_uint()\n\n        # master weight\n        if unpacker.unpack_bool():\n            op.master_weight = unpacker.unpack_uint()\n\n        # low threshold\n        if unpacker.unpack_bool():\n            op.low_threshold = unpacker.unpack_uint()\n\n        # medium threshold\n        if unpacker.unpack_bool():\n            op.medium_threshold = unpacker.unpack_uint()\n\n        # high threshold\n        if unpacker.unpack_bool():\n            op.high_threshold = unpacker.unpack_uint()\n\n        # home domain\n        if unpacker.unpack_bool():\n            op.home_domain = unpacker.unpack_string()\n\n        # signer\n        if unpacker.unpack_bool():\n            op.signer_type = unpacker.unpack_uint()\n            op.signer_key = unpacker.unpack_fopaque(32)\n            op.signer_weight = unpacker.unpack_uint()\n\n        return op\n\n    if type == OP_CHANGE_TRUST:\n        return messages.StellarChangeTrustOp(\n            source_account=source_account,\n            asset=_xdr_read_asset(unpacker),\n            limit=unpacker.unpack_uhyper(),\n        )\n\n    if type == OP_ALLOW_TRUST:\n        op = messages.StellarAllowTrustOp(\n            source_account=source_account,\n            trusted_account=_xdr_read_address(unpacker),\n            asset_type=unpacker.unpack_uint(),\n        )\n\n        if op.asset_type == ASSET_TYPE_ALPHA4:\n            op.asset_code = unpacker.unpack_fstring(4)\n        if op.asset_type == ASSET_TYPE_ALPHA12:\n            op.asset_code = unpacker.unpack_fstring(12)\n\n        op.is_authorized = unpacker.unpack_bool()\n\n        return op\n\n    if type == OP_ACCOUNT_MERGE:\n        return messages.StellarAccountMergeOp(\n            source_account=source_account,\n            destination_account=_xdr_read_address(unpacker),\n        )\n\n    # Inflation is not implemented since anyone can submit this operation to the network\n\n    if type == OP_MANAGE_DATA:\n        op = messages.StellarManageDataOp(\n            source_account=source_account, key=unpacker.unpack_string()\n        )\n\n        # Only set value if the field is present\n        if unpacker.unpack_bool():\n            op.value = unpacker.unpack_opaque()\n\n        return op\n\n    # Bump Sequence\n    # see: https://github.com/stellar/stellar-core/blob/master/src/xdr/Stellar-transaction.x#L269\n    if type == OP_BUMP_SEQUENCE:\n        return messages.StellarBumpSequenceOp(\n            source_account=source_account, bump_to=unpacker.unpack_uhyper()\n        )\n\n    raise ValueError(\"Unknown operation type: \" + str(type))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a stellar Asset from unpacker", "response": "def _xdr_read_asset(unpacker):\n    \"\"\"Reads a stellar Asset from unpacker\"\"\"\n    asset = messages.StellarAssetType(type=unpacker.unpack_uint())\n\n    if asset.type == ASSET_TYPE_ALPHA4:\n        asset.code = unpacker.unpack_fstring(4)\n        asset.issuer = _xdr_read_address(unpacker)\n\n    if asset.type == ASSET_TYPE_ALPHA12:\n        asset.code = unpacker.unpack_fstring(12)\n        asset.issuer = _xdr_read_address(unpacker)\n\n    return asset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _xdr_read_address(unpacker):\n    # First 4 bytes are the address type\n    address_type = unpacker.unpack_uint()\n    if address_type != 0:\n        raise ValueError(\"Unsupported address type\")\n\n    return address_from_public_key(unpacker.unpack_fopaque(32))", "response": "Reads a stellar address and returns the string representing the address"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _crc16_checksum(bytes):\n    crc = 0x0000\n    polynomial = 0x1021\n\n    for byte in bytes:\n        for i in range(8):\n            bit = (byte >> (7 - i) & 1) == 1\n            c15 = (crc >> 15 & 1) == 1\n            crc <<= 1\n            if c15 ^ bit:\n                crc ^= polynomial\n\n    return crc & 0xFFFF", "response": "Calculates the CRC - 16 checksum of a bytearray of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef b58encode(v):\n\n    long_value = 0\n    for c in v:\n        long_value = long_value * 256 + c\n\n    result = \"\"\n    while long_value >= __b58base:\n        div, mod = divmod(long_value, __b58base)\n        result = __b58chars[mod] + result\n        long_value = div\n    result = __b58chars[long_value] + result\n\n    # Bitcoin does a little leading-zero-compression:\n    # leading 0-bytes in the input become leading-1s\n    nPad = 0\n    for c in v:\n        if c == 0:\n            nPad += 1\n        else:\n            break\n\n    return (__b58chars[0] * nPad) + result", "response": "encode v which is a string of bytes to base58."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndecodes v into a string of len bytes. length is optional", "response": "def b58decode(v, length=None):\n    \"\"\" decode v into a string of len bytes.\"\"\"\n    if isinstance(v, bytes):\n        v = v.decode()\n\n    for c in v:\n        if c not in __b58chars:\n            raise ValueError(\"invalid Base58 string\")\n\n    long_value = 0\n    for (i, c) in enumerate(v[::-1]):\n        long_value += __b58chars.find(c) * (__b58base ** i)\n\n    result = b\"\"\n    while long_value >= 256:\n        div, mod = divmod(long_value, 256)\n        result = struct.pack(\"B\", mod) + result\n        long_value = div\n    result = struct.pack(\"B\", long_value) + result\n\n    nPad = 0\n    for c in v:\n        if c == __b58chars[0]:\n            nPad += 1\n        else:\n            break\n\n    result = b\"\\x00\" * nPad + result\n    if length is not None and len(result) != length:\n        return None\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts BIP32 path string to list of uint32 integers with hardened flags.", "response": "def parse_path(nstr: str) -> Address:\n    \"\"\"\n    Convert BIP32 path string to list of uint32 integers with hardened flags.\n    Several conventions are supported to set the hardened flag: -1, 1', 1h\n\n    e.g.: \"0/1h/1\" -> [0, 0x80000001, 1]\n\n    :param nstr: path string\n    :return: list of integers\n    \"\"\"\n    if not nstr:\n        return []\n\n    n = nstr.split(\"/\")\n\n    # m/a/b/c => a/b/c\n    if n[0] == \"m\":\n        n = n[1:]\n\n    # coin_name/a/b/c => 44'/SLIP44_constant'/a/b/c\n    if n[0] in slip44:\n        coin_id = slip44[n[0]]\n        n[0:1] = [\"44h\", \"{}h\".format(coin_id)]\n\n    def str_to_harden(x: str) -> int:\n        if x.startswith(\"-\"):\n            return H_(abs(int(x)))\n        elif x.endswith((\"h\", \"'\")):\n            return H_(int(x[:-1]))\n        else:\n            return int(x)\n\n    try:\n        return [str_to_harden(x) for x in n]\n    except Exception:\n        raise ValueError(\"Invalid BIP32 path\", nstr)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef normalize_nfc(txt):\n    if isinstance(txt, bytes):\n        txt = txt.decode()\n    return unicodedata.normalize(\"NFC\", txt).encode()", "response": "Normalize message to NFC and return bytes suitable for protobuf."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_protocol(handle: Handle, want_v2: bool) -> Protocol:\n    force_v1 = int(os.environ.get(\"TREZOR_PROTOCOL_V1\", 1))\n    if want_v2 and not force_v1:\n        return ProtocolV2(handle)\n    else:\n        return ProtocolV1(handle)", "response": "Return a Protocol instance for the given handle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ping(self) -> bool:\n        assert self.socket is not None\n        resp = None\n        try:\n            self.socket.sendall(b\"PINGPING\")\n            resp = self.socket.recv(8)\n        except Exception:\n            pass\n        return resp == b\"PONGPONG\"", "response": "Test if the device is listening."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pow2(x: int, p: int) -> int:\n    while p > 0:\n        x = x * x % q\n        p -= 1\n    return x", "response": "pow2 - > pow2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninverse of the curve25519 curve.", "response": "def inv(z: int) -> int:\n    \"\"\"$= z^{-1} mod q$, for z != 0\"\"\"\n    # Adapted from curve25519_athlon.c in djb's Curve25519.\n    z2 = z * z % q  # 2\n    z9 = pow2(z2, 2) * z % q  # 9\n    z11 = z9 * z2 % q  # 11\n    z2_5_0 = (z11 * z11) % q * z9 % q  # 31 == 2^5 - 2^0\n    z2_10_0 = pow2(z2_5_0, 5) * z2_5_0 % q  # 2^10 - 2^0\n    z2_20_0 = pow2(z2_10_0, 10) * z2_10_0 % q  # ...\n    z2_40_0 = pow2(z2_20_0, 20) * z2_20_0 % q\n    z2_50_0 = pow2(z2_40_0, 10) * z2_10_0 % q\n    z2_100_0 = pow2(z2_50_0, 50) * z2_50_0 % q\n    z2_200_0 = pow2(z2_100_0, 100) * z2_100_0 % q\n    z2_250_0 = pow2(z2_200_0, 50) * z2_50_0 % q  # 2^250 - 2^0\n    return pow2(z2_250_0, 5) * z11 % q"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef signature_unsafe(m: bytes, sk: bytes, pk: bytes) -> bytes:\n    h = H(sk)\n    a = decodecoord(h)\n    r = Hint(h[b // 8 : b // 4] + m)\n    R = scalarmult_B(r)\n    S = (r + Hint(encodepoint(R) + pk + m) * a) % l\n    return encodepoint(R) + encodeint(S)", "response": "This function is used to use with secret keys or secret data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef combine_keys(pks: Iterable[Ed25519PublicPoint]) -> Ed25519PublicPoint:\n    P = [_ed25519.decodepoint(pk) for pk in pks]\n    combine = reduce(_ed25519.edwards_add, P)\n    return Ed25519PublicPoint(_ed25519.encodepoint(combine))", "response": "Combine a list of Ed25519 points into a global CoSi key."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef combine_sig(\n    global_R: Ed25519PublicPoint, sigs: Iterable[Ed25519Signature]\n) -> Ed25519Signature:\n    \"\"\"Combine a list of signatures into a single CoSi signature.\"\"\"\n    S = [_ed25519.decodeint(si) for si in sigs]\n    s = sum(S) % _ed25519.l\n    sig = global_R + _ed25519.encodeint(s)\n    return Ed25519Signature(sig)", "response": "Combine a list of signatures into a single CoSi signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_nonce(\n    sk: Ed25519PrivateKey, data: bytes, ctr: int = 0\n) -> Tuple[int, Ed25519PublicPoint]:\n    \"\"\"Calculate CoSi nonces for given data.\n    These differ from Ed25519 deterministic nonces in that there is a counter appended at end.\n\n    Returns both the private point `r` and the partial signature `R`.\n    `r` is returned for performance reasons: :func:`sign_with_privkey`\n    takes it as its `nonce` argument so that it doesn't repeat the `get_nonce` call.\n\n    `R` should be combined with other partial signatures through :func:`combine_keys`\n    to obtain a \"global commitment\".\n    \"\"\"\n    # r = hash(hash(sk)[b .. 2b] + M + ctr)\n    # R = rB\n    h = _ed25519.H(sk)\n    bytesize = _ed25519.b // 8\n    assert len(h) == bytesize * 2\n    r = _ed25519.Hint(h[bytesize:] + data + ctr.to_bytes(4, \"big\"))\n    R = _ed25519.scalarmult(_ed25519.B, r)\n    return r, Ed25519PublicPoint(_ed25519.encodepoint(R))", "response": "Calculate CoSi nonces for given data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies Ed25519 signature. Raise exception if the signature is invalid.", "response": "def verify(\n    signature: Ed25519Signature, digest: bytes, pub_key: Ed25519PublicPoint\n) -> None:\n    \"\"\"Verify Ed25519 signature. Raise exception if the signature is invalid.\"\"\"\n    # XXX this *might* change to bool function\n    _ed25519.checkvalid(signature, digest, pub_key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sign_with_privkey(\n    digest: bytes,\n    privkey: Ed25519PrivateKey,\n    global_pubkey: Ed25519PublicPoint,\n    nonce: int,\n    global_commit: Ed25519PublicPoint,\n) -> Ed25519Signature:\n    \"\"\"Create a CoSi signature of `digest` with the supplied private key.\n    This function needs to know the global public key and global commitment.\n    \"\"\"\n    h = _ed25519.H(privkey)\n    a = _ed25519.decodecoord(h)\n\n    S = (nonce + _ed25519.Hint(global_commit + global_pubkey + digest) * a) % _ed25519.l\n    return Ed25519Signature(_ed25519.encodeint(S))", "response": "Create a CoSi signature of digest with the supplied private key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _patch_prebuild(cls):\n    orig_run = cls.run\n\n    def new_run(self):\n        self.run_command(\"prebuild\")\n        orig_run(self)\n\n    cls.run = new_run", "response": "Patch a setuptools command to depend on prebuild"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_default_client(path=None, ui=None, **kwargs):\n    from .transport import get_transport\n    from .ui import ClickUI\n\n    transport = get_transport(path, prefix_search=True)\n    if ui is None:\n        ui = ClickUI()\n\n    return TrezorClient(transport, ui, **kwargs)", "response": "Get a TrezorClient instance for a connected Trezor device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_entropy(strength, internal_entropy, external_entropy):\n    '''\n    strength - length of produced seed. One of 128, 192, 256\n    random - binary stream of random data from external HRNG\n    '''\n    if strength not in (128, 192, 256):\n        raise ValueError(\"Invalid strength\")\n\n    if not internal_entropy:\n        raise ValueError(\"Internal entropy is not provided\")\n\n    if len(internal_entropy) < 32:\n        raise ValueError(\"Internal entropy too short\")\n\n    if not external_entropy:\n        raise ValueError(\"External entropy is not provided\")\n\n    if len(external_entropy) < 32:\n        raise ValueError(\"External entropy too short\")\n\n    entropy = hashlib.sha256(internal_entropy + external_entropy).digest()\n    entropy_stripped = entropy[:strength // 8]\n\n    if len(entropy_stripped) * 8 != strength:\n        raise ValueError(\"Entropy length mismatch\")\n\n    return entropy_stripped", "response": "Generates the entropy of the internal and external HRNGs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initiate_upgrade_action(self, components_mask, action):\n\n        if action in (ACTION_UPLOAD_FOR_UPGRADE, ACTION_UPLOAD_FOR_COMPARE):\n            if self._get_component_count(components_mask) != 1:\n                raise HpmError(\"more than 1 component not support for action\")\n\n        self.send_message_with_name('InitiateUpgradeAction',\n                                    components=components_mask, action=action)", "response": "Initiate Upgrade Action\n        components:\n        action:\n            ACTION_BACKUP_COMPONENT = 0x00\n            ACTION_PREPARE_COMPONENT = 0x01\n            ACTION_UPLOAD_FOR_UPGRADE = 0x02\n            ACTION_UPLOAD_FOR_COMPARE = 0x03"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initiate_upgrade_action_and_wait(self, components_mask, action,\n                                         timeout=2, interval=0.1):\n        \"\"\" Initiate Upgrade Action and wait for\n            long running command. \"\"\"\n        try:\n            self.initiate_upgrade_action(components_mask, action)\n        except CompletionCodeError as e:\n            if e.cc == CC_LONG_DURATION_CMD_IN_PROGRESS:\n                self.wait_for_long_duration_command(\n                        constants.CMDID_HPM_INITIATE_UPGRADE_ACTION,\n                        timeout, interval)\n            else:\n                raise HpmError('initiate_upgrade_action CC=0x%02x' % e.cc)", "response": "Initiate Upgrade Action and wait for long running command."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupload all firmware blocks from a binary and wait for long running command.", "response": "def upload_binary(self, binary, timeout=2, interval=0.1):\n        \"\"\" Upload all firmware blocks from binary and wait for\n            long running command. \"\"\"\n        block_number = 0\n        block_size = self._determine_max_block_size()\n\n        for chunk in chunks(binary, block_size):\n            try:\n                self.upload_firmware_block(block_number, chunk)\n            except CompletionCodeError as e:\n                if e.cc == CC_LONG_DURATION_CMD_IN_PROGRESS:\n                    self.wait_for_long_duration_command(\n                            constants.CMDID_HPM_UPLOAD_FIRMWARE_BLOCK,\n                            timeout, interval)\n                else:\n                    raise HpmError('upload_firmware_block CC=0x%02x' % e.cc)\n            block_number += 1\n            block_number &= 0xff"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinishes the firmware upload process and wait for long running command.", "response": "def finish_upload_and_wait(self, component, length,\n                               timeout=2, interval=0.1):\n        \"\"\" Finish the firmware upload process and wait for\n            long running command. \"\"\"\n        try:\n            rsp = self.finish_firmware_upload(component, length)\n            check_completion_code(rsp.completion_code)\n        except CompletionCodeError as e:\n            if e.cc == CC_LONG_DURATION_CMD_IN_PROGRESS:\n                self.wait_for_long_duration_command(\n                            constants.CMDID_HPM_FINISH_FIRMWARE_UPLOAD,\n                            timeout, interval)\n            else:\n                raise HpmError('finish_firmware_upload CC=0x%02x' % e.cc)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef activate_firmware_and_wait(self, rollback_override=None,\n                                   timeout=2, interval=1):\n        \"\"\" Activate the new uploaded firmware and wait for\n            long running command. \"\"\"\n        try:\n            self.activate_firmware(rollback_override)\n        except CompletionCodeError as e:\n            if e.cc == CC_LONG_DURATION_CMD_IN_PROGRESS:\n                self.wait_for_long_duration_command(\n                            constants.CMDID_HPM_ACTIVATE_FIRMWARE,\n                            timeout, interval)\n            else:\n                raise HpmError('activate_firmware CC=0x%02x' % e.cc)\n        except IpmiTimeoutError:\n            # controller is in reset and flashed new firmware\n            pass", "response": "Activate the new uploaded firmware and wait for long running command."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode the data into the object.", "response": "def _decode_data(self, data):\n        \"\"\"`data` is array.array\"\"\"\n        self.major = data[0]\n\n        if data[1] is 0xff:\n            self.minor = data[1]\n        elif data[1] <= 0x99:\n            self.minor = int(data[1:2].tostring().decode('bcd+'))\n        else:\n            raise DecodingError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the routing of the current user.", "response": "def set_routing(self, routing):\n        \"\"\"Set the path over which a target is reachable.\n\n        The path is given as a list of tuples in the form (address,\n        bridge_channel).\n\n        Example #1: access to an ATCA blade in a chassis\n              slave = 0x81, target = 0x82\n              routing = [(0x81,0x20,0),(0x20,0x82,None)]\n\n        Example #2: access to an AMC in a uTCA chassis\n              slave = 0x81, target = 0x72\n              routing = [(0x81,0x20,0),(0x20,0x82,7),(0x20,0x72,None)]\n\n\n                         uTCA - MCH                        AMC\n                       .-------------------.             .--------.\n                       |       .-----------|             |        |\n                       | ShMC  | CM        |             | MMC    |\n            channel=0  |       |           |  channel=7  |        |\n        81 ------------| 0x20  |0x82  0x20 |-------------| 0x72   |\n                       |       |           |             |        |\n                       |       |           |             |        |\n                       |       `-----------|             |        |\n                       `-------------------\u00b4             `--------\u00b4\n          `------------\u00b4     `---\u00b4        `---------------\u00b4\n\n        Example #3: access to an AMC in a ATCA AMC carrier\n\n        slave = 0x81, target = 0x72\n        routing = [(0x81,0x20,0),(0x20,0x8e,7),(0x20,0x80,None)]\n\n        \"\"\"\n        if is_string(routing):\n            # if type(routing) in [unicode, str]:\n            routing = ast.literal_eval(routing)\n        self.routing = [Routing(*route) for route in routing]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef raw_command(self, lun, netfn, raw_bytes):\n        return self.interface.send_and_receive_raw(self.target, lun, netfn,\n                                                   raw_bytes)", "response": "Send the raw command data and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_and_receive(self, target, lun, netfn, cmdid, payload):\n        self._inc_sequence_number()\n\n        header = IpmbHeaderReq()\n        header.netfn = netfn\n        header.rs_lun = lun\n        header.rs_sa = target.ipmb_address\n        header.rq_seq = self.next_sequence_number\n        header.rq_lun = 0\n        header.rq_sa = self.slave_address\n        header.cmd_id = cmdid\n\n        # Bridge message\n        if target.routing:\n            tx_data = encode_bridged_message(target.routing, header, payload,\n                                             self.next_sequence_number)\n        else:\n            tx_data = encode_ipmb_msg(header, payload)\n\n        self._send_ipmi_msg(tx_data)\n\n        received = False\n        while received is False:\n            if not self._q.empty():\n                rx_data = self._q.get()\n            else:\n                rx_data = self._receive_ipmi_msg()\n\n            if array('B', rx_data)[5] == constants.CMDID_SEND_MESSAGE:\n                rx_data = decode_bridged_message(rx_data)\n\n            received = rx_filter(header, rx_data)\n\n            if not received:\n                self._q.put(rx_data)\n\n        return rx_data[6:-1]", "response": "Send and receive data using RMCP interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninterfaces function to send and receive raw bytes as bytestring.", "response": "def send_and_receive_raw(self, target, lun, netfn, raw_bytes):\n        \"\"\"Interface function to send and receive raw message.\n\n        target: IPMI target\n        lun: logical unit number\n        netfn: network function\n        raw_bytes: RAW bytes as bytestring\n\n        Returns the IPMI message response bytestring.\n        \"\"\"\n        return self._send_and_receive(target=target,\n                                      lun=lun,\n                                      netfn=netfn,\n                                      cmdid=array('B', raw_bytes)[0],\n                                      payload=raw_bytes[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_and_receive(self, req):\n        rx_data = self._send_and_receive(target=req.target,\n                                         lun=req.lun,\n                                         netfn=req.netfn,\n                                         cmdid=req.cmdid,\n                                         payload=encode_message(req))\n        rsp = create_message(req.netfn + 1, req.cmdid, req.group_extension)\n        decode_message(rsp, rx_data)\n        return rsp", "response": "Send and receive an IPMI message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run_ipmitool(cmd):\n\n        log().debug('Running ipmitool \"%s\"', cmd)\n\n        child = Popen(cmd, shell=True, stdout=PIPE)\n        output = child.communicate()[0]\n\n        log().debug('return with rc=%d, output was:\\n%s',\n                    child.returncode,\n                    output)\n\n        return output, child.returncode", "response": "Legacy call of ipmitool"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_sdr(self, record_id):\n\n        reservation_id = self.reserve_device_sdr_repository()\n        rsp = self.send_message_with_name('DeleteSdr',\n                                          reservation_id=reservation_id,\n                                          record_id=record_id)\n        return rsp.record_id", "response": "Deletes the sensor record specified by record_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear_repository_helper(reserve_fn, clear_fn, retry=5, reservation=None):\n\n    if reservation is None:\n        reservation = reserve_fn()\n\n    # start erasure\n    reservation = _clear_repository(reserve_fn, clear_fn,\n                                    INITIATE_ERASE, retry, reservation)\n\n    # give some time to clear\n    time.sleep(0.5)\n\n    # wait until finish\n    reservation = _clear_repository(reserve_fn, clear_fn,\n                                    GET_ERASE_STATUS, retry, reservation)", "response": "Helper function to start erasure and wait until finish."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode an IPMB message.", "response": "def encode_ipmb_msg(header, data):\n    \"\"\"Encode an IPMB message.\n\n    header: IPMB header object\n    data: IPMI message data as bytestring\n\n    Returns the message as bytestring.\n    \"\"\"\n    msg = array('B')\n\n    msg.fromstring(header.encode())\n    if data is not None:\n        a = array('B')\n        a.fromstring(data)\n        msg.extend(a)\n    msg.append(checksum(msg[3:]))\n    return msg.tostring()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a send message command and embedd the message as bytestring", "response": "def encode_send_message(payload, rq_sa, rs_sa, channel, seq, tracking=1):\n    \"\"\"Encode a send message command and embedd the message to be send.\n\n    payload: the message to be send as bytestring\n    rq_sa: the requester source address\n    rs_sa: the responder source address\n    channel: the channel\n    seq: the sequence number\n    tracking: tracking\n\n    Returns an encode send message as bytestring\n    \"\"\"\n    req = create_request_by_name('SendMessage')\n    req.channel.number = channel\n    req.channel.tracking = tracking\n    data = encode_message(req)\n\n    header = IpmbHeaderReq()\n    header.netfn = req.__netfn__\n    header.rs_lun = 0\n    header.rs_sa = rs_sa\n    header.rq_seq = seq\n    header.rq_lun = 0\n    header.rq_sa = rq_sa\n    header.cmd_id = req.__cmdid__\n\n    return encode_ipmb_msg(header, data + payload)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nencode a bridged message and embedd it into a single send message.", "response": "def encode_bridged_message(routing, header, payload, seq):\n    \"\"\"Encode a (multi-)bridged command and embedd the message to be send.\n\n    routing:\n    payload: the message to be send as bytestring\n    header:\n    seq: the sequence number\n\n    Returns the encoded send message as bytestring\n    \"\"\"\n\n    # change header requester addresses for bridging\n    header.rq_sa = routing[-1].rq_sa\n    header.rs_sa = routing[-1].rs_sa\n    tx_data = encode_ipmb_msg(header, payload)\n\n    for bridge in reversed(routing[:-1]):\n        tx_data = encode_send_message(tx_data,\n                                      rq_sa=bridge.rq_sa,\n                                      rs_sa=bridge.rs_sa,\n                                      channel=bridge.channel,\n                                      seq=seq)\n\n    return tx_data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a bridged command.", "response": "def decode_bridged_message(rx_data):\n    \"\"\"Decode a (multi-)bridged command.\n\n    rx_data: the received message as bytestring\n\n    Returns the decoded message as bytestring\n    \"\"\"\n\n    while array('B', rx_data)[5] == constants.CMDID_SEND_MESSAGE:\n        rsp = create_message(constants.NETFN_APP + 1,\n                             constants.CMDID_SEND_MESSAGE, None)\n        decode_message(rsp, rx_data[6:])\n        check_completion_code(rsp.completion_code)\n        rx_data = rx_data[7:-1]\n\n        if len(rx_data) < 6:\n            break\n    return rx_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rx_filter(header, data):\n\n    rsp_header = IpmbHeaderRsp()\n    rsp_header.decode(data)\n\n    data = array('B', data)\n\n    checks = [\n        (checksum(data[0:3]), 0, 'Header checksum failed'),\n        (checksum(data[3:]), 0, 'payload checksum failed'),\n        # rsp_header.rq_sa, header.rq_sa, 'slave address mismatch'),\n        (rsp_header.netfn, header.netfn | 1, 'NetFn mismatch'),\n        # rsp_header.rs_sa, header.rs_sa, 'target address mismatch'),\n        # rsp_header.rq_lun, header.rq_lun, 'request LUN mismatch'),\n        (rsp_header.rs_lun, header.rs_lun, 'responder LUN mismatch'),\n        (rsp_header.rq_seq, header.rq_seq, 'sequence number mismatch'),\n        (rsp_header.cmd_id, header.cmd_id, 'command id mismatch'),\n    ]\n\n    match = True\n\n    for left, right, msg in checks:\n        if left != right:\n            log().debug('{:s}: {:d} {:d}'.format(msg, left, right))\n            match = False\n\n    return match", "response": "Check if the message in rx_data matches to the information in header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npack the message and return an array.", "response": "def _pack(self):\n        \"\"\"Pack the message and return an array.\"\"\"\n        data = ByteBuffer()\n        if not hasattr(self, '__fields__'):\n            return data.array\n\n        for field in self.__fields__:\n            field.encode(self, data)\n        return data.array"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _encode(self):\n        data = ByteBuffer()\n        if not hasattr(self, '__fields__'):\n            return data.tostring()\n\n        for field in self.__fields__:\n            field.encode(self, data)\n        return data.tostring()", "response": "Encode the message and return a bytestring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecode the bytestring message.", "response": "def _decode(self, data):\n        \"\"\"Decode the bytestring message.\"\"\"\n        if not hasattr(self, '__fields__'):\n            raise NotImplementedError('You have to overwrite this method')\n\n        data = ByteBuffer(data)\n        cc = None\n        for field in self.__fields__:\n            try:\n                field.decode(self, data)\n            except CompletionCodeError as e:\n                # stop decoding on completion code != 0\n                cc = e.cc\n                break\n\n        if (cc is None or cc == 0) and len(data) > 0:\n            raise DecodingError('Data has extra bytes')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_and_receive(self, target, lun, netfn, cmdid, payload):\n        self._inc_sequence_number()\n\n        # assemble IPMB header\n        header = IpmbHeaderReq()\n        header.netfn = netfn\n        header.rs_lun = lun\n        header.rs_sa = target.ipmb_address\n        header.rq_seq = self.next_sequence_number\n        header.rq_lun = 0\n        header.rq_sa = self.slave_address\n        header.cmd_id = cmdid\n\n        retries = 0\n        while retries < self.max_retries:\n            try:\n                self._send_raw(header, payload)\n                rx_data = self._receive_raw(header)\n                break\n            except IpmiTimeoutError:\n                log().warning('I2C transaction timed out'),\n\n            retries += 1\n\n        else:\n            raise IpmiTimeoutError()\n\n        return rx_data.tostring()[5:-1]", "response": "Send and receive data using aardvark interface."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_device_sdr(self, record_id, reservation_id=None):\n        (next_id, record_data) = \\\n            get_sdr_data_helper(self.reserve_device_sdr_repository,\n                                self._get_device_sdr_chunk,\n                                record_id, reservation_id)\n\n        return sdr.SdrCommon.from_data(record_data, next_id)", "response": "Gets the SDRCommon object from the sensor device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef device_sdr_entries(self):\n        reservation_id = self.reserve_device_sdr_repository()\n        record_id = 0\n\n        while True:\n            record = self.get_device_sdr(record_id, reservation_id)\n            yield record\n            if record.next_id == 0xffff:\n                break\n            record_id = record.next_id", "response": "A generator that returns the SDR entries. Starting with ID = 0xffff is returned."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the raw reading and assertion states for the given sensor number.", "response": "def get_sensor_reading(self, sensor_number, lun=0):\n        \"\"\"Returns the sensor reading at the assertion states for the given\n        sensor number.\n\n        `sensor_number`\n\n        Returns a tuple with `raw reading`and `assertion states`.\n        \"\"\"\n        rsp = self.send_message_with_name('GetSensorReading',\n                                          sensor_number=sensor_number,\n                                          lun=lun)\n\n        reading = rsp.sensor_reading\n        if rsp.config.initial_update_in_progress:\n            reading = None\n\n        states = None\n        if rsp.states1 is not None:\n            states = rsp.states1\n            if rsp.states2 is not None:\n                states |= (rsp.states2 << 8)\n        return (reading, states)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_sensor_thresholds(self, sensor_number, lun=0,\n                              unr=None, ucr=None, unc=None,\n                              lnc=None, lcr=None, lnr=None):\n        \"\"\"Set the sensor thresholds that are not 'None'\n\n        `sensor_number`\n        `unr` for upper non-recoverable\n        `ucr` for upper critical\n        `unc` for upper non-critical\n        `lnc` for lower non-critical\n        `lcr` for lower critical\n        `lnr` for lower non-recoverable\n        \"\"\"\n        req = create_request_by_name('SetSensorThresholds')\n        req.sensor_number = sensor_number\n        req.lun = lun\n\n        thresholds = dict(unr=unr, ucr=ucr, unc=unc, lnc=lnc, lcr=lcr, lnr=lnr)\n\n        for key, value in thresholds.items():\n            if value is not None:\n                setattr(req.set_mask, key, 1)\n                setattr(req.threshold, key, value)\n\n        rsp = self.send_message(req)\n        check_completion_code(rsp.completion_code)", "response": "Set the sensor thresholds that are not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_feature(feature_type,\n                      feature_size,\n                      signal_magnitude,\n                      thickness=1):\n    \"\"\"Generate features corresponding to signal\n\n    Generate a single feature, that can be inserted into the signal volume.\n    A feature is a region of activation with a specific shape such as cube\n    or ring\n\n    Parameters\n    ----------\n\n    feature_type : str\n        What shape signal is being inserted? Options are 'cube',\n        'loop' (aka ring), 'cavity' (aka hollow sphere), 'sphere'.\n\n    feature_size : int\n        How big is the signal in diameter?\n\n    signal_magnitude : float\n        Set the signal size, a value of 1 means the signal is one standard\n        deviation of the noise\n\n    thickness : int\n        How thick is the surface of the loop/cavity\n\n    Returns\n    ----------\n\n    signal : 3 dimensional array\n        The volume representing the signal\n\n    \"\"\"\n\n    # If the size is equal to or less than 2 then all features are the same\n    if feature_size <= 2:\n        feature_type = 'cube'\n\n    # What kind of signal is it?\n    if feature_type == 'cube':\n\n        # Preset the size of the signal\n        signal = np.ones((feature_size, feature_size, feature_size))\n\n    elif feature_type == 'loop':\n\n        # First make a cube of zeros\n        signal = np.zeros((feature_size, feature_size, feature_size))\n\n        # Make a mesh grid of the space\n        seq = np.linspace(0, feature_size - 1,\n                          feature_size)\n        xx, yy = np.meshgrid(seq, seq)\n\n        # Make a disk corresponding to the whole mesh grid\n        xxmesh = (xx - ((feature_size - 1) / 2)) ** 2\n        yymesh = (yy - ((feature_size - 1) / 2)) ** 2\n        disk = xxmesh + yymesh\n\n        # What are the limits of the rings being made\n        outer_lim = disk[int((feature_size - 1) / 2), 0]\n        inner_lim = disk[int((feature_size - 1) / 2), thickness]\n\n        # What is the outer disk\n        outer = disk <= outer_lim\n\n        # What is the inner disk\n        inner = disk <= inner_lim\n\n        # Subtract the two disks to get a loop\n        loop = outer != inner\n\n        # Check if the loop is a disk\n        if np.all(inner is False):\n            logger.warning('Loop feature reduces to a disk because the loop '\n                           'is too thick')\n\n        # If there is complete overlap then make the signal just the\n        #  outer one\n        if np.all(loop is False):\n            loop = outer\n\n        # store the loop\n        signal[0:feature_size, 0:feature_size, int(np.round(feature_size /\n                                                            2))] = loop\n\n    elif feature_type == 'sphere' or feature_type == 'cavity':\n\n        # Make a mesh grid of the space\n        seq = np.linspace(0, feature_size - 1,\n                          feature_size)\n        xx, yy, zz = np.meshgrid(seq, seq, seq)\n\n        # Make a disk corresponding to the whole mesh grid\n        signal = ((xx - ((feature_size - 1) / 2)) ** 2 +\n                  (yy - ((feature_size - 1) / 2)) ** 2 +\n                  (zz - ((feature_size - 1) / 2)) ** 2)\n\n        # What are the limits of the rings being made\n        outer_lim = signal[int((feature_size - 1) / 2), int((feature_size -\n                                                             1) / 2), 0]\n        inner_lim = signal[int((feature_size - 1) / 2), int((feature_size -\n                                                             1) / 2),\n                           thickness]\n\n        # Is the signal a sphere or a cavity?\n        if feature_type == 'sphere':\n            signal = signal <= outer_lim\n\n        else:\n            # Get the inner and outer sphere\n            outer = signal <= outer_lim\n            inner = signal <= inner_lim\n\n            # Subtract the two disks to get a loop\n            signal = outer != inner\n\n            # Check if the cavity is a sphere\n            if np.all(inner is False):\n                logger.warning('Cavity feature reduces to a sphere because '\n                               'the cavity is too thick')\n\n            # If there is complete overlap then make the signal just the\n            #  outer one\n            if np.all(signal is False):\n                signal = outer\n\n    # Assign the signal magnitude\n    signal = signal * signal_magnitude\n\n    # Return the signal\n    return signal", "response": "Generate a single feature that can be inserted into the signal volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the indices of where to insert the signal into the signal volume", "response": "def _insert_idxs(feature_centre, feature_size, dimensions):\n    \"\"\"Returns the indices of where to put the signal into the signal volume\n\n    Parameters\n    ----------\n\n    feature_centre : list, int\n        List of coordinates for the centre location of the signal\n\n    feature_size : list, int\n        How big is the signal's diameter.\n\n    dimensions : 3 length array, int\n        What are the dimensions of the volume you wish to create\n\n\n    Returns\n    ----------\n    x_idxs : tuple\n        The x coordinates of where the signal is to be inserted\n\n    y_idxs : tuple\n        The y coordinates of where the signal is to be inserted\n\n    z_idxs : tuple\n        The z coordinates of where the signal is to be inserted\n\n    \"\"\"\n\n    # Set up the indexes within which to insert the signal\n    x_idx = [int(feature_centre[0] - (feature_size / 2)) + 1,\n             int(feature_centre[0] - (feature_size / 2) +\n                 feature_size) + 1]\n    y_idx = [int(feature_centre[1] - (feature_size / 2)) + 1,\n             int(feature_centre[1] - (feature_size / 2) +\n                 feature_size) + 1]\n    z_idx = [int(feature_centre[2] - (feature_size / 2)) + 1,\n             int(feature_centre[2] - (feature_size / 2) +\n                 feature_size) + 1]\n\n    # Check for out of bounds\n    # Min Boundary\n    if 0 > x_idx[0]:\n        x_idx[0] = 0\n    if 0 > y_idx[0]:\n        y_idx[0] = 0\n    if 0 > z_idx[0]:\n        z_idx[0] = 0\n\n    # Max Boundary\n    if dimensions[0] < x_idx[1]:\n        x_idx[1] = dimensions[0]\n    if dimensions[1] < y_idx[1]:\n        y_idx[1] = dimensions[1]\n    if dimensions[2] < z_idx[1]:\n        z_idx[1] = dimensions[2]\n\n    # Return the idxs for data\n    return x_idx, y_idx, z_idx"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating signal for a single time series of time series", "response": "def generate_signal(dimensions,\n                    feature_coordinates,\n                    feature_size,\n                    feature_type,\n                    signal_magnitude=[1],\n                    signal_constant=1,\n                    ):\n    \"\"\"Generate volume containing signal\n\n    Generate signal, of a specific shape in specific regions, for a single\n    volume. This will then be convolved with the HRF across time\n\n    Parameters\n    ----------\n\n    dimensions : 1d array, ndarray\n        What are the dimensions of the volume you wish to create\n\n    feature_coordinates : multidimensional array\n        What are the feature_coordinates of the signal being created.\n        Be aware of clipping: features far from the centre of the\n        brain will be clipped. If you wish to have multiple features\n        then list these as a features x 3 array. To create a feature of\n        a unique shape then supply all the individual\n        feature_coordinates of the shape and set the feature_size to 1.\n\n    feature_size : list, int\n        How big is the signal. If feature_coordinates=1 then only one value is\n        accepted, if feature_coordinates>1 then either one value must be\n        supplied or m values\n\n    feature_type : list, string\n        What feature_type of signal is being inserted? Options are cube,\n        loop, cavity, sphere. If feature_coordinates = 1 then\n        only one value is accepted, if feature_coordinates > 1 then either\n        one value must be supplied or m values\n\n    signal_magnitude : list, float\n        What is the (average) magnitude of the signal being generated? A\n        value of 1 means that the signal is one standard deviation from the\n        noise\n\n    signal_constant : list, bool\n        Is the signal constant across the feature (for univariate activity)\n        or is it a random pattern of a given magnitude across the feature (for\n        multivariate activity)\n\n    Returns\n    ----------\n    volume_signal : 3 dimensional array, float\n        Creates a single volume containing the signal\n\n    \"\"\"\n\n    # Preset the volume\n    volume_signal = np.zeros(dimensions)\n\n    feature_quantity = round(feature_coordinates.shape[0])\n\n    # If there is only one feature_size value then make sure to duplicate it\n    # for all signals\n    if len(feature_size) == 1:\n        feature_size = feature_size * feature_quantity\n\n    # Do the same for feature_type\n    if len(feature_type) == 1:\n        feature_type = feature_type * feature_quantity\n\n    if len(signal_magnitude) == 1:\n        signal_magnitude = signal_magnitude * feature_quantity\n\n    # Iterate through the signals and insert in the data\n    for signal_counter in range(feature_quantity):\n\n        # What is the centre of this signal\n        if len(feature_size) > 1:\n            feature_centre = np.asarray(feature_coordinates[signal_counter, ])\n        else:\n            feature_centre = np.asarray(feature_coordinates)[0]\n\n        # Generate the feature to be inserted in the volume\n        signal = _generate_feature(feature_type[signal_counter],\n                                   feature_size[signal_counter],\n                                   signal_magnitude[signal_counter],\n                                   )\n\n        # If the signal is a random noise pattern then multiply these ones by\n        # a noise mask\n        if signal_constant == 0:\n            signal = signal * np.random.random([feature_size[signal_counter],\n                                                feature_size[signal_counter],\n                                                feature_size[signal_counter]])\n\n        # Pull out the idxs for where to insert the data\n        x_idx, y_idx, z_idx = _insert_idxs(feature_centre,\n                                           feature_size[signal_counter],\n                                           dimensions)\n\n        # Insert the signal into the Volume\n        volume_signal[x_idx[0]:x_idx[1], y_idx[0]:y_idx[1], z_idx[0]:z_idx[\n            1]] = signal\n\n    return volume_signal"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the function for the timecourse events", "response": "def generate_stimfunction(onsets,\n                          event_durations,\n                          total_time,\n                          weights=[1],\n                          timing_file=None,\n                          temporal_resolution=100.0,\n                          ):\n    \"\"\"Return the function for the timecourse events\n\n    When do stimuli onset, how long for and to what extent should you\n    resolve the fMRI time course. There are two ways to create this, either\n    by supplying onset, duration and weight information or by supplying a\n    timing file (in the three column format used by FSL).\n\n    Parameters\n    ----------\n\n    onsets : list, int\n        What are the timestamps (in s) for when an event you want to\n        generate onsets?\n\n    event_durations : list, int\n        What are the durations (in s) of the events you want to\n        generate? If there is only one value then this will be assigned\n        to all onsets\n\n    total_time : int\n        How long (in s) is the experiment in total.\n\n    weights : list, float\n        What is the weight for each event (how high is the box car)? If\n        there is only one value then this will be assigned to all onsets\n\n    timing_file : string\n        The filename (with path) to a three column timing file (FSL) to\n        make the events. Still requires total_time to work\n\n    temporal_resolution : float\n        How many elements per second are you modeling for the\n        timecourse. This is useful when you want to model the HRF at an\n        arbitrarily high resolution (and then downsample to your TR later).\n\n    Returns\n    ----------\n\n    stim_function : 1 by timepoint array, float\n        The time course of stimulus evoked activation. This has a temporal\n        resolution of temporal resolution / 1.0 elements per second\n\n    \"\"\"\n\n    # If the timing file is supplied then use this to acquire the\n    if timing_file is not None:\n\n        # Read in text file line by line\n        with open(timing_file) as f:\n            text = f.readlines()  # Pull out file as a an array\n\n        # Preset\n        onsets = list()\n        event_durations = list()\n        weights = list()\n\n        # Pull out the onsets, weights and durations, set as a float\n        for line in text:\n            onset, duration, weight = line.strip().split()\n\n            # Check if the onset is more precise than the temporal resolution\n            upsampled_onset = float(onset) * temporal_resolution\n\n            # Because of float precision, the upsampled values might\n            # not round as expected .\n            # E.g. float('1.001') * 1000 = 1000.99\n            if np.allclose(upsampled_onset, np.round(upsampled_onset)) == 0:\n                warning = 'Your onset: ' + str(onset) + ' has more decimal ' \\\n                                                        'points than the ' \\\n                                                        'specified temporal ' \\\n                                                        'resolution can ' \\\n                                                        'resolve. This means' \\\n                                                        ' that events might' \\\n                                                        ' be missed. ' \\\n                                                        'Consider increasing' \\\n                                                        ' the temporal ' \\\n                                                        'resolution.'\n                logger.warning(warning)\n\n            onsets.append(float(onset))\n            event_durations.append(float(duration))\n            weights.append(float(weight))\n\n    # If only one duration is supplied then duplicate it for the length of\n    # the onset variable\n    if len(event_durations) == 1:\n        event_durations = event_durations * len(onsets)\n\n    if len(weights) == 1:\n        weights = weights * len(onsets)\n\n    # Check files\n    if np.max(onsets) > total_time:\n        raise ValueError('Onsets outside of range of total time.')\n\n    # Generate the time course as empty, each element is a millisecond by\n    # default\n    stimfunction = np.zeros((int(round(total_time * temporal_resolution)), 1))\n\n    # Cycle through the onsets\n    for onset_counter in list(range(len(onsets))):\n        # Adjust for the resolution\n        onset_idx = int(np.floor(onsets[onset_counter] * temporal_resolution))\n\n        # Adjust for the resolution\n        offset_idx = int(np.floor((onsets[onset_counter] + event_durations[\n            onset_counter]) * temporal_resolution))\n\n        # Store the weights\n        stimfunction[onset_idx:offset_idx, 0] = [weights[onset_counter]]\n\n    return stimfunction"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_3_column(stimfunction,\n                    filename,\n                    temporal_resolution=100.0\n                    ):\n    \"\"\" Output a tab separated three column timing file\n\n    This produces a three column tab separated text file, with the three\n    columns representing onset time (s), event duration (s) and weight,\n    respectively. Useful if you want to run the simulated data through FEAT\n    analyses. In a way, this is the reverse of generate_stimfunction\n\n    Parameters\n    ----------\n\n    stimfunction : timepoint by 1 array\n        The stimulus function describing the time course of events. For\n        instance output from generate_stimfunction.\n\n    filename : str\n        The name of the three column text file to be output\n\n    temporal_resolution : float\n        How many elements per second are you modeling with the\n        stimfunction?\n\n    \"\"\"\n\n    # Iterate through the stim function\n    stim_counter = 0\n    event_counter = 0\n    while stim_counter < stimfunction.shape[0]:\n\n        # Is it an event?\n        if stimfunction[stim_counter, 0] != 0:\n\n            # When did the event start?\n            event_onset = str(stim_counter / temporal_resolution)\n\n            # The weight of the stimulus\n            weight = str(stimfunction[stim_counter, 0])\n\n            # Reset\n            event_duration = 0\n\n            # Is the event still ongoing?\n            while stimfunction[stim_counter, 0] != 0 & stim_counter <= \\\n                    stimfunction.shape[0]:\n\n                # Add one millisecond to each duration\n                event_duration = event_duration + 1\n\n                # Increment\n                stim_counter = stim_counter + 1\n\n            # How long was the event in seconds\n            event_duration = str(event_duration / temporal_resolution)\n\n            # Append this row to the data file\n            with open(filename, \"a\") as file:\n                file.write(event_onset + '\\t' + event_duration + '\\t' +\n                           weight + '\\n')\n\n            # Increment the number of events\n            event_counter = event_counter + 1\n\n        # Increment\n        stim_counter = stim_counter + 1", "response": "This function generates a three column timing file for a given stimulus function and time course of events and a temporal resolution."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_epoch_file(stimfunction,\n                      filename,\n                      tr_duration,\n                      temporal_resolution=100.0\n                      ):\n    \"\"\" Output an epoch file, necessary for some inputs into brainiak\n\n    This takes in the time course of stimulus events and outputs the epoch\n    file used in Brainiak. The epoch file is a way to structure the timing\n    information in fMRI that allows you to flexibly input different stimulus\n    sequences. This is a list with each entry a 3d matrix corresponding to a\n    participant. The dimensions of the 3d matrix are condition by epoch by\n    time. For the i-th condition, if its k-th epoch spans time points t_m to\n    t_n-1, then [i, k, t_m:t_n] are 1 in the epoch file.\n\n    Parameters\n    ----------\n\n    stimfunction : list of timepoint by condition arrays\n        The stimulus function describing the time course of events. Each\n        list entry is from a different participant, each row is a different\n        timepoint (with the given temporal precision), each column is a\n        different condition. export_epoch_file is looking for differences in\n        the value of stimfunction to identify the start and end of an\n        epoch. If epochs in stimfunction are coded with the same weight and\n        there is no time between blocks then export_epoch_file won't be able to\n        label them as different epochs\n\n    filename : str\n        The name of the epoch file to be output\n\n    tr_duration : float\n        How long is each TR in seconds\n\n    temporal_resolution : float\n        How many elements per second are you modeling with the\n        stimfunction?\n\n    \"\"\"\n\n    # Cycle through the participants, different entries in the list\n    epoch_file = [0] * len(stimfunction)\n    for ppt_counter in range(len(stimfunction)):\n\n        # What is the time course for the participant (binarized)\n        stimfunction_ppt = np.abs(stimfunction[ppt_counter]) > 0\n\n        # Down sample the stim function\n        stride = tr_duration * temporal_resolution\n        stimfunction_downsampled = stimfunction_ppt[::int(stride), :]\n\n        # Calculates the number of event onsets. This uses changes in value\n        # to reflect different epochs. This might be false in some cases (the\n        # weight is non-uniform over an epoch or there is no break between\n        # identically weighted epochs).\n        epochs = 0  # Preset\n        conditions = stimfunction_ppt.shape[1]\n        for condition_counter in range(conditions):\n\n            weight_change = (np.diff(stimfunction_downsampled[:,\n                                     condition_counter], 1, 0) != 0)\n\n            # If the first or last events are 'on' then make these\n            # represent a epoch change\n            if stimfunction_downsampled[0, condition_counter] == 1:\n                weight_change[0] = True\n            if stimfunction_downsampled[-1, condition_counter] == 1:\n                weight_change[-1] = True\n\n            epochs += int(np.max(np.sum(weight_change, 0)) / 2)\n\n        # Get other information\n        trs = stimfunction_downsampled.shape[0]\n\n        # Make a timing file for this participant\n        epoch_file[ppt_counter] = np.zeros((conditions, epochs, trs))\n\n        # Cycle through conditions\n        epoch_counter = 0  # Reset and count across conditions\n        tr_counter = 0\n        while tr_counter < stimfunction_downsampled.shape[0]:\n\n            for condition_counter in range(conditions):\n\n                # Is it an event?\n                if tr_counter < stimfunction_downsampled.shape[0] and \\\n                                stimfunction_downsampled[\n                                    tr_counter, condition_counter] == 1:\n                    # Add a one for this TR\n                    epoch_file[ppt_counter][condition_counter,\n                                            epoch_counter, tr_counter] = 1\n\n                    # Find the next non event value\n                    end_idx = np.where(stimfunction_downsampled[tr_counter:,\n                                       condition_counter] == 0)[\n                        0][0]\n                    tr_idxs = list(range(tr_counter, tr_counter + end_idx))\n\n                    # Add ones to all the trs within this event time frame\n                    epoch_file[ppt_counter][condition_counter,\n                                            epoch_counter, tr_idxs] = 1\n\n                    # Start from this index\n                    tr_counter += end_idx\n\n                    # Increment\n                    epoch_counter += 1\n\n                # Increment the counter\n                tr_counter += 1\n\n        # Convert to boolean\n        epoch_file[ppt_counter] = epoch_file[ppt_counter].astype('bool')\n\n    # Save the file\n    np.save(filename, epoch_file)", "response": "This function takes in a list of stimulus functions and outputs an epoch file necessary for some inputs into brainiak."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the double gamma HRF for a single - site site.", "response": "def _double_gamma_hrf(response_delay=6,\n                      undershoot_delay=12,\n                      response_dispersion=0.9,\n                      undershoot_dispersion=0.9,\n                      response_scale=1,\n                      undershoot_scale=0.035,\n                      temporal_resolution=100.0,\n                      ):\n    \"\"\"Create the double gamma HRF with the timecourse evoked activity.\n    Default values are based on Glover, 1999 and Walvaert, Durnez,\n    Moerkerke, Verdoolaege and Rosseel, 2011\n\n    Parameters\n    ----------\n\n    response_delay : float\n        How many seconds until the peak of the HRF\n\n    undershoot_delay : float\n        How many seconds until the trough of the HRF\n\n    response_dispersion : float\n        How wide is the rising peak dispersion\n\n    undershoot_dispersion : float\n        How wide is the undershoot dispersion\n\n    response_scale : float\n         How big is the response relative to the peak\n\n    undershoot_scale :float\n        How big is the undershoot relative to the trough\n\n    scale_function : bool\n        Do you want to scale the function to a range of 1\n\n    temporal_resolution : float\n        How many elements per second are you modeling for the stimfunction\n    Returns\n    ----------\n\n    hrf : multi dimensional array\n        A double gamma HRF to be used for convolution.\n\n    \"\"\"\n\n    hrf_length = 30  # How long is the HRF being created\n\n    # How many seconds of the HRF will you model?\n    hrf = [0] * int(hrf_length * temporal_resolution)\n\n    # When is the peak of the two aspects of the HRF\n    response_peak = response_delay * response_dispersion\n    undershoot_peak = undershoot_delay * undershoot_dispersion\n\n    for hrf_counter in list(range(len(hrf) - 1)):\n\n        # Specify the elements of the HRF for both the response and undershoot\n        resp_pow = math.pow((hrf_counter / temporal_resolution) /\n                            response_peak, response_delay)\n        resp_exp = math.exp(-((hrf_counter / temporal_resolution) -\n                              response_peak) /\n                            response_dispersion)\n\n        response_model = response_scale * resp_pow * resp_exp\n\n        undershoot_pow = math.pow((hrf_counter / temporal_resolution) /\n                                  undershoot_peak,\n                                  undershoot_delay)\n        undershoot_exp = math.exp(-((hrf_counter / temporal_resolution) -\n                                    undershoot_peak /\n                                    undershoot_dispersion))\n\n        undershoot_model = undershoot_scale * undershoot_pow * undershoot_exp\n\n        # For this time point find the value of the HRF\n        hrf[hrf_counter] = response_model - undershoot_model\n\n    return hrf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncombining the signal volume with the signal volume", "response": "def apply_signal(signal_function,\n                 volume_signal,\n                 ):\n    \"\"\"Combine the signal volume with its timecourse\n\n    Apply the convolution of the HRF and stimulus time course to the\n    volume.\n\n    Parameters\n    ----------\n\n    signal_function : timepoint by timecourse array, float\n        The timecourse of the signal over time. If there is only one column\n        then the same timecourse is applied to all non-zero voxels in\n        volume_signal. If there is more than one column then each column is\n        paired with a non-zero voxel in the volume_signal (a 3d numpy array\n        generated in generate_signal).\n\n    volume_signal : multi dimensional array, float\n        The volume containing the signal to be convolved with the same\n        dimensions as the output volume. The elements in volume_signal\n        indicate how strong each signal in signal_function are modulated by\n        in the output volume\n\n\n    Returns\n    ----------\n    signal : multidimensional array, float\n        The convolved signal volume with the same 3d as volume signal and\n        the same 4th dimension as signal_function\n\n    \"\"\"\n\n    # How many timecourses are there within the signal_function\n    timepoints = signal_function.shape[0]\n    timecourses = signal_function.shape[1]\n\n    # Preset volume\n    signal = np.zeros([volume_signal.shape[0], volume_signal.shape[\n        1], volume_signal.shape[2], timepoints])\n\n    # Find all the non-zero voxels in the brain\n    idxs = np.where(volume_signal != 0)\n    if timecourses == 1:\n        # If there is only one time course supplied then duplicate it for\n        # every voxel\n        signal_function = np.matlib.repmat(signal_function, 1, len(idxs[0]))\n\n    elif len(idxs[0]) != timecourses:\n        raise IndexError('The number of non-zero voxels in the volume and '\n                         'the number of timecourses does not match. Aborting')\n\n    # For each coordinate with a non zero voxel, fill in the timecourse for\n    # that voxel\n    for idx_counter in range(len(idxs[0])):\n        x = idxs[0][idx_counter]\n        y = idxs[1][idx_counter]\n        z = idxs[2][idx_counter]\n\n        # Pull out the function for this voxel\n        signal_function_temp = signal_function[:, idx_counter]\n\n        # Multiply the voxel value by the function timecourse\n        signal[x, y, z, :] = volume_signal[x, y, z] * signal_function_temp\n\n    return signal"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _calc_fwhm(volume,\n               mask,\n               voxel_size=[1.0, 1.0, 1.0],\n               ):\n    \"\"\" Calculate the FWHM of a volume\n    Estimates the FWHM (mm) of a volume's non-masked voxels\n\n    Parameters\n    ----------\n\n    volume : 3 dimensional array\n        Functional data to have the FWHM measured.\n\n    mask : 3 dimensional array\n        A binary mask of the brain voxels in volume\n\n    voxel_size : length 3 list, float\n        Millimeters per voxel for x, y and z.\n\n    Returns\n    -------\n\n    fwhm : float, list\n        Returns the FWHM of each TR in mm\n\n    \"\"\"\n\n    # What are the dimensions of the volume\n    dimensions = volume.shape\n\n    # Iterate through the TRs, creating a FWHM for each TR\n\n    # Preset\n    v_count = 0\n    v_sum = 0\n    v_sq = 0\n\n    d_sum = [0.0, 0.0, 0.0]\n    d_sq = [0.0, 0.0, 0.0]\n    d_count = [0, 0, 0]\n\n    # Pull out all the voxel coordinates\n    coordinates = list(product(range(dimensions[0]),\n                               range(dimensions[1]),\n                               range(dimensions[2])))\n\n    # Find the sum of squared error for the non-masked voxels in the brain\n    for i in list(range(len(coordinates))):\n\n        # Pull out this coordinate\n        x, y, z = coordinates[i]\n\n        # Is this within the mask?\n        if mask[x, y, z] > 0:\n\n            # Find the the volume sum and squared values\n            v_count += 1\n            v_sum += np.abs(volume[x, y, z])\n            v_sq += volume[x, y, z] ** 2\n\n    # Get the volume variance\n    v_var = (v_sq - ((v_sum ** 2) / v_count)) / (v_count - 1)\n\n    for i in list(range(len(coordinates))):\n\n        # Pull out this coordinate\n        x, y, z = coordinates[i]\n\n        # Is this within the mask?\n        if mask[x, y, z] > 0:\n            # For each xyz dimension calculate the squared\n            # difference of this voxel and the next\n\n            in_range = (x < dimensions[0] - 1)\n            in_mask = in_range and (mask[x + 1, y, z] > 0)\n            included = in_mask and (~np.isnan(volume[x + 1, y, z]))\n            if included:\n                d_sum[0] += volume[x, y, z] - volume[x + 1, y, z]\n                d_sq[0] += (volume[x, y, z] - volume[x + 1, y, z]) ** 2\n                d_count[0] += 1\n\n            in_range = (y < dimensions[1] - 1)\n            in_mask = in_range and (mask[x, y + 1, z] > 0)\n            included = in_mask and (~np.isnan(volume[x, y + 1, z]))\n            if included:\n                d_sum[1] += volume[x, y, z] - volume[x, y + 1, z]\n                d_sq[1] += (volume[x, y, z] - volume[x, y + 1, z]) ** 2\n                d_count[1] += 1\n\n            in_range = (z < dimensions[2] - 1)\n            in_mask = in_range and (mask[x, y, z + 1] > 0)\n            included = in_mask and (~np.isnan(volume[x, y, z + 1]))\n            if included:\n                d_sum[2] += volume[x, y, z] - volume[x, y, z + 1]\n                d_sq[2] += (volume[x, y, z] - volume[x, y, z + 1]) ** 2\n                d_count[2] += 1\n\n    # Find the variance\n    d_var = np.divide((d_sq - np.divide(np.power(d_sum, 2),\n                                        d_count)), (np.add(d_count, -1)))\n\n    o_var = np.divide(-1, (4 * np.log(1 - (0.5 * d_var / v_var))))\n    fwhm3 = np.sqrt(o_var) * 2 * np.sqrt(2 * np.log(2))\n    fwhm = np.prod(np.multiply(fwhm3, voxel_size)) ** (1 / 3)\n\n    return fwhm", "response": "Calculate the FWHM of a given volume and mask."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calc_sfnr(volume,\n               mask,\n               ):\n    \"\"\" Calculate the the SFNR of a volume\n    Calculates the Signal to Fluctuation Noise Ratio, the mean divided\n    by the detrended standard deviation of each brain voxel. Based on\n    Friedman and Glover, 2006\n\n    Parameters\n    ----------\n\n    volume : 4d array, float\n        Take a volume time series\n\n    mask : 3d array, binary\n        A binary mask the same size as the volume\n\n    Returns\n    -------\n\n    snr : float\u2028\n        The SFNR of the volume\n\n    \"\"\"\n\n    # Make a matrix of brain voxels by time\n    brain_voxels = volume[mask > 0]\n\n    # Take the means of each voxel over time\n    mean_voxels = np.nanmean(brain_voxels, 1)\n\n    # Detrend (second order polynomial) the voxels over time and then\n    # calculate the standard deviation.\n    order = 2\n    seq = np.linspace(1, brain_voxels.shape[1], brain_voxels.shape[1])\n    detrend_poly = np.polyfit(seq, brain_voxels.transpose(), order)\n\n    # Detrend for each voxel\n    detrend_voxels = np.zeros(brain_voxels.shape)\n    for voxel in range(brain_voxels.shape[0]):\n        trend = detrend_poly[0, voxel] * seq ** 2 + detrend_poly[1, voxel] * \\\n                                                   seq + detrend_poly[2, voxel]\n        detrend_voxels[voxel, :] = brain_voxels[voxel, :] - trend\n\n    std_voxels = np.nanstd(detrend_voxels, 1)\n\n    # Calculate the sfnr of all voxels across the brain\n    sfnr_voxels = mean_voxels / std_voxels\n\n    # Return the average sfnr\n    return np.mean(sfnr_voxels)", "response": "Calculates the Signal to Fluctuation Noise Ratio the mean divided by the detrended standard deviation of each brain voxel and the mean divided by the mean divided by the std of each brain voxel"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the signal to ultimate noise ratio of a time series and non - brain voxels.", "response": "def _calc_snr(volume,\n              mask,\n              dilation=5,\n              reference_tr=None,\n              ):\n    \"\"\" Calculate the the SNR of a volume\n    Calculates the Signal to  Noise Ratio, the mean of brain voxels\n    divided by the standard deviation across non-brain voxels. Specify a TR\n    value to calculate the mean and standard deviation for that TR. To\n    calculate the standard deviation of non-brain voxels we can subtract\n    any baseline structure away first, hence getting at deviations due to the\n    system noise and not something like high baseline values in non-brain\n    parts of the body.\n\n    Parameters\n    ----------\n\n    volume : 4d array, float\n        Take a volume time series\n\n    mask : 3d array, binary\n        A binary mask the same size as the volume\n\n    dilation : int\n        How many binary dilations do you want to perform on the mask to\n        determine the non-brain voxels. If you increase this the SNR\n        increases and the non-brain voxels (after baseline subtraction) more\n        closely resemble a gaussian\n\n    reference_tr : int or list\n        Specifies the TR to calculate the SNR for. If multiple are supplied\n        then it will use the average of them.\n\n    Returns\n    -------\n\n    snr : float\u2028\n        The SNR of the volume\n\n    \"\"\"\n\n    # If no TR is specified then take all of them\n    if reference_tr is None:\n        reference_tr = list(range(volume.shape[3]))\n\n    # Dilate the mask in order to ensure that non-brain voxels are far from\n    # the brain\n    if dilation > 0:\n        mask_dilated = ndimage.morphology.binary_dilation(mask,\n                                                          iterations=dilation)\n    else:\n        mask_dilated = mask\n\n    # Make a matrix of brain and non_brain voxels, selecting the timepoint/s\n    brain_voxels = volume[mask > 0][:, reference_tr]\n    nonbrain_voxels = (volume[:, :, :, reference_tr]).astype('float64')\n\n    # If you have multiple TRs\n    if len(brain_voxels.shape) > 1:\n        brain_voxels = np.mean(brain_voxels, 1)\n        nonbrain_voxels = np.mean(nonbrain_voxels, 3)\n\n    nonbrain_voxels = nonbrain_voxels[mask_dilated == 0]\n\n    # Take the means of each voxel over time\n    mean_voxels = np.nanmean(brain_voxels)\n\n    # Find the standard deviation of the voxels\n    std_voxels = np.nanstd(nonbrain_voxels)\n\n    # Return the snr\n    return mean_voxels / std_voxels"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the noise properties of the provided volume and returns the noise properties of the provided volume.", "response": "def calc_noise(volume,\n               mask,\n               template,\n               noise_dict=None,\n               ):\n    \"\"\" Calculates the noise properties of the volume supplied.\n    This estimates what noise properties the volume has. For instance it\n    determines the spatial smoothness, the autoregressive noise, system\n    noise etc. Read the doc string for generate_noise to understand how\n    these different types of noise interact.\n\n    Parameters\n    ----------\n\n    volume : 4d numpy array, float\n        Take in a functional volume (either the file name or the numpy\n        array) to be used to estimate the noise properties of this\n\n    mask : 3d numpy array, binary\n        A binary mask of the brain, the same size as the volume\n    template : 3d array, float\n        A continuous (0 -> 1) volume describing the likelihood a voxel is in\n        the brain. This can be used to contrast the brain and non brain.\n\n    noise_dict : dict\n        The initialized dictionary of the calculated noise parameters of the\n        provided dataset (usually it is only the voxel size)\n    Returns\n    -------\n\n    noise_dict : dict\n        Return a dictionary of the calculated noise parameters of the provided\n        dataset\n\n    \"\"\"\n\n    # Check the inputs\n    if template.max() > 1.1:\n        raise ValueError('Template out of range')\n\n    # Create the mask if not supplied and set the mask size\n    if mask is None:\n        raise ValueError('Mask not supplied')\n\n    # Update noise dict if it is not yet created\n    if noise_dict is None:\n        noise_dict = {'voxel_size': [1.0, 1.0, 1.0]}\n    elif 'voxel_size' not in noise_dict:\n        noise_dict['voxel_size'] = [1.0, 1.0, 1.0]\n\n    # What is the max activation of the mean of this voxel (allows you to\n    # convert between the mask and the mean of the brain volume)\n    noise_dict['max_activity'] = np.nanmax(np.mean(volume, 3))\n\n    # Calculate the temporal variability of the volume\n    noise_dict['auto_reg_rho'], noise_dict['ma_rho'] = _calc_ARMA_noise(\n        volume, mask)\n\n    # Set it such that all of the temporal variability will be accounted for\n    #  by the AR component\n    noise_dict['auto_reg_sigma'] = 1\n\n    # Preset these values to be zero, as in you are not attempting to\n    # simulate them\n    noise_dict['physiological_sigma'] = 0\n    noise_dict['task_sigma'] = 0\n    noise_dict['drift_sigma'] = 0\n\n    # Calculate the sfnr\n    noise_dict['sfnr'] = _calc_sfnr(volume,\n                                    mask,\n                                    )\n\n    # Calculate the fwhm on a subset of volumes\n    if volume.shape[3] > 100:\n        # Take only 100 shuffled TRs\n        trs = np.random.choice(volume.shape[3], size=100, replace=False)\n    else:\n        trs = list(range(0, volume.shape[3]))\n\n    # Go through the trs and pull out the fwhm\n    fwhm = [0] * len(trs)\n    for tr in range(len(trs)):\n        fwhm[tr] = _calc_fwhm(volume[:, :, :, trs[tr]],\n                              mask,\n                              noise_dict['voxel_size'],\n                              )\n\n    # Keep only the mean\n    noise_dict['fwhm'] = np.mean(fwhm)\n    noise_dict['snr'] = _calc_snr(volume,\n                                  mask,\n                                  )\n\n    # Return the noise dictionary\n    return noise_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the system noise for the given set of dimensions and time series.", "response": "def _generate_noise_system(dimensions_tr,\n                           spatial_sd,\n                           temporal_sd,\n                           spatial_noise_type='gaussian',\n                           temporal_noise_type='gaussian',\n                           ):\n    \"\"\"Generate the scanner noise\n\n    Generate system noise, either rician, gaussian or exponential, for the\n    scanner. Generates a distribution with a SD of 1. If you look at the\n    distribution of non-brain voxel intensity in modern scans you will see\n    it is rician. However, depending on how you have calculated the SNR and\n    whether the template is being used you will want to use this function\n    differently: the voxels outside the brain tend to be stable over time and\n    usually reflect structure in the MR signal (e.g. the\n    baseline MR of the head coil or skull). Hence the template captures this\n    rician noise structure. If you are adding the machine noise to the\n    template, as is done in generate_noise, then you are likely doubling up\n    on the addition of machine noise. In such cases, machine noise seems to\n    be better modelled by gaussian noise on top of this rician structure.\n\n    Parameters\n    ----------\n\n    dimensions_tr : n length array, int\n        What are the dimensions of the volume you wish to insert\n        noise into. This can be a volume of any size\n\n    spatial_sd : float\n        What is the standard deviation in space of the noise volume to be\n        generated\n\n    temporal_sd : float\n        What is the standard deviation in time of the noise volume to be\n        generated\n\n    noise_type : str\n        String specifying the noise type. If you aren't specifying the noise\n        template then Rician is the appropriate model of noise. However,\n        if you are subtracting the template, as is default, then you should\n        use gaussian. (If the dilation parameter of _calc_snr is <10 then\n        gaussian is only an approximation)\n\n    Returns\n    ----------\n\n    system_noise : multidimensional array, float\n        Create a volume with system noise\n\n    \"\"\"\n    def noise_volume(dimensions,\n                     noise_type,\n                     ):\n\n        if noise_type == 'rician':\n            # Generate the Rician noise (has an SD of 1)\n            noise = stats.rice.rvs(b=0, loc=0, scale=1.527, size=dimensions)\n        elif noise_type == 'exponential':\n            # Make an exponential distribution (has an SD of 1)\n            noise = stats.expon.rvs(0, scale=1, size=dimensions)\n        elif noise_type == 'gaussian':\n            noise = np.random.randn(np.prod(dimensions)).reshape(dimensions)\n\n        # Return the noise\n        return noise\n\n    # Get just the xyz coordinates\n    dimensions = np.asarray([dimensions_tr[0],\n                             dimensions_tr[1],\n                             dimensions_tr[2],\n                             1])\n\n    # Generate noise\n    spatial_noise = noise_volume(dimensions, spatial_noise_type)\n    temporal_noise = noise_volume(dimensions_tr, temporal_noise_type)\n\n    # Make the system noise have a specific spatial variability\n    spatial_noise *= spatial_sd\n\n    # Set the size of the noise\n    temporal_noise *= temporal_sd\n\n    # The mean in time of system noise needs to be zero, so subtract the\n    # means of the temporal noise in time\n    temporal_noise_mean = np.mean(temporal_noise, 3).reshape(dimensions[0],\n                                                             dimensions[1],\n                                                             dimensions[2],\n                                                             1)\n    temporal_noise = temporal_noise - temporal_noise_mean\n\n    # Save the combination\n    system_noise = spatial_noise + temporal_noise\n\n    return system_noise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_noise_temporal_task(stimfunction_tr,\n                                  motion_noise='gaussian',\n                                  ):\n    \"\"\"Generate the signal dependent noise\n\n    Create noise specific to the signal, for instance there is variability\n    in how the signal manifests on each event\n\n    Parameters\n    ----------\n\n    stimfunction_tr : 1 Dimensional array\n        This is the timecourse of the stimuli in this experiment,\n        each element represents a TR\n\n    motion_noise : str\n        What type of noise will you generate? Can be gaussian or rician\n\n    Returns\n    ----------\n\n    noise_task : one dimensional array, float\n        Generates the temporal task noise timecourse\n\n    \"\"\"\n\n    # Make the noise to be added\n    stimfunction_tr = stimfunction_tr != 0\n    if motion_noise == 'gaussian':\n        noise = stimfunction_tr * np.random.normal(0, 1,\n                                                   size=stimfunction_tr.shape)\n    elif motion_noise == 'rician':\n        noise = stimfunction_tr * stats.rice.rvs(0, 1,\n                                                 size=stimfunction_tr.shape)\n\n    noise_task = stimfunction_tr + noise\n\n    # Normalize\n    noise_task = stats.zscore(noise_task).flatten()\n\n    return noise_task", "response": "Generate the signal dependent noise task for the given timecourse and motion"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the drift noise for a given set of timepoints.", "response": "def _generate_noise_temporal_drift(trs,\n                                   tr_duration,\n                                   basis=\"discrete_cos\",\n                                   period=150,\n                                   ):\n\n    \"\"\"Generate the drift noise\n\n    Create a trend (either sine or discrete_cos), of a given period and random\n    phase, to represent the drift of the signal over time\n\n    Parameters\n    ----------\n\n    trs : int\n        How many volumes (aka TRs) are there\n\n    tr_duration : float\n        How long in seconds is each volume acqusition\n\n    basis : str\n        What is the basis function for the drift. Could be made of discrete\n        cosines (for longer run durations, more basis functions are\n        created) or a sine wave.\n\n    period : int\n        How many seconds is the period of oscillation of the drift\n\n    Returns\n    ----------\n    noise_drift : one dimensional array, float\n        The drift timecourse of activity\n\n    \"\"\"\n\n    # Calculate drift differently depending on the basis function\n    if basis == 'discrete_cos':\n\n        # Specify each tr in terms of its phase with the given period\n        timepoints = np.linspace(0, trs - 1, trs)\n        timepoints = ((timepoints * tr_duration) / period) * 2 * np.pi\n\n        # Specify the other timing information\n        duration = trs * tr_duration\n        basis_funcs = int(np.floor(duration / period))  # How bases do you have\n\n        if basis_funcs == 0:\n            err_msg = 'Too few timepoints (' + str(trs) + ') to accurately ' \\\n                                                          'model drift'\n            logger.warning(err_msg)\n            basis_funcs = 1\n\n        noise_drift = np.zeros((timepoints.shape[0], basis_funcs))\n        for basis_counter in list(range(1, basis_funcs + 1)):\n\n            # What steps do you want to take for this basis function\n            timepoints_basis = (timepoints/basis_counter) + (np.random.rand()\n                                                             * np.pi * 2)\n\n            # Store the drift from this basis func\n            noise_drift[:, basis_counter - 1] = np.cos(timepoints_basis)\n\n        # Average the drift\n        noise_drift = np.mean(noise_drift, 1)\n\n    elif basis == 'sine':\n\n        # Calculate the cycles of the drift for a given function.\n        cycles = trs * tr_duration / period\n\n        # Create a sine wave with a given number of cycles and random phase\n        timepoints = np.linspace(0, trs - 1, trs)\n        phaseshift = np.pi * 2 * np.random.random()\n        phase = (timepoints / (trs - 1) * cycles * 2 * np.pi) + phaseshift\n        noise_drift = np.sin(phase)\n\n    # Normalize so the sigma is 1\n    noise_drift = stats.zscore(noise_drift)\n\n    # Return noise\n    return noise_drift"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the autoregression of the given timepoints.", "response": "def _generate_noise_temporal_autoregression(timepoints,\n                                            noise_dict,\n                                            dimensions,\n                                            mask,\n                                            ):\n\n    \"\"\"Generate the autoregression noise\n    Make a slowly drifting timecourse with the given autoregression\n    parameters. This can take in both AR and MA components\n\n    Parameters\n    ----------\n\n    timepoints : 1 Dimensional array\n        What time points are sampled by a TR\n\n    noise_dict : dict\n        A dictionary specifying the types of noise in this experiment. The\n        noise types interact in important ways. First, all noise types\n        ending with sigma (e.g. motion sigma) are mixed together in\n        _generate_temporal_noise. The sigma values describe the proportion of\n        mixing of these elements. However critically, SFNR is the\n        parameter that describes how much noise these components contribute\n        to the brain. If you set the noise dict to matched then it will fit\n        the parameters to match the participant as best as possible.\n\n    dimensions : 3 length array, int\n        What is the shape of the volume to be generated\n\n    mask : 3 dimensional array, binary\n        The masked brain, thresholded to distinguish brain and non-brain\n\n    Returns\n    ----------\n    noise_autoregression : one dimensional array, float\n        Generates the autoregression noise timecourse\n\n    \"\"\"\n\n    # Pull out the relevant noise parameters\n    auto_reg_rho = noise_dict['auto_reg_rho']\n    ma_rho = noise_dict['ma_rho']\n\n    # Specify the order based on the number of rho supplied\n    auto_reg_order = len(auto_reg_rho)\n    ma_order = len(ma_rho)\n\n    # This code assumes that the AR order is higher than the MA order\n    if ma_order > auto_reg_order:\n        msg = 'MA order (%d) is greater than AR order (%d). Cannot run.' % (\n            ma_order, auto_reg_order)\n        raise ValueError(msg)\n\n    # Generate a random variable at each time point that is a decayed value\n    # of the previous time points\n    noise_autoregression = np.zeros((dimensions[0], dimensions[1],\n                                     dimensions[2], len(timepoints)))\n    err_vols = np.zeros((dimensions[0], dimensions[1], dimensions[2],\n                         len(timepoints)))\n    for tr_counter in range(len(timepoints)):\n\n        # Create a brain shaped volume with appropriate smoothing properties\n        noise = _generate_noise_spatial(dimensions=dimensions,\n                                        mask=mask,\n                                        fwhm=noise_dict['fwhm'],\n                                        )\n\n        # Store all of the noise volumes\n        err_vols[:, :, :, tr_counter] = noise\n\n        if tr_counter == 0:\n            noise_autoregression[:, :, :, tr_counter] = noise\n\n        else:\n\n            # Preset the volume to collect the AR estimated process\n            AR_vol = np.zeros((dimensions[0], dimensions[1], dimensions[2]))\n\n            # Iterate through both the AR and MA values\n            for pCounter in list(range(1, auto_reg_order + 1)):\n                past_TR = int(tr_counter - pCounter)\n\n                if tr_counter - pCounter >= 0:\n\n                    # Pull out a previous TR\n                    past_vols = noise_autoregression[:, :, :, past_TR]\n\n                    # Add the discounted previous volume\n                    AR_vol += past_vols * auto_reg_rho[pCounter - 1]\n\n                    # If the MA order has at least this many coefficients\n                    # then consider the error terms\n                    if ma_order >= pCounter:\n\n                        # Pull out a previous TR\n                        past_noise = err_vols[:, :, :, past_TR]\n\n                        # Add the discounted previous noise\n                        AR_vol += past_noise * ma_rho[pCounter - 1]\n\n            noise_autoregression[:, :, :, tr_counter] = AR_vol + noise\n\n    # Z score the data so that all of the standard deviations of the voxels\n    # are one (but the ARMA coefs are unchanged)\n    noise_autoregression = stats.zscore(noise_autoregression, 3)\n\n    return noise_autoregression"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_noise_temporal_phys(timepoints,\n                                  resp_freq=0.2,\n                                  heart_freq=1.17,\n                                  ):\n    \"\"\"Generate the physiological noise.\n    Create noise representing the heart rate and respiration of the data.\n    Default values based on Walvaert, Durnez, Moerkerke, Verdoolaege and\n    Rosseel, 2011\n\n    Parameters\n    ----------\n\n    timepoints : 1 Dimensional array\n        What time points, in seconds, are sampled by a TR\n\n    resp_freq : float\n        What is the frequency of respiration (in Hz)\n\n    heart_freq : float\n        What is the frequency of heart beat (in Hz)\n\n    Returns\n    ----------\n    noise_phys : one dimensional array, float\n        Generates the physiological temporal noise timecourse\n\n    \"\"\"\n\n    resp_phase = (np.random.rand(1) * 2 * np.pi)[0]\n    heart_phase = (np.random.rand(1) * 2 * np.pi)[0]\n\n    # Find the rate for each timepoint\n    resp_rate = (resp_freq * 2 * np.pi)\n    heart_rate = (heart_freq * 2 * np.pi)\n\n    # Calculate the radians for each variable at this\n    # given TR\n    resp_radians = np.multiply(timepoints, resp_rate) + resp_phase\n    heart_radians = np.multiply(timepoints, heart_rate) + heart_phase\n\n    # Combine the two types of noise and append\n    noise_phys = np.cos(resp_radians) + np.sin(heart_radians)\n\n    # Normalize\n    noise_phys = stats.zscore(noise_phys)\n\n    return noise_phys", "response": "Generate the physiological noise for the given timepoints and heart frequencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_noise_spatial(dimensions,\n                            mask=None,\n                            fwhm=4.0,\n                            ):\n    \"\"\"Generate code for Gaussian Random Fields.\n\n    Adapted from code found here:\n    http://andrewwalker.github.io/statefultransitions/post/gaussian-fields/\n    with permission from the author:\n    https://twitter.com/walkera101/status/785578499440377858. Original code\n    comes from http://mathematica.stackexchange.com/questions/4829\n    /efficiently-generating-n-d-gaussian-random-fields with a WTFPL (\n    http://www.wtfpl.net).\n\n    Parameters\n    ----------\n\n    dimensions : 3 length array, int\n        What is the shape of the volume to be generated. This code\n        compresesses the range if the x and y dimensions are not equivalent.\n        This fixes this by upsampling and then downsampling the volume.\n\n    template : 3d array, float\n        A continuous (0 -> 1) volume describing the likelihood a voxel is in\n        the brain. This can be used to contrast the brain and non brain.\n\n    mask : 3 dimensional array, binary\n        The masked brain, thresholded to distinguish brain and non-brain\n\n    fwhm : float\n        What is the full width half max of the gaussian fields being created.\n        This is converted into a sigma which is used in this function.\n        However, this conversion was found empirically by testing values of\n        sigma and how it relates to fwhm values. The relationship that would be\n        found in such a test depends on the size of the brain (bigger brains\n        can have bigger fwhm).\n        However, small errors shouldn't matter too much since the fwhm\n        generated here can only be approximate anyway: firstly, although the\n        distribution that is being drawn from is set to this value,\n        this will manifest differently on every draw. Secondly, because of\n        the masking and dimensions of the generated volume, this does not\n        behave simply- wrapping effects matter (the outputs are\n        closer to the input value if you have no mask).\n        Use _calc_fwhm on this volume alone if you have concerns about the\n        accuracy of the fwhm.\n\n    Returns\n    ----------\n\n    noise_spatial : 3d array, float\n        Generates the spatial noise volume for these parameters\n\n    \"\"\"\n\n    # Check the input is correct\n    if len(dimensions) == 4:\n        logger.warning('4 dimensions have been supplied, only using 3')\n        dimensions = dimensions[0:3]\n\n    # If the dimensions are wrong then upsample now\n    if dimensions[0] != dimensions[1] or dimensions[1] != dimensions[2]:\n        max_dim = np.max(dimensions)\n        new_dim = (max_dim, max_dim, max_dim)\n    else:\n        new_dim = dimensions\n\n    def _logfunc(x, a, b, c):\n        \"\"\"Solve for y given x for log function.\n\n            Parameters\n            ----------\n\n            x : float\n                x value of log function\n\n            a : float\n                x shift of function\n\n            b : float\n                rate of change\n\n            c : float\n                y shift of function\n\n            Returns\n            ----------\n\n            float\n                y value of log function\n            \"\"\"\n        return (np.log(x + a) / np.log(b)) + c\n\n    def _fftIndgen(n):\n        \"\"\"# Specify the fft coefficents\n\n        Parameters\n        ----------\n\n        n : int\n            Dim size to estimate over\n\n        Returns\n        ----------\n\n        array of ints\n            fft indexes\n        \"\"\"\n\n        # Pull out the ascending and descending indexes\n        ascending = np.linspace(0, int(n / 2), int(n / 2 + 1))\n        elements = int(np.ceil(n / 2 - 1))  # Round up so that len(output)==n\n        descending = np.linspace(-elements, -1, elements)\n\n        return np.concatenate((ascending, descending))\n\n    def _Pk2(idxs, sigma):\n        \"\"\"# Specify the amplitude given the fft coefficents\n\n        Parameters\n        ----------\n\n        idxs : 3 by voxel array int\n            fft indexes\n\n        sigma : float\n            spatial sigma\n\n        Returns\n        ----------\n\n        amplitude : 3 by voxel array\n            amplitude of the fft coefficients\n        \"\"\"\n\n        # The first set of idxs ought to be zero so make the first value\n        # zero to avoid a divide by zero error\n        amp_start = np.array((0))\n\n        # Compute the amplitude of the function for a series of indices\n        amp_end = np.sqrt(np.sqrt(np.sum(idxs[:, 1:] ** 2, 0)) ** (-1 * sigma))\n        amplitude = np.append(amp_start, amp_end)\n\n        # Return the output\n        return amplitude\n\n    # Convert from fwhm to sigma (relationship discovered empirical, only an\n    #  approximation up to sigma = 0 -> 5 which corresponds to fwhm = 0 -> 8,\n    # relies on an assumption of brain size).\n    spatial_sigma = _logfunc(fwhm, -0.36778719, 2.10601011, 2.15439247)\n\n    noise = np.fft.fftn(np.random.normal(size=new_dim))\n\n    # Create a meshgrid of the object\n    fft_vol = np.meshgrid(_fftIndgen(new_dim[0]), _fftIndgen(new_dim[1]),\n                          _fftIndgen(new_dim[2]))\n\n    # Reshape the data into a vector\n    fft_vec = np.asarray((fft_vol[0].flatten(), fft_vol[1].flatten(), fft_vol[\n        2].flatten()))\n\n    # Compute the amplitude for each element in the grid\n    amp_vec = _Pk2(fft_vec, spatial_sigma)\n\n    # Reshape to be a brain volume\n    amplitude = amp_vec.reshape(new_dim)\n\n    # The output\n    noise_fft = (np.fft.ifftn(noise * amplitude)).real\n\n    # Fix the dimensionality of the data (if necessary)\n    noise_spatial = noise_fft[:dimensions[0], :dimensions[1], :dimensions[2]]\n\n    # Mask or not, then z score\n    if mask is not None:\n\n        # Mask the output\n        noise_spatial *= mask\n\n        # Z score the specific to the brain\n        noise_spatial[mask > 0] = stats.zscore(noise_spatial[mask > 0])\n    else:\n        # Take the grand mean/std and use for z scoring\n        grand_mean = (noise_spatial).mean()\n        grand_std = (noise_spatial).std()\n\n        noise_spatial = (noise_spatial - grand_mean) / grand_std\n\n    return noise_spatial", "response": "Generate code for Gaussian Random Fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the temporal noise timecourse for the given stimuli.", "response": "def _generate_noise_temporal(stimfunction_tr,\n                             tr_duration,\n                             dimensions,\n                             template,\n                             mask,\n                             noise_dict\n                             ):\n    \"\"\"Generate the temporal noise\n    Generate the time course of the average brain voxel. To change the\n    relative mixing of the noise components, change the sigma's specified\n    below.\n\n    Parameters\n    ----------\n\n    stimfunction_tr : 1 Dimensional array\n        This is the timecourse of the stimuli in this experiment,\n        each element represents a TR\n\n    tr_duration : int\n        How long is a TR, in seconds\n\n    dimensions : 3 length array, int\n        What is the shape of the volume to be generated\n\n    template : 3d array, float\n        A continuous (0 -> 1) volume describing the likelihood a voxel is in\n        the brain. This can be used to contrast the brain and non brain.\n\n    mask : 3 dimensional array, binary\n        The masked brain, thresholded to distinguish brain and non-brain\n\n    noise_dict : dict\n        A dictionary specifying the types of noise in this experiment. The\n        noise types interact in important ways. First, all noise types\n        ending with sigma (e.g. motion sigma) are mixed together in\n        _generate_temporal_noise. The sigma values describe the proportion of\n        mixing of these elements. However critically, SFNR is the\n        parameter that describes how much noise these components contribute\n        to the brain. If you set the noise dict to matched then it will fit\n        the parameters to match the participant as best as possible.\n\n    Returns\n    ----------\n\n    noise_temporal : one dimensional array, float\n        Generates the temporal noise timecourse for these parameters\n\n    \"\"\"\n\n    # Set up common parameters\n    # How many TRs are there\n    trs = len(stimfunction_tr)\n\n    # What time points are sampled by a TR?\n    timepoints = list(np.linspace(0, (trs - 1) * tr_duration, trs))\n\n    # Preset the volume\n    noise_volume = np.zeros((dimensions[0], dimensions[1], dimensions[2], trs))\n\n    # Generate the drift noise\n    if noise_dict['drift_sigma'] != 0:\n        # Calculate the drift time course\n        noise = _generate_noise_temporal_drift(trs,\n                                               tr_duration,\n                                               )\n        # Create a volume with the drift properties\n        volume = np.ones(dimensions)\n\n        # Combine the volume and noise\n        noise_volume += np.multiply.outer(volume, noise) * noise_dict[\n            'drift_sigma']\n\n    # Generate the physiological noise\n    if noise_dict['physiological_sigma'] != 0:\n\n        # Calculate the physiological time course\n        noise = _generate_noise_temporal_phys(timepoints,\n                                              )\n\n        # Create a brain shaped volume with similar smoothing properties\n        volume = _generate_noise_spatial(dimensions=dimensions,\n                                         mask=mask,\n                                         fwhm=noise_dict['fwhm'],\n                                         )\n\n        # Combine the volume and noise\n        noise_volume += np.multiply.outer(volume, noise) * noise_dict[\n            'physiological_sigma']\n\n    # Generate the AR noise\n    if noise_dict['auto_reg_sigma'] != 0:\n\n        # Calculate the AR time course volume\n        noise = _generate_noise_temporal_autoregression(timepoints,\n                                                        noise_dict,\n                                                        dimensions,\n                                                        mask,\n                                                        )\n\n        # Combine the volume and noise\n        noise_volume += noise * noise_dict['auto_reg_sigma']\n\n    # Generate the task related noise\n    if noise_dict['task_sigma'] != 0 and np.sum(stimfunction_tr) > 0:\n\n        # Calculate the task based noise time course\n        noise = _generate_noise_temporal_task(stimfunction_tr,\n                                              )\n\n        # Create a brain shaped volume with similar smoothing properties\n        volume = _generate_noise_spatial(dimensions=dimensions,\n                                         mask=mask,\n                                         fwhm=noise_dict['fwhm'],\n                                         )\n        # Combine the volume and noise\n        noise_volume += np.multiply.outer(volume, noise) * noise_dict[\n            'task_sigma']\n\n    # Finally, z score each voxel so things mix nicely\n    noise_volume = stats.zscore(noise_volume, 3)\n\n    # If it is a nan it is because you just divided by zero (since some\n    # voxels are zeros in the template)\n    noise_volume[np.isnan(noise_volume)] = 0\n\n    return noise_volume"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mask_brain(volume,\n               template_name=None,\n               mask_threshold=None,\n               mask_self=True,\n               ):\n    \"\"\" Mask the simulated volume\n    This creates a mask specifying the approximate likelihood that a voxel is\n    part of the brain. All values are bounded to the range of 0 to 1. An\n    appropriate threshold to isolate brain voxels is >0.2. Critically,\n    the data that should be used to create a template shouldn't already be\n    masked/skull stripped. If it is then it will give in accurate estimates\n    of non-brain noise and corrupt estimations of SNR.\n\n    Parameters\n    ----------\n\n    volume : multidimensional array\n        Either numpy array of a volume or a tuple describing the dimensions\n        of the mask to be created\n\n    template_name : str\n        What is the path to the template to be loaded? If empty then it\n        defaults to an MNI152 grey matter mask. This is ignored if mask_self\n        is True.\n\n    mask_threshold : float\n        What is the threshold (0 -> 1) for including a voxel in the mask? If\n        None then the program will try and identify the last wide peak in a\n        histogram of the template (assumed to be the brain voxels) and takes\n        the minima before that peak as the threshold. Won't work when the\n        data is not bimodal.\n\n    mask_self : bool or None\n        If set to true then it makes a mask from the volume supplied (by\n        averaging across time points and changing the range). If it is set\n        to false then it will use the template_name as an input.\n\n    Returns\n    ----------\n\n    mask : 3 dimensional array, binary\n        The masked brain, thresholded to distinguish brain and non-brain\n\n    template : 3 dimensional array, float\n        A continuous (0 -> 1) volume describing the likelihood a voxel is in\n        the brain. This can be used to contrast the brain and non brain.\n\n    \"\"\"\n\n    # If the volume supplied is a 1d array then output a volume of the\n    # supplied dimensions\n    if len(volume.shape) == 1:\n        volume = np.ones(volume)\n\n    # Load in the mask\n    if mask_self is True:\n        mask_raw = volume\n    elif template_name is None:\n        mask_raw = np.load(resource_stream(__name__, \"grey_matter_mask.npy\"))\n    else:\n        mask_raw = np.load(template_name)\n\n    # Make the masks 3dremove_baseline\n    if len(mask_raw.shape) == 3:\n        mask_raw = np.array(mask_raw)\n    elif len(mask_raw.shape) == 4 and mask_raw.shape[3] == 1:\n        mask_raw = np.array(mask_raw[:, :, :, 0])\n    else:\n        mask_raw = np.mean(mask_raw, 3)\n\n    # Find the max value (so you can calulate these as proportions)\n    mask_max = mask_raw.max()\n\n    # Make sure the mask values range from 0 to 1 (make out of max of volume\n    #  so that this is invertible later)\n    mask_raw = mask_raw / mask_max\n\n    # If there is only one brain volume then make this a forth dimension\n    if len(volume.shape) == 3:\n        temp = np.zeros([volume.shape[0], volume.shape[1], volume.shape[2], 1])\n        temp[:, :, :, 0] = volume\n        volume = temp\n\n    # Reshape the mask to be the size as the brain\n    brain_dim = volume.shape\n    mask_dim = mask_raw.shape\n\n    zoom_factor = (brain_dim[0] / mask_dim[0],\n                   brain_dim[1] / mask_dim[1],\n                   brain_dim[2] / mask_dim[2],\n                   )\n\n    # Scale the mask according to the input brain\n    # You might get a warning if the zoom_factor is not an integer but you\n    # can safely ignore that.\n    template = ndimage.zoom(mask_raw, zoom_factor, order=2)\n    template[template < 0] = 0\n\n    # If the mask threshold is not supplied then guess it is a minima\n    # between the two peaks of the bimodal distribution of voxel activity\n    if mask_threshold is None:\n\n        # How many bins on either side of a peak will be compared\n        order = 5\n\n        # Make the histogram\n        template_vector = template.reshape(brain_dim[0] * brain_dim[1] *\n                                           brain_dim[2])\n        template_hist = np.histogram(template_vector, 100)\n\n        # Zero pad the values\n        binval = np.concatenate([np.zeros((order,)), template_hist[0]])\n        bins = np.concatenate([np.zeros((order,)), template_hist[1]])\n\n        # Identify the first two peaks\n        peaks = signal.argrelmax(binval, order=order)[0][0:2]\n\n        # What is the minima between peaks\n        minima = binval[peaks[0]:peaks[1]].min()\n\n        # What is the index of the last idx with this min value (since if\n        # zero, there may be many)\n        minima_idx = (np.where(binval[peaks[0]:peaks[1]] == minima) + peaks[\n            0])[-1]\n\n        # Convert the minima into a threshold\n        mask_threshold = bins[minima_idx][0]\n\n    # Mask the template based on the threshold\n    mask = np.zeros(template.shape)\n    mask[template > mask_threshold] = 1\n\n    return mask, template", "response": "Masks a brain and non - brain volume."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _noise_dict_update(noise_dict):\n    # Create the default dictionary\n    default_dict = {'task_sigma': 0, 'drift_sigma': 0, 'auto_reg_sigma': 1,\n                    'auto_reg_rho': [0.5], 'ma_rho': [0.0],\n                    'physiological_sigma': 0, 'sfnr': 90, 'snr': 50,\n                    'max_activity': 1000, 'voxel_size': [1.0, 1.0, 1.0],\n                    'fwhm': 4, 'matched': 1}\n\n    # Check what noise is in the dictionary and add if necessary. Numbers\n    # determine relative proportion of noise\n    for default_key in default_dict:\n        if default_key not in noise_dict:\n            noise_dict[default_key] = default_dict[default_key]\n\n    return noise_dict", "response": "Update the noise dictionary with default values in case any of the components are missing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fit_spatial(noise,\n                 noise_temporal,\n                 mask,\n                 template,\n                 spatial_sd,\n                 temporal_sd,\n                 noise_dict,\n                 fit_thresh,\n                 fit_delta,\n                 iterations,\n                 ):\n    \"\"\"\n        Fit the noise model to match the SNR of the data\n\n        Parameters\n        ----------\n\n        noise : multidimensional array, float\n            Initial estimate of the noise\n\n        noise_temporal : multidimensional array, float\n            The temporal noise that was generated by _generate_temporal_noise\n\n        tr_duration : float\n            What is the duration, in seconds, of each TR?\n\n        template : 3d array, float\n            A continuous (0 -> 1) volume describing the likelihood a voxel\n            is in the brain. This can be used to contrast the brain and non\n            brain.\n\n        mask : 3d array, binary\n            The mask of the brain volume, distinguishing brain from non-brain\n\n        spatial_sd : float\n            What is the standard deviation in space of the noise volume to be\n            generated\n\n        temporal_sd : float\n            What is the standard deviation in time of the noise volume to be\n            generated\n\n        noise_dict : dict\n            A dictionary specifying the types of noise in this experiment. The\n            noise types interact in important ways. First, all noise types\n            ending with sigma (e.g. motion sigma) are mixed together in\n            _generate_temporal_noise. These values describe the proportion of\n            mixing of these elements. However critically, SFNR is the\n            parameter that describes how much noise these components contribute\n            to the brain. If you set the noise dict to matched then it will\n            fit the parameters to match the participant as best as possible.\n\n        fit_thresh : float\n            What proportion of the target parameter value is sufficient\n            error to warrant finishing fit search.\n\n        fit_delta : float\n            How much are the parameters attenuated during the fitting process,\n            in terms of the proportion of difference between the target\n            parameter and the actual parameter\n\n        iterations : int\n            The first element is how many steps of fitting the SFNR and SNR\n            values will be performed. Usually converges after < 5. The\n            second element is the number of iterations for the AR fitting.\n            This is much more time consuming (has to make a new timecourse\n            on each iteration) so be careful about setting this appropriately.\n\n        Returns\n        -------\n\n        noise : multidimensional array, float\n            Generates the noise volume given these parameters\n\n        \"\"\"\n\n    # Pull out information that is needed\n    dim_tr = noise.shape\n    base = template * noise_dict['max_activity']\n    base = base.reshape(dim_tr[0], dim_tr[1], dim_tr[2], 1)\n    mean_signal = (base[mask > 0]).mean()\n    target_snr = noise_dict['snr']\n\n    # Iterate through different parameters to fit SNR and SFNR\n    spat_sd_orig = np.copy(spatial_sd)\n    iteration = 0\n    for iteration in list(range(iterations)):\n\n        # Calculate the new metrics\n        new_snr = _calc_snr(noise, mask)\n\n        # Calculate the difference between the real and simulated data\n        diff_snr = abs(new_snr - target_snr) / target_snr\n\n        # If the AR is sufficiently close then break the loop\n        if diff_snr < fit_thresh:\n            logger.info('Terminated SNR fit after ' + str(\n                iteration) + ' iterations.')\n            break\n\n        # Convert the SFNR and SNR\n        spat_sd_new = mean_signal / new_snr\n\n        # Update the variable\n        spatial_sd -= ((spat_sd_new - spat_sd_orig) * fit_delta)\n\n        # Prevent these going out of range\n        if spatial_sd < 0 or np.isnan(spatial_sd):\n            spatial_sd = 10e-3\n\n        # Set up the machine noise\n        noise_system = _generate_noise_system(dimensions_tr=dim_tr,\n                                              spatial_sd=spatial_sd,\n                                              temporal_sd=temporal_sd,\n                                              )\n\n        # Sum up the noise of the brain\n        noise = base + (noise_temporal * temporal_sd) + noise_system\n\n        # Reject negative values (only happens outside of the brain)\n        noise[noise < 0] = 0\n\n    # Failed to converge\n    if iterations == 0:\n        logger.info('No fitting iterations were run')\n    elif iteration == iterations:\n        logger.warning('SNR failed to converge.')\n\n    # Return the updated noise\n    return noise, spatial_sd", "response": "Fit the spatial noise model to the target parameter value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit the noise model to match the SFNR and AR of the brain and non - brain.", "response": "def _fit_temporal(noise,\n                  mask,\n                  template,\n                  stimfunction_tr,\n                  tr_duration,\n                  spatial_sd,\n                  temporal_proportion,\n                  temporal_sd,\n                  noise_dict,\n                  fit_thresh,\n                  fit_delta,\n                  iterations,\n                  ):\n    \"\"\"\n        Fit the noise model to match the SFNR and AR of the data\n\n        Parameters\n        ----------\n\n        noise : multidimensional array, float\n            Initial estimate of the noise\n\n        mask : 3d array, binary\n            The mask of the brain volume, distinguishing brain from non-brain\n\n        template : 3d array, float\n            A continuous (0 -> 1) volume describing the likelihood a voxel\n            is in the brain. This can be used to contrast the brain and non\n            brain.\n\n        stimfunction_tr :  Iterable, list\n            When do the stimuli events occur. Each element is a TR\n\n        tr_duration : float\n            What is the duration, in seconds, of each TR?\n\n        spatial_sd : float\n            What is the standard deviation in space of the noise volume to be\n            generated\n\n        temporal_proportion, float\n            What is the proportion of the temporal variance (as specified by\n            the SFNR noise parameter) that is accounted for by the system\n            noise. If this number is high then all of the temporal\n            variability is due to system noise, if it is low then all of the\n            temporal variability is due to brain variability.\n\n        temporal_sd : float\n            What is the standard deviation in time of the noise volume to be\n            generated\n\n        noise_dict : dict\n            A dictionary specifying the types of noise in this experiment. The\n            noise types interact in important ways. First, all noise types\n            ending with sigma (e.g. motion sigma) are mixed together in\n            _generate_temporal_noise. These values describe the proportion of\n            mixing of these elements. However critically, SFNR is the\n            parameter that describes how much noise these components contribute\n            to the brain. If you set the noise dict to matched then it will\n            fit the parameters to match the participant as best as possible.\n\n        fit_thresh : float\n            What proportion of the target parameter value is sufficient\n            error to warrant finishing fit search.\n\n        fit_delta : float\n            How much are the parameters attenuated during the fitting process,\n            in terms of the proportion of difference between the target\n            parameter and the actual parameter\n\n        iterations : list, int\n            The first element is how many steps of fitting the SFNR and SNR\n            values will be performed. Usually converges after < 5. The\n            second element is the number of iterations for the AR fitting.\n            This is much more time consuming (has to make a new timecourse\n            on each iteration) so be careful about setting this appropriately.\n\n        Returns\n        -------\n\n        noise : multidimensional array, float\n            Generates the noise volume given these parameters\n\n    \"\"\"\n\n    # Pull out the\n    dim_tr = noise.shape\n    dim = dim_tr[0:3]\n    base = template * noise_dict['max_activity']\n    base = base.reshape(dim[0], dim[1], dim[2], 1)\n    mean_signal = (base[mask > 0]).mean()\n\n    # Iterate through different parameters to fit SNR and SFNR\n    temp_sd_orig = np.copy(temporal_sd)\n\n    # Make a copy of the dictionary so it can be modified\n    new_nd = copy.deepcopy(noise_dict)\n\n    # What SFNR do you want\n    target_sfnr = noise_dict['sfnr']\n\n    # What AR do you want?\n    target_ar = noise_dict['auto_reg_rho'][0]\n\n    # Iterate through different MA parameters to fit AR\n    for iteration in list(range(iterations)):\n\n        # If there are iterations left to perform then recalculate the\n        # metrics and try again\n\n        # Calculate the new SFNR\n        new_sfnr = _calc_sfnr(noise, mask)\n\n        # Calculate the AR\n        new_ar, _ = _calc_ARMA_noise(noise,\n                                     mask,\n                                     len(noise_dict['auto_reg_rho']),\n                                     len(noise_dict['ma_rho']),\n                                     )\n\n        # Calculate the difference between the real and simulated data\n        sfnr_diff = abs(new_sfnr - target_sfnr) / target_sfnr\n\n        # Calculate the difference in the first AR component\n        ar_diff = new_ar[0] - target_ar\n\n        # If the SFNR and AR is sufficiently close then break the loop\n        if (abs(ar_diff) / target_ar) < fit_thresh and sfnr_diff < fit_thresh:\n            msg = 'Terminated AR fit after ' + str(iteration) + ' iterations.'\n            logger.info(msg)\n            break\n\n        # Otherwise update the noise metrics. Get the new temporal noise value\n        temp_sd_new = mean_signal / new_sfnr\n        temporal_sd -= ((temp_sd_new - temp_sd_orig) * fit_delta)\n\n        # Prevent these going out of range\n        if temporal_sd < 0 or np.isnan(temporal_sd):\n            temporal_sd = 10e-3\n\n        # Set the new system noise\n        temp_sd_system_new = np.sqrt((temporal_sd ** 2) * temporal_proportion)\n\n        # Get the new AR value\n        new_nd['auto_reg_rho'][0] -= (ar_diff * fit_delta)\n\n        # Don't let the AR coefficient exceed 1\n        if new_nd['auto_reg_rho'][0] >= 1:\n            new_nd['auto_reg_rho'][0] = 0.99\n\n        # Generate the noise. The appropriate\n        noise_temporal = _generate_noise_temporal(stimfunction_tr,\n                                                  tr_duration,\n                                                  dim,\n                                                  template,\n                                                  mask,\n                                                  new_nd,\n                                                  )\n\n        # Set up the machine noise\n        noise_system = _generate_noise_system(dimensions_tr=dim_tr,\n                                              spatial_sd=spatial_sd,\n                                              temporal_sd=temp_sd_system_new,\n                                              )\n\n        # Sum up the noise of the brain\n        noise = base + (noise_temporal * temporal_sd) + noise_system\n\n        # Reject negative values (only happens outside of the brain)\n        noise[noise < 0] = 0\n\n    # Failed to converge\n    if iterations == 0:\n        logger.info('No fitting iterations were run')\n    elif iteration == iterations:\n        logger.warning('AR failed to converge.')\n\n    # Return the updated noise\n    return noise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_noise(dimensions,\n                   stimfunction_tr,\n                   tr_duration,\n                   template,\n                   mask=None,\n                   noise_dict=None,\n                   temporal_proportion=0.5,\n                   iterations=None,\n                   fit_thresh=0.05,\n                   fit_delta=0.5,\n                   ):\n    \"\"\" Generate the noise to be added to the signal.\n    Default noise parameters will create a noise volume with a standard\n    deviation of 0.1 (where the signal defaults to a value of 1). This has\n    built into estimates of how different types of noise mix. All noise\n    values can be set by the user or estimated with calc_noise.\n\n    Parameters\n    ----------\n\n    dimensions : nd array\n        What is the shape of the volume to be generated\n\n    stimfunction_tr :  Iterable, list\n        When do the stimuli events occur. Each element is a TR\n\n    tr_duration : float\n        What is the duration, in seconds, of each TR?\n\n    template : 3d array, float\n        A continuous (0 -> 1) volume describing the likelihood a voxel is in\n        the brain. This can be used to contrast the brain and non brain.\n\n    mask : 3d array, binary\n        The mask of the brain volume, distinguishing brain from non-brain\n\n    noise_dict : dictionary, float\n        This is a dictionary which describes the noise parameters of the\n        data. If there are no other variables provided then it will use\n        default values. The noise variables are as follows:\n\n        snr [float]: Ratio of MR signal to the spatial noise\n        sfnr [float]: Ratio of the MR signal to the temporal noise. This is the\n        total variability that the following sigmas 'sum' to:\n\n        task_sigma [float]: Size of the variance of task specific noise\n        drift_sigma [float]: Size of the variance of drift noise\n        auto_reg_sigma [float]: Size of the variance of autoregressive\n        noise. This is an ARMA process where the AR and MA components can be\n        separately specified\n        physiological_sigma [float]: Size of the variance of physiological\n        noise\n\n        auto_reg_rho [list]: The coefficients of the autoregressive\n        components you are modeling\n        ma_rho [list]:The coefficients of the moving average components you\n        are modeling\n        max_activity [float]: The max value of the averaged brain in order\n        to reference the template\n        voxel_size [list]: The mm size of the voxels\n        fwhm [float]: The gaussian smoothing kernel size (mm)\n        matched [bool]: Specify whether you are fitting the noise parameters\n\n        The volumes of brain noise that are generated have smoothness\n        specified by 'fwhm'\n\n    temporal_proportion, float\n        What is the proportion of the temporal variance (as specified by the\n        SFNR noise parameter) that is accounted for by the system noise. If\n        this number is high then all of the temporal variability is due to\n        system noise, if it is low then all of the temporal variability is\n        due to brain variability.\n\n    iterations : list, int\n        The first element is how many steps of fitting the SFNR and SNR values\n        will be performed. Usually converges after < 5. The second element\n        is the number of iterations for the AR fitting. This is much more\n        time consuming (has to make a new timecourse on each iteration) so\n        be careful about setting this appropriately.\n\n    fit_thresh : float\n        What proportion of the target parameter value is sufficient error to\n        warrant finishing fit search.\n\n    fit_delta : float\n        How much are the parameters attenuated during the fitting process,\n        in terms of the proportion of difference between the target\n        parameter and the actual parameter\n\n\n    Returns\n    ----------\n\n    noise : multidimensional array, float\n        Generates the noise volume for these parameters\n\n    \"\"\"\n\n    # Check the input data\n    if template.max() > 1.1:\n        raise ValueError('Template out of range')\n\n    # Change to be an empty dictionary if it is None\n    if noise_dict is None:\n        noise_dict = {}\n\n    # Take in the noise dictionary and add any missing information\n    noise_dict = _noise_dict_update(noise_dict)\n\n    # How many iterations will you perform? If unspecified it will set\n    # values based on whether you are trying to match noise specifically to\n    # this participant or just get in the ball park\n    if iterations is None:\n        if noise_dict['matched'] == 1:\n            iterations = [20, 20]\n        else:\n            iterations = [0, 0]\n\n    if abs(noise_dict['auto_reg_rho'][0]) - abs(noise_dict['ma_rho'][0]) < 0.1:\n        logger.warning('ARMA coefs are close, may have trouble fitting')\n\n    # What are the dimensions of the volume, including time\n    dimensions_tr = (dimensions[0],\n                     dimensions[1],\n                     dimensions[2],\n                     len(stimfunction_tr))\n\n    # Get the mask of the brain and set it to be 3d\n    if mask is None:\n        mask = np.ones(dimensions)\n\n    # Create the base (this inverts the process to make the template)\n    base = template * noise_dict['max_activity']\n\n    # Reshape the base (to be the same size as the volume to be created)\n    base = base.reshape(dimensions[0], dimensions[1], dimensions[2], 1)\n    base = np.ones(dimensions_tr) * base\n\n    # What is the mean signal of the non masked voxels in this template?\n    mean_signal = (base[mask > 0]).mean()\n\n    # Generate the noise\n    noise_temporal = _generate_noise_temporal(stimfunction_tr=stimfunction_tr,\n                                              tr_duration=tr_duration,\n                                              dimensions=dimensions,\n                                              template=template,\n                                              mask=mask,\n                                              noise_dict=noise_dict,\n                                              )\n\n    # Convert SFNR into the size of the standard deviation of temporal\n    # variability\n    temporal_sd = (mean_signal / noise_dict['sfnr'])\n\n    # Calculate the temporal sd of the system noise (as opposed to the noise\n    #  attributed to the functional variability).\n    temporal_sd_system = np.sqrt((temporal_sd ** 2) * temporal_proportion)\n\n    # What is the standard deviation of the background activity\n    spat_sd = mean_signal / noise_dict['snr']\n    spatial_sd = np.sqrt((spat_sd ** 2) * (1 - temporal_proportion))\n\n    # Set up the machine noise\n    noise_system = _generate_noise_system(dimensions_tr=dimensions_tr,\n                                          spatial_sd=spatial_sd,\n                                          temporal_sd=temporal_sd_system,\n                                          )\n\n    # Sum up the noise of the brain\n    noise = base + (noise_temporal * temporal_sd) + noise_system\n\n    # Reject negative values (only happens outside of the brain)\n    noise[noise < 0] = 0\n\n    # Fit the SNR\n    noise, spatial_sd = _fit_spatial(noise,\n                                     noise_temporal,\n                                     mask,\n                                     template,\n                                     spatial_sd,\n                                     temporal_sd_system,\n                                     noise_dict,\n                                     fit_thresh,\n                                     fit_delta,\n                                     iterations[0],\n                                     )\n\n    # Fit the SFNR and AR noise\n    noise = _fit_temporal(noise,\n                          mask,\n                          template,\n                          stimfunction_tr,\n                          tr_duration,\n                          spatial_sd,\n                          temporal_proportion,\n                          temporal_sd,\n                          noise_dict,\n                          fit_thresh,\n                          fit_delta,\n                          iterations[1],\n                          )\n\n    # Return the noise\n    return noise", "response": "Generates the noise volume for the given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading images from directory.", "response": "def load_images_from_dir(in_dir: Union[str, Path], suffix: str = \"nii.gz\",\n                         ) -> Iterable[SpatialImage]:\n    \"\"\"Load images from directory.\n\n    For efficiency, returns an iterator, not a sequence, so the results cannot\n    be accessed by indexing.\n\n    For every new iteration through the images, load_images_from_dir must be\n    called again.\n\n    Parameters\n    ----------\n    in_dir:\n        Path to directory.\n    suffix:\n        Only load images with names that end like this.\n\n    Yields\n    ------\n    SpatialImage\n        Image.\n    \"\"\"\n    if isinstance(in_dir, str):\n        in_dir = Path(in_dir)\n    files = sorted(in_dir.glob(\"*\" + suffix))\n    for f in files:\n        logger.debug(\n            'Starting to read file %s', f\n        )\n        yield nib.load(str(f))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading images from paths.", "response": "def load_images(image_paths: Iterable[Union[str, Path]]\n                ) -> Iterable[SpatialImage]:\n    \"\"\"Load images from paths.\n\n    For efficiency, returns an iterator, not a sequence, so the results cannot\n    be accessed by indexing.\n\n    For every new iteration through the images, load_images must be called\n    again.\n\n    Parameters\n    ----------\n    image_paths:\n        Paths to images.\n\n    Yields\n    ------\n    SpatialImage\n        Image.\n    \"\"\"\n    for image_path in image_paths:\n        if isinstance(image_path, Path):\n            string_path = str(image_path)\n        else:\n            string_path = image_path\n        logger.debug(\n            'Starting to read file %s', string_path\n        )\n        yield nib.load(string_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_boolean_mask(path: Union[str, Path],\n                      predicate: Callable[[np.ndarray], np.ndarray] = None\n                      ) -> np.ndarray:\n    \"\"\"Load boolean nibabel.SpatialImage mask.\n\n    Parameters\n    ----------\n    path\n        Mask path.\n    predicate\n        Callable used to create boolean values, e.g. a threshold function\n        ``lambda x: x > 50``.\n\n    Returns\n    -------\n    np.ndarray\n        Boolean array corresponding to mask.\n    \"\"\"\n    if not isinstance(path, str):\n        path = str(path)\n    data = nib.load(path).get_data()\n    if predicate is not None:\n        mask = predicate(data)\n    else:\n        mask = data.astype(np.bool)\n    return mask", "response": "Load boolean nibabel. SpatialImage mask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_labels(path: Union[str, Path]) -> List[SingleConditionSpec]:\n    condition_specs = np.load(str(path))\n    return [c.view(SingleConditionSpec) for c in condition_specs]", "response": "Load labels files.\n\n    Parameters\n    ----------\n    path\n        Path of labels file.\n\n    Returns\n    -------\n    List[SingleConditionSpec]\n        List of SingleConditionSpec stored in labels file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_as_nifti_file(data: np.ndarray, affine: np.ndarray,\n                       path: Union[str, Path]) -> None:\n    \"\"\"Create a Nifti file and save it.\n\n    Parameters\n    ----------\n    data\n        Brain data.\n    affine\n        Affine of the image, usually inherited from an existing image.\n    path\n        Output filename.\n    \"\"\"\n    if not isinstance(path, str):\n        path = str(path)\n    img = Nifti1Pair(data, affine)\n    nib.nifti1.save(img, path)", "response": "Create a Nifti file and save it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _converged(self):\n\n        prior = self.global_prior_[0:self.prior_size]\n        posterior = self.global_posterior_[0:self.prior_size]\n        diff = prior - posterior\n        max_diff = np.max(np.fabs(diff))\n        if self.verbose:\n            _, mse = self._mse_converged()\n            diff_ratio = np.sum(diff ** 2) / np.sum(posterior ** 2)\n            logger.info(\n                'htfa prior posterior max diff %f mse %f diff_ratio %f' %\n                ((max_diff, mse, diff_ratio)))\n\n        if max_diff > self.threshold:\n            return False, max_diff\n        else:\n            return True, max_diff", "response": "Check convergence based on maximum absolute difference between prior and posterior."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mse_converged(self):\n\n        prior = self.global_prior_[0:self.prior_size]\n        posterior = self.global_posterior_[0:self.prior_size]\n        mse = mean_squared_error(prior, posterior,\n                                 multioutput='uniform_average')\n        if mse > self.threshold:\n            return False, mse\n        else:\n            return True, mse", "response": "Checks convergence based on mean squared difference between prior and posterior."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _map_update(\n            self,\n            prior_mean,\n            prior_cov,\n            global_cov_scaled,\n            new_observation):\n        \"\"\"Maximum A Posterior (MAP) update of a parameter\n\n        Parameters\n        ----------\n\n        prior_mean : float or 1D array\n            Prior mean of parameters.\n\n        prior_cov : float or 1D array\n            Prior variance of scalar parameter, or\n            prior covariance of multivariate parameter\n\n        global_cov_scaled : float or 1D array\n            Global prior variance of scalar parameter, or\n            global prior covariance of multivariate parameter\n\n        new_observation : 1D or 2D array, with shape [n_dim, n_subj]\n            New observations on parameters.\n\n        Returns\n        -------\n\n        posterior_mean : float or 1D array\n            Posterior mean of parameters.\n\n        posterior_cov : float or 1D array\n            Posterior variance of scalar parameter, or\n            posterior covariance of multivariate parameter\n\n        \"\"\"\n        common = np.linalg.inv(prior_cov + global_cov_scaled)\n        observation_mean = np.mean(new_observation, axis=1)\n        posterior_mean = prior_cov.dot(common.dot(observation_mean)) +\\\n            global_cov_scaled.dot(common.dot(prior_mean))\n        posterior_cov =\\\n            prior_cov.dot(common.dot(global_cov_scaled))\n        return posterior_mean, posterior_cov", "response": "This function is used to calculate the maximum A Posterior update of a parameter set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _map_update_posterior(self):\n        self.global_posterior_ = self.global_prior_.copy()\n        prior_centers = self.get_centers(self.global_prior_)\n        prior_widths = self.get_widths(self.global_prior_)\n        prior_centers_mean_cov = self.get_centers_mean_cov(self.global_prior_)\n        prior_widths_mean_var = self.get_widths_mean_var(self.global_prior_)\n        center_size = self.K * self.n_dim\n        posterior_size = center_size + self.K\n        for k in np.arange(self.K):\n            next_centers = np.zeros((self.n_dim, self.n_subj))\n            next_widths = np.zeros(self.n_subj)\n            for s in np.arange(self.n_subj):\n                center_start = s * posterior_size\n                width_start = center_start + center_size\n                start_idx = center_start + k * self.n_dim\n                end_idx = center_start + (k + 1) * self.n_dim\n                next_centers[:, s] = self.gather_posterior[start_idx:end_idx]\\\n                    .copy()\n                next_widths[s] = self.gather_posterior[width_start + k].copy()\n\n            # centers\n            posterior_mean, posterior_cov = self._map_update(\n                prior_centers[k].T.copy(),\n                from_tri_2_sym(prior_centers_mean_cov[k], self.n_dim),\n                self.global_centers_cov_scaled,\n                next_centers)\n            self.global_posterior_[k * self.n_dim:(k + 1) * self.n_dim] =\\\n                posterior_mean.T\n            start_idx = self.map_offset[2] + k * self.cov_vec_size\n            end_idx = self.map_offset[2] + (k + 1) * self.cov_vec_size\n            self.global_posterior_[start_idx:end_idx] =\\\n                from_sym_2_tri(posterior_cov)\n\n            # widths\n            common = 1.0 /\\\n                (prior_widths_mean_var[k] + self.global_widths_var_scaled)\n            observation_mean = np.mean(next_widths)\n            tmp = common * self.global_widths_var_scaled\n            self.global_posterior_[self.map_offset[1] + k] = \\\n                prior_widths_mean_var[k] * common * observation_mean +\\\n                tmp * prior_widths[k]\n            self.global_posterior_[self.map_offset[3] + k] = \\\n                prior_widths_mean_var[k] * tmp\n\n        return self", "response": "This function updates the maximum A Posterior of the class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_gather_offset(self, size):\n\n        gather_size = np.zeros(size).astype(int)\n        gather_offset = np.zeros(size).astype(int)\n        num_local_subjs = np.zeros(size).astype(int)\n        subject_map = {}\n\n        for idx, s in enumerate(np.arange(self.n_subj)):\n            cur_rank = idx % size\n            gather_size[cur_rank] += self.prior_size\n            subject_map[idx] = (cur_rank, num_local_subjs[cur_rank])\n            num_local_subjs[cur_rank] += 1\n\n        for idx in np.arange(size - 1) + 1:\n            gather_offset[idx] = gather_offset[idx - 1] + gather_size[idx - 1]\n\n        tuple_size = tuple(gather_size)\n        tuple_offset = tuple(gather_offset)\n        return tuple_size, tuple_offset, subject_map", "response": "Calculate the offset for gather result from this process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_weight_size(self, data, n_local_subj):\n\n        weight_size = np.zeros(1).astype(int)\n        local_weight_offset = np.zeros(n_local_subj).astype(int)\n        for idx, subj_data in enumerate(data):\n            if idx > 0:\n                local_weight_offset[idx] = weight_size[0]\n            weight_size[0] += self.K * subj_data.shape[1]\n        return weight_size, local_weight_offset", "response": "Calculate the size of the weight array for this process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate metadata for subjects allocated to this process.", "response": "def _get_subject_info(self, n_local_subj, data):\n        \"\"\"Calculate metadata for subjects allocated to this process\n\n        Parameters\n        ----------\n\n        n_local_subj : int\n            Number of subjects allocated to this process.\n\n        data : list of 2D array. Each in shape [n_voxel, n_tr]\n            Total number of MPI process.\n\n        Returns\n        -------\n\n        max_sample_tr : 1D array\n            Maximum number of TR to subsample for each subject\n\n        max_sample_voxel : 1D array\n            Maximum number of voxel to subsample for each subject\n\n        \"\"\"\n\n        max_sample_tr = np.zeros(n_local_subj).astype(int)\n        max_sample_voxel = np.zeros(n_local_subj).astype(int)\n        for idx in np.arange(n_local_subj):\n            nvoxel = data[idx].shape[0]\n            ntr = data[idx].shape[1]\n            max_sample_voxel[idx] =\\\n                min(self.max_voxel, int(self.voxel_ratio * nvoxel))\n            max_sample_tr[idx] = min(self.max_tr, int(self.tr_ratio * ntr))\n        return max_sample_tr, max_sample_voxel"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_mpi_info(self):\n\n        rank = self.comm.Get_rank()\n        size = self.comm.Get_size()\n        return rank, size", "response": "get basic MPI info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing prior for this subject based on the given process and R", "response": "def _init_prior_posterior(self, rank, R, n_local_subj):\n        \"\"\"set prior for this subject\n\n        Parameters\n        ----------\n\n        rank : integer\n            The rank of this process\n\n        R : list of 2D arrays, element i has shape=[n_voxel, n_dim]\n            Each element in the list contains the scanner coordinate matrix\n            of fMRI data of one subject.\n\n        n_local_subj : integer\n            The number of subjects allocated to this process.\n\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n\n        \"\"\"\n\n        if rank == 0:\n            idx = np.random.choice(n_local_subj, 1)\n            self.global_prior_, self.global_centers_cov,\\\n                self.global_widths_var = self.get_template(R[idx[0]])\n            self.global_centers_cov_scaled =\\\n                self.global_centers_cov / float(self.n_subj)\n            self.global_widths_var_scaled =\\\n                self.global_widths_var / float(self.n_subj)\n            self.gather_posterior = np.zeros(self.n_subj * self.prior_size)\n            self.global_posterior_ = np.zeros(self.prior_size)\n        else:\n            self.global_prior_ = np.zeros(self.prior_bcast_size)\n            self.global_posterior_ = None\n            self.gather_posterior = None\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _gather_local_posterior(self, use_gather,\n                                gather_size, gather_offset):\n        \"\"\"Gather/Gatherv local posterior\n\n\n        Parameters\n        ----------\n        comm : object\n            MPI communication group\n\n        use_gather : boolean\n            Whether to use Gather or Gatherv\n\n        gather_size : 1D array\n            The size of each local posterior\n\n        gather_offset : 1D array\n            The offset of each local posterior\n\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n\n\n        Notes\n        -----\n        We use numpy array rather than generic Python objects for MPI\n        communication because Gatherv is only supported for the former.\n        https://pythonhosted.org/mpi4py/usrman/tutorial.html\n\n        \"\"\"\n        if use_gather:\n            self.comm.Gather(self.local_posterior_,\n                             self.gather_posterior, root=0)\n        else:\n            target = [\n                self.gather_posterior,\n                gather_size,\n                gather_offset,\n                MPI.DOUBLE]\n            self.comm.Gatherv(self.local_posterior_, target)\n        return self", "response": "Gather or Gatherv local posterior of the local posterior of the instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nassigning posterior to the right prior based on Hungarian algorithm Returns the instance itself.", "response": "def _assign_posterior(self):\n        \"\"\"assign posterior to the right prior based on\n           Hungarian algorithm\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n        \"\"\"\n\n        prior_centers = self.get_centers(self.global_prior_)\n        posterior_centers = self.get_centers(self.global_posterior_)\n        posterior_widths = self.get_widths(self.global_posterior_)\n        posterior_centers_mean_cov =\\\n            self.get_centers_mean_cov(self.global_posterior_)\n        posterior_widths_mean_var =\\\n            self.get_widths_mean_var(self.global_posterior_)\n        # linear assignment on centers\n        cost = distance.cdist(prior_centers, posterior_centers, 'euclidean')\n        _, col_ind = linear_sum_assignment(cost)\n        # reorder centers/widths based on cost assignment\n        self.set_centers(self.global_posterior_, posterior_centers)\n        self.set_widths(self.global_posterior_, posterior_widths)\n        # reorder cov/var based on cost assignment\n        self.set_centers_mean_cov(\n            self.global_posterior_,\n            posterior_centers_mean_cov[col_ind])\n        self.set_widths_mean_var(\n            self.global_posterior_,\n            posterior_widths_mean_var[col_ind])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the global posterior and then check convergence .", "response": "def _update_global_posterior(\n            self, rank, m, outer_converged):\n        \"\"\"Update global posterior and then check convergence\n\n        Parameters\n        ----------\n\n        rank : integer\n            The rank of current process.\n\n        m : integer\n            The outer iteration number of HTFA.\n\n        outer_converged : 1D array\n            Record whether HTFA loop converged\n\n\n        Returns\n        -------\n        1D array, contains only 1 element for MPI\n            1 means HTFA converged, 0 means not converged.\n\n        \"\"\"\n        if rank == 0:\n            self._map_update_posterior()\n            self._assign_posterior()\n            is_converged, _ = self._converged()\n            if is_converged:\n                logger.info(\"converged at %d outer iter\" % (m))\n                outer_converged[0] = 1\n            else:\n                self.global_prior_ = self.global_posterior_\n        return outer_converged"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate local weight of the class by computing the factor factors of each subject in the list of data.", "response": "def _update_weight(self, data, R, n_local_subj, local_weight_offset):\n        \"\"\"update local weight\n\n        Parameters\n        ----------\n\n        data : list of 2D array, element i has shape=[n_voxel, n_tr]\n            Subjects' fMRI data.\n\n        R : list of 2D arrays, element i has shape=[n_voxel, n_dim]\n            Each element in the list contains the scanner coordinate matrix\n            of fMRI data of one subject.\n\n        n_local_subj : integer\n            Number of subjects allocated to this process.\n\n        local_weight_offset : 1D array\n            Offset of each subject's weights on this process.\n\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n\n        \"\"\"\n        for s, subj_data in enumerate(data):\n            base = s * self.prior_size\n            centers = self.local_posterior_[base:base + self.K * self.n_dim]\\\n                .reshape((self.K, self.n_dim))\n            start_idx = base + self.K * self.n_dim\n            end_idx = base + self.prior_size\n            widths = self.local_posterior_[start_idx:end_idx]\\\n                .reshape((self.K, 1))\n            unique_R, inds = self.get_unique_R(R[s])\n            F = self.get_factors(unique_R, inds, centers, widths)\n            start_idx = local_weight_offset[s]\n            if s == n_local_subj - 1:\n                self.local_weights_[start_idx:] =\\\n                    self.get_weights(subj_data, F).ravel()\n            else:\n                end_idx = local_weight_offset[s + 1]\n                self.local_weights_[start_idx:end_idx] =\\\n                    self.get_weights(subj_data, F).ravel()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fit_htfa(self, data, R):\n\n        rank, size = self._get_mpi_info()\n        use_gather = True if self.n_subj % size == 0 else False\n        n_local_subj = len(R)\n        max_sample_tr, max_sample_voxel =\\\n            self._get_subject_info(n_local_subj, data)\n\n        tfa = []\n        # init tfa for each subject\n        for s, subj_data in enumerate(data):\n            tfa.append(TFA(\n                max_iter=self.max_local_iter,\n                threshold=self.threshold,\n                K=self.K,\n                nlss_method=self.nlss_method,\n                nlss_loss=self.nlss_loss,\n                x_scale=self.x_scale,\n                tr_solver=self.tr_solver,\n                weight_method=self.weight_method,\n                upper_ratio=self.upper_ratio,\n                lower_ratio=self.lower_ratio,\n                verbose=self.verbose,\n                max_num_tr=max_sample_tr[s],\n                max_num_voxel=max_sample_voxel[s]))\n\n        # map data to processes\n        gather_size, gather_offset, subject_map =\\\n            self._get_gather_offset(size)\n        self.local_posterior_ = np.zeros(n_local_subj * self.prior_size)\n        self._init_prior_posterior(rank, R, n_local_subj)\n        node_weight_size, local_weight_offset =\\\n            self._get_weight_size(data, n_local_subj)\n        self.local_weights_ = np.zeros(node_weight_size[0])\n\n        m = 0\n        outer_converged = np.array([0])\n        while m < self.max_global_iter and not outer_converged[0]:\n            if(self.verbose):\n                logger.info(\"HTFA global iter %d \" % (m))\n            # root broadcast first 4 fields of global_prior to all nodes\n            self.comm.Bcast(self.global_prior_, root=0)\n            # each node loop over its data\n            for s, subj_data in enumerate(data):\n                # update tfa with current local prior\n                tfa[s].set_prior(self.global_prior_[0:self.prior_size].copy())\n                tfa[s].set_seed(m * self.max_local_iter)\n                tfa[s].fit(\n                    subj_data,\n                    R=R[s],\n                    template_prior=self.global_prior_.copy())\n                tfa[s]._assign_posterior()\n                start_idx = s * self.prior_size\n                end_idx = (s + 1) * self.prior_size\n                self.local_posterior_[start_idx:end_idx] =\\\n                    tfa[s].local_posterior_\n\n            self._gather_local_posterior(\n                use_gather,\n                gather_size,\n                gather_offset)\n\n            # root updates global_posterior\n            outer_converged =\\\n                self._update_global_posterior(rank, m, outer_converged)\n            self.comm.Bcast(outer_converged, root=0)\n            m += 1\n\n        # update weight matrix for each subject\n        self._update_weight(\n            data,\n            R,\n            n_local_subj,\n            local_weight_offset)\n\n        return self", "response": "fit the HTFA on the given data and R"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks whether input data and coordinates in right type in right type.", "response": "def _check_input(self, X, R):\n        \"\"\"Check whether input data and coordinates in right type\n\n        Parameters\n        ----------\n        X :  list of 2D arrays, element i has shape=[voxels_i, samples]\n            Each element in the list contains the fMRI data of one subject.\n\n        R : list of 2D arrays, element i has shape=[n_voxel, n_dim]\n            Each element in the list contains the scanner coordinate matrix\n            of fMRI data of one subject.\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n        \"\"\"\n        # Check data type\n        if not isinstance(X, list):\n            raise TypeError(\"Input data should be a list\")\n\n        if not isinstance(R, list):\n            raise TypeError(\"Coordinates should be a list\")\n\n        # Check the number of subjects\n        if len(X) < 1:\n            raise ValueError(\"Need at leat one subject to train the model.\\\n                              Got {0:d}\".format(len(X)))\n\n        for idx, x in enumerate(X):\n            if not isinstance(x, np.ndarray):\n                raise TypeError(\"Each subject data should be an array\")\n            if x.ndim != 2:\n                raise TypeError(\"Each subject data should be 2D array\")\n            if not isinstance(R[idx], np.ndarray):\n                raise TypeError(\n                    \"Each scanner coordinate matrix should be an array\")\n            if R[idx].ndim != 2:\n                raise TypeError(\n                    \"Each scanner coordinate matrix should be 2D array\")\n            if x.shape[0] != R[idx].shape[0]:\n                raise TypeError(\n                    \"n_voxel should be the same in X[idx] and R[idx]\")\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes Hierarchical Topographical Factor Analysis Model", "response": "def fit(self, X, R):\n        \"\"\"Compute Hierarchical Topographical Factor Analysis Model\n           [Manning2014-1][Manning2014-2]\n\n        Parameters\n        ----------\n        X :  list of 2D arrays, element i has shape=[voxels_i, samples]\n            Each element in the list contains the fMRI data of one subject.\n\n        R : list of 2D arrays, element i has shape=[n_voxel, n_dim]\n            Each element in the list contains the scanner coordinate matrix\n            of fMRI data of one subject.\n\n        Returns\n        -------\n        HTFA\n            Returns the instance itself.\n        \"\"\"\n        self._check_input(X, R)\n        if self.verbose:\n            logger.info(\"Start to fit HTFA\")\n        self.n_dim = R[0].shape[1]\n        self.cov_vec_size = np.sum(np.arange(self.n_dim) + 1)\n        # centers,widths\n        self.prior_size = self.K * (self.n_dim + 1)\n        # centers,widths,centerCov,widthVar\n        self.prior_bcast_size =\\\n            self.K * (self.n_dim + 2 + self.cov_vec_size)\n        self.get_map_offset()\n        self._fit_htfa(X, R)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the standard deviations around a 1D GMM.", "response": "def get_sigma(x, min_limit=-np.inf, max_limit=np.inf):\n    \"\"\"Compute the standard deviations around the points for a 1D GMM.\n\n    We take the distance from the nearest left and right neighbors\n    for each point, then use the max as the estimate of standard\n    deviation for the gaussian mixture around that point.\n\n    Arguments\n    ---------\n    x : 1D array\n        Set of points to create the GMM\n\n    min_limit : Optional[float], default : -inf\n        Minimum limit for the distribution\n\n    max_limit : Optional[float], default : inf\n        maximum limit for the distribution\n\n    Returns\n    -------\n    1D array\n        Array of standard deviations\n    \"\"\"\n\n    z = np.append(x, [min_limit, max_limit])\n    sigma = np.ones(x.shape)\n    for i in range(x.size):\n        # Calculate the nearest left neighbor of x[i]\n        # Find the minimum of (x[i] - k) for k < x[i]\n        xleft = z[np.argmin([(x[i] - k) if k < x[i] else np.inf for k in z])]\n\n        # Calculate the nearest right neighbor of x[i]\n        # Find the minimum of (k - x[i]) for k > x[i]\n        xright = z[np.argmin([(k - x[i]) if k > x[i] else np.inf for k in z])]\n\n        sigma[i] = max(x[i] - xleft, xright - x[i])\n        if sigma[i] == np.inf:\n            sigma[i] = min(x[i] - xleft, xright - x[i])\n        if (sigma[i] == -np.inf):  # should never happen\n            sigma[i] = 1.0\n    return sigma"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_next_sample(x, y, min_limit=-np.inf, max_limit=np.inf):\n\n    z = np.array(list(zip(x, y)), dtype=np.dtype([('x', float), ('y', float)]))\n    z = np.sort(z, order='y')\n    n = y.shape[0]\n    g = int(np.round(np.ceil(0.15 * n)))\n    ldata = z[0:g]\n    gdata = z[g:n]\n    lymin = ldata['y'].min()\n    lymax = ldata['y'].max()\n    weights = (lymax - ldata['y']) / (lymax - lymin)\n    lx = gmm_1d_distribution(ldata['x'], min_limit=min_limit,\n                             max_limit=max_limit, weights=weights)\n    gx = gmm_1d_distribution(gdata['x'], min_limit=min_limit,\n                             max_limit=max_limit)\n\n    samples = lx.get_samples(n=1000)\n    ei = lx(samples) / gx(samples)\n\n    h = (x.max() - x.min()) / (10 * x.size)\n    # TODO\n    # assumes prior of x is uniform; should ideally change for other priors\n    # d = np.abs(x - samples[ei.argmax()]).min()\n    # CDF(x+d/2) - CDF(x-d/2) < 1/(10*x.size) then reject else accept\n    s = 0\n    while (np.abs(x - samples[ei.argmax()]).min() < h):\n        ei[ei.argmax()] = 0\n        s = s + 1\n        if (s == samples.size):\n            break\n    xnext = samples[ei.argmax()]\n\n    return xnext", "response": "Get the next sample from the previous samples and return the HPO base."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fmin(loss_fn,\n         space,\n         max_evals,\n         trials,\n         init_random_evals=30,\n         explore_prob=0.2):\n    \"\"\"Find the minimum of function through hyper parameter optimization.\n\n    Arguments\n    ---------\n    loss_fn : ``function(*args) -> float``\n        Function that takes in a dictionary and returns a real value.\n        This is the function to be minimized.\n\n    space : dictionary\n        Custom dictionary specifying the range and distribution of\n        the hyperparamters.\n        E.g. ``space = {'x': {'dist':scipy.stats.uniform(0,1),\n        'lo':0, 'hi':1}}``\n        for a 1-dimensional space with variable x in range [0,1]\n\n    max_evals : int\n        Maximum number of evaluations of loss_fn allowed\n\n    trials : list\n        Holds the output of the optimization trials.\n        Need not be empty to begin with, new trials are appended\n        at the end.\n\n    init_random_evals : Optional[int], default 30\n        Number of random trials to initialize the\n        optimization.\n\n    explore_prob : Optional[float], default 0.2\n        Controls the exploration-vs-exploitation ratio. Value should\n        be in [0,1]. By default, 20% of trails are random samples.\n\n    Returns\n    -------\n    trial entry (dictionary of hyperparameters)\n           Best hyperparameter setting found.\n           E.g. {'x': 5.6, 'loss' : 0.5} where x is the best hyparameter\n           value found and loss is the value of the function for the\n           best hyperparameter value(s).\n\n    Raises\n    ------\n    ValueError\n        If the distribution specified in space does not support a ``rvs()``\n        method to generate random numbers, a ValueError is raised.\n    \"\"\"\n\n    for s in space:\n        if not hasattr(space[s]['dist'], 'rvs'):\n            raise ValueError('Unknown distribution type for variable')\n        if 'lo' not in space[s]:\n            space[s]['lo'] = -np.inf\n        if 'hi' not in space[s]:\n            space[s]['hi'] = np.inf\n\n    if len(trials) > init_random_evals:\n        init_random_evals = 0\n\n    for t in range(max_evals):\n        sdict = {}\n\n        if t >= init_random_evals and np.random.random() > explore_prob:\n            use_random_sampling = False\n        else:\n            use_random_sampling = True\n\n        yarray = np.array([tr['loss'] for tr in trials])\n        for s in space:\n            sarray = np.array([tr[s] for tr in trials])\n            if use_random_sampling:\n                sdict[s] = space[s]['dist'].rvs()\n            else:\n                sdict[s] = get_next_sample(sarray, yarray,\n                                           min_limit=space[s]['lo'],\n                                           max_limit=space[s]['hi'])\n\n        logger.debug('Explore' if use_random_sampling else 'Exploit')\n        logger.info('Next point ', t, ' = ', sdict)\n\n        y = loss_fn(sdict)\n        sdict['loss'] = y\n        trials.append(sdict)\n\n    yarray = np.array([tr['loss'] for tr in trials])\n    yargmin = yarray.argmin()\n\n    logger.info('Best point so far = ', trials[yargmin])\n    return trials[yargmin]", "response": "This function calculates the minimum of function in a 1 - dimensional space and returns a real value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the GMM likelihood for a single point.", "response": "def get_gmm_pdf(self, x):\n        \"\"\"Calculate the GMM likelihood for a single point.\n\n        .. math::\n            y = \\\\sum_{i=1}^{N} w_i\n            \\\\times \\\\text{normpdf}(x, x_i, \\\\sigma_i)/\\\\sum_{i=1}^{N} w_i\n            :label: gmm-likelihood\n\n        Arguments\n        ---------\n        x : float\n            Point at which likelihood needs to be computed\n\n        Returns\n        -------\n        float\n            Likelihood value at x\n        \"\"\"\n\n        def my_norm_pdf(xt, mu, sigma):\n            z = (xt - mu) / sigma\n            return (math.exp(-0.5 * z * z)\n                    / (math.sqrt(2. * np.pi) * sigma))\n\n        y = 0\n        if (x < self.min_limit):\n            return 0\n        if (x > self.max_limit):\n            return 0\n        for _x in range(self.points.size):\n            y += (my_norm_pdf(x, self.points[_x], self.sigma[_x])\n                  * self.weights[_x]) / self.W_sum\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a 1D array of n samples from the GMM distribution.", "response": "def get_samples(self, n):\n        \"\"\"Sample the GMM distribution.\n\n        Arguments\n        ---------\n        n : int\n            Number of samples needed\n\n        Returns\n        -------\n        1D array\n            Samples from the distribution\n        \"\"\"\n\n        normalized_w = self.weights / np.sum(self.weights)\n        get_rand_index = st.rv_discrete(values=(range(self.N),\n                                        normalized_w)).rvs(size=n)\n        samples = np.zeros(n)\n        k = 0\n        j = 0\n        while (k < n):\n            i = get_rand_index[j]\n            j = j + 1\n            if (j == n):\n                get_rand_index = st.rv_discrete(values=(range(self.N),\n                                                normalized_w)).rvs(size=n)\n                j = 0\n            v = np.random.normal(loc=self.points[i], scale=self.sigma[i])\n            if (v > self.max_limit or v < self.min_limit):\n                continue\n            else:\n                samples[k] = v\n                k = k + 1\n                if (k == n):\n                    break\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates data epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch by epoch.", "response": "def _separate_epochs(activity_data, epoch_list):\n    \"\"\" create data epoch by epoch\n\n    Separate data into epochs of interest specified in epoch_list\n    and z-score them for computing correlation\n\n    Parameters\n    ----------\n    activity_data: list of 2D array in shape [nVoxels, nTRs]\n        the masked activity data organized in voxel*TR formats of all subjects\n    epoch_list: list of 3D array in shape [condition, nEpochs, nTRs]\n        specification of epochs and conditions\n        assuming all subjects have the same number of epochs\n        len(epoch_list) equals the number of subjects\n\n    Returns\n    -------\n    raw_data: list of 2D array in shape [epoch length, nVoxels]\n        the data organized in epochs\n        and z-scored in preparation of correlation computation\n        len(raw_data) equals the number of epochs\n    labels: list of 1D array\n        the condition labels of the epochs\n        len(labels) labels equals the number of epochs\n    \"\"\"\n    time1 = time.time()\n    raw_data = []\n    labels = []\n    for sid in range(len(epoch_list)):\n        epoch = epoch_list[sid]\n        for cond in range(epoch.shape[0]):\n            sub_epoch = epoch[cond, :, :]\n            for eid in range(epoch.shape[1]):\n                r = np.sum(sub_epoch[eid, :])\n                if r > 0:   # there is an epoch in this condition\n                    # mat is row-major\n                    # regardless of the order of acitvity_data[sid]\n                    mat = activity_data[sid][:, sub_epoch[eid, :] == 1]\n                    mat = np.ascontiguousarray(mat.T)\n                    mat = zscore(mat, axis=0, ddof=0)\n                    # if zscore fails (standard deviation is zero),\n                    # set all values to be zero\n                    mat = np.nan_to_num(mat)\n                    mat = mat / math.sqrt(r)\n                    raw_data.append(mat)\n                    labels.append(cond)\n    time2 = time.time()\n    logger.debug(\n        'epoch separation done, takes %.2f s' %\n        (time2 - time1)\n    )\n    return raw_data, labels"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _randomize_subject_list(data_list, random):\n    if random == RandomType.REPRODUCIBLE:\n        for i in range(len(data_list)):\n            _randomize_single_subject(data_list[i], seed=i)\n    elif random == RandomType.UNREPRODUCIBLE:\n        for data in data_list:\n            _randomize_single_subject(data)", "response": "Randomly permute the voxels of a subject list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprepare data for correlation - based computation and analysis.", "response": "def prepare_fcma_data(images, conditions, mask1, mask2=None,\n                      random=RandomType.NORANDOM, comm=MPI.COMM_WORLD):\n    \"\"\"Prepare data for correlation-based computation and analysis.\n\n    Generate epochs of interests, then broadcast to all workers.\n\n    Parameters\n    ----------\n    images: Iterable[SpatialImage]\n        Data.\n    conditions: List[UniqueLabelConditionSpec]\n        Condition specification.\n    mask1: np.ndarray\n        Mask to apply to each image.\n    mask2: Optional[np.ndarray]\n        Mask to apply to each image.\n        If it is not specified, the method will assign None to the returning\n        variable raw_data2 and the self-correlation on raw_data1 will be\n        computed\n    random: Optional[RandomType]\n        Randomize the image data within subject or not.\n    comm: MPI.Comm\n        MPI communicator to use for MPI operations.\n\n    Returns\n    -------\n    raw_data1: list of 2D array in shape [epoch length, nVoxels]\n        the data organized in epochs, specified by the first mask.\n        len(raw_data) equals the number of epochs\n    raw_data2: Optional, list of 2D array in shape [epoch length, nVoxels]\n        the data organized in epochs, specified by the second mask if any.\n        len(raw_data2) equals the number of epochs\n    labels: list of 1D array\n        the condition labels of the epochs\n        len(labels) labels equals the number of epochs\n    \"\"\"\n    rank = comm.Get_rank()\n    labels = []\n    raw_data1 = []\n    raw_data2 = []\n    if rank == 0:\n        logger.info('start to apply masks and separate epochs')\n        if mask2 is not None:\n            masks = (mask1, mask2)\n            activity_data1, activity_data2 = zip(*multimask_images(images,\n                                                                   masks,\n                                                                   np.float32))\n            _randomize_subject_list(activity_data2, random)\n            raw_data2, _ = _separate_epochs(activity_data2, conditions)\n        else:\n            activity_data1 = list(mask_images(images, mask1, np.float32))\n        _randomize_subject_list(activity_data1, random)\n        raw_data1, labels = _separate_epochs(activity_data1, conditions)\n        time1 = time.time()\n    raw_data_length = len(raw_data1)\n    raw_data_length = comm.bcast(raw_data_length)\n    # broadcast the data subject by subject to prevent size overflow\n    for i in range(raw_data_length):\n        if rank != 0:\n            raw_data1.append(None)\n            if mask2 is not None:\n                raw_data2.append(None)\n        raw_data1[i] = comm.bcast(raw_data1[i], root=0)\n        if mask2 is not None:\n            raw_data2[i] = comm.bcast(raw_data2[i], root=0)\n\n    if comm.Get_size() > 1:\n        labels = comm.bcast(labels, root=0)\n        if rank == 0:\n            time2 = time.time()\n            logger.info(\n                'data broadcasting done, takes %.2f s' %\n                (time2 - time1)\n            )\n    if mask2 is None:\n        raw_data2 = None\n    return raw_data1, raw_data2, labels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing epoch_list to generate epoch_info defined below", "response": "def generate_epochs_info(epoch_list):\n    \"\"\" use epoch_list to generate epoch_info defined below\n\n    Parameters\n    ----------\n    epoch_list: list of 3D (binary) array in shape [condition, nEpochs, nTRs]\n        Contains specification of epochs and conditions, assuming\n        1. all subjects have the same number of epochs;\n        2. len(epoch_list) equals the number of subjects;\n        3. an epoch is always a continuous time course.\n\n    Returns\n    -------\n    epoch_info: list of tuple (label, sid, start, end).\n        label is the condition labels of the epochs;\n        sid is the subject id, corresponding to the index of raw_data;\n        start is the start TR of an epoch (inclusive);\n        end is the end TR of an epoch(exclusive).\n        Assuming len(labels) labels equals the number of epochs and\n        the epochs of the same sid are adjacent in epoch_info\n    \"\"\"\n    time1 = time.time()\n    epoch_info = []\n    for sid, epoch in enumerate(epoch_list):\n        for cond in range(epoch.shape[0]):\n            sub_epoch = epoch[cond, :, :]\n            for eid in range(epoch.shape[1]):\n                r = np.sum(sub_epoch[eid, :])\n                if r > 0:   # there is an epoch in this condition\n                    start = np.nonzero(sub_epoch[eid, :])[0][0]\n                    epoch_info.append((cond, sid, start, start + r))\n    time2 = time.time()\n    logger.debug(\n        'epoch separation done, takes %.2f s' %\n        (time2 - time1)\n    )\n    return epoch_info"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_mvpa_data(images, conditions, mask):\n    activity_data = list(mask_images(images, mask, np.float32))\n    epoch_info = generate_epochs_info(conditions)\n    num_epochs = len(epoch_info)\n    (d1, _) = activity_data[0].shape\n    processed_data = np.empty([d1, num_epochs])\n    labels = np.empty(num_epochs)\n    subject_count = [0]  # counting the epochs per subject for z-scoring\n    cur_sid = -1\n    # averaging\n    for idx, epoch in enumerate(epoch_info):\n        labels[idx] = epoch[0]\n        if cur_sid != epoch[1]:\n            subject_count.append(0)\n            cur_sid = epoch[1]\n        subject_count[-1] += 1\n        processed_data[:, idx] = \\\n            np.mean(activity_data[cur_sid][:, epoch[2]:epoch[3]],\n                    axis=1)\n    # z-scoring\n    cur_epoch = 0\n    for i in subject_count:\n        if i > 1:\n            processed_data[:, cur_epoch:cur_epoch + i] = \\\n                zscore(processed_data[:, cur_epoch:cur_epoch + i],\n                       axis=1, ddof=0)\n        cur_epoch += i\n    # if zscore fails (standard deviation is zero),\n    # set all values to be zero\n    processed_data = np.nan_to_num(processed_data)\n\n    return processed_data, labels", "response": "Prepare data for activity - based model training and prediction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_searchlight_mvpa_data(images, conditions, data_type=np.float32,\n                                  random=RandomType.NORANDOM):\n    \"\"\" obtain the data for activity-based voxel selection using Searchlight\n\n    Average the activity within epochs and z-scoring within subject,\n    while maintaining the 3D brain structure. In order to save memory,\n    the data is processed subject by subject instead of reading all in before\n    processing. Assuming all subjects live in the identical cube.\n\n    Parameters\n    ----------\n    images: Iterable[SpatialImage]\n        Data.\n    conditions: List[UniqueLabelConditionSpec]\n        Condition specification.\n    data_type\n        Type to cast image to.\n    random: Optional[RandomType]\n        Randomize the image data within subject or not.\n\n    Returns\n    -------\n    processed_data: 4D array in shape [brain 3D + epoch]\n        averaged epoch by epoch processed data\n\n    labels: 1D array\n        contains labels of the data\n    \"\"\"\n    time1 = time.time()\n    epoch_info = generate_epochs_info(conditions)\n    num_epochs = len(epoch_info)\n    processed_data = None\n    logger.info(\n        'there are %d subjects, and in total %d epochs' %\n        (len(conditions), num_epochs)\n    )\n    labels = np.empty(num_epochs)\n    # assign labels\n    for idx, epoch in enumerate(epoch_info):\n        labels[idx] = epoch[0]\n    # counting the epochs per subject for z-scoring\n    subject_count = np.zeros(len(conditions), dtype=np.int32)\n\n    logger.info('start to apply masks and separate epochs')\n    for sid, f in enumerate(images):\n        data = f.get_data().astype(data_type)\n        [d1, d2, d3, d4] = data.shape\n        if random == RandomType.REPRODUCIBLE:\n            data = data.reshape((d1 * d2 * d3, d4))\n            _randomize_single_subject(data, seed=sid)\n            data = data.reshape((d1, d2, d3, d4))\n        elif random == RandomType.UNREPRODUCIBLE:\n            data = data.reshape((d1 * d2 * d3, d4))\n            _randomize_single_subject(data)\n            data = data.reshape((d1, d2, d3, d4))\n        if processed_data is None:\n            processed_data = np.empty([d1, d2, d3, num_epochs],\n                                      dtype=data_type)\n        # averaging\n        for idx, epoch in enumerate(epoch_info):\n            if sid == epoch[1]:\n                subject_count[sid] += 1\n                processed_data[:, :, :, idx] = \\\n                    np.mean(data[:, :, :, epoch[2]:epoch[3]], axis=3)\n\n        logger.debug(\n            'file %s is loaded and processed, with data shape %s',\n            f.get_filename(), data.shape\n        )\n    # z-scoring\n    cur_epoch = 0\n    for i in subject_count:\n        if i > 1:\n            processed_data[:, :, :, cur_epoch:cur_epoch + i] = \\\n                zscore(processed_data[:, :, :, cur_epoch:cur_epoch + i],\n                       axis=3, ddof=0)\n        cur_epoch += i\n    # if zscore fails (standard deviation is zero),\n    # set all values to be zero\n    processed_data = np.nan_to_num(processed_data)\n    time2 = time.time()\n    logger.info(\n        'data processed for activity-based voxel selection, takes %.2f s' %\n        (time2 - time1)\n    )\n\n    return processed_data, labels", "response": "Prepare the data for searchlight - based voxel selection using Searchlight s algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a 1D array containing elements of the upper triangular matrix to 2D symmetric matrix", "response": "def from_tri_2_sym(tri, dim):\n    \"\"\"convert a upper triangular matrix in 1D format\n       to 2D symmetric matrix\n\n\n    Parameters\n    ----------\n\n    tri: 1D array\n        Contains elements of upper triangular matrix\n\n    dim : int\n        The dimension of target matrix.\n\n\n    Returns\n    -------\n\n    symm : 2D array\n        Symmetric matrix in shape=[dim, dim]\n    \"\"\"\n    symm = np.zeros((dim, dim))\n    symm[np.triu_indices(dim)] = tri\n    return symm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_sym_2_tri(symm):\n\n    inds = np.triu_indices_from(symm)\n    tri = symm[inds]\n    return tri", "response": "converts a 2D symmetric matrix to an upper triangular matrix in 1D format"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sumexp_stable(data):\n    max_value = data.max(axis=0)\n    result_exp = np.exp(data - max_value)\n    result_sum = np.sum(result_exp, axis=0)\n    return result_sum, max_value, result_exp", "response": "Compute the sum of exponents for a list of samples\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing a numpy array by stacking not - None arrays in a list of arrays.", "response": "def concatenate_not_none(l, axis=0):\n    \"\"\"Construct a numpy array by stacking not-None arrays in a list\n\n    Parameters\n    ----------\n\n    data : list of arrays\n        The list of arrays to be concatenated. Arrays have same shape in all\n        but one dimension or are None, in which case they are ignored.\n\n    axis : int, default = 0\n        Axis for the concatenation\n\n\n    Returns\n    -------\n\n    data_stacked : array\n        The resulting concatenated array.\n    \"\"\"\n    # Get the indexes of the arrays in the list\n    mask = []\n    for i in range(len(l)):\n        if l[i] is not None:\n            mask.append(i)\n\n    # Concatenate them\n    l_stacked = np.concatenate([l[i] for i in mask], axis=axis)\n    return l_stacked"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cov2corr(cov):\n    assert cov.ndim == 2, 'covariance matrix should be 2D array'\n    inv_sd = 1 / np.sqrt(np.diag(cov))\n    corr = cov * inv_sd[None, :] * inv_sd[:, None]\n    return corr", "response": "Calculate the correlation matrix based on a covariance matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a design matrix for a set of stimulus time files.", "response": "def gen_design(stimtime_files, scan_duration, TR, style='FSL',\n               temp_res=0.01,\n               hrf_para={'response_delay': 6, 'undershoot_delay': 12,\n                         'response_dispersion': 0.9,\n                         'undershoot_dispersion': 0.9,\n                         'undershoot_scale': 0.035}):\n    \"\"\" Generate design matrix based on a list of names of stimulus\n        timing files. The function will read each file, and generate\n        a numpy array of size [time_points \\\\* condition], where\n        time_points equals duration / TR, and condition is the size of\n        stimtime_filenames. Each column is the hypothetical fMRI response\n        based on the stimulus timing in the corresponding file\n        of stimtime_files.\n        This function uses generate_stimfunction and double_gamma_hrf\n        of brainiak.utils.fmrisim.\n\n    Parameters\n    ----------\n\n    stimtime_files: a string or a list of string.\n        Each string is the name of the file storing\n        the stimulus timing information of one task condition.\n        The contents in the files will be interpretated\n        based on the style parameter.\n        Details are explained under the style parameter.\n\n    scan_duration: float or a list (or a 1D numpy array) of numbers.\n        Total duration of each fMRI scan, in unit of seconds.\n        If there are multiple runs, the duration should be\n        a list (or 1-d numpy array) of numbers.\n        If it is a list, then each number in the list\n        represents the duration of the corresponding scan\n        in the stimtime_files.\n        If only a number is provided, it is assumed that\n        there is only one fMRI scan lasting for scan_duration.\n\n    TR: float.\n        The sampling period of fMRI, in unit of seconds.\n\n    style: string, default: 'FSL'\n        Acceptable inputs: 'FSL', 'AFNI'\n        The formating style of the stimtime_files.\n        'FSL' style has one line for each event of the same condition.\n        Each line contains three numbers. The first number is the onset\n        of the event relative to the onset of the first scan,\n        in units of seconds.\n        (Multiple scans should be treated as a concatenated long scan\n        for the purpose of calculating onsets.\n        However, the design matrix from one scan won't leak into the next).\n        The second number is the duration of the event,\n        in unit of seconds.\n        The third number is the amplitude modulation (or weight)\n        of the response.\n        It is acceptable to not provide the weight,\n        or not provide both duration and weight.\n        In such cases, these parameters will default to 1.0.\n        This code will accept timing files with only 1 or 2 columns for\n        convenience but please note that the FSL package does not allow this\n\n        'AFNI' style has one line for each scan (run).\n        Each line has a few triplets in the format of\n        stim_onsets*weight:duration\n        (or simpler, see below), separated by spaces.\n        For example, 3.2\\\\*2.0:1.5 means that one event starts at 3.2s,\n        modulated by weight of 2.0 and lasts for 1.5s.\n        If some run does not include a single event\n        of a condition (stimulus type), then you can put \\\\*,\n        or a negative number, or a very large number in that line.\n        Either duration or weight can be neglected. In such\n        cases, they will default to 1.0.\n        For example, 3.0, 3.0\\\\*1.0, 3.0:1.0 and 3.0\\\\*1.0:1.0 all\n        means an event starting at 3.0s, lasting for 1.0s, with\n        amplitude modulation of 1.0.\n\n    temp_res: float, default: 0.01\n        Temporal resolution of fMRI, in second.\n\n    hrf_para: dictionary\n        The parameters of the double-Gamma hemodynamic response function.\n        To set different parameters, supply a dictionary with\n        the same set of keys as the default, and replace the corresponding\n        values with the new values.\n\n    Returns\n    -------\n\n    design: 2D numpy array\n        design matrix. Each time row represents one TR\n        (fMRI sampling time point) and each column represents\n        one experiment condition, in the order in stimtime_files\n\n    \"\"\"\n    if np.ndim(scan_duration) == 0:\n        scan_duration = [scan_duration]\n    scan_duration = np.array(scan_duration)\n    assert np.all(scan_duration > TR), \\\n        'scan duration should be longer than a TR'\n    if type(stimtime_files) is str:\n        stimtime_files = [stimtime_files]\n    assert TR > 0, 'TR should be positive'\n    assert style == 'FSL' or style == 'AFNI', 'style can only be FSL or AFNI'\n    n_C = len(stimtime_files)  # number of conditions\n    n_S = np.size(scan_duration)  # number of scans\n    if n_S > 1:\n        design = [np.empty([int(np.round(duration / TR)), n_C])\n                  for duration in scan_duration]\n    else:\n        design = [np.empty([int(np.round(scan_duration / TR)), n_C])]\n    scan_onoff = np.insert(np.cumsum(scan_duration), 0, 0)\n    if style == 'FSL':\n        design_info = _read_stimtime_FSL(stimtime_files, n_C, n_S, scan_onoff)\n    elif style == 'AFNI':\n        design_info = _read_stimtime_AFNI(stimtime_files, n_C, n_S, scan_onoff)\n\n    response_delay = hrf_para['response_delay']\n    undershoot_delay = hrf_para['undershoot_delay']\n    response_disp = hrf_para['response_dispersion']\n    undershoot_disp = hrf_para['undershoot_dispersion']\n    undershoot_scale = hrf_para['undershoot_scale']\n    # generate design matrix\n    for i_s in range(n_S):\n        for i_c in range(n_C):\n            if len(design_info[i_s][i_c]['onset']) > 0:\n                stimfunction = generate_stimfunction(\n                    onsets=design_info[i_s][i_c]['onset'],\n                    event_durations=design_info[i_s][i_c]['duration'],\n                    total_time=scan_duration[i_s],\n                    weights=design_info[i_s][i_c]['weight'],\n                    temporal_resolution=1.0/temp_res)\n                hrf = _double_gamma_hrf(response_delay=response_delay,\n                                        undershoot_delay=undershoot_delay,\n                                        response_dispersion=response_disp,\n                                        undershoot_dispersion=undershoot_disp,\n                                        undershoot_scale=undershoot_scale,\n                                        temporal_resolution=1.0/temp_res)\n                design[i_s][:, i_c] = convolve_hrf(\n                    stimfunction, TR, hrf_type=hrf, scale_function=False,\n                    temporal_resolution=1.0 / temp_res).transpose() * temp_res\n            else:\n                design[i_s][:, i_c] = 0.0\n            # We multiply the resulting design matrix with\n            # the temporal resolution to normalize it.\n            # We do not use the internal normalization\n            # in double_gamma_hrf because it does not guarantee\n            # normalizing with the same constant.\n    return np.concatenate(design, axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_stimtime_FSL(stimtime_files, n_C, n_S, scan_onoff):\n    design_info = [[{'onset': [], 'duration': [], 'weight': []}\n                    for i_c in range(n_C)] for i_s in range(n_S)]\n    # Read stimulus timing files\n    for i_c in range(n_C):\n        with open(stimtime_files[i_c]) as f:\n            for line in f.readlines():\n                tmp = line.strip().split()\n                i_s = np.where(\n                    np.logical_and(scan_onoff[:-1] <= float(tmp[0]),\n                                   scan_onoff[1:] > float(tmp[0])))[0]\n                if len(i_s) == 1:\n                    i_s = i_s[0]\n                    design_info[i_s][i_c]['onset'].append(float(tmp[0])\n                                                          - scan_onoff[i_s])\n                    if len(tmp) >= 2:\n                        design_info[i_s][i_c]['duration'].append(float(tmp[1]))\n                    else:\n                        design_info[i_s][i_c]['duration'].append(1.0)\n                    if len(tmp) >= 3:\n                        design_info[i_s][i_c]['weight'].append(float(tmp[2]))\n                    else:\n                        design_info[i_s][i_c]['weight'].append(1.0)\n    return design_info", "response": "Utility called by gen_design. This function reads in one or more stimulus time files and returns a list of dictionaries containing the onset duration and weight of each event in the next scan."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_stimtime_AFNI(stimtime_files, n_C, n_S, scan_onoff):\n    design_info = [[{'onset': [], 'duration': [], 'weight': []}\n                    for i_c in range(n_C)] for i_s in range(n_S)]\n    # Read stimulus timing files\n    for i_c in range(n_C):\n        with open(stimtime_files[i_c]) as f:\n            text = f.readlines()\n            assert len(text) == n_S, \\\n                'Number of lines does not match number of runs!'\n            for i_s, line in enumerate(text):\n                events = line.strip().split()\n                if events[0] == '*':\n                    continue\n                for event in events:\n                    assert event != '*'\n                    tmp = str.split(event, ':')\n                    if len(tmp) == 2:\n                        duration = float(tmp[1])\n                    else:\n                        duration = 1.0\n                    tmp = str.split(tmp[0], '*')\n                    if len(tmp) == 2:\n                        weight = float(tmp[1])\n                    else:\n                        weight = 1.0\n                    if (float(tmp[0]) >= 0\n                            and float(tmp[0])\n                            < scan_onoff[i_s + 1] - scan_onoff[i_s]):\n                        design_info[i_s][i_c]['onset'].append(float(tmp[0]))\n                        design_info[i_s][i_c]['duration'].append(duration)\n                        design_info[i_s][i_c]['weight'].append(weight)\n    return design_info", "response": "Utility called by _read_stimtime_AFNI to read in one or more stimulus timing files and return a list of dicts. Each dictionary contains the key onset duration and weight of each event. Each dictionary contains the value of onset and duration and weight of each event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef center_mass_exp(interval, scale=1.0):\n    assert isinstance(interval, tuple), 'interval must be a tuple'\n    assert len(interval) == 2, 'interval must be length two'\n    (interval_left, interval_right) = interval\n    assert interval_left >= 0, 'interval_left must be non-negative'\n    assert interval_right > interval_left, \\\n        'interval_right must be bigger than interval_left'\n    assert scale > 0, 'scale must be positive'\n\n    if interval_right < np.inf:\n        return ((interval_left + scale) * np.exp(-interval_left / scale) - (\n            scale + interval_right) * np.exp(-interval_right / scale)) / (\n            np.exp(-interval_left / scale) - np.exp(-interval_right / scale))\n    else:\n        return interval_left + scale", "response": "Returns the center of mass of the exponential distribution of the well - defined base class for the base class for the base class for the base class for the base class for the base class for the base class for the base class for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef usable_cpu_count():\n    try:\n        result = len(os.sched_getaffinity(0))\n    except AttributeError:\n        try:\n            result = len(psutil.Process().cpu_affinity())\n        except AttributeError:\n            result = os.cpu_count()\n    return result", "response": "Get number of CPUs usable by the current process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrandomize phase of time series across subjects.", "response": "def phase_randomize(data, voxelwise=False, random_state=None):\n    \"\"\"Randomize phase of time series across subjects\n\n    For each subject, apply Fourier transform to voxel time series\n    and then randomly shift the phase of each frequency before inverting\n    back into the time domain. This yields time series with the same power\n    spectrum (and thus the same autocorrelation) as the original time series\n    but will remove any meaningful temporal relationships among time series\n    across subjects. By default (voxelwise=False), the same phase shift is\n    applied across all voxels; however if voxelwise=True, different random\n    phase shifts are applied to each voxel. The typical input is a time by\n    voxels by subjects ndarray. The first dimension is assumed to be the\n    time dimension and will be phase randomized. If a 2-dimensional ndarray\n    is provided, the last dimension is assumed to be subjects, and different\n    phase randomizations will be applied to each subject.\n\n    The implementation is based on the work in [Lerner2011]_ and\n    [Simony2016]_.\n\n    Parameters\n    ----------\n    data : ndarray (n_TRs x n_voxels x n_subjects)\n        Data to be phase randomized (per subject)\n\n    voxelwise : bool, default: False\n        Apply same (False) or different (True) randomizations across voxels\n\n    random_state : RandomState or an int seed (0 by default)\n        A random number generator instance to define the state of the\n        random permutations generator.\n\n    Returns\n    ----------\n    shifted_data : ndarray (n_TRs x n_voxels x n_subjects)\n        Phase-randomized time series\n    \"\"\"\n\n    # Check if input is 2-dimensional\n    data_ndim = data.ndim\n\n    # Get basic shape of data\n    data, n_TRs, n_voxels, n_subjects = _check_timeseries_input(data)\n\n    # Random seed to be deterministically re-randomized at each iteration\n    if isinstance(random_state, np.random.RandomState):\n        prng = random_state\n    else:\n        prng = np.random.RandomState(random_state)\n\n    # Get randomized phase shifts\n    if n_TRs % 2 == 0:\n        # Why are we indexing from 1 not zero here? n_TRs / -1 long?\n        pos_freq = np.arange(1, data.shape[0] // 2)\n        neg_freq = np.arange(data.shape[0] - 1, data.shape[0] // 2, -1)\n    else:\n        pos_freq = np.arange(1, (data.shape[0] - 1) // 2 + 1)\n        neg_freq = np.arange(data.shape[0] - 1,\n                             (data.shape[0] - 1) // 2, -1)\n\n    if not voxelwise:\n        phase_shifts = (prng.rand(len(pos_freq), 1, n_subjects)\n                        * 2 * np.math.pi)\n    else:\n        phase_shifts = (prng.rand(len(pos_freq), n_voxels, n_subjects)\n                        * 2 * np.math.pi)\n\n    # Fast Fourier transform along time dimension of data\n    fft_data = fft(data, axis=0)\n\n    # Shift pos and neg frequencies symmetrically, to keep signal real\n    fft_data[pos_freq, :, :] *= np.exp(1j * phase_shifts)\n    fft_data[neg_freq, :, :] *= np.exp(-1j * phase_shifts)\n\n    # Inverse FFT to put data back in time domain\n    shifted_data = np.real(ifft(fft_data, axis=0))\n\n    # Go back to 2-dimensions if input was 2-dimensional\n    if data_ndim == 2:\n        shifted_data = shifted_data[:, 0, :]\n\n    return shifted_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the p - value for an observed test statistic given a null distribution.", "response": "def p_from_null(observed, distribution,\n                side='two-sided', exact=False,\n                axis=None):\n    \"\"\"Compute p-value from null distribution\n\n    Returns the p-value for an observed test statistic given a null\n    distribution. Performs either a 'two-sided' (i.e., two-tailed)\n    test (default) or a one-sided (i.e., one-tailed) test for either the\n    'left' or 'right' side. For an exact test (exact=True), does not adjust\n    for the observed test statistic; otherwise, adjusts for observed\n    test statistic (prevents p-values of zero). If a multidimensional\n    distribution is provided, use axis argument to specify which axis indexes\n    resampling iterations.\n\n    The implementation is based on the work in [PhipsonSmyth2010]_.\n\n    .. [PhipsonSmyth2010] \"Permutation p-values should never be zero:\n       calculating exact p-values when permutations are randomly drawn.\",\n       B. Phipson, G. K., Smyth, 2010, Statistical Applications in Genetics\n       and Molecular Biology, 9, 1544-6115.\n       https://doi.org/10.2202/1544-6115.1585\n\n    Parameters\n    ----------\n    observed : float\n        Observed test statistic\n\n    distribution : ndarray\n        Null distribution of test statistic\n\n    side : str, default:'two-sided'\n        Perform one-sided ('left' or 'right') or 'two-sided' test\n\n    axis: None or int, default:None\n        Axis indicating resampling iterations in input distribution\n\n    Returns\n    -------\n    p : float\n        p-value for observed test statistic based on null distribution\n    \"\"\"\n\n    if side not in ('two-sided', 'left', 'right'):\n        raise ValueError(\"The value for 'side' must be either \"\n                         \"'two-sided', 'left', or 'right', got {0}\".\n                         format(side))\n\n    n_samples = len(distribution)\n    logger.info(\"Assuming {0} resampling iterations\".format(n_samples))\n\n    if side == 'two-sided':\n        # Numerator for two-sided test\n        numerator = np.sum(np.abs(distribution) >= np.abs(observed), axis=axis)\n    elif side == 'left':\n        # Numerator for one-sided test in left tail\n        numerator = np.sum(distribution <= observed, axis=axis)\n    elif side == 'right':\n        # Numerator for one-sided test in right tail\n        numerator = np.sum(distribution >= observed, axis=axis)\n\n    # If exact test all possible permutations and do not adjust\n    if exact:\n        p = numerator / n_samples\n\n    # If not exact test, adjust number of samples to account for\n    # observed statistic; prevents p-value from being zero\n    else:\n        p = (numerator + 1) / (n_samples + 1)\n\n    return p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck response time series input data and returns the n_TRs n_voxels n_subjects ndarrays of the n_subjects and n_voxels of the n_subjects array of the n_voxels of the n_subjects array of the n_subjects array of the n_voxels of the n_subjects array of the n_voxels of the n_subjects array of the n_subjects array of the n_ROIs.", "response": "def _check_timeseries_input(data):\n\n    \"\"\"Checks response time series input data (e.g., for ISC analysis)\n\n    Input data should be a n_TRs by n_voxels by n_subjects ndarray\n    (e.g., brainiak.image.MaskedMultiSubjectData) or a list where each\n    item is a n_TRs by n_voxels ndarray for a given subject. Multiple\n    input ndarrays must be the same shape. If a 2D array is supplied,\n    the last dimension is assumed to correspond to subjects. This\n    function is generally intended to be used internally by other\n    functions module (e.g., isc, isfc in brainiak.isc).\n\n    Parameters\n    ----------\n    data : ndarray or list\n        Time series data\n\n    Returns\n    -------\n    data : ndarray\n        Input time series data with standardized structure\n\n    n_TRs : int\n        Number of time points (TRs)\n\n    n_voxels : int\n        Number of voxels (or ROIs)\n\n    n_subjects : int\n        Number of subjects\n\n    \"\"\"\n\n    # Convert list input to 3d and check shapes\n    if type(data) == list:\n        data_shape = data[0].shape\n        for i, d in enumerate(data):\n            if d.shape != data_shape:\n                raise ValueError(\"All ndarrays in input list \"\n                                 \"must be the same shape!\")\n            if d.ndim == 1:\n                data[i] = d[:, np.newaxis]\n        data = np.dstack(data)\n\n    # Convert input ndarray to 3d and check shape\n    elif isinstance(data, np.ndarray):\n        if data.ndim == 2:\n            data = data[:, np.newaxis, :]\n        elif data.ndim == 3:\n            pass\n        else:\n            raise ValueError(\"Input ndarray should have 2 \"\n                             \"or 3 dimensions (got {0})!\".format(data.ndim))\n\n    # Infer subjects, TRs, voxels and log for user to check\n    n_TRs, n_voxels, n_subjects = data.shape\n    logger.info(\"Assuming {0} subjects with {1} time points \"\n                \"and {2} voxel(s) or ROI(s) for ISC analysis.\".format(\n                    n_subjects, n_TRs, n_voxels))\n\n    return data, n_TRs, n_voxels, n_subjects"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the column - or row - wise Pearson correlation between two arrays.", "response": "def array_correlation(x, y, axis=0):\n\n    \"\"\"Column- or row-wise Pearson correlation between two arrays\n\n    Computes sample Pearson correlation between two 1D or 2D arrays (e.g.,\n    two n_TRs by n_voxels arrays). For 2D arrays, computes correlation\n    between each corresponding column (axis=0) or row (axis=1) where axis\n    indexes observations. If axis=0 (default), each column is considered to\n    be a variable and each row is an observation; if axis=1, each row is a\n    variable and each column is an observation (equivalent to transposing\n    the input arrays). Input arrays must be the same shape with corresponding\n    variables and observations. This is intended to be an efficient method\n    for computing correlations between two corresponding arrays with many\n    variables (e.g., many voxels).\n\n    Parameters\n    ----------\n    x : 1D or 2D ndarray\n        Array of observations for one or more variables\n\n    y : 1D or 2D ndarray\n        Array of observations for one or more variables (same shape as x)\n\n    axis : int (0 or 1), default: 0\n        Correlation between columns (axis=0) or rows (axis=1)\n\n    Returns\n    -------\n    r : float or 1D ndarray\n        Pearson correlation values for input variables\n    \"\"\"\n\n    # Accommodate array-like inputs\n    if not isinstance(x, np.ndarray):\n        x = np.asarray(x)\n    if not isinstance(y, np.ndarray):\n        y = np.asarray(y)\n\n    # Check that inputs are same shape\n    if x.shape != y.shape:\n        raise ValueError(\"Input arrays must be the same shape\")\n\n    # Transpose if axis=1 requested (to avoid broadcasting\n    # issues introduced by switching axis in mean and sum)\n    if axis == 1:\n        x, y = x.T, y.T\n\n    # Center (de-mean) input variables\n    x_demean = x - np.mean(x, axis=0)\n    y_demean = y - np.mean(y, axis=0)\n\n    # Compute summed product of centered variables\n    numerator = np.sum(x_demean * y_demean, axis=0)\n\n    # Compute sum squared error\n    denominator = np.sqrt(np.sum(x_demean ** 2, axis=0) *\n                          np.sum(y_demean ** 2, axis=0))\n\n    return numerator / denominator"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute auto-correlation for the input data X1 and X2. it will generate the correlation between some voxels and all voxels Parameters ---------- X1: a list of numpy array in shape [num_TRs, num_voxels1] X1 contains the activity data filtered by ROIs and prepared for correlation computation. All elements of X1 must have the same num_voxels value. X2: a list of numpy array in shape [num_TRs, num_voxels2] len(X1) equals len(X2). All elements of X2 must have the same num_voxels value. X2 can be identical to X1; if not, X1 must have more voxels than X2 (guaranteed by self.fit and/or self.predict). start_voxel: int, default 0 the starting voxel id for correlation computation num_processed_voxels: int, default None the number of voxels it computes for correlation computation if it is None, it is set to self.num_voxels Returns ------- corr_data: the correlation data in shape [len(X), num_processed_voxels, num_voxels2]", "response": "def _prepare_corerelation_data(self, X1, X2,\n                                   start_voxel=0,\n                                   num_processed_voxels=None):\n        \"\"\"Compute auto-correlation for the input data X1 and X2.\n\n        it will generate the correlation between some voxels and all voxels\n\n        Parameters\n        ----------\n        X1: a list of numpy array in shape [num_TRs, num_voxels1]\n            X1 contains the activity data filtered by ROIs\n            and prepared for correlation computation.\n            All elements of X1 must have the same num_voxels value.\n        X2: a list of numpy array in shape [num_TRs, num_voxels2]\n            len(X1) equals len(X2).\n            All elements of X2 must have the same num_voxels value.\n            X2 can be identical to X1; if not, X1 must have more voxels\n            than X2 (guaranteed by self.fit and/or self.predict).\n        start_voxel: int, default 0\n            the starting voxel id for correlation computation\n        num_processed_voxels: int, default None\n            the number of voxels it computes for correlation computation\n            if it is None, it is set to self.num_voxels\n\n        Returns\n        -------\n        corr_data: the correlation data\n                    in shape [len(X), num_processed_voxels, num_voxels2]\n        \"\"\"\n        num_samples = len(X1)\n        assert num_samples > 0, \\\n            'at least one sample is needed for correlation computation'\n        num_voxels1 = X1[0].shape[1]\n        num_voxels2 = X2[0].shape[1]\n        assert num_voxels1 * num_voxels2 == self.num_features_, \\\n            'the number of features provided by the input data ' \\\n            'does not match the number of features defined in the model'\n        assert X1[0].shape[0] == X2[0].shape[0], \\\n            'the numbers of TRs of X1 and X2 are not identical'\n        if num_processed_voxels is None:\n            num_processed_voxels = num_voxels1\n        corr_data = np.zeros((num_samples, num_processed_voxels, num_voxels2),\n                             np.float32, order='C')\n        # compute correlation\n        for idx, data in enumerate(X1):\n            data2 = X2[idx]\n            num_TRs = data.shape[0]\n            blas.compute_corr_vectors('N', 'T',\n                                      num_voxels2, num_processed_voxels,\n                                      num_TRs,\n                                      1.0, data2, num_voxels2,\n                                      data, num_voxels1,\n                                      0.0, corr_data, num_voxels2,\n                                      start_voxel, idx)\n        logger.debug(\n            'correlation computation done'\n        )\n        return corr_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _normalize_correlation_data(self, corr_data, norm_unit):\n        # normalize if necessary\n        if norm_unit > 1:\n            num_samples = len(corr_data)\n            [_, d2, d3] = corr_data.shape\n            second_dimension = d2 * d3\n            # this is a shallow copy\n            normalized_corr_data = corr_data.reshape(1,\n                                                     num_samples,\n                                                     second_dimension)\n            fcma_extension.normalization(normalized_corr_data, norm_unit)\n            normalized_corr_data = normalized_corr_data.reshape(num_samples,\n                                                                d2, d3)\n            logger.debug(\n                'normalization done'\n            )\n        else:\n            normalized_corr_data = corr_data\n        return normalized_corr_data", "response": "Normalizes the correlation data if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_kernel_matrix_in_portion(self, X1, X2):\n        kernel_matrix = np.zeros((self.num_samples_, self.num_samples_),\n                                 np.float32,\n                                 order='C')\n        sr = 0\n        row_length = self.num_processed_voxels\n        num_voxels2 = X2[0].shape[1]\n        normalized_corr_data = None\n        while sr < self.num_voxels_:\n            if row_length >= self.num_voxels_ - sr:\n                row_length = self.num_voxels_ - sr\n            # compute sub-correlation\n            corr_data = self._prepare_corerelation_data(X1, X2,\n                                                        sr, row_length)\n            # normalization\n            normalized_corr_data = self._normalize_correlation_data(\n                corr_data,\n                self.epochs_per_subj)\n            # compute partial kernel matrices\n            # for using kernel matrix computation from voxel selection\n            normalized_corr_data = normalized_corr_data.reshape(\n                1,\n                self.num_samples_,\n                row_length * num_voxels2)\n            blas.compute_kernel_matrix('L', 'T',\n                                       self.num_samples_,\n                                       row_length * num_voxels2,\n                                       1.0, normalized_corr_data,\n                                       0, row_length * num_voxels2,\n                                       1.0, kernel_matrix, self.num_samples_)\n            sr += row_length\n        # shrink the values for getting more stable alpha values\n        # in SVM training iteration\n        num_digits = len(str(int(kernel_matrix[0, 0])))\n        self.num_digits_ = num_digits\n        if num_digits > 2:\n            proportion = 10**(2-num_digits)\n            kernel_matrix *= proportion\n        return kernel_matrix, normalized_corr_data", "response": "Compute the kernel matrix for the given set of voxels X1 and X2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating training data for the classifier.", "response": "def _generate_training_data(self, X1, X2, num_training_samples):\n        \"\"\"Generate training data for the classifier.\n\n        Compute the correlation, do the normalization if necessary,\n        and compute the kernel matrix if the classifier is\n        sklearn.svm.SVC with precomputed kernel.\n\n        Parameters\n        ----------\n        X1: a list of numpy array in shape [num_TRs, num_voxels]\n            X1 contains the activity data filtered by ROIs\n            and prepared for correlation computation.\n            All elements of X1 must have the same num_voxels value,\n        X2: a list of numpy array in shape [num_TRs, num_voxels]\n            len(X1) equals len(X2).\n            All elements of X2 must have the same num_voxels value.\n            X2 can be identical to X1; if not, X1 must have more voxels\n            than X2 (guaranteed by self.fit).\n        num_training_samples: Optional[int]\n            Default None.\n            The number of samples used in the training,\n            which is set when the kernel matrix is constructed\n            portion by portion so the similarity vectors of the\n            test data have to be computed here.\n            This is ONLY set when sklearn.svm.SVC with\n            precomputed kernel is used.\n            If it is set, only those samples will be used to fit the model.\n\n        Returns\n        -------\n        data: 2D numpy array\n            If the classifier is sklearn.svm.SVC with precomputed kernel,\n            data is the kenrl matrix in shape [num_samples, num_samples];\n            otherwise, data is in shape [num_samples, num_features] as\n            the training data.\n        \"\"\"\n        if not (isinstance(self.clf, sklearn.svm.SVC)\n                and self.clf.kernel == 'precomputed'):\n            # correlation computation\n            corr_data = self._prepare_corerelation_data(X1, X2)\n            # normalization\n            normalized_corr_data = self._normalize_correlation_data(\n                corr_data,\n                self.epochs_per_subj)\n            # training data prepare\n            data = normalized_corr_data.reshape(self.num_samples_,\n                                                self.num_features_)\n            self.training_data_ = None\n        else:  # SVM with precomputed kernel\n            if self.num_processed_voxels < self.num_voxels_:\n                if num_training_samples is None:\n                    raise RuntimeError('the kernel matrix will be '\n                                       'computed portion by portion, '\n                                       'the test samples must be predefined '\n                                       'by specifying '\n                                       'num_training_samples')\n                if num_training_samples >= self.num_samples_:\n                    raise ValueError('the number of training samples '\n                                     'must be smaller than '\n                                     'the number of total samples')\n            data, normalized_corr_data = \\\n                self._compute_kernel_matrix_in_portion(X1, X2)\n            if self.num_processed_voxels >= self.num_voxels_:\n                # training data is in shape\n                # [num_samples, num_voxels * num_voxels]\n                self.training_data_ = normalized_corr_data.reshape(\n                    self.num_samples_,\n                    self.num_features_)\n            else:\n                # do not store training data because it was partially computed\n                self.training_data_ = None\n            logger.debug(\n                'kernel computation done'\n            )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y, num_training_samples=None):\n        time1 = time.time()\n        assert len(X) == len(y), \\\n            'the number of samples must be equal to the number of labels'\n        for x in X:\n            assert len(x) == 2, \\\n                'there must be two parts for each correlation computation'\n        X1, X2 = zip(*X)\n        if not (isinstance(self.clf, sklearn.svm.SVC)\n                and self.clf.kernel == 'precomputed'):\n            if num_training_samples is not None:\n                num_training_samples = None\n                logger.warn(\n                    'num_training_samples should not be set for classifiers '\n                    'other than SVM with precomputed kernels'\n                )\n        num_samples = len(X1)\n        num_voxels1 = X1[0].shape[1]\n        num_voxels2 = X2[0].shape[1]\n        # make sure X1 always has more voxels\n        if num_voxels1 < num_voxels2:\n            X1, X2 = X2, X1\n            num_voxels1, num_voxels2 = num_voxels2, num_voxels1\n        self.num_voxels_ = num_voxels1\n        self.num_features_ = num_voxels1 * num_voxels2\n        self.num_samples_ = num_samples\n\n        data = self._generate_training_data(X1, X2, num_training_samples)\n\n        if num_training_samples is not None:\n            self.test_raw_data_ = None\n            self.test_data_ = data[num_training_samples:,\n                                   0:num_training_samples]\n            # limit training to the data specified by num_training_samples\n            data = data[0:num_training_samples, 0:num_training_samples]\n        # training\n        self.clf = self.clf.fit(data, y[0:num_training_samples])\n        # set the test data\n        if num_training_samples is None:\n            self.test_raw_data_ = None\n            self.test_data_ = None\n        time2 = time.time()\n        logger.info(\n            'training done, takes %.2f s' %\n            (time2 - time1)\n        )\n        return self", "response": "Use the correlation data to train a model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict(self, X=None):\n        time1 = time.time()\n        if X is not None:\n            for x in X:\n                assert len(x) == 2, \\\n                    'there must be two parts for each correlation computation'\n            X1, X2 = zip(*X)\n            num_voxels1 = X1[0].shape[1]\n            num_voxels2 = X2[0].shape[1]\n            # make sure X1 always has more voxels\n            if num_voxels1 < num_voxels2:\n                X1, X2 = X2, X1\n                num_voxels1, num_voxels2 = num_voxels2, num_voxels1\n            assert self.num_features_ == num_voxels1 * num_voxels2, \\\n                'the number of features does not match the model'\n            num_test_samples = len(X1)\n            self.test_raw_data_ = X\n\n            # correlation computation\n            corr_data = self._prepare_corerelation_data(X1, X2)\n            # normalization\n            normalized_corr_data = self._normalize_correlation_data(\n                corr_data,\n                num_test_samples)\n            # test data generation\n            self.test_data_ = self._prepare_test_data(normalized_corr_data)\n        # prediction\n        y_pred = self.clf.predict(self.test_data_)\n        time2 = time.time()\n        logger.info(\n            'prediction done, takes %.2f s' %\n            (time2 - time1)\n        )\n        return y_pred", "response": "Use a trained model to predict the correlation of the input data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noutput the decision value of the prediction.", "response": "def decision_function(self, X=None):\n        \"\"\"Output the decision value of the prediction.\n\n        if X is not equal to self.test_raw_data\\\\_, i.e. predict is not called,\n        first generate the test_data\n        after getting the test_data, get the decision value via self.clf.\n        if X is None, test_data\\\\_ is ready to be used\n\n        Parameters\n        ----------\n        X: Optional[list of tuple (data1, data2)]\n            data1 and data2 are numpy array in shape [num_TRs, num_voxels]\n            to be computed for correlation.\n            default None, meaning that the data to be predicted\n            have been processed in the fit method.\n            Otherwise, X contains the activity data filtered by ROIs\n            and prepared for correlation computation.\n            len(X) is the number of test samples.\n            if len(X) > 1: normalization is done on all test samples.\n            Within list, all data1s must have the same num_voxels value,\n            all data2s must have the same num_voxels value.\n\n        Returns\n        -------\n        confidence: the predictions confidence values of X, in shape [len(X),]\n        \"\"\"\n        if X is not None and not self._is_equal_to_test_raw_data(X):\n            for x in X:\n                assert len(x) == 2, \\\n                    'there must be two parts for each correlation computation'\n            X1, X2 = zip(*X)\n            num_voxels1 = X1[0].shape[1]\n            num_voxels2 = X2[0].shape[1]\n            assert len(X1) == len(X2), \\\n                'the list lengths do not match'\n            # make sure X1 always has more voxels\n            if num_voxels1 < num_voxels2:\n                X1, X2 = X2, X1\n                num_voxels1, num_voxels2 = num_voxels2, num_voxels1\n            assert self.num_features_ == num_voxels1 * num_voxels2, \\\n                'the number of features does not match the model'\n            num_test_samples = len(X1)\n            self.test_raw_data_ = X\n            # generate the test_data first\n            # correlation computation\n            corr_data = self._prepare_corerelation_data(X1, X2)\n            # normalization\n            normalized_corr_data = \\\n                self._normalize_correlation_data(corr_data,\n                                                 num_test_samples)\n            # test data generation\n            self.test_data_ = self._prepare_test_data(normalized_corr_data)\n        confidence = self.clf.decision_function(self.test_data_)\n        return confidence"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef score(self, X, y, sample_weight=None):\n        from sklearn.metrics import accuracy_score\n        if isinstance(self.clf, sklearn.svm.SVC) \\\n                and self.clf.kernel == 'precomputed' \\\n                and self.training_data_ is None:\n            result = accuracy_score(y, self.predict(),\n                                    sample_weight=sample_weight)\n        else:\n            result = accuracy_score(y, self.predict(X),\n                                    sample_weight=sample_weight)\n        return result", "response": "Returns the mean accuracy on the given test data and labels."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isc(data, pairwise=False, summary_statistic=None, tolerate_nans=True):\n\n    # Check response time series input format\n    data, n_TRs, n_voxels, n_subjects = _check_timeseries_input(data)\n\n    # No summary statistic if only two subjects\n    if n_subjects == 2:\n        logger.info(\"Only two subjects! Simply computing Pearson correlation.\")\n        summary_statistic = None\n\n    # Check tolerate_nans input and use either mean/nanmean and exclude voxels\n    if tolerate_nans:\n        mean = np.nanmean\n    else:\n        mean = np.mean\n    data, mask = _threshold_nans(data, tolerate_nans)\n\n    # Compute correlation for only two participants\n    if n_subjects == 2:\n\n        # Compute correlation for each corresponding voxel\n        iscs_stack = array_correlation(data[..., 0],\n                                       data[..., 1])[np.newaxis, :]\n\n    # Compute pairwise ISCs using voxel loop and corrcoef for speed\n    elif pairwise:\n\n        # Swap axes for np.corrcoef\n        data = np.swapaxes(data, 2, 0)\n\n        # Loop through voxels\n        voxel_iscs = []\n        for v in np.arange(data.shape[1]):\n            voxel_data = data[:, v, :]\n\n            # Correlation matrix for all pairs of subjects (triangle)\n            iscs = squareform(np.corrcoef(voxel_data), checks=False)\n            voxel_iscs.append(iscs)\n\n        iscs_stack = np.column_stack(voxel_iscs)\n\n    # Compute leave-one-out ISCs\n    elif not pairwise:\n\n        # Loop through left-out subjects\n        iscs_stack = []\n        for s in np.arange(n_subjects):\n\n            # Correlation between left-out subject and mean of others\n            iscs_stack.append(array_correlation(\n                data[..., s],\n                mean(np.delete(data, s, axis=2), axis=2)))\n\n        iscs_stack = np.array(iscs_stack)\n\n    # Get ISCs back into correct shape after masking out NaNs\n    iscs = np.full((iscs_stack.shape[0], n_voxels), np.nan)\n    iscs[:, np.where(mask)[0]] = iscs_stack\n\n    # Summarize results (if requested)\n    if summary_statistic:\n        iscs = compute_summary_statistic(iscs,\n                                         summary_statistic=summary_statistic,\n                                         axis=0)[np.newaxis, :]\n\n    # Throw away first dimension if singleton\n    if iscs.shape[0] == 1:\n        iscs = iscs[0]\n\n    return iscs", "response": "Intersubject correlation between each voxel or ROI and the average of the other subjects response time series."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_isc_input(iscs, pairwise=False):\n\n    \"\"\"Checks ISC inputs for statistical tests\n\n    Input ISCs should be n_subjects (leave-one-out approach) or\n    n_pairs (pairwise approach) by n_voxels or n_ROIs array or a 1D\n    array (or list) of ISC values for a single voxel or ROI. This\n    function is only intended to be used internally by other\n    functions in this module (e.g., bootstrap_isc, permutation_isc).\n\n    Parameters\n    ----------\n    iscs : ndarray or list\n        ISC values\n\n    Returns\n    -------\n    iscs : ndarray\n        Array of ISC values\n\n    n_subjects : int\n        Number of subjects\n\n    n_voxels : int\n        Number of voxels (or ROIs)\n    \"\"\"\n\n    # Standardize structure of input data\n    if type(iscs) == list:\n        iscs = np.array(iscs)[:, np.newaxis]\n\n    elif isinstance(iscs, np.ndarray):\n        if iscs.ndim == 1:\n            iscs = iscs[:, np.newaxis]\n\n    # Check if incoming pairwise matrix is vectorized triangle\n    if pairwise:\n        try:\n            test_square = squareform(iscs[:, 0])\n            n_subjects = test_square.shape[0]\n        except ValueError:\n            raise ValueError(\"For pairwise input, ISCs must be the \"\n                             \"vectorized triangle of a square matrix.\")\n    elif not pairwise:\n        n_subjects = iscs.shape[0]\n\n    # Infer subjects, voxels and print for user to check\n    n_voxels = iscs.shape[1]\n    logger.info(\"Assuming {0} subjects with and {1} \"\n                \"voxel(s) or ROI(s) in bootstrap ISC test.\".format(n_subjects,\n                                                                   n_voxels))\n\n    return iscs, n_subjects, n_voxels", "response": "Checks ISCs input for statistical tests."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the input array for ISFC targets input array for ISFC analysis.", "response": "def _check_targets_input(targets, data):\n\n    \"\"\"Checks ISFC targets input array\n\n    For ISFC analysis, targets input array should either be a list\n    of n_TRs by n_targets arrays (where each array corresponds to\n    a subject), or an n_TRs by n_targets by n_subjects ndarray. This\n    function also checks the shape of the targets array against the\n    input data array.\n\n    Parameters\n    ----------\n    data : list or ndarray (n_TRs x n_voxels x n_subjects)\n        fMRI data for which to compute ISFC\n\n    targets : list or ndarray (n_TRs x n_voxels x n_subjects)\n        fMRI data to use as targets for ISFC\n\n    Returns\n    -------\n    targets : ndarray (n_TRs x n_voxels x n_subjects)\n        ISFC targets with standadized structure\n\n    n_TRs : int\n        Number of time points (TRs) for targets array\n\n    n_voxels : int\n        Number of voxels (or ROIs) for targets array\n\n    n_subjects : int\n        Number of subjects for targets array\n\n    symmetric : bool\n        Indicator for symmetric vs. asymmetric\n    \"\"\"\n\n    if isinstance(targets, np.ndarray) or isinstance(targets, list):\n        targets, n_TRs, n_voxels, n_subjects = (\n            _check_timeseries_input(targets))\n        if data.shape[0] != n_TRs:\n            raise ValueError(\"Targets array must have same number of \"\n                             \"TRs as input data\")\n        if data.shape[2] != n_subjects:\n            raise ValueError(\"Targets array must have same number of \"\n                             \"subjects as input data\")\n        symmetric = False\n    else:\n        targets = data\n        n_TRs, n_voxels, n_subjects = data.shape\n        symmetric = True\n\n    return targets, n_TRs, n_voxels, n_subjects, symmetric"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_summary_statistic(iscs, summary_statistic='mean', axis=None):\n\n    \"\"\"Computes summary statistics for ISCs\n\n    Computes either the 'mean' or 'median' across a set of ISCs. In the\n    case of the mean, ISC values are first Fisher Z transformed (arctanh),\n    averaged, then inverse Fisher Z transformed (tanh).\n\n    The implementation is based on the work in [SilverDunlap1987]_.\n\n    .. [SilverDunlap1987] \"Averaging corrlelation coefficients: should\n       Fisher's z transformation be used?\", N. C. Silver, W. P. Dunlap, 1987,\n       Journal of Applied Psychology, 72, 146-148.\n       https://doi.org/10.1037/0021-9010.72.1.146\n\n    Parameters\n    ----------\n    iscs : list or ndarray\n        ISC values\n\n    summary_statistic : str, default: 'mean'\n        Summary statistic, 'mean' or 'median'\n\n    axis : None or int or tuple of ints, optional\n        Axis or axes along which the means are computed. The default is to\n        compute the mean of the flattened array.\n\n    Returns\n    -------\n    statistic : float or ndarray\n        Summary statistic of ISC values\n\n    \"\"\"\n\n    if summary_statistic not in ('mean', 'median'):\n        raise ValueError(\"Summary statistic must be 'mean' or 'median'\")\n\n    # Compute summary statistic\n    if summary_statistic == 'mean':\n        statistic = np.tanh(np.nanmean(np.arctanh(iscs), axis=axis))\n    elif summary_statistic == 'median':\n        statistic = np.nanmedian(iscs, axis=axis)\n\n    return statistic", "response": "Computes summary statistics for a set of ISCs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef squareform_isfc(isfcs, iscs=None):\n\n    \"\"\"Converts square ISFCs to condensed ISFCs (and ISCs), and vice-versa\n\n    If input is a 2- or 3-dimensional array of square ISFC matrices, converts\n    this to the condensed off-diagonal ISFC values (i.e., the vectorized\n    triangle) and the diagonal ISC values. In this case, input must be a\n    single array of shape either n_voxels x n_voxels or n_subjects (or\n    n_pairs) x n_voxels x n_voxels. The condensed ISFC values are vectorized\n    according to scipy.spatial.distance.squareform, yielding n_voxels *\n    (n_voxels - 1) / 2 values comprising every voxel pair. Alternatively, if\n    input is an array of condensed off-diagonal ISFC values and an array of\n    diagonal ISC values, the square (redundant) ISFC values are returned.\n    This function mimics scipy.spatial.distance.squareform, but is intended\n    to retain the diagonal ISC values.\n\n    Parameters\n    ----------\n    isfcs : ndarray\n        Either condensed or redundant ISFC values\n\n    iscs: ndarray, optional\n        Diagonal ISC values, required when input is condensed\n\n    Returns\n    -------\n    isfcs : ndarray or tuple of ndarrays\n        If condensed ISFCs are passed, a single redundant ISFC array is\n        returned; if redundant ISFCs are passed, both a condensed off-\n        diagonal ISFC array and the diagonal ISC values are returned\n    \"\"\"\n\n    # Check if incoming ISFCs are square (redundant)\n    if not type(iscs) == np.ndarray and isfcs.shape[-2] == isfcs.shape[-1]:\n        if isfcs.ndim == 2:\n            isfcs = isfcs[np.newaxis, ...]\n        if isfcs.ndim == 3:\n            iscs = np.diagonal(isfcs, axis1=1, axis2=2)\n            isfcs = np.vstack([squareform(isfc, checks=False)[np.newaxis, :]\n                               for isfc in isfcs])\n        else:\n            raise ValueError(\"Square (redundant) ISFCs must be square \"\n                             \"with multiple subjects or pairs of subjects \"\n                             \"indexed by the first dimension\")\n        if isfcs.shape[0] == iscs.shape[0] == 1:\n            isfcs, iscs = isfcs[0], iscs[0]\n        return isfcs, iscs\n\n    # Otherwise, convert from condensed to redundant\n    else:\n        if isfcs.ndim == iscs.ndim == 1:\n            isfcs, iscs = isfcs[np.newaxis, :], iscs[np.newaxis, :]\n        isfcs_stack = []\n        for isfc, isc in zip(isfcs, iscs):\n            isfc_sq = squareform(isfc, checks=False)\n            np.fill_diagonal(isfc_sq, isc)\n            isfcs_stack.append(isfc_sq[np.newaxis, ...])\n        isfcs = np.vstack(isfcs_stack)\n        if isfcs.shape[0] == 1:\n            isfcs = isfcs[0]\n        return isfcs", "response": "Takes a square ISFC array and returns the condensed ISFCs and ISCs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _threshold_nans(data, tolerate_nans):\n\n    \"\"\"Thresholds data based on proportion of subjects with NaNs\n\n    Takes in data and a threshold value (float between 0.0 and 1.0) determining\n    the permissible proportion of subjects with non-NaN values. For example, if\n    threshold=.8, any voxel where >= 80% of subjects have non-NaN values will\n    be left unchanged, while any voxel with < 80% non-NaN values will be\n    assigned all NaN values and included in the nan_mask output. Note that the\n    output data has not been masked and will be same shape as the input data,\n    but may have a different number of NaNs based on the threshold.\n\n    Parameters\n    ----------\n    data : ndarray (n_TRs x n_voxels x n_subjects)\n        fMRI time series data\n\n    tolerate_nans : bool or float (0.0 <= threshold <= 1.0)\n        Proportion of subjects with non-NaN values required to keep voxel\n\n    Returns\n    -------\n    data : ndarray (n_TRs x n_voxels x n_subjects)\n        fMRI time series data with adjusted NaNs\n\n    nan_mask : ndarray (n_voxels,)\n        Boolean mask array of voxels with too many NaNs based on threshold\n\n    \"\"\"\n\n    nans = np.all(np.any(np.isnan(data), axis=0), axis=1)\n\n    # Check tolerate_nans input and use either mean/nanmean and exclude voxels\n    if tolerate_nans is True:\n        logger.info(\"ISC computation will tolerate all NaNs when averaging\")\n\n    elif type(tolerate_nans) is float:\n        if not 0.0 <= tolerate_nans <= 1.0:\n            raise ValueError(\"If threshold to tolerate NaNs is a float, \"\n                             \"it must be between 0.0 and 1.0; got {0}\".format(\n                                tolerate_nans))\n        nans += ~(np.sum(~np.any(np.isnan(data), axis=0), axis=1) >=\n                  data.shape[-1] * tolerate_nans)\n        logger.info(\"ISC computation will tolerate voxels with at least \"\n                    \"{0} non-NaN values: {1} voxels do not meet \"\n                    \"threshold\".format(tolerate_nans,\n                                       np.sum(nans)))\n\n    else:\n        logger.info(\"ISC computation will not tolerate NaNs when averaging\")\n\n    mask = ~nans\n    data = data[:, mask, :]\n\n    return data, mask", "response": "Thresholds data based on proportion of subjects with non - NaN values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bootstrap_isc(iscs, pairwise=False, summary_statistic='median',\n                  n_bootstraps=1000, ci_percentile=95, random_state=None):\n\n    \"\"\"One-sample group-level bootstrap hypothesis test for ISCs\n\n    For ISCs from one more voxels or ROIs, resample subjects with replacement\n    to construct a bootstrap distribution. Input is a list or ndarray of\n    ISCs for a single voxel/ROI, or an ISCs-by-voxels ndarray. ISC values\n    should be either N ISC values for N subjects in the leave-one-out appraoch\n    (pairwise=False), N(N-1)/2 ISC values for N subjects in the pairwise\n    approach (pairwise=True). In the pairwise approach, ISC values should\n    correspond to the vectorized upper triangle of a square corrlation matrix\n    (see scipy.stats.distance.squareform). Shifts bootstrap distribution by\n    actual summary statistic (effectively to zero) for two-tailed null\n    hypothesis test (Hall & Wilson, 1991). Uses subject-wise (not pair-wise)\n    resampling in the pairwise approach. Returns the observed ISC, the\n    confidence interval, and a p-value for the bootstrap hypothesis test, as\n    well as the bootstrap distribution of summary statistics. According to\n    Chen et al., 2016, this is the preferred nonparametric approach for\n    controlling false positive rates (FPR) for one-sample tests in the pairwise\n    approach.\n\n    The implementation is based on the work in [Chen2016]_ and\n    [HallWilson1991]_.\n\n    .. [HallWilson1991] \"Two guidelines for bootstrap hypothesis testing.\",\n       P. Hall, S. R., Wilson, 1991, Biometrics, 757-762.\n       https://doi.org/10.2307/2532163\n\n    Parameters\n    ----------\n    iscs : list or ndarray, ISCs by voxels array\n        ISC values for one or more voxels\n\n    pairwise : bool, default: False\n        Indicator of pairwise or leave-one-out, should match ISCs structure\n\n    summary_statistic : str, default: 'median'\n        Summary statistic, either 'median' (default) or 'mean'\n\n    n_bootstraps : int, default: 1000\n        Number of bootstrap samples (subject-level with replacement)\n\n    ci_percentile : int, default: 95\n         Percentile for computing confidence intervals\n\n    random_state = int or None, default: None\n        Initial random seed\n\n    Returns\n    -------\n    observed : float, median (or mean) ISC value\n        Summary statistic for actual ISCs\n\n    ci : tuple, bootstrap confidence intervals\n        Confidence intervals generated from bootstrap distribution\n\n    p : float, p-value\n        p-value based on bootstrap hypothesis test\n\n    distribution : ndarray, bootstraps by voxels (optional)\n        Bootstrap distribution if return_bootstrap=True\n\n    \"\"\"\n\n    # Standardize structure of input data\n    iscs, n_subjects, n_voxels = _check_isc_input(iscs, pairwise=pairwise)\n\n    # Check for valid summary statistic\n    if summary_statistic not in ('mean', 'median'):\n        raise ValueError(\"Summary statistic must be 'mean' or 'median'\")\n\n    # Compute summary statistic for observed ISCs\n    observed = compute_summary_statistic(iscs,\n                                         summary_statistic=summary_statistic,\n                                         axis=0)\n\n    # Set up an empty list to build our bootstrap distribution\n    distribution = []\n\n    # Loop through n bootstrap iterations and populate distribution\n    for i in np.arange(n_bootstraps):\n\n        # Random seed to be deterministically re-randomized at each iteration\n        if isinstance(random_state, np.random.RandomState):\n            prng = random_state\n        else:\n            prng = np.random.RandomState(random_state)\n\n        # Randomly sample subject IDs with replacement\n        subject_sample = sorted(prng.choice(np.arange(n_subjects),\n                                            size=n_subjects))\n\n        # Squareform and shuffle rows/columns of pairwise ISC matrix to\n        # to retain correlation structure among ISCs, then get triangle\n        if pairwise:\n\n            # Loop through voxels\n            isc_sample = []\n            for voxel_iscs in iscs.T:\n\n                # Square the triangle and fill diagonal\n                voxel_iscs = squareform(voxel_iscs)\n                np.fill_diagonal(voxel_iscs, 1)\n\n                # Check that pairwise ISC matrix is square and symmetric\n                assert voxel_iscs.shape[0] == voxel_iscs.shape[1]\n                assert np.allclose(voxel_iscs, voxel_iscs.T)\n\n                # Shuffle square correlation matrix and get triangle\n                voxel_sample = voxel_iscs[subject_sample, :][:, subject_sample]\n                voxel_sample = squareform(voxel_sample, checks=False)\n\n                # Censor off-diagonal 1s for same-subject pairs\n                voxel_sample[voxel_sample == 1.] = np.NaN\n\n                isc_sample.append(voxel_sample)\n\n            isc_sample = np.column_stack(isc_sample)\n\n        # Get simple bootstrap sample if not pairwise\n        elif not pairwise:\n            isc_sample = iscs[subject_sample, :]\n\n        # Compute summary statistic for bootstrap ISCs per voxel\n        # (alternatively could construct distribution for all voxels\n        # then compute statistics, but larger memory footprint)\n        distribution.append(compute_summary_statistic(\n                                isc_sample,\n                                summary_statistic=summary_statistic,\n                                axis=0))\n\n        # Update random state for next iteration\n        random_state = np.random.RandomState(prng.randint(0, MAX_RANDOM_SEED))\n\n    # Convert distribution to numpy array\n    distribution = np.array(distribution)\n\n    # Compute CIs of median from bootstrap distribution (default: 95%)\n    ci = (np.percentile(distribution, (100 - ci_percentile)/2, axis=0),\n          np.percentile(distribution, ci_percentile + (100 - ci_percentile)/2,\n                        axis=0))\n\n    # Shift bootstrap distribution to 0 for hypothesis test\n    shifted = distribution - observed\n\n    # Get p-value for actual median from shifted distribution\n    p = p_from_null(observed, shifted,\n                    side='two-sided', exact=False,\n                    axis=0)\n\n    return observed, ci, p, distribution", "response": "One - sample group - level bootstrap hypothesis test for ISCs from one more voxels or ROIs and resample subjects with replacement\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _permute_one_sample_iscs(iscs, group_parameters, i, pairwise=False,\n                             summary_statistic='median', group_matrix=None,\n                             exact_permutations=None, prng=None):\n\n    \"\"\"Applies one-sample permutations to ISC data\n\n    Input ISCs should be n_subjects (leave-one-out approach) or\n    n_pairs (pairwise approach) by n_voxels or n_ROIs array.\n    This function is only intended to be used internally by the\n    permutation_isc function in this module.\n\n    Parameters\n    ----------\n    iscs : ndarray or list\n        ISC values\n\n    group_parameters : dict\n        Dictionary of group parameters\n\n    i : int\n        Permutation iteration\n\n    pairwise : bool, default: False\n        Indicator of pairwise or leave-one-out, should match ISCs variable\n\n    summary_statistic : str, default: 'median'\n        Summary statistic, either 'median' (default) or 'mean'\n\n    exact_permutations : list\n        List of permutations\n\n    prng = None or np.random.RandomState, default: None\n        Initial random seed\n\n    Returns\n    -------\n    isc_sample : ndarray\n        Array of permuted ISC values\n\n    \"\"\"\n\n    # Randomized sign-flips\n    if exact_permutations:\n        sign_flipper = np.array(exact_permutations[i])\n    else:\n        sign_flipper = prng.choice([-1, 1],\n                                   size=group_parameters['n_subjects'],\n                                   replace=True)\n\n    # If pairwise, apply sign-flips by rows and columns\n    if pairwise:\n        matrix_flipped = (group_parameters['group_matrix'] * sign_flipper\n                                                           * sign_flipper[\n                                                                :, np.newaxis])\n        sign_flipper = squareform(matrix_flipped, checks=False)\n\n    # Apply flips along ISC axis (same across voxels)\n    isc_flipped = iscs * sign_flipper[:, np.newaxis]\n\n    # Get summary statistics on sign-flipped ISCs\n    isc_sample = compute_summary_statistic(\n                    isc_flipped,\n                    summary_statistic=summary_statistic,\n                    axis=0)\n\n    return isc_sample", "response": "Applies one - sample permutations to ISCs array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _permute_two_sample_iscs(iscs, group_parameters, i, pairwise=False,\n                             summary_statistic='median',\n                             exact_permutations=None, prng=None):\n\n    \"\"\"Applies two-sample permutations to ISC data\n\n    Input ISCs should be n_subjects (leave-one-out approach) or\n    n_pairs (pairwise approach) by n_voxels or n_ROIs array.\n    This function is only intended to be used internally by the\n    permutation_isc function in this module.\n\n    Parameters\n    ----------\n    iscs : ndarray or list\n        ISC values\n\n    group_parameters : dict\n        Dictionary of group parameters\n\n    i : int\n        Permutation iteration\n\n    pairwise : bool, default: False\n        Indicator of pairwise or leave-one-out, should match ISCs variable\n\n    summary_statistic : str, default: 'median'\n        Summary statistic, either 'median' (default) or 'mean'\n\n    exact_permutations : list\n        List of permutations\n\n    prng = None or np.random.RandomState, default: None\n        Initial random seed\n        Indicator of pairwise or leave-one-out, should match ISCs variable\n\n    Returns\n    -------\n    isc_sample : ndarray\n        Array of permuted ISC values\n\n    \"\"\"\n\n    # Shuffle the group assignments\n    if exact_permutations:\n        group_shuffler = np.array(exact_permutations[i])\n    elif not exact_permutations and pairwise:\n        group_shuffler = prng.permutation(np.arange(\n            len(np.array(group_parameters['group_assignment'])[\n                            group_parameters['sorter']])))\n    elif not exact_permutations and not pairwise:\n        group_shuffler = prng.permutation(np.arange(\n            len(group_parameters['group_assignment'])))\n\n    # If pairwise approach, convert group assignments to matrix\n    if pairwise:\n\n        # Apply shuffler to group matrix rows/columns\n        group_shuffled = group_parameters['group_matrix'][\n                            group_shuffler, :][:, group_shuffler]\n\n        # Unsort shuffled matrix and squareform to create selector\n        group_selector = squareform(group_shuffled[\n                                    group_parameters['unsorter'], :]\n                                    [:, group_parameters['unsorter']],\n                                    checks=False)\n\n    # Shuffle group assignments in leave-one-out two sample test\n    elif not pairwise:\n\n        # Apply shuffler to group matrix rows/columns\n        group_selector = np.array(\n                    group_parameters['group_assignment'])[group_shuffler]\n\n    # Get difference of within-group summary statistics\n    # with group permutation\n    isc_sample = (compute_summary_statistic(\n                    iscs[group_selector == group_parameters[\n                                            'group_labels'][0], :],\n                    summary_statistic=summary_statistic,\n                    axis=0) -\n                  compute_summary_statistic(\n                    iscs[group_selector == group_parameters[\n                                            'group_labels'][1], :],\n                    summary_statistic=summary_statistic,\n                    axis=0))\n\n    return isc_sample", "response": "Applies two - sample permutations to ISCs data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef permutation_isc(iscs, group_assignment=None, pairwise=False,  # noqa: C901\n                    summary_statistic='median', n_permutations=1000,\n                    random_state=None):\n\n    \"\"\"Group-level permutation test for ISCs\n\n    For ISCs from one or more voxels or ROIs, permute group assignments to\n    construct a permutation distribution. Input is a list or ndarray of\n    ISCs  for a single voxel/ROI, or an ISCs-by-voxels ndarray. If two groups,\n    ISC values should stacked along first dimension (vertically), and a\n    group_assignment list (or 1d array) of same length as the number of\n    subjects should be provided to indicate groups. If no group_assignment\n    is provided, one-sample test is performed using a sign-flipping procedure.\n    Performs exact test if number of possible permutations (2**N for one-sample\n    sign-flipping, N! for two-sample shuffling) is less than or equal to number\n    of requested permutation; otherwise, performs approximate permutation test\n    using Monte Carlo resampling. ISC values should either be N ISC values for\n    N subjects in the leave-one-out approach (pairwise=False) or N(N-1)/2 ISC\n    values for N subjects in the pairwise approach (pairwise=True). In the\n    pairwise approach, ISC values should correspond to the vectorized upper\n    triangle of a square corrlation matrix (scipy.stats.distance.squareform).\n    Note that in the pairwise approach, group_assignment order should match the\n    row/column order of the subject-by-subject square ISC matrix even though\n    the input ISCs should be supplied as the vectorized upper triangle of the\n    square ISC matrix. Returns the observed ISC and permutation-based p-value\n    (two-tailed test), as well as the permutation distribution of summary\n    statistic. According to Chen et al., 2016, this is the preferred\n    nonparametric approach for controlling false positive rates (FPR) for\n    two-sample tests. This approach may yield inflated FPRs for one-sample\n    tests.\n\n    The implementation is based on the work in [Chen2016]_.\n\n    Parameters\n    ----------\n    iscs : list or ndarray, correlation matrix of ISCs\n        ISC values for one or more voxels\n\n    group_assignment : list or ndarray, group labels\n        Group labels matching order of ISC input\n\n    pairwise : bool, default: False\n        Indicator of pairwise or leave-one-out, should match ISCs variable\n\n    summary_statistic : str, default: 'median'\n        Summary statistic, either 'median' (default) or 'mean'\n\n    n_permutations : int, default: 1000\n        Number of permutation iteration (randomizing group assignment)\n\n    random_state = int, None, or np.random.RandomState, default: None\n        Initial random seed\n\n    Returns\n    -------\n    observed : float, ISC summary statistic or difference\n        Actual ISC or group difference (excluding between-group ISCs)\n\n    p : float, p-value\n        p-value based on permutation test\n\n    distribution : ndarray, permutations by voxels (optional)\n        Permutation distribution if return_bootstrap=True\n    \"\"\"\n\n    # Standardize structure of input data\n    iscs, n_subjects, n_voxels = _check_isc_input(iscs, pairwise=pairwise)\n\n    # Check for valid summary statistic\n    if summary_statistic not in ('mean', 'median'):\n        raise ValueError(\"Summary statistic must be 'mean' or 'median'\")\n\n    # Check match between group labels and ISCs\n    group_assignment = _check_group_assignment(group_assignment,\n                                               n_subjects)\n\n    # Get group parameters\n    group_parameters = _get_group_parameters(group_assignment, n_subjects,\n                                             pairwise=pairwise)\n\n    # Set up permutation type (exact or Monte Carlo)\n    if group_parameters['n_groups'] == 1:\n        if n_permutations < 2**n_subjects:\n            logger.info(\"One-sample approximate permutation test using \"\n                        \"sign-flipping procedure with Monte Carlo resampling.\")\n            exact_permutations = None\n        else:\n            logger.info(\"One-sample exact permutation test using \"\n                        \"sign-flipping procedure with 2**{0} \"\n                        \"({1}) iterations.\".format(n_subjects,\n                                                   2**n_subjects))\n            exact_permutations = list(product([-1, 1], repeat=n_subjects))\n            n_permutations = 2**n_subjects\n\n    # Check for exact test for two groups\n    else:\n        if n_permutations < np.math.factorial(n_subjects):\n            logger.info(\"Two-sample approximate permutation test using \"\n                        \"group randomization with Monte Carlo resampling.\")\n            exact_permutations = None\n        else:\n            logger.info(\"Two-sample exact permutation test using group \"\n                        \"randomization with {0}! \"\n                        \"({1}) iterations.\".format(\n                                n_subjects,\n                                np.math.factorial(n_subjects)))\n            exact_permutations = list(permutations(\n                np.arange(len(group_assignment))))\n            n_permutations = np.math.factorial(n_subjects)\n\n    # If one group, just get observed summary statistic\n    if group_parameters['n_groups'] == 1:\n        observed = compute_summary_statistic(\n                        iscs,\n                        summary_statistic=summary_statistic,\n                        axis=0)[np.newaxis, :]\n\n    # If two groups, get the observed difference\n    else:\n        observed = (compute_summary_statistic(\n                        iscs[group_parameters['group_selector'] ==\n                             group_parameters['group_labels'][0], :],\n                        summary_statistic=summary_statistic,\n                        axis=0) -\n                    compute_summary_statistic(\n                        iscs[group_parameters['group_selector'] ==\n                             group_parameters['group_labels'][1], :],\n                        summary_statistic=summary_statistic,\n                        axis=0))\n        observed = np.array(observed)\n\n    # Set up an empty list to build our permutation distribution\n    distribution = []\n\n    # Loop through n permutation iterations and populate distribution\n    for i in np.arange(n_permutations):\n\n        # Random seed to be deterministically re-randomized at each iteration\n        if exact_permutations:\n            prng = None\n        elif isinstance(random_state, np.random.RandomState):\n            prng = random_state\n        else:\n            prng = np.random.RandomState(random_state)\n\n        # If one group, apply sign-flipping procedure\n        if group_parameters['n_groups'] == 1:\n            isc_sample = _permute_one_sample_iscs(\n                            iscs, group_parameters, i,\n                            pairwise=pairwise,\n                            summary_statistic=summary_statistic,\n                            exact_permutations=exact_permutations,\n                            prng=prng)\n\n        # If two groups, set up group matrix get the observed difference\n        else:\n            isc_sample = _permute_two_sample_iscs(\n                            iscs, group_parameters, i,\n                            pairwise=pairwise,\n                            summary_statistic=summary_statistic,\n                            exact_permutations=exact_permutations,\n                            prng=prng)\n\n        # Tack our permuted ISCs onto the permutation distribution\n        distribution.append(isc_sample)\n\n        # Update random state for next iteration\n        if not exact_permutations:\n            random_state = np.random.RandomState(prng.randint(\n                                                    0, MAX_RANDOM_SEED))\n\n    # Convert distribution to numpy array\n    distribution = np.array(distribution)\n\n    # Get p-value for actual median from shifted distribution\n    if exact_permutations:\n        p = p_from_null(observed, distribution,\n                        side='two-sided', exact=True,\n                        axis=0)\n    else:\n        p = p_from_null(observed, distribution,\n                        side='two-sided', exact=False,\n                        axis=0)\n\n    return observed, p, distribution", "response": "Group - level permutation test for ISCs from one or more voxels or ROIs."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nphases randomization for one - sample ISC test for one - sample ROI and one - sample ISC test for one - sample ISC test for one - sample ROI.", "response": "def phaseshift_isc(data, pairwise=False, summary_statistic='median',\n                   n_shifts=1000, tolerate_nans=True, random_state=None):\n\n    \"\"\"Phase randomization for one-sample ISC test\n\n    For each voxel or ROI, compute the actual ISC and p-values\n    from a null distribution of ISCs where response time series\n    are phase randomized prior to computing ISC. If pairwise,\n    apply phase randomization to each subject and compute pairwise\n    ISCs. If leave-one-out approach is used (pairwise=False), only\n    apply phase randomization to the left-out subject in each iteration\n    of the leave-one-out procedure. Input data should be a list where\n    each item is a time-points by voxels ndarray for a given subject.\n    Multiple input ndarrays must be the same shape. If a single ndarray is\n    supplied, the last dimension is assumed to correspond to subjects.\n    When using leave-one-out approach, NaNs are ignored when computing mean\n    time series of N-1 subjects (default: tolerate_nans=True). Alternatively,\n    you may supply a float between 0 and 1 indicating a threshold proportion\n    of N subjects with non-NaN values required when computing the average time\n    series for a given voxel. For example, if tolerate_nans=.8, ISCs will be\n    computed for any voxel where >= 80% of subjects have non-NaN values,\n    while voxels with < 80% non-NaN values will be assigned NaNs. If set to\n    False, NaNs are not tolerated and voxels with one or more NaNs among the\n    N-1 subjects will be assigned NaN. Setting tolerate_nans to True or False\n    will not affect the pairwise approach; however, if a threshold float is\n    provided, voxels that do not reach this threshold will be excluded. Note\n    that accommodating NaNs may be notably slower than setting tolerate_nans to\n    False. Returns the observed ISC and p-values (two-tailed test), as well as\n    the null distribution of ISCs computed on phase-randomized data.\n\n    The implementation is based on the work in [Lerner2011]_ and\n    [Simony2016]_.\n\n    .. [Lerner2011] \"Topographic mapping of a hierarchy of temporal\n       receptive windows using a narrated story.\", Y. Lerner, C. J. Honey,\n       L. J. Silbert, U. Hasson, 2011, Journal of Neuroscience, 31, 2906-2915.\n       https://doi.org/10.1523/jneurosci.3684-10.2011\n\n    Parameters\n    ----------\n    data : list or ndarray (n_TRs x n_voxels x n_subjects)\n        fMRI data for which to compute ISFC\n\n    pairwise : bool, default: False\n        Whether to use pairwise (True) or leave-one-out (False) approach\n\n    summary_statistic : str, default: 'median'\n        Summary statistic, either 'median' (default) or 'mean'\n\n    n_shifts : int, default: 1000\n        Number of randomly shifted samples\n\n    tolerate_nans : bool or float, default: True\n        Accommodate NaNs (when averaging in leave-one-out approach)\n\n    random_state = int, None, or np.random.RandomState, default: None\n        Initial random seed\n\n    Returns\n    -------\n    observed : float, observed ISC (without time-shifting)\n        Actual ISCs\n\n    p : float, p-value\n        p-value based on time-shifting randomization test\n\n    distribution : ndarray, time-shifts by voxels (optional)\n        Time-shifted null distribution if return_bootstrap=True\n    \"\"\"\n\n    # Check response time series input format\n    data, n_TRs, n_voxels, n_subjects = _check_timeseries_input(data)\n\n    # Get actual observed ISC\n    observed = isc(data, pairwise=pairwise,\n                   summary_statistic=summary_statistic,\n                   tolerate_nans=tolerate_nans)\n\n    # Iterate through randomized shifts to create null distribution\n    distribution = []\n    for i in np.arange(n_shifts):\n\n        # Random seed to be deterministically re-randomized at each iteration\n        if isinstance(random_state, np.random.RandomState):\n            prng = random_state\n        else:\n            prng = np.random.RandomState(random_state)\n\n        # Get shifted version of data\n        shifted_data = phase_randomize(data, random_state=prng)\n\n        # In pairwise approach, apply all shifts then compute pairwise ISCs\n        if pairwise:\n\n            # Compute null ISC on shifted data for pairwise approach\n            shifted_isc = isc(shifted_data, pairwise=True,\n                              summary_statistic=summary_statistic,\n                              tolerate_nans=tolerate_nans)\n\n        # In leave-one-out, apply shift only to each left-out participant\n        elif not pairwise:\n\n            # Roll subject axis of phase-randomized data\n            shifted_data = np.rollaxis(shifted_data, 2, 0)\n\n            shifted_isc = []\n            for s, shifted_subject in enumerate(shifted_data):\n\n                # ISC of shifted left-out subject vs mean of N-1 subjects\n                nonshifted_mean = np.mean(np.delete(data, s, axis=2),\n                                          axis=2)\n                loo_isc = isc(np.dstack((shifted_subject, nonshifted_mean)),\n                              pairwise=False, summary_statistic=None,\n                              tolerate_nans=tolerate_nans)\n                shifted_isc.append(loo_isc)\n\n            # Get summary statistics across left-out subjects\n            shifted_isc = compute_summary_statistic(\n                            np.dstack(shifted_isc),\n                            summary_statistic=summary_statistic, axis=2)\n        distribution.append(shifted_isc)\n\n        # Update random state for next iteration\n        random_state = np.random.RandomState(prng.randint(0, MAX_RANDOM_SEED))\n\n    # Convert distribution to numpy array\n    distribution = np.vstack(distribution)\n\n    # Get p-value for actual median from shifted distribution\n    p = p_from_null(observed, distribution,\n                    side='two-sided', exact=False,\n                    axis=0)\n\n    return observed, p, distribution"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_prior(self, R):\n        centers, widths = self.init_centers_widths(R)\n        # update prior\n        prior = np.zeros(self.K * (self.n_dim + 1))\n        self.set_centers(prior, centers)\n        self.set_widths(prior, widths)\n        self.set_prior(prior)\n        return self", "response": "initialize prior for the subject\n        Returns the instance itself"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassigns posterior to prior based on Hungarian algorithm Returns the instance itself.", "response": "def _assign_posterior(self):\n        \"\"\"assign posterior to prior based on Hungarian algorithm\n\n        Returns\n        -------\n        TFA\n            Returns the instance itself.\n        \"\"\"\n\n        prior_centers = self.get_centers(self.local_prior)\n        posterior_centers = self.get_centers(self.local_posterior_)\n        posterior_widths = self.get_widths(self.local_posterior_)\n        # linear assignment on centers\n        cost = distance.cdist(prior_centers, posterior_centers, 'euclidean')\n        _, col_ind = linear_sum_assignment(cost)\n        # reorder centers/widths based on cost assignment\n        self.set_centers(self.local_posterior_, posterior_centers[col_ind])\n        self.set_widths(self.local_posterior_, posterior_widths[col_ind])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _converged(self):\n        diff = self.local_prior - self.local_posterior_\n        max_diff = np.max(np.fabs(diff))\n        if self.verbose:\n            _, mse = self._mse_converged()\n            diff_ratio = np.sum(diff ** 2) / np.sum(self.local_posterior_ ** 2)\n            logger.info(\n                'tfa prior posterior max diff %f mse %f diff_ratio %f' %\n                ((max_diff, mse, diff_ratio)))\n        if max_diff > self.threshold:\n            return False, max_diff\n        else:\n            return True, max_diff", "response": "Check convergence based on maximum absolute difference between prior and posterior."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _mse_converged(self):\n\n        mse = mean_squared_error(self.local_prior, self.local_posterior_,\n                                 multioutput='uniform_average')\n        if mse > self.threshold:\n            return False, mse\n        else:\n            return True, mse", "response": "Check convergence based on mean squared error between prior and posterior."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes offset of prior and posterior entries in the cluster table.", "response": "def get_map_offset(self):\n        \"\"\"Compute offset of prior/posterior\n\n\n        Returns\n        -------\n\n        map_offest : 1D array\n            The offset to different fields in prior/posterior\n\n        \"\"\"\n\n        nfield = 4\n        self.map_offset = np.zeros(nfield).astype(int)\n        field_size = self.K * np.array([self.n_dim, 1, self.cov_vec_size, 1])\n        for i in np.arange(nfield - 1) + 1:\n            self.map_offset[i] = self.map_offset[i - 1] + field_size[i - 1]\n        return self.map_offset"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef init_centers_widths(self, R):\n\n        kmeans = KMeans(\n            init='k-means++',\n            n_clusters=self.K,\n            n_init=10,\n            random_state=100)\n        kmeans.fit(R)\n        centers = kmeans.cluster_centers_\n        widths = self._get_max_sigma(R) * np.ones((self.K, 1))\n        return centers, widths", "response": "Initialize prior of centers and widths of the cluster entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_template(self, R):\n\n        centers, widths = self.init_centers_widths(R)\n        template_prior =\\\n            np.zeros(self.K * (self.n_dim + 2 + self.cov_vec_size))\n        # template centers cov and widths var are const\n        template_centers_cov = np.cov(R.T) * math.pow(self.K, -2 / 3.0)\n        template_widths_var = self._get_max_sigma(R)\n        centers_cov_all = np.tile(from_sym_2_tri(template_centers_cov), self.K)\n        widths_var_all = np.tile(template_widths_var, self.K)\n        # initial mean of centers' mean\n        self.set_centers(template_prior, centers)\n        self.set_widths(template_prior, widths)\n        self.set_centers_mean_cov(template_prior, centers_cov_all)\n        self.set_widths_mean_var(template_prior, widths_var_all)\n        return template_prior, template_centers_cov, template_widths_var", "response": "Compute a template on latent factors of one subject s fMRI data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting estimation on widths", "response": "def set_widths(self, estimation, widths):\n        \"\"\"Set estimation on widths\n\n        Parameters\n        ----------\n        estimation : 1D arrary\n            Either prior of posterior estimation\n\n        widths : 2D array, in shape [K, 1]\n            Estimation on widths\n\n        \"\"\"\n        estimation[self.map_offset[1]:self.map_offset[2]] = widths.ravel()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_centers_mean_cov(self, estimation, centers_mean_cov):\n        estimation[self.map_offset[2]:self.map_offset[3]] =\\\n            centers_mean_cov.ravel()", "response": "Set estimation on centers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets estimation on centers", "response": "def get_centers(self, estimation):\n        \"\"\"Get estimation on centers\n\n        Parameters\n        ----------\n        estimation : 1D arrary\n            Either prior of posterior estimation\n\n\n        Returns\n        -------\n        centers : 2D array, in shape [K, n_dim]\n            Estimation on centers\n\n        \"\"\"\n        centers = estimation[0:self.map_offset[1]]\\\n            .reshape(self.K, self.n_dim)\n        return centers"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_widths(self, estimation):\n        widths = estimation[self.map_offset[1]:self.map_offset[2]]\\\n            .reshape(self.K, 1)\n        return widths", "response": "Get estimation on widths\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget estimation on the covariance of centers mean covariance of centers", "response": "def get_centers_mean_cov(self, estimation):\n        \"\"\"Get estimation on the covariance of centers' mean\n\n\n        Parameters\n        ----------\n        estimation : 1D arrary\n             Either prior of posterior estimation\n\n\n        Returns\n        -------\n\n        centers_mean_cov : 2D array, in shape [K, cov_vec_size]\n            Estimation of the covariance of centers' mean\n\n        \"\"\"\n        centers_mean_cov = estimation[self.map_offset[2]:self.map_offset[3]]\\\n            .reshape(self.K, self.cov_vec_size)\n        return centers_mean_cov"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_widths_mean_var(self, estimation):\n        widths_mean_var = \\\n            estimation[self.map_offset[3]:].reshape(self.K, 1)\n        return widths_mean_var", "response": "Get estimation on the variance of widths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_factors(self, unique_R, inds, centers, widths):\n\n        F = np.zeros((len(inds[0]), self.K))\n        tfa_extension.factor(\n            F,\n            centers,\n            widths,\n            unique_R[0],\n            unique_R[1],\n            unique_R[2],\n            inds[0],\n            inds[1],\n            inds[2])\n\n        return F", "response": "Calculate the latent factors for a set of unique values in one dimension of the scanner coordinate matrix R."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate weight matrix based on fMRI data and factors.", "response": "def get_weights(self, data, F):\n        \"\"\"Calculate weight matrix based on fMRI data and factors\n\n        Parameters\n        ----------\n\n        data : 2D array, with shape [n_voxel, n_tr]\n            fMRI data from one subject\n\n        F : 2D array, with shape [n_voxel,self.K]\n            The latent factors from fMRI data.\n\n\n        Returns\n        -------\n\n        W : 2D array, with shape [K, n_tr]\n            The weight matrix from fMRI data.\n\n        \"\"\"\n\n        beta = np.var(data)\n        trans_F = F.T.copy()\n        W = np.zeros((self.K, data.shape[1]))\n        if self.weight_method == 'rr':\n            W = np.linalg.solve(trans_F.dot(F) + beta * np.identity(self.K),\n                                trans_F.dot(data))\n        else:\n            W = np.linalg.solve(trans_F.dot(F), trans_F.dot(data))\n        return W"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_max_sigma(self, R):\n\n        max_sigma = 2.0 * math.pow(np.nanmax(np.std(R, axis=0)), 2)\n        return max_sigma", "response": "Calculate the maximum sigma of scanner RAS coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_bounds(self, R):\n\n        max_sigma = self._get_max_sigma(R)\n        final_lower = np.zeros(self.K * (self.n_dim + 1))\n        final_lower[0:self.K * self.n_dim] =\\\n            np.tile(np.nanmin(R, axis=0), self.K)\n        final_lower[self.K * self.n_dim:] =\\\n            np.repeat(self.lower_ratio * max_sigma, self.K)\n        final_upper = np.zeros(self.K * (self.n_dim + 1))\n        final_upper[0:self.K * self.n_dim] =\\\n            np.tile(np.nanmax(R, axis=0), self.K)\n        final_upper[self.K * self.n_dim:] =\\\n            np.repeat(self.upper_ratio * max_sigma, self.K)\n        bounds = (final_lower, final_upper)\n        return bounds", "response": "Calculate the lower and upper bounds for the centers and widths of a set of FMRIs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _residual_multivariate(\n            self,\n            estimate,\n            unique_R,\n            inds,\n            X,\n            W,\n            template_centers,\n            template_centers_mean_cov,\n            template_widths,\n            template_widths_mean_var_reci,\n            data_sigma):\n        \"\"\"Residual function for estimating centers and widths\n\n        Parameters\n        ----------\n\n        estimate : 1D array\n            Initial estimation on centers\n\n        unique_R : a list of array,\n            Each element contains unique value in one dimension of\n            coordinate matrix R.\n\n        inds : a list of array,\n            Each element contains the indices to reconstruct one\n            dimension of original cooridnate matrix from the unique\n            array.\n\n        X : 2D array, with shape [n_voxel, n_tr]\n            fMRI data from one subject.\n\n        W : 2D array, with shape [K, n_tr]\n            The weight matrix.\n\n        template_centers: 2D array, with shape [K, n_dim]\n            The template prior on centers\n\n        template_centers_mean_cov: 2D array, with shape [K, cov_size]\n            The template prior on covariance of centers' mean\n\n        template_widths: 1D array\n            The template prior on widths\n\n        template_widths_mean_var_reci: 1D array\n            The reciprocal of template prior on variance of widths' mean\n\n        data_sigma: float\n            The variance of X.\n\n\n        Returns\n        -------\n\n        final_err : 1D array\n            The residual function for estimating centers.\n\n        \"\"\"\n\n        centers = self.get_centers(estimate)\n        widths = self.get_widths(estimate)\n        recon = X.size\n        other_err = 0 if template_centers is None else (2 * self.K)\n        final_err = np.zeros(recon + other_err)\n        F = self.get_factors(unique_R, inds, centers, widths)\n        sigma = np.zeros((1,))\n        sigma[0] = data_sigma\n        tfa_extension.recon(final_err[0:recon], X, F, W, sigma)\n\n        if other_err > 0:\n            # center error\n            for k in np.arange(self.K):\n                diff = (centers[k] - template_centers[k])\n                cov = from_tri_2_sym(template_centers_mean_cov[k], self.n_dim)\n                final_err[recon + k] = math.sqrt(\n                    self.sample_scaling *\n                    diff.dot(np.linalg.solve(cov, diff.T)))\n\n            # width error\n            base = recon + self.K\n            dist = template_widths_mean_var_reci *\\\n                (widths - template_widths) ** 2\n            final_err[base:] = np.sqrt(self.sample_scaling * dist).ravel()\n\n        return final_err", "response": "Residual function for estimating centers and widths."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nestimates the centers and widths of a unique cooridnate matrix.", "response": "def _estimate_centers_widths(\n            self,\n            unique_R,\n            inds,\n            X,\n            W,\n            init_centers,\n            init_widths,\n            template_centers,\n            template_widths,\n            template_centers_mean_cov,\n            template_widths_mean_var_reci):\n        \"\"\"Estimate centers and widths\n\n        Parameters\n        ----------\n\n        unique_R : a list of array,\n            Each element contains unique value in one dimension of\n            coordinate matrix R.\n\n        inds : a list of array,\n            Each element contains the indices to reconstruct one\n            dimension of original cooridnate matrix from the unique\n            array.\n\n        X : 2D array, with shape [n_voxel, n_tr]\n            fMRI data from one subject.\n\n        W : 2D array, with shape [K, n_tr]\n            The weight matrix.\n\n        init_centers : 2D array, with shape [K, n_dim]\n            The initial values of centers.\n\n        init_widths : 1D array\n            The initial values of widths.\n\n        template_centers: 1D array\n            The template prior on centers\n\n        template_widths: 1D array\n            The template prior on widths\n\n        template_centers_mean_cov: 2D array, with shape [K, cov_size]\n            The template prior on centers' mean\n\n        template_widths_mean_var_reci: 1D array\n            The reciprocal of template prior on variance of widths' mean\n\n\n        Returns\n        -------\n\n        final_estimate.x: 1D array\n            The newly estimated centers and widths.\n\n        final_estimate.cost: float\n            The cost value.\n\n        \"\"\"\n        # least_squares only accept x in 1D format\n        init_estimate = np.hstack(\n            (init_centers.ravel(), init_widths.ravel()))  # .copy()\n        data_sigma = 1.0 / math.sqrt(2.0) * np.std(X)\n        final_estimate = least_squares(\n            self._residual_multivariate,\n            init_estimate,\n            args=(\n                unique_R,\n                inds,\n                X,\n                W,\n                template_centers,\n                template_widths,\n                template_centers_mean_cov,\n                template_widths_mean_var_reci,\n                data_sigma),\n            method=self.nlss_method,\n            loss=self.nlss_loss,\n            bounds=self.bounds,\n            verbose=0,\n            x_scale=self.x_scale,\n            tr_solver=self.tr_solver)\n        return final_estimate.x, final_estimate.cost"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fit_tfa(self, data, R, template_prior=None):\n        if template_prior is None:\n            template_centers = None\n            template_widths = None\n            template_centers_mean_cov = None\n            template_widths_mean_var_reci = None\n        else:\n            template_centers = self.get_centers(template_prior)\n            template_widths = self.get_widths(template_prior)\n            template_centers_mean_cov =\\\n                self.get_centers_mean_cov(template_prior)\n            template_widths_mean_var_reci = 1.0 /\\\n                self.get_widths_mean_var(template_prior)\n        inner_converged = False\n        np.random.seed(self.seed)\n        n = 0\n        while n < self.miter and not inner_converged:\n            self._fit_tfa_inner(\n                data,\n                R,\n                template_centers,\n                template_widths,\n                template_centers_mean_cov,\n                template_widths_mean_var_reci)\n            self._assign_posterior()\n            inner_converged, _ = self._converged()\n            if not inner_converged:\n                self.local_prior = self.local_posterior_\n            else:\n                logger.info(\"TFA converged at %d iteration.\" % (n))\n            n += 1\n            gc.collect()\n        return self", "response": "This method is used to fit the TFA."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget unique vlaues from coordinate matrix R", "response": "def get_unique_R(self, R):\n        \"\"\"Get unique vlaues from coordinate matrix\n\n        Parameters\n        ----------\n        R :  2D array\n           The coordinate matrix of a subject's fMRI data\n\n        Return\n        ------\n\n        unique_R : a list of array,\n            Each element contains unique value in one dimension of\n            coordinate matrix R.\n\n        inds : a list of array,\n            Each element contains the indices to reconstruct one\n            dimension of original cooridnate matrix from the unique\n            array.\n\n        \"\"\"\n        unique_R = []\n        inds = []\n        for d in np.arange(self.n_dim):\n            tmp_unique, tmp_inds = np.unique(R[:, d], return_inverse=True)\n            unique_R.append(tmp_unique)\n            inds.append(tmp_inds)\n        return unique_R, inds"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fit_tfa_inner(\n            self,\n            data,\n            R,\n            template_centers,\n            template_widths,\n            template_centers_mean_cov,\n            template_widths_mean_var_reci):\n        \"\"\"Fit TFA model, the inner loop part\n\n        Parameters\n        ----------\n\n        data: 2D array, in shape [n_voxel, n_tr]\n            The fMRI data of a subject\n\n        R : 2D array, in shape [n_voxel, n_dim]\n            The voxel coordinate matrix of fMRI data\n\n        template_centers: 1D array\n            The template prior on centers\n\n        template_widths: 1D array\n            The template prior on widths\n\n        template_centers_mean_cov: 2D array, with shape [K, cov_size]\n            The template prior on covariance of centers' mean\n\n        template_widths_mean_var_reci: 1D array\n            The reciprocal of template prior on variance of widths' mean\n\n\n        Returns\n        -------\n        TFA\n            Returns the instance itself.\n\n        \"\"\"\n        nfeature = data.shape[0]\n        nsample = data.shape[1]\n        feature_indices =\\\n            np.random.choice(nfeature, self.max_num_voxel, replace=False)\n        sample_features = np.zeros(nfeature).astype(bool)\n        sample_features[feature_indices] = True\n        samples_indices =\\\n            np.random.choice(nsample, self.max_num_tr, replace=False)\n        curr_data = np.zeros((self.max_num_voxel, self.max_num_tr))\\\n            .astype(float)\n        curr_data = data[feature_indices]\n        curr_data = curr_data[:, samples_indices].copy()\n        curr_R = R[feature_indices].copy()\n        centers = self.get_centers(self.local_prior)\n        widths = self.get_widths(self.local_prior)\n        unique_R, inds = self.get_unique_R(curr_R)\n        F = self.get_factors(unique_R, inds, centers, widths)\n        W = self.get_weights(curr_data, F)\n        self.local_posterior_, self.total_cost = self._estimate_centers_widths(\n            unique_R, inds, curr_data, W, centers, widths,\n            template_centers, template_centers_mean_cov,\n            template_widths, template_widths_mean_var_reci)\n\n        return self", "response": "Fit TFA model the inner loop part of the TFA model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the Topographical Factor Analysis on the TFA.", "response": "def fit(self, X, R, template_prior=None):\n        \"\"\" Topographical Factor Analysis (TFA)[Manning2014]\n\n        Parameters\n        ----------\n        X : 2D array, in shape [n_voxel, n_sample]\n            The fMRI data of one subject\n\n        R : 2D array, in shape [n_voxel, n_dim]\n            The voxel coordinate matrix of fMRI data\n\n        template_prior : None or 1D array\n            The template prior as an extra constraint\n            None when fitting TFA alone\n        \"\"\"\n        if self.verbose:\n            logger.info('Start to fit TFA ')\n\n        if not isinstance(X, np.ndarray):\n            raise TypeError(\"Input data should be an array\")\n        if X.ndim != 2:\n            raise TypeError(\"Input data should be 2D array\")\n        if not isinstance(R, np.ndarray):\n            raise TypeError(\"Input coordinate matrix should be an array\")\n        if R.ndim != 2:\n            raise TypeError(\"Input coordinate matrix should be 2D array\")\n        if X.shape[0] != R.shape[0]:\n            raise TypeError(\n                \"The number of voxels should be the same in X and R!\")\n        if self.weight_method != 'rr' and self.weight_method != 'ols':\n            raise ValueError(\n                \"only 'rr' and 'ols' are accepted as weight_method!\")\n\n        # main algorithm\n        self.n_dim = R.shape[1]\n        self.cov_vec_size = np.sum(np.arange(self.n_dim) + 1)\n        self.map_offset = self.get_map_offset()\n        self.bounds = self.get_bounds(R)\n        n_voxel = X.shape[0]\n        n_tr = X.shape[1]\n        self.sample_scaling = 0.5 * float(\n            self.max_num_voxel * self.max_num_tr) / float(n_voxel * n_tr)\n        if template_prior is None:\n            self.init_prior(R)\n        else:\n            self.local_prior = template_prior[0: self.map_offset[2]]\n        self._fit_tfa(X, R, template_prior)\n        if template_prior is None:\n            centers = self.get_centers(self.local_posterior_)\n            widths = self.get_widths(self.local_posterior_)\n            unique_R, inds = self.get_unique_R(R)\n            self.F_ = self.get_factors(unique_R, inds, centers, widths)\n            self.W_ = self.get_weights(X, self.F_)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recon_err(data, F, W):\n\n    recon = F.dot(W).ravel()\n    err = mean_squared_error(\n        data.ravel(),\n        recon,\n        multioutput='uniform_average')\n    return math.sqrt(err)", "response": "Calcuate reconstruction error\n\n    Parameters\n    ----------\n\n    data : 2D array \n        True data to recover.\n\n    F : 2D array \n        HTFA factor matrix.\n\n    W : 2D array \n        HTFA weight matrix.\n   \n\n    Returns\n    -------\n    float \n        Returns root mean squared reconstruction error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sfn(l, mask, myrad, bcast_var):\n    clf = bcast_var[2]\n    data = l[0][mask, :].T\n    # print(l[0].shape, mask.shape, data.shape)\n    skf = model_selection.StratifiedKFold(n_splits=bcast_var[1],\n                                          shuffle=False)\n    accuracy = np.mean(model_selection.cross_val_score(clf, data,\n                                                       y=bcast_var[0],\n                                                       cv=skf,\n                                                       n_jobs=1))\n    return accuracy", "response": "Score classifier on searchlight data using cross - validation."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(self, clf):\n        rank = MPI.COMM_WORLD.Get_rank()\n        if rank == 0:\n            logger.info(\n                'running activity-based voxel selection via Searchlight'\n            )\n        self.sl.distribute([self.data], self.mask)\n        self.sl.broadcast((self.labels, self.num_folds, clf))\n        if rank == 0:\n            logger.info(\n                'data preparation done'\n            )\n\n        # obtain a 3D array with accuracy numbers\n        result_volume = self.sl.run_searchlight(_sfn)\n        # get result tuple list from the volume\n        result_list = result_volume[self.mask]\n        results = []\n        if rank == 0:\n            for idx, value in enumerate(result_list):\n                if value is None:\n                    value = 0\n                results.append((idx, value))\n            # Sort the voxels\n            results.sort(key=lambda tup: tup[1], reverse=True)\n            logger.info(\n                'activity-based voxel selection via Searchlight is done'\n            )\n        return result_volume, results", "response": "run the activity - based voxel selection via Searchlight"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nscores classifier on data using cross validation.", "response": "def _cross_validation_for_one_voxel(clf, vid, num_folds, subject_data, labels):\n    \"\"\"Score classifier on data using cross validation.\"\"\"\n    # no shuffling in cv\n    skf = model_selection.StratifiedKFold(n_splits=num_folds,\n                                          shuffle=False)\n    scores = model_selection.cross_val_score(clf, subject_data,\n                                             y=labels,\n                                             cv=skf, n_jobs=1)\n    logger.debug(\n        'cross validation for voxel %d is done' %\n        vid\n    )\n    return (vid, scores.mean())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self, clf):\n        rank = MPI.COMM_WORLD.Get_rank()\n        if rank == self.master_rank:\n            results = self._master()\n            # Sort the voxels\n            results.sort(key=lambda tup: tup[1], reverse=True)\n        else:\n            self._worker(clf)\n            results = []\n        return results", "response": "Run correlation - based voxel selection in master - worker model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmaster node's operation. Assigning tasks to workers and collecting results from them Parameters ---------- None Returns ------- results: list of tuple (voxel_id, accuracy) the accuracy numbers of all voxels, in accuracy descending order the length of array equals the number of voxels", "response": "def _master(self):\n        \"\"\"Master node's operation.\n\n        Assigning tasks to workers and collecting results from them\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        results: list of tuple (voxel_id, accuracy)\n            the accuracy numbers of all voxels, in accuracy descending order\n            the length of array equals the number of voxels\n        \"\"\"\n        logger.info(\n            'Master at rank %d starts to allocate tasks',\n            MPI.COMM_WORLD.Get_rank()\n        )\n        results = []\n        comm = MPI.COMM_WORLD\n        size = comm.Get_size()\n        sending_voxels = self.voxel_unit if self.voxel_unit < self.num_voxels \\\n            else self.num_voxels\n        current_task = (0, sending_voxels)\n        status = MPI.Status()\n        # using_size is used when the number of tasks\n        # is smaller than the number of workers\n        using_size = size\n        for i in range(0, size):\n            if i == self.master_rank:\n                continue\n            if current_task[1] == 0:\n                using_size = i\n                break\n            logger.debug(\n                'master starts to send a task to worker %d' %\n                i\n            )\n            comm.send(current_task,\n                      dest=i,\n                      tag=self._WORKTAG)\n            next_start = current_task[0] + current_task[1]\n            sending_voxels = self.voxel_unit \\\n                if self.voxel_unit < self.num_voxels - next_start \\\n                else self.num_voxels - next_start\n            current_task = (next_start, sending_voxels)\n\n        while using_size == size:\n            if current_task[1] == 0:\n                break\n            result = comm.recv(source=MPI.ANY_SOURCE,\n                               tag=MPI.ANY_TAG,\n                               status=status)\n            results += result\n            comm.send(current_task,\n                      dest=status.Get_source(),\n                      tag=self._WORKTAG)\n            next_start = current_task[0] + current_task[1]\n            sending_voxels = self.voxel_unit \\\n                if self.voxel_unit < self.num_voxels - next_start \\\n                else self.num_voxels - next_start\n            current_task = (next_start, sending_voxels)\n\n        for i in range(0, using_size):\n            if i == self.master_rank:\n                continue\n            result = comm.recv(source=MPI.ANY_SOURCE, tag=MPI.ANY_TAG)\n            results += result\n\n        for i in range(0, size):\n            if i == self.master_rank:\n                continue\n            comm.send(None,\n                      dest=i,\n                      tag=self._TERMINATETAG)\n\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _worker(self, clf):\n        logger.debug(\n            'worker %d is running, waiting for tasks from master at rank %d' %\n            (MPI.COMM_WORLD.Get_rank(), self.master_rank)\n        )\n        comm = MPI.COMM_WORLD\n        status = MPI.Status()\n        while 1:\n            task = comm.recv(source=self.master_rank,\n                             tag=MPI.ANY_TAG,\n                             status=status)\n            if status.Get_tag():\n                break\n            comm.send(self._voxel_scoring(task, clf),\n                      dest=self.master_rank)", "response": "Worker node s operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _correlation_computation(self, task):\n        time1 = time.time()\n        s = task[0]\n        nEpochs = len(self.raw_data)\n        logger.debug(\n            'start to compute the correlation: #epochs: %d, '\n            '#processed voxels: %d, #total voxels to compute against: %d' %\n            (nEpochs, task[1], self.num_voxels2)\n        )\n        corr = np.zeros((task[1], nEpochs, self.num_voxels2),\n                        np.float32, order='C')\n        count = 0\n        for i in range(len(self.raw_data)):\n            mat = self.raw_data[i]\n            mat2 = self.raw_data2[i] if self.raw_data2 is not None else mat\n            no_trans = 'N'\n            trans = 'T'\n            blas.compute_self_corr_for_voxel_sel(no_trans, trans,\n                                                 self.num_voxels2, task[1],\n                                                 mat.shape[0], 1.0,\n                                                 mat2, self.num_voxels2,\n                                                 s, mat, self.num_voxels,\n                                                 0.0, corr,\n                                                 self.num_voxels2 * nEpochs,\n                                                 count)\n            count += 1\n        time2 = time.time()\n        logger.debug(\n            'correlation computation for %d voxels, takes %.2f s' %\n            (task[1], (time2 - time1))\n        )\n        return corr", "response": "Compute the correlation matrix for the current task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _correlation_normalization(self, corr):\n        time1 = time.time()\n        (sv, e, av) = corr.shape\n        for i in range(sv):\n            start = 0\n            while start < e:\n                cur_val = corr[i, start: start + self.epochs_per_subj, :]\n                cur_val = .5 * np.log((cur_val + 1) / (1 - cur_val))\n                corr[i, start: start + self.epochs_per_subj, :] = \\\n                    zscore(cur_val, axis=0, ddof=0)\n                start += self.epochs_per_subj\n        # if zscore fails (standard deviation is zero),\n        # set all values to be zero\n        corr = np.nan_to_num(corr)\n        time2 = time.time()\n        logger.debug(\n            'within-subject normalization for %d voxels '\n            'using numpy zscore function, takes %.2f s' %\n            (sv, (time2 - time1))\n        )\n        return corr", "response": "This method does within - subject normalization."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares data for voxelwise cross validation.", "response": "def _prepare_for_cross_validation(self, corr, clf):\n        \"\"\"Prepare data for voxelwise cross validation.\n\n        If the classifier is sklearn.svm.SVC with precomputed kernel,\n        the kernel matrix of each voxel is computed, otherwise do nothing.\n\n        Parameters\n        ----------\n        corr: 3D array in shape [num_processed_voxels, num_epochs, num_voxels]\n            the normalized correlation values of all subjects in all epochs\n            for the assigned values, in row-major\n        clf: classification function\n            the classifier to be used in cross validation\n\n        Returns\n        -------\n        data: 3D numpy array\n            If using sklearn.svm.SVC with precomputed kernel,\n            it is in shape [num_processed_voxels, num_epochs, num_epochs];\n            otherwise it is the input argument corr,\n            in shape [num_processed_voxels, num_epochs, num_voxels]\n        \"\"\"\n        time1 = time.time()\n        (num_processed_voxels, num_epochs, _) = corr.shape\n        if isinstance(clf, sklearn.svm.SVC) and clf.kernel == 'precomputed':\n            # kernel matrices should be computed\n            kernel_matrices = np.zeros((num_processed_voxels, num_epochs,\n                                        num_epochs),\n                                       np.float32, order='C')\n            for i in range(num_processed_voxels):\n                blas.compute_kernel_matrix('L', 'T',\n                                           num_epochs, self.num_voxels2,\n                                           1.0, corr,\n                                           i, self.num_voxels2,\n                                           0.0, kernel_matrices[i, :, :],\n                                           num_epochs)\n                # shrink the values for getting more stable alpha values\n                # in SVM training iteration\n                num_digits = len(str(int(kernel_matrices[i, 0, 0])))\n                if num_digits > 2:\n                    proportion = 10**(2-num_digits)\n                    kernel_matrices[i, :, :] *= proportion\n            data = kernel_matrices\n        else:\n            data = corr\n        time2 = time.time()\n        logger.debug(\n            'cross validation data preparation takes %.2f s' %\n            (time2 - time1)\n        )\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_cross_validation(self, clf, data, task):\n        time1 = time.time()\n\n        if isinstance(clf, sklearn.svm.SVC) and clf.kernel == 'precomputed'\\\n                and self.use_multiprocessing:\n            inlist = [(clf, i + task[0], self.num_folds, data[i, :, :],\n                       self.labels) for i in range(task[1])]\n\n            with multiprocessing.Pool(self.process_num) as pool:\n                results = list(pool.starmap(_cross_validation_for_one_voxel,\n                                            inlist))\n        else:\n            results = []\n            for i in range(task[1]):\n                result = _cross_validation_for_one_voxel(clf, i + task[0],\n                                                         self.num_folds,\n                                                         data[i, :, :],\n                                                         self.labels)\n                results.append(result)\n        time2 = time.time()\n        logger.debug(\n            'cross validation for %d voxels, takes %.2f s' %\n            (task[1], (time2 - time1))\n        )\n        return results", "response": "Perform voxelwise cross validation for one classifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the Semi - Supervised Shared Response Model for the given set of data.", "response": "def fit(self, X, y, Z):\n        \"\"\"Compute the Semi-Supervised Shared Response Model\n\n        Parameters\n        ----------\n\n        X : list of 2D arrays, element i has shape=[voxels_i, n_align]\n            Each element in the list contains the fMRI data for alignment of\n            one subject. There are n_align samples for each subject.\n\n        y : list of arrays of int, element i has shape=[samples_i]\n            Each element in the list contains the labels for the data samples\n            in Z.\n\n        Z : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject\n            for training the MLR classifier.\n\n        \"\"\"\n        logger.info('Starting SS-SRM')\n\n        # Check that the alpha value is in range (0.0,1.0)\n        if 0.0 >= self.alpha or self.alpha >= 1.0:\n            raise ValueError(\"Alpha parameter should be in range (0.0, 1.0)\")\n\n        # Check that the regularizer value is positive\n        if 0.0 >= self.gamma:\n            raise ValueError(\"Gamma parameter should be positive.\")\n\n        # Check the number of subjects\n        if len(X) <= 1 or len(y) <= 1 or len(Z) <= 1:\n            raise ValueError(\"There are not enough subjects in the input \"\n                             \"data to train the model.\")\n\n        if not (len(X) == len(y)) or not (len(X) == len(Z)):\n            raise ValueError(\"Different number of subjects in data.\")\n\n        # Check for input data sizes\n        if X[0].shape[1] < self.features:\n            raise ValueError(\n                \"There are not enough samples to train the model with \"\n                \"{0:d} features.\".format(self.features))\n\n        # Check if all subjects have same number of TRs for alignment\n        # and if alignment and classification data have the same number of\n        # voxels per subject. Also check that there labels for all the classif.\n        # sample\n        number_trs = X[0].shape[1]\n        number_subjects = len(X)\n        for subject in range(number_subjects):\n            assert_all_finite(X[subject])\n            assert_all_finite(Z[subject])\n            if X[subject].shape[1] != number_trs:\n                raise ValueError(\"Different number of alignment samples \"\n                                 \"between subjects.\")\n            if X[subject].shape[0] != Z[subject].shape[0]:\n                raise ValueError(\"Different number of voxels between alignment\"\n                                 \" and classification data (subject {0:d})\"\n                                 \".\".format(subject))\n            if Z[subject].shape[1] != y[subject].size:\n                raise ValueError(\"Different number of samples and labels in \"\n                                 \"subject {0:d}.\".format(subject))\n\n        # Map the classes to [0..C-1]\n        new_y = self._init_classes(y)\n\n        # Run SS-SRM\n        self.w_, self.s_, self.theta_, self.bias_ = self._sssrm(X, Z, new_y)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_classes(self, y):\n        self.classes_ = unique_labels(utils.concatenate_not_none(y))\n        new_y = [None] * len(y)\n        for s in range(len(y)):\n            new_y[s] = np.digitize(y[s], self.classes_) - 1\n        return new_y", "response": "Map all possible classes to the range [ 0.. C - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict(self, X):\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of subjects\n        if len(X) != len(self.w_):\n            raise ValueError(\"The number of subjects does not match the one\"\n                             \" in the model.\")\n\n        X_shared = self.transform(X)\n        p = [None] * len(X_shared)\n        for subject in range(len(X_shared)):\n            sumexp, _, exponents = utils.sumexp_stable(\n                self.theta_.T.dot(X_shared[subject]) + self.bias_)\n            p[subject] = self.classes_[\n                (exponents / sumexp[np.newaxis, :]).argmax(axis=0)]\n\n        return p", "response": "Classify the output for given data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sssrm(self, data_align, data_sup, labels):\n        classes = self.classes_.size\n\n        # Initialization:\n        self.random_state_ = np.random.RandomState(self.rand_seed)\n        random_states = [\n            np.random.RandomState(self.random_state_.randint(2**32))\n            for i in range(len(data_align))]\n        # Set Wi's to a random orthogonal voxels by TRs\n        w, _ = srm._init_w_transforms(data_align, self.features, random_states)\n\n        # Initialize the shared response S\n        s = SSSRM._compute_shared_response(data_align, w)\n\n        # Initialize theta and bias\n        theta, bias = self._update_classifier(data_sup, labels, w, classes)\n\n        # calculate and print the objective function\n        if logger.isEnabledFor(logging.INFO):\n            objective = self._objective_function(data_align, data_sup, labels,\n                                                 w, s, theta, bias)\n            logger.info('Objective function %f' % objective)\n\n        # Main loop:\n        for iteration in range(self.n_iter):\n            logger.info('Iteration %d' % (iteration + 1))\n\n            # Update the mappings Wi\n            w = self._update_w(data_align, data_sup, labels, w, s, theta, bias)\n\n            # Output the objective function\n            if logger.isEnabledFor(logging.INFO):\n                objective = self._objective_function(data_align, data_sup,\n                                                     labels, w, s, theta, bias)\n                logger.info('Objective function after updating Wi  %f'\n                            % objective)\n\n            # Update the shared response S\n            s = SSSRM._compute_shared_response(data_align, w)\n\n            # Output the objective function\n            if logger.isEnabledFor(logging.INFO):\n                objective = self._objective_function(data_align, data_sup,\n                                                     labels, w, s, theta, bias)\n                logger.info('Objective function after updating S   %f'\n                            % objective)\n\n            # Update the MLR classifier, theta and bias\n            theta, bias = self._update_classifier(data_sup, labels, w, classes)\n\n            # Output the objective function\n            if logger.isEnabledFor(logging.INFO):\n                objective = self._objective_function(data_align, data_sup,\n                                                     labels, w, s, theta, bias)\n                logger.info('Objective function after updating MLR %f'\n                            % objective)\n\n        return w, s, theta, bias", "response": "Private method for fitting the SS - SRM."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_classifier(self, data, labels, w, classes):\n\n        # Stack the data and labels for training the classifier\n        data_stacked, labels_stacked, weights = \\\n            SSSRM._stack_list(data, labels, w)\n\n        features = w[0].shape[1]\n        total_samples = weights.size\n\n        data_th = S.shared(data_stacked.astype(theano.config.floatX))\n        val_ = S.shared(labels_stacked)\n        total_samples_S = S.shared(total_samples)\n        theta_th = T.matrix(name='theta', dtype=theano.config.floatX)\n        bias_th = T.col(name='bias', dtype=theano.config.floatX)\n        constf2 = S.shared(self.alpha / self.gamma, allow_downcast=True)\n        weights_th = S.shared(weights)\n\n        log_p_y_given_x = \\\n            T.log(T.nnet.softmax((theta_th.T.dot(data_th.T)).T + bias_th.T))\n        f = -constf2 * T.sum((log_p_y_given_x[T.arange(total_samples_S), val_])\n                             / weights_th) + 0.5 * T.sum(theta_th ** 2)\n\n        manifold = Product((Euclidean(features, classes),\n                            Euclidean(classes, 1)))\n        problem = Problem(manifold=manifold, cost=f, arg=[theta_th, bias_th],\n                          verbosity=0)\n        solver = ConjugateGradient(mingradnorm=1e-6)\n        solution = solver.solve(problem)\n        theta = solution[0]\n        bias = solution[1]\n\n        del constf2\n        del theta_th\n        del bias_th\n        del data_th\n        del val_\n        del solver\n        del solution\n\n        return theta, bias", "response": "Update the classifier parameters theta and bias."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_shared_response(data, w):\n        s = np.zeros((w[0].shape[1], data[0].shape[1]))\n        for m in range(len(w)):\n            s = s + w[m].T.dot(data[m])\n        s /= len(w)\n        return s", "response": "Compute the shared response for the set of subjects data and the orthogonal transforms of each subject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the objective function of the Semi - Supervised SRM and MLR.", "response": "def _objective_function(self, data_align, data_sup, labels, w, s, theta,\n                            bias):\n        \"\"\"Compute the objective function of the Semi-Supervised SRM\n\n        See :eq:`sssrm-eq`.\n\n        Parameters\n        ----------\n\n        data_align : list of 2D arrays, element i has shape=[voxels_i, n_align]\n            Each element in the list contains the fMRI data for alignment of\n            one subject. There are n_align samples for each subject.\n\n        data_sup : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject for\n            the classification task.\n\n        labels : list of arrays of int, element i has shape=[samples_i]\n            Each element in the list contains the labels for the data samples\n            in data_sup.\n\n        w : list of array, element i has shape=[voxels_i, features]\n            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n\n        s : array, shape=[features, samples]\n            The shared response.\n\n        theta : array, shape=[classes, features]\n            The MLR class plane parameters.\n\n        bias : array, shape=[classes]\n            The MLR class biases.\n\n\n        Returns\n        -------\n\n        f_val : float\n            The SS-SRM objective function evaluated based on the parameters to\n            this function.\n        \"\"\"\n        subjects = len(data_align)\n\n        # Compute the SRM  loss\n        f_val = 0.0\n        for subject in range(subjects):\n            samples = data_align[subject].shape[1]\n            f_val += (1 - self.alpha) * (0.5 / samples) \\\n                * np.linalg.norm(data_align[subject] - w[subject].dot(s),\n                                 'fro')**2\n\n        # Compute the MLR loss\n        f_val += self._loss_lr(data_sup, labels, w, theta, bias)\n\n        return f_val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _objective_function_subject(self, data_align, data_sup, labels, w, s,\n                                    theta, bias):\n        \"\"\"Compute the objective function for one subject.\n\n        .. math:: (1-C)*Loss_{SRM}_i(W_i,S;X_i)\n        .. math:: + C/\\\\gamma * Loss_{MLR_i}(\\\\theta, bias; {(W_i^T*Z_i, y_i})\n        .. math:: + R(\\\\theta)\n\n        Parameters\n        ----------\n\n        data_align : 2D array, shape=[voxels_i, samples_align]\n            Contains the fMRI data for alignment of subject i.\n\n        data_sup : 2D array, shape=[voxels_i, samples_i]\n            Contains the fMRI data of one subject for the classification task.\n\n        labels : array of int, shape=[samples_i]\n            The labels for the data samples in data_sup.\n\n        w : array, shape=[voxels_i, features]\n            The orthogonal transform (mapping) :math:`W_i` for subject i.\n\n        s : array, shape=[features, samples]\n            The shared response.\n\n        theta : array, shape=[classes, features]\n            The MLR class plane parameters.\n\n        bias : array, shape=[classes]\n            The MLR class biases.\n\n\n        Returns\n        -------\n\n        f_val : float\n            The SS-SRM objective function for subject i evaluated on the\n            parameters to this function.\n        \"\"\"\n        # Compute the SRM  loss\n        f_val = 0.0\n        samples = data_align.shape[1]\n        f_val += (1 - self.alpha) * (0.5 / samples) \\\n            * np.linalg.norm(data_align - w.dot(s), 'fro')**2\n\n        # Compute the MLR loss\n        f_val += self._loss_lr_subject(data_sup, labels, w, theta, bias)\n\n        return f_val", "response": "Compute the objective function for one subject."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the Loss MLR for a single subject.", "response": "def _loss_lr_subject(self, data, labels, w, theta, bias):\n        \"\"\"Compute the Loss MLR for a single subject (without regularization)\n\n        Parameters\n        ----------\n\n        data : array, shape=[voxels, samples]\n            The fMRI data of subject i for the classification task.\n\n        labels : array of int, shape=[samples]\n            The labels for the data samples in data.\n\n        w : array, shape=[voxels, features]\n            The orthogonal transform (mapping) :math:`W_i` for subject i.\n\n        theta : array, shape=[classes, features]\n            The MLR class plane parameters.\n\n        bias : array, shape=[classes]\n            The MLR class biases.\n\n        Returns\n        -------\n\n        loss : float\n            The loss MLR for the subject\n        \"\"\"\n        if data is None:\n            return 0.0\n\n        samples = data.shape[1]\n\n        thetaT_wi_zi_plus_bias = theta.T.dot(w.T.dot(data)) + bias\n        sum_exp, max_value, _ = utils.sumexp_stable(thetaT_wi_zi_plus_bias)\n        sum_exp_values = np.log(sum_exp) + max_value\n\n        aux = 0.0\n        for sample in range(samples):\n            label = labels[sample]\n            aux += thetaT_wi_zi_plus_bias[label, sample]\n        return self.alpha / samples / self.gamma * (sum_exp_values.sum() - aux)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the Loss MLR for the SS - SRM model.", "response": "def _loss_lr(self, data, labels, w, theta, bias):\n        \"\"\"Compute the Loss MLR (with the regularization)\n\n        Parameters\n        ----------\n\n        data : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject for\n            the classification task.\n\n        labels : list of arrays of int, element i has shape=[samples_i]\n            Each element in the list contains the labels for the samples in\n            data.\n\n        w : list of array, element i has shape=[voxels_i, features]\n            The orthogonal transforms (mappings) :math:`W_i` for each subject.\n\n        theta : array, shape=[classes, features]\n            The MLR class plane parameters.\n\n        bias : array, shape=[classes]\n            The MLR class biases.\n\n\n        Returns\n        -------\n\n        loss : float\n            The loss MLR for the SS-SRM model\n        \"\"\"\n        subjects = len(data)\n        loss = 0.0\n        for subject in range(subjects):\n            if labels[subject] is not None:\n                loss += self._loss_lr_subject(data[subject], labels[subject],\n                                              w[subject], theta, bias)\n\n        return loss + 0.5 * np.linalg.norm(theta, 'fro')**2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _stack_list(data, data_labels, w):\n        labels_stacked = utils.concatenate_not_none(data_labels)\n\n        weights = np.empty((labels_stacked.size,))\n        data_shared = [None] * len(data)\n        curr_samples = 0\n        for s in range(len(data)):\n            if data[s] is not None:\n                subject_samples = data[s].shape[1]\n                curr_samples_end = curr_samples + subject_samples\n                weights[curr_samples:curr_samples_end] = subject_samples\n                data_shared[s] = w[s].T.dot(data[s])\n                curr_samples += data[s].shape[1]\n\n        data_stacked = utils.concatenate_not_none(data_shared, axis=1).T\n        return data_stacked, labels_stacked, weights", "response": "Stack the data in a list of MRI entries into a 2D array."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun searchlight function on block data in parallel.", "response": "def _singlenode_searchlight(l, msk, mysl_rad, bcast_var, extra_params):\n    \"\"\"Run searchlight function on block data in parallel.\n\n    `extra_params` contains:\n\n    - Searchlight function.\n    - `Shape` mask.\n    - Minimum active voxels proportion required to run the searchlight\n      function.\n    \"\"\"\n\n    voxel_fn = extra_params[0]\n    shape_mask = extra_params[1]\n    min_active_voxels_proportion = extra_params[2]\n    outmat = np.empty(msk.shape, dtype=np.object)[mysl_rad:-mysl_rad,\n                                                  mysl_rad:-mysl_rad,\n                                                  mysl_rad:-mysl_rad]\n    for i in range(0, outmat.shape[0]):\n        for j in range(0, outmat.shape[1]):\n            for k in range(0, outmat.shape[2]):\n                if msk[i+mysl_rad, j+mysl_rad, k+mysl_rad]:\n                    searchlight_slice = np.s_[\n                        i:i+2*mysl_rad+1,\n                        j:j+2*mysl_rad+1,\n                        k:k+2*mysl_rad+1]\n                    voxel_fn_mask = msk[searchlight_slice] * shape_mask\n                    if (min_active_voxels_proportion == 0\n                        or np.count_nonzero(voxel_fn_mask) / voxel_fn_mask.size\n                            > min_active_voxels_proportion):\n                        outmat[i, j, k] = voxel_fn(\n                            [ll[searchlight_slice] for ll in l],\n                            msk[searchlight_slice] * shape_mask,\n                            mysl_rad,\n                            bcast_var)\n    return outmat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine on which rank each subject currently resides Returns the list of ranks indicating the owner of each subject", "response": "def _get_ownership(self, data):\n        \"\"\"Determine on which rank each subject currently resides\n\n        Parameters\n        ----------\n\n        data: list of 4D arrays with subject data\n\n        Returns\n        -------\n\n        list of ranks indicating the owner of each subject\n        \"\"\"\n        rank = self.comm.rank\n\n        B = [(rank, idx) for (idx, c) in enumerate(data) if c is not None]\n        C = self.comm.allreduce(B)\n        ownership = [None] * len(data)\n        for c in C:\n            ownership[c[1]] = c[0]\n        return ownership"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndividing the volume into a list of blocks containing the size in voxels in the block.", "response": "def _get_blocks(self, mask):\n        \"\"\"Divide the volume into a set of blocks\n\n        Ignore blocks that have no active voxels in the mask\n\n        Parameters\n        ----------\n\n        mask: a boolean 3D array which is true at every active voxel\n\n        Returns\n        -------\n\n        list of tuples containing block information:\n           - a triple containing top left point of the block and\n           - a triple containing the size in voxels of the block\n\n        \"\"\"\n        blocks = []\n        outerblk = self.max_blk_edge + 2*self.sl_rad\n        for i in range(0, mask.shape[0], self.max_blk_edge):\n            for j in range(0, mask.shape[1], self.max_blk_edge):\n                for k in range(0, mask.shape[2], self.max_blk_edge):\n                    block_shape = mask[i:i+outerblk,\n                                       j:j+outerblk,\n                                       k:k+outerblk\n                                       ].shape\n                    if np.any(\n                        mask[i+self.sl_rad:i+block_shape[0]-self.sl_rad,\n                             j+self.sl_rad:j+block_shape[1]-self.sl_rad,\n                             k+self.sl_rad:k+block_shape[2]-self.sl_rad]):\n                        blocks.append(((i, j, k), block_shape))\n        return blocks"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_block_data(self, mat, block):\n        (pt, sz) = block\n        if len(mat.shape) == 3:\n            return mat[pt[0]:pt[0]+sz[0],\n                       pt[1]:pt[1]+sz[1],\n                       pt[2]:pt[2]+sz[2]].copy()\n        elif len(mat.shape) == 4:\n            return mat[pt[0]:pt[0]+sz[0],\n                       pt[1]:pt[1]+sz[1],\n                       pt[2]:pt[2]+sz[2],\n                       :].copy()", "response": "Retrieve a block from a 3D or 4D volume"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _split_volume(self, mat, blocks):\n        return [self._get_block_data(mat, block) for block in blocks]", "response": "Convert a volume into a list of block data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndistributing a list from one rank to other ranks in a cyclic manner", "response": "def _scatter_list(self, data, owner):\n        \"\"\"Distribute a list from one rank to other ranks in a cyclic manner\n\n        Parameters\n        ----------\n\n        data: list of pickle-able data\n\n        owner: rank that owns the data\n\n        Returns\n        -------\n\n        A list containing the data in a cyclic layout across ranks\n\n        \"\"\"\n\n        rank = self.comm.rank\n        size = self.comm.size\n        subject_submatrices = []\n        nblocks = self.comm.bcast(len(data)\n                                  if rank == owner else None, root=owner)\n\n        # For each submatrix\n        for idx in range(0, nblocks, size):\n            padded = None\n            extra = max(0, idx+size - nblocks)\n\n            # Pad with \"None\" so scatter can go to all processes\n            if data is not None:\n                padded = data[idx:idx+size]\n                if extra > 0:\n                    padded = padded + [None]*extra\n\n            # Scatter submatrices to all processes\n            mytrans = self.comm.scatter(padded, root=owner)\n\n            # Contribute submatrix to subject list\n            if mytrans is not None:\n                subject_submatrices += [mytrans]\n\n        return subject_submatrices"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndistributes data to MPI ranks and mask.", "response": "def distribute(self, subjects, mask):\n        \"\"\"Distribute data to MPI ranks\n\n        Parameters\n        ----------\n\n        subjects : list of 4D arrays containing data for one or more subjects.\n              Each entry of the list must be present on at most one rank,\n              and the other ranks contain a \"None\" at this list location.\n\n              For example, for 3 ranks you may lay out the data in the\n              following manner:\n\n              Rank 0: [Subj0, None, None]\n              Rank 1: [None, Subj1, None]\n              Rank 2: [None, None, Subj2]\n\n              Or alternatively, you may lay out the data in this manner:\n\n              Rank 0: [Subj0, Subj1, Subj2]\n              Rank 1: [None, None, None]\n              Rank 2: [None, None, None]\n\n        mask: 3D array with \"True\" entries at active vertices\n\n        \"\"\"\n        if mask.ndim != 3:\n            raise ValueError('mask should be a 3D array')\n\n        for (idx, subj) in enumerate(subjects):\n            if subj is not None:\n                if subj.ndim != 4:\n                    raise ValueError('subjects[{}] must be 4D'.format(idx))\n\n        self.mask = mask\n        rank = self.comm.rank\n\n        # Get/set ownership\n        ownership = self._get_ownership(subjects)\n        all_blocks = self._get_blocks(mask) if rank == 0 else None\n        all_blocks = self.comm.bcast(all_blocks)\n\n        # Divide data and mask\n        splitsubj = [self._split_volume(s, all_blocks)\n                     if s is not None else None\n                     for s in subjects]\n        submasks = self._split_volume(mask, all_blocks)\n\n        # Scatter points, data, and mask\n        self.blocks = self._scatter_list(all_blocks, 0)\n        self.submasks = self._scatter_list(submasks,  0)\n        self.subproblems = [self._scatter_list(s, ownership[s_idx])\n                            for (s_idx, s) in enumerate(splitsubj)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a function for each block in a volume.", "response": "def run_block_function(self, block_fn, extra_block_fn_params=None,\n                           pool_size=None):\n        \"\"\"Perform a function for each block in a volume.\n\n        Parameters\n        ----------\n\n        block_fn: function to apply to each block:\n\n                Parameters\n\n                data: list of 4D arrays containing subset of subject data,\n                      which is padded with sl_rad voxels.\n\n                mask: 3D array containing subset of mask data\n\n                sl_rad: radius, in voxels, of the sphere inscribed in the\n                cube\n\n                bcast_var: shared data which is broadcast to all processes\n\n                extra_params: extra parameters\n\n\n                Returns\n\n                3D array which is the same size as the mask\n                input with padding removed\n\n        extra_block_fn_params: tuple\n            Extra parameters to pass to the block function\n\n        pool_size: int\n            Maximum number of processes running the block function in parallel.\n            If None, number of available hardware threads, considering cpusets\n            restrictions.\n        \"\"\"\n        rank = self.comm.rank\n\n        results = []\n        usable_cpus = usable_cpu_count()\n        if pool_size is None:\n            processes = usable_cpus\n        else:\n            processes = min(pool_size, usable_cpus)\n\n        if processes > 1:\n            with Pool(processes) as pool:\n                for idx, block in enumerate(self.blocks):\n                    result = pool.apply_async(\n                        block_fn,\n                        ([subproblem[idx] for subproblem in self.subproblems],\n                         self.submasks[idx],\n                         self.sl_rad,\n                         self.bcast_var,\n                         extra_block_fn_params))\n                    results.append((block[0], result))\n                local_outputs = [(result[0], result[1].get())\n                                 for result in results]\n        else:\n            # If we only are using one CPU core, no need to create a Pool,\n            # cause an underlying fork(), and send the data to that process.\n            # Just do it here in serial. This  will save copying the memory\n            # and will stop a fork() which can cause problems in some MPI\n            # implementations.\n            for idx, block in enumerate(self.blocks):\n                subprob_list = [subproblem[idx]\n                                for subproblem in self.subproblems]\n                result = block_fn(\n                            subprob_list,\n                            self.submasks[idx],\n                            self.sl_rad,\n                            self.bcast_var,\n                            extra_block_fn_params)\n                results.append((block[0], result))\n            local_outputs = [(result[0], result[1]) for result in results]\n\n        # Collect results\n        global_outputs = self.comm.gather(local_outputs)\n\n        # Coalesce results\n        outmat = np.empty(self.mask.shape, dtype=np.object)\n        if rank == 0:\n            for go_rank in global_outputs:\n                for (pt, mat) in go_rank:\n                    coords = np.s_[\n                        pt[0]+self.sl_rad:pt[0]+self.sl_rad+mat.shape[0],\n                        pt[1]+self.sl_rad:pt[1]+self.sl_rad+mat.shape[1],\n                        pt[2]+self.sl_rad:pt[2]+self.sl_rad+mat.shape[2]\n                    ]\n                    outmat[coords] = mat\n        return outmat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_searchlight(self, voxel_fn, pool_size=None):\n\n        extra_block_fn_params = (voxel_fn, self.shape,\n                                 self.min_active_voxels_proportion)\n        block_fn_result = self.run_block_function(_singlenode_searchlight,\n                                                  extra_block_fn_params,\n                                                  pool_size)\n        return block_fn_result", "response": "This function is used to run a function at each voxel in the user - provided mask. The mask is passed to the searchlight function. The searchlight function is applied at each voxel in the user - provided mask. The searchlight shape is the same as the user - provided searchlight shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes the data before computing correlation", "response": "def _normalize_for_correlation(data, axis, return_nans=False):\n    \"\"\"normalize the data before computing correlation\n\n    The data will be z-scored and divided by sqrt(n)\n    along the assigned axis\n\n    Parameters\n    ----------\n    data: 2D array\n\n    axis: int\n        specify which dimension of the data should be normalized\n\n    return_nans: bool, default:False\n        If False, return zeros for NaNs; if True, return NaNs\n\n    Returns\n    -------\n    data: 2D array\n        the normalized data\n    \"\"\"\n    shape = data.shape\n    data = zscore(data, axis=axis, ddof=0)\n    # if zscore fails (standard deviation is zero),\n    # optionally set all values to be zero\n    if not return_nans:\n        data = np.nan_to_num(data)\n    data = data / math.sqrt(shape[axis])\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_correlation(matrix1, matrix2, return_nans=False):\n    matrix1 = matrix1.astype(np.float32)\n    matrix2 = matrix2.astype(np.float32)\n    [r1, d1] = matrix1.shape\n    [r2, d2] = matrix2.shape\n    if d1 != d2:\n        raise ValueError('Dimension discrepancy')\n    # preprocess two components\n    matrix1 = _normalize_for_correlation(matrix1, 1,\n                                         return_nans=return_nans)\n    matrix2 = _normalize_for_correlation(matrix2, 1,\n                                         return_nans=return_nans)\n    corr_data = np.empty((r1, r2), dtype=np.float32, order='C')\n    # blas routine is column-major\n    blas.compute_single_matrix_multiplication('T', 'N',\n                                              r2, r1, d1,\n                                              1.0,\n                                              matrix2, d2,\n                                              matrix1, d1,\n                                              0.0,\n                                              corr_data,\n                                              r2)\n    return corr_data", "response": "compute the correlation between two sets of variables"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nimpose an inverse-Gamma prior onto the variance (tau^2) parameter of a Gaussian Process, which is in turn a prior imposed over an unknown function y = f(x). The inverse-Gamma prior of tau^2, tau^2 ~ invgamma(shape, scale) is described by a shape parameter alpha=2 and a scale parameter beta=tau_range^2. tau_range describes the reasonable range of tau in the inverse-Gamma prior. The data y's at locations x's are assumed to follow Gaussian Process: f(x, x') ~ N(0, K(x, x') / 2 tau^2), where K is a kernel function defined on x. For n observations, K(x1, x2, ..., xn) is an n by n positive definite matrix. Given the prior parameter tau_range, number of observations n_y, and y_invK_y = y * inv(K) * y', the function returns the MAP estimate of tau^2 and the log posterior probability of tau^2 at the MAP value: log(p(tau^2|tau_range)). This function is written primarily for BRSA but can also be used elsewhere. y in this case corresponds to the log of SNR in each voxel. GBRSA does not rely on this function. An alternative form of prior is half-Cauchy prior on tau. Inverse-Gamma prior penalizes for both very small and very large values of tau, while half-Cauchy prior only penalizes for very large values of tau. For more information on usage, see description in BRSA class: `.BRSA` See also: `.prior_GP_var_half_cauchy` Parameters ---------- y_invK_y: float y * inv(K) * y^T, where y=f(x) is a vector of observations of unknown function f at different locations x. K is correlation matrix of f between different locations, based on a Gaussian Process (GP) describing the smoothness property of f. K fully incorporates the form of the kernel and the length scale of the GP, but not the variance of the GP (the purpose of this function is to estimate the variance). n_y: int, number of observations tau_range: float, The reasonable range of tau, the standard deviation of the Gaussian Process imposed on y=f(x). tau_range is parameter of the inverse-Gamma prior. Say, if you expect the standard deviation of the Gaussian process to be around 3, tau_range can be set to 3. The smaller it is, the more penalization is imposed on large variation of y. Returns ------- tau2: The MAP estimation of tau^2 based on the prior on tau and y_invK_y. log_ptau: log(p(tau)) of the returned tau^2 based on the inverse-Gamma prior.", "response": "def prior_GP_var_inv_gamma(y_invK_y, n_y, tau_range):\n    \"\"\" Imposing an inverse-Gamma prior onto the variance (tau^2)\n        parameter of a Gaussian Process, which is in turn a prior\n        imposed over an unknown function y = f(x).\n        The inverse-Gamma prior of tau^2, tau^2 ~ invgamma(shape, scale)\n        is described by a shape parameter alpha=2 and a scale parameter\n        beta=tau_range^2. tau_range describes the reasonable range of\n        tau in the inverse-Gamma prior.\n        The data y's at locations x's are assumed to follow Gaussian Process:\n        f(x, x') ~ N(0, K(x, x') / 2 tau^2), where K is a kernel\n        function defined on x. For n observations, K(x1, x2, ..., xn) is\n        an n by n positive definite matrix.\n        Given the prior parameter tau_range, number of observations\n        n_y, and y_invK_y = y * inv(K) * y',\n        the function returns the MAP estimate of tau^2 and\n        the log posterior probability of tau^2 at the MAP value:\n        log(p(tau^2|tau_range)).\n        This function is written primarily for BRSA but can also\n        be used elsewhere. y in this case corresponds to the log of\n        SNR in each voxel. GBRSA does not rely on this function.\n        An alternative form of prior is half-Cauchy prior on tau.\n        Inverse-Gamma prior penalizes for both very small and very\n        large values of tau, while half-Cauchy prior only penalizes\n        for very large values of tau.\n        For more information on usage, see description in BRSA class:\n        `.BRSA`\n\n        See also: `.prior_GP_var_half_cauchy`\n\n    Parameters\n    ----------\n    y_invK_y: float\n        y * inv(K) * y^T, where y=f(x) is a vector of observations\n        of unknown function f at different locations x.\n        K is correlation matrix of f between different locations, based\n        on a Gaussian Process (GP) describing the smoothness property\n        of f. K fully incorporates the form of the kernel\n        and the length scale of the GP, but not the variance of the GP\n        (the purpose of this function is to estimate the variance).\n    n_y: int, number of observations\n    tau_range: float,\n        The reasonable range of tau, the standard deviation of the\n        Gaussian Process imposed on y=f(x). tau_range is parameter\n        of the inverse-Gamma prior. Say, if you expect the standard\n        deviation of the Gaussian process to be around 3, tau_range\n        can be set to 3.\n        The smaller it is, the more penalization is imposed\n        on large variation of y.\n    Returns\n    -------\n    tau2: The MAP estimation of tau^2 based on the prior on tau\n        and y_invK_y.\n    log_ptau: log(p(tau)) of the returned tau^2 based on the\n        inverse-Gamma prior.\n    \"\"\"\n    alpha = 2\n    tau2 = (y_invK_y + 2 * tau_range**2) / (alpha * 2 + 2 + n_y)\n    log_ptau = scipy.stats.invgamma.logpdf(\n        tau2, scale=tau_range**2, a=2)\n    return tau2, log_ptau"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef Ncomp_SVHT_MG_DLD_approx(X, zscore=True):\n    beta = X.shape[0] / X.shape[1]\n    if beta > 1:\n        beta = 1 / beta\n    omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43\n    if zscore:\n        sing = np.linalg.svd(_zscore(X), False, False)\n    else:\n        sing = np.linalg.svd(X, False, False)\n    thresh = omega * np.median(sing)\n    ncomp = int(np.sum(np.logical_and(sing > thresh, np.logical_not(\n        np.isclose(sing, thresh)))))\n    # In the line above, we look for the singular values larger than\n    # the threshold but excluding those that happen to be \"just\" larger\n    # than the threshold by an amount close to the numerical precision.\n    # This is to prevent close-to-zero singular values to be included if\n    # the median of the eigenvalues is close to 0 (which could happen\n    # when the input X has lower rank than its minimal size.\n    return ncomp", "response": "This function calculates the optimal rank for the given data X."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _zscore(a):\n    assert a.ndim > 1, 'a must have more than one dimensions'\n    zscore = scipy.stats.zscore(a, axis=0)\n    zscore[:, np.logical_not(np.all(np.isfinite(zscore), axis=0))] = 0\n    return zscore", "response": "Calculates the z - score of a on the first axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, design, nuisance=None, scan_onsets=None, coords=None,\n            inten=None):\n        \"\"\"Compute the Bayesian RSA\n\n        Parameters\n        ----------\n        X: numpy array, shape=[time_points, voxels]\n            If you have multiple scans of the same participants that you\n            want to analyze together, you should concatenate them along\n            the time dimension after proper preprocessing (e.g. spatial\n            alignment), and specify the onsets of each scan in scan_onsets.\n        design: numpy array, shape=[time_points, conditions]\n            This is the design matrix. It should only include the hypothetic\n            response for task conditions. You should not include\n            regressors for a DC component or motion parameters, unless you\n            want to estimate their pattern similarity with response patterns\n            to your task conditions. If you want to model head motion,\n            you should include them in nuisance regressors.\n            If you have multiple run, the design matrix\n            of all runs should be concatenated along the time dimension,\n            with every column for one condition across runs.\n            For example, if you have 3 runs of experiment of one participant,\n            with each run lasting 200 TR. And you have 4 conditions,\n            then design should be a 600 x 4 numpy array.\n        nuisance: optional, numpy array, shape=[time_points, nuisance_factors]\n            The responses to these regressors will be marginalized out from\n            each voxel, which means they are considered, but won't be assumed\n            to share the same pseudo-SNR map with the design matrix.\n            Therefore, the pseudo-SNR map will only reflect the\n            relative contribution of design matrix to each voxel.\n            You can provide time courses such as those for head motion\n            to this parameter.\n            Note that if auto_nuisance is set to True, the first\n            n_nureg principal components of residual (excluding the response\n            to the design matrix and the user-provided nuisance regressors\n            and a constant baseline)\n            will be included as additional nuisance regressor after the\n            first round of fitting.\n            If auto_nuisance is set to False, the nuisance regressors supplied\n            by the users together with DC components will be used as\n            nuisance time series.\n            Please do not include time course of constant baseline in nuisance.\n        scan_onsets: optional, numpy array, shape=[runs,]\n            This specifies the indices of X which correspond to the onset\n            of each scanning run. For example, if you have two experimental\n            runs of the same subject, each with 100 TRs, then scan_onsets\n            should be [0,100].\n            If you do not provide the argument, the program will\n            assume all data are from the same run.\n            The effect of them is to make the inverse matrix\n            of the temporal covariance matrix of noise block-diagonal.\n        coords: optional, numpy array, shape=[voxels,3]\n            This is the coordinate of each voxel,\n            used for implementing Gaussian Process prior.\n        inten: optional, numpy array, shape=[voxel,]\n            This is the average fMRI intensity in each voxel.\n            It should be calculated from your data without any preprocessing\n            such as z-scoring. Because it should reflect\n            whether a voxel is bright (grey matter) or dark (white matter).\n            A Gaussian Process kernel defined on both coordinate and intensity\n            imposes a smoothness prior on adjcent voxels\n            but with the same tissue type. The Gaussian Process\n            is experimental and has shown good performance on\n            some visual datasets.\n        \"\"\"\n\n        logger.info('Running Bayesian RSA')\n        self.random_state_ = check_random_state(self.random_state)\n        # setting random seed\n        logger.debug('RandState set to {}'.format(self.random_state_))\n\n        assert not self.GP_inten or (self.GP_inten and self.GP_space),\\\n            'You must speficiy GP_space to True'\\\n            'if you want to use GP_inten'\n\n        # Check input data\n        assert_all_finite(X)\n        assert X.ndim == 2, 'The data should be 2-dimensional ndarray'\n\n        assert np.all(np.std(X, axis=0) > 0),\\\n            'The time courses of some voxels do not change at all.'\\\n            ' Please make sure all voxels are within the brain'\n\n        # check design matrix\n        assert_all_finite(design)\n        assert design.ndim == 2,\\\n            'The design matrix should be 2-dimensional ndarray'\n        assert np.linalg.matrix_rank(design) == design.shape[1], \\\n            'Your design matrix has rank smaller than the number of'\\\n            ' columns. Some columns can be explained by linear '\\\n            'combination of other columns. Please check your design matrix.'\n        assert np.size(design, axis=0) == np.size(X, axis=0),\\\n            'Design matrix and data do not '\\\n            'have the same number of time points.'\n        assert self.rank is None or self.rank <= design.shape[1],\\\n            'Your design matrix has fewer columns than the rank you set'\n\n        # Check the nuisance regressors.\n        if nuisance is not None:\n            assert_all_finite(nuisance)\n            assert nuisance.ndim == 2,\\\n                'The nuisance regressor should be 2-dimensional ndarray'\n            assert np.linalg.matrix_rank(nuisance) == nuisance.shape[1], \\\n                'The nuisance regressor has rank smaller than the number of'\\\n                'columns. Some columns can be explained by linear '\\\n                'combination of other columns. Please check your nuisance' \\\n                'regressors.'\n            assert np.size(nuisance, axis=0) == np.size(X, axis=0), \\\n                'Nuisance regressor and data do not have the same '\\\n                'number of time points.'\n        # check scan_onsets validity\n        assert scan_onsets is None or\\\n            (np.max(scan_onsets) <= X.shape[0] and np.min(scan_onsets) >= 0),\\\n            'Some scan onsets provided are out of the range of time points.'\n\n        # check the size of coords and inten\n        if self.GP_space:\n            logger.info('Fitting with Gaussian Process prior on log(SNR)')\n            assert coords is not None and coords.shape[0] == X.shape[1],\\\n                'Spatial smoothness was requested by setting GP_space. '\\\n                'But the voxel number of coords does not match that of '\\\n                'data X, or voxel coordinates are not provided. '\\\n                'Please make sure that coords is in the shape of '\\\n                '[n_voxel x 3].'\n            assert coords.ndim == 2,\\\n                'The coordinate matrix should be a 2-d array'\n            if self.GP_inten:\n                assert inten is not None and inten.shape[0] == X.shape[1],\\\n                    'The voxel number of intensity does not '\\\n                    'match that of data X, or intensity not provided.'\n                assert np.var(inten) > 0,\\\n                    'All voxels have the same intensity.'\n        if (not self.GP_space and coords is not None) or\\\n                (not self.GP_inten and inten is not None):\n            logger.warning('Coordinates or image intensity provided'\n                           ' but GP_space or GP_inten is not set '\n                           'to True. The coordinates or intensity are'\n                           ' ignored.')\n        # Estimate the number of necessary nuisance regressors\n        if self.auto_nuisance:\n            if self.n_nureg is None:\n                logger.info('number of nuisance regressors is determined '\n                            'automatically.')\n                run_TRs, n_runs = self._run_TR_from_scan_onsets(\n                    X.shape[0], scan_onsets)\n                ts_dc = self._gen_legendre(run_TRs, [0])\n                _, ts_base, _ = self._merge_DC_to_base(\n                    ts_dc, nuisance, False)\n                ts_reg = np.concatenate((ts_base, design), axis=1)\n                beta_hat = np.linalg.lstsq(ts_reg, X)[0]\n                residuals = X - np.dot(ts_reg, beta_hat)\n                self.n_nureg_ = np.max(\n                    [1, Ncomp_SVHT_MG_DLD_approx(residuals,\n                                                 self.nureg_zscore)])\n                logger.info('Use {} nuisance regressors to model the spatial '\n                            'correlation in noise.'.format(self.n_nureg_))\n                self.n_nureg_ = np.int32(self.n_nureg_)\n            else:\n                self.n_nureg_ = self.n_nureg\n            self.n_nureg_ = np.int32(self.n_nureg_)\n        # Run Bayesian RSA\n        # Note that we have a change of notation here. Within _fit_RSA_UV,\n        # design matrix is named X and data is named Y, to reflect the\n        # generative model that data Y is generated by mixing the response\n        # X to experiment conditions and other neural activity.\n        # However, in fit(), we keep the tradition of scikit-learn that\n        # X is the input data to fit and y, a reserved name not used, is\n        # the label to map to from X.\n\n        if not self.GP_space:\n            # If GP_space is not requested, then the model is fitted\n            # without imposing any Gaussian Process prior on log(SNR^2)\n            self.U_, self.L_, self.nSNR_, self.beta_, self.beta0_,\\\n                self._beta_latent_, self.sigma_, self.rho_, _, _, _,\\\n                self.X0_ = self._fit_RSA_UV(X=design, Y=X, X_base=nuisance,\n                                            scan_onsets=scan_onsets)\n        elif not self.GP_inten:\n            # If GP_space is requested, but GP_inten is not, a GP prior\n            # based on spatial locations of voxels will be imposed.\n            self.U_, self.L_, self.nSNR_, self.beta_, self.beta0_,\\\n                self._beta_latent_, self.sigma_, self.rho_, \\\n                self.lGPspace_, self.bGP_, _, \\\n                self.X0_ = self._fit_RSA_UV(\n                    X=design, Y=X, X_base=nuisance,\n                    scan_onsets=scan_onsets, coords=coords)\n        else:\n            # If both self.GP_space and self.GP_inten are True,\n            # a GP prior based on both location and intensity is imposed.\n            self.U_, self.L_, self.nSNR_, self.beta_, self.beta0_,\\\n                self._beta_latent_, self.sigma_, self.rho_, \\\n                self.lGPspace_, self.bGP_, self.lGPinten_, self.X0_ = \\\n                self._fit_RSA_UV(X=design, Y=X, X_base=nuisance,\n                                 scan_onsets=scan_onsets,\n                                 coords=coords, inten=inten)\n\n        self.C_ = utils.cov2corr(self.U_)\n        self.design_ = design.copy()\n        self._rho_design_, self._sigma2_design_ = \\\n            self._est_AR1(self.design_, same_para=True)\n        self._rho_X0_, self._sigma2_X0_ = self._est_AR1(self.X0_)\n        # AR(1) parameters of the design matrix and nuisance regressors,\n        # which will be used in transform or score.\n\n        # Finally, we fit a null model with the same setting except\n        # that there is no response to X\n        self.beta0_null_, self.sigma_null_, self.rho_null_, \\\n            self.X0_null_ = self._fit_null(Y=X, X_base=nuisance,\n                                           scan_onsets=scan_onsets)\n        self._rho_X0_null_, self._sigma2_X0_null_ =\\\n            self._est_AR1(self.X0_null_)\n        return self", "response": "Compute the Bayesian RSA version of the specified resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef score(self, X, design, scan_onsets=None):\n\n        assert X.ndim == 2 and X.shape[1] == self.beta_.shape[1], \\\n            'The shape of X is not consistent with the shape of data '\\\n            'used in the fitting step. They should have the same number '\\\n            'of voxels'\n        assert scan_onsets is None or (scan_onsets.ndim == 1 and\n                                       0 in scan_onsets), \\\n            'scan_onsets should either be None or an array of indices '\\\n            'If it is given, it should include at least 0'\n        if scan_onsets is None:\n            scan_onsets = np.array([0], dtype=int)\n        else:\n            scan_onsets = np.int32(scan_onsets)\n        ll = self._score(Y=X, design=design, beta=self.beta_,\n                         scan_onsets=scan_onsets, beta0=self.beta0_,\n                         rho_e=self.rho_, sigma_e=self.sigma_,\n                         rho_X0=self._rho_X0_, sigma2_X0=self._sigma2_X0_)\n        ll_null = self._score(Y=X, design=None, beta=None,\n                              scan_onsets=scan_onsets, beta0=self.beta0_,\n                              rho_e=self.rho_, sigma_e=self.sigma_,\n                              rho_X0=self._rho_X0_,\n                              sigma2_X0=self._sigma2_X0_)\n        return ll, ll_null", "response": "This function calculates the log likelihood of a nuisance time series and returns the model and parameters estimated by fit function\n            from some new data X and design matrix X0."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares the essential template matrices D and F for the current cluster.", "response": "def _prepare_DF(self, n_T, scan_onsets=None):\n        \"\"\" Prepare the essential template matrices D and F for\n            pre-calculating some terms to be re-used.\n            The inverse covariance matrix of AR(1) noise is\n            sigma^-2 * (I - rho1*D + rho1**2 * F).\n            And we denote A = I - rho1*D + rho1**2 * F\"\"\"\n        run_TRs, n_run = self._run_TR_from_scan_onsets(n_T, scan_onsets)\n        D_ele = map(self._D_gen, run_TRs)\n        F_ele = map(self._F_gen, run_TRs)\n        D = scipy.linalg.block_diag(*D_ele)\n        F = scipy.linalg.block_diag(*F_ele)\n        # D and F above are templates for constructing\n        # the inverse of temporal covariance matrix of noise\n        return D, F, run_TRs, n_run"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares different forms of products of design matrix X and data Y and between them.", "response": "def _prepare_data_XY(self, X, Y, D, F):\n        \"\"\"Prepares different forms of products of design matrix X\n            and data Y, or between themselves.\n            These products are re-used a lot during fitting.\n            So we pre-calculate them. Because these are reused,\n            it is in principle possible to update the fitting\n            as new data come in, by just incrementally adding\n            the products of new data and their corresponding parts\n            of design matrix to these pre-calculated terms.\n        \"\"\"\n        XTY, XTDY, XTFY = self._make_templates(D, F, X, Y)\n\n        YTY_diag = np.sum(Y * Y, axis=0)\n        YTDY_diag = np.sum(Y * np.dot(D, Y), axis=0)\n        YTFY_diag = np.sum(Y * np.dot(F, Y), axis=0)\n\n        XTX, XTDX, XTFX = self._make_templates(D, F, X, X)\n\n        return XTY, XTDY, XTFY, YTY_diag, YTDY_diag, YTFY_diag, XTX, \\\n            XTDX, XTFX"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_data_XYX0(self, X, Y, X_base, X_res, D, F, run_TRs,\n                           no_DC=False):\n        \"\"\"Prepares different forms of products between design matrix X or\n            data Y or nuisance regressors X0.\n            These products are re-used a lot during fitting.\n            So we pre-calculate them.\n            no_DC means not inserting regressors for DC components\n            into nuisance regressor.\n            It will only take effect if X_base is not None.\n        \"\"\"\n        X_DC = self._gen_X_DC(run_TRs)\n        reg_sol = np.linalg.lstsq(X_DC, X)\n        if np.any(np.isclose(reg_sol[1], 0)):\n            raise ValueError('Your design matrix appears to have '\n                             'included baseline time series.'\n                             'Either remove them, or move them to'\n                             ' nuisance regressors.')\n        X_DC, X_base, idx_DC = self._merge_DC_to_base(X_DC, X_base,\n                                                      no_DC)\n        if X_res is None:\n            X0 = X_base\n        else:\n            X0 = np.concatenate((X_base, X_res), axis=1)\n        n_X0 = X0.shape[1]\n        X0TX0, X0TDX0, X0TFX0 = self._make_templates(D, F, X0, X0)\n        XTX0, XTDX0, XTFX0 = self._make_templates(D, F, X, X0)\n        X0TY, X0TDY, X0TFY = self._make_templates(D, F, X0, Y)\n\n        return X0TX0, X0TDX0, X0TFX0, XTX0, XTDX0, XTFX0, \\\n            X0TY, X0TDY, X0TFY, X0, X_base, n_X0, idx_DC", "response": "Prepares different forms of products between design matrix X and data Y or nuisance regressors X0."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_DC_to_base(self, X_DC, X_base, no_DC):\n        if X_base is not None:\n            reg_sol = np.linalg.lstsq(X_DC, X_base)\n            if not no_DC:\n                if not np.any(np.isclose(reg_sol[1], 0)):\n                    # No columns in X_base can be explained by the\n                    # baseline regressors. So we insert them.\n                    X_base = np.concatenate((X_DC, X_base), axis=1)\n                    idx_DC = np.arange(0, X_DC.shape[1])\n                else:\n                    logger.warning('Provided regressors for uninteresting '\n                                   'time series already include baseline. '\n                                   'No additional baseline is inserted.')\n                    idx_DC = np.where(np.isclose(reg_sol[1], 0))[0]\n            else:\n                idx_DC = np.where(np.isclose(reg_sol[1], 0))[0]\n        else:\n            # If a set of regressors for non-interested signals is not\n            # provided, then we simply include one baseline for each run.\n            X_base = X_DC\n            idx_DC = np.arange(0, X_base.shape[1])\n            logger.info('You did not provide time series of no interest '\n                        'such as DC component. Trivial regressors of'\n                        ' DC component are included for further modeling.'\n                        ' The final covariance matrix won''t '\n                        'reflect these components.')\n        return X_DC, X_base, idx_DC", "response": "Merge DC components X_DC to the baseline time series X_base."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbuilding the index parameter dictionary for the parameter .", "response": "def _build_index_param(self, n_l, n_V, n_smooth):\n        \"\"\" Build dictionaries to retrieve each parameter\n            from the combined parameters.\n        \"\"\"\n        idx_param_sing = {'Cholesky': np.arange(n_l), 'a1': n_l}\n        # for simplified fitting\n        idx_param_fitU = {'Cholesky': np.arange(n_l),\n                          'a1': np.arange(n_l, n_l + n_V)}\n        # for the likelihood function when we fit U (the shared covariance).\n        idx_param_fitV = {'log_SNR2': np.arange(n_V - 1),\n                          'c_space': n_V - 1, 'c_inten': n_V,\n                          'c_both': np.arange(n_V - 1, n_V - 1 + n_smooth)}\n        # for the likelihood function when we fit V (reflected by SNR of\n        # each voxel)\n        return idx_param_sing, idx_param_fitU, idx_param_fitV"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _half_log_det(self, M):\n        chol = np.linalg.cholesky(M)\n        if M.ndim == 2:\n            return np.sum(np.log(np.abs(np.diag(chol))))\n        else:\n            return np.sum(np.log(np.abs(np.diagonal(\n                chol, axis1=-2, axis2=-1))), axis=-1)", "response": "Return log - det of the M matrix."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the data Y and the response amplitudes beta and beta0 estimated in the fit step, estimate the corresponding X and X0. It is done by a forward-backward algorithm. We assume X and X0 both are vector autoregressive (VAR) processes, to capture temporal smoothness. Their VAR parameters are estimated from training data at the fit stage.", "response": "def _transform(self, Y, scan_onsets, beta, beta0,\n                   rho_e, sigma_e, rho_X, sigma2_X, rho_X0, sigma2_X0):\n        \"\"\" Given the data Y and the response amplitudes beta and beta0\n            estimated in the fit step, estimate the corresponding X and X0.\n            It is done by a forward-backward algorithm.\n            We assume X and X0 both are vector autoregressive (VAR)\n            processes, to capture temporal smoothness. Their VAR\n            parameters are estimated from training data at the fit stage.\n        \"\"\"\n        logger.info('Transforming new data.')\n        # Constructing the transition matrix and the variance of\n        # innovation noise as prior for the latent variable X and X0\n        # in new data.\n\n        n_C = beta.shape[0]\n        n_T = Y.shape[0]\n        weight = np.concatenate((beta, beta0), axis=0)\n        T_X = np.diag(np.concatenate((rho_X, rho_X0)))\n        Var_X = np.concatenate((sigma2_X / (1 - rho_X**2),\n                                sigma2_X0 / (1 - rho_X0**2)))\n        Var_dX = np.concatenate((sigma2_X, sigma2_X0))\n        sigma2_e = sigma_e ** 2\n        scan_onsets = np.setdiff1d(scan_onsets, n_T)\n        n_scan = scan_onsets.size\n        X = [None] * scan_onsets.size\n        X0 = [None] * scan_onsets.size\n        total_log_p = 0\n        for scan, onset in enumerate(scan_onsets):\n            # Forward step\n            if scan == n_scan - 1:\n                offset = n_T\n            else:\n                offset = scan_onsets[scan + 1]\n            mu, mu_Gamma_inv, Gamma_inv, log_p_data, Lambda_0, \\\n                Lambda_1, H, deltaY, deltaY_sigma2inv_rho_weightT = \\\n                self._forward_step(Y[onset:offset, :],\n                                   T_X, Var_X, Var_dX, rho_e, sigma2_e,\n                                   weight)\n            total_log_p += log_p_data\n            # Backward step\n            mu_hat, mu_Gamma_inv_hat, Gamma_inv_hat \\\n                = self._backward_step(\n                    deltaY, deltaY_sigma2inv_rho_weightT, sigma2_e,\n                    weight, mu, mu_Gamma_inv, Gamma_inv,\n                    Lambda_0, Lambda_1, H)\n            X[scan] = np.concatenate(\n                [mu_t[None, :n_C] for mu_t in mu_hat])\n            X0[scan] = np.concatenate(\n                [mu_t[None, n_C:] for mu_t in mu_hat])\n        X = np.concatenate(X)\n        X0 = np.concatenate(X0)\n        return X, X0, total_log_p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _score(self, Y, design, beta, scan_onsets, beta0, rho_e, sigma_e,\n               rho_X0, sigma2_X0):\n        \"\"\" Given the data Y, and the spatial pattern beta0\n            of nuisance time series, return the cross-validated score\n            of the data Y given all parameters of the subject estimated\n            during the first step.\n            It is assumed that the user has design matrix built for the\n            data Y. Both beta and beta0 are posterior expectation estimated\n            from training data with the estimated covariance matrix U and\n            SNR serving as prior. We marginalize X0 instead of fitting\n            it in this function because this function is for the purpose\n            of evaluating model no new data. We should avoid doing any\n            additional fitting when performing cross-validation.\n            The hypothetic response to the task will be subtracted, and\n            the unknown nuisance activity which contributes to the data\n            through beta0 will be marginalized.\n        \"\"\"\n        logger.info('Estimating cross-validated score for new data.')\n        n_T = Y.shape[0]\n        if design is not None:\n            Y = Y - np.dot(design, beta)\n        # The function works for both full model and null model.\n        # If design matrix is not provided, the whole data is\n        # used as input for _forward_step. If design matrix is provided,\n        # residual after subtracting design * beta is fed to _forward_step\n        T_X = np.diag(rho_X0)\n        Var_X = sigma2_X0 / (1 - rho_X0**2)\n        Var_dX = sigma2_X0\n        # Prior parmeters for X0: T_X is transitioning matrix, Var_X\n        # is the marginal variance of the first time point. Var_dX is the\n        # variance of the updating noise.\n        sigma2_e = sigma_e ** 2\n        # variance of voxel-specific updating noise component\n        scan_onsets = np.setdiff1d(scan_onsets, n_T).astype(int)\n        n_scan = scan_onsets.size\n        total_log_p = 0\n        for scan, onset in enumerate(scan_onsets):\n            # Forward step\n            if scan == n_scan - 1:\n                offset = n_T\n            else:\n                offset = scan_onsets[scan + 1]\n            _, _, _, log_p_data, _, _, _, _, _ = \\\n                self._forward_step(\n                    Y[onset:offset, :], T_X, Var_X, Var_dX, rho_e, sigma2_e,\n                    beta0)\n            total_log_p += log_p_data\n        return total_log_p", "response": "This function calculates the cross - validated score for a new nuisance time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _est_AR1(self, x, same_para=False):\n        if same_para:\n            n_c = x.shape[1]\n            x = np.reshape(x, x.size, order='F')\n            rho, sigma2 = alg.AR_est_YW(x, 1)\n            # We concatenate all the design matrix to estimate common AR(1)\n            # parameters. This creates some bias because the end of one column\n            # and the beginning of the next column of the design matrix are\n            # treated as consecutive samples.\n            rho = np.ones(n_c) * rho\n            sigma2 = np.ones(n_c) * sigma2\n        else:\n            rho = np.zeros(np.shape(x)[1])\n            sigma2 = np.zeros(np.shape(x)[1])\n            for c in np.arange(np.shape(x)[1]):\n                rho[c], sigma2[c] = alg.AR_est_YW(x[:, c], 1)\n        return rho, sigma2", "response": "Estimates the common AR1 parameters of input x."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforwards step for HMM assuming both the hidden state and noise have 1 - step dependence on the previous value.", "response": "def _forward_step(self, Y, T_X, Var_X, Var_dX, rho_e, sigma2_e, weight):\n        \"\"\" forward step for HMM, assuming both the hidden state and noise\n            have 1-step dependence on the previous value.\n        \"\"\"\n        # We currently only implement diagonal form\n        # of covariance matrix for Var_X, Var_dX and T_X, which means\n        # each dimension of X is independent and their innovation noise\n        # are also independent. Note that log_p_data takes this assumption.\n        if Var_X.ndim == 1:\n            inv_Var_X = np.diag(1 / Var_X)\n            half_log_det_Var_X = np.sum(np.log(Var_X)) / 2.0\n            Var_X = np.diag(Var_X)\n            # the marginal variance of X\n        else:\n            half_log_det_Var_X = self._half_log_det(Var_X)\n            inv_Var_X = np.linalg.inv(Var_X)\n        if Var_dX.ndim == 1:\n            inv_Var_dX = np.diag(1 / Var_dX)\n            half_log_det_Var_dX = np.sum(np.log(Var_dX)) / 2.0\n            Var_dX = np.diag(Var_dX)\n            # the marginal variance of Delta X (the change of X from\n            # previous time point)\n        else:\n            inv_Var_dX = np.linalg.inv(Var_dX)\n            half_log_det_Var_dX = self._half_log_det(Var_dX)\n        if T_X.ndim == 1:\n            T_X = np.diag(T_X)\n            # Transfer function of X: the expected mean of X at t+1\n            # time point is T_x * X\n        [n_T, n_V] = np.shape(Y)\n        # numbers of time points and voxels\n        mu = [None] * n_T\n        # posterior mean of X, conditioned on all data up till the current\n        # time point\n        Gamma_inv = [None] * n_T\n        # inverse of poterior Gamma.\n        mu_Gamma_inv = [None] * n_T\n        # mu * inv(Gamma)\n        log_p_data = - np.log(np.pi * 2) * (n_T * n_V) / 2 \\\n            - half_log_det_Var_X - np.sum(np.log(sigma2_e)) * n_T / 2.0\\\n            + np.sum(np.log(1 - rho_e**2)) / 2.0 - half_log_det_Var_dX \\\n            * (n_T - 1)\n        # This is the term to be incremented by c_n at each time step.\n        # We first add all the fixed terms to it.\n\n        # The following are a few fixed terms.\n        Lambda_0 = np.dot(T_X, np.dot(inv_Var_dX, T_X.T)) \\\n            + np.dot(weight * rho_e**2 / sigma2_e, weight.T)\n        H = np.dot(inv_Var_dX, T_X.T) + np.dot(weight * rho_e / sigma2_e,\n                                               weight.T)\n        Lambda_1 = inv_Var_dX + np.dot(weight / sigma2_e, weight.T)\n\n        Gamma_inv[0] = inv_Var_X + np.dot(\n            weight * (1 - rho_e**2) / sigma2_e, weight.T)\n        # We might not need this and only use linalg.solve for related terms.\n        mu_Gamma_inv[0] = np.dot(\n            Y[0, :] * (1 - rho_e**2) / sigma2_e, weight.T)\n        mu[0] = np.linalg.solve(Gamma_inv[0], mu_Gamma_inv[0])\n        log_p_data -= 0.5 * np.sum(Y[0, :]**2 * (1 - rho_e**2) / sigma2_e)\n        # This is the term added for the first time point.\n\n        deltaY = Y[1:, :] - rho_e * Y[:-1, :]\n        deltaY_sigma2inv_rho_weightT = np.dot(\n            deltaY / sigma2_e * rho_e, weight.T)\n        for t in np.arange(1, n_T):\n            Gamma_tilde_inv = Lambda_0 + Gamma_inv[t - 1]\n            tmp = np.linalg.solve(Gamma_tilde_inv, H.T)\n            Gamma_inv[t] = Lambda_1 - np.dot(H, tmp)\n            mu_Gamma_inv[t] = np.dot(deltaY[t - 1, :] / sigma2_e, weight.T) \\\n                + np.dot(mu_Gamma_inv[t - 1]\n                         - deltaY_sigma2inv_rho_weightT[t - 1, :], tmp)\n            mu[t] = np.linalg.solve(Gamma_inv[t], mu_Gamma_inv[t])\n            tmp2 = mu_Gamma_inv[t - 1] - deltaY_sigma2inv_rho_weightT[t - 1, :]\n            log_p_data += -self._half_log_det(Gamma_tilde_inv) \\\n                + np.dot(tmp2, np.linalg.solve(Gamma_tilde_inv, tmp2)) / 2.0\n        log_p_data += -self._half_log_det(Gamma_inv[-1]) \\\n            + np.dot(mu_Gamma_inv[-1], mu[-1]) / 2.0 \\\n            - np.sum(deltaY**2 / sigma2_e) / 2.0\n        return mu, mu_Gamma_inv, Gamma_inv, log_p_data, Lambda_0, \\\n            Lambda_1, H, deltaY, deltaY_sigma2inv_rho_weightT"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initial_fit_singpara(self, XTX, XTDX, XTFX,\n                              YTY_diag, YTDY_diag, YTFY_diag,\n                              XTY, XTDY, XTFY, X0TX0, X0TDX0, X0TFX0,\n                              XTX0, XTDX0, XTFX0, X0TY, X0TDY, X0TFY,\n                              X, Y, X0, idx_param_sing, l_idx,\n                              n_C, n_T, n_V, n_l, n_run, n_X0, rank):\n        \"\"\" Perform initial fitting of a simplified model, which assumes\n            that all voxels share exactly the same temporal covariance\n            matrix for their noise (the same noise variance and\n            auto-correlation). The SNR is implicitly assumed to be 1\n            for all voxels.\n        \"\"\"\n        logger.info('Initial fitting assuming single parameter of '\n                    'noise for all voxels')\n        X_joint = np.concatenate((X0, X), axis=1)\n        beta_hat = np.linalg.lstsq(X_joint, Y)[0]\n        residual = Y - np.dot(X_joint, beta_hat)\n        # point estimates of betas and fitting residuals without assuming\n        # the Bayesian model underlying RSA.\n\n        # There are several possible ways of initializing the covariance.\n        # (1) start from the point estimation of covariance\n\n        cov_point_est = np.cov(beta_hat[n_X0:, :]) / np.var(residual)\n        current_vec_U_chlsk_l = \\\n            np.linalg.cholesky((cov_point_est + np.eye(n_C)) / 2)[l_idx]\n\n        # We use the average of covariance of point estimation and an identity\n        # matrix as the initial value of the covariance matrix, just in case\n        # the user provides data in which n_V is smaller than n_C.\n\n        # (2) start from identity matrix\n\n        # current_vec_U_chlsk_l = np.eye(n_C)[l_idx]\n\n        # (3) random initialization\n\n        # current_vec_U_chlsk_l = self.random_state_.randn(n_l)\n\n        # vectorized version of L, Cholesky factor of U, the shared\n        # covariance matrix of betas across voxels.\n\n        rho1 = np.sum(\n            residual[0:-1, :] * residual[1:, :], axis=0) / \\\n            np.sum(residual[0:-1, :] * residual[0:-1, :], axis=0)\n        # Estimate of auto correlation assuming data includes pure noise.\n        log_sigma2 = np.log(np.var(\n            residual[1:, :] - residual[0:-1, :] * rho1, axis=0))\n        # log of estimates of the variance of the \"innovation\" noise\n        # of AR(1) process at each time point.\n        param0 = np.empty(np.sum(np.size(v)\n                                 for v in idx_param_sing.values()))\n        # Initial parameter\n        # Then we fill each part of the original guess of parameters\n        param0[idx_param_sing['Cholesky']] = current_vec_U_chlsk_l\n        param0[idx_param_sing['a1']] = np.median(np.tan(rho1 * np.pi / 2))\n\n        # Fit it.\n        res = scipy.optimize.minimize(\n            self._loglike_AR1_singpara, param0,\n            args=(XTX, XTDX, XTFX, YTY_diag, YTDY_diag, YTFY_diag,\n                  XTY, XTDY, XTFY, X0TX0, X0TDX0, X0TFX0,\n                  XTX0, XTDX0, XTFX0, X0TY, X0TDY, X0TFY,\n                  l_idx, n_C, n_T, n_V, n_run, n_X0,\n                  idx_param_sing, rank),\n            method=self.optimizer, jac=True, tol=self.tol,\n            options={'disp': self.minimize_options['disp'],\n                     'maxiter': 100})\n        current_vec_U_chlsk_l = res.x[idx_param_sing['Cholesky']]\n        current_a1 = res.x[idx_param_sing['a1']] * np.ones(n_V)\n        # log(sigma^2) assuming the data include no signal is returned,\n        # as a starting point for the iteration in the next step.\n        # Although it should overestimate the variance,\n        # setting it this way might allow it to track log(sigma^2)\n        # more closely for each voxel.\n        return current_vec_U_chlsk_l, current_a1, log_sigma2", "response": "Perform initial fitting of a simplified model which assumes that all voxels share exactly the same temporal covariance matrix for all voxels."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits the diagonally unweighted version of the current state of the object.", "response": "def _fit_diagV_noGP(\n            self, XTY, XTDY, XTFY, YTY_diag, YTDY_diag, YTFY_diag,\n            XTX, XTDX, XTFX, X, Y, X_base, X_res, D, F, run_TRs,\n            current_vec_U_chlsk_l,\n            current_a1, current_logSNR2,\n            idx_param_fitU, idx_param_fitV,\n            l_idx, n_C, n_T, n_V, n_l, n_run, n_X0, rank):\n        \"\"\" (optional) second step of fitting, full model but without\n            GP prior on log(SNR). This step is only done if GP prior\n            is requested.\n        \"\"\"\n        init_iter = self.init_iter\n        logger.info('second fitting without GP prior'\n                    ' for {} times'.format(init_iter))\n\n        # Initial parameters\n        param0_fitU = np.empty(\n            np.sum(np.size(v) for v in idx_param_fitU.values()))\n        param0_fitV = np.empty(np.size(idx_param_fitV['log_SNR2']))\n        # We cannot use the same logic as the line above because\n        # idx_param_fitV also includes entries for GP parameters.\n        param0_fitU[idx_param_fitU['Cholesky']] = \\\n            current_vec_U_chlsk_l.copy()\n        param0_fitU[idx_param_fitU['a1']] = current_a1.copy()\n        param0_fitV[idx_param_fitV['log_SNR2']] = \\\n            current_logSNR2[:-1].copy()\n\n        L = np.zeros((n_C, rank))\n        tol = self.tol * 5\n        for it in range(0, init_iter):\n            X0TX0, X0TDX0, X0TFX0, XTX0, XTDX0, XTFX0, \\\n                X0TY, X0TDY, X0TFY, X0, X_base, n_X0, _ \\\n                = self._prepare_data_XYX0(\n                    X, Y, X_base, X_res, D, F, run_TRs, no_DC=True)\n\n            # fit U, the covariance matrix, together with AR(1) param\n            param0_fitU[idx_param_fitU['Cholesky']] = \\\n                current_vec_U_chlsk_l \\\n                + self.random_state_.randn(n_l) \\\n                * np.linalg.norm(current_vec_U_chlsk_l) \\\n                / n_l**0.5 * np.exp(-it / init_iter * self.anneal_speed - 1)\n            param0_fitU[idx_param_fitU['a1']] = current_a1\n            res_fitU = scipy.optimize.minimize(\n                self._loglike_AR1_diagV_fitU, param0_fitU,\n                args=(XTX, XTDX, XTFX, YTY_diag, YTDY_diag, YTFY_diag,\n                      XTY, XTDY, XTFY, X0TX0, X0TDX0, X0TFX0,\n                      XTX0, XTDX0, XTFX0, X0TY, X0TDY, X0TFY,\n                      current_logSNR2, l_idx, n_C,\n                      n_T, n_V, n_run, n_X0, idx_param_fitU, rank),\n                method=self.optimizer, jac=True, tol=tol,\n                options=self.minimize_options)\n            current_vec_U_chlsk_l = \\\n                res_fitU.x[idx_param_fitU['Cholesky']]\n            current_a1 = res_fitU.x[idx_param_fitU['a1']]\n            norm_fitUchange = np.linalg.norm(res_fitU.x - param0_fitU)\n            logger.debug('norm of parameter change after fitting U: '\n                         '{}'.format(norm_fitUchange))\n            param0_fitU = res_fitU.x.copy()\n\n            # fit V, reflected in the log(SNR^2) of each voxel\n            rho1 = np.arctan(current_a1) * 2 / np.pi\n            L[l_idx] = current_vec_U_chlsk_l\n            X0TAX0, XTAX0, X0TAY, X0TAX0_i, \\\n                XTAcorrX, XTAcorrY, YTAcorrY, \\\n                LTXTAcorrY, XTAcorrXL, LTXTAcorrXL = \\\n                self._precompute_ar1_quad_forms(XTY, XTDY, XTFY,\n                                                YTY_diag, YTDY_diag, YTFY_diag,\n                                                XTX, XTDX, XTFX,\n                                                X0TX0, X0TDX0, X0TFX0,\n                                                XTX0, XTDX0, XTFX0,\n                                                X0TY, X0TDY, X0TFY,\n                                                L, rho1, n_V, n_X0)\n            res_fitV = scipy.optimize.minimize(\n                self._loglike_AR1_diagV_fitV, param0_fitV,\n                args=(X0TAX0, XTAX0, X0TAY,\n                      X0TAX0_i, XTAcorrX, XTAcorrY, YTAcorrY,\n                      LTXTAcorrY, XTAcorrXL, LTXTAcorrXL,\n                      current_vec_U_chlsk_l,\n                      current_a1, l_idx, n_C, n_T, n_V, n_run,\n                      n_X0, idx_param_fitV, rank,\n                      False, False),\n                method=self.optimizer, jac=True, tol=tol,\n                options=self.minimize_options)\n\n            current_logSNR2[0:n_V - 1] = res_fitV.x\n            current_logSNR2[-1] = - np.sum(current_logSNR2[0:n_V - 1])\n\n            norm_fitVchange = np.linalg.norm(res_fitV.x - param0_fitV)\n            logger.debug('norm of parameter change after fitting V: '\n                         '{}'.format(norm_fitVchange))\n            logger.debug('E[log(SNR2)^2]: {}'.format(\n                np.mean(current_logSNR2**2)))\n\n            # The lines below are for debugging purpose.\n            # If any voxel's log(SNR^2) gets to non-finite number,\n            # something might be wrong -- could be that the data has\n            # nothing to do with the design matrix.\n            if np.any(np.logical_not(np.isfinite(current_logSNR2))):\n                logger.warning('Initial fitting: iteration {}'.format(it))\n                logger.warning('current log(SNR^2): '\n                               '{}'.format(current_logSNR2))\n                logger.warning('log(sigma^2) has non-finite number')\n\n            param0_fitV = res_fitV.x.copy()\n\n            # Re-estimating X_res from residuals\n            current_SNR2 = np.exp(current_logSNR2)\n            if self.auto_nuisance:\n                LL, LAMBDA_i, LAMBDA, YTAcorrXL_LAMBDA, current_sigma2 \\\n                    = self._calc_LL(rho1, LTXTAcorrXL, LTXTAcorrY, YTAcorrY,\n                                    X0TAX0, current_SNR2,\n                                    n_V, n_T, n_run, rank, n_X0)\n                betas = current_SNR2 * np.dot(L, YTAcorrXL_LAMBDA.T)\n                beta0s = np.einsum(\n                    'ijk,ki->ji', X0TAX0_i,\n                    (X0TAY - np.einsum('ikj,ki->ji', XTAX0, betas)))\n                residuals = Y - np.dot(X, betas) - np.dot(\n                    X_base, beta0s[:np.shape(X_base)[1], :])\n                X_res = self.nureg_method(\n                    self.n_nureg_).fit_transform(\n                    self.preprocess_residual(residuals))\n            if norm_fitVchange / np.sqrt(param0_fitV.size) < tol \\\n                    and norm_fitUchange / np.sqrt(param0_fitU.size) \\\n                    < tol:\n                break\n        return current_vec_U_chlsk_l, current_a1, current_logSNR2, X_res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fit_null(self, Y, X_base, scan_onsets=None):\n        n_V = np.size(Y, axis=1)\n        n_T = np.size(Y, axis=0)\n        t_start = time.time()\n\n        D, F, run_TRs, n_run = self._prepare_DF(\n            n_T, scan_onsets=scan_onsets)\n        YTY_diag = np.sum(Y * Y, axis=0)\n        YTDY_diag = np.sum(Y * np.dot(D, Y), axis=0)\n        YTFY_diag = np.sum(Y * np.dot(F, Y), axis=0)\n\n        tol = self.tol\n        n_iter = self.n_iter\n        logger.info('Fitting null model'\n                    ' for maximum {} times'.format(n_iter))\n\n        # Add DC components capturing run-specific baselines.\n        X_DC = self._gen_X_DC(run_TRs)\n        X_DC, X_base, idx_DC = self._merge_DC_to_base(\n            X_DC, X_base, no_DC=False)\n        X_res = None\n        param0 = np.zeros(n_V)\n        for it in range(0, n_iter):\n            if X_res is None:\n                X0 = X_base\n            else:\n                X0 = np.concatenate((X_base, X_res), axis=1)\n            n_X0 = X0.shape[1]\n            X0TX0, X0TDX0, X0TFX0 = self._make_templates(D, F, X0, X0)\n            X0TY, X0TDY, X0TFY = self._make_templates(D, F, X0, Y)\n            res_null = scipy.optimize.minimize(\n                self._loglike_AR1_null, param0, args=(\n                    YTY_diag, YTDY_diag, YTFY_diag,\n                    X0TX0, X0TDX0, X0TFX0, X0TY, X0TDY, X0TFY,\n                    n_T, n_V, n_run, n_X0),\n                method=self.optimizer, jac=True, tol=tol,\n                options=self.minimize_options)\n            param_change = res_null.x - param0\n            param0 = res_null.x.copy()\n            est_rho1_AR1_null = 2.0 / np.pi * np.arctan(param0)\n            if self.auto_nuisance:\n                X0TAX0 = X0TX0[None, :, :] \\\n                    - est_rho1_AR1_null[:, None, None] \\\n                    * X0TDX0[None, :, :] \\\n                    + est_rho1_AR1_null[:, None, None]**2 \\\n                    * X0TFX0[None, :, :]\n                # dimension: space*#baseline*#baseline\n                X0TAY = self._make_ar1_quad_form(X0TY, X0TDY, X0TFY,\n                                                 est_rho1_AR1_null)\n                # dimension: #baseline*space\n                beta0s = np.linalg.solve(X0TAX0, X0TAY.T).T\n                residuals = Y - np.dot(X_base, beta0s[:np.shape(X_base)[1], :])\n                X_res = self.nureg_method(self.n_nureg_).fit_transform(\n                    self.preprocess_residual(residuals))\n            if np.max(np.abs(param_change)) < self.tol:\n                logger.info('The change of parameters is smaller than '\n                            'the tolerance value {}. Fitting is finished '\n                            'after {} iterations'.format(self.tol, it + 1))\n                break\n        X0TAX0 = X0TX0[None, :, :] \\\n            - est_rho1_AR1_null[:, None, None] \\\n            * X0TDX0[None, :, :] \\\n            + est_rho1_AR1_null[:, None, None]**2 \\\n            * X0TFX0[None, :, :]\n        # dimension: space*#baseline*#baseline\n        X0TAY = self._make_ar1_quad_form(X0TY, X0TDY, X0TFY,\n                                         est_rho1_AR1_null)\n        # dimension: #baseline*space\n        est_beta0_AR1_null = np.linalg.solve(X0TAX0, X0TAY.T).T\n\n        YTAY = self._make_ar1_quad_form(YTY_diag, YTDY_diag, YTFY_diag,\n                                        est_rho1_AR1_null)\n        # dimension: space,\n        YTAcorrY = YTAY - np.sum(X0TAY * est_beta0_AR1_null, axis=0)\n        # dimension: space,\n        est_sigma_AR1_null = (YTAcorrY / (n_T - n_X0)) ** 0.5\n        if idx_DC.size > 1:\n            collapsed_DC = np.sum(X0[:, idx_DC], axis=1)\n            X0 = np.insert(np.delete(X0, idx_DC, axis=1), 0,\n                           collapsed_DC, axis=1)\n            collapsed_beta0 = np.mean(est_beta0_AR1_null[idx_DC, :], axis=0)\n            est_beta0_AR1_null = np.insert(\n                np.delete(est_beta0_AR1_null, idx_DC, axis=0),\n                0, collapsed_beta0, axis=0)\n        t_finish = time.time()\n        logger.info(\n            'total time of fitting: {} seconds'.format(t_finish - t_start))\n        return est_beta0_AR1_null, est_sigma_AR1_null, est_rho1_AR1_null, X0", "response": "Fit a null model for the given data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the model to data of all participants jointly.", "response": "def fit(self, X, design, nuisance=None, scan_onsets=None):\n        \"\"\" Fit the model to data of all participants jointly.\n\n        Parameters\n        ----------\n        X: list of numpy arrays, shape=[time_points, voxels] for each entry.\n            Data to be fitted. Each participant corresponds to one item in\n            the list. If you have multiple scans of the same participants\n            that you want to analyze together, you should concatenate them\n            along the time dimension after proper preprocessing (e.g. spatial\n            alignment), and specify the onsets of each scan in scan_onsets.\n        design: list of numpy arrays, shape=[time_points, conditions] for each.\n            This is the design matrix of each participant.\n            It should only include the hypothetic response for task conditions.\n            You should not include regressors for a DC component or\n            motion parameters, unless with a strong reason.\n            If you want to model head motion, you should include them\n            in nuisance regressors.\n            If you have multiple run, the design matrix\n            of all runs should be concatenated along the time dimension for\n            each participant, with every column for one condition across runs.\n            If the design matrix is the same for all subjects,\n            either provide a list as required, or provide single numpy array.\n        nuisance: optional, list of numpy arrays,\n            shape=[time_points, nuisance_factors] for each subject in the list.\n            Nuisance regressors of each participant.\n            The responses to these regressors will be marginalized out from\n            each voxel, which means they are considered, but won't be assumed\n            to share the same pseudo-SNR map with the design matrix.\n            Therefore, the pseudo-SNR map will only reflect the\n            relative contribution of design matrix to each voxel.\n            You can provide time courses such as those for head motion\n            to this parameter.\n            Note that if auto_nuisance is set to True, the first\n            n_nureg principal components of residual (excluding the response\n            to the design matrix and the user-provided nuisance regressors)\n            will be included as additional nuisance regressor after the\n            first round of fitting.\n            If auto_nuisance is set to False, the nuisance regressors supplied\n            by the users together with DC components will be used as\n            nuisance time series.\n        scan_onsets: optional, list numpy arrays, shape=[runs,] for each.\n            Each item in the list specifies the indices of X which correspond\n            to the onset of each scanning run for one participant.\n            For example, if you have two experimental runs of\n            the first participant, each with 100 TRs, and one run of the\n            second participant, with 150 TR, then scan_onsets should be\n            [ndarry([0, 100]), ndarry([150])].\n            The effect of this argument is to make the inverse matrix\n            of the temporal covariance matrix of noise block-diagonal.\n            If you do not provide the argument, the program will\n            assume all data are from the same run for each participant.\n        \"\"\"\n\n        logger.info('Running Group Bayesian RSA (which can also analyze'\n                    ' data of a single participant). Voxel-specific parameters'\n                    'are all marginalized.')\n        self.random_state_ = check_random_state(self.random_state)\n        # setting random seed\n        logger.debug('RandState set to {}'.format(self.random_state_))\n        # Checking all inputs.\n        X = self._check_data_GBRSA(X)\n        design = self._check_design_GBRSA(design, X)\n        nuisance = self._check_nuisance_GBRSA(\n            copy.deepcopy(nuisance), X)\n        # The reason that we use copy of nuisance is because they\n        # may be modified inside our code.\n\n        scan_onsets = self._check_scan_onsets_GBRSA(scan_onsets, X)\n        # Run Marginalized Bayesian RSA\n        # Note that we have a change of notation here.\n        # Within _fit_RSA_marginalized, design matrix is named X\n        # and data is named Y, to reflect the\n        # generative model that data Y is generated by mixing the response\n        # X to experiment conditions and other neural activity.\n        # However, in fit(), we keep the scikit-learn API that\n        # X is the input data to fit and y, a reserved name not used, is\n        # the label to map to from X.\n        assert self.SNR_bins >= 10 and self.SNR_prior != 'equal' or \\\n            self.SNR_bins == 1 and self.SNR_prior == 'equal', \\\n            'At least 10 bins are required to perform the numerical'\\\n            ' integration over SNR, unless choosing SNR_prior=''equal'','\\\n            ' in which case SNR_bins should be 1.'\n        assert self.rho_bins >= 10, \\\n            'At least 10 bins are required to perform the numerical'\\\n            ' integration over rho'\n        assert self.logS_range * 6 / self.SNR_bins < 0.5 \\\n            or self.SNR_prior != 'lognorm', \\\n            'The minimum grid of log(SNR) should not be larger than 0.5 '\\\n            'if log normal prior is chosen for SNR.' \\\n            ' Please consider increasing SNR_bins or reducing logS_range'\n        self.n_subj_ = len(X)\n        self.n_V_ = [None] * self.n_subj_\n        for subj, x in enumerate(X):\n            self.n_V_[subj] = x.shape[1]\n        if self.auto_nuisance:\n            if self.n_nureg is None:\n                logger.info('numbers of nuisance regressors are determined '\n                            'automatically.')\n                n_runs = np.zeros(self.n_subj_)\n                n_comps = np.ones(self.n_subj_)\n                for s_id in np.arange(self.n_subj_):\n                    # For each subject, determine the number of nuisance\n                    # regressors needed to account for the covariance\n                    # in residuals.\n                    # Residual is calculated by regrssing\n                    # out the design matrix and DC component and linear trend\n                    # from data of each run.\n                    run_TRs, n_runs[s_id] = self._run_TR_from_scan_onsets(\n                        X[s_id].shape[0], scan_onsets[s_id])\n                    ts_dc = self._gen_legendre(run_TRs, [0])\n                    _, ts_base, _ = self._merge_DC_to_base(\n                        ts_dc, nuisance[s_id], False)\n                    ts_reg = np.concatenate((ts_base, design[s_id]), axis=1)\n                    beta_hat = np.linalg.lstsq(ts_reg, X[s_id])[0]\n                    residuals = X[s_id] - np.dot(ts_reg, beta_hat)\n\n                    n_comps[s_id] = np.min(\n                        [np.max([Ncomp_SVHT_MG_DLD_approx(\n                            residuals, self.nureg_zscore), 1]),\n                         np.linalg.matrix_rank(residuals) - 1])\n                    # n_nureg_ should not exceed the rank of\n                    # residual minus 1.\n                self.n_nureg_ = n_comps\n                logger.info('Use {} nuisance regressors to model the spatial '\n                            'correlation in noise.'.format(self.n_nureg_))\n            else:\n                self.n_nureg_ = self.n_nureg * np.ones(self.n_subj_)\n            self.n_nureg_ = np.int32(self.n_nureg_)\n\n        self.beta0_null_, self.sigma_null_, self.rho_null_, self.X0_null_,\\\n            self._LL_null_train_ = self._fit_RSA_marginalized_null(\n                Y=X, X_base=nuisance, scan_onsets=scan_onsets)\n        self.U_, self.L_, self.nSNR_, self.beta_, self.beta0_,\\\n            self.sigma_, self.rho_, self.X0_, self._LL_train_ = \\\n            self._fit_RSA_marginalized(\n                X=design, Y=X, X_base=nuisance,\n                scan_onsets=scan_onsets)\n\n        self.C_ = utils.cov2corr(self.U_)\n        self.design_ = design.copy()\n\n        self._rho_design_ = [None] * self.n_subj_\n        self._sigma2_design_ = [None] * self.n_subj_\n        self._rho_X0_ = [None] * self.n_subj_\n        self._sigma2_X0_ = [None] * self.n_subj_\n        self._rho_X0_null_ = [None] * self.n_subj_\n        self._sigma2_X0_null_ = [None] * self.n_subj_\n        for subj in np.arange(self.n_subj_):\n            self._rho_design_[subj], self._sigma2_design_[subj] = \\\n                self._est_AR1(self.design_[subj], same_para=True)\n            self._rho_X0_[subj], self._sigma2_X0_[subj] = \\\n                self._est_AR1(self.X0_[subj])\n            self._rho_X0_null_[subj], self._sigma2_X0_null_[subj] =\\\n                self._est_AR1(self.X0_null_[subj])\n        # AR(1) parameters of the design matrix and nuisance regressors,\n        # which will be used in transform or score.\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse the model to estimate the time course of response to each condition (ts), and the time course unrelated to task (ts0) which is spread across the brain. This is equivalent to \"decoding\" the design matrix and nuisance regressors from a new dataset different from the training dataset on which fit() was applied. An AR(1) smooth prior is imposed on the decoded ts and ts0 with the AR(1) parameters learnt from the corresponding time courses in the training data. Parameters ---------- X : list of 2-D arrays. For each item, shape=[time_points, voxels] New fMRI data of the same subjects. The voxels should match those used in the fit() function. The size of the list should match the size of the list X fed to fit(), with each item in the list corresponding to data from the same subject in the X fed to fit(). If you do not need to transform some subjects' data, leave the entry corresponding to that subject as None. If data are z-scored when fitting the model, data should be z-scored as well when calling transform() y : not used (as it is unsupervised learning) scan_onsets : list of 1-D numpy arrays, Each array corresponds to the onsets of scans in the data X for the particular subject. If not provided, data will be assumed to be acquired in a continuous scan. Returns ------- ts : list of 2-D arrays. For each, shape = [time_points, condition] The estimated response to the cognitive dimensions \uff08task dimensions) whose response amplitudes were estimated during the fit step. One item for each subject. If some subjects' data are not provided, None will be returned. ts0: list of 2-D array. For each, shape = [time_points, n_nureg] The estimated time courses spread across the brain, with the loading weights estimated during the fit step. One item for each subject. If some subjects' data are not provided, None will be returned.", "response": "def transform(self, X, y=None, scan_onsets=None):\n        \"\"\" Use the model to estimate the time course of response to\n            each condition (ts), and the time course unrelated to task\n            (ts0) which is spread across the brain.\n            This is equivalent to \"decoding\" the design matrix and\n            nuisance regressors from a new dataset different from the\n            training dataset on which fit() was applied. An AR(1) smooth\n            prior is imposed on the decoded ts and ts0 with the AR(1)\n            parameters learnt from the corresponding time courses in the\n            training data.\n\n        Parameters\n        ----------\n        X : list of 2-D arrays. For each item, shape=[time_points, voxels]\n            New fMRI data of the same subjects. The voxels should\n            match those used in the fit() function.\n            The size of the list should match the size of the list X fed\n            to fit(), with each item in the list corresponding to data\n            from the same subject in the X fed to fit(). If you do not\n            need to transform some subjects' data, leave the entry\n            corresponding to that subject as None.\n            If data are z-scored when fitting the model,\n            data should be z-scored as well when calling transform()\n        y : not used (as it is unsupervised learning)\n        scan_onsets : list of 1-D numpy arrays,\n            Each array corresponds to the onsets of\n            scans in the data X for the particular subject.\n            If not provided, data will be assumed\n            to be acquired in a continuous scan.\n\n        Returns\n        -------\n        ts : list of 2-D arrays. For each, shape = [time_points, condition]\n            The estimated response to the cognitive dimensions\n            \uff08task dimensions) whose response amplitudes were estimated\n            during the fit step.\n            One item for each subject. If some subjects' data are\n            not provided, None will be returned.\n        ts0: list of 2-D array. For each, shape = [time_points, n_nureg]\n            The estimated time courses spread across the brain, with the\n            loading weights estimated during the fit step.\n            One item for each subject. If some subjects' data are\n            not provided, None will be returned.\n        \"\"\"\n        X = self._check_data_GBRSA(X, for_fit=False)\n        scan_onsets = self._check_scan_onsets_GBRSA(scan_onsets, X)\n        assert len(X) == self.n_subj_\n        ts = [None] * self.n_subj_\n        ts0 = [None] * self.n_subj_\n        log_p = [None] * self.n_subj_\n        for i, x in enumerate(X):\n            if x is not None:\n                s = scan_onsets[i]\n                ts[i], ts0[i], log_p[i] = self._transform(\n                    Y=x, scan_onsets=s, beta=self.beta_[i],\n                    beta0=self.beta0_[i], rho_e=self.rho_[i],\n                    sigma_e=self.sigma_[i], rho_X=self._rho_design_[i],\n                    sigma2_X=self._sigma2_design_[i],\n                    rho_X0=self._rho_X0_[i], sigma2_X0=self._sigma2_X0_[i])\n        return ts, ts0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit the model for the marginalized version of the RSA - related variable.", "response": "def _fit_RSA_marginalized(self, X, Y, X_base,\n                              scan_onsets=None):\n        \"\"\" The major utility of fitting Bayesian RSA\n            (marginalized version).\n            Note that there is a naming change of variable. X in fit()\n            is changed to Y here, and design in fit() is changed to X here.\n            This is because we follow the tradition that X expresses the\n            variable defined (controlled) by the experimenter, i.e., the\n            time course of experimental conditions convolved by an HRF,\n            and Y expresses data.\n            However, in wrapper function fit(), we follow the naming\n            routine of scikit-learn.\n        \"\"\"\n        rank = self.rank\n        n_subj = len(Y)\n        n_V = [np.size(y, axis=1) for y in Y]\n        n_T = [np.size(y, axis=0) for y in Y]\n        n_C = np.size(X[0], axis=1)\n        l_idx, rank = self._chol_idx(n_C, rank)\n        n_l = np.size(l_idx[0])  # the number of parameters for L\n\n        t_start = time.time()\n\n        logger.info('Starting to fit the model. Maximum iteration: '\n                    '{}.'.format(self.n_iter))\n\n        # log_SNR_grids, SNR_weights \\\n        #     = np.polynomial.hermite.hermgauss(SNR_bins)\n        # SNR_weights = SNR_weights / np.pi**0.5\n        # SNR_grids = np.exp(log_SNR_grids * self.logS_range * 2**.5)\n\n        SNR_grids, SNR_weights = self._set_SNR_grids()\n        logger.info('The grids of pseudo-SNR used for numerical integration '\n                    'is {}.'.format(SNR_grids))\n        assert np.max(SNR_grids) < 1e10, \\\n            'ATTENTION!! The range of grids of pseudo-SNR' \\\n            ' to be marginalized is too large. Please ' \\\n            'consider reducing logS_range to 1 or 2'\n        rho_grids, rho_weights = self._set_rho_grids()\n        logger.info('The grids of rho used to do numerical integration '\n                    'is {}.'.format(rho_grids))\n        n_grid = self.SNR_bins * self.rho_bins\n        log_weights = np.reshape(\n            np.log(SNR_weights[:, None]) + np.log(rho_weights), n_grid)\n        all_rho_grids = np.reshape(np.repeat(\n            rho_grids[None, :], self.SNR_bins, axis=0), n_grid)\n        all_SNR_grids = np.reshape(np.repeat(\n            SNR_grids[:, None], self.rho_bins, axis=1), n_grid)\n        # Prepare the data for fitting. These pre-calculated matrices\n        # will be re-used a lot in evaluating likelihood function and\n        # gradient.\n        D = [None] * n_subj\n        F = [None] * n_subj\n        run_TRs = [None] * n_subj\n        n_run = [None] * n_subj\n        XTY = [None] * n_subj\n        XTDY = [None] * n_subj\n        XTFY = [None] * n_subj\n        YTY_diag = [None] * n_subj\n        YTDY_diag = [None] * n_subj\n        YTFY_diag = [None] * n_subj\n        XTX = [None] * n_subj\n        XTDX = [None] * n_subj\n        XTFX = [None] * n_subj\n\n        X0TX0 = [None] * n_subj\n        X0TDX0 = [None] * n_subj\n        X0TFX0 = [None] * n_subj\n        XTX0 = [None] * n_subj\n        XTDX0 = [None] * n_subj\n        XTFX0 = [None] * n_subj\n        X0TY = [None] * n_subj\n        X0TDY = [None] * n_subj\n        X0TFY = [None] * n_subj\n        X0 = [None] * n_subj\n        X_res = [None] * n_subj\n        n_X0 = [None] * n_subj\n        idx_DC = [None] * n_subj\n        log_fixed_terms = [None] * n_subj\n\n        # Initialization for L.\n        # There are several possible ways of initializing the covariance.\n        # (1) start from the point estimation of covariance\n\n        cov_point_est = np.zeros((n_C, n_C))\n\n        for subj in range(n_subj):\n            D[subj], F[subj], run_TRs[subj], n_run[subj] = self._prepare_DF(\n                n_T[subj], scan_onsets=scan_onsets[subj])\n            XTY[subj], XTDY[subj], XTFY[subj], YTY_diag[subj], \\\n                YTDY_diag[subj], YTFY_diag[subj], XTX[subj], XTDX[subj], \\\n                XTFX[subj] = self._prepare_data_XY(\n                    X[subj], Y[subj], D[subj], F[subj])\n            # The contents above stay fixed during fitting.\n\n            # Initializing X0 as DC baseline\n            # DC component will be added to the nuisance regressors.\n            # In later steps, we do not need to add DC components again\n            X0TX0[subj], X0TDX0[subj], X0TFX0[subj], XTX0[subj], XTDX0[subj], \\\n                XTFX0[subj], X0TY[subj], X0TDY[subj], X0TFY[subj], X0[subj], \\\n                X_base[subj], n_X0[subj], idx_DC[subj] = \\\n                self._prepare_data_XYX0(\n                    X[subj], Y[subj], X_base[subj], None, D[subj], F[subj],\n                    run_TRs[subj], no_DC=False)\n\n            X_joint = np.concatenate((X0[subj], X[subj]), axis=1)\n            beta_hat = np.linalg.lstsq(X_joint, Y[subj])[0]\n            residual = Y[subj] - np.dot(X_joint, beta_hat)\n            # point estimates of betas and fitting residuals without assuming\n            # the Bayesian model underlying RSA.\n\n            cov_point_est += np.cov(beta_hat[n_X0[subj]:, :]\n                                    / np.std(residual, axis=0))\n            log_fixed_terms[subj] = - (n_T[subj] - n_X0[subj]) \\\n                / 2 * np.log(2 * np.pi) + n_run[subj] \\\n                / 2 * np.log(1 - all_rho_grids**2) \\\n                + scipy.special.gammaln(\n                    (n_T[subj] - n_X0[subj] - 2) / 2) \\\n                + (n_T[subj] - n_X0[subj] - 2) / 2 * np.log(2)\n            # These are terms in the log likelihood that do not\n            # depend on L. Notice that the last term comes from\n            # ther term of marginalizing sigma. We take the 2 in\n            # the denominator out. Accordingly, the \"denominator\"\n            # variable in the _raw_loglike_grids() function is not\n            # divided by 2\n        cov_point_est = cov_point_est / n_subj\n        current_vec_U_chlsk_l = np.linalg.cholesky(\n            (cov_point_est + np.eye(n_C)) / 2)[l_idx]\n        # We use the average of covariance of point estimation and an identity\n        # matrix as the initial value of the covariance matrix, just in case\n        # the user provides data in which n_V is smaller than n_C.\n\n        # (2) start from identity matrix\n\n        # current_vec_U_chlsk_l = np.eye(n_C)[l_idx]\n\n        # (3) random initialization\n\n        # current_vec_U_chlsk_l = self.random_state_.randn(n_l)\n        # vectorized version of L, Cholesky factor of U, the shared\n        # covariance matrix of betas across voxels.\n        L = np.zeros((n_C, rank))\n        L[l_idx] = current_vec_U_chlsk_l\n\n        X0TAX0 = [None] * n_subj\n        X0TAX0_i = [None] * n_subj\n        XTAcorrX = [None] * n_subj\n        s2XTAcorrX = [None] * n_subj\n        YTAcorrY_diag = [None] * n_subj\n        XTAcorrY = [None] * n_subj\n        sXTAcorrY = [None] * n_subj\n        X0TAY = [None] * n_subj\n        XTAX0 = [None] * n_subj\n        half_log_det_X0TAX0 = [None] * n_subj\n        s_post = [None] * n_subj\n        rho_post = [None] * n_subj\n        sigma_post = [None] * n_subj\n        beta_post = [None] * n_subj\n        beta0_post = [None] * n_subj\n        # The contents below can be updated during fitting.\n        # e.g., X0 will be re-estimated\n        logger.info('start real fitting')\n        LL = np.zeros(n_subj)\n        for it in range(self.n_iter):\n            logger.info('Iteration {}'.format(it))\n            # Re-estimate part of X0: X_res\n            for subj in range(n_subj):\n                if self.auto_nuisance and it > 0:\n                    residuals = Y[subj] - np.dot(X[subj], beta_post[subj]) \\\n                        - np.dot(\n                            X_base[subj],\n                            beta0_post[subj][:np.shape(X_base[subj])[1], :])\n                    X_res[subj] = self.nureg_method(\n                        self.n_nureg_[subj]).fit_transform(\n                        self.preprocess_residual(residuals))\n                    X0TX0[subj], X0TDX0[subj], X0TFX0[subj], XTX0[subj],\\\n                        XTDX0[subj], XTFX0[subj], X0TY[subj], X0TDY[subj], \\\n                        X0TFY[subj], X0[subj], X_base[subj], n_X0[subj], _ = \\\n                        self._prepare_data_XYX0(\n                            X[subj], Y[subj], X_base[subj], X_res[subj],\n                            D[subj], F[subj], run_TRs[subj], no_DC=True)\n                X0TAX0[subj], X0TAX0_i[subj], XTAcorrX[subj], XTAcorrY[subj],\\\n                    YTAcorrY_diag[subj], X0TAY[subj], XTAX0[subj] \\\n                    = self._precompute_ar1_quad_forms_marginalized(\n                        XTY[subj], XTDY[subj], XTFY[subj], YTY_diag[subj],\n                        YTDY_diag[subj], YTFY_diag[subj], XTX[subj],\n                        XTDX[subj], XTFX[subj], X0TX0[subj], X0TDX0[subj],\n                        X0TFX0[subj], XTX0[subj], XTDX0[subj], XTFX0[subj],\n                        X0TY[subj], X0TDY[subj], X0TFY[subj], rho_grids,\n                        n_V[subj], n_X0[subj])\n\n                # Now we expand to another dimension including SNR\n                # and collapse the dimension again.\n                half_log_det_X0TAX0[subj], X0TAX0[subj], X0TAX0_i[subj], \\\n                    s2XTAcorrX[subj], YTAcorrY_diag[subj], sXTAcorrY[subj], \\\n                    X0TAY[subj], XTAX0[subj] = self._matrix_flattened_grid(\n                        X0TAX0[subj], X0TAX0_i[subj], SNR_grids,\n                        XTAcorrX[subj], YTAcorrY_diag[subj], XTAcorrY[subj],\n                        X0TAY[subj], XTAX0[subj], n_C, n_V[subj], n_X0[subj],\n                        n_grid)\n\n            res = scipy.optimize.minimize(\n                self._sum_loglike_marginalized, current_vec_U_chlsk_l\n                + self.random_state_.randn(n_l) *\n                np.linalg.norm(current_vec_U_chlsk_l)\n                / n_l**0.5 * np.exp(-it / self.n_iter\n                                    * self.anneal_speed - 1),\n                args=(s2XTAcorrX, YTAcorrY_diag, sXTAcorrY,\n                      half_log_det_X0TAX0,\n                      log_weights, log_fixed_terms,\n                      l_idx, n_C, n_T, n_V, n_X0,\n                      n_grid, rank),\n                method=self.optimizer, jac=True, tol=self.tol,\n                options=self.minimize_options)\n            param_change = res.x - current_vec_U_chlsk_l\n            current_vec_U_chlsk_l = res.x.copy()\n\n            # Estimating a few parameters.\n            L[l_idx] = current_vec_U_chlsk_l\n            for subj in range(n_subj):\n                LL_raw, denominator, L_LAMBDA, L_LAMBDA_LT = \\\n                    self._raw_loglike_grids(\n                        L, s2XTAcorrX[subj], YTAcorrY_diag[subj],\n                        sXTAcorrY[subj], half_log_det_X0TAX0[subj],\n                        log_weights, log_fixed_terms[subj], n_C, n_T[subj],\n                        n_V[subj], n_X0[subj], n_grid, rank)\n                result_sum, max_value, result_exp = utils.sumexp_stable(LL_raw)\n                LL[subj] = np.sum(np.log(result_sum) + max_value)\n                weight_post = result_exp / result_sum\n                s_post[subj] = np.sum(all_SNR_grids[:, None] * weight_post,\n                                      axis=0)\n                # Mean-posterior estimate of SNR.\n                rho_post[subj] = np.sum(all_rho_grids[:, None] * weight_post,\n                                        axis=0)\n                # Mean-posterior estimate of rho.\n                sigma_means = denominator ** 0.5 \\\n                    * (np.exp(scipy.special.gammaln(\n                        (n_T[subj] - n_X0[subj] - 3) / 2)\n                        - scipy.special.gammaln(\n                        (n_T[subj] - n_X0[subj] - 2) / 2)) / 2**0.5)\n                sigma_post[subj] = np.sum(sigma_means * weight_post, axis=0)\n                # The mean of inverse-Gamma distribution is beta/(alpha-1)\n                # The mode is beta/(alpha+1). Notice that beta here does not\n                # refer to the brain activation, but the scale parameter of\n                # inverse-Gamma distribution. In the _UV version, we use the\n                # maximum likelihood estimate of sigma^2. So we divide by\n                # (alpha+1), which is (n_T - n_X0).\n                beta_post[subj] = np.zeros((n_C, n_V[subj]))\n                beta0_post[subj] = np.zeros((n_X0[subj], n_V[subj]))\n                for grid in range(n_grid):\n                    beta_post[subj] += np.dot(L_LAMBDA_LT[grid, :, :],\n                                              sXTAcorrY[subj][grid, :, :])\\\n                        * all_SNR_grids[grid] \\\n                        * weight_post[grid, :]\n                    beta0_post[subj] += weight_post[grid, :] * np.dot(\n                        X0TAX0_i[subj][grid, :, :],\n                        (X0TAY[subj][grid, :, :]\n                         - np.dot(np.dot(XTAX0[subj][grid, :, :].T,\n                                         L_LAMBDA_LT[grid, :, :]),\n                                  sXTAcorrY[subj][grid, :, :])\n                         * all_SNR_grids[grid]))\n            if np.max(np.abs(param_change)) < self.tol:\n                logger.info('The change of parameters is smaller than '\n                            'the tolerance value {}. Fitting is finished '\n                            'after {} iterations'.format(self.tol, it + 1))\n                break\n        for subj in range(n_subj):\n            if idx_DC[subj].size > 1:\n                collapsed_DC = np.sum(X0[subj][:, idx_DC[subj]], axis=1)\n                X0[subj] = np.insert(np.delete(X0[subj], idx_DC[subj], axis=1),\n                                     0, collapsed_DC, axis=1)\n                collapsed_beta0 = np.mean(beta0_post[subj][idx_DC[subj], :],\n                                          axis=0)\n                beta0_post[subj] = np.insert(\n                    np.delete(beta0_post[subj], idx_DC[subj], axis=0),\n                    0, collapsed_beta0, axis=0)\n        t_finish = time.time()\n        logger.info(\n            'total time of fitting: {} seconds'.format(t_finish - t_start))\n        return np.dot(L, L.T), L, s_post, \\\n            beta_post, beta0_post, sigma_post, \\\n            rho_post, X0, LL"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fit_RSA_marginalized_null(self, Y, X_base,\n                                   scan_onsets):\n        \"\"\" The marginalized version of the null model for Bayesian RSA.\n            The null model assumes no task-related response to the\n            design matrix.\n            Note that there is a naming change of variable. X in fit()\n            is changed to Y here.\n            This is because we follow the tradition that Y corresponds\n            to data.\n            However, in wrapper function fit(), we follow the naming\n            routine of scikit-learn.\n        \"\"\"\n        # Because there is nothing to learn that is shared across\n        # participants, we can run each subject in serial.\n        # The only fitting required is to re-estimate X0 after\n        # each iteration\n        n_subj = len(Y)\n        t_start = time.time()\n        logger.info('Starting to fit the model. Maximum iteration: '\n                    '{}.'.format(self.n_iter))\n\n        rho_grids, rho_weights = self._set_rho_grids()\n        logger.info('The grids of rho used to do numerical integration '\n                    'is {}.'.format(rho_grids))\n        n_grid = self.rho_bins\n        log_weights = np.log(rho_weights)\n        rho_post = [None] * n_subj\n        sigma_post = [None] * n_subj\n        beta0_post = [None] * n_subj\n        X0 = [None] * n_subj\n        LL_null = np.zeros(n_subj)\n        for subj in range(n_subj):\n            logger.debug('Running on subject {}.'.format(subj))\n            [n_T, n_V] = np.shape(Y[subj])\n            D, F, run_TRs, n_run = self._prepare_DF(\n                n_T, scan_onsets=scan_onsets[subj])\n            YTY_diag = np.sum(Y[subj] * Y[subj], axis=0)\n            YTDY_diag = np.sum(Y[subj] * np.dot(D, Y[subj]), axis=0)\n            YTFY_diag = np.sum(Y[subj] * np.dot(F, Y[subj]), axis=0)\n\n            # Add DC components capturing run-specific baselines.\n            X_DC = self._gen_X_DC(run_TRs)\n            X_DC, X_base[subj], idx_DC = self._merge_DC_to_base(\n                X_DC, X_base[subj], no_DC=False)\n            X_res = np.empty((n_T, 0))\n            for it in range(0, self.n_iter):\n                X0[subj] = np.concatenate(\n                    (X_base[subj], X_res), axis=1)\n                n_X0 = X0[subj].shape[1]\n                X0TX0, X0TDX0, X0TFX0 = self._make_templates(\n                    D, F, X0[subj], X0[subj])\n                X0TY, X0TDY, X0TFY = self._make_templates(\n                    D, F, X0[subj], Y[subj])\n\n                YTAY_diag = YTY_diag - rho_grids[:, None] * YTDY_diag \\\n                    + rho_grids[:, None]**2 * YTFY_diag\n                # dimension: #rho*space,\n                # A/sigma2 is the inverse of noise covariance matrix.\n                # YTAY means Y'AY\n\n                X0TAX0 = X0TX0[None, :, :] \\\n                    - rho_grids[:, None, None] \\\n                    * X0TDX0[None, :, :] \\\n                    + rho_grids[:, None, None]**2 \\\n                    * X0TFX0[None, :, :]\n                # dimension: #rho*#baseline*#baseline\n                X0TAY = X0TY - rho_grids[:, None, None] * X0TDY \\\n                    + rho_grids[:, None, None]**2 * X0TFY\n                # dimension: #rho*#baseline*space\n                X0TAX0_i = np.linalg.solve(\n                    X0TAX0, np.identity(n_X0)[None, :, :])\n                # dimension: #rho*#baseline*#baseline\n                YTAcorrY_diag = np.empty(np.shape(YTAY_diag))\n                for i_r in range(np.size(rho_grids)):\n                    YTAcorrY_diag[i_r, :] = YTAY_diag[i_r, :] \\\n                        - np.sum(X0TAY[i_r, :, :] * np.dot(\n                            X0TAX0_i[i_r, :, :], X0TAY[i_r, :, :]),\n                        axis=0)\n\n                log_fixed_terms = - (n_T - n_X0) / 2 * np.log(2 * np.pi)\\\n                    + n_run / 2 * np.log(1 - rho_grids**2) \\\n                    + scipy.special.gammaln((n_T - n_X0 - 2) / 2) \\\n                    + (n_T - n_X0 - 2) / 2 * np.log(2)\n                # These are terms in the log likelihood that do not\n                # depend on L. Notice that the last term comes from\n                # ther term of marginalizing sigma. We take the 2 in\n                # the denominator out. Accordingly, the \"denominator\"\n                # variable in the _raw_loglike_grids() function is not\n                # divided by 2\n                half_log_det_X0TAX0 = self._half_log_det(X0TAX0)\n\n                LL_raw = -half_log_det_X0TAX0[:, None] \\\n                    - (n_T - n_X0 - 2) / 2 * np.log(YTAcorrY_diag) \\\n                    + log_weights[:, None] + log_fixed_terms[:, None]\n                # dimension: n_grid * space\n                # The log likelihood at each pair of values of rho1.\n                # half_log_det_X0TAX0 is 0.5*log(det(X0TAX0)) with the size of\n                # number of parameter grids. So is the size of log_weights\n\n                result_sum, max_value, result_exp = utils.sumexp_stable(LL_raw)\n                weight_post = result_exp / result_sum\n                rho_post[subj] = np.sum(rho_grids[:, None] * weight_post,\n                                        axis=0)\n                # Mean-posterior estimate of rho.\n                sigma_means = YTAcorrY_diag ** 0.5 \\\n                    * (np.exp(scipy.special.gammaln((n_T - n_X0 - 3) / 2)\n                              - scipy.special.gammaln((n_T - n_X0 - 2) / 2))\n                       / 2**0.5)\n                sigma_post[subj] = np.sum(sigma_means * weight_post, axis=0)\n                beta0_post[subj] = np.zeros((n_X0, n_V))\n                for grid in range(n_grid):\n                    beta0_post[subj] += weight_post[grid, :] * np.dot(\n                        X0TAX0_i[grid, :, :], X0TAY[grid, :, :])\n                if self.auto_nuisance:\n                    residuals = Y[subj] - np.dot(\n                        X_base[subj],\n                        beta0_post[subj][:np.size(X_base[subj], 1), :])\n                    X_res_new = self.nureg_method(\n                        self.n_nureg_[subj]).fit_transform(\n                        self.preprocess_residual(residuals))\n                    if it >= 1:\n                        if np.max(np.abs(X_res_new - X_res)) <= self.tol:\n                            logger.info('The change of X_res is '\n                                        'smaller than the tolerance value {}.'\n                                        'Fitting is finished after {} '\n                                        'iterations'.format(self.tol, it + 1))\n                            break\n                    X_res = X_res_new\n            if idx_DC.size > 1:\n                collapsed_DC = np.sum(X0[subj][:, idx_DC], axis=1)\n                X0[subj] = np.insert(np.delete(X0[subj], idx_DC, axis=1), 0,\n                                     collapsed_DC, axis=1)\n                collapsed_beta0 = np.mean(beta0_post[subj][idx_DC, :], axis=0)\n                beta0_post[subj] = np.insert(\n                    np.delete(beta0_post[subj], idx_DC, axis=0),\n                    0, collapsed_beta0, axis=0)\n            LL_null[subj] = np.sum(np.log(result_sum) + max_value)\n        t_finish = time.time()\n        logger.info(\n            'total time of fitting: {} seconds'.format(t_finish - t_start))\n        return beta0_post, sigma_post, rho_post, X0, LL_null", "response": "Fits the null model for Bayesian RSA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bin_exp(self, n_bin, scale=1.0):\n        boundaries = np.flip(scipy.stats.expon.isf(\n            np.linspace(0, 1, n_bin + 1),\n            scale=scale), axis=0)\n        bins = np.empty(n_bin)\n        for i in np.arange(n_bin):\n            bins[i] = utils.center_mass_exp(\n                (boundaries[i], boundaries[i + 1]), scale=scale)\n        return bins", "response": "Calculates the bin locations of the exponential distribution of the in - memory exponential distribution of the in - memory exponential distribution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_SNR_grids(self):\n        if self.SNR_prior == 'unif':\n            SNR_grids = np.linspace(0, 1, self.SNR_bins)\n            SNR_weights = np.ones(self.SNR_bins) / (self.SNR_bins - 1)\n            SNR_weights[0] = SNR_weights[0] / 2.0\n            SNR_weights[-1] = SNR_weights[-1] / 2.0\n        elif self.SNR_prior == 'lognorm':\n            dist = scipy.stats.lognorm\n            alphas = np.arange(np.mod(self.SNR_bins, 2),\n                               self.SNR_bins + 2, 2) / self.SNR_bins\n            # The goal here is to divide the area under the pdf curve\n            # to segments representing equal probabilities.\n            bounds = dist.interval(alphas, (self.logS_range,))\n            bounds = np.unique(bounds)\n            # bounds contain the boundaries which equally separate\n            # the probability mass of the distribution\n            SNR_grids = np.zeros(self.SNR_bins)\n            for i in np.arange(self.SNR_bins):\n                SNR_grids[i] = dist.expect(\n                    lambda x: x, args=(self.logS_range,),\n                    lb=bounds[i], ub=bounds[i + 1]) * self.SNR_bins\n            # Center of mass of each segment between consecutive\n            # bounds are set as the grids for SNR.\n            SNR_weights = np.ones(self.SNR_bins) / self.SNR_bins\n        elif self.SNR_prior == 'exp':\n            SNR_grids = self._bin_exp(self.SNR_bins)\n            SNR_weights = np.ones(self.SNR_bins) / self.SNR_bins\n        else:\n            SNR_grids = np.ones(1)\n            SNR_weights = np.ones(1)\n        SNR_weights = SNR_weights / np.sum(SNR_weights)\n        return SNR_grids, SNR_weights", "response": "Set the grids and weights for the SNR parameters for the current class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the grids and weights for rho used in numerical integration of AR ( 1 ) parameters.", "response": "def _set_rho_grids(self):\n        \"\"\" Set the grids and weights for rho used in numerical integration\n            of AR(1) parameters.\n        \"\"\"\n        rho_grids = np.arange(self.rho_bins) * 2 / self.rho_bins - 1 \\\n            + 1 / self.rho_bins\n        rho_weights = np.ones(self.rho_bins) / self.rho_bins\n        return rho_grids, rho_weights"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the Robust Shared Response Model for a set of timepoints.", "response": "def fit(self, X):\n        \"\"\"Compute the Robust Shared Response Model\n\n        Parameters\n        ----------\n\n        X : list of 2D arrays, element i has shape=[voxels_i, timepoints]\n            Each element in the list contains the fMRI data of one subject.\n        \"\"\"\n        logger.info('Starting RSRM')\n\n        # Check that the regularizer value is positive\n        if 0.0 >= self.lam:\n            raise ValueError(\"Gamma parameter should be positive.\")\n\n        # Check the number of subjects\n        if len(X) <= 1:\n            raise ValueError(\"There are not enough subjects in the input \"\n                             \"data to train the model.\")\n\n        # Check for input data sizes\n        if X[0].shape[1] < self.features:\n            raise ValueError(\n                \"There are not enough timepoints to train the model with \"\n                \"{0:d} features.\".format(self.features))\n\n        # Check if all subjects have same number of TRs for alignment\n        number_trs = X[0].shape[1]\n        number_subjects = len(X)\n        for subject in range(number_subjects):\n            assert_all_finite(X[subject])\n            if X[subject].shape[1] != number_trs:\n                raise ValueError(\"Different number of alignment timepoints \"\n                                 \"between subjects.\")\n\n        # Create a new random state\n        self.random_state_ = np.random.RandomState(self.rand_seed)\n\n        # Run RSRM\n        self.w_, self.r_, self.s_ = self._rsrm(X)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing the model to transform new data to Shared Response space", "response": "def transform(self, X):\n        \"\"\"Use the model to transform new data to Shared Response space\n\n        Parameters\n        ----------\n\n        X : list of 2D arrays, element i has shape=[voxels_i, timepoints_i]\n            Each element in the list contains the fMRI data of one subject.\n\n        Returns\n        -------\n\n        r : list of 2D arrays, element i has shape=[features_i, timepoints_i]\n            Shared responses from input data (X)\n\n        s : list of 2D arrays, element i has shape=[voxels_i, timepoints_i]\n            Individual data obtained from fitting model to input data (X)\n        \"\"\"\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of subjects\n        if len(X) != len(self.w_):\n            raise ValueError(\"The number of subjects does not match the one\"\n                             \" in the model.\")\n\n        r = [None] * len(X)\n        s = [None] * len(X)\n        for subject in range(len(X)):\n            if X[subject] is not None:\n                r[subject], s[subject] = self._transform_new_data(X[subject],\n                                                                  subject)\n\n        return r, s"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms new data for a subject by projecting to the shared subspace and computing the individual information.", "response": "def _transform_new_data(self, X, subject):\n        \"\"\"Transform new data for a subjects by projecting to the shared subspace and\n        computing the individual information.\n\n        Parameters\n        ----------\n\n        X : array, shape=[voxels, timepoints]\n            The fMRI data of the subject.\n\n        subject : int\n            The subject id.\n\n        Returns\n        -------\n\n        R : array, shape=[features, timepoints]\n            Shared response from input data (X)\n\n        S : array, shape=[voxels, timepoints]\n            Individual data obtained from fitting model to input data (X)\n        \"\"\"\n        S = np.zeros_like(X)\n        R = None\n        for i in range(self.n_iter):\n            R = self.w_[subject].T.dot(X - S)\n            S = self._shrink(X - self.w_[subject].dot(R), self.lam)\n        return R, S"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms a new subject using the existing model.", "response": "def transform_subject(self, X):\n        \"\"\"Transform a new subject using the existing model\n\n        Parameters\n        ----------\n\n        X : 2D array, shape=[voxels, timepoints]\n            The fMRI data of the new subject.\n\n        Returns\n        -------\n\n        w : 2D array, shape=[voxels, features]\n            Orthogonal mapping `W_{new}` for new subject\n\n        s : 2D array, shape=[voxels, timepoints]\n            Individual term `S_{new}` for new subject\n        \"\"\"\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of TRs in the subject\n        if X.shape[1] != self.r_.shape[1]:\n            raise ValueError(\"The number of timepoints(TRs) does not match the\"\n                             \"one in the model.\")\n\n        s = np.zeros_like(X)\n        for i in range(self.n_iter):\n            w = self._update_transform_subject(X, s, self.r_)\n            s = self._shrink(X - w.dot(self.r_), self.lam)\n\n        return w, s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _init_transforms(self, subjs, voxels, features, random_state):\n\n        # Init the Random seed generator\n        np.random.seed(self.rand_seed)\n\n        # Draw a random W for each subject\n        W = [random_state.random_sample((voxels[i], features))\n             for i in range(subjs)]\n        # Make it orthogonal it with QR decomposition\n        for i in range(subjs):\n            W[i], _ = np.linalg.qr(W[i])\n\n        return W", "response": "Initialize the orthogonal matrices for each subject."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _objective_function(X, W, R, S, gamma):\n        subjs = len(X)\n        func = .0\n        for i in range(subjs):\n            func += 0.5 * np.sum((X[i] - W[i].dot(R) - S[i])**2) \\\n                    + gamma * np.sum(np.abs(S[i]))\n        return func", "response": "Evaluate the objective function for the individual entry set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_individual(X, W, R, gamma):\n        subjs = len(X)\n        S = []\n        for i in range(subjs):\n            S.append(RSRM._shrink(X[i] - W[i].dot(R), gamma))\n        return S", "response": "Update the individual components S_i."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the individual components S_i to empty.", "response": "def _init_individual(subjs, voxels, TRs):\n        \"\"\"Initializes the individual components `S_i` to empty (all zeros).\n\n        Parameters\n        ----------\n\n        subjs : int\n            The number of subjects.\n\n        voxels : list of int\n            A list with the number of voxels per subject.\n\n        TRs : int\n            The number of timepoints in the data.\n\n        Returns\n        -------\n\n        S : list of 2D array, element i has shape=[voxels_i, timepoints]\n            The individual component :math:`S_i` for each subject initialized\n            to zero.\n        \"\"\"\n        return [np.zeros((voxels[i], TRs)) for i in range(subjs)]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_shared_response(X, S, W, features):\n        subjs = len(X)\n        TRs = X[0].shape[1]\n        R = np.zeros((features, TRs))\n        # Project the subject data with the individual component removed into\n        # the shared subspace and average over all subjects.\n        for i in range(subjs):\n            R += W[i].T.dot(X[i]-S[i])\n        R /= subjs\n        return R", "response": "Update the shared response R."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the mappings Xi and Si for one subject.", "response": "def _update_transform_subject(Xi, Si, R):\n        \"\"\"Updates the mappings `W_i` for one subject.\n\n        Parameters\n        ----------\n\n        Xi : array, shape=[voxels, timepoints]\n            The fMRI data :math:`X_i` for aligning the subject.\n\n        Si : array, shape=[voxels, timepoints]\n            The individual component :math:`S_i` for the subject.\n\n        R : array, shape=[features, timepoints]\n            The shared response.\n\n        Returns\n        -------\n\n        Wi : array, shape=[voxels, features]\n            The orthogonal transform (mapping) :math:`W_i` for the subject.\n        \"\"\"\n        A = Xi.dot(R.T)\n        A -= Si.dot(R.T)\n        # Solve the Procrustes problem\n        U, _, V = np.linalg.svd(A, full_matrices=False)\n        return U.dot(V)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_transforms(X, S, R):\n        subjs = len(X)\n        W = []\n        for i in range(subjs):\n            W.append(RSRM._update_transform_subject(X[i], S[i], R))\n        return W", "response": "Updates the orthogonal transforms W_i for each subject."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _shrink(v, gamma):\n        pos = v > gamma\n        neg = v < -gamma\n        v[pos] -= gamma\n        v[neg] += gamma\n        v[np.logical_and(~pos, ~neg)] = .0\n        return v", "response": "Soft - shrinkage of an array with parameter gamma."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a confusion matrix for each subject in the subject list cm", "response": "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n    \"\"\"Plots a confusion matrix for each subject\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import math\n    plt.figure()\n    subjects = len(cm)\n    root_subjects = math.sqrt(subjects)\n    cols = math.ceil(root_subjects)\n    rows = math.ceil(subjects/cols)\n    classes = cm[0].shape[0]\n    for subject in range(subjects):\n        plt.subplot(rows, cols, subject+1)\n        plt.imshow(cm[subject], interpolation='nearest', cmap=plt.cm.bone)\n        plt.xticks(np.arange(classes), range(1, classes+1))\n        plt.yticks(np.arange(classes), range(1, classes+1))\n        cbar = plt.colorbar(ticks=[0.0, 1.0], shrink=0.6)\n        cbar.set_clim(0.0, 1.0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True label\")\n        plt.title(\"{0:d}\".format(subject + 1))\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmask image after optionally casting its type.", "response": "def mask_image(image: SpatialImage, mask: np.ndarray, data_type: type = None\n               ) -> np.ndarray:\n    \"\"\"Mask image after optionally casting its type.\n\n    Parameters\n    ----------\n    image\n        Image to mask. Can include time as the last dimension.\n    mask\n        Mask to apply. Must have the same shape as the image data.\n    data_type\n        Type to cast image to.\n\n    Returns\n    -------\n    np.ndarray\n        Masked image.\n\n    Raises\n    ------\n    ValueError\n        Image data and masks have different shapes.\n    \"\"\"\n    image_data = image.get_data()\n    if image_data.shape[:3] != mask.shape:\n        raise ValueError(\"Image data and mask have different shapes.\")\n    if data_type is not None:\n        cast_data = image_data.astype(data_type)\n    else:\n        cast_data = image_data\n    return cast_data[mask]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmask images with multiple masks.", "response": "def multimask_images(images: Iterable[SpatialImage],\n                     masks: Sequence[np.ndarray], image_type: type = None\n                     ) -> Iterable[Sequence[np.ndarray]]:\n    \"\"\"Mask images with multiple masks.\n\n    Parameters\n    ----------\n    images:\n        Images to mask.\n    masks:\n        Masks to apply.\n    image_type:\n        Type to cast images to.\n\n    Yields\n    ------\n    Sequence[np.ndarray]\n        For each mask, a masked image.\n    \"\"\"\n    for image in images:\n        yield [mask_image(image, mask, image_type) for mask in masks]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmasks images. Parameters ---------- images: Images to mask. mask: Mask to apply. image_type: Type to cast images to. Yields ------ np.ndarray Masked image.", "response": "def mask_images(images: Iterable[SpatialImage], mask: np.ndarray,\n                image_type: type = None) -> Iterable[np.ndarray]:\n    \"\"\"Mask images.\n\n    Parameters\n    ----------\n    images:\n        Images to mask.\n    mask:\n        Mask to apply.\n    image_type:\n        Type to cast images to.\n\n    Yields\n    ------\n    np.ndarray\n        Masked image.\n    \"\"\"\n    for images in multimask_images(images, (mask,), image_type):\n        yield images[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_masked_images(cls: Type[T], masked_images: Iterable[np.ndarray],\n                           n_subjects: int) -> T:\n        \"\"\"Create a new instance of MaskedMultiSubjecData from masked images.\n\n        Parameters\n        ----------\n        masked_images : iterator\n            Images from multiple subjects to stack along 3rd dimension\n        n_subjects : int\n            Number of subjects; must match the number of images\n\n        Returns\n        -------\n        T\n            A new instance of MaskedMultiSubjectData\n\n        Raises\n        ------\n        ValueError\n            Images have different shapes.\n\n            The number of images differs from n_subjects.\n        \"\"\"\n        images_iterator = iter(masked_images)\n        first_image = next(images_iterator)\n        first_image_shape = first_image.T.shape\n        result = np.empty((first_image_shape[0], first_image_shape[1],\n                           n_subjects))\n        for n_images, image in enumerate(itertools.chain([first_image],\n                                                         images_iterator)):\n            image = image.T\n            if image.shape != first_image_shape:\n                raise ValueError(\"Image {} has different shape from first \"\n                                 \"image: {} != {}\".format(n_images,\n                                                          image.shape,\n                                                          first_image_shape))\n            result[:, :, n_images] = image\n        n_images += 1\n        if n_images != n_subjects:\n            raise ValueError(\"n_subjects != number of images: {} != {}\"\n                             .format(n_subjects, n_images))\n        return result.view(cls)", "response": "Create a new instance of MaskedMultiSubjectData from a list of masked images."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract the condition labels.", "response": "def extract_labels(self) -> np.ndarray:\n        \"\"\"Extract condition labels.\n\n        Returns\n        -------\n        np.ndarray\n            The condition label of each epoch.\n        \"\"\"\n        condition_idxs, epoch_idxs, _ = np.where(self)\n        _, unique_epoch_idxs = np.unique(epoch_idxs, return_index=True)\n        return condition_idxs[unique_epoch_idxs]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_w_transforms(data, features, random_states, comm=MPI.COMM_SELF):\n    w = []\n    subjects = len(data)\n    voxels = np.empty(subjects, dtype=int)\n\n    # Set Wi to a random orthogonal voxels by features matrix\n    for subject in range(subjects):\n        if data[subject] is not None:\n            voxels[subject] = data[subject].shape[0]\n            rnd_matrix = random_states[subject].random_sample((\n                voxels[subject], features))\n            q, r = np.linalg.qr(rnd_matrix)\n            w.append(q)\n        else:\n            voxels[subject] = 0\n            w.append(None)\n    voxels = comm.allreduce(voxels, op=MPI.SUM)\n    return w, voxels", "response": "Initialize the orthogonal transforms for the SRM."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit(self, X, y=None):\n        logger.info('Starting Probabilistic SRM')\n\n        # Check the number of subjects\n        if len(X) <= 1:\n            raise ValueError(\"There are not enough subjects \"\n                             \"({0:d}) to train the model.\".format(len(X)))\n\n        # Check for input data sizes\n        number_subjects = len(X)\n        number_subjects_vec = self.comm.allgather(number_subjects)\n        for rank in range(self.comm.Get_size()):\n            if number_subjects_vec[rank] != number_subjects:\n                raise ValueError(\n                    \"Not all ranks have same number of subjects\")\n\n        # Collect size information\n        shape0 = np.zeros((number_subjects,), dtype=np.int)\n        shape1 = np.zeros((number_subjects,), dtype=np.int)\n\n        for subject in range(number_subjects):\n            if X[subject] is not None:\n                assert_all_finite(X[subject])\n                shape0[subject] = X[subject].shape[0]\n                shape1[subject] = X[subject].shape[1]\n\n        shape0 = self.comm.allreduce(shape0, op=MPI.SUM)\n        shape1 = self.comm.allreduce(shape1, op=MPI.SUM)\n\n        # Check if all subjects have same number of TRs\n        number_trs = np.min(shape1)\n        for subject in range(number_subjects):\n            if shape1[subject] < self.features:\n                raise ValueError(\n                    \"There are not enough samples to train the model with \"\n                    \"{0:d} features.\".format(self.features))\n            if shape1[subject] != number_trs:\n                raise ValueError(\"Different number of samples between subjects\"\n                                 \".\")\n        # Run SRM\n        self.sigma_s_, self.w_, self.mu_, self.rho2_, self.s_ = self._srm(X)\n\n        return self", "response": "Compute the probabilistic Shared Response Model for the given set of subjects and data."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse the model to transform matrix to Shared Response space", "response": "def transform(self, X, y=None):\n        \"\"\"Use the model to transform matrix to Shared Response space\n\n        Parameters\n        ----------\n        X : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject\n            note that number of voxels and samples can vary across subjects\n        y : not used (as it is unsupervised learning)\n\n\n        Returns\n        -------\n        s : list of 2D arrays, element i has shape=[features_i, samples_i]\n            Shared responses from input data (X)\n        \"\"\"\n\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of subjects\n        if len(X) != len(self.w_):\n            raise ValueError(\"The number of subjects does not match the one\"\n                             \" in the model.\")\n\n        s = [None] * len(X)\n        for subject in range(len(X)):\n            if X[subject] is not None:\n                s[subject] = self.w_[subject].T.dot(X[subject])\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the mappings Xi and S to update the orthogonal transform Wi for one subject.", "response": "def _update_transform_subject(Xi, S):\n        \"\"\"Updates the mappings `W_i` for one subject.\n\n        Parameters\n        ----------\n\n        Xi : array, shape=[voxels, timepoints]\n            The fMRI data :math:`X_i` for aligning the subject.\n\n        S : array, shape=[features, timepoints]\n            The shared response.\n\n        Returns\n        -------\n\n        Wi : array, shape=[voxels, features]\n            The orthogonal transform (mapping) :math:`W_i` for the subject.\n        \"\"\"\n        A = Xi.dot(S.T)\n        # Solve the Procrustes problem\n        U, _, V = np.linalg.svd(A, full_matrices=False)\n        return U.dot(V)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming a new subject using the existing model.", "response": "def transform_subject(self, X):\n        \"\"\"Transform a new subject using the existing model.\n        The subject is assumed to have recieved equivalent stimulation\n\n        Parameters\n        ----------\n\n        X : 2D array, shape=[voxels, timepoints]\n            The fMRI data of the new subject.\n\n        Returns\n        -------\n\n        w : 2D array, shape=[voxels, features]\n            Orthogonal mapping `W_{new}` for new subject\n\n        \"\"\"\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of TRs in the subject\n        if X.shape[1] != self.s_.shape[1]:\n            raise ValueError(\"The number of timepoints(TRs) does not match the\"\n                             \"one in the model.\")\n\n        w = self._update_transform_subject(X, self.s_)\n\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _srm(self, data):\n\n        subjects = len(data)\n\n        self.random_state_ = np.random.RandomState(self.rand_seed)\n        random_states = [\n            np.random.RandomState(self.random_state_.randint(2 ** 32))\n            for i in range(len(data))]\n\n        # Initialization step: initialize the outputs with initial values,\n        # voxels with the number of voxels in each subject.\n        w, _ = _init_w_transforms(data, self.features, random_states)\n        shared_response = self._compute_shared_response(data, w)\n        if logger.isEnabledFor(logging.INFO):\n            # Calculate the current objective function value\n            objective = self._objective_function(data, w, shared_response)\n            logger.info('Objective function %f' % objective)\n\n        # Main loop of the algorithm\n        for iteration in range(self.n_iter):\n            logger.info('Iteration %d' % (iteration + 1))\n\n            # Update each subject's mapping transform W_i:\n            for subject in range(subjects):\n                a_subject = data[subject].dot(shared_response.T)\n                perturbation = np.zeros(a_subject.shape)\n                np.fill_diagonal(perturbation, 0.001)\n                u_subject, _, v_subject = np.linalg.svd(\n                    a_subject + perturbation, full_matrices=False)\n                w[subject] = u_subject.dot(v_subject)\n\n            # Update the shared response:\n            shared_response = self._compute_shared_response(data, w)\n\n            if logger.isEnabledFor(logging.INFO):\n                # Calculate the current objective function value\n                objective = self._objective_function(data, w, shared_response)\n                logger.info('Objective function %f' % objective)\n\n        return w, shared_response", "response": "Expectation - Maximization algorithm for fitting the probabilistic SRM."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlearning a segmentation on training data and a segmentation to train data.", "response": "def fit(self, X, y=None):\n        \"\"\"Learn a segmentation on training data\n\n        Fits event patterns and a segmentation to training data. After\n        running this function, the learned event patterns can be used to\n        segment other datasets using find_events\n\n        Parameters\n        ----------\n        X: time by voxel ndarray, or a list of such ndarrays\n            fMRI data to be segmented. If a list is given, then all datasets\n            are segmented simultaneously with the same event patterns\n\n        y: not used (added to comply with BaseEstimator definition)\n\n        Returns\n        -------\n        self: the EventSegment object\n        \"\"\"\n\n        X = copy.deepcopy(X)\n        if type(X) is not list:\n            X = check_array(X)\n            X = [X]\n\n        n_train = len(X)\n        for i in range(n_train):\n            X[i] = X[i].T\n\n        self.classes_ = np.arange(self.n_events)\n        n_dim = X[0].shape[0]\n        for i in range(n_train):\n            assert (X[i].shape[0] == n_dim)\n\n        # Double-check that data is z-scored in time\n        for i in range(n_train):\n            X[i] = stats.zscore(X[i], axis=1, ddof=1)\n\n        # Initialize variables for fitting\n        log_gamma = []\n        for i in range(n_train):\n            log_gamma.append(np.zeros((X[i].shape[1],\n                                       self.n_events)))\n        step = 1\n        best_ll = float(\"-inf\")\n        self.ll_ = np.empty((0, n_train))\n        while step <= self.n_iter:\n            iteration_var = self.step_var(step)\n\n            # Based on the current segmentation, compute the mean pattern\n            # for each event\n            seg_prob = [np.exp(lg) / np.sum(np.exp(lg), axis=0)\n                        for lg in log_gamma]\n            mean_pat = np.empty((n_train, n_dim, self.n_events))\n            for i in range(n_train):\n                mean_pat[i, :, :] = X[i].dot(seg_prob[i])\n            mean_pat = np.mean(mean_pat, axis=0)\n\n            # Based on the current mean patterns, compute the event\n            # segmentation\n            self.ll_ = np.append(self.ll_, np.empty((1, n_train)), axis=0)\n            for i in range(n_train):\n                logprob = self._logprob_obs(X[i],\n                                            mean_pat, iteration_var)\n                log_gamma[i], self.ll_[-1, i] = self._forward_backward(logprob)\n\n            # If log-likelihood has started decreasing, undo last step and stop\n            if np.mean(self.ll_[-1, :]) < best_ll:\n                self.ll_ = self.ll_[:-1, :]\n                break\n\n            self.segments_ = [np.exp(lg) for lg in log_gamma]\n            self.event_var_ = iteration_var\n            self.event_pat_ = mean_pat\n            best_ll = np.mean(self.ll_[-1, :])\n            logger.debug(\"Fitting step %d, LL=%f\", step, best_ll)\n\n            step += 1\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the log probability of observing each timepoint under each event pattern and the Gaussian distribution for each event pattern.", "response": "def _logprob_obs(self, data, mean_pat, var):\n        \"\"\"Log probability of observing each timepoint under each event model\n\n        Computes the log probability of each observed timepoint being\n        generated by the Gaussian distribution for each event pattern\n\n        Parameters\n        ----------\n        data: voxel by time ndarray\n            fMRI data on which to compute log probabilities\n\n        mean_pat: voxel by event ndarray\n            Centers of the Gaussians for each event\n\n        var: float or 1D array of length equal to the number of events\n            Variance of the event Gaussians. If scalar, all events are\n            assumed to have the same variance\n\n        Returns\n        -------\n        logprob : time by event ndarray\n            Log probability of each timepoint under each event Gaussian\n        \"\"\"\n\n        n_vox = data.shape[0]\n        t = data.shape[1]\n\n        # z-score both data and mean patterns in space, so that Gaussians\n        # are measuring Pearson correlations and are insensitive to overall\n        # activity changes\n        data_z = stats.zscore(data, axis=0, ddof=1)\n        mean_pat_z = stats.zscore(mean_pat, axis=0, ddof=1)\n\n        logprob = np.empty((t, self.n_events))\n\n        if type(var) is not np.ndarray:\n            var = var * np.ones(self.n_events)\n\n        for k in range(self.n_events):\n            logprob[:, k] = -0.5 * n_vox * np.log(\n                2 * np.pi * var[k]) - 0.5 * np.sum(\n                (data_z.T - mean_pat_z[:, k]).T ** 2, axis=0) / var[k]\n\n        logprob /= n_vox\n        return logprob"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns forward-backward algorithm on observation log probs Given the log probability of each timepoint being generated by each event, run the HMM forward-backward algorithm to find the probability that each timepoint belongs to each event (based on the transition priors in p_start, p_end, and P) See https://en.wikipedia.org/wiki/Forward-backward_algorithm for mathematical details Parameters ---------- logprob : time by event ndarray Log probability of each timepoint under each event Gaussian Returns ------- log_gamma : time by event ndarray Log probability of each timepoint belonging to each event ll : float Log-likelihood of fit", "response": "def _forward_backward(self, logprob):\n        \"\"\"Runs forward-backward algorithm on observation log probs\n\n        Given the log probability of each timepoint being generated by\n        each event, run the HMM forward-backward algorithm to find the\n        probability that each timepoint belongs to each event (based on the\n        transition priors in p_start, p_end, and P)\n\n        See https://en.wikipedia.org/wiki/Forward-backward_algorithm for\n        mathematical details\n\n        Parameters\n        ----------\n        logprob : time by event ndarray\n            Log probability of each timepoint under each event Gaussian\n\n        Returns\n        -------\n        log_gamma : time by event ndarray\n            Log probability of each timepoint belonging to each event\n\n        ll : float\n            Log-likelihood of fit\n        \"\"\"\n        logprob = copy.copy(logprob)\n        t = logprob.shape[0]\n        logprob = np.hstack((logprob, float(\"-inf\") * np.ones((t, 1))))\n\n        # Initialize variables\n        log_scale = np.zeros(t)\n        log_alpha = np.zeros((t, self.n_events + 1))\n        log_beta = np.zeros((t, self.n_events + 1))\n\n        # Set up transition matrix, with final sink state\n        self.p_start = np.zeros(self.n_events + 1)\n        self.p_end = np.zeros(self.n_events + 1)\n        self.P = np.zeros((self.n_events + 1, self.n_events + 1))\n        label_ind = np.unique(self.event_chains, return_inverse=True)[1]\n        n_chains = np.max(label_ind) + 1\n\n        # For each chain of events, link them together and then to sink state\n        for c in range(n_chains):\n            chain_ind = np.nonzero(label_ind == c)[0]\n            self.p_start[chain_ind[0]] = 1 / n_chains\n            self.p_end[chain_ind[-1]] = 1 / n_chains\n\n            p_trans = (len(chain_ind) - 1) / t\n            if p_trans >= 1:\n                raise ValueError('Too few timepoints')\n            for i in range(len(chain_ind)):\n                self.P[chain_ind[i], chain_ind[i]] = 1 - p_trans\n                if i < len(chain_ind) - 1:\n                    self.P[chain_ind[i], chain_ind[i+1]] = p_trans\n                else:\n                    self.P[chain_ind[i], -1] = p_trans\n        self.P[-1, -1] = 1\n\n        # Forward pass\n        for i in range(t):\n            if i == 0:\n                log_alpha[0, :] = self._log(self.p_start) + logprob[0, :]\n            else:\n                log_alpha[i, :] = self._log(np.exp(log_alpha[i - 1, :])\n                                            .dot(self.P)) + logprob[i, :]\n\n            log_scale[i] = np.logaddexp.reduce(log_alpha[i, :])\n            log_alpha[i] -= log_scale[i]\n\n        # Backward pass\n        log_beta[-1, :] = self._log(self.p_end) - log_scale[-1]\n        for i in reversed(range(t - 1)):\n            obs_weighted = log_beta[i + 1, :] + logprob[i + 1, :]\n            offset = np.max(obs_weighted)\n            log_beta[i, :] = offset + self._log(\n                np.exp(obs_weighted - offset).dot(self.P.T)) - log_scale[i]\n\n        # Combine and normalize\n        log_gamma = log_alpha + log_beta\n        log_gamma -= np.logaddexp.reduce(log_gamma, axis=1, keepdims=True)\n\n        ll = np.sum(log_scale[:(t - 1)]) + np.logaddexp.reduce(\n            log_alpha[-1, :] + log_scale[-1] + self._log(self.p_end))\n\n        log_gamma = log_gamma[:, :-1]\n\n        return log_gamma, ll"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _log(self, x):\n\n        xshape = x.shape\n        _x = x.flatten()\n        y = utils.masked_log(_x)\n        return y.reshape(xshape)", "response": "Modified version of np. log that manually sets values <= 0 to - inf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_event_patterns(self, event_pat):\n        if event_pat.shape[1] != self.n_events:\n            raise ValueError((\"Number of columns of event_pat must match \"\n                              \"number of events\"))\n        self.event_pat_ = event_pat.copy()", "response": "Set the event patterns for this HMM entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_events(self, testing_data, var=None, scramble=False):\n\n        if var is None:\n            if not hasattr(self, 'event_var_'):\n                raise NotFittedError((\"Event variance must be provided, if \"\n                                      \"not previously set by fit()\"))\n            else:\n                var = self.event_var_\n\n        if not hasattr(self, 'event_pat_'):\n            raise NotFittedError((\"The event patterns must first be set \"\n                                  \"by fit() or set_event_patterns()\"))\n        if scramble:\n            mean_pat = self.event_pat_[:, np.random.permutation(self.n_events)]\n        else:\n            mean_pat = self.event_pat_\n\n        logprob = self._logprob_obs(testing_data.T, mean_pat, var)\n        lg, test_ll = self._forward_backward(logprob)\n        segments = np.exp(lg)\n\n        return segments, test_ll", "response": "Applies learned event segmentation to new testing dataset and returns the event segments and test_ll."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply learned event segmentation to new testing dataset after using fit to learn a sequence of events.", "response": "def predict(self, X):\n        \"\"\"Applies learned event segmentation to new testing dataset\n\n        Alternative function for segmenting a new dataset after using\n        fit() to learn a sequence of events, to comply with the sklearn\n        Classifier interface\n\n        Parameters\n        ----------\n        X: timepoint by voxel ndarray\n            fMRI data to segment based on previously-learned event patterns\n\n        Returns\n        -------\n        Event label for each timepoint\n        \"\"\"\n        check_is_fitted(self, [\"event_pat_\", \"event_var_\"])\n        X = check_array(X)\n        segments, test_ll = self.find_events(X)\n        return np.argmax(segments, axis=1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute normalized weighted variance around event pattern Utility function for computing variance in a training set of weighted event examples. For each event, the sum of squared differences for all timepoints from the event pattern is computed, and then the weights specify how much each of these differences contributes to the variance (normalized by the number of voxels). Parameters ---------- D : timepoint by voxel ndarray fMRI data for which to compute event variances weights : timepoint by event ndarray specifies relative weights of timepoints for each event event_pat : voxel by event ndarray mean event patterns to compute variance around Returns ------- ev_var : ndarray of variances for each event", "response": "def calc_weighted_event_var(self, D, weights, event_pat):\n        \"\"\"Computes normalized weighted variance around event pattern\n\n        Utility function for computing variance in a training set of weighted\n        event examples. For each event, the sum of squared differences for all\n        timepoints from the event pattern is computed, and then the weights\n        specify how much each of these differences contributes to the\n        variance (normalized by the number of voxels).\n\n        Parameters\n        ----------\n        D : timepoint by voxel ndarray\n            fMRI data for which to compute event variances\n\n        weights : timepoint by event ndarray\n            specifies relative weights of timepoints for each event\n\n        event_pat : voxel by event ndarray\n            mean event patterns to compute variance around\n\n        Returns\n        -------\n        ev_var : ndarray of variances for each event\n        \"\"\"\n        Dz = stats.zscore(D, axis=1, ddof=1)\n        ev_var = np.empty(event_pat.shape[1])\n        for e in range(event_pat.shape[1]):\n            # Only compute variances for weights > 0.1% of max weight\n            nz = weights[:, e] > np.max(weights[:, e])/1000\n            sumsq = np.dot(weights[nz, e],\n                           np.sum(np.square(Dz[nz, :] -\n                                  event_pat[:, e]), axis=1))\n            ev_var[e] = sumsq/(np.sum(weights[nz, e]) -\n                               np.sum(np.square(weights[nz, e])) /\n                               np.sum(weights[nz, e]))\n        ev_var = ev_var / D.shape[1]\n        return ev_var"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the prior probability of the HMM of the model at time t.", "response": "def model_prior(self, t):\n        \"\"\"Returns the prior probability of the HMM\n\n        Runs forward-backward without any data, showing the prior distribution\n        of the model (for comparison with a posterior).\n\n        Parameters\n        ----------\n        t: int\n            Number of timepoints\n\n        Returns\n        -------\n        segments : time by event ndarray\n            segments[t,e] = prior probability that timepoint t is in event e\n\n        test_ll : float\n            Log-likelihood of model (data-independent term)\"\"\"\n\n        lg, test_ll = self._forward_backward(np.zeros((t, self.n_events)))\n        segments = np.exp(lg)\n\n        return segments, test_ll"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chain_getattr(obj, attr, value=None):\n    try:\n        return _resolve_value(safe_chain_getattr(obj, attr))\n    except AttributeError:\n        return value", "response": "Get chain attribute for an object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef trim_iterable(iterable, limit, *, split=None, prefix='', postfix=''):\n    if split is None:\n        sl = 0\n        join = False\n    else:\n        sl = len(split)\n        join = True\n    result = []\n    rl = 0\n    for element in iterable:\n        element = prefix + element + postfix\n        el = len(element)\n        if len(result) > 0:\n            el += sl\n        rl += el\n        if rl <= limit:\n            result.append(element)\n        else:\n            break\n    if join:\n        result = split.join(result)\n    return result", "response": "trim the list to make total length no more than limit. If split is specified a string is returned. If split is specified a string is returned. If prefix and postfix are provided the list is joined with the specified string. If split is specified a string is returned."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chunks(iterable, n):\n    for i in range(0, len(iterable), n):\n        yield iterable[i:i + n]", "response": "Yield successive n - sized chunks from iterable object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflattening a iterable. https://stackoverflow. com/a/215853264", "response": "def flatten(iterable):\n    \"\"\"flat a iterable. https://stackoverflow.com/a/2158532\n    \"\"\"\n    for el in iterable:\n        if isinstance(el, collections.Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns countdown of festivals.", "response": "def iter_festival_countdown(countdown: Optional[int] = None, date_obj: MDate = None,\n                            lang: str = 'zh-Hans') -> FestivalCountdownIterable:\n    \"\"\"Return countdown of festivals.\n    \"\"\"\n    factory = FestivalFactory(lang=lang)\n    return factory.iter_festival_countdown(countdown, date_obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the date object in the solar / lunar year.", "response": "def resolve(self, year: int = YEAR_ANY) -> MDate:\n        \"\"\"Return the date object in the solar / lunar year.\n        :param year:\n        :return:\n        \"\"\"\n        year = year or self.year\n        if year:\n            return self._resolve(year)\n        raise ValueError('Unable resolve the date without a specified year.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the date object in a solar year.", "response": "def resolve_solar(self, year: int) -> date:\n        \"\"\"Return the date object in a solar year.\n        :param year:\n        :return:\n        \"\"\"\n        if self.date_class == date:\n            return self.resolve(year)\n        else:\n            solar_date = LCalendars.cast_date(self.resolve(year), date)\n            offset = solar_date.year - year\n            if offset:\n                solar_date = LCalendars.cast_date(self.resolve(year - offset), date)\n            return solar_date"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse year days from a year info.", "response": "def parse_year_days(year_info):\n    \"\"\"Parse year days from a year info.\n    \"\"\"\n    leap_month, leap_days = _parse_leap(year_info)\n    res = leap_days\n    for month in range(1, 13):\n        res += (year_info >> (16 - month)) % 2 + 29\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _iter_year_month(year_info):\n    # info => month, days, leap\n\n    leap_month, leap_days = _parse_leap(year_info)\n    months = [(i, 0) for i in range(1, 13)]\n    if leap_month > 0:\n        months.insert(leap_month, (leap_month, 1))\n    for month, leap in months:\n        if leap:\n            days = leap_days\n        else:\n            days = (year_info >> (16 - month)) % 2 + 29\n        yield month, days, leap", "response": "Iterate the month days in a lunar year."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse solar term and stem - branch year month day.", "response": "def get_term_info(year, month, day):\n        \"\"\"Parse solar term and stem-branch year/month/day from a solar date.\n        (sy, sm, sd) => (term, next_gz_month)\n        term for year 2101,:2101.1.5(\u521d\u516d) \u5c0f\u5bd2 2101.1.20(\u5eff\u4e00) \u5927\u5bd2\n        \"\"\"\n        if year == 2101:\n            days = [5, 20]\n        else:\n            days = TermUtils.parse_term_days(year)\n        term_index1 = 2 * (month - 1)\n        term_index2 = 2 * (month - 1) + 1\n        day1 = days[term_index1]\n        day2 = days[term_index2]\n        if day == day1:\n            term_name = TERMS_CN[term_index1]\n        elif day == day2:\n            term_name = TERMS_CN[term_index2]\n        else:\n            term_name = None\n\n        next_gz_month = day >= day1\n        return term_name, next_gz_month"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget n - th GanZhi", "response": "def get_gz_cn(offset: int) -> str:\n        \"\"\"Get n-th(0-based) GanZhi\n        \"\"\"\n        return TextUtils.STEMS[offset % 10] + TextUtils.BRANCHES[offset % 12]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the gz year month and day of the term.", "response": "def _get_gz_ymd(self):\n        \"\"\"\n        (sy, sm, sd) -> term / gz_year / gz_month / gz_day\n        \"\"\"\n        solar_date = MIN_SOLAR_DATE + datetime.timedelta(days=self._offset)\n        sy, sm, sd = solar_date.year, solar_date.month, solar_date.day\n        s_offset = (datetime.date(sy, sm, sd) - MIN_SOLAR_DATE).days\n        gz_year = TextUtils.STEMS[(self.year - 4) % 10] + TextUtils.BRANCHES[(self.year - 4) % 12]\n        gz_day = TextUtils.get_gz_cn((s_offset + 40) % 60)\n        term_name, next_gz_month = TermUtils.get_term_info(sy, sm, sd)\n        if next_gz_month:\n            gz_month = TextUtils.get_gz_cn((sy - 1900) * 12 + sm + 12)\n        else:\n            gz_month = TextUtils.get_gz_cn((sy - 1900) * 12 + sm + 11)\n        return gz_year, gz_month, gz_day, term_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a single item from an iterable.", "response": "def ifetch_single(iterable, key, default=EMPTY, getter=None):\n    \"\"\"\n    getter() g(item, key):pass\n    \"\"\"\n\n    def _getter(item):\n        if getter:\n            custom_getter = partial(getter, key=key)\n            return custom_getter(item)\n        else:\n            return partial(bget, key=key, default=default)(item)\n\n    return map(_getter, iterable)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef actual_age_solar(birthday, today=None):\n    birthday = LCalendars.cast_date(birthday, date)\n    if today:\n        today = LCalendars.cast_date(today, date)\n    else:\n        today = date.today()\n    return today.year - birthday.year - ((today.month, today.day) < (birthday.month, birthday.day))", "response": "Returns the actual age of the given birthday."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef model_typedefs(vk, model):\n    model['typedefs'] = {}\n\n    # bitmasks and basetypes\n    bitmasks = [x for x in vk['registry']['types']['type']\n                if x.get('@category') == 'bitmask']\n\n    basetypes = [x for x in vk['registry']['types']['type']\n                 if x.get('@category') == 'basetype']\n\n    for typedef in bitmasks + basetypes:\n        if not typedef.get('type'):\n            continue\n        model['typedefs'][typedef['name']] = typedef['type']\n\n    # handles\n    handles = [x for x in vk['registry']['types']['type']\n               if x.get('@category') == 'handle']\n\n    for handle in handles:\n        if 'name' not in handle or 'type' not in handle:\n            continue\n\n        n = handle['name']\n        t = handle['type']\n        if t == 'VK_DEFINE_HANDLE':\n            model['typedefs']['struct %s_T' % n] = '*%s' % n\n        if t == 'VK_DEFINE_HANDLE':\n            model['typedefs'][n] = 'uint64_t'\n\n    # custom plaform dependant\n    for name in ['Display', 'xcb_connection_t', 'wl_display', 'wl_surface',\n                 'MirConnection', 'MirSurface', 'ANativeWindow',\n                 'SECURITY_ATTRIBUTES']:\n        model['typedefs'][name] = 'struct %s' % name\n\n    model['typedefs'].update({\n        'Window': 'uint32_t', 'VisualID': 'uint32_t',\n        'xcb_window_t': 'uint32_t', 'xcb_visualid_t': 'uint32_t'\n    })", "response": "Fill the model with typedefs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef model_enums(vk, model):\n    model['enums'] = {}\n\n    # init enums dict\n    enums_type = [x['@name'] for x in vk['registry']['types']['type']\n                  if x.get('@category') == 'enum']\n\n    for name in enums_type:\n        model['enums'][name] = {}\n\n    # create enums\n    enums = [x for x in vk['registry']['enums']\n             if x.get('@type') in ('enum', 'bitmask')]\n\n    for enum in enums:\n        name = enum['@name']\n        t = enum.get('@type')\n\n        # enum may have no enums (because of extension)\n        if not enum.get('enum'):\n            continue\n\n        if t in ('enum', 'bitmask'):\n            # add attr to enum\n            for attr in enum['enum']:\n                if '@bitpos' in attr:\n                    num_val = int(attr['@bitpos'], 0)\n                    num_val = 1 << num_val\n                    val = '0x%08x' % num_val\n                elif '@value' in attr:\n                    val = attr['@value']\n\n                model['enums'][name][attr['@name']] = val\n\n        # Add computed value\n        def ext_name(name, extension):\n            if extension:\n                return name + '_' + extension\n            return name\n\n        extension = next(iter([x for x in VENDOR_EXTENSIONS\n                               if name.lower().endswith(x)]), '').upper()\n\n        standard_name = inflection.underscore(name).upper()\n        if extension:\n            standard_name = standard_name.split(extension)[0][:-1]\n\n        if t == 'bitmask':\n            en = ext_name(standard_name, '_MAX_ENUM')\n            model['enums'][name][en] = 0x7FFFFFFF\n        else:\n            values = [int(x, 0) for x in model['enums'][name].values()]\n\n            begin_attr = ext_name(standard_name, '_BEGIN_RANGE')\n            end_attr = ext_name(standard_name, '_END_RANGE')\n            size_attr = ext_name(standard_name, '_RANGE_SIZE')\n            max_attr = ext_name(standard_name, '_MAX_ENUM')\n\n            model['enums'][name][begin_attr] = min(values)\n            model['enums'][name][end_attr] = max(values)\n            model['enums'][name][size_attr] = max(values) - min(values) + 1\n            model['enums'][name][max_attr] = 0x7FFFFFFF\n\n    # Enum in features\n    ext_base = 1000000000\n    ext_blocksize = 1000\n    # base + (ext - 1) * blocksize + offset\n    for feature in vk['registry']['feature']:\n        for require in feature['require']:\n            if not 'enum' in require:\n                continue\n            for enum in require['enum']:\n                if not '@extnumber' in enum:\n                    continue\n                n1 = int(enum['@extnumber'])\n                n2 = int(enum['@offset'])\n                extend = enum['@extends']\n                val = ext_base + (n1 - 1) * ext_blocksize + n2\n                model['enums'][extend][enum['@name']] = val", "response": "Fill the model with the enums for the given vk."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model_macros(vk, model):\n    model['macros'] = {}\n\n    # API Macros\n    macros = [x for x in vk['registry']['enums']\n              if x.get('@type') not in ('bitmask', 'enum')]\n\n    # TODO: Check theses values\n    special_values = {'1000.0f': '1000.0',\n                      '(~0U)': 0xffffffff,\n                      '(~0ULL)': -1,\n                      '(~0U-1)': 0xfffffffe,\n                      '(~0U-2)': 0xfffffffd}\n\n    for macro in macros[0]['enum']:\n        if '@name' not in macro or '@value' not in macro:\n            continue\n\n        name = macro['@name']\n        value = macro['@value']\n\n        if value in special_values:\n                value = special_values[value]\n\n        model['macros'][name] = value\n\n    # Extension Macros\n    for ext in get_extensions_filtered(vk):\n        model['macros'][ext['@name']] = 1\n        for req in ext['require']:\n            for enum in req['enum']:\n                ename = enum['@name']\n                evalue = parse_constant(enum, int(ext['@number']))\n\n                if enum.get('@extends') == 'VkResult':\n                    model['enums']['VkResult'][ename] = evalue\n                else:\n                    model['macros'][ename] = evalue", "response": "Fill the model with macros that are used in the Vk API and extensions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling the model with the function pointers in the given vk.", "response": "def model_funcpointers(vk, model):\n    \"\"\"Fill the model with function pointer\n\n    model['funcpointers'] = {'pfn_name': 'struct_name'}\n    \"\"\"\n    model['funcpointers'] = {}\n\n    funcs = [x for x in vk['registry']['types']['type']\n             if x.get('@category') == 'funcpointer']\n    structs = [x for x in vk['registry']['types']['type']\n               if x.get('@category') == 'struct']\n\n    for f in funcs:\n        pfn_name = f['name']\n        for s in structs:\n            if 'member' not in s:\n                continue\n\n            for m in s['member']:\n                if m['type'] == pfn_name:\n                    struct_name = s['@name']\n        model['funcpointers'][pfn_name] = struct_name"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model_exceptions(vk, model):\n    model['exceptions'] = {}\n    model['errors'] = {}\n\n    all_codes = model['enums']['VkResult']\n    success_names = set()\n    error_names = set()\n\n    commands = [x for x in vk['registry']['commands']['command']]\n\n    for command in commands:\n        successes = command.get('@successcodes', '').split(',')\n        errors = command.get('@errorcodes', '').split(',')\n        success_names.update(successes)\n        error_names.update(errors)\n\n    for key, value in all_codes.items():\n        if key.startswith('VK_RESULT') or key == 'VK_SUCCESS':\n            continue\n        name = inflection.camelize(key.lower())\n\n        if key in success_names:\n            model['exceptions'][value] = name\n        elif key in error_names:\n            model['errors'][value] = name\n        else:\n            print('Warning: return code %s unused' % key)", "response": "Fill the model with exceptions and errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills the model with constructors", "response": "def model_constructors(vk, model):\n    \"\"\"Fill the model with constructors\n\n    model['constructors'] = [{'name': 'x', 'members': [{'name': 'y'}].}]\n    \"\"\"\n    model['constructors'] = []\n    structs = [x for x in vk['registry']['types']['type']\n               if x.get('@category') in {'struct', 'union'}]\n\n    def parse_len(member):\n        mlen = member.get('@len')\n        if not mlen:\n            return None\n\n        if ',' in mlen:\n            mlen = mlen.split(',')[0]\n\n        if 'latex' in mlen or 'null-terminated' in mlen:\n            return None\n\n        return mlen\n\n    for struct in structs:\n        if 'member' not in struct:\n            continue\n\n        model['constructors'].append({\n            'name': struct['@name'],\n            'members': [{\n                'name': x['name'],\n                'type': x['type'],\n                'default': x.get('@values'),\n                'len': parse_len(x)\n            } for x in struct['member']]\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfills the model with functions", "response": "def model_functions(vk, model):\n    \"\"\"Fill the model with functions\"\"\"\n\n    def get_vk_extension_functions():\n        names = set()\n        for extension in get_extensions_filtered(vk):\n            for req in extension['require']:\n                if 'command' not in req:\n                    continue\n                for command in req['command']:\n                    cn = command['@name']\n                    names.add(cn)\n                    \n                    # add alias command too\n                    for alias, n in model['alias'].items():\n                        if n == cn:\n                            names.add(alias)\n        return names\n\n    def get_count_param(command):\n        for param in command['param']:\n            if param['type'] + param.get('#text', '') == 'uint32_t*':\n                return param\n        return None\n\n    def member_has_str(name):\n        c = next(iter([x for x in model['constructors']\n                       if x['name'] == name]), None)\n        if c and any(['char' in x['type'] for x in c['members']]):\n            return True\n        return False\n\n    def format_member(member):\n        type_name = member['type']\n        if '#text' in member:\n            text = member['#text'].replace('const ', '').strip()\n            type_name += ' ' + text\n\n        return {'name': member['name'],\n                'type': member['type'],\n                'none': member['name'] in NULL_MEMBERS,\n                'force_array': True if '@len' in member else False,\n                'to_create': False,\n                'has_str': member_has_str(member['type'])}\n\n    def format_return_member(member):\n        t = member['type']\n\n        static_count = None\n        if '@len' in member and '::' in member['@len']:\n            lens = member['@len'].split('::')\n            static_count = {'key': lens[0], 'value': lens[1]}\n\n        is_handle = t in get_handle_names(vk)\n        is_enum = t in get_enum_names(vk)\n        is_struct = t in get_struct_names(vk)\n        return {'name': member['name'],\n                'type': t,\n                'handle': is_handle,\n                'enum': is_enum,\n                'struct': is_struct,\n                'static_count': static_count,\n                'has_str': member_has_str(member['type'])}\n\n    ALLOCATE_PREFIX = ('vkCreate', 'vkGet', 'vkEnumerate', 'vkAllocate',\n                       'vkMap', 'vkAcquire')\n    ALLOCATE_EXCEPTION = ('vkGetFenceStatus', 'vkGetEventStatus',\n                          'vkGetQueryPoolResults',\n                          'vkGetPhysicalDeviceXlibPresentationSupportKHR')\n    COUNT_EXCEPTION = ('vkAcquireNextImageKHR', 'vkEnumerateInstanceVersion')\n\n    model['functions'] = []\n    model['extension_functions'] = []\n    functions = [f for f in vk['registry']['commands']['command']]\n    extension_function_names = get_vk_extension_functions()\n\n    for function in functions:\n        if '@alias' in function:\n            continue\n\n        fname = function['proto']['name']\n        ftype = function['proto']['type']\n\n        if fname in CUSTOM_FUNCTIONS:\n            continue\n\n        if type(function['param']) is not list:\n            function['param'] = [function['param']]\n\n        count_param = get_count_param(function)\n        if fname in COUNT_EXCEPTION:\n            count_param = None\n        is_allocate = any([fname.startswith(a) for a in ALLOCATE_PREFIX])\n        is_count = is_allocate and count_param is not None\n\n        if fname in ALLOCATE_EXCEPTION or ftype == 'VkBool32':\n            is_allocate = is_count = False\n\n        members = []\n        for member in function['param']:\n            members.append(format_member(member))\n\n        return_member = None\n        if is_allocate:\n            return_member = format_return_member(function['param'][-1])\n            members[-1]['to_create'] = True\n        if is_count:\n            members[-2]['to_create'] = True\n\n        f = {\n            'name': fname,\n            'members': members,\n            'allocate': is_allocate,\n            'count': is_count,\n            'return_boolean': True if ftype == 'VkBool32' else False,\n            'return_result': True if ftype == 'VkResult' else False,\n            'return_member': return_member,\n            'is_extension': fname in extension_function_names\n        }\n\n        model['functions'].append(f)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef model_ext_functions(vk, model):\n    model['ext_functions'] = {'instance': {}, 'device': {}}\n\n    # invert the alias to better lookup\n    alias = {v: k for k, v in model['alias'].items()}\n\n    for extension in get_extensions_filtered(vk):\n        for req in extension['require']:\n            if not req.get('command'):\n                continue\n\n            ext_type = extension['@type']\n            for x in req['command']:\n                name = x['@name']\n                if name in alias.keys():\n                    model['ext_functions'][ext_type][name] = alias[name]\n                else:\n                    model['ext_functions'][ext_type][name] = name", "response": "Fill the model with the extensions functions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef model_alias(vk, model):\n    model['alias'] = {}\n\n    # types\n    for s in vk['registry']['types']['type']:\n        if s.get('@category', None) == 'handle' and s.get('@alias'):\n            model['alias'][s['@alias']] = s['@name'] \n              \n    # commands\n    for c in vk['registry']['commands']['command']:\n        if c.get('@alias'):\n            model['alias'][c['@alias']] = c['@name']", "response": "Fill the model with alias since V1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_vk(vk):\n\n    # Force extension require to be a list\n    for ext in get_extensions_filtered(vk):\n        req = ext['require']\n        if not isinstance(req, list):\n            ext['require'] = [req]", "response": "Format vk before using it"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_py():\n    model = {}\n\n    vk = init()\n    format_vk(vk)\n    model_alias(vk, model)\n    model_typedefs(vk, model)\n    model_enums(vk, model)\n    model_macros(vk, model)\n    model_funcpointers(vk, model)\n    model_exceptions(vk, model)\n    model_constructors(vk, model)\n    model_functions(vk, model)\n    model_ext_functions(vk, model)\n\n    env = jinja2.Environment(\n        autoescape=False,\n        trim_blocks=True,\n        lstrip_blocks=True,\n        loader=jinja2.FileSystemLoader(HERE)\n    )\n\n    out_file = path.join(HERE, path.pardir, 'vulkan', '_vulkan.py')\n    with open(out_file, 'w') as out:\n        out.write(env.get_template('vulkan.template.py').render(model=model))", "response": "Generate the python output file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate the cdef output file", "response": "def generate_cdef():\n    \"\"\"Generate the cdef output file\"\"\"\n    include_libc_path = path.join(HERE, 'fake_libc_include')\n    include_vulkan_path = path.join(HERE, 'vulkan_include')\n    out_file = path.join(HERE, path.pardir, 'vulkan', 'vulkan.cdef.h')\n    header = path.join(include_vulkan_path, 'vulkan.h')\n\n    command = ['cpp',\n               '-std=c99',\n               '-P',\n               '-nostdinc',\n               '-I' + include_libc_path,\n               '-I' + include_vulkan_path,\n               '-o' + out_file,\n               '-DVK_USE_PLATFORM_XCB_KHR',\n               '-DVK_USE_PLATFORM_WAYLAND_KHR',\n               '-DVK_USE_PLATFORM_ANDROID_KHR',\n               '-DVK_USE_PLATFORM_WIN32_KHR',\n               '-DVK_USE_PLATFORM_XLIB_KHR',\n               header]\n    subprocess.run(command, check=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mock_django_setup(settings_module, disabled_features=None):\n    if apps.ready:\n        # We're running in a real Django unit test, don't do anything.\n        return\n\n    if 'DJANGO_SETTINGS_MODULE' not in os.environ:\n        os.environ['DJANGO_SETTINGS_MODULE'] = settings_module\n    django.setup()\n    mock_django_connection(disabled_features)", "response": "This function is called by django. setup to mock django connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverwriting the Django database configuration with a mocked version.", "response": "def mock_django_connection(disabled_features=None):\n    \"\"\" Overwrite the Django database configuration with a mocked version.\n\n    This is a helper function that does the actual monkey patching.\n    \"\"\"\n    db = connections.databases['default']\n    db['PASSWORD'] = '****'\n    db['USER'] = '**Database disabled for unit tests**'\n    ConnectionHandler.__getitem__ = MagicMock(name='mock_connection')\n    # noinspection PyUnresolvedReferences\n    mock_connection = ConnectionHandler.__getitem__.return_value\n    if disabled_features:\n        for feature in disabled_features:\n            setattr(mock_connection.features, feature, False)\n    mock_ops = mock_connection.ops\n\n    # noinspection PyUnusedLocal\n    def compiler(queryset, connection, using, **kwargs):\n        result = MagicMock(name='mock_connection.ops.compiler()')\n        # noinspection PyProtectedMember\n        result.execute_sql.side_effect = NotSupportedError(\n            \"Mock database tried to execute SQL for {} model.\".format(\n                queryset.model._meta.object_name))\n        result.has_results.side_effect = result.execute_sql.side_effect\n        return result\n\n    mock_ops.compiler.return_value.side_effect = compiler\n    mock_ops.integer_field_range.return_value = (-sys.maxsize - 1, sys.maxsize)\n    mock_ops.max_name_length.return_value = sys.maxsize\n\n    Model.refresh_from_db = Mock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_all_models(models):\n    for model in models:\n        yield model\n        # noinspection PyProtectedMember\n        for parent in model._meta.parents.keys():\n            for parent_model in find_all_models((parent,)):\n                yield parent_model", "response": "Yield all models and their parents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmocks all related field managers to make pure unit tests possible.", "response": "def mocked_relations(*models):\n    \"\"\" Mock all related field managers to make pure unit tests possible.\n\n    The resulting patcher can be used just like one from the mock module:\n    As a test method decorator, a test class decorator, a context manager,\n    or by just calling start() and stop().\n\n    @mocked_relations(Dataset):\n    def test_dataset(self):\n        dataset = Dataset()\n        check = dataset.content_checks.create()  # returns a ContentCheck object\n    \"\"\"\n    patchers = []\n\n    for model in find_all_models(models):\n        if isinstance(model.save, Mock):\n            # already mocked, so skip it\n            continue\n\n        model_name = model._meta.object_name\n        patchers.append(_patch_save(model, model_name))\n\n        if hasattr(model, 'objects'):\n            patchers.append(_patch_objects(model, model_name))\n\n        for related_object in chain(model._meta.related_objects,\n                                    model._meta.many_to_many):\n            name = related_object.name\n\n            if name not in model.__dict__ and related_object.one_to_many:\n                name += '_set'\n\n            if name in model.__dict__:\n                # Only mock direct relations, not inherited ones.\n                if getattr(model, name, None):\n                    patchers.append(_patch_relation(\n                        model, name, related_object\n                    ))\n\n    return PatcherChain(patchers, pass_mocks=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decorate_callable(self, target):\n\n        # noinspection PyUnusedLocal\n        def absorb_mocks(test_case, *args):\n            return target(test_case)\n\n        should_absorb = not (self.pass_mocks or isinstance(target, type))\n        result = absorb_mocks if should_absorb else target\n        for patcher in self.patchers:\n            result = patcher(result)\n        return result", "response": "Called as a decorator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if an array contains integers and False otherwise.", "response": "def _is_array_integer(arr):\n    \"\"\"Returns True if an array contains integers (integer type or near-int\n    float values) and False otherwise.\n\n    >>> _is_array_integer(np.arange(10))\n    True\n    >>> _is_array_integer(np.arange(7.0, 20.0, 1.0))\n    True\n    >>> _is_array_integer(np.arange(0, 1, 0.1))\n    False\n    \"\"\"\n    return issubclass(arr.dtype.type, np.integer) or np.allclose(arr, np.round(arr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _zero_on_type_error(column_fn):\n    if not column_fn:\n        return column_fn\n    if not callable(column_fn):\n        raise TypeError('column functions must be callable')\n    @functools.wraps(column_fn)\n    def wrapped(column):\n        try:\n            return column_fn(column)\n        except TypeError:\n            if isinstance(column, np.ndarray):\n                return column.dtype.type() # A typed zero value\n            else:\n                raise\n    return wrapped", "response": "Wrap a function on an np. ndarray to return 0 on a type error."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfill in the missing values in rows with zeros.", "response": "def _fill_with_zeros(partials, rows, zero=None):\n    \"\"\"Find and return values from rows for all partials. In cases where no\n    row matches a partial, zero is assumed as value. For a row, the first\n    (n-1) fields are assumed to be the partial, and the last field,\n    the value, where n is the total number of fields in each row. It is\n    assumed that there is a unique row for each partial.\n    partials -- single field values or tuples of field values\n    rows -- table rows\n    zero -- value used when no rows match a particular partial\n    \"\"\"\n    assert len(rows) > 0\n    if not _is_non_string_iterable(partials):\n        # Convert partials to tuple for comparison against row slice later\n        partials = [(partial,) for partial in partials]\n\n    # Construct mapping of partials to values in rows\n    mapping = {}\n    for row in rows:\n        mapping[tuple(row[:-1])] = row[-1]\n\n    if zero is None:\n        # Try to infer zero from given row values.\n        array = np.array(tuple(mapping.values()))\n        if len(array.shape) == 1:\n            zero = array.dtype.type()\n    return np.array([mapping.get(partial, zero) for partial in partials])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _varargs_labels_as_list(label_list):\n    if len(label_list) == 0:\n        return []\n    elif not _is_non_string_iterable(label_list[0]):\n        # Assume everything is a label.  If not, it'll be caught later.\n        return label_list\n    elif len(label_list) == 1:\n        return label_list[0]\n    else:\n        raise ValueError(\"Labels {} contain more than list.\".format(label_list),\n                         \"Pass just one list of labels.\")", "response": "Return a list of labels for a list of labels or singleton list of list of list of labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _assert_same(values):\n    assert len(values) > 0\n    first, rest = values[0], values[1:]\n    for v in rest:\n        assert v == first\n    return first", "response": "Assert that all values are identical and return the unique value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlabeling of a collected column.", "response": "def _collected_label(collect, label):\n    \"\"\"Label of a collected column.\"\"\"\n    if not collect.__name__.startswith('<'):\n        return label + ' ' + collect.__name__\n    else:\n        return label"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nswitches labels to vertical if they are long.", "response": "def _vertical_x(axis, ticks=None, max_width=5):\n    \"\"\"Switch labels to vertical if they are long.\"\"\"\n    if ticks is None:\n        ticks = axis.get_xticks()\n    if (np.array(ticks) == np.rint(ticks)).all():\n        ticks = np.rint(ticks).astype(np.int)\n    if max([len(str(tick)) for tick in ticks]) > max_width:\n        axis.set_xticklabels(ticks, rotation='vertical')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef empty(cls, labels=None):\n        warnings.warn(\"Table.empty(labels) is deprecated. Use Table(labels)\", FutureWarning)\n        if labels is None:\n            return cls()\n        values = [[] for label in labels]\n        return cls(values, labels)", "response": "Returns an empty table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_rows(cls, rows, labels):\n        warnings.warn(\"Table.from_rows is deprecated. Use Table(labels).with_rows(...)\", FutureWarning)\n        return cls(labels).with_rows(rows)", "response": "Create a new table from a sequence of rows."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_records(cls, records):\n        if not records:\n            return cls()\n        labels = sorted(list(records[0].keys()))\n        columns = [[rec[label] for rec in records] for label in labels]\n        return cls().with_columns(zip(labels, columns))", "response": "Create a table from a sequence of records."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new table from a mapping of column labels to column values.", "response": "def from_columns_dict(cls, columns):\n        \"\"\"Create a table from a mapping of column labels to column values. [Deprecated]\"\"\"\n        warnings.warn(\"Table.from_columns_dict is deprecated. Use Table().with_columns(...)\", FutureWarning)\n        return cls().with_columns(columns.items())"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a table from a file or web address.", "response": "def read_table(cls, filepath_or_buffer, *args, **vargs):\n        \"\"\"Read a table from a file or web address.\n\n        filepath_or_buffer -- string or file handle / StringIO; The string\n                              could be a URL. Valid URL schemes include http,\n                              ftp, s3, and file.\n        \"\"\"\n        # Look for .csv at the end of the path; use \",\" as a separator if found\n        try:\n            path = urllib.parse.urlparse(filepath_or_buffer).path\n            if 'data8.berkeley.edu' in filepath_or_buffer:\n                raise ValueError('data8.berkeley.edu requires authentication, '\n                                 'which is not supported.')\n        except AttributeError:\n            path = filepath_or_buffer\n\n        try:\n            if 'sep' not in vargs and path.endswith('.csv'):\n                vargs['sep'] = ','\n        except AttributeError:\n            pass\n        df = pandas.read_table(filepath_or_buffer, *args, **vargs)\n        return cls.from_df(df)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _with_columns(self, columns):\n        table = type(self)()\n        for label, column in zip(self.labels, columns):\n            self._add_column_and_format(table, label, column)\n        return table", "response": "Create a table from a sequence of columns copying column labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a column to table copying the formatter from self.", "response": "def _add_column_and_format(self, table, label, column):\n        \"\"\"Add a column to table, copying the formatter from self.\"\"\"\n        label = self._as_label(label)\n        table[label] = column\n        if label in self._formats:\n            table._formats[label] = self._formats[label]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a Pandas DataFrame into a Table.", "response": "def from_df(cls, df):\n        \"\"\"Convert a Pandas DataFrame into a Table.\"\"\"\n        t = cls()\n        labels = df.columns\n        for label in df.columns:\n            t.append_column(label, df[label])\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a structured NumPy array into a Table.", "response": "def from_array(cls, arr):\n        \"\"\"Convert a structured NumPy array into a Table.\"\"\"\n        return cls().with_columns([(f, arr[f]) for f in arr.dtype.names])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef column(self, index_or_label):\n        if (isinstance(index_or_label, str)\n                and index_or_label not in self.labels):\n            raise ValueError(\n                'The column \"{}\" is not in the table. The table contains '\n                'these columns: {}'\n                .format(index_or_label, ', '.join(self.labels))\n            )\n        if (isinstance(index_or_label, int)\n                and not 0 <= index_or_label < len(self.labels)):\n            raise ValueError(\n                'The index {} is not in the table. Only indices between '\n                '0 and {} are valid'\n                .format(index_or_label, len(self.labels) - 1)\n            )\n\n        return self._columns[self._as_label(index_or_label)]", "response": "Return the values of a column as an array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning data in self as a numpy array.", "response": "def values(self):\n        \"\"\"Return data in `self` as a numpy array.\n\n        If all columns are the same dtype, the resulting array\n        will have this dtype. If there are >1 dtypes in columns,\n        then the resulting array will have dtype `object`.\n        \"\"\"\n        dtypes = [col.dtype for col in self.columns]\n        if len(set(dtypes)) > 1:\n            dtype = object\n        else:\n            dtype = None\n        return np.array(self.columns, dtype=dtype).T"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, fn, *column_or_columns):\n        if not column_or_columns:\n            return np.array([fn(row) for row in self.rows])\n        else:\n            if len(column_or_columns) == 1 and \\\n                    _is_non_string_iterable(column_or_columns[0]):\n                warnings.warn(\n                   \"column lists are deprecated; pass each as an argument\", FutureWarning)\n                column_or_columns = column_or_columns[0]\n            rows = zip(*self.select(*column_or_columns).columns)\n            return np.array([fn(*row) for row in rows])", "response": "Apply a function to each element or element of column_or_columns in each row of the table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the format of a column.", "response": "def set_format(self, column_or_columns, formatter):\n        \"\"\"Set the format of a column.\"\"\"\n        if inspect.isclass(formatter):\n            formatter = formatter()\n        if callable(formatter) and not hasattr(formatter, 'format_column'):\n            formatter = _formats.FunctionFormatter(formatter)\n        if not hasattr(formatter, 'format_column'):\n            raise Exception('Expected Formatter or function: ' + str(formatter))\n        for label in self._as_labels(column_or_columns):\n            if formatter.converts_values:\n                self[label] = formatter.convert_column(self[label])\n            self._formats[label] = formatter\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmoves a column to the first in order.", "response": "def move_to_start(self, column_label):\n        \"\"\"Move a column to the first in order.\"\"\"\n        self._columns.move_to_end(column_label, last=False)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append(self, row_or_table):\n        if not row_or_table:\n            return\n        if isinstance(row_or_table, Table):\n            t = row_or_table\n            columns = list(t.select(self.labels)._columns.values())\n            n = t.num_rows\n        else:\n            if (len(list(row_or_table)) != self.num_columns):\n                raise Exception('Row should have '+ str(self.num_columns) + \" columns\")\n            columns, n = [[value] for value in row_or_table], 1\n        for i, column in enumerate(self._columns):\n            if self.num_rows:\n                self._columns[column] = np.append(self[column], columns[i])\n            else:\n                self._columns[column] = np.array(columns[i])\n        self._num_rows += n\n        return self", "response": "Append a row or all rows of a table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_column(self, label, values):\n        # TODO(sam): Allow append_column to take in a another table, copying\n        # over formatter as needed.\n        if not isinstance(label, str):\n            raise ValueError('The column label must be a string, but a '\n                '{} was given'.format(label.__class__.__name__))\n\n        if not isinstance(values, np.ndarray):\n            # Coerce a single value to a sequence\n            if not _is_non_string_iterable(values):\n                values = [values] * max(self.num_rows, 1)\n            values = np.array(tuple(values))\n\n        if self.num_rows != 0 and len(values) != self.num_rows:\n            raise ValueError('Column length mismatch. New column does not have '\n                             'the same number of rows as table.')\n        else:\n            self._num_rows = len(values)\n\n        self._columns[label] = values", "response": "Appends a column to the table or replaces a column."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nchange the label of columns in the current table to new_label.", "response": "def relabel(self, column_label, new_label):\n        \"\"\"Changes the label(s) of column(s) specified by ``column_label`` to\n        labels in ``new_label``.\n\n        Args:\n            ``column_label`` -- (single str or array of str) The label(s) of\n                columns to be changed to ``new_label``.\n\n            ``new_label`` -- (single str or array of str): The label name(s)\n                of columns to replace ``column_label``.\n\n        Raises:\n            ``ValueError`` -- if ``column_label`` is not in table, or if\n                ``column_label`` and ``new_label`` are not of equal length.\n            ``TypeError`` -- if ``column_label`` and/or ``new_label`` is not\n                ``str``.\n\n        Returns:\n            Original table with ``new_label`` in place of ``column_label``.\n\n        >>> table = Table().with_columns(\n        ...     'points', make_array(1, 2, 3),\n        ...     'id',     make_array(12345, 123, 5123))\n        >>> table.relabel('id', 'yolo')\n        points | yolo\n        1      | 12,345\n        2      | 123\n        3      | 5,123\n        >>> table.relabel(make_array('points', 'yolo'),\n        ...   make_array('red', 'blue'))\n        red  | blue\n        1    | 12,345\n        2    | 123\n        3    | 5,123\n        >>> table.relabel(make_array('red', 'green', 'blue'),\n        ...   make_array('cyan', 'magenta', 'yellow', 'key'))\n        Traceback (most recent call last):\n            ...\n        ValueError: Invalid arguments. column_label and new_label must be of equal length.\n        \"\"\"\n        if isinstance(column_label, numbers.Integral):\n            column_label = self._as_label(column_label)\n        if isinstance(column_label, str) and isinstance(new_label, str):\n            column_label, new_label = [column_label], [new_label]\n        if len(column_label) != len(new_label):\n            raise ValueError('Invalid arguments. column_label and new_label '\n                'must be of equal length.')\n        old_to_new = dict(zip(column_label, new_label)) # maps old labels to new ones\n        for label in column_label:\n            if not (label in self.labels):\n                raise ValueError('Invalid labels. Column labels must '\n                'already exist in table in order to be replaced.')\n        rewrite = lambda s: old_to_new[s] if s in old_to_new else s\n        columns = [(rewrite(s), c) for s, c in self._columns.items()]\n        self._columns = collections.OrderedDict(columns)\n        for label in self._formats:\n            # TODO(denero) Error when old and new columns share a name\n            if label in column_label:\n                formatter = self._formats.pop(label)\n                self._formats[old_to_new[label]] = formatter\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, row_or_row_indices):\n        if not row_or_row_indices:\n            return\n        if isinstance(row_or_row_indices, int):\n            rows_remove = [row_or_row_indices]\n        else:\n            rows_remove = row_or_row_indices\n        for col in self._columns:\n            self._columns[col] = [elem for i, elem in enumerate(self[col]) if i not in rows_remove]\n        return self", "response": "Removes a row or multiple rows of a table in place."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a copy of a table.", "response": "def copy(self, *, shallow=False):\n        \"\"\"Return a copy of a table.\"\"\"\n        table = type(self)()\n        for label in self.labels:\n            if shallow:\n                column = self[label]\n            else:\n                column = np.copy(self[label])\n            self._add_column_and_format(table, label, column)\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef select(self, *column_or_columns):\n        labels = self._varargs_as_labels(column_or_columns)\n        table = type(self)()\n        for label in labels:\n            self._add_column_and_format(table, label, np.copy(self[label]))\n        return table", "response": "Return a new table with only the columns in column_or_columns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef drop(self, *column_or_columns):\n        exclude = _varargs_labels_as_list(column_or_columns)\n        return self.select([c for (i, c) in enumerate(self.labels)\n                            if i not in exclude and c not in exclude])", "response": "Return a new table with only columns other than selected label or list of columns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef where(self, column_or_label, value_or_predicate=None, other=None):\n        column = self._get_column(column_or_label)\n        if other is not None:\n            assert callable(value_or_predicate), \"Predicate required for 3-arg where\"\n            predicate = value_or_predicate\n            other = self._get_column(other)\n            column = [predicate(y)(x) for x, y in zip(column, other)]\n        elif value_or_predicate is not None:\n            if not callable(value_or_predicate):\n                predicate = _predicates.are.equal_to(value_or_predicate)\n            else:\n                predicate = value_or_predicate\n            column = [predicate(x) for x in column]\n        return self.take(np.nonzero(column)[0])", "response": "Returns a new table containing rows where value_or_predicate returns True for values in column_or_label."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new table with rows sorted according to the values in column_or_label.", "response": "def sort(self, column_or_label, descending=False, distinct=False):\n        \"\"\"Return a Table of rows sorted according to the values in a column.\n\n        Args:\n            ``column_or_label``: the column whose values are used for sorting.\n\n            ``descending``: if True, sorting will be in descending, rather than\n                ascending order.\n\n            ``distinct``: if True, repeated values in ``column_or_label`` will\n                be omitted.\n\n        Returns:\n            An instance of ``Table`` containing rows sorted based on the values\n            in ``column_or_label``.\n\n        >>> marbles = Table().with_columns(\n        ...    \"Color\", make_array(\"Red\", \"Green\", \"Blue\", \"Red\", \"Green\", \"Green\"),\n        ...    \"Shape\", make_array(\"Round\", \"Rectangular\", \"Rectangular\", \"Round\", \"Rectangular\", \"Round\"),\n        ...    \"Amount\", make_array(4, 6, 12, 7, 9, 2),\n        ...    \"Price\", make_array(1.30, 1.30, 2.00, 1.75, 1.40, 1.00))\n        >>> marbles\n        Color | Shape       | Amount | Price\n        Red   | Round       | 4      | 1.3\n        Green | Rectangular | 6      | 1.3\n        Blue  | Rectangular | 12     | 2\n        Red   | Round       | 7      | 1.75\n        Green | Rectangular | 9      | 1.4\n        Green | Round       | 2      | 1\n        >>> marbles.sort(\"Amount\")\n        Color | Shape       | Amount | Price\n        Green | Round       | 2      | 1\n        Red   | Round       | 4      | 1.3\n        Green | Rectangular | 6      | 1.3\n        Red   | Round       | 7      | 1.75\n        Green | Rectangular | 9      | 1.4\n        Blue  | Rectangular | 12     | 2\n        >>> marbles.sort(\"Amount\", descending = True)\n        Color | Shape       | Amount | Price\n        Blue  | Rectangular | 12     | 2\n        Green | Rectangular | 9      | 1.4\n        Red   | Round       | 7      | 1.75\n        Green | Rectangular | 6      | 1.3\n        Red   | Round       | 4      | 1.3\n        Green | Round       | 2      | 1\n        >>> marbles.sort(3) # the Price column\n        Color | Shape       | Amount | Price\n        Green | Round       | 2      | 1\n        Red   | Round       | 4      | 1.3\n        Green | Rectangular | 6      | 1.3\n        Green | Rectangular | 9      | 1.4\n        Red   | Round       | 7      | 1.75\n        Blue  | Rectangular | 12     | 2\n        >>> marbles.sort(3, distinct = True)\n        Color | Shape       | Amount | Price\n        Green | Round       | 2      | 1\n        Red   | Round       | 4      | 1.3\n        Green | Rectangular | 9      | 1.4\n        Red   | Round       | 7      | 1.75\n        Blue  | Rectangular | 12     | 2\n        \"\"\"\n        column = self._get_column(column_or_label)\n        if distinct:\n            _, row_numbers = np.unique(column, return_index=True)\n        else:\n            row_numbers = np.argsort(column, axis=0, kind='mergesort')\n        assert (row_numbers < self.num_rows).all(), row_numbers\n        if descending:\n            row_numbers = np.array(row_numbers[::-1])\n        return self.take(row_numbers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group(self, column_or_label, collect=None):\n        # Assume that a call to group with a list of labels is a call to groups\n        if _is_non_string_iterable(column_or_label) and \\\n                len(column_or_label) != self._num_rows:\n            return self.groups(column_or_label, collect)\n\n        self = self.copy(shallow=True)\n        collect = _zero_on_type_error(collect)\n\n        # Remove column used for grouping\n        column = self._get_column(column_or_label)\n        if isinstance(column_or_label, str) or isinstance(column_or_label, numbers.Integral):\n            column_label = self._as_label(column_or_label)\n            del self[column_label]\n        else:\n            column_label = self._unused_label('group')\n\n        # Group by column\n        groups = self.index_by(column)\n        keys = sorted(groups.keys())\n\n        # Generate grouped columns\n        if collect is None:\n            labels = [column_label, 'count' if column_label != 'count' else self._unused_label('count')]\n            columns = [keys, [len(groups[k]) for k in keys]]\n        else:\n            columns, labels = [], []\n            for i, label in enumerate(self.labels):\n                labels.append(_collected_label(collect, label))\n                c = [collect(np.array([row[i] for row in groups[k]])) for k in keys]\n                columns.append(c)\n\n        grouped = type(self)().with_columns(zip(labels, columns))\n        assert column_label == self._unused_label(column_label)\n        grouped[column_label] = keys\n        grouped.move_to_start(column_label)\n        return grouped", "response": "Returns a Table with each row corresponding to a unique value in column_or_label grouped by count or aggregate others."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef groups(self, labels, collect=None):\n        # Assume that a call to groups with one label is a call to group\n        if not _is_non_string_iterable(labels):\n            return self.group(labels, collect=collect)\n\n        collect = _zero_on_type_error(collect)\n        columns = []\n        labels = self._as_labels(labels)\n        for label in labels:\n            if label not in self.labels:\n                raise ValueError(\"All labels must exist in the table\")\n            columns.append(self._get_column(label))\n        grouped = self.group(list(zip(*columns)), lambda s: s)\n        grouped._columns.popitem(last=False) # Discard the column of tuples\n\n        # Flatten grouping values and move them to front\n        counts = [len(v) for v in grouped[0]]\n        for label in labels[::-1]:\n            grouped[label] = grouped.apply(_assert_same, label)\n            grouped.move_to_start(label)\n\n        # Aggregate other values\n        if collect is None:\n            count = 'count' if 'count' not in labels else self._unused_label('count')\n            return grouped.select(labels).with_column(count, counts)\n        else:\n            for label in grouped.labels:\n                if label in labels:\n                    continue\n                column = [collect(v) for v in grouped[label]]\n                del grouped[label]\n                grouped[_collected_label(collect, label)] = column\n            return grouped", "response": "Groups the rows by multiple columns count or aggregate others."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a pivot table with a column for each unique value in columns rows and values for each unique value in rows.", "response": "def pivot(self, columns, rows, values=None, collect=None, zero=None):\n        \"\"\"Generate a table with a column for each unique value in ``columns``,\n        with rows for each unique value in ``rows``. Each row counts/aggregates\n        the values that match both row and column based on ``collect``.\n\n        Args:\n            ``columns`` -- a single column label or index, (``str`` or ``int``),\n                used to create new columns, based on its unique values.\n            ``rows`` -- row labels or indices, (``str`` or ``int`` or list),\n                used to create new rows based on it's unique values.\n            ``values`` -- column label in table for use in aggregation.\n                Default None.\n            ``collect`` -- aggregation function, used to group ``values``\n                over row-column combinations. Default None.\n            ``zero`` -- zero value for non-existent row-column combinations.\n\n        Raises:\n            TypeError -- if ``collect`` is passed in and ``values`` is not,\n                vice versa.\n\n        Returns:\n            New pivot table, with row-column combinations, as specified, with\n            aggregated ``values`` by ``collect`` across the intersection of\n            ``columns`` and ``rows``. Simple counts provided if values and\n            collect are None, as default.\n\n        >>> titanic = Table().with_columns('age', make_array(21, 44, 56, 89, 95\n        ...    , 40, 80, 45), 'survival', make_array(0,0,0,1, 1, 1, 0, 1),\n        ...    'gender',  make_array('M', 'M', 'M', 'M', 'F', 'F', 'F', 'F'),\n        ...    'prediction', make_array(0, 0, 1, 1, 0, 1, 0, 1))\n        >>> titanic\n        age  | survival | gender | prediction\n        21   | 0        | M      | 0\n        44   | 0        | M      | 0\n        56   | 0        | M      | 1\n        89   | 1        | M      | 1\n        95   | 1        | F      | 0\n        40   | 1        | F      | 1\n        80   | 0        | F      | 0\n        45   | 1        | F      | 1\n        >>> titanic.pivot('survival', 'gender')\n        gender | 0    | 1\n        F      | 1    | 3\n        M      | 3    | 1\n        >>> titanic.pivot('prediction', 'gender')\n        gender | 0    | 1\n        F      | 2    | 2\n        M      | 2    | 2\n        >>> titanic.pivot('survival', 'gender', values='age', collect = np.mean)\n        gender | 0       | 1\n        F      | 80      | 60\n        M      | 40.3333 | 89\n        >>> titanic.pivot('survival', make_array('prediction', 'gender'))\n        prediction | gender | 0    | 1\n        0          | F      | 1    | 1\n        0          | M      | 2    | 0\n        1          | F      | 0    | 2\n        1          | M      | 1    | 1\n        >>> titanic.pivot('survival', 'gender', values = 'age')\n        Traceback (most recent call last):\n           ...\n        TypeError: values requires collect to be specified\n        >>> titanic.pivot('survival', 'gender', collect = np.mean)\n        Traceback (most recent call last):\n           ...\n        TypeError: collect requires values to be specified\n        \"\"\"\n        if collect is not None and values is None:\n            raise TypeError('collect requires values to be specified')\n        if values is not None and collect is None:\n            raise TypeError('values requires collect to be specified')\n        columns = self._as_label(columns)\n        rows = self._as_labels(rows)\n        if values is None:\n            selected = self.select([columns] + rows)\n        else:\n            selected = self.select([columns, values] + rows)\n        grouped = selected.groups([columns] + rows, collect)\n\n        # Generate existing combinations of values from columns in rows\n        rows_values = sorted(list(set(self.select(rows).rows)))\n        pivoted = type(self)(rows).with_rows(rows_values)\n\n        # Generate other columns and add them to pivoted\n        by_columns = grouped.index_by(columns)\n        for label in sorted(by_columns):\n            tuples = [t[1:] for t in by_columns[label]] # Discard column value\n            column = _fill_with_zeros(rows_values, tuples, zero)\n            pivot = self._unused_label(str(label))\n            pivoted[pivot] = column\n        return pivoted"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pivot_bin(self, pivot_columns, value_column, bins=None, **vargs) :\n        pivot_columns = _as_labels(pivot_columns)\n        selected = self.select(pivot_columns + [value_column])\n        grouped = selected.groups(pivot_columns, collect=lambda x:x)\n\n        # refine bins by taking a histogram over all the data\n        if bins is not None:\n            vargs['bins'] = bins\n        _, rbins = np.histogram(self[value_column],**vargs)\n        # create a table with these bins a first column and counts for each group\n        vargs['bins'] = rbins\n        binned = type(self)().with_column('bin',rbins)\n        for group in grouped.rows:\n            col_label = \"-\".join(map(str,group[0:-1]))\n            col_vals = group[-1]\n            counts,_ = np.histogram(col_vals,**vargs)\n            binned[col_label] = np.append(counts,0)\n        return binned", "response": "Form a table with columns formed by the unique tuples in pivot_columns containing counts per bin of the values associated with each tuple in value_column."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake k original columns and returns two columns with col. 1 of all column names and col. 2 of all column names and col. 3 of all associated data.", "response": "def stack(self, key, labels=None):\n        \"\"\"Takes k original columns and returns two columns, with col. 1 of\n        all column names and col. 2 of all associated data.\n        \"\"\"\n        rows, labels = [], labels or self.labels\n        for row in self.rows:\n            [rows.append((getattr(row, key), k, v)) for k, v in row.asdict().items()\n             if k != key and k in labels]\n        return type(self)([key, 'column', 'value']).with_rows(rows)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new table with the columns of self and other containing the rows of other that appear in both tables.", "response": "def join(self, column_label, other, other_label=None):\n        \"\"\"Creates a new table with the columns of self and other, containing\n        rows for all values of a column that appear in both tables.\n\n        Args:\n            ``column_label`` (``str``):  label of column in self that is used to\n                join  rows of ``other``.\n            ``other``: Table object to join with self on matching values of\n                ``column_label``.\n\n        Kwargs:\n            ``other_label`` (``str``): default None, assumes ``column_label``.\n                Otherwise in ``other`` used to join rows.\n\n        Returns:\n            New table self joined with ``other`` by matching values in\n            ``column_label`` and ``other_label``. If the resulting join is\n            empty, returns None.\n\n        >>> table = Table().with_columns('a', make_array(9, 3, 3, 1),\n        ...     'b', make_array(1, 2, 2, 10),\n        ...     'c', make_array(3, 4, 5, 6))\n        >>> table\n        a    | b    | c\n        9    | 1    | 3\n        3    | 2    | 4\n        3    | 2    | 5\n        1    | 10   | 6\n        >>> table2 = Table().with_columns( 'a', make_array(9, 1, 1, 1),\n        ... 'd', make_array(1, 2, 2, 10),\n        ... 'e', make_array(3, 4, 5, 6))\n        >>> table2\n        a    | d    | e\n        9    | 1    | 3\n        1    | 2    | 4\n        1    | 2    | 5\n        1    | 10   | 6\n        >>> table.join('a', table2)\n        a    | b    | c    | d    | e\n        1    | 10   | 6    | 2    | 4\n        1    | 10   | 6    | 2    | 5\n        1    | 10   | 6    | 10   | 6\n        9    | 1    | 3    | 1    | 3\n        >>> table.join('a', table2, 'a') # Equivalent to previous join\n        a    | b    | c    | d    | e\n        1    | 10   | 6    | 2    | 4\n        1    | 10   | 6    | 2    | 5\n        1    | 10   | 6    | 10   | 6\n        9    | 1    | 3    | 1    | 3\n        >>> table.join('a', table2, 'd') # Repeat column labels relabeled\n        a    | b    | c    | a_2  | e\n        1    | 10   | 6    | 9    | 3\n        >>> table2 #table2 has three rows with a = 1\n        a    | d    | e\n        9    | 1    | 3\n        1    | 2    | 4\n        1    | 2    | 5\n        1    | 10   | 6\n        >>> table #table has only one row with a = 1\n        a    | b    | c\n        9    | 1    | 3\n        3    | 2    | 4\n        3    | 2    | 5\n        1    | 10   | 6\n        \"\"\"\n        if self.num_rows == 0 or other.num_rows == 0:\n            return None\n        if not other_label:\n            other_label = column_label\n\n        self_rows = self.index_by(column_label)\n        other_rows = other.index_by(other_label)\n\n        # Gather joined rows from self_rows that have join values in other_rows\n        joined_rows = []\n        for v, rows in self_rows.items():\n            if v in other_rows:\n                joined_rows += [row + o for row in rows for o in other_rows[v]]\n        if not joined_rows:\n            return None\n\n        # Build joined table\n        self_labels = list(self.labels)\n        other_labels = [self._unused_label(s) for s in other.labels]\n        other_labels_map = dict(zip(other.labels, other_labels))\n        joined = type(self)(self_labels + other_labels).with_rows(joined_rows)\n\n        # Copy formats from both tables\n        joined._formats.update(self._formats)\n        for label in other._formats:\n            joined._formats[other_labels_map[label]] = other._formats[label]\n\n        # Remove redundant column, but perhaps save its formatting\n        del joined[other_labels_map[other_label]]\n        if column_label not in self._formats and other_label in other._formats:\n            joined._formats[column_label] = other._formats[other_label]\n\n        return joined.move_to_start(column_label).sort(column_label)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing statistics for each column and place them in a table.", "response": "def stats(self, ops=(min, max, np.median, sum)):\n        \"\"\"Compute statistics for each column and place them in a table.\"\"\"\n        names = [op.__name__ for op in ops]\n        ops = [_zero_on_type_error(op) for op in ops]\n        columns = [[op(column) for op in ops] for column in self.columns]\n        table = type(self)().with_columns(zip(self.labels, columns))\n        stats = table._unused_label('statistic')\n        table[stats] = names\n        table.move_to_start(stats)\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert index to label.", "response": "def _as_label(self, index_or_label):\n        \"\"\"Convert index to label.\"\"\"\n        if isinstance(index_or_label, str):\n            return index_or_label\n        if isinstance(index_or_label, numbers.Integral):\n            return self.labels[index_or_label]\n        else:\n            raise ValueError(str(index_or_label) + ' is not a label or index')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an unused label.", "response": "def _unused_label(self, label):\n        \"\"\"Generate an unused label.\"\"\"\n        original = label\n        existing = self.labels\n        i = 2\n        while label in existing:\n            label = '{}_{}'.format(original, i)\n            i += 1\n        return label"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_column(self, column_or_label):\n        c = column_or_label\n        if isinstance(c, collections.Hashable) and c in self.labels:\n            return self[c]\n        elif isinstance(c, numbers.Integral):\n            return self[c]\n        elif isinstance(c, str):\n            raise ValueError('label \"{}\" not in labels {}'.format(c, self.labels))\n        else:\n            assert len(c) == self.num_rows, 'column length mismatch'\n            return c", "response": "Convert label to column and check column length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new table with one row containing the pth percentile for each column.", "response": "def percentile(self, p):\n        \"\"\"Return a new table with one row containing the pth percentile for\n        each column.\n\n        Assumes that each column only contains one type of value.\n\n        Returns a new table with one row and the same column labels.\n        The row contains the pth percentile of the original column, where the\n        pth percentile of a column is the smallest value that at at least as\n        large as the p% of numbers in the column.\n\n        >>> table = Table().with_columns(\n        ...     'count',  make_array(9, 3, 3, 1),\n        ...     'points', make_array(1, 2, 2, 10))\n        >>> table\n        count | points\n        9     | 1\n        3     | 2\n        3     | 2\n        1     | 10\n        >>> table.percentile(80)\n        count | points\n        9     | 10\n        \"\"\"\n        percentiles = [[_util.percentile(p, column)] for column in self.columns]\n        return self._with_columns(percentiles)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample(self, k=None, with_replacement=True, weights=None):\n        n = self.num_rows\n        if k is None:\n            k = n\n        index = np.random.choice(n, k, replace=with_replacement, p=weights)\n        columns = [[c[i] for i in index] for c in self.columns]\n        sample = self._with_columns(columns)\n        return sample", "response": "Return a new table where k rows are randomly sampled from the original table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new table with the same number of rows and a new column.", "response": "def sample_from_distribution(self, distribution, k, proportions=False):\n        \"\"\"Return a new table with the same number of rows and a new column.\n        The values in the distribution column are define a multinomial.\n        They are replaced by sample counts/proportions in the output.\n\n        >>> sizes = Table(['size', 'count']).with_rows([\n        ...     ['small', 50],\n        ...     ['medium', 100],\n        ...     ['big', 50],\n        ... ])\n        >>> sizes.sample_from_distribution('count', 1000) # doctest: +SKIP\n        size   | count | count sample\n        small  | 50    | 239\n        medium | 100   | 496\n        big    | 50    | 265\n        >>> sizes.sample_from_distribution('count', 1000, True) # doctest: +SKIP\n        size   | count | count sample\n        small  | 50    | 0.24\n        medium | 100   | 0.51\n        big    | 50    | 0.25\n        \"\"\"\n        dist = self._get_column(distribution)\n        total = sum(dist)\n        assert total > 0 and np.all(dist >= 0), 'Counts or a distribution required'\n        dist = dist/sum(dist)\n        sample = np.random.multinomial(k, dist)\n        if proportions:\n            sample = sample / sum(sample)\n        label = self._unused_label(self._as_label(distribution) + ' sample')\n        return self.with_column(label, sample)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split(self, k):\n        if not 1 <= k <= self.num_rows - 1:\n            raise ValueError(\"Invalid value of k. k must be between 1 and the\"\n                             \"number of rows - 1\")\n\n        rows = np.random.permutation(self.num_rows)\n\n        first = self.take(rows[:k])\n        rest = self.take(rows[k:])\n        for column_label in self._formats:\n            first._formats[column_label] = self._formats[column_label]\n            rest._formats[column_label] = self._formats[column_label]\n        return first, rest", "response": "Returns a tuple of two tables where the first table contains the k rows randomly sampled and the second contains the remaining rows."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new table with an additional row.", "response": "def with_row(self, row):\n        \"\"\"Return a table with an additional row.\n\n        Args:\n            ``row`` (sequence): A value for each column.\n\n        Raises:\n            ``ValueError``: If the row length differs from the column count.\n\n        >>> tiles = Table(make_array('letter', 'count', 'points'))\n        >>> tiles.with_row(['c', 2, 3]).with_row(['d', 4, 2])\n        letter | count | points\n        c      | 2     | 3\n        d      | 4     | 2\n        \"\"\"\n        self = self.copy()\n        self.append(row)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new table with additional rows.", "response": "def with_rows(self, rows):\n        \"\"\"Return a table with additional rows.\n\n        Args:\n            ``rows`` (sequence of sequences): Each row has a value per column.\n\n            If ``rows`` is a 2-d array, its shape must be (_, n) for n columns.\n\n        Raises:\n            ``ValueError``: If a row length differs from the column count.\n\n        >>> tiles = Table(make_array('letter', 'count', 'points'))\n        >>> tiles.with_rows(make_array(make_array('c', 2, 3),\n        ...     make_array('d', 4, 2)))\n        letter | count | points\n        c      | 2     | 3\n        d      | 4     | 2\n        \"\"\"\n        self = self.copy()\n        self.append(self._with_columns(zip(*rows)))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_column(self, label, values, *rest):\n        # Ensure that if with_column is called instead of with_columns;\n        # no error is raised.\n        if rest:\n            return self.with_columns(label, values, *rest)\n\n        new_table = self.copy()\n        new_table.append_column(label, values)\n        return new_table", "response": "Return a new table with additional or replaced column."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef with_columns(self, *labels_and_values):\n        if len(labels_and_values) == 1:\n            labels_and_values = labels_and_values[0]\n        if isinstance(labels_and_values, collections.abc.Mapping):\n            labels_and_values = list(labels_and_values.items())\n        if not isinstance(labels_and_values, collections.abc.Sequence):\n            labels_and_values = list(labels_and_values)\n        if not labels_and_values:\n            return self\n        first = labels_and_values[0]\n        if not isinstance(first, str) and hasattr(first, '__iter__'):\n            for pair in labels_and_values:\n                assert len(pair) == 2, 'incorrect columns format'\n            labels_and_values = [x for pair in labels_and_values for x in pair]\n        assert len(labels_and_values) % 2 == 0, 'Even length sequence required'\n        for i in range(0, len(labels_and_values), 2):\n            label, values = labels_and_values[i], labels_and_values[i+1]\n            self = self.with_column(label, values)\n        return self", "response": "Returns a new table with additional or replaced columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relabeled(self, label, new_label):\n        copy = self.copy()\n        copy.relabel(label, new_label)\n        return copy", "response": "Return a new table with label replaced by corresponding new_label."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bin(self, *columns, **vargs):\n        if columns:\n            self = self.select(*columns)\n        if 'normed' in vargs:\n            vargs.setdefault('density', vargs.pop('normed'))\n        density = vargs.get('density', False)\n        tag = 'density' if density else 'count'\n\n        cols = list(self._columns.values())\n        _, bins = np.histogram(cols, **vargs)\n\n        binned = type(self)().with_column('bin', bins)\n        for label in self.labels:\n            counts, _ = np.histogram(self[label], bins=bins, density=density)\n            binned[label + ' ' + tag] = np.append(counts, 0)\n        return binned", "response": "Group values by bin and compute counts per bin by column."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuses the value s HTML rendering if available overriding format_fn.", "response": "def _use_html_if_available(format_fn):\n        \"\"\"Use the value's HTML rendering if available, overriding format_fn.\"\"\"\n        def format_using_as_html(v, label=False):\n            if not label and hasattr(v, 'as_html'):\n                return v.as_html()\n            else:\n                return format_fn(v, label)\n        return format_using_as_html"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning one value formatting function per column.", "response": "def _get_column_formatters(self, max_rows, as_html):\n        \"\"\"Return one value formatting function per column.\n\n        Each function has the signature f(value, label=False) -> str\n        \"\"\"\n        formats = {s: self._formats.get(s, self.formatter) for s in self.labels}\n        cols = self._columns.items()\n        fmts = [formats[k].format_column(k, v[:max_rows]) for k, v in cols]\n        if as_html:\n            fmts = list(map(type(self)._use_html_if_available, fmts))\n        return fmts"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_text(self, max_rows=0, sep=\" | \"):\n        if not max_rows or max_rows > self.num_rows:\n            max_rows = self.num_rows\n        omitted = max(0, self.num_rows - max_rows)\n        labels = self._columns.keys()\n        fmts = self._get_column_formatters(max_rows, False)\n        rows = [[fmt(label, label=True) for fmt, label in zip(fmts, labels)]]\n        for row in itertools.islice(self.rows, max_rows):\n            rows.append([f(v, label=False) for v, f in zip(row, fmts)])\n        lines = [sep.join(row) for row in rows]\n        if omitted:\n            lines.append('... ({} rows omitted)'.format(omitted))\n        return '\\n'.join([line.rstrip() for line in lines])", "response": "Format table as text."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef as_html(self, max_rows=0):\n        if not max_rows or max_rows > self.num_rows:\n            max_rows = self.num_rows\n        omitted = max(0, self.num_rows - max_rows)\n        labels = self.labels\n        lines = [\n            (0, '<table border=\"1\" class=\"dataframe\">'),\n            (1, '<thead>'),\n            (2, '<tr>'),\n            (3, ' '.join('<th>' + label + '</th>' for label in labels)),\n            (2, '</tr>'),\n            (1, '</thead>'),\n            (1, '<tbody>'),\n        ]\n        fmts = self._get_column_formatters(max_rows, True)\n        for row in itertools.islice(self.rows, max_rows):\n            lines += [\n                (2, '<tr>'),\n                (3, ' '.join('<td>' + fmt(v, label=False) + '</td>' for\n                    v, fmt in zip(row, fmts))),\n                (2, '</tr>'),\n            ]\n        lines.append((1, '</tbody>'))\n        lines.append((0, '</table>'))\n        if omitted:\n            lines.append((0, '<p>... ({} rows omitted)</p>'.format(omitted)))\n        return '\\n'.join(4 * indent * ' ' + text for indent, text in lines)", "response": "Format the table as HTML."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index_by(self, column_or_label):\n        column = self._get_column(column_or_label)\n        index = {}\n        for key, row in zip(column, self.rows):\n            index.setdefault(key, []).append(row)\n        return index", "response": "Return a dict keyed by values in a column that contains lists of\n            rows corresponding to each value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_array(self):\n        dt = np.dtype(list(zip(self.labels, (c.dtype for c in self.columns))))\n        arr = np.empty_like(self.columns[0], dt)\n        for label in self.labels:\n            arr[label] = self[label]\n        return arr", "response": "Convert the table to a structured NumPy array."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplots the entire table with the specified columns.", "response": "def plot(self, column_for_xticks=None, select=None, overlay=True, width=6, height=4, **vargs):\n        \"\"\"Plot line charts for the table.\n\n        Args:\n            column_for_xticks (``str/array``): A column containing x-axis labels\n\n        Kwargs:\n            overlay (bool): create a chart with one color per data column;\n                if False, each plot will be displayed separately.\n\n            vargs: Additional arguments that get passed into `plt.plot`.\n                See http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot\n                for additional arguments that can be passed into vargs.\n        Raises:\n            ValueError -- Every selected column must be numerical.\n\n        Returns:\n            Returns a line plot (connected scatter). Each plot is labeled using\n            the values in `column_for_xticks` and one plot is produced for all\n            other columns in self (or for the columns designated by `select`).\n        >>> table = Table().with_columns(\n        ...     'days',  make_array(0, 1, 2, 3, 4, 5),\n        ...     'price', make_array(90.5, 90.00, 83.00, 95.50, 82.00, 82.00),\n        ...     'projection', make_array(90.75, 82.00, 82.50, 82.50, 83.00, 82.50))\n        >>> table\n        days | price | projection\n        0    | 90.5  | 90.75\n        1    | 90    | 82\n        2    | 83    | 82.5\n        3    | 95.5  | 82.5\n        4    | 82    | 83\n        5    | 82    | 82.5\n        >>> table.plot('days') # doctest: +SKIP\n        <line graph with days as x-axis and lines for price and projection>\n        >>> table.plot('days', overlay=False) # doctest: +SKIP\n        <line graph with days as x-axis and line for price>\n        <line graph with days as x-axis and line for projection>\n        >>> table.plot('days', 'price') # doctest: +SKIP\n        <line graph with days as x-axis and line for price>\n        \"\"\"\n        options = self.default_options.copy()\n        options.update(vargs)\n\n        if column_for_xticks is not None:\n            x_data, y_labels = self._split_column_and_labels(column_for_xticks)\n            x_label = self._as_label(column_for_xticks)\n        else:\n            x_data, y_labels = None, self.labels\n            x_label = None\n        if select is not None:\n            y_labels = self._as_labels(select)\n\n        if x_data is not None:\n            self = self.sort(x_data)\n            x_data = np.sort(x_data)\n\n        def draw(axis, label, color):\n            if x_data is None:\n                axis.plot(self[label], color=color, **options)\n            else:\n                axis.plot(x_data, self[label], color=color, **options)\n\n        self._visualize(x_label, y_labels, None, overlay, draw, _vertical_x, width=width, height=height)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a bar chart for the table.", "response": "def bar(self, column_for_categories=None, select=None, overlay=True, width=6, height=4, **vargs):\n        \"\"\"Plot bar charts for the table.\n\n        Each plot is labeled using the values in `column_for_categories` and\n        one plot is produced for every other column (or for the columns\n        designated by `select`).\n\n        Every selected column except `column_for_categories` must be numerical.\n\n        Args:\n            column_for_categories (str): A column containing x-axis categories\n\n        Kwargs:\n            overlay (bool): create a chart with one color per data column;\n                if False, each will be displayed separately.\n\n            vargs: Additional arguments that get passed into `plt.bar`.\n                See http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.bar\n                for additional arguments that can be passed into vargs.\n        \"\"\"\n        options = self.default_options.copy()\n\n        # Matplotlib tries to center the labels, but we already handle that\n        # TODO consider changing the custom centering code and using matplotlib's default\n        vargs['align'] = 'edge'\n        options.update(vargs)\n\n        xticks, labels = self._split_column_and_labels(column_for_categories)\n        if select is not None:\n            labels = self._as_labels(select)\n\n        index = np.arange(self.num_rows)\n\n        def draw(axis, label, color):\n            axis.bar(index-0.5, self[label], 1.0, color=color, **options)\n\n        def annotate(axis, ticks):\n            if (ticks is not None) :\n                tick_labels = [ticks[int(l)] if 0<=l<len(ticks) else '' for l in axis.get_xticks()]\n                axis.set_xticklabels(tick_labels, stretch='ultra-condensed')\n\n        self._visualize(column_for_categories, labels, xticks, overlay, draw, annotate, width=width, height=height)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef group_bar(self, column_label, **vargs):\n        self.group(column_label).bar(column_label, **vargs)", "response": "Plot a bar chart for the table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef barh(self, column_for_categories=None, select=None, overlay=True, width=6, **vargs):\n        options = self.default_options.copy()\n        # Matplotlib tries to center the labels, but we already handle that\n        # TODO consider changing the custom centering code and using matplotlib's default\n        vargs['align'] = 'edge'\n        options.update(vargs)\n\n        yticks, labels = self._split_column_and_labels(column_for_categories)\n        if select is not None:\n            labels = self._as_labels(select)\n        n = len(labels)\n\n        index = np.arange(self.num_rows)\n        margin = 0.1\n        bwidth = 1 - 2 * margin\n        if overlay:\n            bwidth /= len(labels)\n\n        if 'height' in options:\n            height = options.pop('height')\n        else:\n            height = max(4, len(index)/2)\n\n        def draw(axis, label, color):\n            if overlay:\n                ypos = index + margin + (1-2*margin)*(n - 1 - labels.index(label))/n\n            else:\n                ypos = index\n            # barh plots entries in reverse order from bottom to top\n            axis.barh(ypos, self[label][::-1], bwidth,  color=color, **options)\n\n        ylabel = self._as_label(column_for_categories)\n\n        def annotate(axis, ticks):\n            axis.set_yticks(index+0.5) # Center labels on bars\n            # barh plots entries in reverse order from bottom to top\n            axis.set_yticklabels(ticks[::-1], stretch='ultra-condensed')\n            axis.set_xlabel(axis.get_ylabel())\n            axis.set_ylabel(ylabel)\n\n        self._visualize('', labels, yticks, overlay, draw, annotate, width=width, height=height)", "response": "Plot horizontal bar charts for the table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef group_barh(self, column_label, **vargs):\n        self.group(column_label).barh(column_label, **vargs)", "response": "Plot a horizontal bar chart for the table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scatter(self, column_for_x, select=None, overlay=True, fit_line=False,\n        colors=None, labels=None, sizes=None, width=5, height=5, s=20, **vargs):\n        \"\"\"Creates scatterplots, optionally adding a line of best fit.\n\n        Args:\n            ``column_for_x`` (``str``): The column to use for the x-axis values\n                and label of the scatter plots.\n\n        Kwargs:\n            ``overlay`` (``bool``): If true, creates a chart with one color\n                per data column; if False, each plot will be displayed separately.\n\n            ``fit_line`` (``bool``): draw a line of best fit for each set of points.\n\n            ``vargs``: Additional arguments that get passed into `plt.scatter`.\n                See http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.scatter\n                for additional arguments that can be passed into vargs. These\n                include: `marker` and `norm`, to name a couple.\n\n            ``colors``: A column of categories to be used for coloring dots.\n\n            ``labels``: A column of text labels to annotate dots.\n\n            ``sizes``:  A column of values to set the relative areas of dots.\n\n            ``s``: Size of dots. If sizes is also provided, then dots will be\n              in the range 0 to 2 * s.\n\n        Raises:\n            ValueError -- Every column, ``column_for_x`` or ``select``, must be numerical\n\n        Returns:\n            Scatter plot of values of ``column_for_x`` plotted against\n            values for all other columns in self. Each plot uses the values in\n            `column_for_x` for horizontal positions. One plot is produced for\n            all other columns in self as y (or for the columns designated by\n            `select`).\n\n\n        >>> table = Table().with_columns(\n        ...     'x', make_array(9, 3, 3, 1),\n        ...     'y', make_array(1, 2, 2, 10),\n        ...     'z', make_array(3, 4, 5, 6))\n        >>> table\n        x    | y    | z\n        9    | 1    | 3\n        3    | 2    | 4\n        3    | 2    | 5\n        1    | 10   | 6\n        >>> table.scatter('x') # doctest: +SKIP\n        <scatterplot of values in y and z on x>\n\n        >>> table.scatter('x', overlay=False) # doctest: +SKIP\n        <scatterplot of values in y on x>\n        <scatterplot of values in z on x>\n\n        >>> table.scatter('x', fit_line=True) # doctest: +SKIP\n        <scatterplot of values in y and z on x with lines of best fit>\n        \"\"\"\n        options = self.default_options.copy()\n        options.update(vargs)\n\n        x_data, y_labels =  self._split_column_and_labels(column_for_x)\n        if colors is not None:\n            y_labels.remove(self._as_label(colors))\n        if sizes is not None:\n            y_labels.remove(self._as_label(sizes))\n        if select is not None:\n            y_labels = self._as_labels(select)\n        if len(y_labels) > 1 and colors is not None and overlay:\n            warnings.warn(\"Colors and overlay are incompatible in a scatter\")\n            overlay = False\n\n        def draw(axis, label, color):\n            if colors is not None:\n                colored = sorted(np.unique(self.column(colors)))\n                color_list = list(itertools.islice(itertools.cycle(self.chart_colors), len(colored)))\n                color_map = collections.OrderedDict(zip(colored, color_list))\n                color = [color_map[x] for x in self.column(colors)]\n            elif 'color' in options:\n                color = options.pop('color')\n            y_data = self[label]\n            if sizes is not None:\n                max_size = max(self[sizes]) ** 0.5\n                size = 2 * s * self[sizes] ** 0.5 / max_size\n            else:\n                size = s\n            axis.scatter(x_data, y_data, color=color, s=size, **options)\n            if fit_line:\n                m, b = np.polyfit(x_data, self[label], 1)\n                minx, maxx = np.min(x_data),np.max(x_data)\n                axis.plot([minx,maxx],[m*minx+b,m*maxx+b], color=color)\n            if labels is not None:\n                for x, y, label in zip(x_data, y_data, self[labels]):\n                    axis.annotate(label, (x, y),\n                        xytext=(-20, 20),\n                        textcoords='offset points', ha='right', va='bottom',\n                        bbox=dict(boxstyle='round,pad=0.5', fc='white', alpha=0.7),\n                        arrowprops = dict(arrowstyle = '->', connectionstyle = 'arc3,rad=0', color='black'))\n            if colors is not None:\n                import matplotlib.patches as mpatches\n                patches = [mpatches.Patch(color=c, label=v) for (v, c) in color_map.items()]\n                axis.legend(loc=2, bbox_to_anchor=(1.05, 1), handles=patches)\n\n        x_label = self._as_label(column_for_x)\n        self._visualize(x_label, y_labels, None, overlay, draw, _vertical_x, width=width, height=height)", "response": "Creates a scatter plot of the values of the column_for_x column of the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _visualize(self, x_label, y_labels, ticks, overlay, draw, annotate, width=6, height=4):\n        for label in y_labels:\n            if not all(isinstance(x, numbers.Real) for x in self[label]):\n                raise ValueError(\"The column '{0}' contains non-numerical \"\n                    \"values. A plot cannot be drawn for this column.\"\n                    .format(label))\n\n        n = len(y_labels)\n        colors = list(itertools.islice(itertools.cycle(self.chart_colors), n))\n        if overlay and n > 1:\n            _, axis = plt.subplots(figsize=(width, height))\n            if x_label is not None:\n                axis.set_xlabel(x_label)\n            for label, color in zip(y_labels, colors):\n                draw(axis, label, color)\n            if ticks is not None:\n                annotate(axis, ticks)\n            axis.legend(y_labels, loc=2, bbox_to_anchor=(1.05, 1))\n            type(self).plots.append(axis)\n        else:\n            fig, axes = plt.subplots(n, 1, figsize=(width, height*n))\n            if not isinstance(axes, collections.Iterable):\n                axes=[axes]\n            for axis, y_label, color in zip(axes, y_labels, colors):\n                draw(axis, y_label, color)\n                axis.set_ylabel(y_label, fontsize=16)\n                if x_label is not None:\n                    axis.set_xlabel(x_label, fontsize=16)\n                if ticks is not None:\n                    annotate(axis, ticks)\n                type(self).plots.append(axis)", "response": "Generic visualization that overlays or separates the draw function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_column_and_labels(self, column_or_label):\n        column = None if column_or_label is None else self._get_column(column_or_label)\n        labels = [label for i, label in enumerate(self.labels) if column_or_label not in (i, label)]\n        return column, labels", "response": "Return the specified column and labels of other columns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw histograms of each category in a column.", "response": "def pivot_hist(self, pivot_column_label, value_column_label, overlay=True, width=6, height=4, **vargs):\n        \"\"\"Draw histograms of each category in a column.\"\"\"\n        warnings.warn(\"pivot_hist is deprecated; use \"\n                      \"hist(value_column_label, group=pivot_column_label), or \"\n                      \"with side_by_side=True if you really want side-by-side \"\n                      \"bars.\")\n        pvt_labels = np.unique(self[pivot_column_label])\n        pvt_columns = [self[value_column_label][np.where(self[pivot_column_label] == pivot)] for pivot in pvt_labels]\n        n = len(pvt_labels)\n        colors = list(itertools.islice(itertools.cycle(self.chart_colors), n))\n        if overlay:\n            plt.figure(figsize=(width, height))\n            vals, bins, patches = plt.hist(pvt_columns, color=colors, **vargs)\n            plt.legend(pvt_labels)\n        else:\n            _, axes = plt.subplots(n, 1, figsize=(width, height * n))\n            vals = []\n            bins = None\n            for axis, label, column, color in zip(axes, pvt_labels, pvt_columns, colors):\n                if isinstance(bins, np.ndarray):\n                    avals, abins, patches = axis.hist(column, color=color, bins=bins, **vargs)\n                else:\n                    avals, abins, patches = axis.hist(column, color=color, **vargs)\n                axis.set_xlabel(label, fontsize=16)\n                vals.append(avals)\n                if not isinstance(bins, np.ndarray):\n                    bins = abins\n                else:\n                    assert bins.all() == abins.all(), \"Inconsistent bins in hist\"\n        t = type(self)()\n        t['start'] = bins[0:-1]\n        t['end'] = bins[1:]\n        for label, column in zip(pvt_labels,vals):\n            t[label] = column"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hist(self, *columns, overlay=True, bins=None, bin_column=None, unit=None, counts=None, group=None, side_by_side=False, width=6, height=4, **vargs):\n        if counts is not None and bin_column is None:\n            warnings.warn(\"counts arg of hist is deprecated; use bin_column\")\n            bin_column=counts\n        if columns:\n            columns_included = list(columns)\n            if bin_column is not None:\n                columns_included.append(bin_column)\n            if group is not None:\n                columns_included.append(group)\n            self = self.select(*columns_included)\n        if group is not None:\n            if bin_column is not None:\n                raise ValueError(\"Using bin_column and group together is \"\n                                 \"currently unsupported.\")\n            if len(columns) > 1:\n                raise ValueError(\"Using group with multiple histogram value \"\n                                 \"columns is currently unsupported.\")\n\n        # Check for non-numerical values and raise a ValueError if any found\n        for col in self:\n            if col != group and any(isinstance(cell, np.flexible) for cell in self[col]):\n                raise ValueError(\"The column '{0}' contains non-numerical \"\n                    \"values. A histogram cannot be drawn for this table.\"\n                    .format(col))\n\n\n        if bin_column is not None and bins is None:\n            bins = np.unique(self.column(bin_column))\n        if bins is not None:\n            vargs['bins'] = bins\n\n        # Matplotlib has deprecated the normed keyword.\n        # TODO consider changing this function to use density= instead too\n        if 'normed' not in vargs and 'density' not in vargs:\n            vargs['density'] = True\n        elif 'normed' in vargs and 'density' not in vargs:\n            vargs['density'] = vargs.pop('normed')\n        elif 'normed' in vargs and 'density' in vargs:\n            raise ValueError(\"You can't specify both normed and density. \"\n                             \"Use one or the other.\")\n\n        def prepare_hist_with_bin_column(bin_column):\n            # This code is factored as a function for clarity only.\n            weight_columns = [c for c in self.labels if c != bin_column]\n            bin_values = self.column(bin_column)\n            values_dict = [(w.rstrip(' count'), (bin_values, self.column(w))) for w in weight_columns]\n            return values_dict\n\n        def prepare_hist_with_group(group):\n            # This code is factored as a function for clarity only.\n            grouped = self.group(group, np.array)\n            if grouped.num_rows > 20:\n                warnings.warn(\"It looks like you're making a grouped histogram with \"\n                              \"a lot of groups ({:d}), which is probably incorrect.\"\n                              .format(grouped.num_rows))\n            return [(\"{}={}\".format(group, k), (v[0][1],)) for k, v in grouped.index_by(group).items()]\n\n        # Populate values_dict: An ordered dict from column name to singleton\n        # tuple of array of values or a (values, weights) pair of arrays.  If\n        # any values have weights, they all must have weights.\n        if bin_column is not None:\n            values_dict = prepare_hist_with_bin_column(bin_column)\n        elif group is not None:\n            values_dict = prepare_hist_with_group(group)\n        else:\n            values_dict = [(k, (self.column(k),)) for k in self.labels]\n        values_dict = collections.OrderedDict(values_dict)\n\n        def draw_hist(values_dict):\n            # This code is factored as a function for clarity only.\n            n = len(values_dict)\n            colors = [rgb_color + (self.default_alpha,) for rgb_color in\n                itertools.islice(itertools.cycle(self.chart_colors), n)]\n            hist_names = list(values_dict.keys())\n            values = [v[0] for v in values_dict.values()]\n            weights = [v[1] for v in values_dict.values() if len(v) > 1]\n            if n > len(weights) > 0:\n                raise ValueError(\"Weights were provided for some columns, but not \"\n                                 \" all, and that's not supported.\")\n            if vargs['density']:\n                y_label = 'Percent per ' + (unit if unit else 'unit')\n                percentage = plt.FuncFormatter(lambda x, _: \"{:g}\".format(100*x))\n            else:\n                y_label = 'Count'\n\n            if overlay and n > 1:\n                # Reverse because legend prints bottom-to-top\n                values = values[::-1]\n                weights = weights[::-1]\n                colors = list(colors)[::-1]\n                if len(weights) == n:\n                    vargs['weights'] = weights\n                if not side_by_side:\n                    vargs.setdefault('histtype', 'stepfilled')\n                figure = plt.figure(figsize=(width, height))\n                plt.hist(values, color=colors, **vargs)\n                axis = figure.get_axes()[0]\n                _vertical_x(axis)\n                axis.set_ylabel(y_label)\n                if vargs['density']:\n                    axis.yaxis.set_major_formatter(percentage)\n                if unit:\n                    axis.set_xlabel('(' + unit + ')', fontsize=16)\n                plt.legend(hist_names, loc=2, bbox_to_anchor=(1.05, 1))\n                type(self).plots.append(axis)\n            else:\n                _, axes = plt.subplots(n, 1, figsize=(width, height * n))\n                if 'bins' in vargs:\n                    bins = vargs['bins']\n                    if isinstance(bins, numbers.Integral) and bins > 76 or hasattr(bins, '__len__') and len(bins) > 76:\n                        # Use stepfilled when there are too many bins\n                        vargs.setdefault('histtype', 'stepfilled')\n                if n == 1:\n                    axes = [axes]\n                for i, (axis, hist_name, values_for_hist, color) in enumerate(zip(axes, hist_names, values, colors)):\n                    axis.set_ylabel(y_label)\n                    if vargs['density']:\n                        axis.yaxis.set_major_formatter(percentage)\n                    x_unit = ' (' + unit + ')' if unit else ''\n                    if len(weights) == n:\n                        vargs['weights'] = weights[i]\n                    axis.set_xlabel(hist_name + x_unit, fontsize=16)\n                    axis.hist(values_for_hist, color=color, **vargs)\n                    _vertical_x(axis)\n                    type(self).plots.append(axis)\n\n        draw_hist(values_dict)", "response": "Plots one histogram for each column in columns."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hist_of_counts(self, *columns, overlay=True, bins=None, bin_column=None,\n                       group=None, side_by_side=False, width=6, height=4, **vargs):\n        \"\"\"\n        Plots one count-based histogram for each column in columns. The\n        heights of each bar will represent the counts, and all the bins\n        must be of equal size.\n\n        If no column is specified, plot all columns.\n\n        Kwargs:\n            overlay (bool): If True, plots 1 chart with all the histograms\n                overlaid on top of each other (instead of the default behavior\n                of one histogram for each column in the table). Also adds a\n                legend that matches each bar color to its column.  Note that\n                if the histograms are not overlaid, they are not forced to the\n                same scale.\n\n            bins (array or int): Lower bound for each bin in the\n                histogram or number of bins. If None, bins will\n                be chosen automatically.\n\n            bin_column (column name or index): A column of bin lower bounds.\n                All other columns are treated as counts of these bins.\n                If None, each value in each row is assigned a count of 1.\n\n            group (column name or index): A column of categories.  The rows are\n                grouped by the values in this column, and a separate histogram is\n                generated for each group.  The histograms are overlaid or plotted\n                separately depending on the overlay argument.  If None, no such\n                grouping is done.\n\n            side_by_side (bool): Whether histogram bins should be plotted side by\n                side (instead of directly overlaid).  Makes sense only when\n                plotting multiple histograms, either by passing several columns\n                or by using the group option.\n\n            vargs: Additional arguments that get passed into :func:plt.hist.\n                See http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist\n                for additional arguments that can be passed into vargs. These\n                include: `range`, `cumulative`, and\n                `orientation`, to name a few.\n\n        >>> t = Table().with_columns(\n        ...     'count',  make_array(9, 3, 3, 1),\n        ...     'points', make_array(1, 2, 2, 10))\n        >>> t\n        count | points\n        9     | 1\n        3     | 2\n        3     | 2\n        1     | 10\n        >>> t.hist_of_counts() # doctest: +SKIP\n        <histogram of values in count with counts on y-axis>\n        <histogram of values in points with counts on y-axis>\n\n        >>> t = Table().with_columns(\n        ...     'value', make_array(101, 102, 103),\n        ...     'count', make_array(5, 10, 5))\n        >>> t.hist_of_counts(bin_column='value') # doctest: +SKIP\n        <histogram of values weighted by corresponding counts>\n\n        >>> t = Table().with_columns(\n        ...     'value',    make_array(1,   2,   3,   2,   5  ),\n        ...     'category', make_array('a', 'a', 'a', 'b', 'b'))\n        >>> t.hist('value', group='category') # doctest: +SKIP\n        <two overlaid histograms of the data [1, 2, 3] and [2, 5]>\n        \"\"\"\n\n        if bin_column is not None and bins is None:\n            bins = np.unique(self.column(bin_column))\n            # TODO ensure counts are integers even when `columns` is empty\n            for column in columns:\n                if not _is_array_integer(self.column(column)):\n                    raise ValueError('The column {0} contains non-integer values '\n                                     'When using hist_of_counts with bin_columns, '\n                                     'all columns should contain counts.'\n                                     .format(column))\n\n        if vargs.get('normed', False) or vargs.get('density', False):\n            raise ValueError(\"hist_of_counts is for displaying counts only, \"\n                             \"and should not be used with the normed or \"\n                             \"density keyword arguments\")\n        vargs['density'] = False\n\n        if bins is not None:\n            if len(bins) < 2:\n                raise ValueError(\"bins must have at least two items\")\n            diffs = np.diff(sorted(bins))\n            # Diffs should all be equal (up to floating point error)\n            normalized_diff_deviances = np.abs((diffs - diffs[0])/diffs[0])\n            if np.any(normalized_diff_deviances > 1e-11):\n                raise ValueError(\"Bins of unequal size should not be used \"\n                                 \"with hist_of_counts. Please use hist() and \"\n                                 \"make sure to set normed=True\")\n        return self.hist(*columns, overlay=overlay, bins=bins, bin_column=bin_column, group=group, side_by_side=side_by_side, width=width, height=height, **vargs)", "response": "Plots one count - based histogram for each column in columns."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot a boxplot for the table.", "response": "def boxplot(self, **vargs):\n        \"\"\"Plots a boxplot for the table.\n\n        Every column must be numerical.\n\n        Kwargs:\n            vargs: Additional arguments that get passed into `plt.boxplot`.\n                See http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.boxplot\n                for additional arguments that can be passed into vargs. These include\n                `vert` and `showmeans`.\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: The Table contains columns with non-numerical values.\n\n        >>> table = Table().with_columns(\n        ...     'test1', make_array(92.5, 88, 72, 71, 99, 100, 95, 83, 94, 93),\n        ...     'test2', make_array(89, 84, 74, 66, 92, 99, 88, 81, 95, 94))\n        >>> table\n        test1 | test2\n        92.5  | 89\n        88    | 84\n        72    | 74\n        71    | 66\n        99    | 92\n        100   | 99\n        95    | 88\n        83    | 81\n        94    | 95\n        93    | 94\n        >>> table.boxplot() # doctest: +SKIP\n        <boxplot of test1 and boxplot of test2 side-by-side on the same figure>\n        \"\"\"\n        # Check for non-numerical values and raise a ValueError if any found\n        for col in self:\n            if any(isinstance(cell, np.flexible) for cell in self[col]):\n                raise ValueError(\"The column '{0}' contains non-numerical \"\n                    \"values. A histogram cannot be drawn for this table.\"\n                    .format(col))\n\n        columns = self._columns.copy()\n        vargs['labels'] = columns.keys()\n        values = list(columns.values())\n        plt.boxplot(values, **vargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef percentile(p, arr=None):\n    if arr is None:\n        return lambda arr: percentile(p, arr)\n    if hasattr(p, '__iter__'):\n        return np.array([percentile(x, arr) for x in p])\n    if p == 0:\n        return min(arr)\n    assert 0 < p <= 100, 'Percentile requires a percent'\n    i = (p/100) * len(arr)\n    return sorted(arr)[math.ceil(i) - 1]", "response": "Returns the pth percentile of the input array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef proportions_from_distribution(table, label, sample_size,\n                                  column_name='Random Sample'):\n    \"\"\"\n    Adds a column named ``column_name`` containing the proportions of a random\n    draw using the distribution in ``label``.\n\n    This method uses ``np.random.multinomial`` to draw ``sample_size`` samples\n    from the distribution in ``table.column(label)``, then divides by\n    ``sample_size`` to create the resulting column of proportions.\n\n    Args:\n        ``table``: An instance of ``Table``.\n\n        ``label``: Label of column in ``table``. This column must contain a\n            distribution (the values must sum to 1).\n\n        ``sample_size``: The size of the sample to draw from the distribution.\n\n        ``column_name``: The name of the new column that contains the sampled\n            proportions. Defaults to ``'Random Sample'``.\n\n    Returns:\n        A copy of ``table`` with a column ``column_name`` containing the\n        sampled proportions. The proportions will sum to 1.\n\n    Throws:\n        ``ValueError``: If the ``label`` is not in the table, or if\n            ``table.column(label)`` does not sum to 1.\n    \"\"\"\n    proportions = sample_proportions(sample_size, table.column(label))\n    return table.with_column('Random Sample', proportions)", "response": "Returns a new table with the proportions drawn from the distribution in label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef table_apply(table, func, subset=None):\n    from . import Table\n    df = table.to_df()\n\n    if subset is not None:\n        # Iterate through columns\n        subset = np.atleast_1d(subset)\n        if any([i not in df.columns for i in subset]):\n            err = np.where([i not in df.columns for i in subset])[0]\n            err = \"Column mismatch: {0}\".format(\n                [subset[i] for i in err])\n            raise ValueError(err)\n        for col in subset:\n            df[col] = df[col].apply(func)\n    else:\n        df = df.apply(func)\n    if isinstance(df, pd.Series):\n        # Reshape it so that we can easily convert back\n        df = pd.DataFrame(df).T\n    tab = Table.from_df(df)\n    return tab", "response": "Applies a function to each column and returns a Table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nminimizing a function f of one or more arguments.", "response": "def minimize(f, start=None, smooth=False, log=None, array=False, **vargs):\n    \"\"\"Minimize a function f of one or more arguments.\n\n    Args:\n        f: A function that takes numbers and returns a number\n\n        start: A starting value or list of starting values\n\n        smooth: Whether to assume that f is smooth and use first-order info\n\n        log: Logging function called on the result of optimization (e.g. print)\n\n        vargs: Other named arguments passed to scipy.optimize.minimize\n\n    Returns either:\n        (a) the minimizing argument of a one-argument function\n        (b) an array of minimizing arguments of a multi-argument function\n    \"\"\"\n    if start is None:\n        assert not array, \"Please pass starting values explicitly when array=True\"\n        arg_count = f.__code__.co_argcount\n        assert arg_count > 0, \"Please pass starting values explicitly for variadic functions\"\n        start = [0] * arg_count\n    if not hasattr(start, '__len__'):\n        start = [start]\n\n    if array:\n        objective = f\n    else:\n        @functools.wraps(f)\n        def objective(args):\n            return f(*args)\n\n    if not smooth and 'method' not in vargs:\n        vargs['method'] = 'Powell'\n    result = optimize.minimize(objective, start, **vargs)\n    if log is not None:\n        log(result)\n    if len(start) == 1:\n        return result.x.item(0)\n    else:\n        return result.x"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _lat_lons_from_geojson(s):\n    if len(s) >= 2 and isinstance(s[0], _number) and isinstance(s[0], _number):\n        lat, lon = s[1], s[0]\n        return [(lat, lon)]\n    else:\n        return [lat_lon for sub in s for lat_lon in _lat_lons_from_geojson(sub)]", "response": "Return a latitude - longitude pairs from nested GeoJSON coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate HTML to display map.", "response": "def as_html(self):\n        \"\"\"Generate HTML to display map.\"\"\"\n        if not self._folium_map:\n            self.draw()\n        return self._inline_map(self._folium_map, self._width, self._height)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows the publish HTML.", "response": "def show(self):\n        \"\"\"Publish HTML.\"\"\"\n        IPython.display.display(IPython.display.HTML(self.as_html()))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a copy of the current Map.", "response": "def copy(self):\n        \"\"\"\n        Copies the current Map into a new one and returns it.\n        \"\"\"\n\n        m = Map(features=self._features, width=self._width,\n                   height=self._height, **self._attrs)\n        m._folium_map = self._folium_map\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _autozoom(self):\n        bounds = self._autobounds()\n        attrs = {}\n\n        midpoint = lambda a, b: (a + b)/2\n        attrs['location'] = (\n            midpoint(bounds['min_lat'], bounds['max_lat']),\n            midpoint(bounds['min_lon'], bounds['max_lon'])\n        )\n\n        # self._folium_map.fit_bounds(\n        #     [bounds['min_long'], bounds['min_lat']],\n        #     [bounds['max_long'], bounds['max_lat']]\n        # )\n\n        # remove the following with new Folium release\n        # rough approximation, assuming max_zoom is 18\n        import math\n        try:\n            lat_diff = bounds['max_lat'] - bounds['min_lat']\n            lon_diff = bounds['max_lon'] - bounds['min_lon']\n            area, max_area = lat_diff*lon_diff, 180*360\n            if area:\n                factor = 1 + max(0, 1 - self._width/1000)/2 + max(0, 1-area**0.5)/2\n                zoom = math.log(area/max_area)/-factor\n            else:\n                zoom = self._default_zoom\n            zoom = max(1, min(18, round(zoom)))\n            attrs['zoom_start'] = zoom\n        except ValueError as e:\n            raise Exception('Check that your locations are lat-lon pairs', e)\n\n        return attrs", "response": "Calculate zoom and location."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenders features as a FeatureCollection.", "response": "def geojson(self):\n        \"\"\"Render features as a FeatureCollection.\"\"\"\n        return {\n            \"type\": \"FeatureCollection\",\n            \"features\": [f.geojson(i) for i, f in self._features.items()]\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef color(self, values, ids=(), key_on='feature.id', palette='YlOrBr', **kwargs):\n        # Set values and ids to both be simple sequences by inspecting values\n        id_name, value_name = 'IDs', 'values'\n        if isinstance(values, collections.abc.Mapping):\n            assert not ids, 'IDs and a map cannot both be used together'\n            if hasattr(values, 'columns') and len(values.columns) == 2:\n                table = values\n                ids, values = table.columns\n                id_name, value_name = table.labels\n            else:\n                dictionary = values\n                ids, values = list(dictionary.keys()), list(dictionary.values())\n        if len(ids) != len(values):\n            assert len(ids) == 0\n            # Use indices as IDs\n            ids = list(range(len(values)))\n\n        m = self._create_map()\n        data = pandas.DataFrame({id_name: ids, value_name: values})\n        attrs = {\n            'geo_str': json.dumps(self.geojson()),\n            'data': data,\n            'columns': [id_name, value_name],\n            'key_on': key_on,\n            'fill_color': palette,\n        }\n        kwargs.update(attrs)\n        m.geo_json(**kwargs)\n        colored = self.format()\n        colored._folium_map = m\n        return colored", "response": "Color map features by binning values and ids."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef overlay(self, feature, color='Blue', opacity=0.6):\n        result = self.copy()\n        if type(feature) == Table:\n            # if table of features e.g. Table.from_records(taz_map.features)\n            if 'feature' in feature:\n                feature = feature['feature']\n\n            # if marker table e.g. table with columns: latitudes,longitudes,popup,color,radius\n            else:\n                feature = Circle.map_table(feature)\n\n        if type(feature) in [list, np.ndarray]:\n            for f in feature:\n                f._attrs['fill_color'] = color\n                f._attrs['fill_opacity'] = opacity\n                f.draw_on(result._folium_map)\n\n        elif type(feature) == Map:\n            for i in range(len(feature._features)):\n                f = feature._features[i]\n                f._attrs['fill_color'] = color\n                f._attrs['fill_opacity'] = opacity\n                f.draw_on(result._folium_map)\n        elif type(feature) == Region:\n            feature._attrs['fill_color'] = color\n            feature._attrs['fill_opacity'] = opacity\n            feature.draw_on(result._folium_map)\n        return result", "response": "Overlays a feature on the map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a geoJSON string object or file. Return a dict of features keyed by ID.", "response": "def read_geojson(cls, path_or_json_or_string):\n        \"\"\"Read a geoJSON string, object, or file. Return a dict of features keyed by ID.\"\"\"\n        assert path_or_json_or_string\n        data = None\n        if isinstance(path_or_json_or_string, (dict, list)):\n            data = path_or_json_or_string\n        try:\n            data = json.loads(path_or_json_or_string)\n        except ValueError:\n            pass\n        try:\n            path = path_or_json_or_string\n            if path.endswith('.gz') or path.endswith('.gzip'):\n                import gzip\n                contents = gzip.open(path, 'r').read().decode('utf-8')\n            else:\n                contents = open(path, 'r').read()\n            data = json.loads(contents)\n        except FileNotFoundError:\n            pass\n        # TODO web address\n        assert data, 'MapData accepts a valid geoJSON object, geoJSON string, or path to a geoJSON file'\n        return cls(cls._read_geojson_features(data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _read_geojson_features(data, features=None, prefix=\"\"):\n        if features is None:\n            features = collections.OrderedDict()\n        for i, feature in enumerate(data['features']):\n            key = feature.get('id', prefix + str(i))\n            feature_type = feature['geometry']['type']\n            if feature_type == 'FeatureCollection':\n                _read_geojson_features(feature, features, prefix + '.' + key)\n            elif feature_type == 'Point':\n                value = Circle._convert_point(feature)\n            elif feature_type in ['Polygon', 'MultiPolygon']:\n                value = Region(feature)\n            else:\n                # TODO Support all http://geojson.org/geojson-spec.html#geometry-objects\n                value = None\n            features[key] = value\n        return features", "response": "Return a dict of features keyed by ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef draw_on(self, folium_map):\n        f = getattr(folium_map, self._map_method_name)\n        f(**self._folium_kwargs)", "response": "Add feature to Folium map object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _convert_point(cls, feature):\n        lon, lat = feature['geometry']['coordinates']\n        popup = feature['properties'].get('name', '')\n        return cls(lat, lon)", "response": "Convert a GeoJSON point to a Marker."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef map(cls, latitudes, longitudes, labels=None, colors=None, areas=None, **kwargs):\n        assert len(latitudes) == len(longitudes)\n        assert areas is None or hasattr(cls, '_has_radius'), \"A \" + cls.__name__ + \" has no radius\"\n        inputs = [latitudes, longitudes]\n        if labels is not None:\n            assert len(labels) == len(latitudes)\n            inputs.append(labels)\n        else:\n            inputs.append((\"\",) * len(latitudes))\n        if colors is not None:\n            assert len(colors) == len(latitudes)\n            inputs.append(colors)\n        if areas is not None:\n            assert len(areas) == len(latitudes)\n            inputs.append(np.array(areas) ** 0.5 / math.pi)\n        ms = [cls(*args, **kwargs) for args in zip(*inputs)]\n        return Map(ms)", "response": "Return a Map object from columns of coordinates labels colors and areas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef polygons(self):\n        if self.type == 'Polygon':\n            polygons = [self._geojson['geometry']['coordinates']]\n        elif self.type == 'MultiPolygon':\n            polygons = self._geojson['geometry']['coordinates']\n        return [   [   [_lat_lons_from_geojson(s) for\n                        s in ring  ]              for\n                    ring in polygon]              for\n                polygon in polygons]", "response": "Return a list of polygons describing the region."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn GeoJSON with ID substituted.", "response": "def geojson(self, feature_id):\n        \"\"\"Return GeoJSON with ID substituted.\"\"\"\n        if self._geojson.get('id', feature_id) == feature_id:\n            return self._geojson\n        else:\n            geo = self._geojson.copy()\n            geo['id'] = feature_id\n            return geo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_column(self, label, column):\n        if len(column) == 0:\n            val_width = 0\n        else:\n            val_width = max(len(self.format_value(v)) for v in column)\n        val_width = min(val_width, self.max_width)\n        width = max(val_width, len(str(label)), self.min_width, len(self.etc))\n        def pad(value, label=False):\n            if label:\n                raw = value\n            else:\n                raw = self.format_value(value)\n            if len(raw) > width:\n                prefix = raw[:width-len(self.etc)] + self.etc\n            else:\n                prefix = raw\n            return prefix.ljust(width)\n        return pad", "response": "Return a formatting function that pads & truncates values."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts string 93000. 00 to float 93000. 0.", "response": "def convert_value(self, value):\n        \"\"\"Convert string 93,000.00 to float 93000.0.\"\"\"\n        if isinstance(value, str):\n            value = value.replace(self.separator, '')\n            if self.decimal_point not in value:\n                return int(value)\n            else:\n                return float(value.replace(self.decimal_point, '.'))\n        elif self.int_to_float:\n            return float(value)\n        else:\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert_value(self, value):\n        if isinstance(value, str):\n            assert value.startswith(self.symbol), \"Currency does not start with \" + self.symbol\n            value = value.lstrip(self.symbol)\n        return super().convert_value(value)", "response": "Convert value to float."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef encode(value):\n\n  if value > MAX_VALUE:\n    raise ValueError('value {!r} exceeds MAC address range'.format(value))\n  if value < 0:\n    raise ValueError('value must not be negative')\n\n  # todo: convert to the right byte order. the resulting\n  # mac address is reversed on my machine compared to the\n  # mac address displayed by the hello-myo SDK sample.\n  # See issue #7\n\n  string = ('%x' % value).rjust(12, '0')\n  assert len(string) == 12\n\n  result = ':'.join(''.join(pair) for pair in zip(*[iter(string)]*2))\n  return result.upper()", "response": "Encodes the given number to a MAC address ASCII string in binary form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecode a binary MAC address tring into a number.", "response": "def decode(bstr):\n  \"\"\"\n  Decodes an ASCII encoded binary MAC address tring into a number.\n  \"\"\"\n\n  bstr = bstr.replace(b':', b'')\n  if len(bstr) != 12:\n    raise ValueError('not a valid MAC address: {!r}'.format(bstr))\n\n  try:\n    return int(bstr, 16)\n  except ValueError:\n    raise ValueError('not a valid MAC address: {!r}'.format(bstr))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(lib_name=None, bin_path=None, sdk_path=None):\n\n  if sum(bool(x) for x in [lib_name, bin_path, sdk_path]) > 1:\n    raise ValueError('expected zero or one arguments')\n\n  if sdk_path:\n    if sys.platform.startswith('win32'):\n      bin_path = os.path.join(sdk_path, 'bin')\n    elif sys.platform.startswith('darwin'):\n      bin_path = os.path.join(sdk_path, 'myo.framework')\n    else:\n      raise RuntimeError('unsupported platform: {!r}'.format(sys.platform))\n  if bin_path:\n    lib_name = os.path.join(bin_path, _getdlname())\n  if not lib_name:\n    lib_name = _getdlname()\n\n  global libmyo\n  libmyo = ffi.dlopen(lib_name)", "response": "Initialize Myo SDK by loading the shared library."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(self, handler, duration_ms):\n\n    if not callable(handler):\n      if hasattr(handler, 'on_event'):\n        handler = handler.on_event\n      else:\n        raise TypeError('expected callable or DeviceListener')\n\n    with self._lock:\n      if self._running:\n        raise RuntimeError('a handler is already running in the Hub')\n      self._running = True\n      self._stop_requested = False\n      self._stopped = False\n\n    exc_box = []\n\n    def callback_on_error(*exc_info):\n      exc_box.append(exc_info)\n      with self._lock:\n        self._stopped = True\n      return HandlerResult.stop\n\n    def callback(_, event):\n      with self._lock:\n        if self._stop_requested:\n          self._stopped = True\n          return HandlerResult.stop\n\n      result = handler(Event(event))\n      if result is None or result is True:\n        result = HandlerResult.continue_\n      elif result is False:\n        result = HandlerResult.stop\n      else:\n        result = HandlerResult(result)\n      if result == HandlerResult.stop:\n        with self._lock:\n          self._stopped = True\n\n      return result\n\n    cdecl = 'libmyo_handler_result_t(void*, libmyo_event_t)'\n    callback = ffi.callback(cdecl, callback, onerror=callback_on_error)\n\n    try:\n      error = ErrorDetails()\n      libmyo.libmyo_run(self._handle[0], duration_ms, callback, ffi.NULL, error.handle)\n      error.raise_for_kind()\n      if exc_box:\n        six.reraise(*exc_box[0])\n    finally:\n      with self._lock:\n        self._running = False\n        result = not self._stopped\n\n    return result", "response": "Runs the handler function for the specified duration_ms milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_single_device(self, timeout=None, interval=0.5):\n\n    timer = TimeoutManager(timeout)\n    with self._cond:\n      # As long as there are no Myo's connected, wait until we\n      # get notified about a change.\n      while not timer.check():\n        # Check if we found a Myo that is connected.\n        for device in self._devices.values():\n          if device.connected:\n            return device\n        self._cond.wait(timer.remainder(interval))\n\n    return None", "response": "Waits until a Myo is paired and connected with the Hub and returns the Myo object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the time interval has passed.", "response": "def check(self):\n    \"\"\"\n    Returns #True if the time interval has passed.\n    \"\"\"\n\n    if self.value is None:\n      return True\n    return (time.clock() - self.start) >= self.value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets the start time of the interval to the specified value.", "response": "def reset(self, value=None):\n    \"\"\"\n    Resets the start time of the interval to now or the specified value.\n    \"\"\"\n\n    if value is None:\n      value = time.clock()\n    self.start = value\n    if self.value_on_reset:\n      self.value = self.value_on_reset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_and_reset(self, value=None):\n\n    if self.check():\n      self.reset(value)\n      return True\n    return False", "response": "Checks if the sequence contains a value and resets it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(self):\n\n    if self.value is None:\n      return False\n    return (time.clock() - self.start) >= self.value", "response": "Checks if the timeout is exceeded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the time remaining for the timeout or max_value if max_value is larger.", "response": "def remainder(self, max_value=None):\n    \"\"\"\n    Returns the time remaining for the timeout, or *max_value* if that\n    remainder is larger.\n    \"\"\"\n\n    if self.value is None:\n      return max_value\n    remainder = self.value - (time.clock() - self.start)\n    if remainder < 0.0:\n      return 0.0\n    elif max_value is not None and remainder > max_value:\n      return max_value\n    else:\n      return remainder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef normalized(self):\n\n    norm = self.magnitude()\n    return Vector(self.x / norm, self.y / norm, self.z / norm)", "response": "Returns a normalized copy of this vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dot(self, rhs):\n\n    return self.x * rhs.x + self.y * rhs.y + self.z * rhs.z", "response": "Return the dot product of this vector and rhs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cross(self, rhs):\n\n    return Vector(\n      self.y * rhs.z - self.z * rhs.y,\n      self.z * rhs.x - self.x * rhs.z,\n      self.x * rhs.y - self.y * rhs.x)", "response": "Returns the cross product of this vector and rhs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef angle_to(self, rhs):\n\n    return math.acos(self.dot(rhs) / (self.magnitude() * rhs.magnitude()))", "response": "Return the angle between this vector and rhs in radians."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the magnitude of the quaternion.", "response": "def magnitude(self):\n    \"\"\"\n    Returns the magnitude of the quaternion.\n    \"\"\"\n\n    return math.sqrt(self.x ** 2 + self.y ** 2 + self.z ** 2 + self.w ** 2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the unit quaternion corresponding to the same rotation as this one.", "response": "def normalized(self):\n    \"\"\"\n    Returns the unit quaternion corresponding to the same rotation\n    as this one.\n    \"\"\"\n\n    magnitude = self.magnitude()\n    return Quaternion(\n      self.x / magnitude, self.y / magnitude,\n      self.z / magnitude, self.w / magnitude)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rotate(self, vec):\n\n    qvec = self * Quaternion(vec.x, vec.y, vec.z, 0) * ~self\n    return type(vec)(qvec.x, qvec.y, qvec.z)", "response": "Rotates this vector by another vector."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the Roll of the Quaternion.", "response": "def roll(self):\n    \"\"\" Calculates the Roll of the Quaternion. \"\"\"\n\n    x, y, z, w = self.x, self.y, self.z, self.w\n    return math.atan2(2*y*w - 2*x*z, 1 - 2*y*y - 2*z*z)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pitch(self):\n\n    x, y, z, w = self.x, self.y, self.z, self.w\n    return math.atan2(2*x*w - 2*y*z, 1 - 2*x*x - 2*z*z)", "response": "Calculates the Pitch of the Quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the Yaw of the Quaternion.", "response": "def yaw(self):\n    \"\"\" Calculates the Yaw of the Quaternion. \"\"\"\n\n    x, y, z, w = self.x, self.y, self.z, self.w\n    return math.asin(2*x*y + 2*z*w)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the Roll Pitch and Yaw of the Quaternion.", "response": "def rpy(self):\n    \"\"\" Calculates the Roll, Pitch and Yaw of the Quaternion. \"\"\"\n\n    x, y, z, w = self.x, self.y, self.z, self.w\n    roll = math.atan2(2*y*w - 2*x*z, 1 - 2*y*y - 2*z*z)\n    pitch = math.atan2(2*x*w - 2*y*z, 1 - 2*x*x - 2*z*z)\n    yaw = math.asin(2*x*y + 2*z*w)\n    return (roll, pitch, yaw)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rotation_of(source, dest):\n\n    source = Vector(source.x, source.y, source.z)\n    dest = Vector(dest.x, dest.y, dest.z)\n    cross = source.cross(dest)\n    cos_theta = source.dot(dest)\n\n    # Return identity if the vectors are the same direction.\n    if cos_theta >= 1.0:\n      return Quaternion.identity()\n\n    # Product of the square of the magnitudes.\n    k = math.sqrt(source.dot(source), dest.dot(dest))\n\n    # Return identity in the degenerate case.\n    if k <= 0.0:\n      return Quaternion.identity()\n\n    # Special handling for vectors facing opposite directions.\n    if cos_theta / k <= -1:\n      x_axis = Vector(1, 0, 0)\n      y_axis = Vector(0, 1, 1)\n      if abs(source.dot(x_ais)) < 1.0:\n        cross = source.cross(x_axis)\n      else:\n        cross = source.cross(y_axis)\n\n    return Quaternion(cross.x, cross.y, cross.z, k + cos_theta)", "response": "Returns a rotation matrix that represents a rotation from vector source to vector dest."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_axis_angle(axis, angle):\n\n    sincomp = math.sin(angle / 2.0)\n    return Quaternion(\n      axis.x * sincomp, axis.y * sincomp,\n      axis.z * sincomp, math.cos(angle / 2.0))", "response": "Returns a Quaternion that represents the right - handed\n    rotation of angle radians about the givne axis."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef newStruct(field_name_list):\n    field_list = []\n    append = field_list.append\n    for field in field_name_list:\n        type_prefix = ''\n        for char in field:\n            if not char.islower():\n                break\n            type_prefix += char\n        append((field, _desc_type_dict[type_prefix]))\n    result = type('some_descriptor', (LittleEndianStructure, ), {})\n    # Not using type()'s 3rd param to initialise class, as per ctypes\n    # documentation:\n    #   _pack_ must already be defined when _fields_ is assigned, otherwise it\n    #   will have no effect.\n    # pylint: disable=protected-access\n    result._pack_ = 1\n    result._fields_ = field_list\n    # pylint: enable=protected-access\n    return result", "response": "Create a ctype structure class based on USB standard field naming."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_extra(descriptor):\n    result = []\n    extra_length = descriptor.extra_length\n    if extra_length:\n        extra = buffer_at(descriptor.extra.value, extra_length)\n        append = result.append\n        while extra:\n            length = _string_item_to_int(extra[0])\n            if not 0 < length <= len(extra):\n                raise ValueError(\n                    'Extra descriptor %i is incomplete/invalid' % (\n                        len(result),\n                    ),\n                )\n            append(extra[:length])\n            extra = extra[length:]\n    return result", "response": "Returns a list of extra entries in a node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a binary buffer for the given size.", "response": "def create_binary_buffer(init_or_size):\n    \"\"\"\n    ctypes.create_string_buffer variant which does not add a trailing null\n    when init_or_size is not a size.\n    \"\"\"\n    # As per ctypes.create_string_buffer, as of python 2.7.10 at least:\n    # - int or long is a length\n    # - str or unicode is an initialiser\n    # Testing the latter confuses 2to3, so test the former.\n    if isinstance(init_or_size, (int, long)):\n        init_or_size = bytearray(init_or_size)\n    return create_initialised_buffer(init_or_size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns underlying libusb s version information as a 6 - namedtuple", "response": "def getVersion():\n    \"\"\"\n    Returns underlying libusb's version information as a 6-namedtuple (or\n    6-tuple if namedtuples are not avaiable):\n    - major\n    - minor\n    - micro\n    - nano\n    - rc\n    - describe\n    Returns (0, 0, 0, 0, '', '') if libusb doesn't have required entry point.\n    \"\"\"\n    version = libusb1.libusb_get_version().contents\n    return Version(\n        version.major,\n        version.minor,\n        version.micro,\n        version.nano,\n        version.rc,\n        version.describe,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the USB device and free all references to the USB device.", "response": "def close(self):\n        \"\"\"\n        Break reference cycles to allow instance to be garbage-collected.\n        Raises if called on a submitted transfer.\n        \"\"\"\n        if self.__submitted:\n            raise ValueError('Cannot close a submitted transfer')\n        self.doom()\n        self.__initialized = False\n        # Break possible external reference cycles\n        self.__callback = None\n        self.__user_data = None\n        # Break libusb_transfer reference cycles\n        self.__ctypesCallbackWrapper = None\n        # For some reason, overwriting callback is not enough to remove this\n        # reference cycle - though sometimes it works:\n        #   self -> self.__dict__ -> libusb_transfer -> dict[x] -> dict[x] ->\n        #   CThunkObject -> __callbackWrapper -> self\n        # So free transfer altogether.\n        if self.__transfer is not None:\n            self.__libusb_free_transfer(self.__transfer)\n            self.__transfer = None\n        self.__transfer_buffer = None\n        # Break USBDeviceHandle reference cycle\n        self.__before_submit = None\n        self.__after_completion = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __callbackWrapper(self, transfer_p):\n        self.__submitted = False\n        self.__after_completion(self)\n        callback = self.__callback\n        if callback is not None:\n            callback(self)\n        if self.__doomed:\n            self.close()", "response": "Wrapper for the callback that is called when a new entry is created."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setIsochronous(\n            self, endpoint, buffer_or_len, callback=None,\n            user_data=None, timeout=0, iso_transfer_length_list=None):\n        \"\"\"\n        Setup transfer for isochronous use.\n\n        endpoint\n            Endpoint to submit transfer to. Defines transfer direction (see\n            ENDPOINT_OUT and ENDPOINT_IN)).\n        buffer_or_len\n            Either a string (when sending data), or expected data length (when\n            receiving data)\n            To avoid memory copies, use an object implementing the writeable\n            buffer interface (ex: bytearray).\n        callback\n            Callback function to be invoked on transfer completion.\n            Called with transfer as parameter, return value ignored.\n        user_data\n            User data to pass to callback function.\n        timeout\n            Transfer timeout in milliseconds. 0 to disable.\n        iso_transfer_length_list\n            List of individual transfer sizes. If not provided, buffer_or_len\n            will be divided evenly among available transfers if possible, and\n            raise ValueError otherwise.\n        \"\"\"\n        if self.__submitted:\n            raise ValueError('Cannot alter a submitted transfer')\n        num_iso_packets = self.__num_iso_packets\n        if num_iso_packets == 0:\n            raise TypeError(\n                'This transfer canot be used for isochronous I/O. '\n                'You must get another one with a non-zero iso_packets '\n                'parameter.'\n            )\n        if self.__doomed:\n            raise DoomedTransferError('Cannot reuse a doomed transfer')\n        string_buffer, transfer_py_buffer = create_binary_buffer(buffer_or_len)\n        buffer_length = sizeof(string_buffer)\n        if iso_transfer_length_list is None:\n            iso_length, remainder = divmod(buffer_length, num_iso_packets)\n            if remainder:\n                raise ValueError(\n                    'Buffer size %i cannot be evenly distributed among %i '\n                    'transfers' % (\n                        buffer_length,\n                        num_iso_packets,\n                    )\n                )\n            iso_transfer_length_list = [iso_length] * num_iso_packets\n        configured_iso_packets = len(iso_transfer_length_list)\n        if configured_iso_packets > num_iso_packets:\n            raise ValueError(\n                'Too many ISO transfer lengths (%i), there are '\n                'only %i ISO transfers available' % (\n                    configured_iso_packets,\n                    num_iso_packets,\n                )\n            )\n        if sum(iso_transfer_length_list) > buffer_length:\n            raise ValueError(\n                'ISO transfers too long (%i), there are only '\n                '%i bytes available' % (\n                    sum(iso_transfer_length_list),\n                    buffer_length,\n                )\n            )\n        transfer_p = self.__transfer\n        self.__initialized = False\n        self.__transfer_buffer = string_buffer\n        self.__transfer_py_buffer = transfer_py_buffer\n        self.__user_data = user_data\n        libusb1.libusb_fill_iso_transfer(\n            transfer_p, self.__handle, endpoint, string_buffer, buffer_length,\n            configured_iso_packets, self.__ctypesCallbackWrapper, None,\n            timeout)\n        for length, iso_packet_desc in zip(\n                iso_transfer_length_list,\n                libusb1.get_iso_packet_list(transfer_p)):\n            if length <= 0:\n                raise ValueError(\n                    'Negative/null length transfers are not possible.'\n                )\n            iso_packet_desc.length = length\n        self.__callback = callback\n        self.__initialized = True", "response": "Sets the isochronous state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getISOBufferList(self):\n        transfer_p = self.__transfer\n        transfer = transfer_p.contents\n        # pylint: disable=undefined-variable\n        if transfer.type != TRANSFER_TYPE_ISOCHRONOUS:\n            # pylint: enable=undefined-variable\n            raise TypeError(\n                'This method cannot be called on non-iso transfers.'\n            )\n        return libusb1.get_iso_packet_buffer_list(transfer_p)", "response": "Get individual ISO transfer s buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getISOSetupList(self):\n        transfer_p = self.__transfer\n        transfer = transfer_p.contents\n        # pylint: disable=undefined-variable\n        if transfer.type != TRANSFER_TYPE_ISOCHRONOUS:\n            # pylint: enable=undefined-variable\n            raise TypeError(\n                'This method cannot be called on non-iso transfers.'\n            )\n        return [\n            {\n                'length': x.length,\n                'actual_length': x.actual_length,\n                'status': x.status,\n            }\n            for x in libusb1.get_iso_packet_list(transfer_p)\n        ]", "response": "Returns a list of dicts each containing an individual ISO transfer s setup."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iterISO(self):\n        transfer_p = self.__transfer\n        transfer = transfer_p.contents\n        # pylint: disable=undefined-variable\n        if transfer.type != TRANSFER_TYPE_ISOCHRONOUS:\n            # pylint: enable=undefined-variable\n            raise TypeError(\n                'This method cannot be called on non-iso transfers.'\n            )\n        buffer_position = transfer.buffer\n        for iso_transfer in libusb1.get_iso_packet_list(transfer_p):\n            yield (\n                iso_transfer.status,\n                libusb1.buffer_at(buffer_position, iso_transfer.actual_length),\n            )\n            buffer_position += iso_transfer.length", "response": "Generator yielding status and buffer for each isochornous transfer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the buffer for this object.", "response": "def setBuffer(self, buffer_or_len):\n        \"\"\"\n        Replace buffer with a new one.\n        Allows resizing read buffer and replacing data sent.\n        Note: resizing is not allowed for isochronous buffer (use\n        setIsochronous).\n        Note: disallowed on control transfers (use setControl).\n        \"\"\"\n        if self.__submitted:\n            raise ValueError('Cannot alter a submitted transfer')\n        transfer = self.__transfer.contents\n        # pylint: disable=undefined-variable\n        if transfer.type == TRANSFER_TYPE_CONTROL:\n            # pylint: enable=undefined-variable\n            raise ValueError(\n                'To alter control transfer buffer, use setControl'\n            )\n        buff, transfer_py_buffer = create_binary_buffer(buffer_or_len)\n        # pylint: disable=undefined-variable\n        if transfer.type == TRANSFER_TYPE_ISOCHRONOUS and \\\n                sizeof(buff) != transfer.length:\n            # pylint: enable=undefined-variable\n            raise ValueError(\n                'To alter isochronous transfer buffer length, use '\n                'setIsochronous'\n            )\n        self.__transfer_buffer = buff\n        self.__transfer_py_buffer = transfer_py_buffer\n        transfer.buffer = cast(buff, c_void_p)\n        transfer.length = sizeof(buff)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsubmitting a transfer for asynchronous handling.", "response": "def submit(self):\n        \"\"\"\n        Submit transfer for asynchronous handling.\n        \"\"\"\n        if self.__submitted:\n            raise ValueError('Cannot submit a submitted transfer')\n        if not self.__initialized:\n            raise ValueError(\n                'Cannot submit a transfer until it has been initialized'\n            )\n        if self.__doomed:\n            raise DoomedTransferError('Cannot submit doomed transfer')\n        self.__before_submit(self)\n        self.__submitted = True\n        result = libusb1.libusb_submit_transfer(self.__transfer)\n        if result:\n            self.__after_completion(self)\n            self.__submitted = False\n            raiseUSBError(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncancels the current transfer.", "response": "def cancel(self):\n        \"\"\"\n        Cancel transfer.\n        Note: cancellation happens asynchronously, so you must wait for\n        TRANSFER_CANCELLED.\n        \"\"\"\n        if not self.__submitted:\n            # XXX: Workaround for a bug reported on libusb 1.0.8: calling\n            # libusb_cancel_transfer on a non-submitted transfer might\n            # trigger a segfault.\n            raise self.__USBErrorNotFound\n        self.__mayRaiseUSBError(self.__libusb_cancel_transfer(self.__transfer))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setEventCallback(self, event, callback):\n        if event not in EVENT_CALLBACK_SET:\n            raise ValueError('Unknown event %r.' % (event, ))\n        self.__event_callback_dict[event] = callback", "response": "Sets a function to call for a given event."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npoll for events. timeout can be a float in seconds, or None for no timeout. Returns a list of (descriptor, event) pairs.", "response": "def poll(self, timeout=None):\n        \"\"\"\n        Poll for events.\n        timeout can be a float in seconds, or None for no timeout.\n        Returns a list of (descriptor, event) pairs.\n        \"\"\"\n        next_usb_timeout = self.__context.getNextTimeout()\n        if timeout is None or timeout < 0:\n            usb_timeout = next_usb_timeout\n        elif next_usb_timeout:\n            usb_timeout = min(next_usb_timeout, timeout)\n        else:\n            usb_timeout = timeout\n        event_list = self.__poller.poll(usb_timeout)\n        if event_list:\n            fd_set = self.__fd_set\n            result = [(x, y) for x, y in event_list if x not in fd_set]\n            if len(result) != len(event_list):\n                self.__context.handleEventsTimeout()\n        else:\n            result = event_list\n            self.__context.handleEventsTimeout()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister an USB - unrelated fd to poller.", "response": "def register(self, fd, events):\n        \"\"\"\n        Register an USB-unrelated fd to poller.\n        Convenience method.\n        \"\"\"\n        if fd in self.__fd_set:\n            raise ValueError(\n                'This fd is a special USB event fd, it cannot be polled.'\n            )\n        self.__poller.register(fd, events)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing this handle. If not called explicitely, will be called by destructor. This method cancels any in-flight transfer when it is called. As cancellation is not immediate, this method needs to let libusb handle events until transfers are actually cancelled. In multi-threaded programs, this can lead to stalls. To avoid this, do not close nor let GC collect a USBDeviceHandle which has in-flight transfers.", "response": "def close(self):\n        \"\"\"\n        Close this handle. If not called explicitely, will be called by\n        destructor.\n\n        This method cancels any in-flight transfer when it is called. As\n        cancellation is not immediate, this method needs to let libusb handle\n        events until transfers are actually cancelled.\n        In multi-threaded programs, this can lead to stalls. To avoid this,\n        do not close nor let GC collect a USBDeviceHandle which has in-flight\n        transfers.\n        \"\"\"\n        handle = self.__handle\n        if handle is None:\n            return\n        # Build a strong set from weak self.__transfer_set so we can doom\n        # and close all contained transfers.\n        # Because of backward compatibility, self.__transfer_set might be a\n        # wrapper around WeakKeyDictionary. As it might be modified by gc,\n        # we must pop until there is not key left instead of iterating over\n        # it.\n        weak_transfer_set = self.__transfer_set\n        transfer_set = self.__set()\n        while True:\n            try:\n                transfer = weak_transfer_set.pop()\n            except self.__KeyError:\n                break\n            transfer_set.add(transfer)\n            transfer.doom()\n        inflight = self.__inflight\n        for transfer in inflight:\n            try:\n                transfer.cancel()\n            except (self.__USBErrorNotFound, self.__USBErrorNoDevice):\n                pass\n        while inflight:\n            try:\n                self.__context.handleEvents()\n            except self.__USBErrorInterrupted:\n                pass\n        for transfer in transfer_set:\n            transfer.close()\n        self.__libusb_close(handle)\n        self.__handle = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getConfiguration(self):\n        configuration = c_int()\n        mayRaiseUSBError(libusb1.libusb_get_configuration(\n            self.__handle, byref(configuration),\n        ))\n        return configuration.value", "response": "Get the current configuration number for this device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nclaiming the given interface number.", "response": "def claimInterface(self, interface):\n        \"\"\"\n        Claim (= get exclusive access to) given interface number. Required to\n        receive/send data.\n\n        Can be used as a context manager:\n            with handle.claimInterface(0):\n                # do stuff\n            # handle.releaseInterface(0) gets automatically called\n        \"\"\"\n        mayRaiseUSBError(\n            libusb1.libusb_claim_interface(self.__handle, interface),\n        )\n        return _ReleaseInterface(self, interface)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setInterfaceAltSetting(self, interface, alt_setting):\n        mayRaiseUSBError(libusb1.libusb_set_interface_alt_setting(\n            self.__handle, interface, alt_setting,\n        ))", "response": "Set the alternative setting of an interface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kernelDriverActive(self, interface):\n        result = libusb1.libusb_kernel_driver_active(self.__handle, interface)\n        if result == 0:\n            return False\n        elif result == 1:\n            return True\n        raiseUSBError(result)", "response": "Tell whether a kernel driver is active on given interface number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nenabling or disable automatic kernel driver detach.", "response": "def setAutoDetachKernelDriver(self, enable):\n        \"\"\"\n        Control automatic kernel driver detach.\n        enable (bool)\n            True to enable auto-detach, False to disable it.\n        \"\"\"\n        mayRaiseUSBError(libusb1.libusb_set_auto_detach_kernel_driver(\n            self.__handle, bool(enable),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of USB language identifiers supported by the current device.", "response": "def getSupportedLanguageList(self):\n        \"\"\"\n        Return a list of USB language identifiers (as integers) supported by\n        current device for its string descriptors.\n\n        Note: language identifiers seem (I didn't check them all...) very\n        similar to windows language identifiers, so you may want to use\n        locales.windows_locale to get an rfc3066 representation. The 5 standard\n        HID language codes are missing though.\n        \"\"\"\n        descriptor_string, _ = create_binary_buffer(STRING_LENGTH)\n        result = libusb1.libusb_get_string_descriptor(\n            self.__handle, 0, 0, descriptor_string, sizeof(descriptor_string),\n        )\n        # pylint: disable=undefined-variable\n        if result == ERROR_PIPE:\n            # pylint: enable=undefined-variable\n            # From libusb_control_transfer doc:\n            # control request not supported by the device\n            return []\n        mayRaiseUSBError(result)\n        langid_list = cast(descriptor_string, POINTER(c_uint16))\n        return [\n            libusb1.libusb_le16_to_cpu(langid_list[offset])\n            for offset in xrange(1, cast(descriptor_string, POINTER(c_ubyte))[0] // 2)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getStringDescriptor(self, descriptor, lang_id, errors='strict'):\n        if descriptor == 0:\n            return None\n        descriptor_string = bytearray(STRING_LENGTH)\n        try:\n            received = mayRaiseUSBError(libusb1.libusb_get_string_descriptor(\n                self.__handle, descriptor, lang_id,\n                create_binary_buffer(descriptor_string)[0],\n                STRING_LENGTH,\n            ))\n        # pylint: disable=undefined-variable\n        except USBErrorNotFound:\n            # pylint: enable=undefined-variable\n            return None\n        if received < 2 or descriptor_string[1] != DT_STRING:\n            raise ValueError('Invalid string descriptor')\n        return descriptor_string[2:min(\n            received,\n            descriptor_string[0],\n        )].decode('UTF-16-LE', errors=errors)", "response": "Fetch description string for given descriptor and in given language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getASCIIStringDescriptor(self, descriptor, errors='strict'):\n        if descriptor == 0:\n            return None\n        descriptor_string = bytearray(STRING_LENGTH)\n        try:\n            received = mayRaiseUSBError(libusb1.libusb_get_string_descriptor_ascii(\n                self.__handle, descriptor,\n                create_binary_buffer(descriptor_string)[0],\n                STRING_LENGTH,\n            ))\n        # pylint: disable=undefined-variable\n        except USBErrorNotFound:\n            # pylint: enable=undefined-variable\n            return None\n        return descriptor_string[:received].decode('ASCII', errors=errors)", "response": "Fetch the description string for given descriptor in first available ASCII language."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef controlRead(\n            self, request_type, request, value, index, length, timeout=0):\n        \"\"\"\n        Synchronous control read.\n        timeout: in milliseconds, how long to wait for data. Set to 0 to\n          disable.\n        See controlWrite for other parameters description.\n\n        To avoid memory copies, use an object implementing the writeable buffer\n        interface (ex: bytearray) for the \"data\" parameter.\n\n        Returns received data.\n        \"\"\"\n        # pylint: disable=undefined-variable\n        request_type = (request_type & ~ENDPOINT_DIR_MASK) | ENDPOINT_IN\n        # pylint: enable=undefined-variable\n        data, data_buffer = create_binary_buffer(length)\n        transferred = self._controlTransfer(\n            request_type, request, value, index, data, length, timeout,\n        )\n        return data_buffer[:transferred]", "response": "Synchronous control read.\n        timeout: in milliseconds, how long to wait for data. Set to 0 to\n          disable.\n        See controlWrite for other parameters description.\n\n        To avoid memory copies, use an object implementing the writeable buffer\n        interface (ex: bytearray) for the \"data\" parameter.\n\n        Returns received data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bulkWrite(self, endpoint, data, timeout=0):\n        # pylint: disable=undefined-variable\n        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_OUT\n        # pylint: enable=undefined-variable\n        data, _ = create_initialised_buffer(data)\n        return self._bulkTransfer(endpoint, data, sizeof(data), timeout)", "response": "Synchronous bulk write.\n        endpoint: endpoint to send data to.\n        data: data to send.\n        timeout: in milliseconds, how long to wait for device acknowledgement.\n          Set to 0 to disable.\n\n        To avoid memory copies, use an object implementing the writeable buffer\n        interface (ex: bytearray) for the \"data\" parameter.\n\n        Returns the number of bytes actually sent.\n\n        May raise an exception from the USBError family. USBErrorTimeout\n        exception has a \"transferred\" property giving the number of bytes sent\n        up to the timeout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bulkRead(self, endpoint, length, timeout=0):\n        # pylint: disable=undefined-variable\n        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_IN\n        # pylint: enable=undefined-variable\n        data, data_buffer = create_binary_buffer(length)\n        try:\n            transferred = self._bulkTransfer(endpoint, data, length, timeout)\n        except USBErrorTimeout as exception:\n            exception.received = data_buffer[:exception.transferred]\n            raise\n        return data_buffer[:transferred]", "response": "Bulk read from the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef interruptWrite(self, endpoint, data, timeout=0):\n        # pylint: disable=undefined-variable\n        endpoint = (endpoint & ~ENDPOINT_DIR_MASK) | ENDPOINT_OUT\n        # pylint: enable=undefined-variable\n        data, _ = create_initialised_buffer(data)\n        return self._interruptTransfer(endpoint, data, sizeof(data), timeout)", "response": "Synchronous interrupt write.\n        endpoint: endpoint to send data to.\n        data: data to send.\n        timeout: in milliseconds, how long to wait for device acknowledgement.\n          Set to 0 to disable.\n\n        To avoid memory copies, use an object implementing the writeable buffer\n        interface (ex: bytearray) for the \"data\" parameter.\n\n        Returns the number of bytes actually sent.\n\n        May raise an exception from the USBError family. USBErrorTimeout\n        exception has a \"transferred\" property giving the number of bytes sent\n        up to the timeout."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget an USBTransfer instance for asynchronous use.", "response": "def getTransfer(self, iso_packets=0):\n        \"\"\"\n        Get an USBTransfer instance for asynchronous use.\n        iso_packets: the number of isochronous transfer descriptors to\n          allocate.\n        \"\"\"\n        result = USBTransfer(\n            self.__handle, iso_packets,\n            self.__inflight_add, self.__inflight_remove,\n        )\n        self.__transfer_set.add(result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the port number list of each hub toward device.", "response": "def getPortNumberList(self):\n        \"\"\"\n        Get the port number of each hub toward device.\n        \"\"\"\n        port_list = (c_uint8 * PATH_MAX_DEPTH)()\n        result = libusb1.libusb_get_port_numbers(\n            self.device_p, port_list, len(port_list))\n        mayRaiseUSBError(result)\n        return list(port_list[:result])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getMaxPacketSize(self, endpoint):\n        result = libusb1.libusb_get_max_packet_size(self.device_p, endpoint)\n        mayRaiseUSBError(result)\n        return result", "response": "Get the maximum packet size for given endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the maximum size for a single isochronous packet for given endpoint.", "response": "def getMaxISOPacketSize(self, endpoint):\n        \"\"\"\n        Get the maximum size for a single isochronous packet for given\n        endpoint.\n\n        Warning: this function will not always give you the expected result.\n        See https://libusb.org/ticket/77 . You should instead consult the\n        endpoint descriptor of current configuration and alternate setting.\n        \"\"\"\n        result = libusb1.libusb_get_max_iso_packet_size(self.device_p, endpoint)\n        mayRaiseUSBError(result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self):\n        handle = libusb1.libusb_device_handle_p()\n        mayRaiseUSBError(libusb1.libusb_open(self.device_p, byref(handle)))\n        result = USBDeviceHandle(self.__context, handle, self)\n        self.__close_set.add(result)\n        return result", "response": "Open device. Returns an USBDeviceHandle instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open(self):\n        assert self.__context_refcount == 0\n        mayRaiseUSBError(libusb1.libusb_init(byref(self.__context_p)))\n        return self", "response": "Open a new USB context."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclose this USB context and all related instances.", "response": "def close(self):\n        \"\"\"\n        Close (destroy) this USB context, and all related instances.\n\n        When this method has been called, methods on its instance will\n        become mosty no-ops, returning None until explicitly re-opened\n        (by calling open() or __enter__()).\n\n        Note: \"exit\" is a deprecated alias of \"close\".\n        \"\"\"\n        self.__auto_open = False\n        self.__context_cond.acquire()\n        try:\n            while self.__context_refcount and self.__context_p:\n                self.__context_cond.wait()\n            self._exit()\n        finally:\n            self.__context_cond.notifyAll()\n            self.__context_cond.release()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an iterator over all USB devices currently plugged in.", "response": "def getDeviceIterator(self, skip_on_error=False):\n        \"\"\"\n        Return an iterator over all USB devices currently plugged in, as USBDevice\n        instances.\n\n        skip_on_error (bool)\n            If True, ignore devices which raise USBError.\n        \"\"\"\n        device_p_p = libusb1.libusb_device_p_p()\n        libusb_device_p = libusb1.libusb_device_p\n        device_list_len = libusb1.libusb_get_device_list(self.__context_p,\n                                                         byref(device_p_p))\n        mayRaiseUSBError(device_list_len)\n        try:\n            for device_p in device_p_p[:device_list_len]:\n                try:\n                    # Instanciate our own libusb_device_p object so we can free\n                    # libusb-provided device list. Is this a bug in ctypes that\n                    # it doesn't copy pointer value (=pointed memory address) ?\n                    # At least, it's not so convenient and forces using such\n                    # weird code.\n                    device = USBDevice(self, libusb_device_p(device_p.contents))\n                except USBError:\n                    if not skip_on_error:\n                        raise\n                else:\n                    self.__close_set.add(device)\n                    yield device\n        finally:\n            libusb1.libusb_free_device_list(device_p_p, 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getDeviceList(self, skip_on_access_error=False, skip_on_error=False):\n        return list(\n            self.getDeviceIterator(\n                skip_on_error=skip_on_access_error or skip_on_error,\n            ),\n        )", "response": "Return a list of all USB devices currently plugged in."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getByVendorIDAndProductID(\n            self, vendor_id, product_id,\n            skip_on_access_error=False, skip_on_error=False):\n        \"\"\"\n        Get the first USB device matching given vendor and product ids.\n        Returns an USBDevice instance, or None if no present device match.\n        skip_on_error (bool)\n            (see getDeviceList)\n        skip_on_access_error (bool)\n            (see getDeviceList)\n        \"\"\"\n        for device in self.getDeviceIterator(\n                skip_on_error=skip_on_access_error or skip_on_error,\n            ):\n            if device.getVendorID() == vendor_id and \\\n                    device.getProductID() == product_id:\n                return device", "response": "Get the first USB device matching given vendor and product ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef openByVendorIDAndProductID(\n            self, vendor_id, product_id,\n            skip_on_access_error=False, skip_on_error=False):\n        \"\"\"\n        Get the first USB device matching given vendor and product ids.\n        Returns an USBDeviceHandle instance, or None if no present device\n        match.\n        skip_on_error (bool)\n            (see getDeviceList)\n        skip_on_access_error (bool)\n            (see getDeviceList)\n        \"\"\"\n        result = self.getByVendorIDAndProductID(\n            vendor_id, product_id,\n            skip_on_access_error=skip_on_access_error,\n            skip_on_error=skip_on_error)\n        if result is not None:\n            return result.open()", "response": "Open a USB device handle by vendor and product ids."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of file descriptors that are used to poll USB events.", "response": "def getPollFDList(self):\n        \"\"\"\n        Return file descriptors to be used to poll USB events.\n        You should not have to call this method, unless you are integrating\n        this class with a polling mechanism.\n        \"\"\"\n        pollfd_p_p = libusb1.libusb_get_pollfds(self.__context_p)\n        if not pollfd_p_p:\n            errno = get_errno()\n            if errno:\n                raise OSError(errno)\n            else:\n                # Assume not implemented\n                raise NotImplementedError(\n                    'Your libusb does not seem to implement pollable FDs')\n        try:\n            result = []\n            append = result.append\n            fd_index = 0\n            while pollfd_p_p[fd_index]:\n                append((\n                    pollfd_p_p[fd_index].contents.fd,\n                    pollfd_p_p[fd_index].contents.events,\n                ))\n                fd_index += 1\n        finally:\n            _free(pollfd_p_p)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle any pending events.", "response": "def handleEventsTimeout(self, tv=0):\n        \"\"\"\n        Handle any pending event.\n        If tv is 0, will return immediately after handling already-pending\n        events.\n        Otherwise, defines the maximum amount of time to wait for events, in\n        seconds.\n        \"\"\"\n        if tv is None:\n            tv = 0\n        tv_s = int(tv)\n        real_tv = libusb1.timeval(tv_s, int((tv - tv_s) * 1000000))\n        mayRaiseUSBError(\n            libusb1.libusb_handle_events_timeout(\n                self.__context_p, byref(real_tv),\n            ),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setPollFDNotifiers(\n            self, added_cb=None, removed_cb=None, user_data=None):\n        \"\"\"\n        Give libusb1 methods to call when it should add/remove file descriptor\n        for polling.\n        You should not have to call this method, unless you are integrating\n        this class with a polling mechanism.\n        \"\"\"\n        if added_cb is None:\n            added_cb = self.__null_pointer\n        else:\n            added_cb = libusb1.libusb_pollfd_added_cb_p(added_cb)\n        if removed_cb is None:\n            removed_cb = self.__null_pointer\n        else:\n            removed_cb = libusb1.libusb_pollfd_removed_cb_p(removed_cb)\n        if user_data is None:\n            user_data = self.__null_pointer\n        self.__added_cb = added_cb\n        self.__removed_cb = removed_cb\n        self.__poll_cb_user_data = user_data\n        self.__libusb_set_pollfd_notifiers(\n            self.__context_p,\n            self.__cast(added_cb, libusb1.libusb_pollfd_added_cb_p),\n            self.__cast(removed_cb, libusb1.libusb_pollfd_removed_cb_p),\n            user_data,\n        )", "response": "Sets the pollfd notifiers for the specified file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getNextTimeout(self):\n        timeval = libusb1.timeval()\n        result = libusb1.libusb_get_next_timeout(\n            self.__context_p, byref(timeval))\n        if result == 0:\n            return None\n        elif result == 1:\n            return timeval.tv_sec + (timeval.tv_usec * 0.000001)\n        raiseUSBError(result)", "response": "Returns the next internal timeout that libusb needs to handle in\n        seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for an event from the USB context.", "response": "def waitForEvent(self, tv=0):\n        \"\"\"\n        See libusb_wait_for_event doc.\n        \"\"\"\n        if tv is None:\n            tv = 0\n        tv_s = int(tv)\n        real_tv = libusb1.timeval(tv_s, int((tv - tv_s) * 1000000))\n        libusb1.libusb_wait_for_event(self.__context_p, byref(real_tv))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a callback function for a specific hotplug event.", "response": "def hotplugRegisterCallback(\n            self, callback,\n            # pylint: disable=undefined-variable\n            events=HOTPLUG_EVENT_DEVICE_ARRIVED | HOTPLUG_EVENT_DEVICE_LEFT,\n            flags=HOTPLUG_ENUMERATE,\n            vendor_id=HOTPLUG_MATCH_ANY,\n            product_id=HOTPLUG_MATCH_ANY,\n            dev_class=HOTPLUG_MATCH_ANY,\n            # pylint: enable=undefined-variable\n        ):\n        \"\"\"\n        Registers an hotplug callback.\n        On success, returns an opaque value which can be passed to\n        hotplugDeregisterCallback.\n        Callback must accept the following positional arguments:\n        - this USBContext instance\n        - an USBDevice instance\n          If device has left, configuration descriptors may not be\n          available. Its device descriptor will be available.\n        - event type, one of:\n            HOTPLUG_EVENT_DEVICE_ARRIVED\n            HOTPLUG_EVENT_DEVICE_LEFT\n        Callback must return whether it must be unregistered (any true value\n        to be unregistered, any false value to be kept registered).\n\n        Note: given callback will be invoked during event handling, meaning\n        it cannot call any synchronous libusb function.\n        \"\"\"\n        def wrapped_callback(context_p, device_p, event, _):\n            assert addressof(context_p.contents) == addressof(\n                self.__context_p.contents), (context_p, self.__context_p)\n            device = USBDevice(\n                self,\n                device_p,\n                # pylint: disable=undefined-variable\n                event != HOTPLUG_EVENT_DEVICE_LEFT,\n                # pylint: enable=undefined-variable\n            )\n            self.__close_set.add(device)\n            unregister = bool(callback(\n                self,\n                device,\n                event,\n            ))\n            if unregister:\n                del self.__hotplug_callback_dict[handle]\n            return unregister\n        handle = c_int()\n        callback_p = libusb1.libusb_hotplug_callback_fn_p(wrapped_callback)\n        mayRaiseUSBError(libusb1.libusb_hotplug_register_callback(\n            self.__context_p, events, flags, vendor_id, product_id, dev_class,\n            callback_p, None, byref(handle),\n        ))\n        handle = handle.value\n        # Keep strong references\n        assert handle not in self.__hotplug_callback_dict, (\n            handle,\n            self.__hotplug_callback_dict,\n        )\n        self.__hotplug_callback_dict[handle] = (callback_p, wrapped_callback)\n        return handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hotplugDeregisterCallback(self, handle):\n        del self.__hotplug_callback_dict[handle]\n        libusb1.libusb_hotplug_deregister_callback(self.__context_p, handle)", "response": "Deregisters an hotplug callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare an i2c read transaction.", "response": "def read(address, length):\n        \"\"\"\n        Prepares an i2c read transaction.\n\n        :param address: Slave address.\n        :type: address: int\n        :param length: Number of bytes to read.\n        :type: length: int\n        :return: New :py:class:`i2c_msg` instance for read operation.\n        :rtype: :py:class:`i2c_msg`\n        \"\"\"\n        arr = create_string_buffer(length)\n        return i2c_msg(\n            addr=address, flags=I2C_M_RD, len=length,\n            buf=arr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprepare an i2c write transaction.", "response": "def write(address, buf):\n        \"\"\"\n        Prepares an i2c write transaction.\n\n        :param address: Slave address.\n        :type address: int\n        :param buf: Bytes to write. Either list of values or str.\n        :type buf: list\n        :return: New :py:class:`i2c_msg` instance for write operation.\n        :rtype: :py:class:`i2c_msg`\n        \"\"\"\n        if sys.version_info.major >= 3:\n            if type(buf) is str:\n                buf = bytes(map(ord, buf))\n            else:\n                buf = bytes(buf)\n        else:\n            if type(buf) is not str:\n                buf = ''.join([chr(x) for x in buf])\n        arr = create_string_buffer(buf, len(buf))\n        return i2c_msg(\n            addr=address, flags=0, len=len(arr),\n            buf=arr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a i2c_rdwr_ioctl_data struct that can be called with ioctl ( fd I2C_RDWR data ).", "response": "def create(*i2c_msg_instances):\n        \"\"\"\n        Factory method for creating a i2c_rdwr_ioctl_data struct that can\n        be called with ``ioctl(fd, I2C_RDWR, data)``.\n\n        :param i2c_msg_instances: Up to 42 i2c_msg instances\n        :rtype: i2c_rdwr_ioctl_data\n        \"\"\"\n        n_msg = len(i2c_msg_instances)\n        msg_array = (i2c_msg * n_msg)(*i2c_msg_instances)\n        return i2c_rdwr_ioctl_data(\n            msgs=msg_array,\n            nmsgs=n_msg\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a given i2c bus and set the functions attribute.", "response": "def open(self, bus):\n        \"\"\"\n        Open a given i2c bus.\n\n        :param bus: i2c bus number (e.g. 0 or 1)\n        :type bus: int\n        \"\"\"\n        self.fd = os.open(\"/dev/i2c-{}\".format(bus), os.O_RDWR)\n        self.funcs = self._get_funcs()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        if self.fd:\n            os.close(self.fd)\n            self.fd = None", "response": "Close the i2c connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets i2c slave address to use for subsequent calls.", "response": "def _set_address(self, address, force=None):\n        \"\"\"\n        Set i2c slave address to use for subsequent calls.\n\n        :param address:\n        :type address: int\n        :param force:\n        :type force: Boolean\n        \"\"\"\n        force = force if force is not None else self.force\n        if self.address != address or self._force_last != force:\n            if force is True:\n                ioctl(self.fd, I2C_SLAVE_FORCE, address)\n            else:\n                ioctl(self.fd, I2C_SLAVE, address)\n            self.address = address\n            self._force_last = force"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_funcs(self):\n        f = c_uint32()\n        ioctl(self.fd, I2C_FUNCS, f)\n        return f.value", "response": "Returns a 32 - bit value stating supported I2C functions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform a quick write on the cache.", "response": "def write_quick(self, i2c_addr, force=None):\n        \"\"\"\n        Perform quick transaction. Throws IOError if unsuccessful.\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param force:\n        :type force: Boolean\n        \"\"\"\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=0, size=I2C_SMBUS_QUICK)\n        ioctl(self.fd, I2C_SMBUS, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_byte(self, i2c_addr, force=None):\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_READ, command=0, size=I2C_SMBUS_BYTE\n        )\n        ioctl(self.fd, I2C_SMBUS, msg)\n        return msg.data.contents.byte", "response": "Read a single byte from a device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_byte(self, i2c_addr, value, force=None):\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=value, size=I2C_SMBUS_BYTE\n        )\n        ioctl(self.fd, I2C_SMBUS, msg)", "response": "Write a single byte to a device."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_byte_data(self, i2c_addr, register, force=None):\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_READ, command=register, size=I2C_SMBUS_BYTE_DATA\n        )\n        ioctl(self.fd, I2C_SMBUS, msg)\n        return msg.data.contents.byte", "response": "Read a single byte from a designated register."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting a byte to a given register.", "response": "def write_byte_data(self, i2c_addr, register, value, force=None):\n        \"\"\"\n        Write a byte to a given register.\n\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param register: Register to write to\n        :type register: int\n        :param value: Byte value to transmit\n        :type value: int\n        :param force:\n        :type force: Boolean\n        :rtype: None\n        \"\"\"\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=register, size=I2C_SMBUS_BYTE_DATA\n        )\n        msg.data.contents.byte = value\n        ioctl(self.fd, I2C_SMBUS, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single word from a given register.", "response": "def read_word_data(self, i2c_addr, register, force=None):\n        \"\"\"\n        Read a single word (2 bytes) from a given register.\n\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param register: Register to read\n        :type register: int\n        :param force:\n        :type force: Boolean\n        :return: 2-byte word\n        :rtype: int\n        \"\"\"\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_READ, command=register, size=I2C_SMBUS_WORD_DATA\n        )\n        ioctl(self.fd, I2C_SMBUS, msg)\n        return msg.data.contents.word"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_call(self, i2c_addr, register, value, force=None):\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=register, size=I2C_SMBUS_PROC_CALL\n        )\n        msg.data.contents.word = value\n        ioctl(self.fd, I2C_SMBUS, msg)\n        return msg.data.contents.word", "response": "Executes a SMBus Process Call and returns the 16 - bit value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a block of up to 32 - bytes from a given register.", "response": "def read_block_data(self, i2c_addr, register, force=None):\n        \"\"\"\n        Read a block of up to 32-bytes from a given register.\n\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param register: Start register\n        :type register: int\n        :param force:\n        :type force: Boolean\n        :return: List of bytes\n        :rtype: list\n        \"\"\"\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_READ, command=register, size=I2C_SMBUS_BLOCK_DATA\n        )\n        ioctl(self.fd, I2C_SMBUS, msg)\n        length = msg.data.contents.block[0]\n        return msg.data.contents.block[1:length + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a block of byte data to a given register.", "response": "def write_block_data(self, i2c_addr, register, data, force=None):\n        \"\"\"\n        Write a block of byte data to a given register.\n\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param register: Start register\n        :type register: int\n        :param data: List of bytes\n        :type data: list\n        :param force:\n        :type force: Boolean\n        :rtype: None\n        \"\"\"\n        length = len(data)\n        if length > I2C_SMBUS_BLOCK_MAX:\n            raise ValueError(\"Data length cannot exceed %d bytes\" % I2C_SMBUS_BLOCK_MAX)\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=register, size=I2C_SMBUS_BLOCK_DATA\n        )\n        msg.data.contents.block[0] = length\n        msg.data.contents.block[1:length + 1] = data\n        ioctl(self.fd, I2C_SMBUS, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a block of byte data from a given register.", "response": "def read_i2c_block_data(self, i2c_addr, register, length, force=None):\n        \"\"\"\n        Read a block of byte data from a given register.\n\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param register: Start register\n        :type register: int\n        :param length: Desired block length\n        :type length: int\n        :param force:\n        :type force: Boolean\n        :return: List of bytes\n        :rtype: list\n        \"\"\"\n        if length > I2C_SMBUS_BLOCK_MAX:\n            raise ValueError(\"Desired block length over %d bytes\" % I2C_SMBUS_BLOCK_MAX)\n        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_READ, command=register, size=I2C_SMBUS_I2C_BLOCK_DATA\n        )\n        msg.data.contents.byte = length\n        ioctl(self.fd, I2C_SMBUS, msg)\n        return msg.data.contents.block[1:length + 1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef i2c_rdwr(self, *i2c_msgs):\n        ioctl_data = i2c_rdwr_ioctl_data.create(*i2c_msgs)\n        ioctl(self.fd, I2C_RDWR, ioctl_data)", "response": "This method is used to combine a series of i2c read and write operations in a single i2c transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef max_dimension(cellmap, sheet = None):\n\n    cells = list(cellmap.values())\n    rows = 0\n    cols = 0\n    for cell in cells:\n        if sheet is None or cell.sheet == sheet:\n            rows = max(rows, int(cell.row))\n            cols = max(cols, int(col2num(cell.col)))\n\n    return (rows, cols)", "response": "This function calculates the maximum dimension of the worksheet or optionally the worksheet."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to find the number of rows in an array.", "response": "def rows(array):\n    \"\"\"\n    Function to find the number of rows in an array.\n    Excel reference: https://support.office.com/en-ie/article/rows-function-b592593e-3fc2-47f2-bec1-bda493811597\n\n    :param array: the array of which the rows should be counted.\n    :return: the number of rows.\n    \"\"\"\n\n    if isinstance(array, (float, int)):\n        rows = 1  # special case for A1:A1 type ranges which for some reason only return an int/float\n    elif array is None:\n        rows = 1  # some A1:A1 ranges return None (issue with ref cell)\n    else:\n        rows = len(array.values)\n\n    return rows"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef irr(values, guess = None):\n    if isinstance(values, Range):\n        values = values.values\n\n    if guess is not None and guess != 0:\n        raise ValueError('guess value for excellib.irr() is %s and not 0' % guess)\n    else:\n        try:\n            return np.irr(values)\n        except Exception as e:\n            return ExcelError('#NUM!', e)", "response": "Function to calculate the internal rate of return of a single item in a single item."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to calculate the internal rate of return using payments and non - periodic dates.", "response": "def xirr(values, dates, guess=0):\n    \"\"\"\n    Function to calculate the internal rate of return (IRR) using payments and non-periodic dates. It resembles the\n    excel function XIRR().\n\n    Excel reference: https://support.office.com/en-ie/article/xirr-function-de1242ec-6477-445b-b11b-a303ad9adc9d\n\n    :param values: the payments of which at least one has to be negative.\n    :param dates: the dates as excel dates (e.g. 43571 for 16/04/2019).\n    :param guess: an initial guess which is required by Excel but isn't used by this function.\n    :return: a float being the IRR.\n    \"\"\"\n\n    if isinstance(values, Range):\n        values = values.values\n\n    if isinstance(dates, Range):\n        dates = dates.values\n\n    if guess is not None and guess != 0:\n        raise ValueError('guess value for excellib.irr() is %s and not 0' % guess)\n    else:\n        try:\n            return scipy.optimize.newton(lambda r: xnpv(r, values, dates, lim_rate=False), 0.0)\n        except RuntimeError:  # Failed to converge?\n            return scipy.optimize.brentq(lambda r: xnpv(r, values, dates, lim_rate=False), -1.0, 1e10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xnpv(rate, values, dates, lim_rate = True):  # Excel reference: https://support.office.com/en-us/article/XNPV-function-1b42bbf6-370f-4532-a0eb-d67c16b664b7\n    if isinstance(values, Range):\n        values = values.values\n\n    if isinstance(dates, Range):\n        dates = dates.values\n\n    if len(values) != len(dates):\n        return ExcelError('#NUM!', '`values` range must be the same length as `dates` range in XNPV, %s != %s' % (len(values), len(dates)))\n\n    if lim_rate and rate < 0:\n        return ExcelError('#NUM!', '`excel cannot handle a negative `rate`' % (len(values), len(dates)))\n\n    xnpv = 0\n    for v, d in zip(values, dates):\n        xnpv += v / np.power(1.0 + rate, (d - dates[0]) / 365)\n\n    return xnpv", "response": "Function to calculate the net present value using the discount rate and the amount of payment in a given date range."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the date of the current date in Excel.", "response": "def today():\n    reference_date = datetime.datetime.today().date()\n    days_since_epoch = reference_date - EXCEL_EPOCH\n    # why +2 ?\n    # 1 based from 1900-01-01\n    # I think it is \"inclusive\" / to the _end_ of the day.\n    # https://support.office.com/en-us/article/date-function-e36c0c8c-4104-49da-ab83-82328b832349\n    \"\"\"Note: Excel stores dates as sequential serial numbers so that they can be used in calculations.\n    January 1, 1900 is serial number 1, and January 1, 2008 is serial number 39448 because it is 39,447 days after January 1, 1900.\n     You will need to change the number format (Format Cells) in order to display a proper date.\"\"\"\n    return days_since_epoch.days + 2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrepair the central directory of a ZIP file", "response": "def repair_central_directory(zipFile, is_file_instance): # source: https://bitbucket.org/openpyxl/openpyxl/src/93604327bce7aac5e8270674579af76d390e09c0/openpyxl/reader/excel.py?at=default&fileviewer=file-view-default\n    ''' trims trailing data from the central directory\n    code taken from http://stackoverflow.com/a/7457686/570216, courtesy of Uri Cohen\n    '''\n    f = zipFile if is_file_instance else open(zipFile, 'rb+')\n    data = f.read()\n    pos = data.find(CENTRAL_DIRECTORY_SIGNATURE)  # End of central directory signature\n    if (pos > 0):\n        sio = BytesIO(data)\n        sio.seek(pos + 22)  # size of 'ZIP end of central directory record'\n        sio.truncate()\n        sio.seek(0)\n        return sio\n\n    f.seek(0)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert numbers as string to an int or float", "response": "def _cast_number(value): # source: https://bitbucket.org/openpyxl/openpyxl/src/93604327bce7aac5e8270674579af76d390e09c0/openpyxl/cell/read_only.py?at=default&fileviewer=file-view-default\n    \"Convert numbers as string to an int or float\"\n    m = FLOAT_REGEX.search(value)\n    if m is not None:\n        return float(value)\n    return int(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread relationships for a workbook", "response": "def read_rels(archive):\n    \"\"\"Read relationships for a workbook\"\"\"\n    xml_source = archive.read(ARC_WORKBOOK_RELS)\n    tree = fromstring(xml_source)\n    for element in safe_iterator(tree, '{%s}Relationship' % PKG_REL_NS):\n        rId = element.get('Id')\n        pth = element.get(\"Target\")\n        typ = element.get('Type')\n        # normalise path\n        if pth.startswith(\"/xl\"):\n            pth = pth.replace(\"/xl\", \"xl\")\n        elif not pth.startswith(\"xl\") and not pth.startswith(\"..\"):\n            pth = \"xl/\" + pth\n        yield rId, {'path':pth, 'type':typ}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_content_types(archive):\n    xml_source = archive.read(ARC_CONTENT_TYPES)\n    root = fromstring(xml_source)\n    contents_root = root.findall('{%s}Override' % CONTYPES_NS)\n    for type in contents_root:\n        yield type.get('ContentType'), type.get('PartName')", "response": "Read content types from the archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread worksheet titles and ids for a workbook", "response": "def read_sheets(archive):\n    \"\"\"Read worksheet titles and ids for a workbook\"\"\"\n    xml_source = archive.read(ARC_WORKBOOK)\n    tree = fromstring(xml_source)\n    for element in safe_iterator(tree, '{%s}sheet' % SHEET_MAIN_NS):\n        attrib = element.attrib\n        attrib['id'] = attrib[\"{%s}id\" % REL_NS]\n        del attrib[\"{%s}id\" % REL_NS]\n        if attrib['id']:\n            yield attrib"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef detect_worksheets(archive):\n    # content types has a list of paths but no titles\n    # workbook has a list of titles and relIds but no paths\n    # workbook_rels has a list of relIds and paths but no titles\n    # rels = {'id':{'title':'', 'path':''} }\n    content_types = read_content_types(archive)\n    valid_sheets = dict((path, ct) for ct, path in content_types if ct == WORKSHEET_TYPE)\n    rels = dict(read_rels(archive))\n    for sheet in read_sheets(archive):\n        rel = rels[sheet['id']]\n        rel['title'] = sheet['name']\n        rel['sheet_id'] = sheet['sheetId']\n        rel['state'] = sheet.get('state', 'visible')\n        if (\"/\" + rel['path'] in valid_sheets\n            or \"worksheets\" in rel['path']): # fallback in case content type is missing\n            yield rel", "response": "Return a list of worksheets in the archive"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_string_table(xml_source):\n    strings = []\n    src = _get_xml_iter(xml_source)\n\n    for _, node in iterparse(src):\n        if node.tag == '{%s}si' % SHEET_MAIN_NS:\n\n            text = Text.from_tree(node).content\n            text = text.replace('x005F_', '')\n            strings.append(text)\n\n            node.clear()\n\n    return IndexedList(strings)", "response": "Read in all shared strings in the table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_xml_iter(xml_source):\n    if not hasattr(xml_source, 'read'):\n        try:\n            xml_source = xml_source.encode(\"utf-8\")\n        except (AttributeError, UnicodeDecodeError):\n            pass\n        return BytesIO(xml_source)\n    else:\n        try:\n            xml_source.seek(0)\n        except:\n            # could be a zipfile\n            pass\n        return xml_source", "response": "Returns an iterator over the XML source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_node(t, ref = None, debug = False):\n    if t.ttype == \"operand\":\n        if t.tsubtype in [\"range\", \"named_range\", \"pointer\"] :\n            # print 'Creating Node', t.tvalue, t.tsubtype\n            return RangeNode(t, ref, debug = debug)\n        else:\n            return OperandNode(t)\n    elif t.ttype == \"function\":\n        return FunctionNode(t, ref, debug = debug)\n    elif t.ttype.startswith(\"operator\"):\n        return OperatorNode(t, ref, debug = debug)\n    else:\n        return ASTNode(t, debug = debug)", "response": "Create a node from a tree"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef shunting_yard(expression, named_ranges, ref = None, tokenize_range = False):\n\n    #remove leading =\n    if expression.startswith('='):\n        expression = expression[1:]\n\n    p = ExcelParser(tokenize_range = tokenize_range);\n    p.parse(expression)\n\n    # insert tokens for '(' and ')', to make things clearer below\n    tokens = []\n    for t in p.tokens.items:\n        if t.ttype == \"function\" and t.tsubtype == \"start\":\n            t.tsubtype = \"\"\n            tokens.append(t)\n            tokens.append(f_token('(','arglist','start'))\n        elif t.ttype == \"function\" and t.tsubtype == \"stop\":\n            tokens.append(f_token(')','arglist','stop'))\n        elif t.ttype == \"subexpression\" and t.tsubtype == \"start\":\n            t.tvalue = '('\n            tokens.append(t)\n        elif t.ttype == \"subexpression\" and t.tsubtype == \"stop\":\n            t.tvalue = ')'\n            tokens.append(t)\n        elif t.ttype == \"operand\" and t.tsubtype == \"range\" and t.tvalue in named_ranges:\n            t.tsubtype = \"named_range\"\n            tokens.append(t)\n        else:\n            tokens.append(t)\n\n    #http://office.microsoft.com/en-us/excel-help/calculation-operators-and-precedence-HP010078886.aspx\n    operators = {}\n    operators[':'] = Operator(':',8,'left')\n    operators[''] = Operator(' ',8,'left')\n    operators[','] = Operator(',',8,'left')\n    operators['u-'] = Operator('u-',7,'left') #unary negation\n    operators['%'] = Operator('%',6,'left')\n    operators['^'] = Operator('^',5,'left')\n    operators['*'] = Operator('*',4,'left')\n    operators['/'] = Operator('/',4,'left')\n    operators['+'] = Operator('+',3,'left')\n    operators['-'] = Operator('-',3,'left')\n    operators['&'] = Operator('&',2,'left')\n    operators['='] = Operator('=',1,'left')\n    operators['<'] = Operator('<',1,'left')\n    operators['>'] = Operator('>',1,'left')\n    operators['<='] = Operator('<=',1,'left')\n    operators['>='] = Operator('>=',1,'left')\n    operators['<>'] = Operator('<>',1,'left')\n\n    output = collections.deque()\n    stack = []\n    were_values = []\n    arg_count = []\n\n    new_tokens = []\n\n    # reconstruct expressions with ':' and replace the corresponding tokens by the reconstructed expression\n    if not tokenize_range:\n        for index, token in enumerate(tokens):\n            new_tokens.append(token)\n\n            if type(token.tvalue) == str or type(token.tvalue) == unicode:\n\n                if token.tvalue.startswith(':'): # example -> :OFFSET( or simply :A10\n                    depth = 0\n                    expr = ''\n\n                    rev = reversed(tokens[:index])\n\n                    for t in rev: # going backwards, 'stop' starts, 'start' stops\n                        if t.tsubtype == 'stop':\n                            depth += 1\n                        elif depth > 0 and t.tsubtype == 'start':\n                            depth -= 1\n\n                        expr = t.tvalue + expr\n\n                        new_tokens.pop()\n\n                        if depth == 0:\n                            new_tokens.pop() # these 2 lines are needed to remove INDEX()\n                            new_tokens.pop()\n                            expr = six.next(rev).tvalue + expr\n                            break\n\n                    expr += token.tvalue\n\n                    depth = 0\n\n                    if token.tvalue[1:] in ['OFFSET', 'INDEX']:\n                        for t in tokens[(index + 1):]:\n                            if t.tsubtype == 'start':\n                                depth += 1\n                            elif depth > 0 and t.tsubtype == 'stop':\n                                depth -= 1\n\n                            expr += t.tvalue\n\n                            tokens.remove(t)\n\n                            if depth == 0:\n                                break\n\n                    new_tokens.append(f_token(expr, 'operand', 'pointer'))\n\n                elif ':OFFSET' in token.tvalue or ':INDEX' in token.tvalue: # example -> A1:OFFSET(\n                    depth = 0\n                    expr = ''\n\n                    expr += token.tvalue\n\n                    for t in tokens[(index + 1):]:\n                        if t.tsubtype == 'start':\n                            depth += 1\n                        elif t.tsubtype == 'stop':\n                            depth -= 1\n\n                        expr += t.tvalue\n\n                        tokens.remove(t)\n\n                        if depth == 0:\n                            new_tokens.pop()\n                            break\n\n                    new_tokens.append(f_token(expr, 'operand', 'pointer'))\n\n\n    tokens = new_tokens if new_tokens else tokens\n\n    for t in tokens:\n\n        if t.ttype == \"operand\":\n            output.append(create_node(t, ref))\n            if were_values:\n                were_values.pop()\n                were_values.append(True)\n\n        elif t.ttype == \"function\":\n            stack.append(t)\n            arg_count.append(0)\n            if were_values:\n                were_values.pop()\n                were_values.append(True)\n            were_values.append(False)\n\n        elif t.ttype == \"argument\":\n\n            while stack and (stack[-1].tsubtype != \"start\"):\n                output.append(create_node(stack.pop(), ref))\n\n            if were_values.pop(): arg_count[-1] += 1\n            were_values.append(False)\n\n            if not len(stack):\n                raise Exception(\"Mismatched or misplaced parentheses\")\n\n        elif t.ttype.startswith('operator'):\n\n            if t.ttype.endswith('-prefix') and t.tvalue ==\"-\":\n                o1 = operators['u-']\n            else:\n                o1 = operators[t.tvalue]\n\n            while stack and stack[-1].ttype.startswith('operator'):\n\n                if stack[-1].ttype.endswith('-prefix') and stack[-1].tvalue ==\"-\":\n                    o2 = operators['u-']\n                else:\n                    o2 = operators[stack[-1].tvalue]\n\n                if ( (o1.associativity == \"left\" and o1.precedence <= o2.precedence)\n                        or\n                      (o1.associativity == \"right\" and o1.precedence < o2.precedence) ):\n                    output.append(create_node(stack.pop(), ref))\n                else:\n                    break\n            stack.append(t)\n\n        elif t.tsubtype == \"start\":\n            stack.append(t)\n\n        elif t.tsubtype == \"stop\":\n\n            while stack and stack[-1].tsubtype != \"start\":\n                output.append(create_node(stack.pop(), ref))\n\n            if not stack:\n                raise Exception(\"Mismatched or misplaced parentheses\")\n            stack.pop()\n\n            if stack and stack[-1].ttype == \"function\":\n                f = create_node(stack.pop(), ref)\n                a = arg_count.pop()\n                w = were_values.pop()\n                if w: a += 1\n                f.num_args = a\n                #print f, \"has \",a,\" args\"\n                output.append(f)\n\n\n\n    while stack:\n        if (stack[-1].tsubtype == \"start\" or stack[-1].tsubtype == \"stop\"):\n            raise Exception(\"Mismatched or misplaced parentheses\")\n\n        output.append(create_node(stack.pop(), ref))\n\n    # convert to list\n    return [x for x in output]", "response": "This function is used to generate a tree of nodes that are used in the Shunting Yard algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_ast(expression, debug = False):\n    #use a directed graph to store the tree\n    G = DiGraph()\n    stack = []\n\n\n    for n in expression:\n        # Since the graph does not maintain the order of adding nodes/edges\n        # add an extra attribute 'pos' so we can always sort to the correct order\n        if isinstance(n,OperatorNode):\n            if n.ttype == \"operator-infix\":\n                arg2 = stack.pop()\n                arg1 = stack.pop()\n                # Hack to write the name of sheet in 2argument address\n                if(n.tvalue == ':'):\n                    if '!' in arg1.tvalue and arg2.ttype == 'operand' and '!' not in arg2.tvalue:\n                        arg2.tvalue = arg1.tvalue.split('!')[0] + '!' + arg2.tvalue\n\n                G.add_node(arg1,pos = 1)\n                G.add_node(arg2,pos = 2)\n                G.add_edge(arg1, n)\n                G.add_edge(arg2, n)\n            else:\n                arg1 = stack.pop()\n                G.add_node(arg1,pos = 1)\n                G.add_edge(arg1, n)\n\n        elif isinstance(n,FunctionNode):\n            args = []\n            for _ in range(n.num_args):\n                try:\n                    args.append(stack.pop())\n                except:\n                    raise Exception()\n            #try:\n                # args = [stack.pop() for _ in range(n.num_args)]\n            #except:\n            #        print 'STACK', stack, type(n)\n            #        raise Exception('prut')\n            args.reverse()\n            for i,a in enumerate(args):\n                G.add_node(a,pos = i)\n                G.add_edge(a,n)\n        else:\n            G.add_node(n,pos=0)\n\n        stack.append(n)\n\n    return G,stack.pop()", "response": "build an AST from an Excel formula expression in reverse polish notation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cell2code(cell, named_ranges):\n    if cell.formula:\n\n        debug = False\n        # if 'OFFSET' in cell.formula or 'INDEX' in cell.formula:\n        #     debug = True\n        # if debug:\n        #     print 'FORMULA', cell.formula\n\n        ref = parse_cell_address(cell.address()) if not cell.is_named_range else None\n        sheet = cell.sheet\n\n        e = shunting_yard(cell.formula, named_ranges, ref=ref, tokenize_range = False)\n\n        ast,root = build_ast(e, debug = debug)\n        code = root.emit(ast, context=sheet)\n\n        # print 'CODE', code, ref\n\n    else:\n        ast = None\n        if isinstance(cell.value, unicode):\n            code = u'u\"' + cell.value.replace(u'\"', u'\\\\\"') + u'\"'\n        elif isinstance(cell.value, str):\n            raise RuntimeError(\"Got unexpected non-unicode str\")\n        else:\n            code = str(cell.value)\n    return code,ast", "response": "Generate python code for the given cell"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef graph_from_seeds(seeds, cell_source):\n\n    # when called from Spreadsheet instance, use the Spreadsheet cellmap and graph\n    if hasattr(cell_source, 'G'): # ~ cell_source is a Spreadsheet\n        cellmap = cell_source.cellmap\n        cells = cellmap\n        G = cell_source.G\n        for c in seeds:\n            G.add_node(c)\n            cellmap[c.address()] = c\n    # when called from ExcelCompiler instance, construct cellmap and graph from seeds\n    else: # ~ cell_source is a ExcelCompiler\n        cellmap = dict([(x.address(),x) for x in seeds])\n        cells = cell_source.cells\n        # directed graph\n        G = networkx.DiGraph()\n        # match the info in cellmap\n        for c in cellmap.values(): G.add_node(c)\n\n    # cells to analyze: only formulas\n    todo = [s for s in seeds if s.formula]\n    steps = [i for i,s in enumerate(todo)]\n    names = cell_source.named_ranges\n\n    while todo:\n        c1 = todo.pop()\n        step = steps.pop()\n        cursheet = c1.sheet\n\n        ###### 1) looking for cell c1 dependencies ####################\n        # print 'C1', c1.address()\n        # in case a formula, get all cells that are arguments\n        pystr, ast = cell2code(c1, names)\n        # set the code & compile it (will flag problems sooner rather than later)\n        c1.python_expression = pystr.replace('\"', \"'\") # compilation is done later\n\n        if 'OFFSET' in c1.formula or 'INDEX' in c1.formula:\n            if c1.address() not in cell_source.named_ranges: # pointers names already treated in ExcelCompiler\n                cell_source.pointers.add(c1.address())\n\n        # get all the cells/ranges this formula refers to\n        deps = [x for x in ast.nodes() if isinstance(x,RangeNode)]\n        # remove dupes\n        deps = uniqueify(deps)\n\n        ###### 2) connect dependencies in cells in graph ####################\n\n        # ### LOG\n        # tmp = []\n        # for dep in deps:\n        #     if dep not in names:\n        #         if \"!\" not in dep and cursheet != None:\n        #             dep = cursheet + \"!\" + dep\n        #     if dep not in cellmap:\n        #         tmp.append(dep)\n        # #deps = tmp\n        # logStep = \"%s %s = %s \" % ('|'*step, c1.address(), '',)\n        # print logStep\n\n        # if len(deps) > 1 and 'L' in deps[0] and deps[0] == deps[-1].replace('DG','L'):\n        #     print logStep, \"[%s...%s]\" % (deps[0], deps[-1])\n        # elif len(deps) > 0:\n        #     print logStep, \"->\", deps\n        # else:\n        #     print logStep, \"done\"\n\n        for dep in deps:\n            dep_name = dep.tvalue.replace('$','')\n\n            # this is to avoid :A1 or A1: dep due to clean_pointers() returning an ExcelError\n            if dep_name.startswith(':') or dep_name.endswith(':'):\n                dep_name = dep_name.replace(':', '')\n\n            # if not pointer, we need an absolute address\n            if dep.tsubtype != 'pointer' and dep_name not in names and \"!\" not in dep_name and cursheet != None:\n                dep_name = cursheet + \"!\" + dep_name\n\n            # Named_ranges + ranges already parsed (previous iterations)\n            if dep_name in cellmap:\n                origins = [cellmap[dep_name]]\n                target = cellmap[c1.address()]\n            # if the dep_name is a multi-cell range, create a range object\n            elif is_range(dep_name) or (dep_name in names and is_range(names[dep_name])):\n                if dep_name in names:\n                    reference = names[dep_name]\n                else:\n                    reference = dep_name\n\n                if 'OFFSET' in reference or 'INDEX' in reference:\n                    start_end = prepare_pointer(reference, names, ref_cell = c1)\n                    rng = cell_source.Range(start_end)\n\n                    if dep_name in names: # dep is a pointer range\n                        address = dep_name\n                    else:\n                        if c1.address() in names: # c1 holds is a pointer range\n                            address = c1.address()\n                        else: # a pointer range with no name, its address will be its name\n                            address = '%s:%s' % (start_end[\"start\"], start_end[\"end\"])\n                            cell_source.pointers.add(address)\n                else:\n                    address = dep_name\n\n                    # get a list of the addresses in this range that are not yet in the graph\n                    range_addresses = list(resolve_range(reference, should_flatten=True)[0])\n                    cellmap_add_addresses = [addr for addr in range_addresses if addr not in cellmap.keys()]\n\n                    if len(cellmap_add_addresses) > 0:\n                        # this means there are cells to be added\n\n                        # get row and col dimensions for the sheet, assuming the whole range is in one sheet\n                        sheet_initial = split_address(cellmap_add_addresses[0])[0]\n                        max_rows, max_cols = max_dimension(cellmap, sheet_initial)\n\n                        # create empty cells that aren't in the cellmap\n                        for addr in cellmap_add_addresses:\n                            sheet_new, col_new, row_new = split_address(addr)\n\n                            # if somehow a new sheet comes up in the range, get the new dimensions\n                            if sheet_new != sheet_initial:\n                                sheet_initial = sheet_new\n                                max_rows, max_cols = max_dimension(cellmap, sheet_new)\n\n                            # add the empty cells\n                            if int(row_new) <= max_rows and int(col2num(col_new)) <= max_cols:\n                                # only add cells within the maximum bounds of the sheet to avoid too many evaluations\n                                # for A:A or 1:1 ranges\n\n                                cell_new = Cell(addr, sheet_new, value=\"\", should_eval='False') # create new cell object\n                                cellmap[addr] = cell_new # add it to the cellmap\n                                G.add_node(cell_new) # add it to the graph\n                                cell_source.cells[addr] = cell_new # add it to the cell_source, used in this function\n\n                    rng = cell_source.Range(reference)\n\n                if address in cellmap:\n                    virtual_cell = cellmap[address]\n                else:\n                    virtual_cell = Cell(address, None, value = rng, formula = reference, is_range = True, is_named_range = True )\n                    # save the range\n                    cellmap[address] = virtual_cell\n\n                # add an edge from the range to the parent\n                G.add_node(virtual_cell)\n                # Cell(A1:A10) -> c1 or Cell(ExampleName) -> c1\n                G.add_edge(virtual_cell, c1)\n                # cells in the range should point to the range as their parent\n                target = virtual_cell\n                origins = []\n\n                if len(list(rng.keys())) != 0: # could be better, but can't check on Exception types here...\n                    for child in rng.addresses:\n                        if child not in cellmap:\n                            origins.append(cells[child])\n                        else:\n                            origins.append(cellmap[child])\n            else:\n                # not a range\n                if dep_name in names:\n                    reference = names[dep_name]\n                else:\n                    reference = dep_name\n\n                if reference in cells:\n                    if dep_name in names:\n                        virtual_cell = Cell(dep_name, None, value = cells[reference].value, formula = reference, is_range = False, is_named_range = True )\n\n                        G.add_node(virtual_cell)\n                        G.add_edge(cells[reference], virtual_cell)\n\n                        origins = [virtual_cell]\n                    else:\n                        cell = cells[reference]\n                        origins = [cell]\n\n                    cell = origins[0]\n\n                    if cell.formula is not None and ('OFFSET' in cell.formula or 'INDEX' in cell.formula):\n                        cell_source.pointers.add(cell.address())\n                else:\n                    virtual_cell = Cell(dep_name, None, value = None, formula = None, is_range = False, is_named_range = True )\n                    origins = [virtual_cell]\n\n                target = c1\n\n\n            # process each cell\n            for c2 in flatten(origins):\n\n                # if we havent treated this cell allready\n                if c2.address() not in cellmap:\n                    if c2.formula:\n                        # cell with a formula, needs to be added to the todo list\n                        todo.append(c2)\n                        steps.append(step+1)\n                    else:\n                        # constant cell, no need for further processing, just remember to set the code\n                        pystr,ast = cell2code(c2, names)\n                        c2.python_expression = pystr\n                        c2.compile()\n\n                    # save in the cellmap\n                    cellmap[c2.address()] = c2\n                    # add to the graph\n                    G.add_node(c2)\n\n                # add an edge from the cell to the parent (range or cell)\n                if(target != []):\n                    # print \"Adding edge %s --> %s\" % (c2.address(), target.address())\n                    G.add_edge(c2,target)\n\n        c1.compile() # cell compilation is done here because pointer ranges might update python_expressions\n\n\n    return (cellmap, G)", "response": "Creates a networkx graph from a list of seeds and a cell_source."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef examine(self):\n        if self._changes:\n            return self._changes.pop()\n\n        action_file = None\n        if self._changed:\n            self._changed = False\n            action_file = self._action_file or True  # TODO: Hack (see above)\n        return action_file, None", "response": "Called by the poll_tasks method to examine the current state of the user s entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nscheduling a watch for the specified path.", "response": "def watch(self, path, action, *args, **kwargs):\n        \"\"\"\n        Called by the Server instance when a new watch task is requested.\n        \"\"\"\n        if action is None:\n            action = _set_changed\n        event_handler = _WatchdogHandler(self, action)\n        self._observer.schedule(event_handler, path=path, recursive=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the AUTHORS file with a list of committers from GIT.", "response": "def authors():\n    \"\"\"\n    Updates the AUTHORS file with a list of committers from GIT.\n    \"\"\"\n    fmt_re = re.compile(r'([^<]+) <([^>]+)>')\n    authors = local('git shortlog -s -e -n | cut -f 2-', capture=True)\n    with open('AUTHORS', 'w') as fh:\n        fh.write('Project contributors\\n')\n        fh.write('====================\\n\\n')\n        for line in authors.splitlines():\n            match = fmt_re.match(line)\n            name, email = match.groups()\n            if email in env.ignored_authors:\n                continue\n            fh.write(' * ')\n            fh.write(line)\n            fh.write('\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef release():\n\n    if not is_working_tree_clean():\n        print('Your working tree is not clean. Refusing to create a release.')\n        return\n\n    print('Rebuilding the AUTHORS file to check for modifications...')\n    authors()\n\n    if not is_working_tree_clean():\n        print('Your working tree is not clean after the AUTHORS file was '\n              'rebuilt.')\n        print('Please commit the changes before continuing.')\n        return\n\n    if not is_manifest_up_to_date():\n        print('Manifest is not up to date.')\n        print('Please update MANIFEST.in or remove spurious files.')\n        return\n\n    # Get version\n    version = 'v{}'.format(local('python setup.py --version', capture=True))\n    name = local('python setup.py --name', capture=True)\n\n    # Tag\n    tag_message = '{} release version {}.'.format(name, version)\n\n    print('----------------------')\n    print('Proceeding will tag the release, push the repository upstream,')\n    print('and release a new version on PyPI.')\n    print()\n    print('Version: {}'.format(version))\n    print('Tag message: {}'.format(tag_message))\n    print()\n    if not confirm('Continue?', default=True):\n        print('Aborting.')\n        return\n\n    local('git tag -a {} -m {}'.format(pipes.quote(version),\n                                       pipes.quote(tag_message)))\n\n    # Push\n    local('git push --tags origin develop')\n\n    # Package and upload to pypi\n    local('python setup.py sdist bdist_wheel upload')", "response": "Create a new release and upload it to PyPI."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup(self, iterable, index=0, gather=False, edit_distance=0, max_edit_distance=0, match_threshold=0.0, matched_length=0):\n        if self.is_terminal:\n            if index == len(iterable) or \\\n                    (gather and index < len(iterable) and iterable[index] == ' '):  # only gather on word break)\n                confidence = float(len(self.key) - edit_distance) / float(max(len(self.key), index))\n                if confidence > match_threshold:\n                    yield {\n                        'key': self.key,\n                        'match': iterable[:index],\n                        'data': self.data,\n                        'confidence': confidence * self.weight\n                    }\n\n        if index < len(iterable) and iterable[index] in self.children:\n            for result in self.children[iterable[index]]\\\n                    .lookup(iterable, index + 1, gather=gather,\n                            edit_distance=edit_distance, max_edit_distance=max_edit_distance, matched_length=matched_length + 1):\n                yield result\n\n        # if there's edit distance remaining and it's possible to match a word above the confidence threshold\n        potential_confidence = float(index - edit_distance + (max_edit_distance - edit_distance)) / \\\n                               (float(index) + (max_edit_distance - edit_distance)) if index + max_edit_distance - edit_distance > 0 else 0.0\n        if edit_distance < max_edit_distance and potential_confidence > match_threshold:\n            for child in list(self.children):\n                if index >= len(iterable) or child != iterable[index]:\n                    # substitution\n                    for result in self.children[child]\\\n                        .lookup(iterable, index + 1, gather=gather,\n                                edit_distance=edit_distance + 1, max_edit_distance=max_edit_distance, matched_length=matched_length):\n                        yield result\n                    # delete\n                    for result in self.children[child]\\\n                        .lookup(iterable, index + 2, gather=gather,\n                                edit_distance=edit_distance + 1, max_edit_distance=max_edit_distance, matched_length=matched_length):\n                        yield result\n                    # insert\n                    for result in self.children[child]\\\n                        .lookup(iterable, index, gather=gather,\n                                edit_distance=edit_distance + 1, max_edit_distance=max_edit_distance, matched_length=matched_length):\n                        yield result", "response": "This function will look up the key in the tree and return the result of the search."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef insert(self, iterable, index=0, data=None, weight=1.0):\n        if index == len(iterable):\n            self.is_terminal = True\n            self.key = iterable\n            self.weight = weight\n            if data:\n                self.data.add(data)\n        else:\n            if iterable[index] not in self.children:\n                self.children[iterable[index]] = TrieNode()\n            self.children[iterable[index]].insert(iterable, index + 1, data)", "response": "Insert new node into tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves an element from the trie", "response": "def remove(self, iterable, data=None, index=0):\n        \"\"\"Remove an element from the trie\n\n        Args\n            iterable(hashable): key used to find what is to be removed\n            data(object): data associated with the key\n            index(int): index of what is to me removed\n\n        Returns:\n        bool:\n            True: if it was removed\n            False: if it was not removed\n        \"\"\"\n        if index == len(iterable):\n            if self.is_terminal:\n                if data:\n                    self.data.remove(data)\n                    if len(self.data) == 0:\n                        self.is_terminal = False\n                else:\n                    self.data.clear()\n                    self.is_terminal = False\n                return True\n            else:\n                return False\n        elif iterable[index] in self.children:\n            return self.children[iterable[index]].remove(iterable, index=index+1, data=data)\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the lookup with gather True Passing iterable yields the result.", "response": "def gather(self, iterable):\n        \"\"\"Calls the lookup with gather True Passing iterable and yields\n        the result.\n        \"\"\"\n        for result in self.lookup(iterable, gather=True):\n            yield result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookup(self, iterable, gather=False):\n        for result in self.root.lookup(iterable,\n                                       gather=gather,\n                                       edit_distance=0,\n                                       max_edit_distance=self.max_edit_distance,\n                                       match_threshold=self.match_threshold):\n            yield result", "response": "This is a generator function that returns the tree items from the root node with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to insert into the root node", "response": "def insert(self, iterable, data=None, weight=1.0):\n        \"\"\"Used to insert into he root node\n\n        Args\n            iterable(hashable): index or key used to identify\n            data(object): data to be paired with the key\n        \"\"\"\n        self.root.insert(iterable, index=0, data=data, weight=1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to remove from the root node", "response": "def remove(self, iterable, data=None):\n        \"\"\"Used to remove from the root node\n\n        Args:\n            iterable(hashable): index or key used to identify\n                item to remove\n            data: data to be paired with the key\n        \"\"\"\n        return self.root.remove(iterable, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a key from a tagged entity", "response": "def graph_key_from_tag(tag, entity_index):\n    \"\"\"Returns a key from a tag entity\n\n    Args:\n        tag (tag) : this is the tag selected to get the key from\n        entity_index (int) : this is the index of the tagged entity\n\n    Returns:\n        str : String representing the key for the given tagged entity.\n    \"\"\"\n    start_token = tag.get('start_token')\n    entity = tag.get('entities', [])[entity_index]\n    return str(start_token) + '-' + entity.get('key') + '-' + str(entity.get('confidence'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_edge(self, a, b):\n        neighbors_of_a = self.adjacency_lists.get(a)\n        if not neighbors_of_a:\n            neighbors_of_a = set()\n            self.adjacency_lists[a] = neighbors_of_a\n\n        neighbors_of_a.add(b)\n\n        neighbors_of_b = self.adjacency_lists.get(b)\n        if not neighbors_of_b:\n            neighbors_of_b = set()\n            self.adjacency_lists[b] = neighbors_of_b\n\n        neighbors_of_b.add(a)", "response": "Adds an edge between two nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, data):\n        if isinstance(data, list) and len(data) > 0:\n            self.nodes.append(data)\n        else:\n            self.nodes.append([data])", "response": "Appends items or lists to the Lattice\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a graph from the entities included in the tags.", "response": "def _build_graph(self, tags):\n        \"\"\"Builds a graph from the entities included in the tags.\n        Note this is used internally.\n\n        Args:\n            tags (list): A list of the tags to include in graph\n\n        Returns:\n            graph : this is the resulting graph of the tagged entities.\n\n        \"\"\"\n        graph = SimpleGraph()\n        for tag_index in xrange(len(tags)):\n            for entity_index in xrange(len(tags[tag_index].get('entities'))):\n                a_entity_name = graph_key_from_tag(tags[tag_index], entity_index)\n                tokens = self.tokenizer.tokenize(tags[tag_index].get('entities', [])[entity_index].get('match'))\n                for tag in tags[tag_index + 1:]:\n                    start_token = tag.get('start_token')\n                    if start_token >= tags[tag_index].get('start_token') + len(tokens):\n                        for b_entity_index in xrange(len(tag.get('entities'))):\n                            b_entity_name = graph_key_from_tag(tag, b_entity_index)\n                            graph.add_edge(a_entity_name, b_entity_name)\n\n        return graph"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expand(self, tags, clique_scoring_func=None):\n        lattice = Lattice()\n        overlapping_spans = []\n\n        def end_token_index():\n            return max([t.get('end_token') for t in overlapping_spans])\n\n        for i in xrange(len(tags)):\n            tag = tags[i]\n\n            if len(overlapping_spans) > 0 and end_token_index() >= tag.get('start_token'):\n                overlapping_spans.append(tag)\n            elif len(overlapping_spans) > 1:\n                cliques = list(self._sub_expand(overlapping_spans))\n                if clique_scoring_func:\n                    cliques = sorted(cliques, key=lambda e: -1 * clique_scoring_func(e))\n                lattice.append(cliques)\n                overlapping_spans = [tag]\n            else:\n                lattice.append(overlapping_spans)\n                overlapping_spans = [tag]\n        if len(overlapping_spans) > 1:\n            cliques = list(self._sub_expand(overlapping_spans))\n            if clique_scoring_func:\n                    cliques = sorted(cliques, key=lambda e: -1 * clique_scoring_func(e))\n            lattice.append(cliques)\n        else:\n            lattice.append(overlapping_spans)\n\n        return lattice.traverse()", "response": "This function expands the tags into cliques that are not overlapping with the current one."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nyielding the subsequences of the given tokens.", "response": "def _iterate_subsequences(self, tokens):\n        \"\"\"\n        Using regex invokes this function, which significantly impacts performance of adapt. it is an N! operation.\n\n        Args:\n            tokens(list): list of tokens for Yield results.\n\n        Yields:\n            str: ?\n        \"\"\"\n        for start_idx in xrange(len(tokens)):\n            for end_idx in xrange(start_idx + 1, len(tokens) + 1):\n                yield ' '.join(tokens[start_idx:end_idx]), start_idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntags the utterance with the known entities.", "response": "def tag(self, utterance, context_trie=None):\n        \"\"\"\n        Tag known entities within the utterance.\n        Args:\n            utterance(str): a string of natural language text\n            context_trie(trie): optional, a trie containing only entities from context\n                for this request\n\n        Returns: dictionary, with the following keys\n            match(str): the proper entity matched\n            key(str): the string that was matched to the entity\n            start_token(int): 0-based index of the first token matched\n            end_token(int): 0-based index of the last token matched\n            entities(list): a list of entity kinds as strings (Ex: Artist, Location)\n        \"\"\"\n        tokens = self.tokenizer.tokenize(utterance)\n        entities = []\n        if len(self.regex_entities) > 0:\n            for part, idx in self._iterate_subsequences(tokens):\n                local_trie = Trie()\n                for regex_entity in self.regex_entities:\n                    match = regex_entity.match(part)\n                    groups = match.groupdict() if match else {}\n                    for key in list(groups):\n                        match_str = groups.get(key)\n                        local_trie.insert(match_str, (match_str, key))\n                sub_tagger = EntityTagger(local_trie, self.tokenizer, max_tokens=self.max_tokens)\n                for sub_entity in sub_tagger.tag(part):\n                    sub_entity['start_token'] += idx\n                    sub_entity['end_token'] += idx\n                    for e in sub_entity['entities']:\n                        e['confidence'] = 0.5\n                    entities.append(sub_entity)\n        additional_sort = len(entities) > 0\n\n        context_entities = []\n        for i in xrange(len(tokens)):\n            part = ' '.join(tokens[i:])\n\n            for new_entity in self.trie.gather(part):\n                new_entity['data'] = list(new_entity['data'])\n                entities.append({\n                    'match': new_entity.get('match'),\n                    'key': new_entity.get('key'),\n                    'start_token': i,\n                    'entities': [new_entity],\n                    'end_token': i + len(self.tokenizer.tokenize(new_entity.get('match'))) - 1,\n                    'from_context': False\n                })\n\n            if context_trie:\n                for new_entity in context_trie.gather(part):\n                    new_entity['data'] = list(new_entity['data'])\n                    new_entity['confidence'] *= 2.0  # context entities get double the weight!\n                    context_entities.append({\n                        'match': new_entity.get('match'),\n                        'key': new_entity.get('key'),\n                        'start_token': i,\n                        'entities': [new_entity],\n                        'end_token': i + len(self.tokenizer.tokenize(new_entity.get('match'))) - 1,\n                        'from_context': True\n                    })\n\n        additional_sort = additional_sort or len(entities) > 0\n\n        if additional_sort:\n            entities = self._sort_and_merge_tags(entities + context_entities)\n\n        return entities"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecide the best intent for given parse_result", "response": "def __best_intent(self, parse_result, context=[]):\n        \"\"\"\n        Decide the best intent\n\n        Args:\n            parse_result(list): results used to match the best intent.\n            context(list): ?\n\n        Returns:\n            best_intent, best_tags:\n                best_intent : The best intent for given results\n                best_tags : The Tags for result\n        \"\"\"\n        best_intent = None\n        best_tags = None\n        context_as_entities = [{'entities': [c]} for c in context]\n        for intent in self.intent_parsers:\n            i, tags = intent.validate_with_tags(parse_result.get('tags') + context_as_entities, parse_result.get('confidence'))\n            if not best_intent or (i and i.get('confidence') > best_intent.get('confidence')):\n                best_intent = i\n                best_tags = tags\n\n        return best_intent, best_tags"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse to get unused context from parse_result.", "response": "def __get_unused_context(self, parse_result, context):\n        \"\"\" Used to get unused context from context.  Any keys not in\n        parse_result\n\n        Args:\n            parse_results(list): parsed results used to identify what keys\n                in the context are used.\n            context(list): this is the context used to match with parsed results\n                keys missing in the parsed results are the unused context\n\n        Returns:\n            list: A list of the unused context results.\n        \"\"\"\n        tags_keys = set([t['key'] for t in parse_result['tags'] if t['from_context']])\n        result_context = [c for c in context if c['key'] not in tags_keys]\n        return result_context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an utterance provide a valid intent.", "response": "def determine_intent(self, utterance, num_results=1, include_tags=False, context_manager=None):\n        \"\"\"\n        Given an utterance, provide a valid intent.\n\n        Args:\n            utterance(str): an ascii or unicode string representing natural language speech\n            include_tags(list): includes the parsed tags (including position and confidence)\n                as part of result\n            context_manager(list): a context manager to provide context to the utterance\n            num_results(int): a maximum number of results to be returned.\n\n        Returns: A generator that yields dictionaries.\n        \"\"\"\n        parser = Parser(self.tokenizer, self.tagger)\n        parser.on('tagged_entities',\n                  (lambda result:\n                   self.emit(\"tagged_entities\", result)))\n\n        context = []\n        if context_manager:\n            context = context_manager.get_context()\n\n        for result in parser.parse(utterance, N=num_results, context=context):\n            self.emit(\"parse_result\", result)\n            # create a context without entities used in result\n            remaining_context = self.__get_unused_context(result, context)\n            best_intent, tags = self.__best_intent(result, remaining_context)\n            if best_intent and best_intent.get('confidence', 0.0) > 0:\n                if include_tags:\n                    best_intent['__tags__'] = tags\n                yield best_intent"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_entity(self, entity_value, entity_type, alias_of=None):\n        if alias_of:\n            self.trie.insert(entity_value.lower(), data=(alias_of, entity_type))\n        else:\n            self.trie.insert(entity_value.lower(), data=(entity_value, entity_type))\n            self.trie.insert(entity_type.lower(), data=(entity_type, 'Concept'))", "response": "Register an entity instance in the potential parse results."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a regular expression entity that can be used to match the log entries in the log file.", "response": "def register_regex_entity(self, regex_str):\n        \"\"\"\n        A regular expression making use of python named group expressions.\n\n        Example: (?P<Artist>.*)\n\n        regex_str(str): a string representing a regular expression as defined above\n        \"\"\"\n        if regex_str and regex_str not in self._regex_strings:\n            self._regex_strings.add(regex_str)\n            self.regular_expressions_entities.append(re.compile(regex_str, re.IGNORECASE))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_intent_parser(self, intent_parser):\n        if hasattr(intent_parser, 'validate') and callable(intent_parser.validate):\n            self.intent_parsers.append(intent_parser)\n        else:\n            raise ValueError(\"%s is not an intent parser\" % str(intent_parser))", "response": "Enforce the intent parser interface at registration time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tokenizer(self):\n        domain = 0\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        return self.domains[domain].tokenizer", "response": "A property to link into IntentEngine s tokenizer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef intent_parsers(self):\n        domain = 0\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        return self.domains[domain].intent_parsers", "response": "A property to link into IntentEngine s intent_parsers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _regex_strings(self):\n        domain = 0\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        return self.domains[domain]._regex_strings", "response": "A property to link into IntentEngine s _regex_strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef regular_expressions_entities(self):\n        domain = 0\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        return self.domains[domain].regular_expressions_entities", "response": "A property to link into IntentEngine s regular_expressions_entities."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a domain with the intent engine.", "response": "def register_domain(self, domain=0, tokenizer=None, trie=None):\n        \"\"\"\n        Register a domain with the intent engine.\n\n        Args:\n            tokenizer(tokenizer): The tokenizer you wish to use.\n            trie(Trie): the Trie() you wish to use.\n            domain(str): a string representing the domain you wish to add\n        \"\"\"\n        self.domains[domain] = IntentDeterminationEngine(\n            tokenizer=tokenizer, trie=trie)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_entity(self, entity_value, entity_type, alias_of=None, domain=0):\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        self.domains[domain].register_entity(entity_value=entity_value,\n                                             entity_type=entity_type,\n                                             alias_of=alias_of)", "response": "Register an entity to be tagged in potential parse results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef register_regex_entity(self, regex_str, domain=0):\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        self.domains[domain].register_regex_entity(regex_str=regex_str)", "response": "Register a regular expression entity in the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef determine_intent(self, utterance, num_results=1):\n        intents = []\n        for domain in self.domains:\n            gen = self.domains[domain].determine_intent(utterance=utterance,\n                                                        num_results=1)\n            for intent in gen:\n                intents.append(intent)\n\n        heapq.nlargest(\n            num_results, intents, key=lambda domain: domain['confidence'])\n        for intent in intents:\n            yield intent", "response": "Given an utterance provide a valid intent."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_intent_parser(self, intent_parser, domain=0):\n        if domain not in self.domains:\n            self.register_domain(domain=domain)\n        self.domains[domain].register_intent_parser(\n            intent_parser=intent_parser)", "response": "Register a intent parser with a domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to parce a string into tokens This function is to take in a string and return a list of tokens Args: string(str): This is a string of words or a sentance to be parsed into tokens Returns: list: a list of tokens from the string passed in. Notes: Doesn't seem to parse contractions correctly for example don't would parse as two tokens 'do' and \"n't\" and this seems to be not what we would want. Maybe should be \"don't\" or maybe contractions should be expanded into \"do not\" or \"do\",\"not\". This could be done with a contraction dictionary and some preprocessing.", "response": "def tokenize(self, string):\n        \"\"\"Used to parce a string into tokens\n\n        This function is to take in a string and return a list of tokens\n\n        Args:\n            string(str): This is a string of words or a sentance to be parsed into tokens\n\n        Returns:\n            list: a list of tokens from the string passed in.\n\n        Notes:\n            Doesn't seem to parse contractions correctly for example don't\n            would parse as two tokens 'do' and \"n't\" and this seems to be not\n            what we would want.  Maybe should be \"don't\" or maybe contractions\n            should be expanded into \"do not\" or \"do\",\"not\".  This could be\n            done with a contraction dictionary and some preprocessing.\n        \"\"\"\n        s = string\n        s = re.sub('\\t', \" \", s)\n        s = re.sub(\"(\" + regex_separator + \")\", \" \\g<1> \", s)\n        s = re.sub(\"([^0-9]),\", \"\\g<1> , \", s)\n        s = re.sub(\",([^0-9])\", \" , \\g<1>\", s)\n        s = re.sub(\"^(')\", \"\\g<1> \", s)\n        s = re.sub(\"(\" + regex_not_letter_number + \")'\", \"\\g<1> '\", s)\n        s = re.sub(\"(\" + regex_clitics + \")$\", \" \\g<1>\", s)\n        s = re.sub(\"(\" + regex_clitics + \")(\" + regex_not_letter_number + \")\", \" \\g<1> \\g<2>\", s)\n\n        words = s.strip().split()\n        p1 = re.compile(\".*\" + regex_letter_number + \"\\\\.\")\n        p2 = re.compile(\"^([A-Za-z]\\\\.([A-Za-z]\\\\.)+|[A-Z][bcdfghj-nptvxz]+\\\\.)$\")\n\n        token_list = []\n\n        for word in words:\n            m1 = p1.match(word)\n            m2 = p2.match(word)\n\n            if m1 and word not in abbreviations_list and not m2:\n                token_list.append(word[0: word.find('.')])\n                token_list.append(word[word.find('.')])\n            else:\n                token_list.append(word)\n\n        return token_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse to find tags within a given utterance with a given confidence.", "response": "def parse(self, utterance, context=None, N=1):\n        \"\"\"Used to find tags within utterance with a given confidence\n\n        Args:\n            utterance(str): conversational piece given by the user\n            context(list): a list of entities\n            N(int): number of results\n        Returns: yield an object with the following fields\n            utterance(str): the value passed in\n            tags(list) : a list of tags found in utterance\n            time(time) : duration since call of function\n            confidence(float) : float indicating how confident of a match to the\n                utterance. This might be used to determan the most likely intent.\n\n        \"\"\"\n        start = time.time()\n        context_trie = None\n        if context and isinstance(context, list):\n            # sort by confidence in ascending order, so\n            # highest confidence for an entity is last.\n            # see comment on TrieNode ctor\n            context.sort(key=lambda x: x.get('confidence'))\n\n            context_trie = Trie()\n            for entity in context:\n                entity_value, entity_type = entity.get('data')[0]\n                context_trie.insert(entity_value.lower(),\n                                    data=(entity_value, entity_type),\n                                    weight=entity.get('confidence'))\n\n        tagged = self._tagger.tag(utterance.lower(), context_trie=context_trie)\n        self.emit(\"tagged_entities\",\n                  {\n                      'utterance': utterance,\n                      'tags': list(tagged),\n                      'time': time.time() - start\n                  })\n        start = time.time()\n        bke = BronKerboschExpander(self._tokenizer)\n\n        def score_clique(clique):\n            score = 0.0\n            for tagged_entity in clique:\n                ec = tagged_entity.get('entities', [{'confidence': 0.0}])[0].get('confidence')\n                score += ec * len(tagged_entity.get('entities', [{'match': ''}])[0].get('match')) / (\n                    len(utterance) + 1)\n            return score\n\n        parse_results = bke.expand(tagged, clique_scoring_func=score_clique)\n        count = 0\n        for result in parse_results:\n            count += 1\n            parse_confidence = 0.0\n            for tag in result:\n                sample_entity = tag['entities'][0]\n                entity_confidence = sample_entity.get('confidence', 0.0) * float(\n                    len(sample_entity.get('match'))) / len(utterance)\n                parse_confidence += entity_confidence\n            yield {\n                'utterance': utterance,\n                'tags': result,\n                'time': time.time() - start,\n                'confidence': parse_confidence\n            }\n\n            if count >= N:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef metadata_matches(self, query={}):\n        result = len(query.keys()) > 0\n        for key in query.keys():\n            result = result and query[key] == self.metadata.get(key)\n\n        return result", "response": "Returns True if every key in query matches to metadata\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge_context(self, tag, metadata):\n        self.entities.append(tag)\n        for k in metadata.keys():\n            if k not in self.metadata:\n                self.metadata[k] = k", "response": "Merge entity and metadata into contextManagerFrame new entity and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninjects a context into the stack.", "response": "def inject_context(self, entity, metadata={}):\n        \"\"\"\n        Args:\n            entity(object):\n                format {'data': 'Entity tag as <str>',\n                        'key': 'entity proper name as <str>',\n                         'confidence': <float>'\n                         }\n            metadata(object): dict, arbitrary metadata about the entity being added\n        \"\"\"\n        top_frame = self.frame_stack[0] if len(self.frame_stack) > 0 else None\n        if top_frame and top_frame.metadata_matches(metadata):\n            top_frame.merge_context(entity, metadata)\n        else:\n            frame = ContextManagerFrame(entities=[entity], metadata=metadata.copy())\n            self.frame_stack.insert(0, frame)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_context(self, max_frames=None, missing_entities=[]):\n        if not max_frames or max_frames > len(self.frame_stack):\n            max_frames = len(self.frame_stack)\n\n        missing_entities = list(missing_entities)\n        context = []\n        for i in xrange(max_frames):\n            frame_entities = [entity.copy() for entity in self.frame_stack[i].entities]\n            for entity in frame_entities:\n                entity['confidence'] = entity.get('confidence', 1.0) / (2.0 + i)\n            context += frame_entities\n\n        result = []\n        if len(missing_entities) > 0:\n            for entity in context:\n                if entity.get('data') in missing_entities:\n                    result.append(entity)\n                    # NOTE: this implies that we will only ever get one\n                    # of an entity kind from context, unless specified\n                    # multiple times in missing_entities. Cannot get\n                    # an arbitrary number of an entity kind.\n                    missing_entities.remove(entity.get('data'))\n        else:\n            result = context\n\n        return result", "response": "Constructs a list of entities from the context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch tags for entity type after given index.", "response": "def find_first_tag(tags, entity_type, after_index=-1):\n    \"\"\"Searches tags for entity type after given index\n\n    Args:\n        tags(list): a list of tags with entity types to be compaired too entity_type\n        entity_type(str): This is he entity type to be looking for in tags\n        after_index(int): the start token must be greaterthan this.\n\n    Returns:\n        ( tag, v, confidence ):\n            tag(str): is the tag that matched\n            v(str): ? the word that matched?\n            confidence(float): is a mesure of accuacy.  1 is full confidence and 0 is none.\n    \"\"\"\n    for tag in tags:\n        for entity in tag.get('entities'):\n            for v, t in entity.get('data'):\n                if t.lower() == entity_type.lower() and tag.get('start_token', 0) > after_index:\n                    return tag, v, entity.get('confidence')\n\n    return None, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choose_1_from_each(lists):\n    if len(lists) == 0:\n        yield []\n    else:\n        for el in lists[0]:\n            for next_list in choose_1_from_each(lists[1:]):\n                yield [el] + next_list", "response": "Takes a list of lists and returns a list of lists with one item\n            from each list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_one_of(tags, at_least_one):\n    if len(tags) < len(at_least_one):\n        return None\n    for possible_resolution in choose_1_from_each(at_least_one):\n        resolution = {}\n        pr = possible_resolution[:]\n        for entity_type in pr:\n            last_end_index = -1\n            if entity_type in resolution:\n                last_end_index = resolution.get[entity_type][-1].get('end_token')\n            tag, value, c = find_first_tag(tags, entity_type, after_index=last_end_index)\n            if not tag:\n                break\n            else:\n                if entity_type not in resolution:\n                    resolution[entity_type] = []\n                resolution[entity_type].append(tag)\n        if len(resolution) == len(possible_resolution):\n            return resolution\n\n    return None", "response": "This function searches tags for Entites in at_least_one and returns any match in the object\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, tags, confidence):\n        intent, tags = self.validate_with_tags(tags, confidence)\n        return intent", "response": "Using this method removes tags from the result of validate_with_tags\n            and returns the intent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate weather tags for this intent to fire.", "response": "def validate_with_tags(self, tags, confidence):\n        \"\"\"Validate weather tags has required entites for this intent to fire\n\n        Args:\n            tags(list): Tags and Entities used for validation\n            confidence(float): ?\n\n        Returns:\n            intent, tags: Returns intent and tags used by the intent on\n                falure to meat required entities then returns intent with confidence\n                of 0.0 and an empty list for tags.\n        \"\"\"\n        result = {'intent_type': self.name}\n        intent_confidence = 0.0\n        local_tags = tags[:]\n        used_tags = []\n\n        for require_type, attribute_name in self.requires:\n            required_tag, canonical_form, confidence = find_first_tag(local_tags, require_type)\n            if not required_tag:\n                result['confidence'] = 0.0\n                return result, []\n\n            result[attribute_name] = canonical_form\n            if required_tag in local_tags:\n                local_tags.remove(required_tag)\n            used_tags.append(required_tag)\n            # TODO: use confidence based on edit distance and context\n            intent_confidence += confidence\n\n        if len(self.at_least_one) > 0:\n            best_resolution = resolve_one_of(tags, self.at_least_one)\n            if not best_resolution:\n                result['confidence'] = 0.0\n                return result, []\n            else:\n                for key in best_resolution:\n                    result[key] = best_resolution[key][0].get('key') # TODO: at least one must support aliases\n                    intent_confidence += 1.0\n                used_tags.append(best_resolution)\n                if best_resolution in local_tags:\n                    local_tags.remove(best_resolution)\n\n        for optional_type, attribute_name in self.optional:\n            optional_tag, canonical_form, conf = find_first_tag(local_tags, optional_type)\n            if not optional_tag or attribute_name in result:\n                continue\n            result[attribute_name] = canonical_form\n            if optional_tag in local_tags:\n                local_tags.remove(optional_tag)\n            used_tags.append(optional_tag)\n            intent_confidence += 1.0\n\n        total_confidence = intent_confidence / len(tags) * confidence\n\n        target_client, canonical_form, confidence = find_first_tag(local_tags, CLIENT_ENTITY_NAME)\n\n        result['target'] = target_client.get('key') if target_client else None\n        result['confidence'] = total_confidence\n\n        return result, used_tags"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef require(self, entity_type, attribute_name=None):\n        if not attribute_name:\n            attribute_name = entity_type\n        self.requires += [(entity_type, attribute_name)]\n        return self", "response": "This method is used to add an entity of the provided type to the list of requires."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an optional intent to the list of optional intent entries.", "response": "def optionally(self, entity_type, attribute_name=None):\n        \"\"\"\n        Parsed intents from this parser can optionally include an entity of the provided type.\n\n        Args:\n            entity_type(str): an entity type\n            attribute_name(str): the name of the attribute on the parsed intent. Defaults to match entity_type.\n\n        Returns:\n            self: to continue modifications.\n        \"\"\"\n        if not attribute_name:\n            attribute_name = entity_type\n        self.optional += [(entity_type, attribute_name)]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self):\n        return Intent(self.name, self.requires, self.at_least_one, self.optional)", "response": "Constructs an intent from the builder s specifications."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extract_subset(self, subset):\n    \n        # note subsets have an unusual encoding\n        query = \"\"\"\n        prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n        SELECT ?c WHERE {{\n        GRAPH <{g}>  {{\n        ?c oboInOwl:inSubset ?s\n        FILTER regex(?s,'#{s}$','i')\n        }}\n        }}\n        \"\"\".format(s=subset, g=self.graph_name)\n        bindings = run_sparql(query)\n        return [r['c']['value'] for r in bindings]", "response": "Extract all nodes in a subset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding all subsets for an ontology", "response": "def subsets(self):\n        \"\"\"\n        Find all subsets for an ontology\n        \"\"\"\n    \n        # note subsets have an unusual encoding\n        query = \"\"\"\n        prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n        SELECT DISTINCT ?s WHERE {{\n        GRAPH <{g}>  {{\n        ?c oboInOwl:inSubset ?s \n        }}\n        }}\n        \"\"\".format(g=self.graph_name)\n        bindings = run_sparql(query)\n        return [r['s']['value'] for r in bindings]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches for things using labels", "response": "def _search(self, searchterm, pred, **args):\n        \"\"\"\n        Search for things using labels\n        \"\"\"\n        # TODO: DRY with sparql_ontol_utils\n        searchterm = searchterm.replace('%','.*')\n        namedGraph = get_named_graph(self.handle)\n        query = \"\"\"\n        prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n        SELECT ?c WHERE {{\n        GRAPH <{g}>  {{\n        ?c {pred} ?l\n        FILTER regex(?l,'{s}','i')\n        }}\n        }}\n        \"\"\".format(pred=pred, s=searchterm, g=namedGraph)\n        bindings = run_sparql(query)\n        return [r['c']['value'] for r in bindings]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a SPARQL query on the local cache entry.", "response": "def sparql(self, select='*', body=None, inject_prefixes=None, single_column=False):\n        \"\"\"\n        Execute a SPARQL query.\n\n        The query is specified using `select` and `body` parameters.\n        The argument for the Named Graph is injected into the query.\n\n        The select parameter should be either '*' or a list of vars (not prefixed with '?').\n\n         - If '*' is passed, then the result is a list of dicts, { $var: {value: $val } }\n         - If a list of vars is passed, then the result is a list of lists\n         - Unless single_column=True, in which case the results are a simple list of values from the first var\n\n        The inject_prefixes argument can be used to inject a list of prefixes - these are expanded\n        using the prefixcommons library\n        \n        \"\"\"\n        if inject_prefixes is None:\n            inject_prefixes = []\n        namedGraph = get_named_graph(self.handle)\n        cols = []\n        select_val = None\n        if select is None or select=='*':\n            if not single_column:\n                cols=None\n            select_val='*'\n        else:\n            if isinstance(cols,list):\n                cols = [select]\n            else:\n                cols = select\n            select_val = \", \".join(['?'+c for c in cols])\n\n        prefixes = \"\"\n        if inject_prefixes is not None:\n            plist = [\"prefix {}: <{}> \".format(p,expand_uri(p+\":\")) for p in inject_prefixes if p != \"\" and p is not None]\n            prefixes = \"\\n\".join(plist)\n        query = \"\"\"\n        {prefixes}\n        SELECT {s} WHERE {{\n        GRAPH <{g}>  {{\n        {b}\n        }}\n        }}\n        \"\"\".format(prefixes=prefixes, s=select_val, b=body, g=namedGraph)\n        bindings = run_sparql(query)\n        if len(bindings) == 0:\n            return []\n        if cols is None:\n            return bindings\n        else:\n            if single_column:\n                c = list(bindings[0].keys())[0]\n                return [r[c]['value'] for r in bindings]\n            else:\n                return [r[c]['value'] for c in cols for r in bindings]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a matrix of terms that are in the specified list of objects.", "response": "def term_matrix(idlist, subject_category, taxon, **kwargs):\n    \"\"\"\n    Intersection between annotated objects\n\n             P1  not(P1)\n    F1       0   5\n    not(F1)  6   0\n    \"\"\"\n    results = search_associations(objects=idlist,\n                                  subject_taxon=taxon,\n                                  subject_category=subject_category,\n                                  select_fields=[M.SUBJECT, M.OBJECT_CLOSURE],\n                                  facet_fields=[],\n                                  rows=-1,\n                                  include_raw=True,\n                                  **kwargs)\n    docs = results['raw'].docs\n\n    subjects_per_term = {}\n    smap = {}\n    for d in docs:\n        smap[d[M.SUBJECT]] = 1\n        for c in d[M.OBJECT_CLOSURE]:\n            if c in idlist:\n                if c not in subjects_per_term:\n                    subjects_per_term[c] = []\n                subjects_per_term[c].append(d[M.SUBJECT])\n    pop_n = len(smap.keys())\n\n    cells = []\n    for cx in idlist:\n        csubjs = set(subjects_per_term[cx])\n        for dx in idlist:\n            dsubjs = set(subjects_per_term[dx])\n            a = len(csubjs.intersection(dsubjs))\n            b = len(csubjs) - a\n            c = len(dsubjs) - a\n            d = pop_n - len(dsubjs) - b\n            ctable = [[a, b], [c, d]]\n\n            _, p_under = sp.stats.fisher_exact(ctable, 'less')\n            _, p_over = sp.stats.fisher_exact(ctable, 'greater')\n\n            cells.append({'c':cx, 'd':dx,\n                          'nc':len(csubjs),\n                          'nd':len(dsubjs),\n                          'n':a,\n                          'p_l':p_under,\n                          'p_g':p_over\n            })\n    return cells"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the ancestors of a GO term through a subontology.", "response": "def get_ancestors_through_subont(self, go_term, relations):\n        \"\"\"\n        Returns the ancestors from the relation filtered GO subontology of go_term's ancestors.\n\n        subontology() primarily used here for speed when specifying relations to traverse. Point of this is to first get\n        a smaller graph (all ancestors of go_term regardless of relation) and then filter relations on that instead of\n        the whole GO.\n        \"\"\"\n        all_ancestors = self.ontology.ancestors(go_term, reflexive=True)\n        subont = self.ontology.subontology(all_ancestors)\n        return subont.ancestors(go_term, relations)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True is go_term has is_a part_of ancestor of biological process GO : 0008150", "response": "def is_biological_process(self, go_term):\n        \"\"\"\n        Returns True is go_term has is_a, part_of ancestor of biological process GO:0008150\n        \"\"\"\n        bp_root = \"GO:0008150\"\n        if go_term == bp_root:\n            return True\n        ancestors = self.get_isa_closure(go_term)\n        if bp_root in ancestors:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_molecular_function(self, go_term):\n        mf_root = \"GO:0003674\"\n        if go_term == mf_root:\n            return True\n        ancestors = self.get_isa_closure(go_term)\n        if mf_root in ancestors:\n            return True\n        else:\n            return False", "response": "Returns True is go_term has is_a part_of ancestor of molecular function GO : 0003674\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_cellular_component(self, go_term):\n        cc_root = \"GO:0005575\"\n        if go_term == cc_root:\n            return True\n        ancestors = self.get_isa_closure(go_term)\n        if cc_root in ancestors:\n            return True\n        else:\n            return False", "response": "Returns True is go_term has is_a part_of ancestor of cellular component GO : 0005575"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning F C or P corresponding to the aspect of a GO term.", "response": "def go_aspect(self, go_term):\n        \"\"\"\n        For GO terms, returns F, C, or P corresponding to its aspect\n        \"\"\"\n        if not go_term.startswith(\"GO:\"):\n            return None\n        else:\n            # Check ancestors for root terms\n            if self.is_molecular_function(go_term):\n                return 'F'\n            elif self.is_cellular_component(go_term):\n                return 'C'\n            elif self.is_biological_process(go_term):\n                return 'P'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _neighbors_graph(self, **params) -> Dict:\n        response = self._get_response(\"graph/neighbors\", format=\"json\", **params)\n        return response.json()", "response": "Get neighbors of a node in SciGraph"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an Ontology object from an rdflib graph object", "response": "def rdfgraph_to_ontol(rg):\n    \"\"\"\n    Return an Ontology object from an rdflib graph object\n\n    Status: Incomplete\n    \"\"\"\n    digraph = networkx.MultiDiGraph()\n    from rdflib.namespace import RDF\n    label_map = {}\n    for c in rg.subjects(RDF.type, OWL.Class):\n        cid = contract_uri_wrap(c)\n        logging.info(\"C={}\".format(cid))\n        for lit in rg.objects(c, RDFS.label):\n            label_map[cid] = lit.value\n            digraph.add_node(cid, label=lit.value)\n        for s in rg.objects(c, RDFS.subClassOf):\n            # todo - blank nodes\n            sid = contract_uri_wrap(s)\n            digraph.add_edge(sid, cid, pred='subClassOf')\n\n    logging.info(\"G={}\".format(digraph))\n    payload = {\n        'graph': digraph,\n        #'xref_graph': xref_graph,\n        #'graphdoc': obographdoc,\n        #'logical_definitions': logical_definitions\n    }\n            \n    ont = Ontology(handle='wd', payload=payload)\n    return ont"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a tuple of counts for each entity in a given category.", "response": "def get_counts(entities=None,\n               object_category=None,\n               min_count=1,\n               **kwargs):\n    \"\"\"\n    given a set of entities (genes, diseases, etc), finds the number of entities associated with each descriptor in a given category.\n\n    The result is a tuple (cmap, results), where cmap is a dict of TERM:COUNT\n\n    \"\"\"\n    if entities is None:\n        entities = []\n    results = search_associations(subjects=entities,\n                                  subject_direct=True,\n                                  rows=0,\n                                  facet_fields=[M.IS_DEFINED_BY, M.SUBJECT_TAXON, M.SUBJECT_CATEGORY],\n                                  object_category=object_category,\n                                  facet_mincount=3, # TODO\n                                  facet_limit=-1,\n                                  json_facet={\n                                      'categories':{\n                                          'limit':-1,\n                                          'type': 'terms',\n                                          'field' : M.OBJECT_CLOSURE,\n                                          'facet' : {\n                                              'uniq_subject': \"unique(subject)\"\n                                          }\n                                      }\n                                  },\n                                  **kwargs)\n    buckets = results['facets']['categories']['buckets']\n    cmap = {}\n    for bucket in buckets:\n        if bucket['uniq_subject'] >= min_count:\n            cmap[bucket['val']] = bucket['uniq_subject']\n    return (cmap, results)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_enriched(sample_entities=None,\n                  background_entities=None,\n                  object_category=None,\n                  **kwargs):\n\n    \"\"\"\n    Given a sample set of sample_entities (e.g. overexpressed genes) and a background set (e.g. all genes assayed), and a category of descriptor (e.g. phenotype, function),\n    return enriched descriptors/classes\n    \"\"\"\n    if sample_entities is None:\n        sample_entites = []\n\n    (sample_counts, sample_results) = get_counts(entities=sample_entities,\n                                                 object_category=object_category,\n                                                 min_count=2,\n                                                 **kwargs)\n    print(str(sample_counts))\n\n    sample_fcs = sample_results['facet_counts']\n    taxon_count_dict = sample_fcs[M.SUBJECT_TAXON]\n\n    taxon=None\n    for (t,tc) in taxon_count_dict.items():\n        # TODO - throw error if multiple taxa\n        taxon = t\n\n    if background_entities is None:\n        objects = list(sample_counts.keys())\n        print(\"OBJECTS=\"+str(objects))\n        background_entities = get_background(objects, taxon, object_category)\n\n    # TODO: consider caching\n    (bg_counts,_) = get_counts(entities=background_entities,\n                               object_category=object_category,\n                               **kwargs)\n\n    sample_n = len(sample_entities) # TODO - annotated only?\n    pop_n = len(background_entities)\n    # adapted from goatools\n    for (sample_termid,sample_count) in sample_counts.items():\n        pop_count = bg_counts[sample_termid]\n\n        # https://en.wikipedia.org/wiki/Fisher's_exact_test\n        #              Cls  NotCls\n        # study/sample [a,      b]\n        # rest of ref  [c,      d]\n        #              \n        a = sample_count\n        b = sample_n - sample_count\n        c = pop_count - sample_count\n        d = pop_n - pop_count - b\n        print(\"ABCD=\"+str((sample_termid,a,b,c,d,sample_n)))\n        _, p_uncorrected = sp.stats.fisher_exact( [[a, b], [c, d]])\n        print(\"P=\"+str(p_uncorrected))", "response": "Given a set of sample_entities and background_entities and a category of descriptor return enriched descriptors and classes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch an association object by ID", "response": "def get_association(id, **kwargs):\n    \"\"\"\n    Fetch an association object by ID\n    \"\"\"\n    results = search_associations(id=id, **kwargs)\n    assoc = results['associations'][0] if len(results['associations']) > 0 else {}\n    return assoc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_associations(**kwargs):\n\n    \"\"\"\n    Fetch a set of association objects based on a query.\n    \"\"\"\n    logging.info(\"CREATING_GOLR_QUERY {}\".format(kwargs))\n    q = GolrAssociationQuery(**kwargs)\n    return q.exec()", "response": "Returns a set of association objects based on a query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_objects_for_subject(subject=None,\n                            object_category=None,\n                            relation=None,\n                            **kwargs):\n    \"\"\"\n    Convenience method: Given a subject (e.g. gene, disease, variant), return all associated objects (phenotypes, functions, interacting genes, etc)\n    \"\"\"\n    searchresult = search_associations(subject=subject,\n                                       fetch_objects=True,\n                                       rows=0,\n                                       object_category=object_category,\n                                       relation=relation,\n                                       **kwargs\n    )\n    objs = searchresult['objects']\n    return objs", "response": "Returns all objects associated with a given subject."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_subjects_for_object(object=None,\n                            subject_category=None,\n                            subject_taxon=None,\n                            relation=None,\n                            **kwargs):\n    \"\"\"\n    Convenience method: Given a object (e.g. ontology term like phenotype or GO; interacting gene; disease; pathway etc), return all associated subjects (genes, variants, pubs, etc)\n    \"\"\"\n    searchresult = search_associations(object=object,\n                                       fetch_subjects=True,\n                                       rows=0,\n                                       subject_category=subject_category,\n                                       subject_taxon=subject_taxon,\n                                       relation=relation,\n                                       **kwargs\n    )\n    subjs = searchresult['subjects']\n    return subjs", "response": "Returns a list of subjects associated with an object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map2slim(subjects, slim, **kwargs):\n    logging.info(\"SLIM SUBJECTS:{} SLIM:{} CAT:{}\".format(subjects,slim,kwargs.get('category')))\n    searchresult = search_associations(subjects=subjects,\n                                       slim=slim,\n                                       facet_fields=[],\n                                       **kwargs\n    )\n    pmap = {}\n    for a in searchresult['associations']:\n        subj = a['subject']['id']\n        slimmed_terms = a['slim']\n        #logging.info('SLIM: {} {}'.format(subj,slimmed_terms))\n        for t in slimmed_terms:\n            k = (subj,t)\n            if k not in pmap:\n                pmap[k] = []\n            pmap[k].append(a)\n    results = [ {'subject': subj, 'slim':t, 'assocs': assocs} for ((subj,t),assocs) in pmap.items()]\n    return results", "response": "Maps a set of subjects to a set of slims"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbulk fetch of associations for a species and pair of categories in a single object.", "response": "def bulk_fetch(subject_category, object_category, taxon, rows=MAX_ROWS, **kwargs):\n    \"\"\"\n    Fetch associations for a species and pair of categories in bulk.\n\n    Arguments:\n\n     - subject_category: String (not None)\n     - object_category: String (not None)\n     - taxon: String\n     - rows: int\n\n    Additionally, any argument for search_associations can be passed\n    \"\"\"\n    assert subject_category is not None\n    assert object_category is not None\n    time.sleep(1)\n    logging.info(\"Bulk query: {} {} {}\".format(subject_category, object_category, taxon))\n    assocs = search_associations_compact(subject_category=subject_category,\n                                         object_category=object_category,\n                                         subject_taxon=taxon,\n                                         rows=rows,\n                                         iterate=True,\n                                         **kwargs)\n    logging.info(\"Rows retrieved: {}\".format(len(assocs)))\n    if len(assocs) == 0:\n        logging.error(\"No associations returned for query: {} {} {}\".format(subject_category, object_category, taxon))\n    return assocs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the pivot table for the given facet.", "response": "def pivot_query(facet=None, facet_pivot_fields=None, **kwargs):\n    \"\"\"\n    Pivot query\n    \"\"\"\n    if facet_pivot_fields is None:\n        facet_pivot_fields = []\n    results = search_associations(rows=0,\n                                  facet_fields=[facet],\n                                  #facet_pivot_fields=facet_pivot_fields + [facet],\n                                  facet_pivot_fields=facet_pivot_fields,\n                                  **kwargs)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pivot_query_as_matrix(facet=None, facet_pivot_fields=None, **kwargs):\n    if facet_pivot_fields is None:\n        facet_pivot_fields = []\n    logging.info(\"Additional args: {}\".format(kwargs))\n    fp = search_associations(rows=0,\n                             facet_fields=[facet],\n                             facet_pivot_fields=facet_pivot_fields,\n                             **kwargs)['facet_pivot']\n\n    # we assume only one\n    results = list(fp.items())[0][1]\n    tups = []\n    xtype=None\n    ytype=None\n    xlabels=set()\n    ylabels=set()\n\n    \n    for r in results:\n        logging.info(\"R={}\".format(r))\n        xtype=r['field']\n        rv = r['value']\n        xlabels.add(rv)\n        for piv in r['pivot']:\n            ytype=piv['field']\n            pv = piv['value']\n            ylabels.add(pv)\n            tups.append( (rv,pv,piv['count']) )\n\n    z = [ [0] * len(xlabels) for i1 in range(len(ylabels)) ]\n\n    xlabels=list(xlabels)\n    ylabels=list(ylabels)\n    xmap = dict([x[::-1] for x in enumerate(xlabels)])\n    ymap = dict([x[::-1] for x in enumerate(ylabels)])\n    for t in tups:\n        z[ymap[t[1]]][xmap[t[0]]] = t[2]\n    m = {'xtype':xtype,\n         'ytype':ytype,\n         'xaxis':xlabels,\n         'yaxis':ylabels,\n         'z':z}\n    return m", "response": "Returns a matrix of the pivot query results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_associations_go(\n        subject_category=None,\n        object_category=None,\n        relation=None,\n        subject=None,\n        **kwargs):\n    \"\"\"\n    Perform association search using Monarch golr\n    \"\"\"\n    go_golr_url = \"http://golr.geneontology.org/solr/\"\n    go_solr = pysolr.Solr(go_golr_url, timeout=5)\n    go_solr.get_session().headers['User-Agent'] = get_user_agent(caller_name=__name__)\n    return search_associations(subject_category,\n                               object_category,\n                               relation,\n                               subject,\n                               solr=go_solr,\n                               field_mapping=goassoc_fieldmap(),\n                               **kwargs)", "response": "Perform association search using Monarch golr"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef select_distinct(distinct_field=None, **kwargs):\n    results = search_associations(rows=0,\n                                  select_fields=[],\n                                  facet_field_limits = {\n                                      distinct_field : -1\n                                  },\n                                  facet_fields=[distinct_field],\n                                  **kwargs\n    )\n    # TODO: map field\n    return list(results['facet_counts'][distinct_field].keys())", "response": "select distinct values for a given field"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pw_score_jaccard(self, s1 : ClassId, s2 : ClassId) -> SimScore:\n        am = self.assocmodel\n        a1 = am.inferred_types(s1)\n        a2 = am.inferred_types(s2)\n        num_union = len(a1 | a2)\n        if num_union == 0:\n            return 0.0\n        return len(a1 & a2) / num_union", "response": "Calculate the jaccard index of inferred associations of two subjects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pw_score_cosine(self, s1 : ClassId, s2 : ClassId) -> SimScore:\n        df = self.assoc_df\n        slice1 = df.loc[s1].values\n        slice2 = df.loc[s2].values\n        return 1 - cosine(slice1, slice2)", "response": "Returns the cosine similarity of two subjects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_all_information_content(self) -> pd.Series:\n        logging.info(\"Calculating all class ICs\")\n        df = self.assoc_df\n        freqs = df.sum(axis=0)\n        n_subjects, _ = df.shape\n        ics = freqs.apply(lambda x: -math.log(x / n_subjects)/math.log(2))\n        self.ics = ics\n        logging.info(\"DONE calculating all class ICs\")\n        return ics", "response": "Calculates the Information Content value of every class in the ICmap cache and returns an array \n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the MRCA for a class pair c1 and c2.", "response": "def calculate_mrcas(self, c1 : ClassId, c2 : ClassId) -> Set[ClassId]:\n        \"\"\"\n        Calculate the MRCA for a class pair\n        \"\"\"\n        G = self.G\n        # reflexive ancestors\n        ancs1 = self._ancestors(c1) | {c1}\n        ancs2 = self._ancestors(c2) | {c2}\n        common_ancestors = ancs1 & ancs2\n        redundant = set()\n        for a in common_ancestors:\n            redundant = redundant | nx.ancestors(G, a)\n        return common_ancestors - redundant"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the MICA of every class - pair containing all the class - pairs of the current class.", "response": "def calculate_all_micas(self):\n        \"\"\"\n        Calculate the MICA (Most Informative Common Ancestor) of every class-pair\n        \"\"\"\n        G = self.G\n        ics = self._information_content_frame()\n        classes = list(dfs.dfs_preorder_nodes(G))\n        #mica_df = pd.DataFrame(index=classes, columns=classes)\n        #mica_ic_df = pd.DataFrame(index=classes, columns=classes)\n        ncs = len(classes)\n        ic_grid = np.empty([ncs,ncs])\n        mica_arrs = []\n        logging.info('Calculating ICs for {} x {} classes'.format(ncs, ncs))\n        for c1i in range(0,ncs):\n            c1 = classes[c1i]\n            logging.debug('Calculating ICs for {}'.format(c1))\n            ancs1r = self._ancestors(c1) | {c1}\n            c2i = 0\n            mica_arr = []            \n            for c2i in range(0,ncs):\n                c2 = classes[c2i]\n                # TODO: optimize; matrix is symmetrical\n                #if c1i > c2i:\n                #    continue\n                ancs2r = self._ancestors(c2) | {c2}\n                common_ancs = ancs1r & ancs2r\n                if len(common_ancs) > 0:\n                    ic_cas = ics.loc[common_ancs]\n                    max_ic = float(ic_cas.max())\n                    ic_micas = ic_cas[ic_cas >= max_ic]\n                    micas = set(ic_micas.index)\n                    mica_arr.append(micas)\n                    #mica_grid[c1i, c2i] = micas\n                    #mica_grid[c2i, c1i] = micas\n                    ic_grid[c1i, c2i] = max_ic\n                    #ic_grid[c2i, c1i] = max_ic\n                else:\n                    ic_grid[c1i, c2i] = 0\n                    mica_arr.append(set())\n                    #ic_grid[c2i, c1i] = 0\n                    # TODO: consider optimization step; blank out all anc pairs\n            mica_arrs.append(mica_arr)\n        logging.info('DONE Calculating ICs for {} x {} classes'.format(ncs, ncs))\n        #self.mica_df = pd.DataFrame(mica_grid, index=classes, columns=classes)\n        self.mica_df = pd.DataFrame(mica_arrs, index=classes, columns=classes)\n        self.mica_ic_df = pd.DataFrame(ic_grid, index=classes, columns=classes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomparing two class profiles.", "response": "def pw_compare_class_sets(self, cset1: Set[ClassId], cset2: Set[ClassId]) -> Tuple[ICValue, ICValue, ICValue]:\n        \"\"\"\n        Compare two class profiles\n        \"\"\"\n        pairs = self.mica_ic_df.loc[cset1, cset2]\n        max0 = pairs.max(axis=0)\n        max1 = pairs.max(axis=1)\n        idxmax0 = pairs.idxmax(axis=0)\n        idxmax1 = pairs.idxmax(axis=1)\n        mean0 = max0.mean()\n        mean1 = max1.mean()\n        return (mean0+mean1)/2, mean0, mean1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for a set of class ids in the MICIC table.", "response": "def search(self,\n               id_list: Set,\n               negated_classes: Set,\n               method: Optional) -> List[SimResult]:\n        \"\"\"\n        def search(self, cset: Set[ClassId]):\n            slice = self.mica_ic_df.loc[cset]\n            am = self.assocmodel\n            for i in am.subjects:\n                pass # TODO\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomparing two lists of entites and return their similarity", "response": "def compare(self,\n                query_classes: Set,\n                reference_classes: Set,\n                method: Optional) -> SimResult:\n        \"\"\"\n        Given two lists of entites (classes, individual)\n        return their similarity\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_file(self,filename=None, format=None):\n        rdfgraph = rdflib.Graph()\n        if format is None:\n            if filename.endswith(\".ttl\"):\n                format='turtle'\n            elif filename.endswith(\".rdf\"):\n                format='xml'\n        rdfgraph.parse(filename, format=format)\n        return self.process_rdfgraph(rdfgraph)", "response": "Parse a file into an ontology object using rdflib"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_rdfgraph(self, rg, ont=None):\n        # TODO: ontology metadata\n        if ont is None:\n            ont = Ontology()\n            subjs = list(rg.subjects(RDF.type, SKOS.ConceptScheme))\n            if len(subjs) == 0:\n                logging.warning(\"No ConceptScheme\")\n            else:\n                ont.id = self._uri2id(subjs[0])\n            \n        subset_map = {}\n        for concept in rg.subjects(RDF.type, SKOS.Concept):\n            for s in self._get_schemes(rg, concept):\n                subset_map[self._uri2id(s)] = s\n                \n        for concept in sorted(list(rg.subjects(RDF.type, SKOS.Concept))):\n            concept_uri = str(concept)\n            id=self._uri2id(concept)\n            logging.info(\"ADDING: {}\".format(id))\n            ont.add_node(id, self._get_label(rg,concept))\n                    \n            for defn in rg.objects(concept, SKOS.definition):\n                if (defn.language == self.lang):\n                    td = TextDefinition(id, escape_value(defn.value))\n                    ont.add_text_definition(td)\n                    \n            for s in rg.objects(concept, SKOS.broader):\n                ont.add_parent(id, self._uri2id(s))\n                \n            for s in rg.objects(concept, SKOS.related):\n                ont.add_parent(id, self._uri2id(s), self._uri2id(SKOS.related))\n                \n            for m in rg.objects(concept, SKOS.exactMatch):\n                ont.add_xref(id, self._uri2id(m))\n                \n            for m in rg.objects(concept, SKOS.altLabel):\n                syn = Synonym(id, val=self._uri2id(m))\n                ont.add_synonym(syn)\n                \n            for s in self._get_schemes(rg,concept):\n                ont.add_to_subset(id, self._uri2id(s))\n                \n        return ont", "response": "Transform a skos terminology expressed in an rdflib. Graph into an Ontology object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching the owlsim server for individuals with the given attribute set.", "response": "def search_by_attribute_set(\n        url: str,\n        profile: Tuple[str],\n        limit: Optional[int] = 100,\n        namespace_filter: Optional[str]=None) -> Dict:\n    \"\"\"\n    Given a list of phenotypes, returns a ranked list of individuals\n    individuals can be filtered by namespace, eg MONDO, MGI, HGNC\n    :returns Dict with the structure: {\n        'unresolved' : [...]\n        'query_IRIs' : [...]\n        'results': {...}\n    }\n    :raises JSONDecodeError: If the response body does not contain valid json.\n    \"\"\"\n    owlsim_url = url + 'searchByAttributeSet'\n\n    params = {\n        'a': profile,\n        'limit': limit,\n        'target': namespace_filter\n    }\n    return requests.get(owlsim_url, params=params, timeout=TIMEOUT).json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving two phenotype profiles returns their similarity", "response": "def compare_attribute_sets(\n        url: str,\n        profile_a: Tuple[str],\n        profile_b: Tuple[str]) -> Dict:\n    \"\"\"\n    Given two phenotype profiles, returns their similarity\n    :returns Dict with the structure: {\n        'unresolved' : [...]\n        'query_IRIs' : [...]\n        'target_IRIs': [...]\n        'results': {...}\n    }\n    \"\"\"\n    owlsim_url = url + 'compareAttributeSets'\n\n    params = {\n        'a': profile_a,\n        'b': profile_b,\n    }\n\n    return requests.get(owlsim_url, params=params, timeout=TIMEOUT).json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the information content for a list of phenotypes and the annotation sufficiency simple and categorical scores", "response": "def get_attribute_information_profile(\n        url: str,\n        profile: Optional[Tuple[str]]=None,\n        categories: Optional[Tuple[str]]=None) -> Dict:\n    \"\"\"\n    Get the information content for a list of phenotypes\n    and the annotation sufficiency simple and\n    and categorical scores if categories are provied\n\n    Ref: https://zenodo.org/record/834091#.W8ZnCxhlCV4\n    Note that the simple score varies slightly from the pub in that\n    it uses max_max_ic instead of mean_max_ic\n\n    If no arguments are passed this function returns the\n    system (loaded cohort) stats\n    :raises JSONDecodeError: If the response body does not contain valid json.\n    \"\"\"\n    owlsim_url = url + 'getAttributeInformationProfile'\n\n    params = {\n        'a': profile,\n        'r': categories\n    }\n    return requests.get(owlsim_url, params=params, timeout=TIMEOUT).json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_owlsim_stats(url) -> Tuple[IcStatistic, Dict[str, IcStatistic]]:\n    scigraph = OntologyFactory().create('scigraph:ontology')\n    category_stats = {}\n    categories = [enum.value for enum in HpoUpperLevel]\n    sim_response = get_attribute_information_profile(url, categories=tuple(categories))\n\n    try:\n        global_stats = IcStatistic(\n            mean_mean_ic=float(sim_response['system_stats']['meanMeanIC']),\n            mean_sum_ic=float(sim_response['system_stats']['meanSumIC']),\n            mean_cls=float(sim_response['system_stats']['meanN']),\n            max_max_ic=float(sim_response['system_stats']['maxMaxIC']),\n            max_sum_ic=float(sim_response['system_stats']['maxSumIC']),\n            individual_count=int(sim_response['system_stats']['individuals']),\n            mean_max_ic=float(sim_response['system_stats']['meanMaxIC'])\n        )\n        for cat_stat in sim_response['categorical_scores']:\n            category_stats[cat_stat['id']] = IcStatistic(\n                mean_mean_ic=float(cat_stat['system_stats']['meanMeanIC']),\n                mean_sum_ic=float(cat_stat['system_stats']['meanSumIC']),\n                mean_cls=float(cat_stat['system_stats']['meanN']),\n                max_max_ic=float(cat_stat['system_stats']['maxMaxIC']),\n                max_sum_ic=float(cat_stat['system_stats']['maxSumIC']),\n                individual_count=int(cat_stat['system_stats']['individuals']),\n                mean_max_ic=float(cat_stat['system_stats']['meanMaxIC']),\n                descendants=scigraph.descendants(cat_stat['id'], relations=[\"subClassOf\"])\n            )\n\n    except JSONDecodeError as json_exc:\n        raise JSONDecodeError(\n            \"Cannot parse owlsim2 response: {}\".format(json_exc.msg),\n            json_exc.doc,\n            json_exc.pos\n        )\n\n    return global_stats, category_stats", "response": "Returns a tuple of IcStatistic and a dictionary of all the information about the ontology."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(\n            self,\n            id_list: List,\n            negated_classes: List,\n            limit: Optional[int] = 100,\n            method: Optional[SimAlgorithm] = SimAlgorithm.PHENODIGM) -> SimResult:\n        \"\"\"\n        Owlsim2 search, calls search_by_attribute_set, and converts to SimResult object\n\n        :raises JSONDecodeError: If the owlsim response is not valid json.\n        \"\"\"\n\n        return self.filtered_search(\n            id_list=id_list,\n            negated_classes=negated_classes,\n            limit=limit,\n            taxon_filter=None,\n            category_filter=None,\n            method=method\n        )", "response": "This method calls search_by_attribute_set and converts to SimResult object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filtered_search(\n            self,\n            id_list: List,\n            negated_classes: List,\n            limit: Optional[int] = 100,\n            taxon_filter: Optional[int] = None,\n            category_filter: Optional[str] = None,\n            method: Optional[SimAlgorithm] = SimAlgorithm.PHENODIGM) -> SimResult:\n        \"\"\"\n        Owlsim2 filtered search, resolves taxon and category to a namespace,\n        calls search_by_attribute_set, and converts to SimResult object\n        \"\"\"\n        if len(negated_classes) > 0:\n            logging.warning(\"Owlsim2 does not support negation, ignoring neg classes\")\n\n        namespace_filter = self._get_namespace_filter(taxon_filter, category_filter)\n        owlsim_results = search_by_attribute_set(self.url, tuple(id_list), limit, namespace_filter)\n        return self._simsearch_to_simresult(owlsim_results, method)", "response": "This method uses the owlsim2 search method to search the Owlsim2 API and converts the results to a SimResult object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare(self,\n                reference_classes: List,\n                query_classes: List,\n                method: Optional[SimAlgorithm] = SimAlgorithm.PHENODIGM) -> SimResult:\n        \"\"\"\n        Owlsim2 compare, calls compare_attribute_sets, and converts to SimResult object\n        :return: SimResult object\n        \"\"\"\n        owlsim_results = compare_attribute_sets(self.url, tuple(reference_classes), tuple(query_classes))\n        return self._simcompare_to_simresult(owlsim_results, method)", "response": "Compares two list of classes and returns SimResult object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef matchers() -> List[SimAlgorithm]:\n        return [\n            SimAlgorithm.PHENODIGM,\n            SimAlgorithm.JACCARD,\n            SimAlgorithm.SIM_GIC,\n            SimAlgorithm.RESNIK,\n            SimAlgorithm.SYMMETRIC_RESNIK\n        ]", "response": "Returns a list of all the matchers in owlsim2\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of individuals return their information content", "response": "def get_profile_ic(self, profile: List) -> Dict:\n        \"\"\"\n        Given a list of individuals, return their information content\n        \"\"\"\n        sim_response = get_attribute_information_profile(self.url, tuple(profile))\n\n        profile_ic = {}\n        try:\n            for cls in sim_response['input']:\n                profile_ic[cls['id']] = cls['IC']\n        except JSONDecodeError as json_exc:\n            raise JSONDecodeError(\n                \"Cannot parse owlsim2 response: {}\".format(json_exc.msg),\n                json_exc.doc,\n                json_exc.pos\n            )\n\n        return profile_ic"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _simsearch_to_simresult(self, sim_resp: Dict, method: SimAlgorithm) -> SimResult:\n\n        sim_ids = get_nodes_from_ids(sim_resp['query_IRIs'])\n        sim_resp['results'] = OwlSim2Api._rank_results(sim_resp['results'], method)\n\n        # get id type map:\n        ids = [result['j']['id'] for result in sim_resp['results']]\n        id_type_map = get_id_type_map(ids)\n\n        matches = []\n\n        for result in sim_resp['results']:\n            matches.append(\n                SimMatch(\n                    id=result['j']['id'],\n                    label=result['j']['label'],\n                    rank=result['rank'],\n                    score=result[OwlSim2Api.method2key[method]],\n                    type=id_type_map[result['j']['id']][0],\n                    taxon=get_taxon(result['j']['id']),\n                    significance=\"NaN\",\n                    pairwise_match=OwlSim2Api._make_pairwise_matches(result)\n                )\n            )\n\n        return SimResult(\n            query=SimQuery(\n                ids=sim_ids,\n                unresolved_ids=sim_resp['unresolved'],\n                target_ids=[[]]\n            ),\n            matches=matches,\n            metadata=SimMetadata(\n                max_max_ic=self.statistics.max_max_ic\n            )\n        )", "response": "Convert owlsim json to SimResult object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_pairwise_matches(result: Dict) -> List[PairwiseMatch]:\n        pairwise_matches = []\n        for pairwise_match in result['matches']:\n            pairwise_matches.append(\n                PairwiseMatch(\n                    reference=ICNode(**pairwise_match['a']),\n                    match=ICNode(**pairwise_match['b']),\n                    lcs=ICNode(**pairwise_match['lcs'])\n                )\n            )\n\n        return pairwise_matches", "response": "Make a list of PairwiseMatch objects from owlsim results."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rank_results(results: List[Dict], method: SimAlgorithm) -> List[Dict]:\n        # https://stackoverflow.com/a/73050\n        sorted_results = sorted(\n            results, reverse=True, key=lambda k: k[OwlSim2Api.method2key[method]]\n        )\n        if len(sorted_results) > 0:\n            rank = 1\n            previous_score = sorted_results[0][OwlSim2Api.method2key[method]]\n            for result in sorted_results:\n                if previous_score > result[OwlSim2Api.method2key[method]]:\n                    rank += 1\n                result['rank'] = rank\n                previous_score = result[OwlSim2Api.method2key[method]]\n\n        return sorted_results", "response": "Rank results based on method"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives either a taxon and a category filter return the correct namespace_filter", "response": "def _get_namespace_filter(\n            taxon_filter: Optional[int]=None,\n            category_filter: Optional[str]=None) -> Union[None, str]:\n        \"\"\"\n        Given either a taxon and/or category, return the correct namespace\n        :raises ValueError: If category is provided without a taxon\n        \"\"\"\n        namespace_filter = None\n        taxon_category_default = {\n            10090: 'gene',\n            9606:  'disease',\n            7227:  'gene',\n            6239:  'gene',\n            7955:  'gene'\n        }\n        if category_filter is not None and taxon_filter is None:\n            raise ValueError(\"Must provide taxon filter along with category\")\n        elif category_filter is None and taxon_filter is not None:\n            category_filter = taxon_category_default[taxon_filter]\n        else:\n            return namespace_filter\n\n        return OwlSim2Api.TAX_TO_NS[taxon_filter][category_filter.lower()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntranslates solr facet_fields results into something easier to manipulate", "response": "def translate_facet_field(fcs, invert_subject_object = False):\n    \"\"\"\n    Translates solr facet_fields results into something easier to manipulate\n\n    A solr facet field looks like this: [field1, count1, field2, count2, ..., fieldN, countN]\n\n    We translate this to a dict {f1: c1, ..., fn: cn}\n\n    This has slightly higher overhead for sending over the wire, but is easier to use\n    \"\"\"\n    if 'facet_fields' not in fcs:\n        return {}\n    ffs = fcs['facet_fields']\n    rs={}\n    for (facet, facetresults) in ffs.items():\n        if invert_subject_object:\n            for (k,v) in INVERT_FIELDS_MAP.items():\n                if facet == k:\n                    facet = v\n                    break\n                elif facet == v:\n                    facet = k\n                    break\n\n        pairs = {}\n        rs[facet] = pairs\n        for i in range(int(len(facetresults)/2)):\n            (fv,fc) = (facetresults[i*2],facetresults[i*2+1])\n            pairs[fv] = fc\n    return rs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a mapping of canonical monarch fields to amigo - golr.", "response": "def goassoc_fieldmap(relationship_type=ACTS_UPSTREAM_OF_OR_WITHIN):\n    \"\"\"\n    Returns a mapping of canonical monarch fields to amigo-golr.\n\n    See: https://github.com/geneontology/amigo/blob/master/metadata/ann-config.yaml\n\n    \"\"\"\n    return {\n        M.SUBJECT: 'bioentity',\n        M.SUBJECT_CLOSURE: 'bioentity',\n        ## In the GO AmiGO instance, the type field is not correctly populated\n        ## See above in the code for hack that restores this for planteome instance\n        ## M.SUBJECT_CATEGORY: 'type',\n        M.SUBJECT_CATEGORY: None,\n        M.SUBJECT_LABEL: 'bioentity_label',\n        M.SUBJECT_TAXON: 'taxon',\n        M.SUBJECT_TAXON_LABEL: 'taxon_label',\n        M.SUBJECT_TAXON_CLOSURE: 'taxon_closure',\n        M.RELATION: 'qualifier',\n        M.OBJECT: 'annotation_class',\n        M.OBJECT_CLOSURE: REGULATES_CLOSURE if relationship_type == ACTS_UPSTREAM_OF_OR_WITHIN else ISA_PARTOF_CLOSURE,\n        M.OBJECT_LABEL: 'annotation_class_label',\n        M.OBJECT_TAXON: 'object_taxon',\n        M.OBJECT_TAXON_LABEL: 'object_taxon_label',\n        M.OBJECT_TAXON_CLOSURE: 'object_taxon_closure',\n        M.OBJECT_CATEGORY: None,\n        M.EVIDENCE_OBJECT_CLOSURE: 'evidence_subset_closure',\n        M.IS_DEFINED_BY: 'assigned_by'\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmaps a field name given a mapping file. Returns input if fieldname is unmapped.", "response": "def map_field(fn, m) :\n    \"\"\"\n    Maps a field name, given a mapping file.\n    Returns input if fieldname is unmapped.\n    \"\"\"\n    if m is None:\n        return fn\n    if fn in m:\n        return m[fn]\n    else:\n        return fn"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a solr query on a given solrInstance category and id and fields", "response": "def run_solr_on(solrInstance, category, id, fields):\n    \"\"\"\n    Return the result of a solr query on the given solrInstance (Enum ESOLR), for a certain document_category (ESOLRDoc) and id\n    \"\"\"\n    query = solrInstance.value + \"select?q=*:*&fq=document_category:\\\"\" + category.value + \"\\\"&fq=id:\\\"\" + id + \"\\\"&fl=\" + fields + \"&wt=json&indent=on\"\n    response = requests.get(query)\n    return response.json()['response']['docs'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_solr_text_on(solrInstance, category, q, qf, fields, optionals):\n    if optionals == None:\n        optionals = \"\"\n    query = solrInstance.value + \"select?q=\" + q + \"&qf=\" + qf + \"&fq=document_category:\\\"\" + category.value + \"\\\"&fl=\" + fields + \"&wt=json&indent=on\" + optionals\n    # print(\"QUERY: \", query)\n\n    response = requests.get(query)\n    return response.json()['response']['docs']", "response": "Run a solr query on a given solrInstance and category and return the result of a solr query on the given solrInstance and id\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(json, firstField, secondField):\n    merged = []\n\n    for i in range(0, len(json[firstField])):\n        merged.append({ json[firstField][i] : json[secondField][i] })\n    return merged", "response": "merge two fields of a json into an array of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges two fields of a json into a single array of dicts", "response": "def mergeWithLabels(json, firstField, firstFieldLabel, secondField, secondFieldLabel):\n    \"\"\"\n    merge two fields of a json into an array of { firstFieldLabel : firstFieldLabel, secondFieldLabel : secondField }\n    \"\"\"\n    merged = []\n\n    for i in range(0, len(json[firstField])):\n        merged.append({ firstFieldLabel : json[firstField][i],\n                        secondFieldLabel : json[secondField][i] })\n    return merged"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search(self):\n        params = self.solr_params()\n        logging.info(\"PARAMS=\" + str(params))\n        results = self.solr.search(**params)\n        logging.info(\"Docs found: {}\".format(results.hits))\n        return self._process_search_results(results)", "response": "Execute solr search query"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_search_results(self,\n                                results: pysolr.Results) -> SearchResults:\n        \"\"\"\n        Convert solr docs to biolink object\n\n        :param results: pysolr.Results\n        :return: model.GolrResults.SearchResults\n        \"\"\"\n\n        # map go-golr fields to standard\n        for doc in results.docs:\n            if 'entity' in doc:\n                doc['id'] = doc['entity']\n                doc['label'] = doc['entity_label']\n\n        highlighting = {\n            doc['id']: self._process_highlight(results, doc)._asdict()\n            for doc in results.docs if results.highlighting\n        }\n        payload = SearchResults(\n            facet_counts=translate_facet_field(results.facets),\n            highlighting=highlighting,\n            docs=results.docs,\n            numFound=results.hits\n        )\n        logging.debug('Docs: {}'.format(len(results.docs)))\n\n        return payload", "response": "Convert solr docs to biolink object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_autocomplete_results(\n            self,\n            results: pysolr.Results) -> Dict[str, List[AutocompleteResult]]:\n        \"\"\"\n        Convert results to biolink autocomplete object\n        :param results: pysolr.Results\n        :return: {'docs': List[AutocompleteResult]}\n        \"\"\"\n        # map go-golr fields to standard\n        for doc in results.docs:\n            if 'entity' in doc:\n                doc['id'] = doc['entity']\n                doc['label'] = doc['entity_label']\n\n        docs = []\n        for doc in results.docs:\n            if results.highlighting:\n                hl = self._process_highlight(results, doc)\n            else:\n                hl = Highlight(None, None, None)\n\n            # In some cases a node does not have a category\n            category = doc['category'] if 'category' in doc else []\n\n            doc['taxon'] = doc['taxon'] if 'taxon' in doc else \"\"\n            doc['taxon_label'] = doc['taxon_label'] if 'taxon_label' in doc else \"\"\n            doc = AutocompleteResult(\n                id=doc['id'],\n                label=doc['label'],\n                match=hl.match,\n                category=category,\n                taxon=doc['taxon'],\n                taxon_label=doc['taxon_label'],\n                highlight=hl.highlight,\n                has_highlight=hl.has_highlight\n            )\n            docs.append(doc)\n\n        payload = {\n            'docs': docs\n        }\n        logging.debug('Docs: {}'.format(len(results.docs)))\n\n        return payload", "response": "Convert results to biolink autocomplete object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_longest_hl(self, highlights):\n        len_dict = OrderedDict()\n        for hl in highlights:\n            # dummy tags to make it valid xml\n            dummy_xml = \"<p>\" + hl + \"</p>\"\n            try:\n                element_tree = ET.fromstring(dummy_xml)\n                hl_length = 0\n                for emph in element_tree.findall('em'):\n                    hl_length += len(emph.text)\n                len_dict[hl] = hl_length\n            except ET.ParseError:\n                raise ET.ParseError\n\n        return max(len_dict, key=len_dict.get)", "response": "Returns the longest highlight in the list of highlighted text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _hl_as_string(self, highlight):\n        # dummy tags to make it valid xml\n        dummy_xml = \"<p>\" + highlight + \"</p>\"\n        try:\n            element_tree = ET.fromstring(dummy_xml)\n        except ET.ParseError:\n            raise ET.ParseError\n        return \"\".join(list(element_tree.itertext()))", "response": "Given a solr string of highlighted text returns the\n        str representations\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting solr query for autocomplete", "response": "def autocomplete(self):\n        \"\"\"\n        Execute solr query for autocomplete\n        \"\"\"\n        params = self.set_lay_params()\n        logging.info(\"PARAMS=\"+str(params))\n        results = self.solr.search(**params)\n        logging.info(\"Docs found: {}\".format(results.hits))\n        return self._process_layperson_results(results)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_layperson_results(self, results):\n\n        payload = {\n            'results': []\n        }\n\n        for doc in results.docs:\n            hl = self._process_highlight(results, doc)\n            highlight = {\n                'id': doc['id'],\n                'highlight': hl.highlight,\n                'label': doc['label'],\n                'matched_synonym': hl.match\n            }\n            payload['results'].append(highlight)\n\n        logging.debug('Docs: {}'.format(len(results.docs)))\n\n        return payload", "response": "Convert pysolr. Results to biolink object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solr_params(self):\n\n        ## Main query params for solr\n        fq=self.fq\n        if fq is None:\n            fq = {}\n        logging.info(\"TEMPx FQ={}\".format(fq))\n\n        # subject_or_object_ids is a list of identifiers that can be matched to either subjects or objects\n        subject_or_object_ids = self.subject_or_object_ids\n        if subject_or_object_ids is not None:\n            subject_or_object_ids = [self.make_canonical_identifier(c) for c in subject_or_object_ids]\n\n        # canonical form for MGI is a CURIE MGI:nnnn\n        #if subject is not None and subject.startswith('MGI:MGI:'):\n        #    logging.info('Unhacking MGI ID presumably from GO:'+str(subject))\n        #    subject = subject.replace(\"MGI:MGI:\",\"MGI\")\n        subject=self.subject\n        if subject is not None:\n            subject = self.make_canonical_identifier(subject)\n        subjects=self.subjects\n        if subjects is not None:\n            subjects = [self.make_canonical_identifier(s) for s in subjects]\n\n        # temporary: for querying go solr, map fields. TODO\n        object_category=self.object_category\n        logging.info(\"Object category: {}\".format(object_category))\n\n        object=self.object\n        if object_category is None and object is not None and object.startswith('GO:'):\n            # Infer category\n            object_category = 'function'\n            logging.info(\"Inferring Object category: {} from {}\".\n                         format(object_category, object))\n\n        solr_config = {'url': self.url, 'timeout': 5}\n        if self.solr is None:\n            if self.url is None:\n                endpoint = self.get_config().solr_assocs\n                solr_config = {'url': endpoint.url, 'timeout': endpoint.timeout}\n            else:\n                solr_config = {'url': self.url, 'timeout': 5}\n\n        # URL to use for querying solr\n        if self._use_amigo_schema(object_category):\n            if self.url is None:\n                endpoint = self.get_config().amigo_solr_assocs\n                solr_config = {'url': endpoint.url, 'timeout': endpoint.timeout}\n            self.field_mapping=goassoc_fieldmap(self.relationship_type)\n\n            # awkward hack: we want to avoid typing on the amigo golr gene field,\n            # UNLESS this is a planteome golr\n            if \"planteome\" in self.get_config().amigo_solr_assocs.url:\n                self.field_mapping[M.SUBJECT_CATEGORY] = 'type'\n\n            fq['document_category'] = 'annotation'\n            if subject is not None:\n                subject = self.make_gostyle_identifier(subject)\n            if subjects is not None:\n                subjects = [self.make_gostyle_identifier(s) for s in subjects]\n\n            # the AmiGO schema lacks an object_category field;\n            # we could use the 'aspect' field but instead we use a mapping of\n            # the category to a root class\n            if object_category is not None:\n                cc = self.get_config().get_category_class(object_category)\n                if cc is not None and object is None:\n                    object = cc\n\n        self.update_solr_url(**solr_config)\n\n        ## subject params\n        subject_taxon=self.subject_taxon\n        subject_category=self.subject_category\n\n        # heuristic procedure to guess unspecified subject_category\n        if subject_category is None and subject is not None:\n            subject_category = self.infer_category(subject)\n\n        if subject_category is not None and subject_category == 'disease':\n            if subject_taxon is not None and subject_taxon=='NCBITaxon:9606':\n                logging.info(\"Unsetting taxon, until indexed correctly\")\n                subject_taxon = None\n\n        if self.invert_subject_object is None:\n            # TODO: consider placing in a separate lookup\n            p = (subject_category, object_category)\n            if p == ('disease', 'gene'):\n                self.invert_subject_object = True\n            elif p == ('disease', 'model'):\n                self.invert_subject_object = True\n            else:\n                self.invert_subject_object = False\n            if self.invert_subject_object:\n                logging.info(\"Inferred that subject/object should be inverted for {}\".format(p))\n\n        ## taxon of object of triple\n        object_taxon=self.object_taxon\n\n        # typically information is stored one-way, e.g. model-disease;\n        # sometimes we want associations from perspective of object\n        if self.invert_subject_object:\n            (subject,object) = (object,subject)\n            (subject_category,object_category) = (object_category,subject_category)\n            (subject_taxon,object_taxon) = (object_taxon,subject_taxon)\n\n        ## facet fields\n        facet_fields=self.facet_fields\n        facet_on=self.facet_on\n        facet_limit=self.facet_limit\n        select_fields=self.select_fields\n\n        if self.use_compact_associations:\n            facet_fields = []\n            facet_on = 'off'\n            facet_limit = 0\n            select_fields = [\n                M.SUBJECT,\n                M.SUBJECT_LABEL,\n                M.RELATION,\n                M.OBJECT]\n\n        if subject_category is not None:\n            fq['subject_category'] = subject_category\n        if object_category is not None:\n            fq['object_category'] = object_category\n\n\n        if object is not None:\n            # TODO: make configurable whether to use closure\n            fq['object_closure'] = object\n        if subject is not None:\n            # note: by including subject closure by default,\n            # we automaticaly get equivalent nodes\n            if self.subject_direct:\n                fq['subject'] = subject\n            else:\n                fq['subject_closure'] = subject\n        if subjects is not None:\n            # lists are assumed to be disjunctive\n            if self.subject_direct:\n                fq['subject'] = subjects\n            else:\n                fq['subject_closure'] = subjects\n\n        objects=self.objects\n        if objects is not None:\n            # lists are assumed to be disjunctive\n            fq['object_closure'] = objects\n        relation=self.relation\n        if relation is not None:\n            fq['relation_closure'] = relation\n        if subject_taxon is not None:\n            fq['subject_taxon_closure'] = subject_taxon\n        if object_taxon is not None:\n            fq['object_taxon_closure'] = object_taxon\n        if self.id is not None:\n            fq['id'] = self.id\n        if self.evidence is not None:\n            e = self.evidence\n            if e.startswith(\"-\"):\n                fq['-evidence_object_closure'] = e.replace(\"-\",\"\")\n            else:\n                fq['evidence_object_closure'] = e\n\n        if self.exclude_automatic_assertions:\n            fq['-evidence_object_closure'] = 'ECO:0000501'\n\n        # Homolog service params\n        # TODO can we sync with argparse.choices?\n        if self.homology_type is not None:\n            if self.homology_type == 'O':\n                fq['relation_closure'] = HomologyTypes.Ortholog.value\n            elif self.homology_type == 'P':\n                fq['relation_closure'] = HomologyTypes.Paralog.value\n            elif self.homology_type == 'LDO':\n                fq['relation_closure'] = \\\n            HomologyTypes.LeastDivergedOrtholog.value\n\n\n        ## pivots\n        facet_pivot_fields=self.facet_pivot_fields\n        if self.pivot_subject_object:\n            facet_pivot_fields = [M.SUBJECT, M.OBJECT]\n\n\n        # Map solr field names for fq. The generic Monarch schema is\n        # canonical, GO schema is mapped to this using\n        # field_mapping dictionary\n        if self.field_mapping is not None:\n            for (k,v) in self.field_mapping.items():\n\n                # map fq[k] -> fq[k]\n                if k in fq:\n                    if v is None:\n                        del fq[k]\n                    else:\n                        fq[v] = fq[k]\n                        del fq[k]\n\n                # in solr, the fq field can be\n                # a negated expression, e.g. -evidence_object_closure:\"ECO:0000501\"\n                # ideally we would have a higher level representation rather than\n                # relying on string munging...\n                negk = '-' + k\n                if negk in fq:\n                    if v is None:\n                        del fq[negk]\n                    else:\n                        negv = '-' + v\n                        fq[negv] = fq[negk]\n                        del fq[negk]\n\n\n        filter_queries = []\n        qstr = \"*:*\"\n        if self.q is not None:\n            qstr = self.q\n        filter_queries = [ '{}:{}'.format(k,solr_quotify(v)) for (k,v) in fq.items()]\n\n        # We want to match all associations that have either a subject or object\n        # with an ID that is contained in subject_or_object_ids.\n        if subject_or_object_ids is not None:\n            quotified_ids = solr_quotify(subject_or_object_ids)\n            subject_id_filter = '{}:{}'.format('subject_closure', quotified_ids)\n            object_id_filter = '{}:{}'.format('object_closure', quotified_ids)\n\n            # If subject_or_object_category is provided, we add it to the filter.\n            if self.subject_or_object_category is not None:\n                quotified_categories = solr_quotify(self.subject_or_object_category)\n                subject_category_filter = '{}:{}'.format('subject_category', quotified_categories)\n                object_category_filter = '{}:{}'.format('object_category', quotified_categories)\n\n                filter_queries.append(\n                    '(' + subject_id_filter + ' AND ' + object_category_filter + ')' \\\n                    ' OR '                                                      \\\n                    '(' + object_id_filter + ' AND ' + subject_category_filter + ')'\n                )\n\n            else:\n                filter_queries.append(subject_id_filter + ' OR ' + object_id_filter)\n\n        # unless caller specifies a field list, use default\n        if select_fields is None:\n            select_fields = [\n                M.ID,\n                M.IS_DEFINED_BY,\n                M.SOURCE,\n                M.SUBJECT,\n                M.SUBJECT_LABEL,\n                M.SUBJECT_TAXON,\n                M.SUBJECT_TAXON_LABEL,\n                M.RELATION,\n                M.RELATION_LABEL,\n                M.OBJECT,\n                M.OBJECT_LABEL,\n                M.OBJECT_TAXON,\n                M.OBJECT_TAXON_LABEL\n            ]\n            if not self.unselect_evidence:\n                select_fields += [\n                    M.EVIDENCE_OBJECT,\n                    M.EVIDENCE_GRAPH\n                ]\n\n        if not self._use_amigo_schema(object_category):\n            select_fields.append(M.SUBJECT_CATEGORY)\n            select_fields.append(M.OBJECT_CATEGORY)\n\n        if self.map_identifiers is not None:\n            select_fields.append(M.SUBJECT_CLOSURE)\n\n        if self.slim is not None and len(self.slim)>0:\n            select_fields.append(M.OBJECT_CLOSURE)\n\n        if self.field_mapping is not None:\n            logging.info(\"Applying field mapping to SELECT: {}\".format(self.field_mapping))\n            select_fields = [ map_field(fn, self.field_mapping) for fn in select_fields ]\n            if facet_pivot_fields is not None:\n                logging.info(\"Applying field mapping to PIV: {}\".format(facet_pivot_fields))\n                facet_pivot_fields = [ map_field(fn, self.field_mapping) for fn in facet_pivot_fields ]\n                logging.info(\"APPLIED field mapping to PIV: {}\".format(facet_pivot_fields))\n\n        facet_fields = [ map_field(fn, self.field_mapping) for fn in facet_fields ]\n\n        if self._use_amigo_schema(object_category):\n            select_fields += [x for x in M.AMIGO_SPECIFIC_FIELDS if x not in select_fields]\n\n        ## true if iterate in windows of max_size until all results found\n        iterate=self.iterate\n\n        #logging.info('FL'+str(select_fields))\n        is_unlimited = False\n        rows=self.rows\n        if rows < 0:\n            is_unlimited = True\n            iterate = True\n            rows = self.max_rows\n\n        for field in self.non_null_fields:\n            filter_queries.append(field + \":['' TO *]\")\n\n        params = {\n            'q': qstr,\n            'fq': filter_queries,\n            'facet': facet_on,\n            'facet.field': facet_fields,\n            'facet.limit': facet_limit,\n            'facet.mincount': self.facet_mincount,\n            'fl': \",\".join(select_fields),\n            'rows': rows\n        }\n\n        if self.start is not None:\n            params['start'] = self.start\n\n        json_facet=self.json_facet\n        if json_facet:\n            params['json.facet'] = json.dumps(json_facet)\n\n        facet_field_limits=self.facet_field_limits\n        if facet_field_limits is not None:\n            for (f,flim) in facet_field_limits.items():\n                params[\"f.\"+f+\".facet.limit\"] = flim\n\n\n        if len(facet_pivot_fields) > 0:\n            params['facet.pivot'] = \",\".join(facet_pivot_fields)\n            params['facet.pivot.mincount'] = 1\n\n        if self.stats_field:\n            self.stats = True\n            params['stats.field'] = self.stats_field\n        params['stats'] = json.dumps(self.stats)\n\n        return params", "response": "Generate HTTP parameters for querying solr for a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute solr query Result object is a dict with the following keys: - raw - associations : list - compact_associations : list - facet_counts - facet_pivot", "response": "def exec(self, **kwargs):\n        \"\"\"\n        Execute solr query\n\n        Result object is a dict with the following keys:\n\n         - raw\n         - associations : list\n         - compact_associations : list\n         - facet_counts\n         - facet_pivot\n\n        \"\"\"\n\n        params = self.solr_params()\n        logging.info(\"PARAMS=\"+str(params))\n        results = self.solr.search(**params)\n        n_docs = len(results.docs)\n        logging.info(\"Docs found: {}\".format(results.hits))\n\n        if self.iterate:\n            docs = results.docs\n            start = n_docs\n            while n_docs >= self.rows:\n                logging.info(\"Iterating; start={}\".format(start))\n                next_results = self.solr.search(start=start, **params)\n                next_docs = next_results.docs\n                n_docs = len(next_docs)\n                docs += next_docs\n                start += self.rows\n            results.docs = docs\n\n        fcs = results.facets\n\n        payload = {\n            'facet_counts': translate_facet_field(fcs, self.invert_subject_object),\n            'pagination': {},\n            'numFound': results.hits\n        }\n\n        include_raw=self.include_raw\n        if include_raw:\n            # note: this is not JSON serializable, do not send via REST\n            payload['raw'] = results\n\n        # TODO - check if truncated\n\n        logging.info(\"COMPACT={} INV={}\".format(self.use_compact_associations, self.invert_subject_object))\n        if self.use_compact_associations:\n            payload['compact_associations'] = self.translate_docs_compact(results.docs, field_mapping=self.field_mapping,\n                                                                     slim=self.slim, invert_subject_object=self.invert_subject_object,\n                                                                     map_identifiers=self.map_identifiers, **kwargs)\n        else:\n            payload['associations'] = self.translate_docs(results.docs, field_mapping=self.field_mapping, map_identifiers=self.map_identifiers, **kwargs)\n\n        if 'facet_pivot' in fcs:\n            payload['facet_pivot'] = fcs['facet_pivot']\n        if 'facets' in results.raw_response:\n            payload['facets'] = results.raw_response['facets']\n\n        # For solr, we implement this by finding all facets\n        # TODO: no need to do 2nd query, see https://wiki.apache.org/solr/SimpleFacetParameters#Parameters\n        fetch_objects=self.fetch_objects\n        if fetch_objects:\n            core_object_field = M.OBJECT\n            if self.slim is not None and len(self.slim)>0:\n                core_object_field = M.OBJECT_CLOSURE\n            object_field = map_field(core_object_field, self.field_mapping)\n            if self.invert_subject_object:\n                object_field = map_field(M.SUBJECT, self.field_mapping)\n            oq_params = params.copy()\n            oq_params['fl'] = []\n            oq_params['facet.field'] = [object_field]\n            oq_params['facet.limit'] = -1\n            oq_params['rows'] = 0\n            oq_params['facet.mincount'] = 1\n            oq_results = self.solr.search(**oq_params)\n            ff = oq_results.facets['facet_fields']\n            ofl = ff.get(object_field)\n            # solr returns facets counts as list, every 2nd element is number, we don't need the numbers here\n            payload['objects'] = ofl[0::2]\n\n        fetch_subjects=self.fetch_subjects\n        if fetch_subjects:\n            core_subject_field = M.SUBJECT\n            if self.slim is not None and len(self.slim)>0:\n                core_subject_field = M.SUBJECT_CLOSURE\n            subject_field = map_field(core_subject_field, self.field_mapping)\n            if self.invert_subject_object:\n                subject_field = map_field(M.SUBJECT, self.field_mapping)\n            oq_params = params.copy()\n            oq_params['fl'] = []\n            oq_params['facet.field'] = [subject_field]\n            oq_params['facet.limit'] = self.max_rows\n            oq_params['rows'] = 0\n            oq_params['facet.mincount'] = 1\n            oq_results = self.solr.search(**oq_params)\n            ff = oq_results.facets['facet_fields']\n            ofl = ff.get(subject_field)\n            # solr returns facets counts as list, every 2nd element is number, we don't need the numbers here\n            payload['subjects'] = ofl[0::2]\n            if len(payload['subjects']) == self.max_rows:\n                payload['is_truncated'] = True\n\n        if self.slim is not None and len(self.slim)>0:\n            if 'objects' in payload:\n                payload['objects'] = [x for x in payload['objects'] if x in self.slim]\n            if 'associations' in payload:\n                for a in payload['associations']:\n                    a['slim'] = [x for x in a['object_closure'] if x in self.slim]\n                    del a['object_closure']\n\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninfer a category from an id.", "response": "def infer_category(self, id):\n        \"\"\"\n        heuristic to infer a category from an id, e.g. DOID:nnn --> disease\n        \"\"\"\n        logging.info(\"Attempting category inference on id={}\".format(id))\n        toks = id.split(\":\")\n        idspace = toks[0]\n        c = None\n        if idspace == 'DOID':\n            c='disease'\n        if c is not None:\n            logging.info(\"Inferred category: {} based on id={}\".format(c, id))\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_canonical_identifier(self,id):\n        if id is not None:\n            for (k,v) in PREFIX_NORMALIZATION_MAP.items():\n                s = k+':'\n                if id.startswith(s):\n                    return id.replace(s,v+':')\n        return id", "response": "Make a canonical identifier for the given ID."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef translate_objs(self,d,fname):\n        if fname not in d:\n            # TODO: consider adding arg for failure on null\n            return None\n\n        #lf = M.label_field(fname)\n\n        v = d[fname]\n        if not isinstance(v,list):\n            v = [v]\n        objs = [{'id': idval} for idval in v]\n        # todo - labels\n\n        return objs", "response": "Translate a field whose value is expected to be a list of objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef translate_obj(self,d,fname):\n        if fname not in d:\n            # TODO: consider adding arg for failure on null\n            return None\n\n        lf = M.label_field(fname)\n        id = d[fname]\n        id = self.make_canonical_identifier(id)\n        #if id.startswith('MGI:MGI:'):\n        #    id = id.replace('MGI:MGI:','MGI:')\n        obj = {'id': id}\n\n        if id:\n            if self._use_amigo_schema(self.object_category):\n                iri = expand_uri(id)\n            else:\n                iri = expand_uri(id, [get_curie_map('{}/cypher/curies'.format(self.config.scigraph_data.url))])\n            obj['iri'] = iri\n\n        if lf in d:\n            obj['label'] = d[lf]\n\n        cf = fname + \"_category\"\n        if cf in d:\n            obj['category'] = [d[cf]]\n\n        if 'aspect' in d and id.startswith('GO:'):\n            obj['category'] = [ASPECT_MAP[d['aspect']]]\n            del d['aspect']\n\n        return obj", "response": "Translate a field value from a solr document to a solr object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates a solr document into a solr document.", "response": "def translate_doc(self, d, field_mapping=None, map_identifiers=None, **kwargs):\n        \"\"\"\n        Translate a solr document (i.e. a single result row)\n        \"\"\"\n        if field_mapping is not None:\n            self.map_doc(d, field_mapping)\n        subject = self.translate_obj(d, M.SUBJECT)\n        obj = self.translate_obj(d, M.OBJECT)\n\n        # TODO: use a more robust method; we need equivalence as separate field in solr\n        if map_identifiers is not None:\n            if M.SUBJECT_CLOSURE in d:\n                subject['id'] = self.map_id(subject, map_identifiers, d[M.SUBJECT_CLOSURE])\n            else:\n                logging.info(\"NO SUBJECT CLOSURE IN: \"+str(d))\n\n        if M.SUBJECT_TAXON in d:\n            subject['taxon'] = self.translate_obj(d,M.SUBJECT_TAXON)\n        if M.OBJECT_TAXON in d:\n            obj['taxon'] = self.translate_obj(d, M.OBJECT_TAXON)\n\n        qualifiers = []\n        if M.RELATION in d and isinstance(d[M.RELATION],list):\n            # GO overloads qualifiers and relation\n            relation = None\n            for rel in d[M.RELATION]:\n                if rel.lower() == 'not':\n                    qualifiers.append(rel)\n                else:\n                    relation = rel\n            if relation is not None:\n                d[M.RELATION] = relation\n            else:\n                d[M.RELATION] = None\n\n        negated = 'not' in qualifiers\n\n        assoc = {'id':d.get(M.ID),\n                 'subject': subject,\n                 'object': obj,\n                 'negated': negated,\n                 'relation': self.translate_obj(d,M.RELATION),\n                 'publications': self.translate_objs(d,M.SOURCE),  # note 'source' is used in the golr schema\n        }\n\n        if self.invert_subject_object and assoc['relation'] is not None:\n            assoc['relation']['inverse'] = True\n\n        if len(qualifiers) > 0:\n            assoc['qualifiers'] = qualifiers\n\n        if M.OBJECT_CLOSURE in d:\n            assoc['object_closure'] = d.get(M.OBJECT_CLOSURE)\n        if M.IS_DEFINED_BY in d:\n            if isinstance(d[M.IS_DEFINED_BY],list):\n                assoc['provided_by'] = d[M.IS_DEFINED_BY]\n            else:\n                # hack for GO Golr instance\n                assoc['provided_by'] = [d[M.IS_DEFINED_BY]]\n        if M.EVIDENCE_OBJECT in d:\n            assoc['evidence'] = d[M.EVIDENCE_OBJECT]\n            assoc['types'] = [t for t in d[M.EVIDENCE_OBJECT] if t.startswith('ECO:')]\n\n        if self._use_amigo_schema(self.object_category):\n            for f in M.AMIGO_SPECIFIC_FIELDS:\n                if f in d:\n                    assoc[f] = d[f]\n\n        # solr does not allow nested objects, so evidence graph is json-encoded\n        if M.EVIDENCE_GRAPH in d:\n            assoc[M.EVIDENCE_GRAPH] = json.loads(d[M.EVIDENCE_GRAPH])\n        return assoc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate_docs(self, ds, **kwargs):\n        for d in ds:\n            self.map_doc(d, {}, self.invert_subject_object)\n\n        return [self.translate_doc(d, **kwargs) for d in ds]", "response": "Translate a set of solr results into a list of solr results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntranslating a set of documents into a compact representation.", "response": "def translate_docs_compact(self, ds, field_mapping=None, slim=None, map_identifiers=None, invert_subject_object=False, **kwargs):\n        \"\"\"\n        Translate golr association documents to a compact representation\n        \"\"\"\n        amap = {}\n        logging.info(\"Translating docs to compact form. Slim={}\".format(slim))\n        for d in ds:\n            self.map_doc(d, field_mapping, invert_subject_object=invert_subject_object)\n\n            subject = d[M.SUBJECT]\n            subject_label = d[M.SUBJECT_LABEL]\n\n            # TODO: use a more robust method; we need equivalence as separate field in solr\n            if map_identifiers is not None:\n                if M.SUBJECT_CLOSURE in d:\n                    subject = self.map_id(subject, map_identifiers, d[M.SUBJECT_CLOSURE])\n                else:\n                    logging.debug(\"NO SUBJECT CLOSURE IN: \"+str(d))\n\n            rel = d.get(M.RELATION)\n            skip = False\n\n            # TODO\n            if rel == 'not' or rel == 'NOT':\n                skip = True\n\n            # this is a list in GO\n            if isinstance(rel,list):\n                if 'not' in rel or 'NOT' in rel:\n                    skip = True\n                if len(rel) > 1:\n                    logging.warn(\">1 relation: {}\".format(rel))\n                rel = \";\".join(rel)\n\n            if skip:\n                logging.debug(\"Skipping: {}\".format(d))\n                continue\n\n            subject = self.make_canonical_identifier(subject)\n            #if subject.startswith('MGI:MGI:'):\n            #    subject = subject.replace('MGI:MGI:','MGI:')\n\n            k = (subject,rel)\n            if k not in amap:\n                amap[k] = {'subject':subject,\n                           'subject_label':subject_label,\n                           'relation':rel,\n                           'objects': []}\n            if slim is not None and len(slim)>0:\n                mapped_objects = [x for x in d[M.OBJECT_CLOSURE] if x in slim]\n                logging.debug(\"Mapped objects: {}\".format(mapped_objects))\n                amap[k]['objects'] += mapped_objects\n            else:\n                amap[k]['objects'].append(d[M.OBJECT])\n        for k in amap.keys():\n            amap[k]['objects'] = list(set(amap[k]['objects']))\n\n        return list(amap.values())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping an identifier based on an equivalence closure list.", "response": "def map_id(self,id, prefix, closure_list):\n        \"\"\"\n        Map identifiers based on an equivalence closure list.\n        \"\"\"\n        prefixc = prefix + ':'\n        ids = [eid for eid in closure_list if eid.startswith(prefixc)]\n        # TODO: add option to fail if no mapping, or if >1 mapping\n        if len(ids) == 0:\n            # default to input\n            return id\n        return ids[0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, ontology=None,subject_category=None,object_category=None,evidence=None,taxon=None,relation=None, file=None, fmt=None, skim=True):\n        meta = AssociationSetMetadata(subject_category=subject_category,\n                                      object_category=object_category,\n                                      taxon=taxon)\n\n        if file is not None:\n            return self.create_from_file(file=file,\n                                         fmt=fmt,\n                                         ontology=ontology,\n                                         meta=meta,\n                                         skim=skim)\n\n        logging.info(\"Fetching assocs from store\")\n        assocs = bulk_fetch_cached(subject_category=subject_category,\n                                   object_category=object_category,\n                                   evidence=evidence,\n                                   taxon=taxon)\n\n        logging.info(\"Creating map for {} subjects\".format(len(assocs)))\n\n\n\n        amap = {}\n        subject_label_map = {}\n        for a in assocs:\n            rel = a['relation']\n            subj = a['subject']\n            subject_label_map[subj] = a['subject_label']\n            amap[subj] = a['objects']\n\n\n        aset = AssociationSet(ontology=ontology,\n                              meta=meta,\n                              subject_label_map=subject_label_map,\n                              association_map=amap)\n        return aset", "response": "Creates an AssociationSet instance from a file or an association set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_from_tuples(self, tuples, **args):\n        amap = {}\n        subject_label_map = {}\n        for a in tuples:\n            subj = a[0]\n            subject_label_map[subj] = a[1]\n            if subj not in amap:\n                amap[subj] = []\n            amap[subj].append(a[2])\n\n        aset = AssociationSet(subject_label_map=subject_label_map, association_map=amap, **args)\n        return aset", "response": "Creates an AssociationSet object from a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new AssociationSet from a list of association objects", "response": "def create_from_assocs(self, assocs, **args):\n        \"\"\"\n        Creates from a list of association objects\n        \"\"\"\n        amap = defaultdict(list)\n        subject_label_map = {}\n        for a in assocs:\n            subj = a['subject']\n            subj_id = subj['id']\n            subj_label = subj['label']\n            subject_label_map[subj_id] = subj_label\n            if not a['negated']:\n                amap[subj_id].append(a['object']['id'])\n\n        aset = AssociationSet(subject_label_map=subject_label_map, association_map=amap, **args)\n        aset.associations_by_subj = defaultdict(list)\n        aset.associations_by_subj_obj = defaultdict(list)\n        for a in assocs:\n            sub_id = a['subject']['id']\n            obj_id = a['object']['id']\n            aset.associations_by_subj[sub_id].append(a)\n            aset.associations_by_subj_obj[(sub_id,obj_id)].append(a)\n\n        return aset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new instance of a new instance of a new instance of a new class based on the input file.", "response": "def create_from_file(self, file=None, fmt='gaf', skim=True, **args):\n        \"\"\"\n        Creates from a file. If fmt is set to None then the file suffixes will\n        be used to choose a parser.\n\n        Arguments\n        ---------\n        file : str or file\n            input file or filename\n        fmt : str\n            name of format e.g. gaf\n\n        \"\"\"\n        if fmt is not None and not fmt.startswith('.'):\n            fmt = '.{}'.format(fmt)\n\n        d = {\n            '.gaf' : GafParser,\n            '.gpad' : GpadParser,\n            '.hpoa' : HpoaParser,\n        }\n\n        if fmt is None:\n            filename = file if isinstance(file, str) else file.name\n            suffixes = pathlib.Path(filename).suffixes\n            iterator = (fn() for ext, fn in d.items() if ext in suffixes)\n        else:\n            iterator = (fn() for ext, fn in d.items() if ext == fmt)\n\n        try:\n            parser = next(iterator)\n        except StopIteration:\n            logging.error(\"Format not recognized: {}\".format(fmt))\n\n        logging.info(\"Parsing {} with {}/{}\".format(file, fmt, parser))\n\n        if skim:\n            results = parser.skim(file)\n            return self.create_from_tuples(results, **args)\n        else:\n            assocs = parser.parse(file, skipheader=True)\n            return self.create_from_assocs(assocs, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate from remote GAF", "response": "def create_from_remote_file(self, group, snapshot=True, **args):\n        \"\"\"\n        Creates from remote GAF\n        \"\"\"\n        import requests\n        url = \"http://snapshot.geneontology.org/annotations/{}.gaf.gz\".format(group)\n        r = requests.get(url, stream=True, headers={'User-Agent': get_user_agent(modules=[requests], caller_name=__name__)})\n        p = GafParser()\n        results = p.skim(r.raw)\n        return self.create_from_tuples(results, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting or displays graph", "response": "def render(ont, query_ids, args):\n    \"\"\"\n    Writes or displays graph\n    \"\"\"\n    if args.slim.find('m') > -1:\n        logging.info(\"SLIMMING\")\n        g = get_minimal_subgraph(g, query_ids)\n    w = GraphRenderer.create(args.to)\n    if args.showdefs:\n        w.config.show_text_definition = True\n    if args.render:\n        if 'd' in args.render:\n            logging.info(\"Showing text defs\")\n            w.config.show_text_definition = True\n    if args.outfile is not None:\n        w.outfile = args.outfile\n    w.write(ont, query_ids=query_ids, container_predicates=args.container_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_object_closure(subject, object_category=None, **kwargs):\n    results = search_associations(subject=subject,\n                                  object_category=object_category,\n                                  select_fields=[],\n                                  facet_fields=[M.OBJECT_CLOSURE],\n                                  facet_limit=-1,\n                                  rows=0,\n                                  **kwargs)\n    return set(results['facet_counts'][M.OBJECT_CLOSURE].keys())", "response": "Get all terms used to annotate subject plus ancestors."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of subjects that overlap between two subjects.", "response": "def subject_pair_overlap(subject1, subject2, object_category=None, **kwargs):\n    \"\"\"\n    Jaccard similarity\n    \"\"\"\n    set1 = get_object_closure(subject1,\n                              object_category=object_category,\n                              **kwargs)\n    set2 = get_object_closure(subject2,\n                              object_category=object_category,\n                              **kwargs)\n    return len(set1.intersection(set2)), len(set1.union(set2))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subject_pair_simj(subject1, subject2, **kwargs):\n    i, u = subject_pair_overlap(subject1, subject2, **kwargs)\n    if i==0:\n        return 0.0\n    return i / u", "response": "Compute similarity between two subjects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a single association to the output file.", "response": "def write_assoc(self, assoc):\n        \"\"\"\n        Write a single association to a line in the output file\n        \"\"\"\n        if assoc.get(\"header\", False):\n            return\n\n        subj = assoc['subject']\n\n        db, db_object_id = self._split_prefix(subj)\n\n        rel = assoc['relation']\n        qualifier = rel['id']\n        if assoc['negated']:\n            qualifier = 'NOT|' + qualifier\n\n        goid = assoc['object']['id']\n\n        ev = assoc['evidence']\n\n        evidence = self.ecomap.coderef_to_ecoclass(ev['type'])\n        withfrom = \"|\".join(ev['with_support_from'])\n        reference = \"|\".join(ev['has_supporting_reference'])\n\n        date = assoc['date']\n        assigned_by = assoc['provided_by']\n\n        annotation_properties = '' # TODO\n        interacting_taxon_id = assoc['interacting_taxon']\n        vals = [db,\n                db_object_id,\n                qualifier,\n                goid,\n                reference,\n                evidence,\n                withfrom,\n                interacting_taxon_id, # TODO\n                date,\n                assigned_by,\n                self._extension_expression(assoc['object_extensions']),\n                annotation_properties]\n\n        self._write_row(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_assoc(self, assoc):\n        # Handle comment 'associations'\n        if assoc.get(\"header\", False):\n\n            # Skip incoming gaf-version headers, as we created the version above already\n            if re.match(\"![\\s]*gaf.?version\", assoc[\"line\"]):\n                return\n\n            self._write(assoc[\"line\"] + \"\\n\")\n            return\n\n        subj = assoc['subject']\n\n        db, db_object_id = self._split_prefix(subj)\n\n        qualifier = \"|\".join(assoc[\"qualifiers\"])\n        # qualifier is | seperated string, filter on None removes empty strings in case qualifiers is empty\n        if assoc['negated']:\n            qualifier = \"|\".join(list(filter(None, [\"NOT\", qualifier])))\n\n        goid = assoc['object']['id']\n\n        ev = assoc['evidence']\n        evidence = ev['type']\n        withfrom = \"|\".join(ev['with_support_from'])\n        reference = \"|\".join(ev['has_supporting_reference'])\n\n        date = assoc['date']\n        assigned_by = assoc['provided_by']\n\n        annotation_properties = '' # TODO\n        # if we have any subject extensions, list each one that has a \"property\" equal to \"isoform\", take the first one, and grab the \"filler\"\n        gene_product_isoform = [e for e in assoc[\"subject_extensions\"] if e[\"property\"] == \"isoform\"][0][\"filler\"] if len(assoc[\"subject_extensions\"]) > 0 else \"\"\n\n        aspect = assoc['aspect']\n        interacting_taxon_id = assoc[\"interacting_taxon\"]\n        taxon = self._full_taxon_field(self.normalize_taxon(subj['taxon']['id']), self.normalize_taxon(interacting_taxon_id))\n\n        extension_expression = self._extension_expression(assoc['object_extensions'])\n\n        vals = [db,\n                db_object_id,\n                subj.get('label'),\n                qualifier,\n                goid,\n                reference,\n                evidence,\n                withfrom,\n                aspect,\n                subj[\"fullname\"],\n                \"|\".join(subj.get('synonyms',[])),\n                subj.get('type'),\n                taxon,\n                date,\n                assigned_by,\n                extension_expression,\n                gene_product_isoform]\n\n        self._write_row(vals)", "response": "Write a single association to a file in the output file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict mapping namespace to taxon mapping", "response": "def namespace_to_taxon() -> Dict[str, Node]:\n    \"\"\"\n    namespace to taxon mapping\n    \"\"\"\n    human_taxon = Node(\n        id='NCBITaxon:9606',\n        label='Homo sapiens'\n    )\n    return {\n        'MGI': Node(\n            id='NCBITaxon:10090',\n            label='Mus musculus'\n        ),\n        'MONDO':   human_taxon,\n        'OMIM':    human_taxon,\n        'MONARCH': human_taxon,\n        'HGNC':    human_taxon,\n        'FlyBase': Node(\n            id='NCBITaxon:7227',\n            label='Drosophila melanogaster'\n        ),\n        'WormBase': Node(\n            id='NCBITaxon:6239',\n            label='Caenorhabditis elegans'\n        ),\n        'ZFIN': Node(\n            id='NCBITaxon:7955',\n            label='Danio rerio'\n        )\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_scigraph_nodes(id_list)-> Iterator[Dict]:\n    scigraph = OntologyFactory().create('scigraph:data')\n\n    chunks = [id_list[i:i + 400] for i in range(0, len(list(id_list)), 400)]\n    for chunk in chunks:\n        params = {\n            'id': chunk,\n            'depth': 0\n        }\n\n        try:\n            result_graph = scigraph._neighbors_graph(**params)\n            for node in result_graph['nodes']:\n                yield node\n        except JSONDecodeError as exception:\n            # Assume json decode is due to an incorrect class ID\n            # Should we handle this?\n            raise ValueError(exception.doc)", "response": "Queries scigraph neighbors to get a list of nodes back\n    Queries scigraph neighbors to get a list of nodes back\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of ids return their types", "response": "def get_id_type_map(id_list: Iterable[str]) -> Dict[str, List[str]]:\n    \"\"\"\n    Given a list of ids return their types\n\n    :param id_list: list of ids\n    :return: dictionary where the id is the key and the value is a list of types\n    \"\"\"\n    type_map = {}\n    filter_out_types = [\n        'cliqueLeader',\n        'Class',\n        'Node',\n        'Individual',\n        'quality',\n        'sequence feature'\n    ]\n\n    for node in get_scigraph_nodes(id_list):\n        type_map[node['id']] = [typ.lower() for typ in node['meta']['types']\n                                if typ not in filter_out_types]\n\n    return type_map"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a list of ids return their types", "response": "def get_nodes_from_ids(id_list: Iterable[str]) -> List[Node]:\n    \"\"\"\n    Given a list of ids return their types\n\n    :param id_list: list of ids\n    :return: dictionary where the id is the key and the value is a list of types\n    \"\"\"\n    node_list = []\n\n    for result in get_scigraph_nodes(id_list):\n        if 'lbl' in result:\n            label = result['lbl']\n        else:\n            label = None  # Empty string or None?\n        node_list.append(Node(result['id'], label))\n\n    return node_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_taxon(id: str) -> Optional[Node]:\n    taxon = None\n    namespace = id.split(\":\")[0]\n    if namespace in namespace_to_taxon():\n        taxon = namespace_to_taxon()[namespace]\n\n    return taxon", "response": "get taxon for id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef typed_node_from_id(id: str) -> TypedNode:\n    filter_out_types = [\n        'cliqueLeader',\n        'Class',\n        'Node',\n        'Individual',\n        'quality',\n        'sequence feature'\n    ]\n    node = next(get_scigraph_nodes([id]))\n\n    if 'lbl' in node:\n        label = node['lbl']\n    else:\n        label = None  # Empty string or None?\n\n    types = [typ.lower() for typ in node['meta']['types']\n             if typ not in filter_out_types]\n\n    return TypedNode(\n        id=node['id'],\n        label=label,\n        type=types[0],\n        taxon = get_taxon(id)\n    )", "response": "Get typed node from id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting CURIE prefix map from SciGraph cypher endpoint", "response": "def get_curie_map(url):\n    \"\"\"\n    Get CURIE prefix map from SciGraph cypher/curies endpoint\n    \"\"\"\n    curie_map = {}\n    response = requests.get(url)\n    if response.status_code == 200:\n        curie_map = response.json()\n    else:\n        curie_map = {}\n\n    return curie_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a summary in json format", "response": "def to_report_json(self):\n        \"\"\"\n        Generate a summary in json format\n        \"\"\"\n\n        return self.reporter.json(self.n_lines, self.n_assocs, self.skipped)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a summary in markdown format format", "response": "def to_markdown(self):\n        \"\"\"\n        Generate a summary in markdown format\n        \"\"\"\n        json = self.to_report_json()\n        # summary = json['summary']\n\n        s = \"# Group: {group} - Dataset: {dataset}\\n\".format(group=json[\"group\"], dataset=json[\"dataset\"])\n        s += \"\\n## SUMMARY\\n\\n\"\n        s += \"This report generated on {}\\n\\n\".format(datetime.date.today())\n        s += \"  * Associations: {}\\n\" . format(json[\"associations\"])\n        s += \"  * Lines in file (incl headers): {}\\n\" . format(json[\"lines\"])\n        s += \"  * Lines skipped: {}\\n\" . format(json[\"skipped_lines\"])\n        # Header from GAF\n        s += \"## Header From Original Association File\\n\\n\"\n        s += \"\\n\".join([\"> {}  \".format(head) for head in self.header])\n        ## Table of Contents\n        s += \"\\n\\n## Contents\\n\\n\"\n        for rule, messages in sorted(json[\"messages\"].items(), key=lambda t: t[0]):\n            any_suppress_tag_in_rule_metadata = any([tag in self.config.rule_metadata.get(rule, {}).get(\"tags\", []) for tag in self.config.suppress_rule_reporting_tags])\n            # For each tag we say to suppress output for, check if it matches any tag in the rule. If any matches\n            if self.config.rule_metadata and any_suppress_tag_in_rule_metadata:\n                print(\"Skipping {rule_num} because the tag(s) '{tag}' are suppressed\".format(rule_num=rule, tag=\", \".join(self.config.suppress_rule_reporting_tags)))\n                continue\n\n            s += \"[{rule}](#{rule})\\n\\n\".format(rule=rule)\n\n        s += \"\\n## MESSAGES\\n\\n\"\n        for (rule, messages) in sorted(json[\"messages\"].items(), key=lambda t: t[0]):\n            any_suppress_tag_in_rule_metadata = any([tag in self.config.rule_metadata.get(rule, {}).get(\"tags\", []) for tag in self.config.suppress_rule_reporting_tags])\n\n            # Skip if the rule metadata has \"silent\" as a tag\n            if self.config.rule_metadata and any_suppress_tag_in_rule_metadata:\n                # If there is a rule metadata, and the rule ID is in the config,\n                # get the list of tags if present and check for existence of \"silent\".\n                # If contained, continue to the next rule.\n                continue\n\n\n            s += \"### {rule}\\n\\n\".format(rule=rule)\n            if rule != \"other\" and self.config.rule_metadata:\n                s += \"{title}\\n\\n\".format(title=self.config.rule_metadata.get(rule, {}).get(\"title\", \"\"))\n            s += \"* total: {amount}\\n\".format(amount=len(messages))\n            if len(messages) > 0:\n                s += \"#### Messages\\n\"\n            for message in messages:\n                obj = \" ({})\".format(message[\"obj\"]) if message[\"obj\"] else \"\"\n                s += \"* {level} - {type}: {message}{obj} -- `{line}`\\n\".format(level=message[\"level\"], type=message[\"type\"], message=message[\"message\"], line=message[\"line\"], obj=obj)\n\n        # for g in json['groups']:\n        #     s += \" * {}: {}\\n\".format(g['level'], g['count'])\n        # s += \"\\n\\n\"\n        # for g in json['groups']:\n        #     level = g['level']\n        #     msgs = g['messages']\n        #     if len(msgs) > 0:\n        #         s += \"### {}\\n\\n\".format(level)\n        #         for m in msgs:\n        #             s += \" * {}: obj:'{}' \\\"{}\\\" `{}`\\n\".format(m['type'],m['obj'],m['message'],m['line'])\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(self, file, skipheader=False, outfile=None):\n        associations = self.association_generator(file, skipheader=skipheader, outfile=outfile)\n        a = list(associations)\n        return a", "response": "Parse a line - oriented association file into a list of association dict objects"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef association_generator(self, file, skipheader=False, outfile=None) -> Dict:\n        file = self._ensure_file(file)\n        for line in file:\n            parsed_result = self.parse_line(line)\n            self.report.report_parsed_result(parsed_result, outfile, self.config.filtered_evidence_file, self.config.filter_out_evidence)\n            for association in parsed_result.associations:\n                # yield association if we don't care if it's a header or if it's definitely a real gaf line\n                if not skipheader or \"header\" not in association:\n                    yield association\n\n        logging.info(self.report.short_summary())\n        file.close()", "response": "Returns a generator that yields successive associations from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_to_subset(self, file, outfile=None, ontology=None, subset=None, class_map=None, relations=None):\n        if subset is not None:\n            logging.info(\"Creating mapping for subset: {}\".format(subset))\n            class_map = ontology.create_slim_mapping(subset=subset, relations=relations)\n\n        if class_map is None:\n            raise ValueError(\"Neither class_map not subset is set\")\n        col = self.ANNOTATION_CLASS_COLUMN\n        file = self._ensure_file(file)\n        tuples = []\n        for line in file:\n            if line.startswith(\"!\"):\n                continue\n            vals = line.split(\"\\t\")\n            logging.info(\"LINE: {} VALS: {}\".format(line, vals))\n            if len(vals) < col:\n                raise ValueError(\"Line: {} has too few cols, expect class id in col {}\".format(line, col))\n            cid = vals[col]\n            if cid not in class_map or len(class_map[cid]) == 0:\n                self.report.error(line, Report.UNMAPPED_ID, cid)\n                continue\n            else:\n                for mcid in class_map[cid]:\n                    vals[col] = mcid\n                    line = \"\\t\".join(vals)\n                    if outfile is not None:\n                        outfile.write(line)\n                    else:\n                        print(line)", "response": "Maps a file to a subset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a single line of a HPOA file and returns a tuple of parsed_line associations", "response": "def parse_line(self, line):\n        \"\"\"\n        Parses a single line of a HPOA file\n\n        Return a tuple `(processed_line, associations)`. Typically\n        there will be a single association, but in some cases there\n        may be none (invalid line) or multiple (disjunctive clause in\n        annotation extensions)\n\n        Note: most applications will only need to call this directly if they require fine-grained control of parsing. For most purposes,\n        :method:`parse_file` can be used over the whole file\n\n        Arguments\n        ---------\n        line : str\n            A single tab-seperated line from a GPAD file\n\n        \"\"\"\n        config = self.config\n\n        parsed = super().validate_line(line)\n        if parsed:\n            return parsed\n\n        if self.is_header(line):\n            return assocparser.ParseResult(line, [], False)\n\n        # http://human-phenotype-ontology.github.io/documentation.html#annot\n        vals = line.split(\"\\t\")\n        if len(vals) != 14:\n            self.report.error(line, assocparser.Report.WRONG_NUMBER_OF_COLUMNS, \"\",\n                msg=\"There were {columns} columns found in this line, and there should be 14\".format(columns=len(vals)))\n            return assocparser.ParseResult(line, [], True)\n\n        [db,\n         db_object_id,\n         db_object_symbol,\n         qualifier,\n         hpoid,\n         reference,\n         evidence,\n         onset,\n         frequency,\n         withfrom,\n         aspect,\n         db_object_synonym,\n         date,\n         assigned_by] = vals\n\n        # hardcode this, as HPOA is currently human-only\n        taxon = 'NCBITaxon:9606'\n        split_line = assocparser.SplitLine(line=line, values=vals, taxon=taxon)\n\n\n        # hardcode this, as HPOA is currently disease-only\n        db_object_type = 'disease'\n\n        ## --\n        ## db + db_object_id. CARD=1\n        ## --\n        id = self._pair_to_id(db, db_object_id)\n        if not self._validate_id(id, split_line, context=ENTITY):\n            return assocparser.ParseResult(line, [], True)\n\n        if not self._validate_id(hpoid, split_line, context=ANNOTATION):\n            return assocparser.ParseResult(line, [], True)\n\n        valid_hpoid = self._validate_ontology_class_id(hpoid, split_line)\n        if valid_hpoid == None:\n            return assocparser.ParseResult(line, [], True)\n        hpoid = valid_hpoid\n\n        # validation\n        #self._validate_symbol(db_object_symbol, line)\n\n        #TODO: HPOA has different date styles\n        #date = self._normalize_gaf_date(date, line)\n\n        # Example use case: mapping from OMIM to Orphanet\n        if config.entity_map is not None:\n            id = self.map_id(id, config.entity_map)\n            toks = id.split(\":\")\n            db = toks[0]\n            db_object_id = toks[1:]\n            vals[1] = db_object_id\n\n        ## --\n        ## end of line re-processing\n        ## --\n        # regenerate line post-mapping\n        line = \"\\t\".join(vals)\n\n        ## --\n        ## db_object_synonym CARD=0..*\n        ## --\n        synonyms = db_object_synonym.split(\"|\")\n        if db_object_synonym == \"\":\n            synonyms = []\n\n\n        ## --\n        ## qualifier\n        ## --\n        ## we generate both qualifier and relation field\n        relation = None\n        qualifiers = qualifier.split(\"|\")\n        if qualifier == '':\n            qualifiers = []\n        negated =  'NOT' in qualifiers\n        other_qualifiers = [q for q in qualifiers if q != 'NOT']\n\n        ## CURRENTLY NOT USED\n        if len(other_qualifiers) > 0:\n            relation = other_qualifiers[0]\n        else:\n            if aspect == 'O':\n                relation = 'has_phenotype'\n            elif aspect == 'I':\n                relation = 'has_inheritance'\n            elif aspect == 'M':\n                relation = 'mortality'\n            elif aspect == 'C':\n                relation = 'has_onset'\n            else:\n                relation = None\n\n        # With/From\n        withfroms = self.validate_pipe_separated_ids(withfrom, split_line, empty_allowed=True, extra_delims=\",\")\n        if withfroms == None:\n            # Reporting occurs in above function call\n            return assocparser.ParseResult(line, [], True)\n\n        ## --\n        ## hpoid\n        ## --\n        object = {'id':hpoid,\n                  'taxon': taxon}\n\n        # construct subject dict\n        subject = {\n            'id':id,\n            'label':db_object_symbol,\n            'type': db_object_type,\n            'synonyms': synonyms,\n            'taxon': {\n                'id': taxon\n            }\n        }\n\n        ## --\n        ## evidence\n        ## reference\n        ## withfrom\n        ## --\n        evidence = {\n            'type': evidence,\n            'has_supporting_reference': reference.split(\"; \"),\n            'with_support_from': withfroms\n        }\n\n        ## Construct main return dict\n        assoc = {\n            'source_line': line,\n            'subject': subject,\n            'object': object,\n            'negated': negated,\n            'qualifiers': qualifiers,\n            'relation': {\n                'id': relation\n            },\n            'interacting_taxon': None,\n            'evidence': evidence,\n            'provided_by': assigned_by,\n            'date': date,\n\n        }\n\n        return assocparser.ParseResult(line, [assoc], False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n\n    parser = argparse.ArgumentParser(\n        description='Command line interface to python-ontobio.golr library'\n        \"\"\"\n\n        Provides command line interface onto the ontobio.golr python library, a high level\n        abstraction layer over Monarch and GO solr indices.\n        \"\"\",\n        formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-o', '--outfile', type=str, required=False,\n                        help='Path to output file')\n    parser.add_argument('-f', '--facet', type=str, required=True,\n                        help='Facet field to query')\n    parser.add_argument('-q', '--fq', type=json.loads, default={}, required=False,\n                        help='Facet query (solr fq) - should be json')\n    parser.add_argument('-Q', '--qargs', type=json.loads, default={}, required=False,\n                        help='Query to be passed directly to python golr_associations query')\n    parser.add_argument('-P', '--search', nargs='*', type=str, required=False,\n                        help='Search fields. E.f subject_category object_category, relation')\n    parser.add_argument('-u', '--url', type=str, required=False,\n                        help='Solr URL. E.g. http://localhost:8983/solr/golr')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n        \n    logging.info(\"Welcome!\")\n\n    r = search_query_as_matrix(facet=args.facet,\n                               fq=args.fq,\n                               facet_search_fields=args.search,\n                               url=args.url,\n                               **args.qargs)\n\n    print(str(r))\n    trace = go.Heatmap(z=r['z'],\n                       x=r['xaxis'],\n                       y=r['yaxis'])\n    data=[trace]\n    py.plot(data, filename='search-heatmap')", "response": "Entry point for the main function of the golr_associations command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_config():\n    if session.config is None:\n        path = session.default_config_path\n        if os.path.isfile(path):\n            logging.info(\"LOADING FROM: {}\".format(path))\n            session.config = load_config(path)\n        else:\n            session.config = Config()\n            logging.info(\"using default session: {}, path does not exist: {}\".format(session, path))\n    else:\n        logging.info(\"Using pre-loaded object: {}\".format(session.config))\n    return session.config", "response": "Returns the current config for the current session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads configuration from file and set it as current session. config", "response": "def set_config(path):\n    \"\"\"\n    Set configuration for current session.\n    \"\"\"\n    logging.info(\"LOADING FROM: {}\".format(path))\n    session.config = load_config(path)\n    return session.config"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the solr search URL for the current set of attributes.", "response": "def get_solr_search_url(self, use_amigo=False):\n        \"\"\"\n        Return solr URL to be used for lexical entity searches\n\n        A solr search URL is used to search entities/concepts based on a limited set of parameters.\n\n        Arguments\n        ---------\n        use_amigo : bool\n            If true, get the URL for the GO/AmiGO instance of GOlr. This is typically used for category='function' queries\n        \"\"\"\n        url = self.endpoint_url(self.solr_search)\n        if use_amigo:\n            url = self.endpoint_url(self.amigo_solr_search)\n        return url"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_solr_assocs_url(self, use_amigo=False):\n        url = self.endpoint_url(self.solr_assocs)\n        if use_amigo:\n            url = self.endpoint_url(self.amigo_solr_assocs)\n        return url", "response": "Returns the solr url to use for assocation of the subject - relation - object - set - of - association"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_source_gafs(group_metadata, target_dir, exclusions=[], base_download_url=None):\n    gaf_urls = [ (data, data[\"source\"]) for data in group_metadata[\"datasets\"] if data[\"type\"] == \"gaf\" and data[\"dataset\"] not in exclusions ]\n    # List of dataset metadata to gaf download url\n\n    click.echo(\"Found {}\".format(\", \".join( [ kv[0][\"dataset\"] for kv in gaf_urls ] )))\n    downloaded_paths = []\n    for dataset_metadata, gaf_url in gaf_urls:\n        dataset = dataset_metadata[\"dataset\"]\n        # Local target download path setup - path and then directories\n        path = download_a_dataset_source(group_metadata[\"id\"], dataset_metadata, target_dir, gaf_url, base_download_url=base_download_url)\n\n        if dataset_metadata[\"compression\"] == \"gzip\":\n            # Unzip any downloaded file that has gzip, strip of the gzip extension\n            unzipped = os.path.splitext(path)[0]\n            unzip(path, unzipped)\n            path = unzipped\n        else:\n            # otherwise file is coming in uncompressed. But we want to make sure\n            # to zip up the original source also\n            zipup(path)\n\n        downloaded_paths.append((dataset_metadata, path))\n\n    return downloaded_paths", "response": "Download all GAF files from GAFs in group_metadata."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n    parser = argparse.ArgumentParser(description='Wrapper for obographs assocmodel library'\n                                                 \"\"\"\n                                                 By default, ontologies and assocs are cached locally and synced from a remote sparql endpoint\n                                                 \"\"\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-r', '--resource', type=str, required=False,\n                        help='Name of ontology')\n    parser.add_argument('-f', '--file', type=str, required=False,\n                        help='Name of input file for associations - currently GAF is assumed')\n    parser.add_argument('-F', '--format', type=str, required=False,\n                        help='Format of assoc file. One of GAF, GPAD or HPOA')\n    parser.add_argument('-o', '--outfile', type=str, required=False,\n                        help='Path to output file')\n    parser.add_argument(\"--report-md\", type=str, required=False, dest=\"report_md\",\n                        help=\"Path to report markdown file\")\n    parser.add_argument(\"--report-json\", type=str, required=False, dest=\"report_json\",\n                        help=\"Path to report JSON file\")\n    parser.add_argument('-t', '--to', type=str, required=False,\n                        help='Output to (tree, dot, ...)')\n    parser.add_argument(\"--filter-out\", nargs=\"+\", required=False, default=[], metavar=\"EVIDENCE\",\n                        help=\"List of any evidence codes to filter out of the GAF. E.G. --filter-out IEA IMP\")\n    parser.add_argument(\"--filtered-file\", required=False, default=None, metavar=\"FILTERED_FILE\",\n                        help=\"File to write the filtered out evidence GAF to\")\n    parser.add_argument('-T', '--taxon', nargs='*', required=False,\n                        help='valid taxon (NCBITaxon ID) - validate against this')\n    parser.add_argument('--subject_prefix', nargs='*', required=False,\n                        help='E.g PomBase - validate against this')\n    parser.add_argument('--object_prefix', nargs='*', required=False,\n                        help='E.g GO - validate against this')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-command help')\n\n    parser_n = subparsers.add_parser('validate', help='Validate associations')\n    parser_n.set_defaults(function=validate_assocs)\n\n    parser_n = subparsers.add_parser('filter', help='Filter associations')\n    parser_n.set_defaults(function=filter_assocs)\n\n    parser_n = subparsers.add_parser('convert', help='Convert associations')\n    parser_n.set_defaults(function=convert_assocs)\n    parser_n.add_argument('-t', '--to', type=str, required=True,\n                          help='Format to convert to')\n\n    parser_n = subparsers.add_parser('map2slim', help='Map to a subset/slim')\n    parser_n.set_defaults(function=map2slim)\n    parser_n.add_argument('-p', '--properties', nargs='*', type=str, default=['subClassOf', 'BFO:0000050'],\n                          help='Properties')\n    parser_n.add_argument('-s', '--subset', type=str, required=True,\n                          help='subset (e.g. map2slim)')\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n\n    logging.info(\"Welcome!\")\n\n    handle = args.resource\n\n    # Ontology Factory\n    ofactory = OntologyFactory()\n    logging.info(\"Creating ont object from: {} {}\".format(handle, ofactory))\n    ont = ofactory.create(handle)\n    logging.info(\"ont: {}\".format(ont))\n\n    func = args.function\n\n    # Upper case all evidence codes\n    args.filter_out = [code.upper() for code in args.filter_out]\n\n    # set configuration\n    filtered_evidence_file = open(args.filtered_file, \"w\") if args.filtered_file else None\n    config = assocparser.AssocParserConfig(\n        valid_taxa=args.taxon,\n        ontology=ont,\n        class_idspaces=args.object_prefix,\n        entity_idspaces=args.subject_prefix,\n        filter_out_evidence=args.filter_out,\n        filtered_evidence_file=filtered_evidence_file\n    )\n    p = None\n    fmt = None\n    if args.format is None:\n        fmt = 'gaf'\n    else:\n        fmt = args.format.lower()\n\n    # TODO: use a factory\n    if fmt == 'gaf':\n        p = GafParser(dataset=args.file)\n    elif fmt == 'gpad':\n        p = GpadParser()\n    elif fmt == 'hpoa':\n        p = HpoaParser()\n    elif fmt == \"gpi\":\n        p = entityparser.GpiParser()\n        func = validate_entity\n\n    p.config = config\n\n    outfh = None\n    if args.outfile is not None:\n        two_mb = 2097152\n        outfh = open(args.outfile, \"w\", buffering=two_mb)\n    func(ont, args.file, outfh, p, args)\n    if filtered_evidence_file:\n        filtered_evidence_file.close()\n\n    if outfh is not None:\n        outfh.close()\n\n    if args.report_md is not None:\n        report_md = open(args.report_md, \"w\")\n        report_md.write(p.report.to_markdown())\n        report_md.close()\n    if args.report_json is not None:\n        report_json = open(args.report_json, \"w\")\n        report_json.write(json.dumps(p.report.to_report_json(), indent=4))\n        report_json.close()\n    if not (args.report_md or args.report_json):\n        print(p.report.to_markdown())", "response": "This function is the main entry point for the Assoc Parsing\n    script. It is used to parse the input file for the ontology and associated associations."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_annotation_sufficiency(\n            self,\n            profile: List[str],\n            negated_classes: List[str],\n            categories: Optional[List] = None,\n            negation_weight: Optional[float] = .25,\n            category_weight: Optional[float] = .5) -> AnnotationSufficiency:\n        \"\"\"\n        Given a list of individuals, return the simple, scaled, and categorical scores\n        \"\"\"\n\n        if categories is None:\n            categories = [enum.value for enum in HpoUpperLevel]\n        ic_map = self.ic_store.get_profile_ic(profile + negated_classes)\n\n        # Simple score is the weighted average of the present and\n        # explicitly stated negative/absent phenotypes\n        #\n        # Note that we're deviating from the publication\n        # to match the reference java implementation where\n        # mean_max_ic is replaced with max_max_ic:\n        # https://github.com/owlcollab/owltools/blob/452b4a/\n        # OWLTools-Sim/src/main/java/owltools/sim2/AbstractOwlSim.java#L1038\n        simple_score = self._get_simple_score(\n            profile, negated_classes, self.ic_store.statistics.mean_mean_ic,\n            self.ic_store.statistics.max_max_ic, self.ic_store.statistics.mean_sum_ic,\n            negation_weight, ic_map\n        )\n\n        categorical_score = self._get_categorical_score(\n            profile, negated_classes, categories,\n            negation_weight, ic_map\n        )\n        scaled_score = self._get_scaled_score(\n            simple_score, categorical_score, category_weight)\n\n        return AnnotationSufficiency(\n            simple_score=simple_score,\n            scaled_score=scaled_score,\n            categorical_score=categorical_score\n        )", "response": "Given a list of individuals return the simple scaled and categorical scores and categorical scores."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_simple_score(self,\n                          profile: List[str],\n                          negated_classes: List[str],\n                          bg_mean_pic: float,\n                          bg_mean_max_pic: float,\n                          bg_mean_sum_pic: float,\n                          negation_weight: Optional[float] = .25,\n                          ic_map: Optional[Dict[str, float]] = None) -> float:\n        \"\"\"\n        Simple score is the average of the relative\n        mean ic, max ic, and sum ic (relative to global stats)\n\n        :param ic_map: dictionary of class - information content mappings\n        :param bg_mean_pic: the average of the average IC in\n                            the background profile annotations\n        :param bg_mean_max_pic: max IC annotated to the background set of profiles\n        :param bg_mean_sum_pic: Average of the profile sum IC in background set\n        :param negation_weight: Average of the profile sum IC in background set\n        :param ic_map: Average of the profile sum IC in background set\n        :return: simple score (float)\n        \"\"\"\n        if ic_map is None:\n            ic_map = self.ic_store.get_profile_ic(profile + negated_classes)\n\n        pos_map = {cls: ic for cls, ic in ic_map.items() if cls in profile}\n        neg_map = {cls: ic for cls, ic in ic_map.items() if cls in negated_classes}\n\n        mean_ic = mean(pos_map.values()) if len(profile) > 0 else 0\n        max_ic = max(pos_map.values()) if len(profile) > 0 else 0\n        sum_ic = sum(pos_map.values()) if len(profile) > 0 else 0\n\n        if len(negated_classes) > 0:\n            weighted_ic = [ic * negation_weight for ic in neg_map.values()]\n            mean_ic = max([np.average([mean_ic, mean(neg_map.values())],\n                                      weights=[1, negation_weight]),\n                           mean_ic])\n            max_ic = max([max_ic] + weighted_ic)\n            sum_ic = sum_ic + sum(weighted_ic)\n\n        return mean([\n            min([mean_ic / bg_mean_pic, 1.0]),\n            min([max_ic / bg_mean_max_pic, 1.0]),\n            min([sum_ic / bg_mean_sum_pic, 1.0])\n        ])", "response": "Get the simple score of the current set of information content content."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_scaled_score(\n            simple_score: float,\n            categorical_score: float,\n            category_weight: Optional[float] = .5) -> float:\n        \"\"\"\n        Scaled score is the weighted average of the simple score and\n        categorical score\n        \"\"\"\n        return np.average(\n            [simple_score, categorical_score], weights=[1, category_weight]\n        )", "response": "Returns the scaled score of the given node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_categorical_score(\n            self,\n            profile: List,\n            negated_classes: List,\n            categories: List,\n            negation_weight: Optional[float] = 1,\n            ic_map: Optional[Dict[str, float]] = None) -> float:\n        \"\"\"\n        The average of the simple scores across a list of categories\n        \"\"\"\n\n        if ic_map is None:\n            ic_map = self.ic_store.get_profile_ic(profile + negated_classes)\n\n        scores = []\n\n        for cat in categories:\n            if cat not in self.ic_store.category_statistics:\n                raise ValueError(\"statistics for {} not indexed\".format(cat))\n\n            pos_profile = [cls for cls in profile\n                           if cls in self.ic_store.category_statistics[cat].descendants]\n            neg_profile = [cls for cls in negated_classes\n                           if cls in self.ic_store.category_statistics[cat].descendants]\n\n            # Note that we're deviating from the publication\n            # to match the reference java implementation where\n            # mean_max_ic is replaced by max_max_ic\n            scores.append(self._get_simple_score(\n                pos_profile, neg_profile,\n                self.ic_store.category_statistics[cat].mean_mean_ic,\n                self.ic_store.category_statistics[cat].max_max_ic,\n                self.ic_store.category_statistics[cat].mean_sum_ic,\n                negation_weight, ic_map\n            ))\n        return mean(scores)", "response": "Calculates the simple score across a list of categories and returns the average of the simple scores across a list of categories."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting a search using the specified list of ids and return a list of results.", "response": "def search(\n            self,\n            id_list: List[str],\n            negated_ids: Optional[List] = None,\n            limit: Optional[int] = 100,\n            taxon_filter: Optional[int]= None,\n            category_filter: Optional[str]= None,\n            method: Optional[SimAlgorithm] = SimAlgorithm.PHENODIGM\n    ) -> SimResult:\n        \"\"\"\n        Execute a search using sim_api, resolving non-phenotype ids to\n        phenotype lists then adding them to the profile (eg genes, diseases)\n\n        :raises NotImplementedError:\n            - If sim method or filters are not supported\n        \"\"\"\n\n        if negated_ids is None: negated_ids = []\n\n        if method not in self.sim_api.matchers():\n            raise NotImplementedError(\"Sim method not implemented \"\n                                      \"in {}\".format(str(self.sim_api)))\n\n        # Determine if entity is a phenotype or individual containing\n        # a pheno profile (gene, disease, case, etc)\n        pheno_list = PhenoSimEngine._resolve_nodes_to_phenotypes(id_list)\n\n        if taxon_filter is not None or category_filter is not None:\n            if not isinstance(self.sim_api, FilteredSearchable):\n                raise NotImplementedError(\"filtered search not implemented \"\n                                          \"in {}\".format(str(self.sim_api)))\n            search_result = self.sim_api.filtered_search(\n                pheno_list, negated_ids, limit, taxon_filter, category_filter, method\n            )\n        else:\n            search_result = self.sim_api.search(pheno_list, negated_ids, limit, method)\n\n        return search_result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute one or more comparisons using sim_api", "response": "def compare(self,\n                reference_ids: List,\n                query_profiles: List[List],\n                method: Optional[SimAlgorithm] = SimAlgorithm.PHENODIGM) -> SimResult:\n        \"\"\"\n        Execute one or more comparisons using sim_api\n        :param reference_ids: a list of phenotypes or ids that comprise\n                              one or more phenotypes\n        :param query_profiles: a list of lists of phenotypes or ids\n                                   that comprise one or more phenotypes\n        :param method: comparison method\n        :return: SimResult object\n        :raises NotImplementedError: If sim method or filters are not supported\n        \"\"\"\n        if method not in self.sim_api.matchers():\n            raise NotImplementedError(\"Sim method not implemented \"\n                                      \"in {}\".format(str(self.sim_api)))\n\n        is_first_result = True\n        comparisons = None\n\n        reference_phenos = PhenoSimEngine._resolve_nodes_to_phenotypes(reference_ids)\n        for query_profile in query_profiles:\n            query_phenos = PhenoSimEngine._resolve_nodes_to_phenotypes(query_profile)\n            sim_result = self.sim_api.compare(reference_phenos, query_phenos, method)\n            if len(query_profile) > 1:\n                id = \" + \".join(query_profile)\n                sim_result.matches[0].id = id\n                sim_result.matches[0].label = id\n            else:\n                node = typed_node_from_id(query_profile[0])\n                sim_result.matches[0].id = node.id\n                sim_result.matches[0].label = node.label\n                sim_result.matches[0].type = node.type\n                sim_result.matches[0].taxon = node.taxon\n\n            if is_first_result:\n                comparisons = sim_result\n                is_first_result = False\n            else:\n                comparisons.matches.append(sim_result.matches[0])\n                comparisons.query.target_ids.append(sim_result.query.target_ids[0])\n\n        if len(reference_ids) == 1:\n            comparisons.query.reference = typed_node_from_id(reference_ids[0])\n        else:\n            reference_id = \" + \".join(reference_ids)\n            comparisons.query.reference = TypedNode(\n                id=reference_id,\n                label=reference_id,\n                type='unknown'\n            )\n\n        return comparisons"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_nodes_to_phenotypes(id_list: List[str]) -> List[str]:\n        pheno_list = []\n        node_types = get_id_type_map(id_list)\n\n        for node in id_list:\n            if 'phenotype' in node_types[node]:\n                pheno_list.append(node)\n            else:\n                phenotypes = get_objects_for_subject(\n                    subject=node, object_category='phenotype', relation='RO:0002200'\n                )\n                pheno_list = pheno_list + phenotypes\n        return pheno_list", "response": "Given a list of ids of unknown type determine which ids are phenotypes and add them to the list of nodes that are associated with the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a single entity to a line in the output file.", "response": "def write_entity(self, entity):\n        \"\"\"\n        Write a single entity to a line in the output file\n        \"\"\"\n        db, db_object_id = self._split_prefix(entity)\n        taxon = normalize_taxon(entity[\"taxon\"][\"id\"])\n\n        vals = [\n            db,\n            db_object_id,\n            entity.get('label'),\n            entity.get('full_name'),\n            entity.get('synonyms'),\n            entity.get('type'),\n            taxon,\n            entity.get('parents'),\n            entity.get('xrefs'),\n            entity.get('properties')\n        ]\n\n        self._write_row(vals)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search(self,\n               id_list: Iterable,\n               negated_classes: Iterable,\n               limit: Optional[int],\n               method: Optional) -> List[SimResult]:\n        \"\"\"\n        Given an input list of classes, searches for similar lists of classes\n        and provides a ranked list of matches\n        \"\"\"\n        pass", "response": "Search for similar lists of classes and provides a ranked list of matches."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving an input iterable of classes or individuals, provides a ranking of similar profiles", "response": "def filtered_search(self,\n               id_list: Iterable,\n               negated_classes: Iterable,\n               limit: Optional[int],\n               taxon_filter: Optional,\n               category_filter: Optional,\n               method: Optional) -> SimResult:\n        \"\"\"\n        Given an input iterable of classes or individuals,\n        provides a ranking of similar profiles\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompare two lists of entities and returns their similarity", "response": "def compare(self,\n                reference_ids: Iterable,\n                query_profiles: Iterable[Iterable],\n                method: Optional) -> SimResult:\n        \"\"\"\n        Given two lists of entities (classes, individuals),\n        resolves them to some type (phenotypes, go terms, etc) and\n        returns their similarity\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsearch for a set of target classes or individuals.", "response": "def search(self,\n               id_list: Iterable,\n               negated_ids: Iterable,\n               limit: Optional[int],\n               taxon_filter: Optional,\n               category_filter: Optional,\n               method: Optional)-> SimResult:\n        \"\"\"\n        Given an input iterable of classes or individuals,\n        resolves to target classes (phenotypes, go terms, etc)\n        and provides a ranking of similar profiles\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_ontology(self, ont):\n        self.merged_ontology.merge([ont])\n        syns = ont.all_synonyms(include_label=True)\n        \n        include_id = self._is_meaningful_ids()\n        logging.info(\"Include IDs as synonyms: {}\".format(include_id))\n        if include_id:\n            for n in ont.nodes():\n                v = n\n                # Get fragment\n                if v.startswith('http'):\n                    v = re.sub('.*/','',v)\n                    v = re.sub('.*#','',v)\n                syns.append(Synonym(n, val=v, pred='label'))\n        \n        logging.info(\"Indexing {} syns in {}\".format(len(syns),ont))\n        logging.info(\"Distinct lexical values: {}\".format(len(self.lmap.keys())))\n        for syn in syns:\n            self.index_synonym(syn, ont)\n        for nid in ont.nodes():\n            self.id_to_ontology_map[nid].append(ont)", "response": "Adds an ontology to the index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef index_synonym(self, syn, ont):\n        if not syn.val:\n            if syn.pred == 'label':\n                if not self._is_meaningful_ids():\n                    if not ont.is_obsolete(syn.class_id):\n                        pass\n                        #logging.error('Use meaningful ids if label not present: {}'.format(syn))\n            else:\n                logging.warning(\"Incomplete syn: {}\".format(syn))\n            return\n        if self.exclude_obsolete and ont.is_obsolete(syn.class_id):\n            return\n\n        syn.ontology = ont\n        prefix,_ = ont.prefix_fragment(syn.class_id)\n        \n        v = syn.val\n\n        caps_match = re.match('[A-Z]+',v)\n        if caps_match:\n            # if > 75% of length is caps, assume abbreviation\n            if caps_match.span()[1] >= len(v)/3:\n                syn.is_abbreviation(True)\n                \n        # chebi 'synonyms' are often not real synonyms\n        # https://github.com/ebi-chebi/ChEBI/issues/3294\n        if not re.match('.*[a-zA-Z]',v):\n            if prefix != 'CHEBI':\n                logging.warning('Ignoring suspicous synonym: {}'.format(syn))\n            return\n        \n        v = self._standardize_label(v)\n\n        # TODO: do this once ahead of time\n        wsmap = {}\n        for w,s in self.wsmap.items():\n            wsmap[w] = s\n        for ss in self._get_config_val(prefix,'synsets',[]):\n            # TODO: weights\n            wsmap[ss['synonym']] = ss['word']\n        nv = self._normalize_label(v, wsmap)\n        \n        self._index_synonym_val(syn, v)\n        nweight = self._get_config_val(prefix, 'normalized_form_confidence', 0.8)\n        if nweight > 0 and not syn.is_abbreviation():\n            if nv != v:\n                nsyn = Synonym(syn.class_id,\n                               val=syn.val,\n                               pred=syn.pred,\n                               lextype=syn.lextype,\n                               ontology=ont,\n                               confidence=syn.confidence * nweight)\n                self._index_synonym_val(nsyn, nv)", "response": "Index a synonym from a ChEBI object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnormalizes a label to be a string", "response": "def _normalize_label(self, s, wsmap):\n        \"\"\"\n        normalized form of a synonym\n        \"\"\"\n        toks = []\n        for tok in list(set(self.npattern.sub(' ', s).split(' '))):\n            if tok in wsmap:\n                tok=wsmap[tok]\n            if tok != \"\":\n                toks.append(tok)\n        toks.sort()\n        return \" \".join(toks)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an xref graph (see ref:`get_xref_graph`), this will adjust scores based on the semantic similarity of matches.", "response": "def score_xrefs_by_semsim(self, xg, ont=None):\n        \"\"\"\n        Given an xref graph (see ref:`get_xref_graph`), this will adjust scores based on\n        the semantic similarity of matches.\n        \"\"\"\n        logging.info(\"scoring xrefs by semantic similarity for {} nodes in {}\".format(len(xg.nodes()), ont))\n        for (i,j,d) in xg.edges(data=True):\n            pfx1 = self._id_to_ontology(i)\n            pfx2 = self._id_to_ontology(j)\n            ancs1 = self._blanket(i)\n            ancs2 = self._blanket(j)\n            s1,_,_ = self._sim(xg, ancs1, ancs2, pfx1, pfx2)\n            s2,_,_ = self._sim(xg, ancs2, ancs1, pfx2, pfx1)\n            s = 1 - ((1-s1) * (1-s2))\n            logging.debug(\"Score {} x {} = {} x {} = {} // {}\".format(i,j,s1,s2,s, d))\n            xg[i][j][self.SIMSCORES] = (s1,s2)\n            xg[i][j][self.SCORE] *= s"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the similarity between two lineages.", "response": "def _sim(self, xg, ancs1, ancs2, pfx1, pfx2):\n        \"\"\"\n        Compare two lineages\n        \"\"\"\n        xancs1 = set()\n        for a in ancs1:\n            if a in xg:\n                # TODO: restrict this to neighbors in single ontology\n                for n in xg.neighbors(a):\n                    pfx = self._id_to_ontology(n)\n                    if pfx == pfx2:\n                        xancs1.add(n)\n        logging.debug('SIM={}/{} ## {}'.format(len(xancs1.intersection(ancs2)), len(xancs1), xancs1.intersection(ancs2), xancs1))\n        n_shared = len(xancs1.intersection(ancs2))\n        n_total = len(xancs1)\n        return (1+n_shared) / (1+n_total), n_shared, n_total"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_to_xrefs(self, xg1, xg2):\n        ont = self.merged_ontology\n        for (i,j,d) in xg1.edges(data=True):\n            ont_left = self._id_to_ontology(i)\n            ont_right = self._id_to_ontology(j)\n            unique_lr = True\n            num_xrefs_left = 0\n            same_left = False\n            if i in xg2:\n                for j2 in xg2.neighbors(i):\n                    ont_right2 = self._id_to_ontology(j2)\n                    if ont_right2 == ont_right:\n                        unique_lr = False\n                        num_xrefs_left += 1\n                        if j2 == j:\n                            same_left = True\n            unique_rl = True\n            num_xrefs_right = 0\n            same_right = False\n            if j in xg2:\n                for i2 in xg2.neighbors(j):\n                    ont_left2 = self._id_to_ontology(i2)\n                    if ont_left2 == ont_left:\n                        unique_rl = False\n                        num_xrefs_right += 1\n                        if i2 == i:\n                            same_right = True\n\n            (x,y) = d['idpair']\n            xg1[x][y]['left_novel'] = num_xrefs_left==0\n            xg1[x][y]['right_novel'] = num_xrefs_right==0\n            xg1[x][y]['left_consistent'] = same_left\n            xg1[x][y]['right_consistent'] = same_right", "response": "Compares a base xref graph with another xref graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nassigning the best matches for each node in the xref graph to the edges that are best for each edge.", "response": "def assign_best_matches(self, xg):\n        \"\"\"\n        For each node in the xref graph, tag best match edges\n        \"\"\"\n        logging.info(\"assigning best matches for {} nodes\".format(len(xg.nodes())))\n        for i in xg.nodes():\n            xrefmap = self._neighborscores_by_ontology(xg, i)\n            for (ontid,score_node_pairs) in xrefmap.items():\n                score_node_pairs.sort(reverse=True)\n                (best_score,best_node) = score_node_pairs[0]\n                logging.info(\"BEST for {}: {} in {} from {}\".format(i, best_node, ontid, score_node_pairs))\n                edge = xg[i][best_node]\n                dirn = self._dirn(edge, i, best_node)\n                best_kwd = 'best_' + dirn\n                if len(score_node_pairs) == 1 or score_node_pairs[0] > score_node_pairs[1]:\n                    edge[best_kwd] = 2\n                else:\n                    edge[best_kwd] = 1\n                for (score,j) in score_node_pairs:\n                    edge_ij = xg[i][j]\n                    dirn_ij = self._dirn(edge_ij, i, j)\n                    edge_ij['cpr_'+dirn_ij] = score / sum([s for s,_ in score_node_pairs])\n        for (i,j,edge) in xg.edges(data=True):\n            # reciprocal score is set if (A) i is best for j, and (B) j is best for i\n            rs = 0\n            if 'best_fwd' in edge and 'best_rev' in edge:\n                rs = edge['best_fwd'] * edge['best_rev']\n            edge['reciprocal_score'] = rs\n            edge['cpr'] = edge['cpr_fwd'] * edge['cpr_rev']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _best_match_syn(self, sx, sys, scope_map):\n        SUBSTRING_WEIGHT = 0.2\n        WBEST = None\n        sbest = None\n        sxv = self._standardize_label(sx.val)\n        sxp = self._id_to_ontology(sx.class_id)\n        for sy in sys:\n            syv = self._standardize_label(sy.val)\n            syp = self._id_to_ontology(sy.class_id)\n            W = None\n            if sxv == syv:\n                confidence = sx.confidence * sy.confidence\n                if sx.is_abbreviation() or sy.is_abbreviation:\n                    confidence *= self._get_config_val(sxp, 'abbreviation_confidence', 0.5)\n                    confidence *= self._get_config_val(syp, 'abbreviation_confidence', 0.5)\n                W = scope_map[sx.scope()][sy.scope()] + logit(confidence/2)\n            elif sxv in syv:\n                W = np.array((-SUBSTRING_WEIGHT, SUBSTRING_WEIGHT, 0, 0))\n            elif syv in sxv:\n                W = np.array((SUBSTRING_WEIGHT, -SUBSTRING_WEIGHT, 0, 0))\n            if W is not None:\n                # The best match is determined by the highest magnitude weight\n                if WBEST is None or max(abs(W)) > max(abs(WBEST)):\n                    WBEST = W\n                    sbest = sy\n        return WBEST, sbest", "response": "This function returns the best match for a single entry in the system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple of sub sup eq other for a mapping between x and y.", "response": "def weighted_axioms(self, x, y, xg):\n        \"\"\"\n        return a tuple (sub,sup,equiv,other) indicating estimated prior probabilities for an interpretation of a mapping\n        between x and y.\n\n        See kboom paper\n        \"\"\"\n        # TODO: allow additional weighting\n        # weights are log odds w=log(p/(1-p))\n        # (Sub,Sup,Eq,Other)\n        scope_pairs = [\n            ('label',   'label',   0.0, 0.0, 3.0,-0.8), \n            ('label',   'exact',   0.0, 0.0, 2.5,-0.5), \n            ('label',   'broad',  -1.0, 1.0, 0.0, 0.0), \n            ('label',   'narrow',  1.0,-1.0, 0.0, 0.0), \n            ('label',   'related', 0.0, 0.0, 0.0, 0.0), \n            ('exact',   'exact',   0.0, 0.0, 2.5,-0.5), \n            ('exact',   'broad',  -1.0, 1.0, 0.0, 0.0), \n            ('exact',   'narrow',  1.0,-1.0, 0.0, 0.0), \n            ('exact',   'related', 0.0, 0.0, 0.0, 0.0), \n            ('related', 'broad',  -0.5, 0.5, 0.0, 0.0), \n            ('related', 'narrow',  0.5,-0.5, 0.0, 0.0), \n            ('related', 'related', 0.0, 0.0, 0.0, 0.0), \n            ('broad',   'broad',   0.0, 0.0, 0.0, 1.0), \n            ('broad',   'narrow', -0.5, 0.5, 0.0, 0.2), \n            ('narrow',  'narrow',  0.0, 0.0, 0.0, 0.0)\n        ]\n        # populate symmetric lookup matrix\n        scope_map = defaultdict(dict)\n        for (l,r,w1,w2,w3,w4) in scope_pairs:\n            l = l.upper()\n            r = r.upper()\n            scope_map[l][r] = np.array((w1,w2,w3,w4))\n            scope_map[r][l] = np.array((w2,w1,w3,w4))\n\n        # TODO: get prior based on ontology pair\n        # cumulative sum of weights\n        WS = None\n        pfx1 = self._id_to_ontology(x)\n        pfx2 = self._id_to_ontology(y)\n        for mw in self.config.get('match_weights', []):\n            mpfx1 = mw.get('prefix1','')\n            mpfx2 = mw.get('prefix2','')\n            X = np.array(mw['weights'])\n            if mpfx1 == pfx1 and mpfx2 == pfx2:\n                WS = X\n            elif mpfx2 == pfx1 and mpfx1 == pfx2:\n                WS = self._flipweights(X)\n            elif mpfx1 == pfx1 and mpfx2 == '' and WS is None:\n                WS = X\n            elif mpfx2 == pfx1 and mpfx1 == '' and WS is None:\n                WS = self._flipweights(X)\n\n        if WS is None:\n            WS = np.array((0.0, 0.0, 0.0, 0.0))\n        # defaults\n        WS += np.array(self.config.get('default_weights', [0.0, 0.0, 1.5, -0.1]))\n        logging.info('WS defaults={}'.format(WS))\n\n        for xw in self.config.get('xref_weights', []):\n            left = xw.get('left','')\n            right = xw.get('right','')\n            X = np.array(xw['weights'])\n            if x == left and y == right:\n                WS += X\n                logging.info('MATCH: {} for {}-{}'.format(X, x, y))\n            elif y == left and x == right:\n                WS += self._flipweights(X)\n                logging.info('IMATCH: {}'.format(X))\n\n        smap = self.smap\n        # TODO: symmetrical\n        WT = np.array((0.0, 0.0, 0.0, 0.0))\n        WBESTMAX = np.array((0.0, 0.0, 0.0, 0.0))\n        n = 0\n        for sx in smap[x]:\n            WBEST, _ = self._best_match_syn(sx, smap[y], scope_map)\n            if WBEST is not None:\n                WT += WBEST\n                n += 1\n                if max(abs(WBEST)) > max(abs(WBESTMAX)):\n                    WBESTMAX = WBEST\n        for sy in smap[y]:\n            WBEST, _ = self._best_match_syn(sy, smap[x], scope_map)\n            if WBEST is not None:\n                WT += WBEST\n                n += 1\n        # average best match\n        if n > 0:\n            logging.info('Adding BESTMAX={}'.format(WBESTMAX))\n            WS += WBESTMAX\n                    \n        # TODO: xref, many to many\n        WS += self._graph_weights(x, y, xg)\n        # TODO: include additional defined weights, eg ORDO\n        logging.info('Adding WS, gw={}'.format(WS))\n\n        # jaccard similarity\n        (ss1,ss2) = xg[x][y][self.SIMSCORES]\n        WS[3] += ((1-ss1) + (1-ss2)) / 2\n\n        # reciprocal best hits are higher confidence of equiv\n        rs = xg[x][y]['reciprocal_score']\n        if rs == 4:\n            WS[2] += 0.5\n        if rs == 0:\n            WS[2] -= 0.2\n        \n        #P = np.expit(WS)\n        P = 1/(1+np.exp(-WS))\n        logging.info('Final WS={}, init P={}'.format(WS, P))\n        # probs should sum to 1.0\n        P = P / np.sum(P)\n        return P"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grouped_mappings(self,id):\n        g = self.get_xref_graph()\n        m = {}\n        for n in g.neighbors(id):\n            [prefix, local] = n.split(':')\n            if prefix not in m:\n                m[prefix] = []\n            m[prefix].append(n)\n        return m", "response": "return all mappings for a node grouped by ID prefix"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cliques(self, xg):\n        g = nx.DiGraph()\n        for (x,y) in self.merged_ontology.get_graph().edges():\n            g.add_edge(x,y)\n        for (x,y) in xg.edges():\n            g.add_edge(x,y)\n            g.add_edge(y,x)\n        return list(strongly_connected_components(g))", "response": "Return all equivalence set cliques assuming each edge in the xref graph is treated as equivalent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    parser = argparse.ArgumentParser(description='Phenologs'\n                                                 \"\"\"\n                                                 By default, ontologies are cached locally and synced from a remote sparql endpoint\n                                                 \"\"\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-r', '--resource1', type=str, required=False,\n                        help='Name of ontology1')\n    parser.add_argument('-R', '--resource2', type=str, required=False,\n                        help='Name of ontology2')\n    parser.add_argument('-T', '--taxon', type=str, default='NCBITaxon:10090', required=False,\n                        help='NCBITaxon ID')\n    parser.add_argument('-s', '--search', type=str, default='', required=False,\n                        help='Search type. p=partial, r=regex')\n    parser.add_argument('-b', '--background', type=str, default=None, required=False,\n                        help='Class to use for background')\n    parser.add_argument('-p', '--pthreshold', type=float, default=0.05, required=False,\n                        help='P-value threshold')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n    parser.add_argument('ids',nargs='*')\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    if args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    logging.info(\"Welcome!\")\n\n    ofactory = OntologyFactory()\n    afactory = AssociationSetFactory()\n    handle = args.resource1\n    ont1 = ofactory.create(args.resource1)\n    ont2 = ofactory.create(args.resource2)\n    logging.info(\"onts: {} {}\".format(ont1, ont2))\n    searchp = args.search\n\n    category = 'gene'\n\n    aset1 = afactory.create(ontology=ont1,\n                            subject_category=category,\n                            object_category='phenotype',\n                            taxon=args.taxon)\n    aset2 = afactory.create(ontology=ont2,\n                            subject_category=category,\n                            object_category='function',\n                            taxon=args.taxon)\n\n    bg_cls = None\n    if args.background is not None:\n        bg_ids = resolve(ont1,[args.background],searchp)\n        if len(bg_ids) == 0:\n            logging.error(\"Cannnot resolve: '{}' using {} in {}\".format(args.background, searchp, ont1))\n            sys.exit(1)\n        elif len(bg_ids) > 1:\n            logging.error(\"Multiple matches: '{}' using {} MATCHES={}\".format(args.background, searchp,bg_ids))\n            sys.exit(1)\n        else:\n            logging.info(\"Background: {}\".format(bg_cls))\n            [bg_cls] = bg_ids\n    \n    for id in resolve(ont1,args.ids,searchp):\n\n        sample = aset1.query([id],[])\n        print(\"Gene set class:{} Gene set: {}\".format(id, sample))\n        bg = None\n        if bg_cls is not None:\n            bg = aset1.query([bg_cls],[])\n            print(\"BACKGROUND SUBJECTS: {}\".format(bg))\n\n        rs = aset2.enrichment_test(sample, bg, threshold=args.pthreshold, labels=True)\n        print(\"RESULTS: {} < {}\".format(len(rs), args.pthreshold))\n        for r in rs:\n            print(str(r))", "response": "Entry point for the applet command line interface"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd triples to an ontology object.", "response": "def add_triples(self, ontol):\n        \"\"\"\n        Adds triples to an ontology object.\n\n        Currently assumes gocam/lego-style\n        \"\"\"\n        rg = self.rdfgraph\n        g = ontol.get_graph()\n        typemap = {}\n        inds = rg.subjects(RDF.type, OWL.NamedIndividual)\n        for s in inds:\n            for (s,p,o) in rg.triples((s,None,None)):\n                s_id = id(s)\n                p_id = id(p)\n                g.add_node(s_id)\n                if isinstance(o,URIRef):\n                    o_id = id(o)\n                    if p == RDF.type:\n                        if o != OWL.NamedIndividual:\n                            if s_id not in typemap:\n                                typemap[s_id] = []\n                            typemap[s_id].append(o_id)\n                    else:\n                        g.add_edge(o_id,s_id,pred=p_id)\n\n        # propagate label from type\n        for s in typemap.keys():\n            g.nodes[s]['types'] = typemap[s]\n            if self.tbox_ontology is not None:\n                if 'label' not in g.nodes[s]:\n                    g.nodes[s]['label'] = \";\".join([self.tbox_ontology.label(x) for x in typemap[s] if self.tbox_ontology.label(x) is not None])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, ontol, **args):\n        s = self.render(ontol, **args)\n        if self.outfile is None:\n            print(s)\n        else:\n            f = open(self.outfile, 'w')\n            f.write(s)\n            f.close()", "response": "Write a single object to file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender a subgraph of a set of nodes.", "response": "def render_subgraph(self, ontol, nodes, **args):\n        \"\"\"\n        Render a `ontology` object after inducing a subgraph\n        \"\"\"\n        subont = ontol.subontology(nodes, **args)\n        return self.render(subont, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a subgraph of a set of nodes to the log.", "response": "def write_subgraph(self, ontol, nodes, **args):\n        \"\"\"\n        Write a `ontology` object after inducing a subgraph\n        \"\"\"\n        subont = ontol.subontology(nodes, **args)\n        self.write(subont, **args)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_relation(self, r, **args):\n        if r is None:\n            return \".\"\n        m = self.config.relsymbolmap\n        if r in m:\n            return m[r]\n        return r", "response": "Render an object property r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a node object", "response": "def render_noderef(self, ontol, n, query_ids=None, **args):\n        \"\"\"\n        Render a node object\n        \"\"\"\n        if query_ids is None:\n            query_ids = []\n        marker = \"\"\n        if n in query_ids:\n            marker = \" * \"\n        label = ontol.label(n)\n        s = None\n        if label is not None:\n            s = '{} ! {}{}'.format(n,\n                                   label,\n                                   marker)\n        else:\n            s = str(n)\n        if self.config.show_text_definition:\n            td = ontol.text_definition(n)\n            if td:\n                s += ' \"{}\"'.format(td.val)\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a GraphRenderer object for the given format.", "response": "def create(fmt):\n        \"\"\"\n        Creates a GraphRenderer\n        \"\"\"\n        w = None\n        if fmt == 'tree':\n            w = AsciiTreeGraphRenderer()\n        elif fmt == 'dot':\n            w = DotGraphRenderer(image_format='dot')\n        elif fmt == 'png':\n            w = DotGraphRenderer(image_format='png')\n        elif fmt == 'ndot':\n            w = NativeDotGraphRenderer()\n        elif fmt == 'obo':\n            w = OboFormatGraphRenderer()\n        elif fmt == 'obog':\n            w = OboJsonGraphRenderer()\n        else:\n            w = SimpleListGraphRenderer()\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a single line of a GPAD file and returns a tuple of parsed_line associations", "response": "def parse_line(self, line):\n        \"\"\"Parses a single line of a GPAD.\n\n        Return a tuple `(processed_line, associations)`. Typically\n        there will be a single association, but in some cases there\n        may be none (invalid line) or multiple (disjunctive clause in\n        annotation extensions)\n\n        Note: most applications will only need to call this directly if they require fine-grained control of parsing. For most purposes,\n        :method:`parse_file` can be used over the whole file\n\n        Arguments\n        ---------\n        line : str\n            A single tab-seperated line from a GPAD file\n\n        \"\"\"\n        parsed = super().validate_line(line)\n        if parsed:\n            return parsed\n\n        if self.is_header(line):\n            return assocparser.ParseResult(line, [{ \"header\": True, \"line\": line.strip() }], False)\n\n        vals = [el.strip() for el in line.split(\"\\t\")]\n        if len(vals) < 10 or len(vals) > 12:\n            self.report.error(line, assocparser.Report.WRONG_NUMBER_OF_COLUMNS, \"\",\n                msg=\"There were {columns} columns found in this line, and there should be between 10 and 12\".format(columns=len(vals)))\n            return assocparser.ParseResult(line, [], True)\n\n        if len(vals) < 12:\n            vals += [\"\"] * (12 - len(vals))\n\n        [db,\n         db_object_id,\n         qualifier,\n         goid,\n         reference,\n         evidence,\n         withfrom,\n         interacting_taxon_id,\n         date,\n         assigned_by,\n         annotation_xp,\n         annotation_properties] = vals\n\n        split_line = assocparser.SplitLine(line=line, values=vals, taxon=\"\")\n\n        id = self._pair_to_id(db, db_object_id)\n        if not self._validate_id(id, split_line, context=ENTITY):\n            return assocparser.ParseResult(line, [], True)\n\n        if not self._validate_id(goid, split_line, context=ANNOTATION):\n            return assocparser.ParseResult(line, [], True)\n\n        valid_goid = self._validate_ontology_class_id(goid, split_line)\n        if valid_goid == None:\n            return assocparser.ParseResult(line, [], True)\n        goid = valid_goid\n\n        date = self._normalize_gaf_date(date, split_line)\n\n        if reference == \"\":\n            self.report.error(line, Report.INVALID_ID, \"EMPTY\", \"reference column 6 is empty\")\n            return assocparser.ParseResult(line, [], True)\n\n        self._validate_id(evidence, split_line)\n\n        interacting_taxon = None if interacting_taxon_id == \"\" else interacting_taxon_id\n        if interacting_taxon != None:\n            interacting_taxon = self._taxon_id(interacting_taxon_id, split_line)\n            if interacting_taxon == None:\n                self.report.error(line, assocparser.Report.INVALID_TAXON, interacting_taxon_id,\n                                    msg=\"Taxon ID is invalid\")\n                return assocparser.ParseResult(line, [], True)\n\n        #TODO: ecomap is currently one-way only\n        #ecomap = self.config.ecomap\n        #if ecomap != None:\n        #    if ecomap.ecoclass_to_coderef(evidence) == (None,None):\n        #        self.report.error(line, Report.UNKNOWN_EVIDENCE_CLASS, evidence,\n        #                          msg=\"Expecting a known ECO class ID\")\n\n        ## --\n        ## qualifier\n        ## --\n        negated, relation, other_qualifiers = self._parse_qualifier(qualifier, None)\n\n\n        # Reference Column\n        references = self.validate_pipe_separated_ids(reference, split_line)\n        if references == None:\n            # Reporting occurs in above function call\n            return assocparser.ParseResult(line, [], True)\n\n        # With/From\n        withfroms = self.validate_pipe_separated_ids(withfrom, split_line, empty_allowed=True, extra_delims=\",\")\n        if withfroms == None:\n            # Reporting occurs in above function call\n            return assocparser.ParseResult(line, [], True)\n\n\n        ## --\n        ## parse annotation extension\n        ## See appending in http://doi.org/10.1186/1471-2105-15-155\n        ## --\n        object_or_exprs = self._parse_full_extension_expression(annotation_xp, line=split_line)\n\n        subject_symbol = id\n        subject_fullname = id\n        subject_synonyms = []\n        if self.gpi is not None:\n            gp = self.gpi.get(id, {})\n            if gp is not {}:\n                subject_symbol = gp[\"symbol\"]\n                subject_fullname = gp[\"name\"]\n                subject_synonyms = gp[\"synonyms\"].split(\"|\")\n\n        assoc = {\n            'source_line': line,\n            'subject': {\n                'id': id,\n                'label': subject_symbol,\n                'fullname': subject_fullname,\n                'synonyms': subject_synonyms,\n                'taxon': {\n                    'id': interacting_taxon\n                },\n            },\n            'object': {\n                'id':goid\n            },\n            'negated': negated,\n            'relation': {\n                'id': relation\n            },\n            'interacting_taxon': interacting_taxon,\n            'evidence': {\n                'type': evidence,\n                'with_support_from': withfroms,\n                'has_supporting_reference': references\n            },\n            'subject_extensions': [],\n            'object_extensions': {},\n            'aspect': self.compute_aspect(goid),\n            'provided_by': assigned_by,\n            'date': date,\n        }\n        if len(other_qualifiers) > 0:\n            assoc['qualifiers'] = other_qualifiers\n        if object_or_exprs is not None and len(object_or_exprs) > 0:\n            assoc['object_extensions'] = {'union_of': object_or_exprs}\n\n\n        return assocparser.ParseResult(line, [assoc], False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_user_agent(name=\"ontobio\", version=ontobio_version, modules=None, caller_name=None):\n\n    user_agent_array = [\"{}/{}\".format(name, version)]\n    if modules:\n        module_info_array = []\n        for m in modules:\n            mod_name = m.__name__\n            mod_version = None\n            if hasattr(m, 'get_version'):\n                mod_version = m.get_version()\n            else:\n                mod_version = m.__version__\n            module_info_array.append(\"{}/{}\".format(mod_name, mod_version))\n\n        if caller_name:\n            module_info_array.append(caller_name)\n\n        user_agent_array.append(\"({})\".format('; '.join(module_info_array)))\n    else:\n        if caller_name:\n            user_agent_array.append(\"({})\".format(caller_name))\n\n    return ' '.join(user_agent_array)", "response": "Create a User - Agent string for the specified modules and caller_name."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for similar profiles in a list of classes or individuals.", "response": "def search(self,\n               id_list: List,\n               negated_classes: List,\n               limit: Optional[int],\n               method: Optional) -> List[SimResult]:\n        \"\"\"\n        Given an input list of classes or individuals,\n        provides a ranking of similar profiles\n        \"\"\"\n        raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare(self,\n                query_classes: List,\n                reference_classes: List,\n                method: Optional) -> SimResult:\n        \"\"\"\n        Given two lists of entites (classes, individual)\n        return their similarity\n        \"\"\"\n        raise NotImplementedError", "response": "Compare two lists of entites and return their similarity"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of all the available matchers in owlsim3.", "response": "def matchers() -> List[SimAlgorithm]:\n        \"\"\"\n        Matchers in owlsim3\n        Could be dynamically retrieved from\n        http://owlsim3.monarchinitiative.org/api/match/matchers\n        \"\"\"\n        return [\n            SimAlgorithm.PHENODIGM,\n            SimAlgorithm.BAYES_NETWORK,\n            SimAlgorithm.BAYES_VARIABLE,\n            SimAlgorithm.NAIVE_BAYES_THREE_STATE,\n            SimAlgorithm.NAIVE_BAYES_TWO_STATE,\n            SimAlgorithm.NAIVE_BAYES_TWO_STATE_NO_BLANKET,\n            SimAlgorithm.GRID,\n            SimAlgorithm.GRID_NEGATED,\n            SimAlgorithm.JACCARD,\n            SimAlgorithm.MAX_INFORMATION\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a set of nodes extract a subgraph that contains only non - informative nodes.", "response": "def get_minimal_subgraph(g, nodes):\n    \"\"\"\n    given a set of nodes, extract a subgraph that excludes non-informative nodes - i.e.\n    those that are not MRCAs of pairs of existing nodes.\n    \n    Note: no property chain reasoning is performed. As a result, edge labels are lost.\n    \"\"\"\n    logging.info(\"Slimming {} to {}\".format(g,nodes))\n    # maps ancestor nodes to members of the focus node set they subsume\n    mm = {}\n    subnodes = set()\n    for n in nodes:\n        subnodes.add(n)\n        ancs = nx.ancestors(g, n)\n        ancs.add(n)\n        for a in ancs:\n            subnodes.add(a)\n            if a not in mm:\n                mm[a] = set()\n            mm[a].add(n)\n\n    # merge graph\n    egraph = nx.MultiDiGraph()\n\n    # TODO: ensure edge labels are preserved\n    for a, aset in mm.items():\n        for p in g.predecessors(a):\n            logging.info(\" cmp {} -> {} // {} {}\".format(len(aset),len(mm[p]), a, p))\n            if p in mm and len(aset) == len(mm[p]):\n                egraph.add_edge(p, a)\n                egraph.add_edge(a, p)\n                logging.info(\"will merge {} <-> {} (members identical)\".format(p,a))\n\n    nmap = {}\n    leafmap = {}\n    disposable = set()\n    for cliq in nx.strongly_connected_components(egraph):\n        leaders = set()\n        leafs = set()\n        for n in cliq:\n            is_src = False\n            if n in nodes:\n                logging.info(\"Preserving: {} in {}\".format(n,cliq))\n                leaders.add(n)\n                is_src = True\n                \n            is_leaf = True\n            for p in g.successors(n):\n                if p in cliq:\n                    is_leaf = False\n\n            if not(is_leaf or is_src):\n                disposable.add(n)\n                \n            if is_leaf:\n                logging.info(\"Clique leaf: {} in {}\".format(n,cliq))\n                leafs.add(n)\n\n                \n        leader = None\n        if len(leaders) > 1:\n            logging.info(\"UHOH: {}\".format(leaders))\n        if len(leaders) > 0:\n            leader = list(leaders)[0]\n        else:\n            leader = list(leafs)[0]\n        leafmap[n] = leafs\n\n    subg = g.subgraph(subnodes)\n    fg = remove_nodes(subg, disposable)\n    return fg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main():\n    parser = argparse.ArgumentParser(description='Wrapper for obographs assocmodel library'\n                                                 \"\"\"\n                                                 By default, ontologies and assocs are cached locally and synced from a remote sparql endpoint\n                                                 \"\"\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-r', '--resource', type=str, required=False,\n                        help='Name of ontology')\n    parser.add_argument('-f', '--assocfile', type=str, required=False,\n                        help='Name of input file for associations')\n    parser.add_argument('-A', '--assocformat', type=str, default='gaf', required=False,\n                        help='Format of association file, if passed (default: gaf)')\n    parser.add_argument('-o', '--outfile', type=str, required=False,\n                        help='Path to output file')\n    parser.add_argument('-t', '--to', type=str, required=False,\n                        help='Output to (tree, dot, ...)')\n    parser.add_argument('-d', '--direction', type=str, default='u', required=False,\n                        help='u = up, d = down, ud = up and down')\n    parser.add_argument('-e', '--evidence', type=str, required=False,\n                        help='ECO class')\n    parser.add_argument('-p', '--properties', nargs='*', type=str, required=False,\n                        help='Properties')\n    parser.add_argument('-P', '--plot', type=bool, default=False,\n                        help='if set, plot output (requires plotly)')\n    parser.add_argument('-y', '--yamlconfig', type=str, required=False,\n                        help='Path to setup/configuration yaml file')\n    parser.add_argument('-S', '--slim', type=str, default='', required=False,\n                        help='Slim type. m=minimal')\n    parser.add_argument('-c', '--container_properties', nargs='*', type=str, required=False,\n                        help='Properties to nest in graph')\n    parser.add_argument('-C', '--category', nargs=2, type=str, required=False,\n                        help='category tuple (SUBJECT OBJECT)')\n    parser.add_argument('-T', '--taxon', type=str, required=False,\n                        help='Taxon of associations')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n    subparsers = parser.add_subparsers(dest='subcommand', help='sub-command help')\n\n    # EXTRACT ONTOLOGY\n    parser_n = subparsers.add_parser('subontology',\n                                     help='Extract sub-ontology, include only annotated nodes or their descendants')\n    parser_n.add_argument('-M', '--minimal', dest='minimal', action='store_true', default=False, help='If set, remove non-MRCA nodes')\n    parser_n.set_defaults(function=extract_ontology)\n\n    # ENRICHMENT\n    parser_n = subparsers.add_parser('enrichment',\n                                     help='Perform an enrichment test over a sample set of annotated entities')\n    parser_n.add_argument('-q', '--query',type=str, help='query all genes for this class an use as subject')\n    parser_n.add_argument('-H', '--hypotheses',nargs='*', help='list of classes to test against')\n    parser_n.add_argument('-s', '--sample_file', type=str, help='file containing list of gene IDs in sample set')\n    parser_n.add_argument('-b', '--background_file', type=str, help='file containing list of gene IDs in background set')\n    parser_n.add_argument('-t', '--threshold', type=float, help='p-value threshold')\n    parser_n.add_argument('sample_ids', nargs='*', help='list of gene IDs in sample set')\n    parser_n.set_defaults(function=run_enrichment_test)\n\n    # PHENOLOG\n    parser_n = subparsers.add_parser('phenolog',\n                                     help='Perform multiple enrichment tests, using a second ontology and assoc set to build gene sets')\n    parser_n.add_argument('-R', '--resource2',type=str, required=True, help='path to second GAF')\n    parser_n.add_argument('-F', '--file2',type=str, required=True, help='handle for second ontology')\n    parser_n.set_defaults(function=run_phenolog)\n\n    # QUERY\n    parser_n = subparsers.add_parser('query',\n                                     help='Query for entities (e.g. genes) based on positive and negative terms')\n    parser_n.add_argument('-q', '--query',nargs='*', help='positive classes')\n    parser_n.add_argument('-N', '--negative',type=str, help='negative classes')\n    parser_n.set_defaults(function=run_query)\n\n    # QUERY ASSOCIATIONS\n    parser_n = subparsers.add_parser('associations',\n                                     help='Query for associations for a set of entities (e.g. genes)')\n    parser_n.add_argument('subjects',nargs='*', help='subject ids')\n    parser_n.add_argument('-D', '--dendrogram',type=bool, default=False)\n    parser_n.set_defaults(function=run_query_associations)\n\n    # INTERSECTIONS\n    parser_n = subparsers.add_parser('intersections',\n                                     help='Query intersections')\n    parser_n.add_argument('-X', '--xterms',nargs='*', help='x classes')\n    parser_n.add_argument('-Y', '--yterms',nargs='*', help='y classes')\n    parser_n.add_argument('--useids',type=bool, default=False, help='if true, use IDs not labels on axes')\n    parser_n.add_argument('terms',nargs='*', help='all terms (x and y)')\n    parser_n.set_defaults(function=plot_intersections)\n\n    # INTERSECTION DENDROGRAM (TODO: merge into previous?)\n    parser_n = subparsers.add_parser('intersection-dendrogram',\n                                     help='Plot dendrogram from intersections')\n    parser_n.add_argument('-X', '--xterms',nargs='*', help='x classes')\n    parser_n.add_argument('-Y', '--yterms',nargs='*', help='y classes')\n    parser_n.add_argument('--useids',type=bool, default=False, help='if true, use IDs not labels on axes')\n    parser_n.add_argument('terms',nargs='*', help='all terms (x and y)')\n    parser_n.set_defaults(function=plot_term_intersection_dendrogram)\n\n    # SIMILARITY MATRIX (may move to another module)\n    parser_n = subparsers.add_parser('simmatrix',\n                                     help='Plot dendrogram for similarities between subjects')\n    parser_n.add_argument('-X', '--xsubjects',nargs='*', help='x subjects')\n    parser_n.add_argument('-Y', '--ysubjects',nargs='*', help='y subjects')\n    parser_n.add_argument('--useids',type=bool, default=False, help='if true, use IDs not labels on axes')\n    parser_n.add_argument('subjects',nargs='*', help='all terms (x and y)')\n    parser_n.set_defaults(function=plot_simmatrix)\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n\n    if not args.assocfile:\n        if not args.taxon or not args.category:\n            raise ValueError(\"Must specify EITHER assocfile OR both taxon and category\")\n            \n        \n    logging.info(\"Welcome!\")\n\n    if args.yamlconfig is not None:\n        logging.info(\"Setting config from: {}\".format(args.yamlconfig))\n        # note this sets a global:\n        # we would not do this outside the context of a standalone script\n        from ontobio.config import set_config\n        set_config(args.yamlconfig)\n\n\n    handle = args.resource\n\n    # Ontology Factory\n    ofactory = OntologyFactory()\n    logging.info(\"Creating ont object from: {} {}\".format(handle, ofactory))\n    ont = ofactory.create(handle)\n    logging.info(\"ont: {}\".format(ont))\n\n    evidence = args.evidence\n    if evidence is not None and evidence.lower() == 'noiea':\n        evidence = \"-ECO:0000501\"\n\n\n    # Association Factory\n    afactory = AssociationSetFactory()\n    aset = None\n    if args.assocfile is not None:\n        aset = afactory.create_from_file(file=args.assocfile,\n                                         fmt=args.assocformat,\n                                         ontology=ont)\n    else:\n        [subject_category, object_category] = args.category\n        # create using GO/Monarch services\n        aset = afactory.create(ontology=ont,\n                               subject_category=subject_category,\n                               object_category=object_category,\n                               taxon=args.taxon)\n\n\n    func = args.function\n    func(ont, aset, args)", "response": "Entry point for the command line tool for the APKs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_phenolog(ont, aset, args):\n    ofactory = OntologyFactory()\n    ont2 = ofactory.create(args.resource2)\n\n    afactory = AssociationSetFactory()\n    aset2 = afactory.create(ontology=ont2,\n                            file=args.file2)\n\n    # only test for genes (or other subjects of statements) in common\n    common = set(aset.subjects).intersection(aset2.subjects)\n    num_common = len(common)\n    logging.info(\"Genes in common between two KBs: {}/\\{} = {}\".format(len(aset.subjects), len(aset2.subjects), num_common))\n    if num_common < 2:\n        logging.error(\"TOO FEW\")\n        return None\n    for n in aset.ontology.nodes():\n        nl = ont.label(n, id_if_null=True)\n        genes = aset.query([n])\n        num_genes = len(genes)\n        if num_genes > 2:\n            logging.info(\"BASE: {} {} num={}\".format(n,nl, num_genes))\n            enr = aset2.enrichment_test(subjects=genes, background=aset2.subjects, labels=True)\n            for r in enr:\n                print(\"{:8.3g} {} {:20s} <-> {} {:20s}\".format(r['p'],n,nl,r['c'],str(r['n'])))", "response": "Run enrichment test on a 2nd ontology and a 2nd association set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_association(self, association: Association) -> Entity:\n        if \"header\" not in association or association[\"header\"] == False:\n            # print(json.dumps(association, indent=4))\n            gpi_obj = {\n                'id': association[\"subject\"][\"id\"],\n                'label': association[\"subject\"][\"label\"],  # db_object_symbol,\n                'full_name': association[\"subject\"][\"fullname\"],  # db_object_name,\n                'synonyms': association[\"subject\"][\"synonyms\"],\n                'type': association[\"subject\"][\"type\"], #db_object_type,\n                'parents': \"\", # GAF does not have this field, but it's optional in GPI\n                'xrefs': \"\", # GAF does not have this field, but it's optional in GPI\n                'taxon': {\n                    'id': association[\"subject\"][\"taxon\"][\"id\"]\n                }\n            }\n            return Entity(gpi_obj)\n\n        return None", "response": "Convert an association to a GAF Entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a networkx MultiDiGraph representing the whole ontology for a subset of relations.", "response": "def get_filtered_graph(self, relations=None, prefix=None):\n        \"\"\"\n        Returns a networkx graph for the whole ontology, for a subset of relations\n\n        Only implemented for eager methods.\n\n        Implementation notes: currently this is not cached\n\n        Arguments\n        ---------\n          - relations : list\n             list of object property IDs, e.g. subClassOf, BFO:0000050. If empty, uses all.\n          - prefix : String\n             if specified, create a subgraph using only classes with this prefix, e.g. ENVO, PATO, GO\n        Return\n        ------\n        nx.MultiDiGraph\n            A networkx MultiDiGraph object representing the filtered ontology\n        \"\"\"\n\n        # trigger synonym cache\n        self.all_synonyms()\n        self.all_obsoletes()\n\n        # default method - wrap get_graph\n        srcg = self.get_graph()\n        if prefix is not None:\n            srcg = srcg.subgraph([n for n in srcg.nodes() if n.startswith(prefix+\":\")])\n        if relations is None:\n            logger.info(\"No filtering on \"+str(self))\n            return srcg\n        logger.info(\"Filtering {} for {}\".format(self, relations))\n        g = nx.MultiDiGraph()\n\n        # TODO: copy full metadata\n        logger.info(\"copying nodes\")\n        for (n,d) in srcg.nodes(data=True):\n            g.add_node(n, **d)\n\n        logger.info(\"copying edges\")\n        num_edges = 0\n        for (x,y,d) in srcg.edges(data=True):\n            if d['pred'] in relations:\n                num_edges += 1\n                g.add_edge(x,y,**d)\n        logger.info(\"Filtered edges: {}\".format(num_edges))\n        return g"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge the specified ontology into current ontology.", "response": "def merge(self, ontologies):\n        \"\"\"\n        Merges specified ontology into current ontology\n        \"\"\"\n        if self.xref_graph is None:\n            self.xref_graph = nx.MultiGraph()\n        logger.info(\"Merging source: {} xrefs: {}\".format(self, len(self.xref_graph.edges())))\n        for ont in ontologies:\n            logger.info(\"Merging {} into {}\".format(ont, self))\n            g = self.get_graph()\n            srcg = ont.get_graph()\n            for n in srcg.nodes():\n                g.add_node(n, **srcg.node[n])\n            for (o,s,m) in srcg.edges(data=True):\n                g.add_edge(o,s,**m)\n            if ont.xref_graph is not None:\n                for (o,s,m) in ont.xref_graph.edges(data=True):\n                    self.xref_graph.add_edge(o,s,**m)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subontology(self, nodes=None, minimal=False, relations=None):\n        g = None\n        if nodes is not None:\n            g = self.subgraph(nodes)\n        else:\n            g = self.get_graph()\n        if minimal:\n            from ontobio.slimmer import get_minimal_subgraph\n            g = get_minimal_subgraph(g, nodes)\n\n        ont = Ontology(graph=g, xref_graph=self.xref_graph) # TODO - add metadata\n        if relations is not None:\n            g = ont.get_filtered_graph(relations)\n            ont = Ontology(graph=g, xref_graph=self.xref_graph)\n        return ont", "response": "Returns a new ontology that is an extract of this one\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a dictionary that maps between all nodes in an ontology to a subset.", "response": "def create_slim_mapping(self, subset=None, subset_nodes=None, relations=None, disable_checks=False):\n        \"\"\"\n        Create a dictionary that maps between all nodes in an ontology to a subset\n\n        Arguments\n        ---------\n        ont : `Ontology`\n            Complete ontology to be mapped. Assumed pre-filtered for relationship types\n        subset : str\n            Name of subset to map to, e.g. goslim_generic\n        nodes : list\n            If no named subset provided, subset is passed in as list of node ids\n        relations : list\n            List of relations to filter on\n        disable_checks: bool\n            Unless this is set, this will prevent a mapping being generated with non-standard relations.\n            The motivation here is that the ontology graph may include relations that it is inappropriate to\n            propagate gene products over, e.g. transports, has-part\n\n        Return\n        ------\n        dict\n            maps all nodes in ont to one or more non-redundant nodes in subset\n\n        Raises\n        ------\n        ValueError\n            if the subset is empty\n        \"\"\"\n        if subset is not None:\n            subset_nodes = self.extract_subset(subset)\n            logger.info(\"Extracting subset: {} -> {}\".format(subset, subset_nodes))\n\n        if subset_nodes is None or len(subset_nodes) == 0:\n            raise ValueError(\"subset nodes is blank\")\n        subset_nodes = set(subset_nodes)\n        logger.debug(\"SUBSET: {}\".format(subset_nodes))\n\n        # Use a sub-ontology for mapping\n        subont = self\n        if relations is not None:\n            subont = self.subontology(relations=relations)\n\n        if not disable_checks:\n            for r in subont.relations_used():\n                if r != 'subClassOf' and r != 'BFO:0000050' and r != 'subPropertyOf':\n                    raise ValueError(\"Not safe to propagate over a graph with edge type: {}\".format(r))\n\n        m = {}\n        for n in subont.nodes():\n            ancs = subont.ancestors(n, reflexive=True)\n            ancs_in_subset = subset_nodes.intersection(ancs)\n            m[n] = list(subont.filter_redundant(ancs_in_subset))\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all non - redundant ids from a list of ids", "response": "def filter_redundant(self, ids):\n        \"\"\"\n        Return all non-redundant ids from a list\n        \"\"\"\n        sids = set(ids)\n        for id in ids:\n            sids = sids.difference(self.ancestors(id, reflexive=False))\n        return sids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_subset(self, subset, contract=True):\n        return [n for n in self.nodes() if subset in self.subsets(n, contract=contract)]", "response": "Return all nodes in a subset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of subsets for a class or ontology object", "response": "def subsets(self, nid, contract=True):\n        \"\"\"\n        Retrieves subset ids for a class or ontology object\n        \"\"\"\n        n = self.node(nid)\n        subsets = []\n        meta = self._meta(nid)\n        if 'subsets' in meta:\n            subsets = meta['subsets']\n        else:\n            subsets = []\n        if contract:\n            subsets = [self._contract_subset(s) for s in subsets]\n        return subsets"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist all prefixes used by this instance", "response": "def prefixes(self):\n        \"\"\"\n        list all prefixes used\n        \"\"\"\n        pset = set()\n        for n in self.nodes():\n            pfx = self.prefix(n)\n            if pfx is not None:\n                pset.add(pfx)\n        return list(pset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prefix_fragment(self, nid):\n        sep=':'\n        if nid.startswith('http'):\n            if '#' in nid:\n               sep='#'\n            else:\n                sep='/'\n        parts = nid.split(sep)\n        frag = parts.pop()\n        prefix = sep.join(parts)\n        return prefix, frag", "response": "Return prefix and fragment for a node"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn list of all relations used to connect edges", "response": "def relations_used(self):\n        \"\"\"\n        Return list of all relations used to connect edges\n        \"\"\"\n        g = self.get_graph()\n        types = set()\n        for (x,y,d) in g.edges(data=True):\n            types.add(d['pred'])\n        return list(types)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef child_parent_relations(self, subj, obj, graph=None):\n        if graph is None:\n            graph = self.get_graph()\n        preds = set()\n        for _,ea in graph[obj][subj].items():\n            preds.add(ea['pred'])\n        logger.debug('{}->{} = {}'.format(subj,obj,preds))\n        return preds", "response": "Get all relationship type ids between a subject and a parent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all direct parents of a node.", "response": "def parents(self, node, relations=None):\n        \"\"\"\n        Return all direct parents of specified node.\n\n        Wraps networkx by default.\n\n        Arguments\n        ---------\n        node: string\n           identifier for node in ontology\n        relations: list of strings\n           list of relation (object property) IDs used to filter\n\n        \"\"\"\n        g = self.get_graph()\n        if node in g:\n            parents = list(g.predecessors(node))\n            if relations is None:\n                return parents\n            else:\n                rset = set(relations)\n                return [p for p in parents if len(self.child_parent_relations(node, p, graph=g).intersection(rset)) > 0 ]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all direct children of a node.", "response": "def children(self, node, relations=None):\n        \"\"\"\n        Return all direct children of specified node.\n\n        Wraps networkx by default.\n\n        Arguments\n        ---------\n\n        node: string\n\n           identifier for node in ontology\n\n        relations: list of strings\n\n           list of relation (object property) IDs used to filter\n\n        \"\"\"\n        g = self.get_graph()\n        if node in g:\n            children = list(g.successors(node))\n            if relations is None:\n                return children\n            else:\n                rset = set(relations)\n                return [c for c in children if len(self.child_parent_relations(c, node, graph=g).intersection(rset)) > 0 ]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all ancestors of the specified node.", "response": "def ancestors(self, node, relations=None, reflexive=False):\n        \"\"\"Return all ancestors of specified node.\n\n        The default implementation is to use networkx, but some\n        implementations of the Ontology class may use a database or\n        service backed implementation, for large graphs.\n\n        Arguments\n        ---------\n        node : str\n            identifier for node in ontology\n        reflexive : bool\n            if true, return query node in graph\n        relations : list\n             relation (object property) IDs used to filter\n\n        Returns\n        -------\n        list[str]\n            ancestor node IDs\n\n        \"\"\"\n        if reflexive:\n            ancs = self.ancestors(node, relations, reflexive=False)\n            ancs.append(node)\n            return ancs\n\n        g = None\n        if relations is None:\n            g = self.get_graph()\n        else:\n            g = self.get_filtered_graph(relations)\n        if node in g:\n            return list(nx.ancestors(g, node))\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef descendants(self, node, relations=None, reflexive=False):\n        if reflexive:\n            decs = self.descendants(node, relations, reflexive=False)\n            decs.append(node)\n            return decs\n        g = None\n        if relations is None:\n            g = self.get_graph()\n        else:\n            g = self.get_filtered_graph(relations)\n        if node in g:\n            return list(nx.descendants(g, node))\n        else:\n            return []", "response": "Returns all descendants of the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a bidirectional networkx graph of all equivalency relations that are in the same source as the source.", "response": "def equiv_graph(self):\n        \"\"\"\n        Returns\n        -------\n        graph\n            bidirectional networkx graph of all equivalency relations\n        \"\"\"\n        eg = nx.Graph()\n        for (u,v,d) in self.get_graph().edges(data=True):\n            if d['pred'] == 'equivalentTo':\n                eg.add_edge(u,v)\n        return eg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traverse_nodes(self, qids, up=True, down=False, **args):\n        g = self.get_filtered_graph(**args)\n        nodes = set()\n        for id in qids:\n            # reflexive - always add self\n            nodes.add(id)\n            if down:\n                nodes.update(nx.descendants(g, id))\n            if up:\n                nodes.update(nx.ancestors(g, id))\n        return nodes", "response": "Traverse the nodes in the set of seed node IDs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_roots(self, relations=None, prefix=None):\n        g = self.get_filtered_graph(relations=relations, prefix=prefix)\n        # note: we also eliminate any singletons, which includes obsolete classes\n        roots = [n for n in g.nodes() if len(list(g.predecessors(n))) == 0 and len(list(g.successors(n))) > 0]\n        return roots", "response": "Get all nodes that lack parents and return a list of all the roots."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget all nodes at a particular level", "response": "def get_level(self, level, relations=None, **args):\n        \"\"\"\n        Get all nodes at a particular level\n\n        Arguments\n        ---------\n        relations : list[str]\n            list of relations used to filter\n        \"\"\"\n        g = self.get_filtered_graph(relations)\n        nodes = self.get_roots(relations=relations, **args)\n        for i in range(level):\n            logger.info(\" ITERATING TO LEVEL: {} NODES: {}\".format(i, nodes))\n            nodes = [c for n in nodes\n                     for c in g.successors(n)]\n        logger.info(\" FINAL: {}\".format(nodes))\n        return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parent_index(self, relations=None):\n        g = None\n        if relations is None:\n            g = self.get_graph()\n        else:\n            g = self.get_filtered_graph(relations)\n        l = []\n        for n in g:\n            l.append([n] + list(g.predecessors(n)))\n        return l", "response": "Returns a mapping of nodes to all direct parents of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef text_definition(self, nid):\n        tdefs = []\n        meta = self._meta(nid)\n        if 'definition' in meta:\n            obj = meta['definition']\n            return TextDefinition(nid, **obj)\n        else:\n            return None", "response": "Returns the text definition for a class or relation id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of all logical definitions for a class id", "response": "def logical_definitions(self, nid):\n        \"\"\"\n        Retrieves logical definitions for a class id\n\n        Arguments\n        ---------\n        nid : str\n            Node identifier for entity to be queried\n\n        Returns\n        -------\n        LogicalDefinition\n        \"\"\"\n        ldefs = self.all_logical_definitions\n        if ldefs is not None:\n            #print(\"TESTING: {} AGAINST LD: {}\".format(nid, str(ldefs)))\n            return [x for x in ldefs if x.class_id == nid]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_property_chain_axioms(self, nid):\n        pcas = self.all_property_chain_axioms\n        if pcas is not None:\n            return [x for x in pcas if x.predicate_id == nid]\n        else:\n            return []", "response": "Returns the property chain axioms for a class id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef replaced_by(self, nid, strict=True):\n        vs = self._get_basic_property_value(nid, 'IAO:0100001')\n        if len(vs) > 1:\n            msg = \"replaced_by has multiple values: {}\".format(vs)\n            if strict:\n                raise ValueError(msg)\n            else:\n                logger.error(msg)\n\n        return vs", "response": "Returns value of replaced_by property for obsolete nodes\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef synonyms(self, nid, include_label=False):\n        n = self.node(nid)\n        syns = []\n        if 'meta' in n:\n            meta = n['meta']\n            if 'synonyms' in meta:\n                for obj in meta['synonyms']:\n                    syns.append(Synonym(nid, **obj))\n        if include_label:\n            syns.append(Synonym(nid, val=self.label(nid), pred='label'))\n        return syns", "response": "Returns a list of Synonym objects for a class with the given node identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a new node to the ontology", "response": "def add_node(self, id, label=None, type='CLASS', meta=None):\n        \"\"\"\n        Add a new node to the ontology\n        \"\"\"\n        g = self.get_graph()\n        if meta is None:\n            meta={}\n        g.add_node(id, label=label, type=type, meta=meta)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying contents of xref_graph to inlined meta object for each node", "response": "def inline_xref_graph(self):\n        \"\"\"\n        Copy contents of xref_graph to inlined meta object for each node\n        \"\"\"\n        xg = self.xref_graph\n        for n in self.nodes():\n            if n in xg:\n                self._add_meta_element(n, 'xrefs', [{'val':x} for x in xg.neighbors(n)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_parent(self, id, pid, relation='subClassOf'):\n        g = self.get_graph()\n        g.add_edge(pid, id, pred=relation)", "response": "Add a new edge to the ontology\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an xref to the xref graph", "response": "def add_xref(self, id, xref):\n        \"\"\"\n        Adds an xref to the xref graph\n        \"\"\"\n        # note: does not update meta object\n        if self.xref_graph is None:\n            self.xref_graph = nx.MultiGraph()\n        self.xref_graph.add_edge(xref, id)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_synonym(self, syn):\n        n = self.node(syn.class_id)\n        if 'meta' not in n:\n            n['meta'] = {}\n        meta = n['meta']\n        if 'synonyms' not in meta:\n            meta['synonyms'] = []\n        meta['synonyms'].append(syn.as_dict())", "response": "Adds a synonym for a node"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a node to a subset", "response": "def add_to_subset(self, id, s):\n        \"\"\"\n        Adds a node to a subset\n        \"\"\"\n        n = self.node(id)\n        if 'meta' not in n:\n            n['meta'] = {}\n        meta = n['meta']\n        if 'subsets' not in meta:\n            meta['subsets'] = []\n        meta['subsets'].append(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all_synonyms(self, include_label=False):\n        syns = []\n        for n in self.nodes():\n            syns = syns + self.synonyms(n, include_label=include_label)\n        return syns", "response": "Returns a list of all synonyms in the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef label(self, nid, id_if_null=False):\n        g = self.get_graph()\n        if nid in g:\n            n = g.node[nid]\n            if 'label' in n:\n                return n['label']\n            else:\n                if id_if_null:\n                    return nid\n                else:\n                    return None\n        else:\n            if id_if_null:\n                return nid\n            else:\n                return None", "response": "Returns the label for a node in the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xrefs(self, nid, bidirectional=False):\n        if self.xref_graph is not None:\n            xg = self.xref_graph\n            if nid not in xg:\n                return []\n            if bidirectional:\n                return list(xg.neighbors(nid))\n            else:\n                return [x for x in xg.neighbors(nid) if xg[nid][x][0]['source'] == nid]\n\n        return []", "response": "Returns a list of node identifiers that are xreffed to the node nid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of identifiers based on an input list of names and synonyms.", "response": "def resolve_names(self, names, synonyms=False, **args):\n        \"\"\"\n        returns a list of identifiers based on an input list of labels and identifiers.\n\n        Arguments\n        ---------\n        names: list\n           search terms. '%' treated as wildcard\n        synonyms: bool\n           if true, search on synonyms in addition to labels\n        is_regex : bool\n           if true, treats each name as a regular expression\n        is_partial_match : bool\n           if true, treats each name as a regular expression .*name.*\n        \"\"\"\n        g = self.get_graph()\n        r_ids = []\n        for n in names:\n            logger.debug(\"Searching for {} syns={}\".format(n,synonyms))\n            if len(n.split(\":\")) == 2:\n                r_ids.append(n)\n            else:\n                matches = set([nid for nid in g.nodes() if self._is_match(self.label(nid), n, **args)])\n                if synonyms:\n                    logger.debug(\"Searching syns for {}\".format(names))\n                    for nid in g.nodes():\n                        for s in self.synonyms(nid):\n                            if self._is_match(s.val, n, **args):\n                                matches.add(nid)\n                r_ids += list(matches)\n        return r_ids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main():\n\n    parser = argparse.ArgumentParser(\n        description='Command line interface to python-ontobio.golr library'\n        \"\"\"\n\n        Provides command line interface onto the ontobio.golr python library, a high level\n        abstraction layer over Monarch and GO solr indices.\n        \"\"\",\n        formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-r', '--resource', type=str, required=False,\n                        help='Name of ontology')\n    parser.add_argument('-d', '--display', type=str, default='o', required=False,\n                        help='What to display: some combination of o, s, r. o=object ancestors, s=subject ancestors. If r present, draws s<->o relations ')\n    parser.add_argument('-o', '--outfile', type=str, required=False,\n                        help='Path to output file')\n    parser.add_argument('-t', '--to', type=str, required=False,\n                        help='Output to (tree, dot, ...)')\n    parser.add_argument('-C', '--category', type=str, required=False,\n                        help='Category')\n    parser.add_argument('-c', '--container_properties', nargs='*', type=str, required=False,\n                        help='Properties to nest in graph')\n    parser.add_argument('-s', '--species', type=str, required=False,\n                        help='NCBITaxon ID')\n    parser.add_argument('-e', '--evidence', type=str, required=False,\n                        help='ECO ID')\n    parser.add_argument('-G', '--graph', type=str, default='', required=False,\n                        help='Graph type. m=minimal')\n    parser.add_argument('-S', '--slim', nargs='*', type=str, required=False,\n                        help='Slim IDs')\n    parser.add_argument('-M', '--mapids', type=str, required=False,\n                        help='Map identifiers to this ID space, e.g. ENSEMBL')\n    parser.add_argument('-p', '--properties', nargs='*', type=str, required=False,\n                        help='Properties')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n    parser.add_argument('ids',nargs='*')\n\n    # ontology\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n        \n    logging.info(\"Welcome!\")\n\n    ont = None\n    g = None\n    handle = args.resource\n    if handle is not None:\n        logging.info(\"Handle: {}\".format(handle))\n        factory = OntologyFactory()\n        logging.info(\"Factory: {}\".format(factory))\n        ont = factory.create(handle)\n        logging.info(\"Created ont: {}\".format(ont))\n        g = ont.get_filtered_graph(relations=args.properties)\n\n    w = GraphRenderer.create(args.to)\n\n    nodes = set()\n\n    display = args.display\n\n    # query all IDs, gathering associations\n    assocs = []\n    for id in args.ids:\n        this_assocs, facets = search_golr_wrap(id,\n                                               args.category,\n                                               subject_taxon=args.species,\n                                               rows=1000,\n                                               slim=args.slim,\n                                               evidence=args.evidence,\n                                               map_identifiers=args.mapids)\n\n        assocs += this_assocs\n\n    logging.info(\"Num assocs: {}\".format(len(assocs)))\n\n    for a in assocs:\n        print(\"{}\\t{}\\t{}\\t{}\".format(a['subject'],\n                                  a['subject_label'],\n                                  a['relation'],\n                                  \";\".join(a['objects'])))\n\n    if ont is not None:\n        # gather all ontology classes used\n        for a in assocs:\n            objs = a['objects']\n\n            if display.find('r') > -1:\n                pass\n\n            if display.find('o') > -1:\n                for obj in objs:\n                    nodes.add(obj)\n                    if ont is not None:\n                        nodes.update(ont.ancestors(obj))\n\n            if display.find('s') > -1:\n                sub = a['subject']\n                nodes.add(sub)\n                if ont is not None:\n                    nodes.update(ont.ancestors(sub))\n\n        # create a subgraph\n        subg = g.subgraph(nodes)\n\n        # optionally add edges between subj and obj nodes\n        if display.find('r') > -1:\n            for a in assocs:\n                rel = a['relation']\n                sub = a['subject']\n                objs = a['objects']\n                if rel is None:\n                    rel = 'rdfs:seeAlso'\n                for obj in objs:\n                    logging.info(\"Adding assoc rel {} {} {}\".format(sub,obj,rel))\n                    subg.add_edge(obj,sub,pred=rel)\n\n        # display tree/graph\n        show_graph(subg, nodes, objs, args)", "response": "Entry point for the base command line interface to python - ontobio. golr library"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a graph to file", "response": "def show_graph(g, nodes, query_ids, args):\n    \"\"\"\n    Writes graph\n    \"\"\"\n    if args.graph.find('m') > -1:\n        logging.info(\"SLIMMING\")\n        g = get_minimal_subgraph(g, query_ids)\n    w = GraphRenderer.create(args.to)\n    if args.outfile is not None:\n        w.outfile = args.outfile\n    logging.info(\"Writing subg from \"+str(g))\n    w.write(g, query_ids=query_ids, container_predicates=args.container_properties)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n    parser = argparse.ArgumentParser(description='Wrapper for ontobio lexical mapping'\n                                                 \"\"\"\n                                                 Lexically maps one or more ontologies. Ontologies can be local or remote,\n                                                 any input handle can be specified, see docs for more details on handles.\n\n                                                 If multiple ontologies are specified, then each ontology in the list is compared against the first one.\n\n                                                 If a simgle ontology is specified, then all pairs in that ontology will be compared\n                                                 \n                                                 Output format to be documented - see lexmap.py for the various scoring attributes for now.\n                                                 \"\"\",\n                                     formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-o', '--outfile', type=str, nargs='*', default=[], required=False,\n                        help='Path to output file')\n    parser.add_argument('-t', '--to', type=str, required=False, default='tsv',\n                        help='Output to (tree, dot, ...)')\n    parser.add_argument('-l', '--labels', type=str,\n                        help='If set, then include node labels in results. DEPRECATED')\n    parser.add_argument('-s', '--scoring', default='sim', type=str,\n                        help='Score weighting scheme. Default=sim')\n    parser.add_argument('-P', '--prefix', type=str, required=False,\n                        help='Prefix to constrain traversal on, e.g. PATO, ENVO')\n    parser.add_argument('-c', '--config', type=str, required=False,\n                        help='lexmap configuration file (yaml). See schema for details')\n    parser.add_argument('-X', '--xref_weights', type=str, required=False,\n                        help='csv of curated per-xref weights')\n    parser.add_argument('-e', '--eval_xrefs', default=False, action='store_true', dest='eval_xrefs',\n                        help='only compare xrefs')\n    parser.add_argument('-u', '--unmapped', type=str, required=False,\n                        help='File to export unmapped nodes to')\n    parser.add_argument('-A', '--all-by-all', dest='all_by_all', action='store_true',\n                        help='compare all ontologies against all.')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n    parser.add_argument('ontologies',nargs='*',\n                        help='one or more ontologies to be aligned. Any input handle can be specified')\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n        \n    logging.info(\"Welcome!\")\n\n    factory = OntologyFactory()\n    onts = [filter_by_prefix(factory.create(h)) for h in args.ontologies]\n\n    \n    config = {}\n    if args.config is not None:\n        f = open(args.config,'r')\n        config = yaml.load(f, Loader=yaml.FullLoader)\n        f.close()\n\n    # add pre-defined weights to config\n    if args.xref_weights is not None:\n        if 'xref_weights' not in config:\n            config['xref_weights'] = []\n        xws = config['xref_weights']\n        df = pd.read_csv(args.xref_weights)\n        df = df.fillna(0.0)\n        for _, row in df.iterrows():\n            w = float(row['weight'])\n            WA = np.array((0.0, 0.0, 0.0, 0.0))\n            if w < 0:\n                WA[2] = w\n                WA[3] = abs(w)\n            else:\n                WA[2] = w\n                WA[3] = -w\n            xws.append({'left':row['left'],\n                        'right':row['right'],\n                        'weights':WA})\n        \n    logging.info(\"ALL: {}\".format(args.all_by_all))\n    \n    lexmap = LexicalMapEngine(config=config)\n\n    if args.eval_xrefs:\n        cpairs = []\n        for n in onts[0].nodes():\n            for x in onts[0].xrefs(n):\n                cpairs.append((n,x))\n        lexmap.class_pairs = cpairs\n        logging.info(\"Comparing {} class pairs. Sample: {}\".format(len(cpairs), cpairs[0:50]))\n    \n    \n    \n    if len(onts) == 0:\n        raise ValueException(\"must pass one or more ontologies\")\n    else:\n        logging.info(\"Indexing ontologies: {}\".format(onts))\n        for ont in onts:\n            lexmap.index_ontology(ont)\n        oid0 = onts[0].id\n        pairs = [(oid0,oid0)]\n        if len(onts) > 1:\n            if args.all_by_all:\n                logging.info(\"All vs ALL: {}\".format(onts))\n                pairs = []\n                for i in onts:\n                    for j in onts:\n                        if i.id < j.id:\n                            pairs.append((i.id, j.id))\n            else:\n                logging.info(\"All vs first in list: {}\".format(oid0))\n                pairs = [(oid0, ont.id) for ont in onts[1:]]\n        logging.info(\"Comparing the following pairs of ontologies: {}\".format(pairs))\n        lexmap.ontology_pairs = pairs\n    mo = Ontology()\n    mo.merge(onts)\n\n    g = lexmap.get_xref_graph()\n    \n    \n    if args.to == 'obo':\n        write_obo(g,mo,args)\n    else:\n        write_tsv(lexmap,g,mo,args)\n\n        \n    if args.unmapped:\n        udf = lexmap.unmapped_dataframe(g)\n        udf.to_csv(args.unmapped, sep=\"\\t\", index=False)", "response": "This function is the main entry point for the lexical mapping script. It is used to create a parser for the input list of ontology names and input values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n\n    parser = argparse.ArgumentParser(\n        description='Command line interface to python-ontobio.golr library'\n        \"\"\"\n\n        Provides command line interface onto the ontobio.golr python library, a high level\n        abstraction layer over Monarch and GO solr indices.\n        \"\"\",\n        formatter_class=argparse.RawTextHelpFormatter)\n\n    parser.add_argument('-o', '--outfile', type=str, required=False,\n                        help='Path to output file')\n    parser.add_argument('-C', '--category', nargs=2, type=str, required=True,\n                        help='Category pair. E.g. disease gene')\n    parser.add_argument('-s', '--species', type=str, required=True,\n                        help='NCBITaxon ID')\n    parser.add_argument('-S', '--slim', nargs='*', type=str, required=False,\n                        help='Slim IDs')\n    parser.add_argument('-L', '--limit', type=int, default=100000, required=False,\n                        help='Limit on number of rows')\n    parser.add_argument('-u', '--url', type=str, required=False,\n                        help='Solr URL. E.g. http://localhost:8983/solr/golr')\n    parser.add_argument('-v', '--verbosity', default=0, action='count',\n                        help='Increase output verbosity')\n\n    parser.add_argument('ids',nargs='*')\n\n    args = parser.parse_args()\n\n    if args.verbosity >= 2:\n        logging.basicConfig(level=logging.DEBUG)\n    elif args.verbosity == 1:\n        logging.basicConfig(level=logging.INFO)\n    else:\n        logging.basicConfig(level=logging.WARNING)\n        \n    logging.info(\"Welcome!\")\n\n    [subject_category, object_category] = args.category\n\n    assocs = bulk_fetch(subject_category,\n                        object_category,\n                        args.species,\n                        rows=args.limit,\n                        slim=args.slim,\n                        url=args.url)\n\n    for a in assocs:\n        print(\"{}\\t{}\\t{}\".format(a['subject'],\n                                    a['relation'],\n                                    \";\".join(a['objects'])))", "response": "Entry point for the command line interface to python - ontobio. golr library"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_gpi(self, gpi_path):\n    if self.config.gpi_authority_path is not None:\n        gpis = dict()\n        parser = entityparser.GpiParser()\n        with open(self.config.gpi_authority_path) as gpi_f:\n            entities = parser.parse(file=gpi_f)\n            for entity in entities:\n                gpis[entity[\"id\"]] = {\n                    \"symbol\": entity[\"label\"],\n                    \"name\": entity[\"full_name\"],\n                    \"synonyms\": entitywriter.stringify(entity[\"synonyms\"]),\n                    \"type\": entity[\"type\"]\n                }\n        return gpis\n\n    # If there is no config file path, return None\n    return None", "response": "Loads a GPI as a file from the config. gpi_authority_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a line - oriented entity file into a list of entity dict objects.", "response": "def parse(self, file, outfile=None):\n        \"\"\"Parse a line-oriented entity file into a list of entity dict objects\n\n        Note the returned list is of dict objects. TODO: These will\n        later be specified using marshmallow and it should be possible\n        to generate objects\n\n        Arguments\n        ---------\n        file : file or string\n            The file is parsed into entity objects. Can be a http URL, filename or `file-like-object`, for input assoc file\n        outfile : file\n            Optional output file in which processed lines are written. This a file or `file-like-object`\n\n        Return\n        ------\n        list\n            Entities generated from the file\n        \"\"\"\n        file = self._ensure_file(file)\n        ents = []\n        skipped = []\n        n_lines = 0\n        for line in file:\n            n_lines += 1\n            if line.startswith(\"!\"):\n                if outfile is not None:\n                    outfile.write(line)\n                continue\n            line = line.strip(\"\\n\")\n            if line == \"\":\n                logging.warning(\"EMPTY LINE\")\n                continue\n\n            parsed_line, new_ents  = self.parse_line(line)\n            if self._skipping_line(new_ents): # Skip if there were no ents\n                logging.warning(\"SKIPPING: {}\".format(line))\n                skipped.append(line)\n            else:\n                ents += new_ents\n                if outfile is not None:\n                    outfile.write(parsed_line + \"\\n\")\n\n        self.report.skipped += len(skipped)\n        self.report.n_lines += n_lines\n        #self.report.n_associations += len(ents)\n        logging.info(\"Parsed {} ents from {} lines. Skipped: {}\".\n                     format(len(ents),\n                            n_lines,\n                            len(skipped)))\n        file.close()\n        return ents"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_line(self, line):\n        vals = line.split(\"\\t\")\n\n        if len(vals) < 7:\n            self.report.error(line, assocparser.Report.WRONG_NUMBER_OF_COLUMNS, \"\")\n            return line, []\n\n        if len(vals) < 10 and len(vals) >= 7:\n            missing_columns = 10 - len(vals)\n            vals += [\"\" for i in range(missing_columns)]\n\n        [\n            db,\n            db_object_id,\n            db_object_symbol,\n            db_object_name,\n            db_object_synonym,\n            db_object_type,\n            taxon,\n            parent_object_id,\n            xrefs,\n            properties\n        ] = vals\n\n        split_line = assocparser.SplitLine(line=line, values=vals, taxon=taxon)\n\n        ## --\n        ## db + db_object_id. CARD=1\n        ## --\n        id = self._pair_to_id(db, db_object_id)\n        if not self._validate_id(id, split_line, context=assocparser.Report):\n            return line, []\n\n        ## --\n        ## db_object_synonym CARD=0..*\n        ## --\n        synonyms = db_object_synonym.split(\"|\")\n        if db_object_synonym == \"\":\n            synonyms = []\n\n        # TODO: DRY\n        parents = parent_object_id.split(\"|\")\n        if parent_object_id == \"\":\n            parents = []\n        else:\n            parents = [self._normalize_id(x) for x in parents]\n            for p in parents:\n                self._validate_id(p, split_line, context=assocparser.Report)\n\n        xref_ids = xrefs.split(\"|\")\n        if xrefs == \"\":\n            xref_ids = []\n\n        obj = {\n            'id': id,\n            'label': db_object_symbol,\n            'full_name': db_object_name,\n            'synonyms': synonyms,\n            'type': db_object_type,\n            'parents': parents,\n            'xrefs': xref_ids,\n            'taxon': {\n                'id': self._taxon_id(taxon, split_line)\n            }\n        }\n        return line, [obj]", "response": "Parses a single line of a GPI file and returns a tuple of processed_line entities"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a BGI basic gene info JSON file and return a list of items.", "response": "def parse(self, file, outfile=None):\n        \"\"\"Parse a BGI (basic gene info) JSON file\n        \"\"\"\n        file = self._ensure_file(file)\n        obj = json.load(file)\n        items = obj['data']\n        return [self.transform_item(item) for item in items]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform_item(self, item):\n        obj = {\n            'id': item['primaryId'],\n            'label': item['symbol'],\n            'full_name': item['name'],\n            'type': item['soTermId'],\n            'taxon': {'id': item['taxonId']},\n        }\n        if 'synonyms' in item:\n            obj['synonyms'] = item['synonyms']\n        if 'crossReferenceIds' in item:\n            obj['xrefs'] = [self._normalize_id(x) for x in item['crossReferenceIds']]\n\n        # TODO: synonyms\n        # TODO: genomeLocations\n        # TODO: geneLiteratureUrl\n        return obj", "response": "Transforms a dictionary of items into a JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates indexes based on inferred terms.", "response": "def index(self):\n        \"\"\"\n        Creates indexes based on inferred terms.\n\n        You do not need to call this yourself; called on initialization\n        \"\"\"\n        self.subjects = list(self.association_map.keys())\n\n        # ensure annotations unique\n        for (subj,terms) in self.association_map.items():\n            self.association_map[subj] = list(set(self.association_map[subj]))\n            \n        logging.info(\"Indexing {} items\".format(len(self.subjects)))\n        n = 0\n        all_objs = set()\n        for (subj,terms) in self.association_map.items():\n            ancs = self.termset_ancestors(terms)\n            all_objs.update(ancs)\n            self.subject_to_inferred_map[subj] = ancs\n            n = n+1\n            if n<5:\n                logging.info(\" Indexed: {} -> {}\".format(subj, ancs))\n            elif n == 6:\n                logging.info(\"[TRUNCATING>5]....\")\n        self.objects = all_objs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the set of inferred types for a subject.", "response": "def inferred_types(self, subj):\n        \"\"\"\n        Returns: set of reflexive inferred types for a subject.\n\n        E.g. if a gene is directly associated with terms A and B, and these terms have ancestors C, D and E\n        then the set returned will be {A,B,C,D,E}\n        \n        Arguments\n        ---------\n\n          subj - ID string\n\n        Returns: set of class IDs\n\n        \"\"\"\n        if subj in self.subject_to_inferred_map:\n            return self.subject_to_inferred_map[subj]\n        if self.strict:\n            raise UnknownSubjectException(subj)\n        else:\n            return set([])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the set of class IDs that are reflexive ancestors of the given terms.", "response": "def termset_ancestors(self, terms):\n        \"\"\"\n        reflexive ancestors\n\n        Arguments\n        ---------\n\n          terms - a set or list of class IDs\n\n        Returns: set of class IDs\n        \"\"\"\n        ancs = set()\n        for term in terms:\n            ancs = ancs.union(self.ontology.ancestors(term))\n        return ancs.union(set(terms))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query_associations(self, subjects=None, infer_subjects=True, include_xrefs=True):\n        if subjects is None:\n            subjects = []\n        mset = set()\n        if infer_subjects:\n            for subj in subjects:\n                mset.update(self.ontology.descendants(subj))\n        mset.update(set(subjects))\n        if include_xrefs:\n            xset = set()\n            for m in mset:\n                xrefs = self.ontology.xrefs(m, bidirectional=True)\n                if xrefs is not None:\n                    xset.update(xrefs)\n            mset.update(xset)\n        logging.debug(\"Matching subjects: {}\".format(mset))\n        mset = mset.intersection(self.subjects)\n        logging.debug(\"Matching subjects with anns: {}\".format(mset))\n        amap = self.association_map\n        results = []\n        for m in mset:\n            if m in amap:\n                for t in amap[m]:\n                    results.append( (m,t) )\n        return results", "response": "Query for a set of associations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef query(self, terms=None, negated_terms=None):\n\n        if terms is None:\n            terms = []\n        matches_all = 'owl:Thing' in terms\n        if negated_terms is None:\n            negated_terms = []\n        termset = set(terms)\n        negated_termset = set(negated_terms)\n        matches = []\n        n_terms = len(termset)\n        for subj in self.subjects:\n            if matches_all or len(termset.intersection(self.inferred_types(subj))) == n_terms:\n                if len(negated_termset.intersection(self.inferred_types(subj))) == 0:\n                    matches.append(subj)\n        return matches", "response": "Basic boolean query using inference."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef query_intersections(self, x_terms=None, y_terms=None, symmetric=False):\n        if x_terms is None:\n            x_terms = []\n        if y_terms is None:\n            y_terms = []\n        xset = set(x_terms)\n        yset = set(y_terms)\n        zset = xset.union(yset)\n\n        # first built map of gene->termClosure.\n        # this could be calculated ahead of time for all g,\n        # but this may be space-expensive. TODO: benchmark\n        gmap={}\n        for z in zset:\n            gmap[z] = []\n        for subj in self.subjects:\n            ancs = self.inferred_types(subj)\n            for a in ancs.intersection(zset):\n                gmap[a].append(subj)\n        for z in zset:\n            gmap[z] = set(gmap[z])\n        ilist = []\n        for x in x_terms:\n            for y in y_terms:\n                if not symmetric or x<y:\n                    shared = gmap[x].intersection(gmap[y])\n                    union = gmap[x].union(gmap[y])\n                    j = 0\n                    if len(union)>0:\n                        j = len(shared) / len(union)\n                    ilist.append({'x':x,'y':y,'shared':shared, 'c':len(shared), 'j':j})\n        return ilist", "response": "Query for intersections of terms in two lists\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef intersectionlist_to_matrix(ilist, xterms, yterms):\n        z = [ [0] * len(xterms) for i1 in range(len(yterms)) ]\n    \n        xmap = {}\n        xi = 0\n        for x in xterms:\n            xmap[x] = xi\n            xi = xi+1\n        \n        ymap = {}\n        yi = 0\n        for y in yterms:\n            ymap[y] = yi\n            yi = yi+1\n            \n        for i in ilist:\n            z[ymap[i['y']]][xmap[i['x']]] = i['j']\n            \n        logging.debug(\"Z={}\".format(z))\n        return (z,xterms,yterms)", "response": "Convert intersectionlist to a matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_dataframe(self, fillna=True, subjects=None):\n        entries = []\n        selected_subjects = self.subjects\n        if subjects is not None:\n            selected_subjects = subjects\n            \n        for s in selected_subjects:\n            vmap = {}\n            for c in self.inferred_types(s):\n                vmap[c] = 1\n            entries.append(vmap)\n        logging.debug(\"Creating DataFrame\")\n        df = pd.DataFrame(entries, index=selected_subjects)\n        if fillna:\n            logging.debug(\"Performing fillna...\")\n            df = df.fillna(0)\n        return df", "response": "Return association set as pandas DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef label(self, id):\n        if self.ontology is not None:\n            label = self.ontology.label(id)\n            if label is not None:\n                return label\n        if self.subject_label_map is not None and id in self.subject_label_map:\n            return self.subject_label_map[id]\n        return None", "response": "Returns the label for a subject id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a sub - ontology based on the objects in the object.", "response": "def subontology(self, minimal=False):\n        \"\"\"\n        Generates a sub-ontology based on associations\n        \"\"\"\n        return self.ontology.subontology(self.objects, minimal=minimal)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef associations(self, subject, object=None):\n        if object is None:\n            if self.associations_by_subj is not None:\n                return self.associations_by_subj[subject]\n            else:\n                return []\n        else:\n            if self.associations_by_subj_obj is not None:\n                return self.associations_by_subj_obj[(subject,object)]\n            else:\n                return []", "response": "Returns a list of all association objects that match the given subject - object pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the jaccard index of inferred associations of two subjects s1 and s2.", "response": "def jaccard_similarity(self,s1,s2):\n        \"\"\"\n        Calculate jaccard index of inferred associations of two subjects\n\n        |ancs(s1) /\\ ancs(s2)|\n        ---\n        |ancs(s1) \\/ ancs(s2)|\n\n        \"\"\"\n        a1 = self.inferred_types(s1)\n        a2 = self.inferred_types(s2)\n        num_union = len(a1.union(a2))\n        if num_union == 0:\n            return 0.0\n        return len(a1.intersection(a2)) / num_union"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef similarity_matrix(self, x_subjects=None, y_subjects=None, symmetric=False):\n        if x_subjects is None:\n            x_subjects = []\n        if y_subjects is None:\n            y_subjects = []\n\n        xset = set(x_subjects)\n        yset = set(y_subjects)\n        zset = xset.union(yset)\n\n        # first built map of gene->termClosure.\n        # this could be calculated ahead of time for all g,\n        # but this may be space-expensive. TODO: benchmark\n        gmap={}\n        for z in zset:\n            gmap[z] = self.inferred_types(z)\n        ilist = []\n        for x in x_subjects:\n            for y in y_subjects:\n                if not symmetric or x<y:\n                    shared = gmap[x].intersection(gmap[y])\n                    union = gmap[x].union(gmap[y])\n                    j = 0\n                    if len(union)>0:\n                        j = len(shared) / len(union)\n                    ilist.append({'x':x,'y':y,'shared':shared, 'c':len(shared), 'j':j})\n        return self.intersectionlist_to_matrix(ilist, x_subjects, y_subjects)", "response": "Query for similarity matrix between groups of subjects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef jaccard_similarity(aset:AssociationSet, s1:str, s2:str) -> float:\n    a1 = aset.inferred_types(s1)\n    a2 = aset.inferred_types(s2)\n    num_union = len(a1.union(a2))\n    if num_union == 0:\n        return 0.0\n    return len(a1.intersection(a2)) / num_union", "response": "Calculate the jaccard index of inferred associations of two subjects s1 and s2."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_digraph(ont, relations=None, writecache=False):\n    digraph = networkx.MultiDiGraph()\n    logging.info(\"Getting edges (may be cached)\")\n    for (s,p,o) in get_edges(ont):\n        p = map_legacy_pred(p)\n        if relations is None or p in relations:\n            digraph.add_edge(o,s,pred=p)\n    logging.info(\"Getting labels (may be cached)\")\n    for (n,label) in fetchall_labels(ont):\n        digraph.add_node(n, **{'label':label})\n    return digraph", "response": "Returns a networkx. MultiDiGraph object corresponding to a remote ontology."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xref_graph(ont):\n    g = networkx.MultiGraph()\n    for (c,x) in fetchall_xrefs(ont):\n        g.add_edge(c,x,source=c)\n    return g", "response": "Returns a basic graph object corresponding to a remote ontology"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching all basic edges from a remote ontology", "response": "def get_edges(ont):\n    \"\"\"\n    Fetches all basic edges from a remote ontology\n    \"\"\"\n    logging.info(\"QUERYING:\"+ont)\n    edges = [(c,SUBCLASS_OF, d) for (c,d) in fetchall_isa(ont)]\n    edges += fetchall_svf(ont)\n    edges += [(c,SUBPROPERTY_OF, d) for (c,d) in fetchall_subPropertyOf(ont)]\n    if len(edges) == 0:\n        logging.warn(\"No edges for {}\".format(ont))\n    return edges"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for things using a given searchterm", "response": "def search(ont, searchterm):\n    \"\"\"\n    Search for things using labels\n    \"\"\"\n    namedGraph = get_named_graph(ont)\n    query = \"\"\"\n    SELECT ?c ?l WHERE {{\n    GRAPH <{g}>  {{\n    ?c rdfs:label ?l\n    FILTER regex(?l,'{s}','i')\n    }}\n    }}\n    \"\"\".format(s=searchterm, g=namedGraph)\n    bindings = run_sparql(query)\n    return [(r['c']['value'],r['l']['value']) for r in bindings]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding all terms in a subset", "response": "def get_terms_in_subset(ont, subset):\n    \"\"\"\n    Find all nodes in a subset.\n\n    We assume the oboInOwl encoding of subsets, and subset IDs are IRIs\n    \"\"\"\n    namedGraph = get_named_graph(ont)\n\n    # note subsets have an unusual encoding\n    query = \"\"\"\n    prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n    SELECT ?c ? WHERE {{\n    GRAPH <{g}>  {{\n    ?c oboInOwl:inSubset ?s ;\n       rdfs:label ?l\n    FILTER regex(?s,'#{s}$','i')\n    }}\n    }}\n    \"\"\".format(s=subset, g=namedGraph)\n    bindings = run_sparql(query)\n    return [(r['c']['value'],r['l']['value']) for r in bindings]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun a SPARQL query on a given Ontology", "response": "def run_sparql_on(q, ontology):\n    \"\"\"\n    Run a SPARQL query (q) on a given Ontology (Enum EOntology)\n    \"\"\"\n    logging.info(\"Connecting to \" + ontology.value + \" SPARQL endpoint...\")\n    sparql = SPARQLWrapper(ontology.value)\n    logging.info(\"Made wrapper: {}\".format(sparql))\n    sparql.setQuery(q)\n    sparql.setReturnFormat(JSON)\n    logging.info(\"Query: {}\".format(q))\n    results = sparql.query().convert()\n    bindings = results['results']['bindings']\n    logging.info(\"Rows: {}\".format(len(bindings)))\n    for r in bindings:\n        curiefy(r)\n    return bindings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching all rdfs labels assertions for an ontology", "response": "def fetchall_labels(ont):\n    \"\"\"\n    fetch all rdfs:label assertions for an ontology\n    \"\"\"\n    logging.info(\"fetching rdfs:labels for: \"+ont)\n    namedGraph = get_named_graph(ont)\n    queryBody = querybody_label()\n    query = \"\"\"\n    SELECT * WHERE {{\n    GRAPH <{g}>  {q}\n    }}\n    \"\"\".format(q=queryBody, g=namedGraph)\n    bindings = run_sparql(query)\n    rows = [(r['c']['value'], r['l']['value']) for r in bindings]\n    return rows"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch all synonyms for an ontology", "response": "def fetchall_syns(ont):\n    \"\"\"\n    fetch all synonyms for an ontology\n    \"\"\"\n    logging.info(\"fetching syns for: \"+ont)\n    namedGraph = get_named_graph(ont)\n    queryBody = querybody_syns()\n    query = \"\"\"\n    prefix oboInOwl: <http://www.geneontology.org/formats/oboInOwl#>\n    SELECT * WHERE {{\n    GRAPH <{g}>  {q}\n    }}\n    \"\"\".format(q=queryBody, g=namedGraph)\n    bindings = run_sparql(query)\n    rows = [(r['c']['value'], r['r']['value'], r['l']['value']) for r in bindings]\n    return rows"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetchall_textdefs(ont):\n    logging.info(\"fetching text defs for: \"+ont)\n    namedGraph = get_named_graph(ont)\n    query = \"\"\"\n    prefix IAO: <http://purl.obolibrary.org/obo/IAO_>\n    SELECT * WHERE {{\n    GRAPH <{g}>  {{\n      ?c IAO:0000115 ?d\n    }}\n    FILTER (!isBlank(?c))\n    }}\n    \"\"\".format(g=namedGraph)\n    bindings = run_sparql(query)\n    rows = [(r['c']['value'], r['d']['value']) for r in bindings]\n    return rows", "response": "fetch all text defs for an ontology"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching all rdfs : label assertions for a URI", "response": "def anyont_fetch_label(id):\n    \"\"\"\n    fetch all rdfs:label assertions for a URI\n    \"\"\"\n    iri = expand_uri(id, strict=False)\n    query = \"\"\"\n    SELECT ?label WHERE {{\n    <{iri}> rdfs:label ?label\n    }}\n    \"\"\".format(iri=iri)\n    bindings = run_sparql(query)\n    rows = [r['label']['value'] for r in bindings]\n    return rows[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef batch_fetch_labels(ids):\n    m = {}\n    for id in ids:\n        label = anyont_fetch_label(id)\n        if label is not None:\n            m[id] = label\n    return m", "response": "fetch all rdfs : label assertions for a set of CURIEs\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a SPARQL json result by splitting it into a list of strings", "response": "def transform(data, keysToSplit=[]):\n    \"\"\"\n    Transform a SPARQL json result by:\n    1) outputing only { key : value }, removing datatype\n    2) for some keys, transform them into array based on SEPARATOR\n    \"\"\"\n    transformed = { }\n    for key in data:\n        if key in keysToSplit:\n            transformed[key] = data[key]['value'].split(SEPARATOR)\n        else:\n            transformed[key] = data[key]['value']\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming a SPARQL json array based on the rules of transform", "response": "def transformArray(data, keysToSplit=[]):\n    \"\"\"\n    Transform a SPARQL json array based on the rules of transform\n    \"\"\"\n    transformed = [ ]\n    for item in data:\n        transformed.append(transform(item, keysToSplit))\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef coderef_to_ecoclass(self, code, reference=None):\n        mcls = None\n        for (this_code,this_ref,cls) in self.mappings():\n            if str(this_code) == str(code):\n                if this_ref  == reference:\n                    return cls\n                if this_ref is None:\n                    mcls = cls\n                    \n        return mcls", "response": "Returns the ECO class corresponding to a GAF code and reference."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ecoclass_to_coderef(self, cls):\n        code = ''\n        ref = None\n        for (code,ref,this_cls) in self.mappings():\n            if cls == this_cls:\n                return code, ref\n        return None, None", "response": "This function maps an ECO class to a GAF code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_checksum(file):\n    with open(file, 'rb') as FH:\n        contents = FH.read()\n    return hashlib.sha256(contents).hexdigest()", "response": "Get SHA256 hash of the contents of a given file"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, handle=None, handle_type=None, **args):\n        if handle is None:\n            self.test = self.test+1\n            logging.info(\"T: \"+str(self.test))\n            global default_ontology\n            if default_ontology is None:\n                logging.info(\"Creating new instance of default ontology\")\n                default_ontology = create_ontology(default_ontology_handle)\n            logging.info(\"Using default_ontology\")\n            return default_ontology\n        return create_ontology(handle, **args)", "response": "Creates an ontology based on a handle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching all xrefs for a prefix", "response": "def fetchall_triples_xrefs(prefix):\n    \"\"\"\n    fetch all xrefs for a prefix, e.g. CHEBI\n    \"\"\"\n    logging.info(\"fetching xrefs for: \"+prefix)\n    query = \"\"\"\n    SELECT * WHERE {{\n    ?c <{p}> ?x\n    }}\n    \"\"\".format(p=prefixmap[prefix])\n    bindings = run_sparql(query)\n    rows = [(r['c']['value'], r['x']['value']) for r in bindings]\n    return rows"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetchall_sp(s,p):\n    query = \"\"\"\n    SELECT * WHERE {{\n    <{s}> <{p}> ?x\n    }}\n    \"\"\".format(s=s, p=prefixmap[p])\n    bindings = run_sparql(query)\n    rows = [r['x']['value'] for r in bindings]\n    return rows", "response": "fetch all triples for a property s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_line(self, line):\n\n        # Returns assocparser.ParseResult\n        parsed = super().validate_line(line)\n        if parsed:\n            return parsed\n\n        if self.is_header(line):\n            return assocparser.ParseResult(line, [{ \"header\": True, \"line\": line.strip() }], False)\n\n        vals = [el.strip() for el in line.split(\"\\t\")]\n\n        # GAF v1 is defined as 15 cols, GAF v2 as 17.\n        # We treat everything as GAF2 by adding two blank columns.\n        # TODO: check header metadata to see if columns corresponds to declared dataformat version\n        if 17 > len(vals) >= 15:\n            vals += [\"\"] * (17 - len(vals))\n\n        if len(vals) > 17:\n            # If we see more than 17 columns, we will just cut off the columns after column 17\n            self.report.warning(line, assocparser.Report.WRONG_NUMBER_OF_COLUMNS, \"\",\n                msg=\"There were more than 17 columns in this line. Proceeding by cutting off extra columns after column 17.\",\n                rule=1)\n            vals = vals[:17]\n\n\n        if len(vals) != 17:\n            self.report.error(line, assocparser.Report.WRONG_NUMBER_OF_COLUMNS, \"\",\n                msg=\"There were {columns} columns found in this line, and there should be 15 (for GAF v1) or 17 (for GAF v2)\".format(columns=len(vals)),\n                rule=1)\n            return assocparser.ParseResult(line, [], True)\n\n        [db,\n         db_object_id,\n         db_object_symbol,\n         qualifier,\n         goid,\n         reference,\n         evidence,\n         withfrom,\n         aspect,\n         db_object_name,\n         db_object_synonym,\n         db_object_type,\n         taxon,\n         date,\n         assigned_by,\n         annotation_xp,\n         gene_product_isoform] = vals\n\n        split_line = assocparser.SplitLine(line=line, values=vals, taxon=taxon)\n\n        ## check for missing columns\n        if db == \"\":\n            self.report.error(line, Report.INVALID_IDSPACE, \"EMPTY\", \"col1 is empty\", taxon=taxon, rule=1)\n            return assocparser.ParseResult(line, [], True)\n        if db_object_id == \"\":\n            self.report.error(line, Report.INVALID_ID, \"EMPTY\", \"col2 is empty\", taxon=taxon, rule=1)\n            return assocparser.ParseResult(line, [], True)\n        if taxon == \"\":\n            self.report.error(line, Report.INVALID_TAXON, \"EMPTY\", \"taxon column is empty\", taxon=taxon, rule=1)\n            return assocparser.ParseResult(line, [], True)\n        if reference == \"\":\n            self.report.error(line, Report.INVALID_ID, \"EMPTY\", \"reference column 6 is empty\", taxon=taxon, rule=1)\n            return assocparser.ParseResult(line, [], True)\n\n        if self.config.group_idspace is not None and assigned_by not in self.config.group_idspace:\n            self.report.warning(line, Report.INVALID_ID, assigned_by,\n                \"GORULE:0000027: assigned_by is not present in groups reference\", taxon=taxon, rule=27)\n\n        if self.config.entity_idspaces is not None and db not in self.config.entity_idspaces:\n            # Are we a synonym?\n            upgrade = self.config.entity_idspaces.reverse(db)\n            if upgrade is not None:\n                # If we found a synonym\n                self.report.warning(line, Report.INVALID_ID_DBXREF, db, \"GORULE:0000027: {} is a synonym for the correct ID {}, and has been updated\".format(db, upgrade), taxon=taxon, rule=27)\n                db = upgrade\n\n        ## --\n        ## db + db_object_id. CARD=1\n        ## --\n        id = self._pair_to_id(db, db_object_id)\n        if not self._validate_id(id, split_line, allowed_ids=self.config.entity_idspaces):\n\n            return assocparser.ParseResult(line, [], True)\n\n\n        # Using a given gpi file to validate the gene object\n        if self.gpi is not None:\n            entity = self.gpi.get(id, None)\n            if entity is not None:\n                db_object_symbol = entity[\"symbol\"]\n                db_object_name = entity[\"name\"]\n                db_object_synonym = entity[\"synonyms\"]\n                db_object_type = entity[\"type\"]\n\n        if not self._validate_id(goid, split_line, context=ANNOTATION):\n            print(\"skipping because {} not validated!\".format(goid))\n            return assocparser.ParseResult(line, [], True)\n\n        valid_goid = self._validate_ontology_class_id(goid, split_line)\n        if valid_goid == None:\n            return assocparser.ParseResult(line, [], True)\n        goid = valid_goid\n\n        date = self._normalize_gaf_date(date, split_line)\n        if date == None:\n            return assocparser.ParseResult(line, [], True)\n\n        vals[13] = date\n\n        ecomap = self.config.ecomap\n        if ecomap is not None:\n            if ecomap.coderef_to_ecoclass(evidence, reference) is None:\n                self.report.error(line, assocparser.Report.UNKNOWN_EVIDENCE_CLASS, evidence,\n                                msg=\"Expecting a known ECO GAF code, e.g ISS\", rule=1)\n                return assocparser.ParseResult(line, [], True)\n\n        # Throw out the line if it uses GO_REF:0000033, see https://github.com/geneontology/go-site/issues/563#event-1519351033\n        if \"GO_REF:0000033\" in reference.split(\"|\"):\n            self.report.error(line, assocparser.Report.INVALID_ID, reference,\n                                msg=\"Disallowing GO_REF:0000033 in reference field as of 03/13/2018\", rule=30)\n            return assocparser.ParseResult(line, [], True)\n\n        references = self.validate_pipe_separated_ids(reference, split_line)\n        if references == None:\n            # Reporting occurs in above function call\n            return assocparser.ParseResult(line, [], True)\n\n        # With/From\n        withfroms = self.validate_pipe_separated_ids(withfrom, split_line, empty_allowed=True, extra_delims=\",\")\n        if withfroms == None:\n            # Reporting occurs in above function call\n            return assocparser.ParseResult(line, [], True)\n\n        # validation\n        self._validate_symbol(db_object_symbol, split_line)\n\n        # Example use case: mapping from UniProtKB to MOD ID\n        if self.config.entity_map is not None:\n            id = self.map_id(id, self.config.entity_map)\n            toks = id.split(\":\")\n            db = toks[0]\n            db_object_id = toks[1:]\n            vals[1] = db_object_id\n\n        if goid.startswith(\"GO:\") and aspect.upper() not in [\"C\", \"F\", \"P\"]:\n            self.report.error(line, assocparser.Report.INVALID_ASPECT, aspect, rule=28)\n            return assocparser.ParseResult(line, [], True)\n\n\n        go_rule_results = qc.test_go_rules(vals, self.config)\n        for rule_id, result in go_rule_results.items():\n            if result.result_type == qc.ResultType.WARNING:\n                self.report.warning(line, assocparser.Report.VIOLATES_GO_RULE, goid,\n                                    msg=\"{id}: {message}\".format(id=rule_id, message=result.message), rule=int(rule_id.split(\":\")[1]))\n                # Skip the annotation\n                return assocparser.ParseResult(line, [], True)\n\n            if result.result_type == qc.ResultType.ERROR:\n                self.report.error(line, assocparser.Report.VIOLATES_GO_RULE, goid,\n                                    msg=\"{id}: {message}\".format(id=rule_id, message=result.message), rule=int(rule_id.split(\":\")[1]))\n                # Skip the annotation\n                return assocparser.ParseResult(line, [], True)\n\n        ## --\n        ## end of line re-processing\n        ## --\n        # regenerate line post-mapping\n        line = \"\\t\".join(vals)\n\n        ## --\n        ## taxon CARD={1,2}\n        ## --\n        ## if a second value is specified, this is the interacting taxon\n        ## We do not use the second value\n        taxons = taxon.split(\"|\")\n        normalized_taxon = self._taxon_id(taxons[0], split_line)\n        if normalized_taxon == None:\n            self.report.error(line, assocparser.Report.INVALID_TAXON, taxon,\n                                msg=\"Taxon ID is invalid\")\n            return assocparser.ParseResult(line, [], True)\n\n        self._validate_taxon(normalized_taxon, split_line)\n\n        interacting_taxon = None\n        if len(taxons) == 2:\n            interacting_taxon = self._taxon_id(taxons[1], split_line)\n            if interacting_taxon == None:\n                self.report.error(line, assocparser.Report.INVALID_TAXON, taxon,\n                                    msg=\"Taxon ID is invalid\")\n                return assocparser.ParseResult(line, [], True)\n\n        ## --\n        ## db_object_synonym CARD=0..*\n        ## --\n        synonyms = db_object_synonym.split(\"|\")\n        if db_object_synonym == \"\":\n            synonyms = []\n\n        ## --\n        ## parse annotation extension\n        ## See appendix in http://doi.org/10.1186/1471-2105-15-155\n        ## --\n        object_or_exprs = self._parse_full_extension_expression(annotation_xp, line=split_line)\n\n        ## --\n        ## qualifier\n        ## --\n        negated, relation, other_qualifiers = self._parse_qualifier(qualifier, aspect)\n\n        ## --\n        ## goid\n        ## --\n        # TODO We shouldn't overload buildin keywords/functions\n        object = {'id': goid,\n                  'taxon': normalized_taxon}\n\n        # construct subject dict\n        subject = {\n            'id': id,\n            'label': db_object_symbol,\n            'type': db_object_type,\n            'fullname': db_object_name,\n            'synonyms': synonyms,\n            'taxon': {\n                'id': normalized_taxon\n            }\n        }\n\n        ## --\n        ## gene_product_isoform\n        ## --\n        ## This is mapped to a more generic concept of subject_extensions\n        subject_extns = []\n        if gene_product_isoform is not None and gene_product_isoform != '':\n            subject_extns.append({'property': 'isoform', 'filler': gene_product_isoform})\n\n        object_extensions = {}\n        if object_or_exprs is not None and len(object_or_exprs) > 0:\n            object_extensions['union_of'] = object_or_exprs\n\n        ## --\n        ## evidence\n        ## reference\n        ## withfrom\n        ## --\n        evidence_obj = {\n            'type': evidence,\n            'has_supporting_reference': references,\n            'with_support_from': withfroms\n        }\n\n        ## Construct main return dict\n        assoc = {\n            'source_line': line,\n            'subject': subject,\n            'object': object,\n            'negated': negated,\n            'qualifiers': other_qualifiers,\n            'aspect': aspect,\n            'relation': {\n                'id': relation\n            },\n            'interacting_taxon': interacting_taxon,\n            'evidence': evidence_obj,\n            'provided_by': assigned_by,\n            'date': date,\n            'subject_extensions': subject_extns,\n            'object_extensions': object_extensions\n        }\n\n        return assocparser.ParseResult(line, [assoc], False, evidence.upper())", "response": "Parses a single line of GAF file and returns a tuple of parsed_line associations"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an integer into a gorule key id.", "response": "def _rule_id(self, id: int) -> str:\n        \"\"\"\n        Convert an integer into a gorule key id.\n        \"\"\"\n        if id is None or id == 0 or id >= 10000000:\n            return \"other\"\n\n        return \"gorule-{:0>7}\".format(id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message(self, message: Message, rule: Optional[int]) -> None:\n        rule_id = self._rule_id(rule)\n        if rule_id not in self.messages:\n            self.messages[rule_id] = []\n\n        if len(self.messages[rule_id]) < self._rule_message_cap:\n            self.messages[rule_id].append(message)", "response": "Adds a message to the appropriate list of messages."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a json file to a networkx MultiDiGraph", "response": "def convert_json_file(obographfile, **args):\n    \"\"\"\n    Return a networkx MultiDiGraph of the ontologies\n    serialized as a json string\n\n    \"\"\"\n    f = open(obographfile, 'r')\n    jsonstr = f.read()\n    f.close()\n    return convert_json_object(json.loads(jsonstr), **args)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_json_object(obographdoc, **args):\n    digraph = networkx.MultiDiGraph()\n    xref_graph = networkx.MultiGraph()\n    logical_definitions = []\n    property_chain_axioms = []\n    context = obographdoc.get('@context',{})\n    logging.info(\"CONTEXT: {}\".format(context))\n    mapper = OboJsonMapper(digraph=digraph, context=context)\n    ogs = obographdoc['graphs']\n    base_og = ogs[0]\n    for og in ogs:\n        # TODO: refactor this\n        mapper.add_obograph_digraph(og, xref_graph=xref_graph,\n                                    logical_definitions=logical_definitions,\n                                    property_chain_axioms=property_chain_axioms, **args)\n\n    return {\n        'id': base_og.get('id'),\n        'meta': base_og.get('meta'),\n        'graph': mapper.digraph,\n        'xref_graph': xref_graph,\n        'graphdoc': obographdoc,\n        'logical_definitions': logical_definitions,\n        'property_chain_axioms': property_chain_axioms\n        }", "response": "Converts a json object from the OboOntology to a networkx MultiDiGraph of the ontologies\n    serialized as a json object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_obograph_digraph(self, og, node_type=None, predicates=None, xref_graph=None, logical_definitions=None,\n                             property_chain_axioms=None,\n                             parse_meta=True,\n                             **args):\n        \"\"\"\n        Converts a single obograph to Digraph edges and adds to an existing networkx DiGraph\n        \"\"\"\n        digraph = self.digraph\n        logging.info(\"NODES: {}\".format(len(og['nodes'])))\n\n        # if client passes an xref_graph we must parse metadata\n        if xref_graph is not None:\n            parse_meta = True\n\n        for n in og['nodes']:\n            is_obsolete =  'is_obsolete' in n and n['is_obsolete'] == 'true'\n            if is_obsolete:\n                continue\n            if node_type is not None and ('type' not in n or n['type'] != node_type):\n                continue\n            id = self.contract_uri(n['id'])\n            digraph.add_node(id, **n)\n            if 'lbl' in n:\n                digraph.node[id]['label'] = n['lbl']\n            if parse_meta and 'meta' in n:\n                if n['meta'] is None:\n                    n['meta'] = {}\n                meta = self.transform_meta(n['meta'])\n                if xref_graph is not None and 'xrefs' in meta:\n                    for x in meta['xrefs']:\n                        xref_graph.add_edge(self.contract_uri(x['val']), id, source=id)\n        logging.info(\"EDGES: {}\".format(len(og['edges'])))\n        for e in og['edges']:\n            sub = self.contract_uri(e['sub'])\n            obj = self.contract_uri(e['obj'])\n            pred = self.contract_uri(e['pred'])\n            pred = map_legacy_pred(pred)\n            if pred == 'is_a':\n                pred = 'subClassOf'\n            if predicates is None or pred in predicates:\n                digraph.add_edge(obj, sub, pred=pred)\n        if 'equivalentNodesSets' in og:\n            nslist = og['equivalentNodesSets']\n            logging.info(\"CLIQUES: {}\".format(len(nslist)))\n            for ns in nslist:\n                equivNodeIds = ns['nodeIds']\n                for i in ns['nodeIds']:\n                    ix = self.contract_uri(i)\n                    for j in ns['nodeIds']:\n                        if i != j:\n                            jx = self.contract_uri(j)\n                            digraph.add_edge(ix, jx, pred='equivalentTo')\n        if logical_definitions is not None and 'logicalDefinitionAxioms' in og:\n            for a in og['logicalDefinitionAxioms']:\n                ld = LogicalDefinition(self.contract_uri(a['definedClassId']),\n                                       [self.contract_uri(x) for x in a['genusIds']],\n                                       [(self.contract_uri(x['propertyId']),\n                                         self.contract_uri(x['fillerId'])) for x in a['restrictions'] if x is not None])\n                logical_definitions.append(ld)\n        if property_chain_axioms is not None and 'propertyChainAxioms' in og:\n            for a in og['propertyChainAxioms']:\n                pca = PropertyChainAxiom(predicate_id=self.contract_uri(a['predicateId']),\n                                         chain_predicate_ids=[self.contract_uri(x) for x in a['chainPredicateIds']])\n                property_chain_axioms.append(pca)", "response": "Converts a single obograph to Digraph edges and adds to an existing networkx DiGraph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert \".module\" to \"./module\", \"..module\" to \"../module\", etc.", "response": "def convert_to_path(name):\n    \"\"\"Converts \".module\" to \"./module\", \"..module\" to \"../module\", etc.\"\"\"\n    if name.startswith('.'):\n        remainder = name.lstrip('.')\n        dot_count = (len(name) - len(remainder))\n        prefix = '../'*(dot_count-1)\n    else:\n        remainder = name\n        dot_count = 0\n        prefix = ''\n    filename = prefix + os.path.join(*remainder.split('.'))\n    return (filename, dot_count)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a python filename to a module relative to pythonpath.", "response": "def infer_module_name(filename, fspath):\n    \"\"\"Convert a python filename to a module relative to pythonpath.\"\"\"\n    filename, _ = os.path.splitext(filename)\n    for f in fspath:\n        short_name = f.relative_path(filename)\n        if short_name:\n            # The module name for __init__.py files is the directory.\n            if short_name.endswith(os.path.sep + \"__init__\"):\n                short_name = short_name[:short_name.rfind(os.path.sep)]\n            return short_name.replace(os.path.sep, '.')\n    # We have not found filename relative to anywhere in pythonpath.\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_absolute_name(package, relative_name):\n    path = package.split('.') if package else []\n    name = relative_name.lstrip('.')\n    ndots = len(relative_name) - len(name)\n    if ndots > len(path):\n        return relative_name\n    absolute_path = path[:len(path) + 1 - ndots]\n    if name:\n        absolute_path.append(name)\n    return '.'.join(absolute_path)", "response": "Joins a package name and a relative name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resolve_import(self, item):\n        name = item.name\n        # The last part in `from a.b.c import d` might be a symbol rather than a\n        # module, so we try a.b.c and a.b.c.d as names.\n        short_name = None\n        if item.is_from and not item.is_star:\n            if '.' in name.lstrip('.'):\n                # The name is something like `a.b.c`, so strip off `.c`.\n                rindex = name.rfind('.')\n            else:\n                # The name is something like `..c`, so strip off just `c`.\n                rindex = name.rfind('.') + 1\n            short_name = name[:rindex]\n\n        if import_finder.is_builtin(name):\n            filename = name + '.so'\n            return Builtin(filename, name)\n\n        filename, level = convert_to_path(name)\n        if level:\n            # This is a relative import; we need to resolve the filename\n            # relative to the importing file path.\n            filename = os.path.normpath(\n                os.path.join(self.current_directory, filename))\n\n        files = [(name, filename)]\n        if short_name:\n            short_filename = os.path.dirname(filename)\n            files.append((short_name, short_filename))\n\n        for module_name, path in files:\n            for fs in self.fs_path:\n                f = self._find_file(fs, path)\n                if not f or f == self.current_module.path:\n                    # We cannot import a file from itself.\n                    continue\n                if item.is_relative():\n                    package_name = self.current_module.package_name\n                    if package_name is None:\n                        # Relative import in non-package\n                        raise ImportException(name)\n                    module_name = get_absolute_name(package_name, module_name)\n                    if isinstance(self.current_module, System):\n                        return System(f, module_name)\n                return Local(f, module_name, fs)\n\n        # If the module isn't found in the explicit pythonpath, see if python\n        # itself resolved it.\n        if item.source:\n            prefix, ext = os.path.splitext(item.source)\n            mod_name = name\n            # We need to check for importing a symbol here too.\n            if short_name:\n                mod = prefix.replace(os.path.sep, '.')\n                mod = utils.strip_suffix(mod, '.__init__')\n                if not mod.endswith(name) and mod.endswith(short_name):\n                    mod_name = short_name\n\n            if ext == '.pyc':\n                pyfile = prefix + '.py'\n                if os.path.exists(pyfile):\n                    return System(pyfile, mod_name)\n            elif not ext:\n                pyfile = os.path.join(prefix, \"__init__.py\")\n                if os.path.exists(pyfile):\n                    return System(pyfile, mod_name)\n            return System(item.source, mod_name)\n\n        raise ImportException(name)", "response": "Simulate how Python resolves imports."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_all(self, import_items):\n        for import_item in import_items:\n            try:\n                yield self.resolve_import(import_item)\n            except ImportException as err:\n                logging.info('unknown module %s', err.module_name)", "response": "Resolves a list of imports.\n            yields filenames.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef path_from_pythonpath(pythonpath):\n    path = fs.Path()\n    for p in pythonpath.split(os.pathsep):\n        path.add_path(utils.expand_path(p), 'os')\n    return path", "response": "Create an fs. Path object from a string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_node(import_graph, node, indent):\n    if isinstance(node, graph.NodeSet):\n        ind = '  ' * indent\n        out = [ind + 'cycle {'] + [\n                format_file_node(import_graph, n, indent + 1)\n                for n in node.nodes\n        ] + [ind + '}']\n        return '\\n'.join(out)\n    else:\n        return format_file_node(import_graph, node, indent)", "response": "Helper function for print_tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_package(parts):\n    for i in range(len(parts), 0, -1):\n        prefix = '.'.join(parts[0:i])\n        if prefix in sys.modules:\n            return i, sys.modules[prefix]\n    return 0, None", "response": "Helper function for _resolve_import_versioned."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef resolve_import(name, is_from, is_star):\n    # Don't try to resolve relative imports or builtins here; they will be\n    # handled by resolve.Resolver\n    if name.startswith('.') or is_builtin(name):\n        return None\n    ret = _resolve_import(name)\n    if ret is None and is_from and not is_star:\n        package, _ = name.rsplit('.', 1)\n        ret = _resolve_import(package)\n    return ret", "response": "Use python to resolve an import.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all the imports in a file.", "response": "def get_imports(filename):\n    \"\"\"Get all the imports in a file.\n\n    Each import is a tuple of:\n      (name, alias, is_from, is_star, source_file)\n    \"\"\"\n    with open(filename, \"rb\") as f:\n        src = f.read()\n    finder = ImportFinder()\n    finder.visit(ast.parse(src, filename=filename))\n    imports = []\n    for i in finder.imports:\n        name, _, is_from, is_star = i\n        imports.append(i + (resolve_import(name, is_from, is_star),))\n    return imports"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a file and all its immediate dependencies to the graph.", "response": "def add_file(self, filename):\n        \"\"\"Add a file and all its immediate dependencies to the graph.\"\"\"\n\n        assert not self.final, 'Trying to mutate a final graph.'\n        self.add_source_file(filename)\n        resolved, unresolved = self.get_file_deps(filename)\n        self.graph.add_node(filename)\n        for f in resolved:\n            self.graph.add_node(f)\n            self.graph.add_edge(filename, f)\n        for imp in unresolved:\n            self.broken_deps[filename].add(imp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef follow_file(self, f, seen, trim):\n        return (f not in self.graph.nodes and\n                f not in seen and\n                (not trim or\n                 not isinstance(self.provenance[f],\n                                (resolve.Builtin, resolve.System))))", "response": "Whether to recurse into a file s dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_file_recursive(self, filename, trim=False):\n\n        assert not self.final, 'Trying to mutate a final graph.'\n        self.add_source_file(filename)\n        queue = collections.deque([filename])\n        seen = set()\n        while queue:\n            filename = queue.popleft()\n            self.graph.add_node(filename)\n            try:\n                deps, broken = self.get_file_deps(filename)\n            except parsepy.ParseError:\n                # Python couldn't parse `filename`. If we're sure that it is a\n                # Python file, we mark it as unreadable and keep the node in the\n                # graph so importlab's callers can do their own syntax error\n                # handling if desired.\n                if filename.endswith('.py'):\n                    self.unreadable_files.add(filename)\n                else:\n                    self.graph.remove_node(filename)\n                continue\n            for f in broken:\n                self.broken_deps[filename].add(f)\n            for f in deps:\n                if self.follow_file(f, seen, trim):\n                    queue.append(f)\n                    seen.add(f)\n                self.graph.add_node(f)\n                self.graph.add_edge(filename, f)", "response": "Add a file and all its recursive dependencies to the graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shrink_to_node(self, scc):\n        assert not self.final, 'Trying to mutate a final graph.'\n        self.graph.add_node(scc)\n        edges = list(self.graph.edges)\n        for k, v in edges:\n            if k not in scc and v in scc:\n                self.graph.remove_edge(k, v)\n                self.graph.add_edge(k, scc)\n            elif k in scc and v not in scc:\n                self.graph.remove_edge(k, v)\n                self.graph.add_edge(scc, v)\n        for node in scc.nodes:\n            self.graph.remove_node(node)", "response": "Shrink a strongly connected component into a node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(self):\n\n        assert not self.final, 'Trying to mutate a final graph.'\n\n        # Replace each strongly connected component with a single node `NodeSet`\n        for scc in sorted(nx.kosaraju_strongly_connected_components(self.graph),\n                          key=len, reverse=True):\n            if len(scc) == 1:\n                break\n\n            self.shrink_to_node(NodeSet(scc))\n\n        self.final = True", "response": "Finalise the graph after adding all input files to it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sorted_source_files(self):\n\n        assert self.final, 'Call build() before using the graph.'\n        out = []\n        for node in nx.topological_sort(self.graph):\n            if isinstance(node, NodeSet):\n                out.append(node.nodes)\n            else:\n                # add a one-element list for uniformity\n                out.append([node])\n        return list(reversed(out))", "response": "Returns a list of targets in topologically sorted order."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deps_list(self):\n\n        assert self.final, 'Call build() before using the graph.'\n        out = []\n        for node in nx.topological_sort(self.graph):\n            deps = [v for k, v in self.graph.out_edges([node])]\n            out.append((node, deps))\n        return out", "response": "Returns a list of tuples containing target and dependencies."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_unresolved(self):\n        assert self.final, 'Call build() before using the graph.'\n        out = set()\n        for v in self.broken_deps.values():\n            out |= v\n        return out", "response": "Returns a set of all unresolved imports."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate and return a final import graph.", "response": "def create(cls, env, filenames, trim=False):\n        \"\"\"Create and return a final graph.\n\n        Args:\n          env: An environment.Environment object\n          filenames: A list of filenames\n          trim: Whether to trim the dependencies of builtin and system files.\n\n        Returns:\n          An immutable ImportGraph with the recursive dependencies of all the\n          files in filenames\n        \"\"\"\n        import_graph = cls(env)\n        for filename in filenames:\n            import_graph.add_file_recursive(os.path.abspath(filename), trim)\n        import_graph.build()\n        return import_graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_source_file_provenance(self, filename):\n        module_name = resolve.infer_module_name(filename, self.path)\n        return resolve.Direct(filename, module_name)", "response": "Infer the module name if possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef collect_files(path, extension):\n\n    # We should only call this on an actual directory; callers should do the\n    # validation.\n    assert os.path.isdir(path)\n    out = []\n    # glob would be faster (see PEP471) but python glob doesn't do **/*\n    for root, _, files in os.walk(path):\n        out += [os.path.join(root, f) for f in files if f.endswith(extension)]\n    return out", "response": "Collect all the files with extension in a directory tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands a list of filenames passed in as sources.", "response": "def expand_source_files(filenames, cwd=None):\n    \"\"\"Expand a list of filenames passed in as sources.\n\n    This is a helper function for handling command line arguments that specify a\n    list of source files and directories.\n\n    Any directories in filenames will be scanned recursively for .py files.\n    Any files that do not end with \".py\" will be dropped.\n\n    Args:\n        filenames: A list of filenames to process.\n        cwd: An optional working directory to expand relative paths\n    Returns:\n        A list of sorted full paths to .py files\n    \"\"\"\n    out = []\n    for f in expand_paths(filenames, cwd):\n        if os.path.isdir(f):\n            # If we have a directory, collect all the .py files within it.\n            out += collect_files(f, \".py\")\n        else:\n            if f.endswith(\".py\"):\n                out.append(f)\n    return sorted(set(out))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a suffix from a string if it exists.", "response": "def strip_suffix(string, suffix):\n    \"\"\"Remove a suffix from a string if it exists.\"\"\"\n    if string.endswith(suffix):\n        return string[:-(len(suffix))]\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_directory(self, filename):\n        path = os.path.join(self.path, filename)\n        makedirs(path)\n        return path", "response": "Create a subdirectory in the temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_file(self, filename, indented_data=None):\n        filedir, filename = os.path.split(filename)\n        if filedir:\n            self.create_directory(filedir)\n        path = os.path.join(self.path, filedir, filename)\n        data = indented_data\n        if isinstance(data, bytes) and not isinstance(data, str):\n            # This is binary data rather than text.\n            mode = 'wb'\n        else:\n            mode = 'w'\n            if data:\n                data = textwrap.dedent(data)\n        with open(path, mode) as fi:\n            if data:\n                fi.write(data)\n        return path", "response": "Create a file in the temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new reply for an existing Thread.", "response": "def new_reply(cls, thread, user, content):\n        \"\"\"\n        Create a new reply for an existing Thread.\n\n        Mark thread as unread for all other participants, and\n        mark thread as read by replier.\n        \"\"\"\n        msg = cls.objects.create(thread=thread, sender=user, content=content)\n        thread.userthread_set.exclude(user=user).update(deleted=False, unread=True)\n        thread.userthread_set.filter(user=user).update(deleted=False, unread=False)\n        message_sent.send(sender=cls, message=msg, thread=thread, reply=True)\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_message(cls, from_user, to_users, subject, content):\n        thread = Thread.objects.create(subject=subject)\n        for user in to_users:\n            thread.userthread_set.create(user=user, deleted=False, unread=True)\n        thread.userthread_set.create(user=from_user, deleted=True, unread=False)\n        msg = cls.objects.create(thread=thread, sender=from_user, content=content)\n        message_sent.send(sender=cls, message=msg, thread=thread, reply=False)\n        return msg", "response": "Create a new Message and Thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether there are unread messages for a particular thread for a user.", "response": "def unread(thread, user):\n    \"\"\"\n    Check whether there are any unread messages for a particular thread for a user.\n    \"\"\"\n    return bool(thread.userthread_set.filter(user=user, unread=True))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits text and returns a list of the resulting sentences.", "response": "def split(self, text):\n        \"\"\"Splits text and returns a list of the resulting sentences.\"\"\"\n        text = cleanup(text)\n        return self.sent_detector.tokenize(text.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns file contents as string.", "response": "def str_from_file(path):\n    \"\"\"\n    Return file contents as string.\n\n    \"\"\"\n    with open(path) as f:\n        s = f.read().strip()\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xml_equal(xml_file1, xml_file2):\n    def canonical(xml_file):\n        # poor man's canonicalization, since we don't want to install\n        # external packages just for unittesting\n        s = et.tostring(et.parse(xml_file).getroot()).decode(\"UTF-8\")\n        s = re.sub(\"[\\n|\\t]*\", \"\", s)\n        s = re.sub(\"\\s+\", \" \", s)\n        s = \"\".join(sorted(s)).strip()\n        return s\n\n    return canonical(xml_file1) == canonical(xml_file2)", "response": "Parse xml and convert to a canonical string representation so we don t worry about semantically meaningless differences between two XML files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_files(dir_path, recursive=True):\n\n    for root, dirs, files in os.walk(dir_path):\n        file_list = [os.path.join(root, f) for f in files]\n        if recursive:\n            for dir in dirs:\n                dir = os.path.join(root, dir)\n                file_list.extend(list_files(dir, recursive=True))\n        return file_list", "response": "Return a list of files in dir_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process(input_dir, output_dir, function):\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        logger = log.get_global_console_logger()\n        logger.info(\"Processing files in {}.\".format(input_dir))\n        input_file_names = os.listdir(input_dir)\n        for input_file_name in input_file_names:\n            logger.info(\"Processing {}.\".format(input_file_name))\n            input_file = os.path.join(input_dir, input_file_name)\n            with codecs.open(input_file, \"r\", encoding=\"UTF-8\") as f:\n                input_string = f.read()\n            output_string = function(input_string)\n            output_file = os.path.join(output_dir, input_file_name)\n            with codecs.open(output_file, \"w\", encoding=\"UTF-8\") as f:\n                f.write(output_string)\n        logger.info(\"Saved processed files to {}.\".format(output_dir))", "response": "Process all files in input_dir and save the resulting ouput\n        files in output_dir."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef split_sentences(self):\n        from pyrouge.utils.sentence_splitter import PunktSentenceSplitter\n        self.log.info(\"Splitting sentences.\")\n        ss = PunktSentenceSplitter()\n        sent_split_to_string = lambda s: \"\\n\".join(ss.split(s))\n        process_func = partial(\n            DirectoryProcessor.process, function=sent_split_to_string)\n        self.__process_summaries(process_func)", "response": "Splits the texts into sentences."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a text to a format ROUGE understands.", "response": "def convert_text_to_rouge_format(text, title=\"dummy title\"):\n        \"\"\"\n        Convert a text to a format ROUGE understands. The text is\n        assumed to contain one sentence per line.\n\n            text:   The text to convert, containg one sentence per line.\n            title:  Optional title for the text. The title will appear\n                    in the converted file, but doesn't seem to have\n                    any other relevance.\n\n        Returns: The converted text as string.\n\n        \"\"\"\n        sentences = text.split(\"\\n\")\n        sent_elems = [\n            \"<a name=\\\"{i}\\\">[{i}]</a> <a href=\\\"#{i}\\\" id={i}>\"\n            \"{text}</a>\".format(i=i, text=sent)\n            for i, sent in enumerate(sentences, start=1)]\n        html = \"\"\"<html>\n<head>\n<title>{title}</title>\n</head>\n<body bgcolor=\"white\">\n{elems}\n</body>\n</html>\"\"\".format(title=title, elems=\"\\n\".join(sent_elems))\n\n        return html"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the ROUGE configuration file for a given system summary file and model summary file.", "response": "def write_config_static(system_dir, system_filename_pattern,\n                            model_dir, model_filename_pattern,\n                            config_file_path, system_id=None):\n        \"\"\"\n        Write the ROUGE configuration file, which is basically a list\n        of system summary files and their corresponding model summary\n        files.\n\n        pyrouge uses regular expressions to automatically find the\n        matching model summary files for a given system summary file\n        (cf. docstrings for system_filename_pattern and\n        model_filename_pattern).\n\n            system_dir:                 Path of directory containing\n                                        system summaries.\n            system_filename_pattern:    Regex string for matching\n                                        system summary filenames.\n            model_dir:                  Path of directory containing\n                                        model summaries.\n            model_filename_pattern:     Regex string for matching model\n                                        summary filenames.\n            config_file_path:           Path of the configuration file.\n            system_id:                  Optional system ID string which\n                                        will appear in the ROUGE output.\n\n        \"\"\"\n        system_filenames = [f for f in os.listdir(system_dir)]\n        system_models_tuples = []\n\n        system_filename_pattern = re.compile(system_filename_pattern)\n        for system_filename in sorted(system_filenames):\n            match = system_filename_pattern.match(system_filename)\n            if match:\n                id = match.groups(0)[0]\n                model_filenames = Rouge155.__get_model_filenames_for_id(\n                    id, model_dir, model_filename_pattern)\n                system_models_tuples.append(\n                    (system_filename, sorted(model_filenames)))\n        if not system_models_tuples:\n            raise Exception(\n                \"Did not find any files matching the pattern {} \"\n                \"in the system summaries directory {}.\".format(\n                    system_filename_pattern.pattern, system_dir))\n\n        with codecs.open(config_file_path, 'w', encoding='utf-8') as f:\n            f.write('<ROUGE-EVAL version=\"1.55\">')\n            for task_id, (system_filename, model_filenames) in enumerate(\n                    system_models_tuples, start=1):\n\n                eval_string = Rouge155.__get_eval_string(\n                    task_id, system_id,\n                    system_dir, system_filename,\n                    model_dir, model_filenames)\n                f.write(eval_string)\n            f.write(\"</ROUGE-EVAL>\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the ROUGE configuration file.", "response": "def write_config(self, config_file_path=None, system_id=None):\n        \"\"\"\n        Write the ROUGE configuration file, which is basically a list\n        of system summary files and their matching model summary files.\n\n        This is a non-static version of write_config_file_static().\n\n            config_file_path:   Path of the configuration file.\n            system_id:          Optional system ID string which will\n                                appear in the ROUGE output.\n\n        \"\"\"\n        if not system_id:\n            system_id = 1\n        if (not config_file_path) or (not self._config_dir):\n            self._config_dir = mkdtemp()\n            config_filename = \"rouge_conf.xml\"\n        else:\n            config_dir, config_filename = os.path.split(config_file_path)\n            verify_dir(config_dir, \"configuration file\")\n        self._config_file = os.path.join(self._config_dir, config_filename)\n        Rouge155.write_config_static(\n            self._system_dir, self._system_filename_pattern,\n            self._model_dir, self._model_filename_pattern,\n            self._config_file, system_id)\n        self.log.info(\n            \"Written ROUGE configuration to {}\".format(self._config_file))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun ROUGE to evaluate the system summaries in system_dir against the model summaries in model_dir.", "response": "def evaluate(self, system_id=1, rouge_args=None):\n        \"\"\"\n        Run ROUGE to evaluate the system summaries in system_dir against\n        the model summaries in model_dir. The summaries are assumed to\n        be in the one-sentence-per-line HTML format ROUGE understands.\n\n            system_id:  Optional system ID which will be printed in\n                        ROUGE's output.\n\n        Returns: Rouge output as string.\n\n        \"\"\"\n        self.write_config(system_id=system_id)\n        options = self.__get_options(rouge_args)\n        command = [self._bin_path] + options\n        env = None\n        if hasattr(self, \"_home_dir\") and self._home_dir:\n            env = {'ROUGE_EVAL_HOME': self._home_dir}\n        self.log.info(\n            \"Running ROUGE with command {}\".format(\" \".join(command)))\n        rouge_output = check_output(command, env=env).decode(\"UTF-8\")\n        return rouge_output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_and_evaluate(self, system_id=1,\n                             split_sentences=False, rouge_args=None):\n        \"\"\"\n        Convert plain text summaries to ROUGE format and run ROUGE to\n        evaluate the system summaries in system_dir against the model\n        summaries in model_dir. Optionally split texts into sentences\n        in case they aren't already.\n\n        This is just a convenience method combining\n        convert_summaries_to_rouge_format() and evaluate().\n\n            split_sentences:    Optional argument specifying if\n                                sentences should be split.\n            system_id:          Optional system ID which will be printed\n                                in ROUGE's output.\n\n        Returns: ROUGE output as string.\n\n        \"\"\"\n        if split_sentences:\n            self.split_sentences()\n        self.__write_summaries()\n        rouge_output = self.evaluate(system_id, rouge_args)\n        return rouge_output", "response": "Convert plain text summaries to ROUGE format and run ROUGE evaluation against model_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef output_to_dict(self, output):\n        #0 ROUGE-1 Average_R: 0.02632 (95%-conf.int. 0.02632 - 0.02632)\n        pattern = re.compile(\n            r\"(\\d+) (ROUGE-\\S+) (Average_\\w): (\\d.\\d+) \"\n            r\"\\(95%-conf.int. (\\d.\\d+) - (\\d.\\d+)\\)\")\n        results = {}\n        for line in output.split(\"\\n\"):\n            match = pattern.match(line)\n            if match:\n                sys_id, rouge_type, measure, result, conf_begin, conf_end = \\\n                    match.groups()\n                measure = {\n                    'Average_R': 'recall',\n                    'Average_P': 'precision',\n                    'Average_F': 'f_score'\n                    }[measure]\n                rouge_type = rouge_type.lower().replace(\"-\", '_')\n                key = \"{}_{}\".format(rouge_type, measure)\n                results[key] = float(result)\n                results[\"{}_cb\".format(key)] = float(conf_begin)\n                results[\"{}_ce\".format(key)] = float(conf_end)\n        return results", "response": "Convert the ROUGE output into a python dictionary for further ROUGE processing."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets ROUGE - 1. 5. pl and data folder.", "response": "def __set_rouge_dir(self, home_dir=None):\n        \"\"\"\n        Verfify presence of ROUGE-1.5.5.pl and data folder, and set\n        those paths.\n\n        \"\"\"\n        if not home_dir:\n            self._home_dir = self.__get_rouge_home_dir_from_settings()\n        else:\n            self._home_dir = home_dir\n            self.save_home_dir()\n        self._bin_path = os.path.join(self._home_dir, 'ROUGE-1.5.5.pl')\n        self.data_dir = os.path.join(self._home_dir, 'data')\n        if not os.path.exists(self._bin_path):\n            raise Exception(\n                \"ROUGE binary not found at {}. Please set the \"\n                \"correct path by running pyrouge_set_rouge_path \"\n                \"/path/to/rouge/home.\".format(self._bin_path))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the string that can be used to evaluate the entire tree of text that is part of the ROUGE system.", "response": "def __get_eval_string(\n            task_id, system_id,\n            system_dir, system_filename,\n            model_dir, model_filenames):\n        \"\"\"\n        ROUGE can evaluate several system summaries for a given text\n        against several model summaries, i.e. there is an m-to-n\n        relation between system and model summaries. The system\n        summaries are listed in the <PEERS> tag and the model summaries\n        in the <MODELS> tag. pyrouge currently only supports one system\n        summary per text, i.e. it assumes a 1-to-n relation between\n        system and model summaries.\n\n        \"\"\"\n        peer_elems = \"<P ID=\\\"{id}\\\">{name}</P>\".format(\n            id=system_id, name=system_filename)\n\n        model_elems = [\"<M ID=\\\"{id}\\\">{name}</M>\".format(\n            id=chr(65 + i), name=name)\n            for i, name in enumerate(model_filenames)]\n\n        model_elems = \"\\n\\t\\t\\t\".join(model_elems)\n        eval_string = \"\"\"\n    <EVAL ID=\"{task_id}\">\n        <MODEL-ROOT>{model_root}</MODEL-ROOT>\n        <PEER-ROOT>{peer_root}</PEER-ROOT>\n        <INPUT-FORMAT TYPE=\"SEE\">\n        </INPUT-FORMAT>\n        <PEERS>\n            {peer_elems}\n        </PEERS>\n        <MODELS>\n            {model_elems}\n        </MODELS>\n    </EVAL>\n\"\"\".format(\n            task_id=task_id,\n            model_root=model_dir, model_elems=model_elems,\n            peer_root=system_dir, peer_elems=peer_elems)\n        return eval_string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __process_summaries(self, process_func):\n        temp_dir = mkdtemp()\n        new_system_dir = os.path.join(temp_dir, \"system\")\n        os.mkdir(new_system_dir)\n        new_model_dir = os.path.join(temp_dir, \"model\")\n        os.mkdir(new_model_dir)\n        self.log.info(\n            \"Processing summaries. Saving system files to {} and \"\n            \"model files to {}.\".format(new_system_dir, new_model_dir))\n        process_func(self._system_dir, new_system_dir)\n        process_func(self._model_dir, new_model_dir)\n        self._system_dir = new_system_dir\n        self._model_dir = new_model_dir", "response": "Helper method that applies process_func to the files in the system and model folders and saves the resulting files to the new system and model folders."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __get_options(self, rouge_args=None):\n        if self.args:\n            options = self.args.split()\n        elif rouge_args:\n            options = rouge_args.split()\n        else:\n            options = [\n                '-e', self._data_dir,\n                '-c', 95,\n                '-2',\n                '-1',\n                '-U',\n                '-r', 1000,\n                '-n', 4,\n                '-w', 1.2,\n                '-a',\n                ]\n            options = list(map(str, options))\n\n        options = self.__add_config_option(options)\n        return options", "response": "Get command line arguments for ROUGE or use default ones."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate getter and setter for a directory property.", "response": "def __create_dir_property(self, dir_name, docstring):\n        \"\"\"\n        Generate getter and setter for a directory property.\n\n        \"\"\"\n        property_name = \"{}_dir\".format(dir_name)\n        private_name = \"_\" + property_name\n        setattr(self, private_name, None)\n\n        def fget(self):\n            return getattr(self, private_name)\n\n        def fset(self, path):\n            verify_dir(path, dir_name)\n            setattr(self, private_name, path)\n\n        p = property(fget=fget, fset=fset, doc=docstring)\n        setattr(self.__class__, property_name, p)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __set_dir_properties(self):\n        directories = [\n            (\"home\", \"The ROUGE home directory.\"),\n            (\"data\", \"The path of the ROUGE 'data' directory.\"),\n            (\"system\", \"Path of the directory containing system summaries.\"),\n            (\"model\", \"Path of the directory containing model summaries.\"),\n            ]\n        for (dirname, docstring) in directories:\n            self.__create_dir_property(dirname, docstring)", "response": "Automatically generate the properties for directories."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __clean_rouge_args(self, rouge_args):\n        if not rouge_args:\n            return\n        quot_mark_pattern = re.compile('\"(.+)\"')\n        match = quot_mark_pattern.match(rouge_args)\n        if match:\n            cleaned_args = match.group(1)\n            return cleaned_args\n        else:\n            return rouge_args", "response": "Remove quotation marks from ROUGE_ARGS and return the cleaned version of ROUGE_ARGS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a username and password combination is valid.", "response": "def check_auth(self, username, password):\n        ''' This function is called to check if a username password combination\n            is valid. '''\n        return username == self.queryname and password == self.querypw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhandling an error by sending an error page.", "response": "def handle_exp_error(exception):\n    \"\"\"Handle errors by sending an error page.\"\"\"\n    app.logger.error(\n        \"%s (%s) %s\", exception.value, exception.errornum, str(dict(request.args)))\n    return exception.error_page(request, CONFIG.get('HIT Configuration',\n                                                    'contact_email_on_error'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a random condition count for a single resource.", "response": "def get_random_condcount(mode):\n    \"\"\"\n    HITs can be in one of three states:\n        - jobs that are finished\n        - jobs that are started but not finished\n        - jobs that are never going to finish (user decided not to do it)\n    Our count should be based on the first two, so we count any tasks finished\n    or any tasks not finished that were started in the last cutoff_time\n    minutes, as specified in the cutoff_time variable in the config file.\n\n    Returns a tuple: (cond, condition)\n    \"\"\"\n    cutofftime = datetime.timedelta(minutes=-CONFIG.getint('Server Parameters',\n                                                           'cutoff_time'))\n    starttime = datetime.datetime.now() + cutofftime\n\n    try: \n        conditions = json.load(open(os.path.join(app.root_path, 'conditions.json')))\n        numconds = len(conditions.keys())\n        numcounts = 1\n    except IOError as e:\n        numconds = CONFIG.getint('Task Parameters', 'num_conds')\n        numcounts = CONFIG.getint('Task Parameters', 'num_counters')\n\n    participants = Participant.query.\\\n        filter(Participant.codeversion == \\\n               CONFIG.get('Task Parameters', 'experiment_code_version')).\\\n        filter(Participant.mode == mode).\\\n        filter(or_(Participant.status == COMPLETED,\n                   Participant.status == CREDITED,\n                   Participant.status == SUBMITTED,\n                   Participant.status == BONUSED,\n                   Participant.beginhit > starttime)).all()\n    counts = Counter()\n    for cond in range(numconds):\n        for counter in range(numcounts):\n            counts[(cond, counter)] = 0\n    for participant in participants:\n        condcount = (participant.cond, participant.counterbalance)\n        if condcount in counts:\n            counts[condcount] += 1\n    mincount = min(counts.values())\n    minima = [hsh for hsh, count in counts.iteritems() if count == mincount]\n    chosen = choice(minima)\n    #conds += [ 0 for _ in range(1000) ]\n    #conds += [ 1 for _ in range(1000) ]\n    app.logger.info(\"given %(a)s chose %(b)s\" % {'a': counts, 'b': chosen})\n\n    return chosen"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_worker_status():\n    ''' Check worker status route '''\n    if 'workerId' not in request.args:\n        resp = {\"status\": \"bad request\"}\n        return jsonify(**resp)\n    else:\n        worker_id = request.args['workerId']\n        assignment_id = request.args['assignmentId']\n        allow_repeats = CONFIG.getboolean('HIT Configuration', 'allow_repeats')\n        if allow_repeats: # if you allow repeats focus on current worker/assignment combo\n            try:\n                part = Participant.query.\\\n                    filter(Participant.workerid == worker_id).\\\n                    filter(Participant.assignmentid == assignment_id).one()\n                status = part.status\n            except exc.SQLAlchemyError:\n                status = NOT_ACCEPTED\n        else: # if you disallow repeats search for highest status of anything by this worker\n            try:\n                matches = Participant.query.\\\n                    filter(Participant.workerid == worker_id).all()\n                numrecs = len(matches)\n                if numrecs==0: # this should be caught by exception, but just to be safe\n                    status = NOT_ACCEPTED\n                else:\n                    status = max([record.status for record in matches])\n            except exc.SQLAlchemyError:\n                status = NOT_ACCEPTED\n        resp = {\"status\" : status}\n        return jsonify(**resp)", "response": "Check worker status route"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef give_consent():\n    if not ('hitId' in request.args and 'assignmentId' in request.args and\n            'workerId' in request.args):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_consent')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    worker_id = request.args['workerId']\n    mode = request.args['mode']\n    with open('templates/consent.html', 'r') as temp_file:\n        consent_string = temp_file.read()\n    consent_string = insert_mode(consent_string, mode)\n    return render_template_string(\n        consent_string,\n        hitid=hit_id,\n        assignmentid=assignment_id,\n        workerid=worker_id\n    )", "response": "Returns the consent in the popup window."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the ad via HIT id", "response": "def get_ad_via_hitid(hit_id):\n    ''' Get ad via HIT id '''\n    username = CONFIG.get('psiTurk Access', 'psiturk_access_key_id')\n    password = CONFIG.get('psiTurk Access', 'psiturk_secret_access_id')\n    try:\n        req = requests.get('https://api.psiturk.org/api/ad/lookup/' + hit_id,\n                           auth=(username, password))\n    except:\n        raise ExperimentError('api_server_not_reachable')\n    else:\n        if req.status_code == 200:\n            return req.json()['ad_id']\n        else:\n            return \"error\""}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nserving up the experiment applet.", "response": "def start_exp():\n    \"\"\" Serves up the experiment applet. \"\"\"\n    if not (('hitId' in request.args) and ('assignmentId' in request.args) and\n            ('workerId' in request.args) and ('mode' in request.args)):\n        raise ExperimentError('hit_assign_worker_id_not_set_in_exp')\n    hit_id = request.args['hitId']\n    assignment_id = request.args['assignmentId']\n    worker_id = request.args['workerId']\n    mode = request.args['mode']\n    app.logger.info(\"Accessing /exp: %(h)s %(a)s %(w)s \" % {\n        \"h\" : hit_id,\n        \"a\": assignment_id,\n        \"w\": worker_id\n    })\n    if hit_id[:5] == \"debug\":\n        debug_mode = True\n    else:\n        debug_mode = False\n\n    # Check first to see if this hitId or assignmentId exists.  If so, check to\n    # see if inExp is set\n    allow_repeats = CONFIG.getboolean('HIT Configuration', 'allow_repeats')\n    if allow_repeats:\n        matches = Participant.query.\\\n            filter(Participant.workerid == worker_id).\\\n            filter(Participant.assignmentid == assignment_id).\\\n            all()\n    else:\n        matches = Participant.query.\\\n            filter(Participant.workerid == worker_id).\\\n            all()\n\n    numrecs = len(matches)\n    if numrecs == 0:\n        # Choose condition and counterbalance\n        subj_cond, subj_counter = get_random_condcount(mode)\n\n        worker_ip = \"UNKNOWN\" if not request.remote_addr else \\\n            request.remote_addr\n        browser = \"UNKNOWN\" if not request.user_agent.browser else \\\n            request.user_agent.browser\n        platform = \"UNKNOWN\" if not request.user_agent.platform else \\\n            request.user_agent.platform\n        language = \"UNKNOWN\" if not request.user_agent.language else \\\n            request.user_agent.language\n\n        # Set condition here and insert into database.\n        participant_attributes = dict(\n            assignmentid=assignment_id,\n            workerid=worker_id,\n            hitid=hit_id,\n            cond=subj_cond,\n            counterbalance=subj_counter,\n            ipaddress=worker_ip,\n            browser=browser,\n            platform=platform,\n            language=language,\n            mode=mode\n        )\n        part = Participant(**participant_attributes)\n        db_session.add(part)\n        db_session.commit()\n\n    else:\n        # A couple possible problems here:\n        # 1: They've already done an assignment, then we should tell them they\n        #    can't do another one\n        # 2: They've already worked on this assignment, and got too far to\n        #    start over.\n        # 3: They're in the database twice for the same assignment, that should\n        #    never happen.\n        # 4: They're returning and all is well.\n        nrecords = 0\n        for record in matches:\n            other_assignment = False\n            if record.assignmentid != assignment_id:\n                other_assignment = True\n            else:\n                nrecords += 1\n        if nrecords <= 1 and not other_assignment:\n            part = matches[0]\n            # In experiment (or later) can't restart at this point\n            if part.status >= STARTED and not debug_mode:\n                raise ExperimentError('already_started_exp')\n        else:\n            if nrecords > 1:\n                app.logger.error(\"Error, hit/assignment appears in database \\\n                                 more than once (serious problem)\")\n                raise ExperimentError(\n                    'hit_assign_appears_in_database_more_than_once'\n                )\n            if other_assignment:\n                raise ExperimentError('already_did_exp_hit')\n\n    use_psiturk_ad_server = CONFIG.getboolean('Shell Parameters', 'use_psiturk_ad_server')\n    if use_psiturk_ad_server and (mode == 'sandbox' or mode == 'live'):\n        # If everything goes ok here relatively safe to assume we can lookup\n        # the ad.\n        ad_id = get_ad_via_hitid(hit_id)\n        if ad_id != \"error\":\n            if mode == \"sandbox\":\n                ad_server_location = 'https://sandbox.ad.psiturk.org/complete/'\\\n                    + str(ad_id)\n            elif mode == \"live\":\n                ad_server_location = 'https://ad.psiturk.org/complete/' +\\\n                str(ad_id)\n        else:\n            raise ExperimentError('hit_not_registered_with_ad_server')\n    else:\n        ad_server_location = '/complete'\n\n    return render_template(\n        'exp.html', uniqueId=part.uniqueid,\n        condition=part.cond,\n        counterbalance=part.counterbalance,\n        adServerLoc=ad_server_location,\n        mode = mode,\n        contact_address=CONFIG.get('HIT Configuration', 'contact_email_on_error')\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enterexp():\n    app.logger.info(\"Accessing /inexp\")\n    if not 'uniqueId' in request.form:\n        raise ExperimentError('improper_inputs')\n    unique_id = request.form['uniqueId']\n\n    try:\n        user = Participant.query.\\\n            filter(Participant.uniqueid == unique_id).one()\n        user.status = STARTED\n        user.beginexp = datetime.datetime.now()\n        db_session.add(user)\n        db_session.commit()\n        resp = {\"status\": \"success\"}\n    except exc.SQLAlchemyError:\n        app.logger.error(\"DB error: Unique user not found.\")\n        resp = {\"status\": \"error, uniqueId not found\"}\n    return jsonify(**resp)", "response": "This function is called by the server when the user enters the applet and the user enters the real experiment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading experiment data, which should be a JSON object and will be stored after converting to string.", "response": "def load(uid=None):\n    \"\"\"\n    Load experiment data, which should be a JSON object and will be stored\n    after converting to string.\n    \"\"\"\n    app.logger.info(\"GET /sync route with id: %s\" % uid)\n\n    try:\n        user = Participant.query.\\\n            filter(Participant.uniqueid == uid).\\\n            one()\n    except exc.SQLAlchemyError:\n        app.logger.error(\"DB error: Unique user not found.\")\n\n    try:\n        resp = json.loads(user.datastring)\n    except:\n        resp = {\n            \"condition\": user.cond,\n            \"counterbalance\": user.counterbalance,\n            \"assignmentId\": user.assignmentid,\n            \"workerId\": user.workerid,\n            \"hitId\": user.hitid,\n            \"bonus\": user.bonus\n        }\n\n    return jsonify(**resp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(uid=None):\n    app.logger.info(\"PUT /sync route with id: %s\" % uid)\n\n    try:\n        user = Participant.query.\\\n            filter(Participant.uniqueid == uid).\\\n            one()\n    except exc.SQLAlchemyError:\n        app.logger.error(\"DB error: Unique user not found.\")\n\n    if hasattr(request, 'json'):\n        user.datastring = request.data.decode('utf-8').encode(\n            'ascii', 'xmlcharrefreplace'\n        )\n        db_session.add(user)\n        db_session.commit()\n\n    try:\n        data = json.loads(user.datastring)\n    except:\n        data = {}\n\n    trial = data.get(\"currenttrial\", None)\n    app.logger.info(\"saved data for %s (current trial: %s)\", uid, trial)\n    resp = {\"status\": \"user data saved\"}\n    return jsonify(**resp)", "response": "Update the user s data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef quitter():\n    unique_id = request.form['uniqueId']\n    if unique_id[:5] == \"debug\":\n        debug_mode = True\n    else:\n        debug_mode = False\n\n    if debug_mode:\n        resp = {\"status\": \"didn't mark as quitter since this is debugging\"}\n        return jsonify(**resp)\n    else:\n        try:\n            unique_id = request.form['uniqueId']\n            app.logger.info(\"Marking quitter %s\" % unique_id)\n            user = Participant.query.\\\n                filter(Participant.uniqueid == unique_id).\\\n                one()\n            user.status = QUITEARLY\n            db_session.add(user)\n            db_session.commit()\n        except exc.SQLAlchemyError:\n            raise ExperimentError('tried_to_quit')\n        else:\n            resp = {\"status\": \"marked as quitter\"}\n            return jsonify(**resp)", "response": "Mark the user as quitter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef debug_complete():\n    ''' Debugging route for complete. '''\n    if not 'uniqueId' in request.args:\n        raise ExperimentError('improper_inputs')\n    else:\n        unique_id = request.args['uniqueId']\n        mode = request.args['mode']\n        try:\n            user = Participant.query.\\\n                filter(Participant.uniqueid == unique_id).one()\n            user.status = COMPLETED\n            user.endhit = datetime.datetime.now()\n            db_session.add(user)\n            db_session.commit()\n        except:\n            raise ExperimentError('error_setting_worker_complete')\n        else:\n            if (mode == 'sandbox' or mode == 'live'): # send them back to mturk.\n                return render_template('closepopup.html')\n            else:\n                return render_template('complete.html')", "response": "Debugging route for complete."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef worker_submitted():\n    ''' Submit worker '''\n    if not 'uniqueId' in request.args:\n        resp = {\"status\": \"bad request\"}\n        return jsonify(**resp)\n    else:\n        unique_id = request.args['uniqueId']\n        app.logger.info(\"Submitted experiment for %s\" % unique_id)\n        try:\n            user = Participant.query.\\\n                filter(Participant.uniqueid == unique_id).one()\n            user.status = SUBMITTED\n            db_session.add(user)\n            db_session.commit()\n            status = \"success\"\n        except exc.SQLAlchemyError:\n            status = \"database error\"\n        resp = {\"status\" : status}\n        return jsonify(**resp)", "response": "Submit worker for submission of a new resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts mode into a node.", "response": "def insert_mode(page_html, mode):\n    ''' Insert mode '''\n    page_html = page_html.decode(\"utf-8\")\n    match_found = False\n    matches = re.finditer('workerId={{ workerid }}', page_html)\n    match = None\n    for match in matches:\n        match_found = True\n    if match_found:\n        new_html = page_html[:match.end()] + \"&mode=\" + mode +\\\n            page_html[match.end():]\n        return new_html\n    else:\n        raise ExperimentError(\"insert_mode_failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef regularpage(foldername=None, pagename=None):\n    if foldername is None and pagename is None:\n        raise ExperimentError('page_not_found')\n    if foldername is None and pagename is not None:\n        return render_template(pagename)\n    else:\n        return render_template(foldername+\"/\"+pagename)", "response": "Returns a regular page."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates random id numbers", "response": "def random_id_generator(self, size=6, chars=string.ascii_uppercase +\n                            string.digits):\n        ''' Generate random id numbers '''\n        return ''.join(random.choice(chars) for x in range(size))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd DB - logged worker bonus to worker list data", "response": "def add_bonus(worker_dict):\n        \" Adds DB-logged worker bonus to worker list data \"\n        try:\n            unique_id = '{}:{}'.format(worker_dict['workerId'], worker_dict['assignmentId'])\n            worker = Participant.query.filter(\n                Participant.uniqueid == unique_id).one()\n            worker_dict['bonus'] = worker.bonus\n        except sa.exc.InvalidRequestError:\n            # assignment is found on mturk but not in local database.\n            worker_dict['bonus'] = 'N/A'\n        return worker_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the workers from AMT services.", "response": "def get_workers(self, status=None, chosen_hits=None, assignment_ids=None, all_studies=False):\n        '''\n        Status, if set, can be one of `Submitted`, `Approved`, or `Rejected`\n        '''\n        if assignment_ids:\n            workers = [self.get_worker(assignment_id) for assignment_id in assignment_ids]\n        else:\n            workers = self.amt_services.get_workers(assignment_status=status, chosen_hits=chosen_hits)\n\n        if workers is False:\n            raise Exception('*** failed to get workers')\n\n        if not all_studies:\n            my_hitids = self._get_my_hitids()\n            workers = [worker for worker in workers if worker['hitId'] in my_hitids]\n\n        workers = [self.add_bonus(worker) for worker in workers]\n        return workers"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napproves worker for the given assignment.", "response": "def approve_worker(self, worker, force=False):\n        ''' Approve worker '''\n        assignment_id = worker['assignmentId']\n        init_db()\n        found_worker = False\n        parts = Participant.query.\\\n               filter(Participant.assignmentid == assignment_id).\\\n               filter(Participant.status.in_([3, 4])).\\\n               all()\n        # Iterate through all the people who completed this assignment.\n        # This should be one person, and it should match the person who\n        # submitted the HIT, but that doesn't always hold.\n        status_report = ''\n        for part in parts:\n            if part.workerid == worker['workerId']:\n                found_worker = True\n                success = self.amt_services.approve_worker(assignment_id)\n                if success:\n                    part.status = 5\n                    db_session.add(part)\n                    db_session.commit()\n                    status_report = 'approved worker {} for assignment {}'.format(part.workerid, assignment_id)\n                else:\n                    error_msg = '*** failed to approve worker {} for assignment {}'.format(part.workerid, assignment_id)\n                    raise Exception(error_msg)\n            else:\n                status_report = 'found unexpected worker {} for assignment {}'.format(part.workerid, assignment_id)\n        if not found_worker:\n            # approve workers not found in DB if the assignment id has been specified\n            if force:\n                success = self.amt_services.approve_worker(assignment_id)\n                if success:\n                    _status_report = 'approved worker {} for assignment {} but not found in DB'.format(worker['workerId'], assignment_id)\n                    status_report = '\\n'.join([status_report,_status_report])\n                else:\n                    error_msg = '*** failed to approve worker {} for assignment {}'.format(worker['workerId'], assignment_id)\n                    raise Exception(error_msg)\n            # otherwise don't approve, and print warning\n            else:\n                _status_report = 'worker {} not found in DB for assignment {}. Not automatically approved. Use --force to approve anyway.'.format(worker['workerId'], assignment_id)\n                if status_report:\n                    status_report = '\\n'.join([status_report,_status_report])\n                else:\n                    status_report = _status_report\n        return status_report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef worker_reject(self, chosen_hit, assignment_ids = None):\n        ''' Reject worker '''\n        if chosen_hit:\n            workers = self.amt_services.get_workers(\"Submitted\")\n            assignment_ids = [worker['assignmentId'] for worker in workers if \\\n                              worker['hitId'] == chosen_hit]\n            print 'rejecting workers for HIT', chosen_hit\n        for assignment_id in assignment_ids:\n            success = self.amt_services.reject_worker(assignment_id)\n            if success:\n                print 'rejected', assignment_id\n            else:\n                print '*** failed to reject', assignment_id", "response": "Reject worker for HIT"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef worker_unreject(self, chosen_hit, assignment_ids = None):\n        ''' Unreject worker '''\n        if chosen_hit:\n            workers = self.amt_services.get_workers(\"Rejected\")\n            assignment_ids = [worker['assignmentId'] for worker in workers if \\\n                              worker['hitId'] == chosen_hit]\n        for assignment_id in assignment_ids:\n            success = self.amt_services.unreject_worker(assignment_id)\n            if success:\n                print 'unrejected %s' % (assignment_id)\n            else:\n                print '*** failed to unreject', assignment_id", "response": "Unreject worker with given assignment_ids"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd additional worker assignments or minutes to a HIT.", "response": "def hit_extend(self, hit_id, assignments, minutes):\n        \"\"\" Add additional worker assignments or minutes to a HIT.\n\n        Args:\n            hit_id: A list conaining one hit_id string.\n            assignments: Variable <int> for number of assignments to add.\n            minutes: Variable <int> for number of minutes to add.\n\n        Returns:\n            A side effect of this function is that the state of a HIT changes\n            on AMT servers.\n\n        Raises:\n\n        \"\"\"\n\n        assert type(hit_id) is list\n        assert type(hit_id[0]) is str\n\n        if self.amt_services.extend_hit(hit_id[0], assignments, minutes):\n            print \"HIT extended.\""}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef db_aws_list_regions(self):\n        ''' List AWS DB regions '''\n        regions = self.db_services.list_regions()\n        if regions != []:\n            print \"Avaliable AWS regions:\"\n        for reg in regions:\n            print '\\t' + reg,\n            if reg == self.db_services.get_region():\n                print \"(currently selected)\"\n            else:\n                print ''", "response": "List AWS DB regions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef db_aws_list_instances(self):\n        ''' List AWS DB instances '''\n        instances = self.db_services.get_db_instances()\n        if not instances:\n            print(\"There are no DB instances associated with your AWS account \" \\\n                \"in region \" + self.db_services.get_region())\n        else:\n            print(\"Here are the current DB instances associated with your AWS \" \\\n                \"account in region \" + self.db_services.get_region())\n            for dbinst in instances:\n                print '\\t'+'-'*20\n                print \"\\tInstance ID: \" + dbinst.id\n                print \"\\tStatus: \" + dbinst.status", "response": "List AWS DB instances"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting AWS DB instance", "response": "def db_aws_delete_instance(self, instance_id):\n        ''' Delete AWS DB instance '''\n        interactive = False\n        if instance_id is None:\n            interactive = True\n\n        instances = self.db_services.get_db_instances()\n        instance_list = [dbinst.id for dbinst in instances]\n\n        if interactive:\n            valid = False\n            if len(instances) == 0:\n                print(\"There are no instances you can delete currently.  Use \"\n                      \"`db aws_create_instance` to make one.\")\n                return\n            print \"Here are the available instances you can delete:\"\n            for inst in instances:\n                print \"\\t \", inst.id, \"(\", inst.status, \")\"\n            while not valid:\n                instance_id = raw_input('Enter the instance identity you would '\n                                        'like to delete: ')\n                res = self.db_services.validate_instance_id(instance_id)\n                if res is True:\n                    valid = True\n                else:\n                    print(res + \" Try again, instance name not valid.  Check \" \\\n                        \"for typos.\")\n                if instance_id in instance_list:\n                    valid = True\n                else:\n                    valid = False\n                    print(\"Try again, instance not present in this account.  \"\n                          \"Try again checking for typos.\")\n        else:\n            res = self.db_services.validate_instance_id(instance_id)\n            if res is not True:\n                print(\"*** Error, instance name either not valid.  Try again \"\n                     \"checking for typos.\")\n                return\n            if instance_id not in instance_list:\n                print(\"*** Error, This instance not present in this account.  \"\n                     \"Try again checking for typos.  Run `db aws_list_instances` to \"\n                     \"see valid list.\")\n                return\n\n        user_input = raw_input(\n            \"Deleting an instance will erase all your data associated with the \"\n            \"database in that instance.  Really quit? y or n:\"\n        )\n        if user_input == 'y':\n            res = self.db_services.delete_db_instance(instance_id)\n            if res:\n                print(\"AWS RDS database instance %s deleted.  Run `db \" \\\n                    \"aws_list_instances` for current status.\" % instance_id)\n            else:\n                print(\"*** Error deleting database instance %s.  \" \\\n                    \"It maybe because it is still being created, deleted, or is \" \\\n                    \"being backed up.  Run `db aws_list_instances` for current \" \\\n                    \"status.\" % instance_id)\n        else:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef db_use_aws_instance(self, instance_id, arg):\n        ''' set your database info to use the current instance configure a\n        security zone for this based on your ip '''\n        interactive = False\n        if instance_id is None:\n            interactive = True\n\n        instances = self.db_services.get_db_instances()\n        instance_list = [dbinst.id for dbinst in instances]\n\n        if len(instances) == 0:\n            print(\"There are no instances in this region/account.  Use `db \"\n                \"aws_create_instance` to make one first.\")\n            return\n\n        # show list of available instances, if there are none cancel immediately\n        if interactive:\n            valid = False\n            print(\"Here are the available instances you have.  You can only \"\n                \"use those listed as 'available':\")\n            for inst in instances:\n                print \"\\t \", inst.id, \"(\", inst.status, \")\"\n            while not valid:\n                instance_id = raw_input('Enter the instance identity you would '\n                                        'like to use: ')\n                res = self.db_services.validate_instance_id(instance_id)\n                if res is True:\n                    valid = True\n                else:\n                    print(res + \" Try again, instance name not valid.  Check \"\n                          \"for typos.\")\n                if instance_id in instance_list:\n                    valid = True\n                else:\n                    valid = False\n                    print(\"Try again, instance not present in this account. \"\n                          \"Try again checking for typos.\")\n        else:\n            res = self.db_services.validate_instance_id(instance_id)\n            if res != True:\n                print(\"*** Error, instance name either not valid.  Try again \"\n                      \"checking for typos.\")\n                return\n            if instance_id not in instance_list:\n                print(\"*** Error, This instance not present in this account. \"\n                      \"Try again checking for typos.  Run `db aws_list_instances` to \"\n                      \"see valid list.\")\n                return\n\n        user_input = raw_input(\n            \"Switching your DB settings to use this instance. Are you sure you \"\n            \"want to do this? \"\n        )\n        if user_input == 'y':\n            # ask for password\n            valid = False\n            while not valid:\n                password = raw_input('enter the master password for this '\n                                     'instance: ')\n                res = self.db_services.validate_instance_password(password)\n                if res != True:\n                    print(\"*** Error: password seems incorrect, doesn't \"\n                          \"conform to AWS rules.  Try again\")\n                else:\n                    valid = True\n\n            # Get instance\n            myinstance = self.db_services.get_db_instance_info(instance_id)\n            if myinstance:\n                # Add security zone to this node to allow connections\n                my_ip = get_my_ip()\n                if (not self.db_services.allow_access_to_instance(myinstance,\n                                                                  my_ip)):\n                    print(\"*** Error authorizing your ip address to connect to \" \\\n                          \"server (%s).\" % my_ip)\n                    return\n                print \"AWS RDS database instance %s selected.\" % instance_id\n\n                # Using regular SQL commands list available database on this\n                # node\n                try:\n                    db_url = 'mysql+pymysql://' + myinstance.master_username + \":\" \\\n                        + password + \"@\" + myinstance.endpoint[0] + \":\" + \\\n                        str(myinstance.endpoint[1])\n                    engine = sa.create_engine(db_url, echo=False)\n                    eng = engine.connect().execute\n                    db_names = eng(\"show databases\").fetchall()\n                except:\n                    print(\"***  Error connecting to instance.  Your password \"\n                          \"might be incorrect.\")\n                    return\n                existing_dbs = [db[0] for db in db_names if db not in \\\n                                [('information_schema',), ('innodb',), \\\n                                 ('mysql',), ('performance_schema',)]]\n                create_db = False\n                if len(existing_dbs) == 0:\n                    valid = False\n                    while not valid:\n                        db_name = raw_input(\"No existing DBs in this instance. \"\n                                            \"Enter a new name to create one: \")\n                        res = self.db_services.validate_instance_dbname(db_name)\n                        if res is True:\n                            valid = True\n                        else:\n                            print res + \" Try again.\"\n                    create_db = True\n                else:\n                    print \"Here are the available database tables\"\n                    for database in existing_dbs:\n                        print \"\\t\" + database\n                    valid = False\n                    while not valid:\n                        db_name = raw_input(\n                            \"Enter the name of the database you want to use or \"\n                            \"a new name to create  a new one: \"\n                        )\n                        res = self.db_services.validate_instance_dbname(db_name)\n                        if res is True:\n                            valid = True\n                        else:\n                            print res + \" Try again.\"\n                    if db_name not in existing_dbs:\n                        create_db = True\n                if create_db:\n                    try:\n                        connection.execute(\"CREATE DATABASE %s;\" % db_name)\n                    except:\n                        print(\"*** Error creating database %s on instance \" \\\n                              \"%s\" % (db_name, instance_id))\n                        return\n                base_url = 'mysql+pymysql://' + myinstance.master_username + \":\" + \\\n                    password + \"@\" + myinstance.endpoint[0] + \":\" + \\\n                    str(myinstance.endpoint[1]) + \"/\" + db_name\n                self.config.set(\"Database Parameters\", \"database_url\", base_url)\n                print(\"Successfully set your current database (database_url) \" \\\n                      \"to \\n\\t%s\" % base_url)\n                if (self.server.is_server_running() == 'maybe' or\n                        self.server.is_server_running() == 'yes'):\n                    self.do_restart_server('')\n            else:\n                print '\\n'.join([\n                    \"*** Error selecting database instance %s.\" % arg['<id>'],\n                    \"Run `db list_db_instances` for current status of instances, only `available`\",\n                    \"instances can be used.  Also, your password may be incorrect.\"\n                ])\n        else:\n            return", "response": "set your database info to use the current instance configure a\n        security zone for this based on your ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new db instance on AWS.", "response": "def db_create_aws_db_instance(self, instid=None, size=None, username=None,\n                                  password=None, dbname=None):\n        ''' Create db instance on AWS '''\n        interactive = False\n        if instid is None:\n            interactive = True\n\n        if interactive:\n            print '\\n'.join(['*************************************************',\n                             'Ok, here are the rules on creating instances:',\n                             '',\n                             'instance id:',\n                             '  Each instance needs an identifier.  This is the name',\n                             '  of the virtual machine created for you on AWS.',\n                             '  Rules are 1-63 alphanumeric characters, first must',\n                             '  be a letter, must be unique to this AWS account.',\n                             '',\n                             'size:',\n                             '  The maximum size of you database in GB.  Enter an',\n                             '  integer between 5-1024',\n                             '',\n                             'master username:',\n                             '  The username you will use to connect.  Rules are',\n                             '  1-16 alphanumeric characters, first must be a letter,',\n                             '  cannot be a reserved MySQL word/phrase',\n                             '',\n                             'master password:',\n                             '  Rules are 8-41 alphanumeric characters',\n                             '',\n                             'database name:',\n                             '  The name for the first database on this instance.  Rules are',\n                             '  1-64 alphanumeric characters, cannot be a reserved MySQL word',\n                             '*************************************************',\n                             ''])\n\n        if interactive:\n            valid = False\n            while not valid:\n                instid = raw_input('enter an identifier for the instance (see '\n                                   'rules above): ')\n                res = self.db_services.validate_instance_id(instid)\n                if res is True:\n                    valid = True\n                else:\n                    print res + \" Try again.\"\n        else:\n            res = self.db_services.validate_instance_id(instid)\n            if res is not True:\n                print res\n                return\n\n        if interactive:\n            valid = False\n            while not valid:\n                size = raw_input('size of db in GB (5-1024): ')\n                res = self.db_services.validate_instance_size(size)\n                if res is True:\n                    valid = True\n                else:\n                    print res + \" Try again.\"\n        else:\n            res = self.db_services.validate_instance_size(size)\n            if res is not True:\n                print res\n                return\n\n        if interactive:\n            valid = False\n            while not valid:\n                username = raw_input('master username (see rules above): ')\n                res = self.db_services.validate_instance_username(username)\n                if res is True:\n                    valid = True\n                else:\n                    print res + \" Try again.\"\n        else:\n            res = self.db_services.validate_instance_username(username)\n            if res is not True:\n                print res\n                return\n\n        if interactive:\n            valid = False\n            while not valid:\n                password = raw_input('master password (see rules above): ')\n                res = self.db_services.validate_instance_password(password)\n                if res is True:\n                    valid = True\n                else:\n                    print res + \" Try again.\"\n        else:\n            res = self.db_services.validate_instance_password(password)\n            if res is not True:\n                print res\n                return\n\n        if interactive:\n            valid = False\n            while not valid:\n                dbname = raw_input('name for first database on this instance \\\n                                   (see rules): ')\n                res = self.db_services.validate_instance_dbname(dbname)\n                if res is True:\n                    valid = True\n                else:\n                    print res + \" Try again.\"\n        else:\n            res = self.db_services.validate_instance_dbname(dbname)\n            if res is not True:\n                print res\n                return\n\n        options = {\n            'id': instid,\n            'size': size,\n            'username': username,\n            'password': password,\n            'dbname': dbname\n        }\n        instance = self.db_services.create_db_instance(options)\n        if not instance:\n            print '\\n'.join(['*****************************',\n                             '  Sorry, there was an error creating db instance.'])\n        else:\n            print '\\n'.join(['*****************************',\n                             '  Creating AWS RDS MySQL Instance',\n                             '    id: ' + str(options['id']),\n                             '    size: ' + str(options['size']) + \" GB\",\n                             '    username: ' + str(options['username']),\n                             '    password: ' + str(options['password']),\n                             '    dbname: ' +  str(options['dbname']),\n                             '    type: MySQL/db.t1.micro',\n                             '    ________________________',\n                             ' Be sure to store this information in a safe place.',\n                             ' Please wait 5-10 minutes while your database is created in the cloud.',\n                             ' You can run \\'db aws_list_instances\\' to verify it was created (status',\n                             ' will say \\'available\\' when it is ready'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting until the server is online.", "response": "def wait_until_online(function, ip, port):\n    \"\"\"\n    Uses Wait_For_State to wait for the server to come online, then runs the given function.\n    \"\"\"\n    awaiting_service = Wait_For_State(lambda: not is_port_available(ip, port), function)\n    awaiting_service.start()\n    return awaiting_service"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process():\n    ''' Figure out how we were invoked '''\n    invoked_as = os.path.basename(sys.argv[0])\n\n    if invoked_as == \"psiturk\":\n        launch_shell()\n    elif invoked_as == \"psiturk-server\":\n        launch_server()\n    elif invoked_as == \"psiturk-shell\":\n        launch_shell()\n    elif invoked_as == \"psiturk-setup-example\":\n        setup_example()\n    elif invoked_as == \"psiturk-install\":\n        install_from_exchange()", "response": "Figure out how we were invoked"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninstalls from experiment exchange.", "response": "def install_from_exchange():\n    ''' Install from experiment exchange. '''\n    parser = argparse.ArgumentParser(\n        description='Download experiment from the psiturk.org experiment\\\n        exchange (http://psiturk.org/ee).'\n    )\n    parser.add_argument(\n        'exp_id', metavar='exp_id', type=str, help='the id number of the\\\n        experiment in the exchange'\n    )\n    args = parser.parse_args()\n    exp_exch = ExperimentExchangeServices()\n    exp_exch.download_experiment(args.exp_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds commands for testing etc.", "response": "def setup_example():\n    ''' Add commands for testing, etc. '''\n    parser = argparse.ArgumentParser(\n        description='Creates a simple default project (stroop) in the current\\\n        directory with the necessary psiTurk files.'\n    )\n\n    # Optional flags\n    parser.add_argument(\n        '-v', '--version', help='Print version number.', action=\"store_true\"\n    )\n    args = parser.parse_args()\n\n    # If requested version just print and quite\n    if args.version:\n        print version_number\n    else:\n        import psiturk.setup_example as se\n        se.setup_example()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef launch_server():\n    ''' Add commands for testing, etc.. '''\n    parser = argparse.ArgumentParser(\n        description='Launch psiTurk experiment webserver process on the\\\n        host/port defined in config.txt.'\n    )\n\n    # Optional flags\n    parser.add_argument(\n        '-v', '--version', help='Print version number.', action=\"store_true\"\n    )\n    args = parser.parse_args()\n\n    # If requested version just print and quite\n    if args.version:\n        print version_number\n    else:\n        import psiturk.experiment_server as es\n        es.launch()", "response": "Add commands for testing etc.."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef launch_shell():\n    ''' Add commands for testing, etc.. '''\n    parser = argparse.ArgumentParser(\n        description='Launch the psiTurk interactive shell.'\n    )\n\n    # Optional flags\n    parser.add_argument(\n        '-v', '--version', help='Print version number.', action=\"store_true\"\n    )\n    parser.add_argument(\n        '-c', '--cabinmode', help='Launch psiturk in cabin (offline) mode',\n        action=\"store_true\"\n    )\n    script_group = parser.add_mutually_exclusive_group()\n    script_group.add_argument(\n        '-s', '--script', help='Run commands from a script file'\n    )\n    script_group.add_argument(\n        '-e', '--execute', help='Execute one command specified on the command line'\n    )\n    args, unknownargs = parser.parse_known_args()\n    # If requested version just print and quit\n    if args.version:\n        print version_number\n    else:\n        import psiturk.psiturk_shell as ps\n        if args.script:\n            ps.run(cabinmode=args.cabinmode, script=args.script, quiet=True)\n        elif args.execute or unknownargs:\n            if unknownargs:\n                execute= ' '.join(unknownargs)\n            else:\n                execute = args.execute\n            ps.run(cabinmode=args.cabinmode, execute=execute, quiet=True)\n        else:\n            ps.run(cabinmode=args.cabinmode)", "response": "Launch the psiTurk interactive shell."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the IP that the user is trying to connect to.", "response": "def get_my_ip():\r\n    \"\"\"\r\n        Asks and external server what your ip appears to be (useful is\r\n        running from behind a NAT/wifi router).  Of course, incoming port\r\n        to the router must be forwarded correctly.\r\n    \"\"\"\r\n    if 'OPENSHIFT_SECRET_TOKEN' in os.environ:\r\n        my_ip = os.environ['OPENSHIFT_APP_DNS']\r\n    else:\r\n        my_ip = json.load(urllib2.urlopen(\r\n            'http://httpbin.org/ip'\r\n        ))['origin'].split(',')[0]\r\n    return my_ip"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef colorize(target, color, use_escape=True):\r\n    ''' Colorize target string. Set use_escape to false when text will not be\r\n    interpreted by readline, such as in intro message.'''\r\n    def escape(code):\r\n        ''' Escape character '''\r\n        return '\\001%s\\002' % code\r\n    if color == 'purple':\r\n        color_code = '\\033[95m'\r\n    elif color == 'cyan':\r\n        color_code = '\\033[96m'\r\n    elif color == 'darkcyan':\r\n        color_code = '\\033[36m'\r\n    elif color == 'blue':\r\n        color_code = '\\033[93m'\r\n    elif color == 'green':\r\n        color_code = '\\033[92m'\r\n    elif color == 'yellow':\r\n        color_code = '\\033[93m'\r\n    elif color == 'red':\r\n        color_code = '\\033[91m'\r\n    elif color == 'white':\r\n        color_code = '\\033[37m'\r\n    elif color == 'bold':\r\n        color_code = '\\033[1m'\r\n    elif color == 'underline':\r\n        color_code = '\\033[4m'\r\n    else:\r\n        color_code = ''\r\n    if use_escape:\r\n        return escape(color_code) + target + escape('\\033[0m')\r\n    else:\r\n        return color_code + target + '\\033[m'", "response": "Colorize target string. Set use_escape to false when text will not be displayed by readline."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef docopt_cmd(func):\n    def helper_fn(self, arg):\n        '''helper function for docopt'''\n        try:\n            opt = docopt(helper_fn.__doc__, arg)\n        except DocoptExit as exception:\n            # The DocoptExit is thrown when the args do not match.\n            # We print a message to the user and the usage block.\n            print 'Invalid Command!'\n            print exception\n            return\n        except SystemExit:\n            # The SystemExit exception prints the usage for --help\n            # We do not need to do the print here.\n            return\n        return func(self, opt)\n    helper_fn.__name__ = func.__name__\n    helper_fn.__doc__ = func.__doc__\n    helper_fn.__dict__.update(func.__dict__)\n    return helper_fn", "response": "Decorator for docopt parsing"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef default(self, cmd):\n        ''' Collect incorrect and mistyped commands '''\n        choices = [\"help\", \"mode\", \"psiturk_status\", \"server\", \"shortcuts\",\n                   \"worker\", \"db\", \"edit\", \"open\", \"config\", \"show\",\n                   \"debug\", \"setup_example\", \"status\", \"tunnel\", \"amt_balance\",\n                   \"download_datafiles\", \"exit\", \"hit\", \"load\", \"quit\", \"save\",\n                   \"shell\", \"version\"]\n        print \"%s is not a psiTurk command. See 'help'.\" %(cmd)\n        print \"Did you mean this?\\n      %s\" %(process.extractOne(cmd,\n                                                                  choices)[0])", "response": "Collect incorrect and mistyped commands"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks offline configuration file", "response": "def check_offline_configuration(self):\n        ''' Check offline configuration file'''\n        quit_on_start = False\n        database_url = self.config.get('Database Parameters', 'database_url')\n        host = self.config.get('Server Parameters', 'host', 'localhost')\n        if database_url[:6] != 'sqlite':\n            print(\"*** Error: config.txt option 'database_url' set to use \"\n                \"mysql://.  Please change this sqllite:// while in cabin mode.\")\n            quit_on_start = True\n        if host != 'localhost':\n            print(\"*** Error: config option 'host' is not set to localhost. \"\n                \"Please change this to localhost while in cabin mode.\")\n            quit_on_start = True\n        if quit_on_start:\n            exit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints cabin mode message", "response": "def get_intro_prompt(self):\n        ''' Print cabin mode message '''\n        sys_status = open(self.help_path + 'cabin.txt', 'r')\n        server_msg = sys_status.read()\n        return server_msg + colorize('psiTurk version ' + version_number +\n                                     '\\nType \"help\" for more information.',\n                                     'green', False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct psiTurk shell prompt", "response": "def color_prompt(self):\n        ''' Construct psiTurk shell prompt '''\n        prompt = '[' + colorize('psiTurk', 'bold')\n        server_string = ''\n        server_status = self.server.is_server_running()\n        if server_status == 'yes':\n            server_string = colorize('on', 'green')\n        elif server_status == 'no':\n            server_string = colorize('off', 'red')\n        elif server_status == 'maybe':\n            server_string = colorize('unknown', 'yellow')\n        elif server_status == 'blocked':\n            server_string = colorize('blocked', 'red')\n        prompt += ' server:' + server_string\n        prompt += ' mode:' + colorize('cabin', 'bold')\n        prompt += ']$ '\n        self.prompt = prompt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nkeep persistent command history.", "response": "def preloop(self):\n        ''' Keep persistent command history. '''\n        if not self.already_prelooped:\n            self.already_prelooped = True\n            open('.psiturk_history', 'a').close()  # create file if it doesn't exist\n            readline.read_history_file('.psiturk_history')\n            for i in range(readline.get_current_history_length()):\n                if readline.get_history_item(i) is not None:\n                    self.history.append(readline.get_history_item(i))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntriggering hooks after command.", "response": "def onecmd_plus_hooks(self, line):\n        ''' Trigger hooks after command. '''\n        if not line:\n            return self.emptyline()\n        return Cmd.onecmd_plus_hooks(self, line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _estimate_expenses(self, num_workers, reward):\n        ''' Returns tuple describing expenses:\n        amount paid to workers\n        amount paid to amazon'''\n\n        # fee structure changed 07.22.15:\n        # 20% for HITS with < 10 assignments\n        # 40% for HITS with >= 10 assignments\n        commission = 0.2\n        if float(num_workers) >= 10:\n            commission = 0.4\n        work = float(num_workers) * float(reward)\n        fee = work * commission\n        return (work, fee, work+fee)", "response": "Estimate the amount of expenses for a given worker and reward."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprompting for a yes or no to given prompt.", "response": "def _confirm_dialog(self, prompt):\n        ''' Prompts for a 'yes' or 'no' to given prompt. '''\n        response = raw_input(prompt).strip().lower()\n        valid = {'y': True, 'ye': True, 'yes': True, 'n': False, 'no': False}\n        while True:\n            try:\n                return valid[response]\n            except:\n                response = raw_input(\"Please respond 'y' or 'n': \").strip().lower()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_config(self, arg):\n        if arg['print']:\n            self.print_config(arg)\n        elif arg['reload']:\n            self.reload_config(arg)\n        else:\n            self.help_server()", "response": "Print and reload the configuration of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complete_config(self, text, line, begidx, endidx):\n        ''' Tab-complete config command '''\n        return  [i for i in PsiturkShell.config_commands if i.startswith(text)]", "response": "Tab - complete config command"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_status(self, _):\n        ''' Notify user of server status. '''\n        server_status = self.server.is_server_running()\n        if server_status == 'yes':\n            print 'Server: ' + colorize('currently online', 'green')\n        elif server_status == 'no':\n            print 'Server: ' + colorize('currently offline', 'red')\n        elif server_status == 'maybe':\n            print 'Server: ' + colorize('status unknown', 'yellow')\n        elif server_status == 'blocked':\n            print 'Server: ' + colorize('blocked', 'red')", "response": "Notify user of server status."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing local file for DB.", "response": "def db_use_local_file(self, arg, filename=None):\n        ''' Use local file for DB. '''\n        # interactive = False  # Never used\n        if filename is None:\n            # interactive = True  # Never used\n            filename = raw_input('Enter the filename of the local SQLLite '\n                                 'database you would like to use '\n                                 '[default=participants.db]: ')\n            if filename == '':\n                filename = 'participants.db'\n        base_url = \"sqlite:///\" + filename\n        self.config.set(\"Database Parameters\", \"database_url\", base_url)\n        print \"Updated database setting (database_url): \\n\\t\", \\\n        self.config.get(\"Database Parameters\", \"database_url\")\n        if self.server.is_server_running() == 'yes':\n            self.server_restart()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_server(self, arg):\n        if arg['on']:\n            self.server_on()\n        elif arg['off']:\n            self.server_off()\n        elif arg['restart']:\n            self.server_restart()\n        elif arg['log']:\n            self.server_log()\n        else:\n            self.help_server()", "response": "Usage:\n          server on\n          server off\n          server restart\n          server log\n          server help"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef complete_server(self, text, line, begidx, endidx):\n        ''' Tab-complete server command '''\n        return  [i for i in PsiturkShell.server_commands if i.startswith(text)]", "response": "Tab - complete server command"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_quit(self, _):\n        '''Override do_quit for network clean up.'''\n        if (self.server.is_server_running() == 'yes' or\n                self.server.is_server_running() == 'maybe'):\n            user_input = raw_input(\"Quitting shell will shut down experiment \"\n                                    \"server. Really quit? y or n: \")\n            if user_input == 'y':\n                self.server_off()\n            else:\n                return False\n        return True", "response": "Override do_quit for network clean up."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncleaning up child and orphaned processes.", "response": "def clean_up(self):\n        ''' Clean up child and orphaned processes. '''\n        if self.tunnel.is_open:\n            print 'Closing tunnel...'\n            self.tunnel.close()\n            print 'Done.'\n        else:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverload intro prompt with network - aware version if you can reach psiTurk. org request system status message", "response": "def get_intro_prompt(self):\n        ''' Overloads intro prompt with network-aware version if you can reach\n        psiTurk.org, request system status message'''\n        server_msg = self.web_services.get_system_status()\n        return server_msg + colorize('psiTurk version ' + version_number +\n                                     '\\nType \"help\" for more information.',\n                                     'green', False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_hit_tally(self):\n        ''' Tally hits '''\n        if not self.quiet:\n            num_hits = self.amt_services_wrapper.tally_hits()\n            if self.sandbox:\n                self.sandbox_hits = num_hits\n            else:\n                self.live_hits = num_hits", "response": "Update the number of hits in the resource table with the number of live or sandbox hits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_db(self, arg):\n        if arg['get_config']:\n            self.db_get_config()\n        elif arg['use_local_file']:\n            self.db_use_local_file(arg, filename=arg['<filename>'])\n        elif arg['use_aws_instance']:\n            self.db_use_aws_instance(arg['<instance_id>'], arg)\n        elif arg['aws_list_regions']:\n            self.db_aws_list_regions()\n        elif arg['aws_get_region']:\n            self.db_aws_get_region()\n        elif arg['aws_set_region']:\n            self.db_aws_set_region(arg['<region_name>'])\n        elif arg['aws_list_instances']:\n            self.db_aws_list_instances()\n        elif arg['aws_create_instance']:\n            self.db_create_aws_db_instance(arg['<instance_id>'], arg['<size>'],\n                                           arg['<username>'],\n                                           arg['<password>'], arg['<dbname>'])\n        elif arg['aws_delete_instance']:\n            self.db_aws_delete_instance(arg['<instance_id>'])\n        else:\n            self.help_db()", "response": "\\ ~english DB command line interface"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef complete_db(self, text, line, begidx, endidx):\n        ''' Tab-complete db command '''\n        return  [i for i in PsiturkNetworkShell.db_commands if \\\n                 i.startswith(text)]", "response": "Tab - complete db command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_mode(self, arg):\n        restart_server = False\n        if self.server.is_server_running() == 'yes' or self.server.is_server_running() == 'maybe':\n            r = raw_input(\"Switching modes requires the server to restart. Really \"\n                          \"switch modes? y or n: \")\n            if r != 'y':\n                return\n            restart_server = True\n        if arg['<which>'] is None:\n            if self.sandbox:\n                arg['<which>'] = 'live'\n            else:\n                arg['<which>'] = 'sandbox'\n        if arg['<which>'] == 'live':\n            self.set_sandbox(False)\n            self.update_hit_tally()\n            print 'Entered %s mode' % colorize('live', 'bold')\n        else:\n            self.set_sandbox(True)\n            self.update_hit_tally()\n            print 'Entered %s mode' % colorize('sandbox', 'bold')\n        if restart_server:\n            self.server_restart()", "response": "Switches the user mode"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_tunnel(self, arg):\n        if arg['open']:\n            self.tunnel_open()\n        elif arg['change']:\n            self.tunnel_change()\n        elif arg['status']:\n            self.tunnel_status()", "response": "Tunnels the user with the specified set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the HIT command", "response": "def do_hit(self, arg):\n        \"\"\"\n        Usage:\n          hit create [<numWorkers> <reward> <duration>]\n          hit extend <HITid> [(--assignments <number>)] [(--expiration <minutes>)]\n          hit expire (--all | <HITid> ...)\n          hit dispose (--all | <HITid> ...)\n          hit delete (--all | <HITid> ...)\n          hit list [--active | --reviewable] [--all-studies]\n          hit help\n        \"\"\"\n\n        if arg['create']:\n            self.hit_create(arg['<numWorkers>'], arg['<reward>'],\n                            arg['<duration>'])\n            self.update_hit_tally()\n        elif arg['extend']:\n            self.amt_services_wrapper.hit_extend(arg['<HITid>'], arg['<number>'], arg['<minutes>'])\n        elif arg['expire']:\n            self.amt_services_wrapper.hit_expire(arg['--all'], arg['<HITid>'])\n            self.update_hit_tally()\n        elif arg['delete'] or arg['dispose']:\n            self.amt_services_wrapper.hit_delete(arg['--all'], arg['<HITid>'])\n            self.update_hit_tally()\n        elif arg['list']:\n            self.hit_list(arg['--active'], arg['--reviewable'], arg['--all-studies'])\n        else:\n            self.help_hit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef complete_hit(self, text, line, begidx, endidx):\n        ''' Tab-complete hit command. '''\n        return  [i for i in PsiturkNetworkShell.hit_commands if \\\n                 i.startswith(text)]", "response": "Tab - complete hit command."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the worker command", "response": "def do_worker(self, arg):\n        \"\"\"\n        Usage:\n          worker approve (--all | --hit <hit_id> ... | <assignment_id> ...) [--all-studies] [--force]\n          worker reject (--hit <hit_id> | <assignment_id> ...)\n          worker unreject (--hit <hit_id> | <assignment_id> ...)\n          worker bonus  (--amount <amount> | --auto) (--hit <hit_id> | <assignment_id> ...)\n          worker list [--submitted | --approved | --rejected] [(--hit <hit_id>)] [--all-studies]\n          worker help\n        \"\"\"\n        if arg['approve']:\n            self.worker_approve(arg['--all'], arg['<hit_id>'], arg['<assignment_id>'], arg['--all-studies'], arg['--force'])\n        elif arg['reject']:\n            self.amt_services_wrapper.worker_reject(arg['<hit_id>'], arg['<assignment_id>'])\n        elif arg['unreject']:\n            self.amt_services_wrapper.worker_unreject(arg['<hit_id>'], arg['<assignment_id>'])\n        elif arg['list']:\n            self.worker_list(arg['--submitted'], arg['--approved'], arg['--rejected'], arg['<hit_id>'], arg['--all-studies'])\n        elif arg['bonus']:\n            self.amt_services_wrapper.worker_bonus(arg['<hit_id>'], arg['--auto'], arg['<amount>'], '',\n                              arg['<assignment_id>'])\n        else:\n            self.help_worker()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_debug(self, arg):\n        revproxy_url = False\n        if self.config.has_option('Server Parameters','adserver_revproxy_host'):\n            if self.config.has_option( 'Server Parameters', 'adserver_revproxy_port'):\n                port = self.config.get( 'Server Parameters', 'adserver_revproxy_port')\n            else:\n                port = 80\n            revproxy_url = \"http://{}:{}/ad\".format(self.config.get('Server Parameters',\n                                                                    'adserver_revproxy_host'),\n                                                    port)\n\n        if revproxy_url:\n            base_url = revproxy_url\n        elif 'OPENSHIFT_SECRET_TOKEN' in os.environ:\n            base_url = \"http://\" + self.config.get('Server Parameters', 'host') + \"/ad\"\n        else:\n            if arg['--print-only']:\n                my_ip = get_my_ip()\n                base_url = \"http://\" + my_ip + \":\" + \\\n                    self.config.get('Server Parameters', 'port') + \"/ad\"\n            else:\n                base_url = \"http://\" + self.config.get('Server Parameters',\n                                                   'host') + \\\n                \":\" + self.config.get('Server Parameters', 'port') + \"/ad\"\n\n\n        launch_url = base_url + \"?assignmentId=debug\" + \\\n            str(self.random_id_generator()) \\\n            + \"&hitId=debug\" + str(self.random_id_generator()) \\\n            + \"&workerId=debug\" + str(self.random_id_generator() \\\n            + \"&mode=debug\")\n\n        if arg['--print-only']:\n            print(\"Here's your randomized debug link, feel free to request \" \\\n                  \"another:\\n\\t\" + launch_url)\n\n        else:\n            print(\"Launching browser pointed at your randomized debug link, \" \\\n                  \"feel free to request another.\\n\\t\" + launch_url)\n            webbrowser.open(launch_url, new=1, autoraise=True)", "response": "Displays the URL of the current instance of the current instance of the current instance of the current class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate credentials for the current class.", "response": "def update_credentials(self, aws_access_key_id, aws_secret_access_key):\n        ''' Update credentials '''\n        self.aws_access_key_id = aws_access_key_id\n        self.aws_secret_access_key = aws_secret_access_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_db_instance_info(self, dbid):\n        ''' Get DB instance info '''\n        if not self.connect_to_aws_rds():\n            return False\n        try:\n            instances = self.rdsc.describe_db_instances(dbid).get('DBInstances')\n        except:\n            return False\n        else:\n            myinstance = instances[0]\n            return myinstance", "response": "Get DB instance info"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nallow access to instance.", "response": "def allow_access_to_instance(self, _, ip_address):\n        ''' Allow access to instance. '''\n        if not self.connect_to_aws_rds():\n            return False\n        try:\n            conn = boto.ec2.connect_to_region(\n                self.region,\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key\n            )\n            sgs = conn.get_all_security_groups('default')\n            default_sg = sgs[0]\n            default_sg.authorize(ip_protocol='tcp', from_port=3306,\n                                 to_port=3306, cidr_ip=str(ip_address)+'/32')\n        except EC2ResponseError, exception:\n            if exception.error_code == \"InvalidPermission.Duplicate\":\n                return True  # ok it already exists\n            else:\n                return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of DB Instances", "response": "def get_db_instances(self):\n        '''  DB instance '''\n        if not self.connect_to_aws_rds():\n            return False\n        try:\n            instances = self.rdsc.describe_db_instances().get('DBInstances')\n        except:\n            return False\n        else:\n            return instances"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the size of the instance.", "response": "def validate_instance_size(self, size):\n        ''' integer between 5-1024 (inclusive) '''\n        try:\n            int(size)\n        except ValueError:\n            return '*** Error: size must be a whole number between 5 and 1024.'\n        if int(size) < 5 or int(size) > 1024:\n            return '*** Error: size must be between 5-1024 GB.'\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_instance_dbname(self, dbname):\n        ''' Validate instance database name '''\n        # 1-64 alphanumeric characters, cannot be a reserved MySQL word\n        if re.match('[\\w-]+$', dbname) is not None:\n            if len(dbname) <= 41 and len(dbname) >= 1:\n                if dbname.lower() not in MYSQL_RESERVED_WORDS:\n                    return True\n        return '*** Error: Database names must be 1-64 alphanumeric characters,\\\n            cannot be a reserved MySQL word.'", "response": "Validate instance database name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_hits(self):\n        ''' Get all HITs '''\n        if not self.connect_to_turk():\n            return False\n        try:\n            hits = []\n            paginator = self.mtc.get_paginator('list_hits')\n            for page in paginator.paginate():\n                hits.extend(page['HITs'])\n        except Exception as e:\n            print e\n            return False\n        hits_data = self._hit_xml_to_object(hits)\n        return hits_data", "response": "Get all HITs from Turk"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting workers from Turk", "response": "def get_workers(self, assignment_status=None, chosen_hits=None):\n        ''' Get workers '''\n        if not self.connect_to_turk():\n            return False\n        try:\n            if chosen_hits:\n                hit_ids = chosen_hits\n            else:\n                hits = self.get_all_hits()\n                hit_ids = [hit.options['hitid'] for hit in hits]\n\n            assignments = []\n            for hit_id in hit_ids:\n                paginator = self.mtc.get_paginator('list_assignments_for_hit')\n                args = dict(\n                    HITId=hit_id\n                )\n                if assignment_status:\n                    args['AssignmentStatuses'] = [assignment_status]\n\n                for page in paginator.paginate(**args):\n                    assignments.extend(page['Assignments'])\n\n        except Exception as e:\n            print e\n            return False\n        workers = [{\n            'hitId': assignment['HITId'],\n            'assignmentId': assignment['AssignmentId'],\n            'workerId': assignment['WorkerId'],\n            'submit_time': assignment['SubmitTime'],\n            'accept_time': assignment['AcceptTime'],\n            'status': assignment['AssignmentStatus'],\n        } for assignment in assignments]\n        return workers"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup_mturk_connection(self):\n        ''' Connect to turk '''\n        if ((self.aws_access_key_id == 'YourAccessKeyId') or\n                (self.aws_secret_access_key == 'YourSecretAccessKey')):\n            print \"AWS access key not set in ~/.psiturkconfig; please enter a valid access key.\"\n            assert False\n\n        if self.is_sandbox:\n            endpoint_url = 'https://mturk-requester-sandbox.us-east-1.amazonaws.com'\n        else:\n            endpoint_url = 'https://mturk-requester.us-east-1.amazonaws.com'\n\n        self.mtc = boto3.client('mturk',\n                region_name='us-east-1',\n                aws_access_key_id=self.aws_access_key_id,\n                aws_secret_access_key=self.aws_secret_access_key,\n                endpoint_url=endpoint_url)\n        return True", "response": "Setup connection to turk"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring HIT for this HIT.", "response": "def configure_hit(self, hit_config):\n        ''' Configure HIT '''\n\n        # Qualification:\n        quals = []\n        quals.append(dict(\n            QualificationTypeId=PERCENT_ASSIGNMENTS_APPROVED_QUAL_ID,\n            Comparator='GreaterThanOrEqualTo',\n            IntegerValues=[int(hit_config['approve_requirement'])]\n        ))\n\n        quals.append(dict(\n            QualificationTypeId=NUMBER_HITS_APPROVED_QUAL_ID,\n            Comparator='GreaterThanOrEqualTo',\n            IntegerValues=[int(hit_config['number_hits_approved'])]\n        ))\n\n        if hit_config['require_master_workers']:\n            master_qualId = MASTERS_SANDBOX_QUAL_ID if self.is_sandbox else MASTERS_QUAL_ID\n            quals.append(dict(\n                QualificationTypeId=master_qualId,\n                Comparator='Exists'\n            ))\n\n        if hit_config['us_only']:\n            quals.append(dict(\n                QualificationTypeId=LOCALE_QUAL_ID,\n                Comparator='EqualTo',\n                LocaleValues=[{'Country': 'US'}]\n            ))\n\n        # Create a HIT type for this HIT.\n        hit_type = self.mtc.create_hit_type(\n            Title=hit_config['title'],\n            Description=hit_config['description'],\n            Reward=str(hit_config['reward']),\n            AssignmentDurationInSeconds=int(hit_config['duration'].total_seconds()),\n            Keywords=hit_config['keywords'],\n            QualificationRequirements=quals)\n\n        # Check the config file to see if notifications are wanted.\n        config = PsiturkConfig()\n        config.load_config()\n\n        try:\n            url = config.get('Server Parameters', 'notification_url')\n\n            all_event_types = [\n                \"AssignmentAccepted\",\n                \"AssignmentAbandoned\",\n                \"AssignmentReturned\",\n                \"AssignmentSubmitted\",\n                \"HITReviewable\",\n                \"HITExpired\",\n            ]\n\n            # TODO: not sure if this works. Can't find documentation in PsiTurk or MTurk\n            self.mtc.update_notification_settings(\n                HitTypeId=hit_type['HITTypeId'],\n                Notification=dict(\n                    Destination=url,\n                    Transport='REST',\n                    Version=NOTIFICATION_VERSION,\n                    EventTypes=all_event_types,\n                ),\n            )\n\n        except Exception as e:\n            pass\n\n        schema_url = \"http://mechanicalturk.amazonaws.com/AWSMechanicalTurkDataSchemas/2006-07-14/ExternalQuestion.xsd\"\n        template = '<ExternalQuestion xmlns=\"%(schema_url)s\"><ExternalURL>%%(external_url)s</ExternalURL><FrameHeight>%%(frame_height)s</FrameHeight></ExternalQuestion>' % vars()\n        question = template % dict(\n            external_url=hit_config['ad_location'],\n            frame_height=600,\n        )\n\n\n        # Specify all the HIT parameters\n        self.param_dict = dict(\n            HITTypeId=hit_type['HITTypeId'],\n            Question=question,\n            LifetimeInSeconds=int(hit_config['lifetime'].total_seconds()),\n            MaxAssignments=hit_config['max_assignments'],\n            # TODO\n            # ResponseGroups=[\n            #     'Minimal',\n            #     'HITDetail',\n            #     'HITQuestion',\n            #     'HITAssignmentSummary'\n            # ]\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating HIT and return HITId", "response": "def create_hit(self, hit_config):\n        ''' Create HIT '''\n        try:\n            if not self.connect_to_turk():\n                return False\n            self.configure_hit(hit_config)\n            myhit = self.mtc.create_hit_with_hit_type(**self.param_dict)['HIT']\n            self.hitid = myhit['HITId']\n        except Exception as e:\n            print e\n            return False\n        else:\n            return self.hitid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget HIT from Turk", "response": "def get_hit(self, hitid):\n        ''' Get HIT '''\n        if not self.connect_to_turk():\n            return False\n        try:\n            hitdata = self.mtc.get_hit(HITId=hitid)\n        except Exception as e:\n            print e\n            return False\n        return hitdata['HIT']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck credentials for the current user", "response": "def check_credentials(self):\n        ''' Check credentials '''\n        req = requests.get(self.api_server + '/api/ad',\n                           auth=(self.access_key, self.secret_key))\n        # Not sure 500 server error should be included here\n        if req.status_code in [401, 403, 500]:\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_credentials(self, key, secret):\n        ''' Update credentials '''\n        self.access_key = key\n        self.secret_key = secret", "response": "Update the credentials of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_system_status(self):\n        try:\n            api_server_status_link = self.api_server + '/status_msg?version=' +\\\n                version_number\n            response = urllib2.urlopen(api_server_status_link, timeout=1)\n            status_msg = json.load(response)['status']\n        except urllib2.HTTPError:\n            status_msg = \"Sorry, can't connect to psiturk.org, please check\\\n                your internet connection.\\nYou will not be able to create new\\\n                hits, but testing locally should work.\\n\"\n        return status_msg", "response": "get_system_status - Returns the status of the system"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a record with the given content.", "response": "def create_record(self, name, content, username, password):\n        ''' Create record '''\n        #headers = {'key': username, 'secret': password}\n        req = requests.post(self.api_server + '/api/' + name,\n                            data=json.dumps(content), auth=(username, password))\n        return req"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery records for a specific record", "response": "def query_records(self, name, username, password, query=''):\n        ''' Query records '''\n        #headers = {'key': username, 'secret': password}\n        req = requests.get(self.api_server + '/api/' + name + \"/\" + query,\n                           auth=(username, password))\n        return req"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_ad(self, ad_content):\n        if not 'is_sandbox' in ad_content:\n            return False\n        else:\n            if ad_content['is_sandbox']:\n                req = self.create_record(\n                    'sandboxad', ad_content, self.access_key, self.secret_key\n                )\n            else:\n                req = self.create_record(\n                    'ad', ad_content, self.access_key, self.secret_key\n                )\n            if req.status_code == 201:\n                return req.json()['ad_id']\n            else:\n                return False", "response": "create_ad - Creates an ad record"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_experiment(self, experiment_id):\n        req = self.query_records('experiment', self.access_key,\n                                 self.secret_key,\n                                 query='download/'+experiment_id)\n        print req.text\n        return", "response": "Download the data for the given experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query_records_no_auth(self, name, query=''):\n        ''' Query records without authorization '''\n        #headers = {'key': username, 'secret': password}\n        req = requests.get(self.api_server + '/api/' + name + \"/\" + query)\n        return req", "response": "Query records without authorization"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_experiment(self, experiment_id):\n        req = self.query_records_no_auth('experiment',\n                                         query='download/'+experiment_id)\n        if req.status_code == 404:\n            print \"Sorry, no experiment matching id # \" + experiment_id\n            print \"Please double check the code you obtained on the\\\n                http://psiturk.org/ee\"\n        else:\n            # Check if folder with same name already exists.\n            expinfo = req.json()\n            gitr = requests.get(expinfo['git_url']).json()\n            if os.path.exists('./'+gitr['name']):\n                print \"*\"*20\n                print \"Sorry, you already have a file or folder named\\\n                    \"+gitr['name']+\". Please rename or delete it before trying\\\n                    to download this experiment.  You can do this by typing `rm\\\n                    -rf \" + gitr['name'] + \"`\"\n                print \"*\"*20\n                return\n            if \"clone_url\" in gitr:\n                git.Git().clone(gitr[\"clone_url\"])\n                print \"=\"*20\n                print \"Downloading...\"\n                print \"Name: \" + expinfo['name']\n                print \"Downloads: \" + str(expinfo['downloads'])\n                print \"Keywords: \" + expinfo['keywords']\n                print \"psiTurk Version: \" +\\\n                    str(expinfo['psiturk_version_string'])\n                print \"URL: http://psiturk.org/ee/\"+experiment_id\n                print \"\\n\"\n                print \"Experiment downloaded into the `\" + gitr['name'] + \"`\\\n                    folder of the current directory\"\n                print \"Type 'cd \" + gitr['name'] + \"` then run the `psiturk`\\\n                    command.\"\n                print \"=\"*20\n            else:\n                print \"Sorry, experiment not located on github.  You might\\\n                    contact the author of this experiment.  Experiment NOT\\\n                    downloaded.\"\n            return", "response": "Download the given experiment."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_compatible(cls):\n        ''' Check OS '''\n        is_64bit = struct.calcsize('P')*8 == 64\n        if (_platform == \"darwin\" and is_64bit):\n            return True\n        else:\n            print(\"Linux tunnels are currenlty unsupported.  Please notify \"\\\n                  \"authors@psiturk.org\\nif you'd like to see this feature.\")\n            return False", "response": "Check OS and OS version of the linux tunnels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget tunnel ad url from psiturk. org", "response": "def get_tunnel_ad_url(self):\n        ''' Get tunnel hostname from psiturk.org  '''\n        req = requests.get('https://api.psiturk.org/api/tunnel',\n                           auth=(self.access_key, self.secret_key))\n        if req.status_code in [401, 403, 500]:\n            print(req.content)\n            return False\n        else:\n            return req.json()['tunnel_hostname']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_tunnel_ad_url(self):\n        ''' Change tunnel ad url. '''\n        if self.is_open:\n            self.close()\n        req = requests.delete('https://api.psiturk.org/api/tunnel/',\n                              auth=(self.access_key, self.secret_key))\n        # the request content here actually will include the tunnel_hostname\n        # if needed or wanted.\n        if req.status_code in [401, 403, 500]:\n            print(req.content)\n            return False", "response": "Change tunnel ad url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self):\n        ''' Open tunnel '''\n        if self.is_compatible():\n            tunnel_ad_url = self.get_tunnel_ad_url()\n            if not tunnel_ad_url:\n                return(\"Tunnel server appears to be down.\")\n            cmd = '%s -subdomain=%s -config=%s -log=stdout %s 2>&1 > server.log' \\\n                      %(self.tunnel_server, tunnel_ad_url, self.tunnel_config,\n                        self.local_port)\n            self.tunnel = subprocess.Popen(cmd, shell=True)\n            self.url = '%s.%s' %(tunnel_ad_url, self.tunnel_host)\n            self.full_url = 'http://%s.%s:%s' %(tunnel_ad_url, self.tunnel_host,\n                                                self.tunnel_port)\n            self.is_open = True\n            print \"Tunnel URL: %s\" % self.full_url\n            print \"Hint: In OSX, you can open a terminal link using cmd + click\"", "response": "Open tunnel and open the terminal link."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef traveling_salesman(G, sampler=None, lagrange=2, weight='weight',\n                       **sampler_args):\n    \"\"\"Returns an approximate minimum traveling salesperson route.\n\n    Defines a QUBO with ground states corresponding to the\n    minimum routes and uses the sampler to sample\n    from it.\n\n    A route is a cycle in the graph that reaches each node exactly once.\n    A minimum route is a route with the smallest total edge weight.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to find a minimum traveling salesperson route.\n        This should be a complete graph with non-zero weights on every edge.\n\n    sampler :\n        A binary quadratic model sampler. A sampler is a process that\n        samples from low energy states in models defined by an Ising\n        equation or a Quadratic Unconstrained Binary Optimization\n        Problem (QUBO). A sampler is expected to have a 'sample_qubo'\n        and 'sample_ising' method. A sampler is expected to return an\n        iterable of samples, in order of increasing energy. If no\n        sampler is provided, one must be provided using the\n        `set_default_sampler` function.\n\n    lagrange : optional (default 2)\n        Lagrange parameter to weight constraints (visit every city once)\n        versus objective (shortest distance route).\n\n    weight : optional (default 'weight')\n        The name of the edge attribute containing the weight.\n\n    sampler_args :\n        Additional keyword parameters are passed to the sampler.\n\n    Returns\n    -------\n    route : list\n       List of nodes in order to be visited on a route\n\n    Examples\n    --------\n    This example uses a `dimod <https://github.com/dwavesystems/dimod>`_ sampler\n    to find a minimum route in a five-cities problem.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> import dimod\n    ...\n    >>> G = nx.complete_graph(4)\n    >>> G.add_weighted_edges_from({(0, 1, 1), (0, 2, 2), (0, 3, 3), (1, 2, 3),\n    ...                            (1, 3, 4), (2, 3, 5)})\n    >>> dnx.traveling_salesman(G, dimod.ExactSolver())\n    [2, 1, 0, 3]\n\n    Notes\n    -----\n    Samplers by their nature may not return the optimal solution. This\n    function does not attempt to confirm the quality of the returned\n    sample.\n\n    \"\"\"\n    # Get a QUBO representation of the problem\n    Q = traveling_salesman_qubo(G, lagrange, weight)\n\n    # use the sampler to find low energy states\n    response = sampler.sample_qubo(Q, **sampler_args)\n\n    # we want the lowest energy sample, in order by stop number\n    sample = next(iter(response))\n    route = []\n    for entry in sample:\n        if sample[entry] > 0:\n            route.append(entry)\n    route.sort(key=lambda x: x[1])\n    return list((x[0] for x in route))", "response": "Returns an approximate minimum traveling salesperson route."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the QUBO that is a QUBO that is a minimum travelling salesperson route.", "response": "def traveling_salesman_qubo(G, lagrange=2, weight='weight'):\n    \"\"\"Return the QUBO with ground states corresponding to a minimum TSP route.\n\n    If :math:`|G|` is the number of nodes in the graph, the resulting qubo will have:\n\n    * :math:`|G|^2` variables/nodes\n    * :math:`2 |G|^2 (|G| - 1)` interactions/edges\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        A complete graph in which each edge has a attribute giving its weight.\n\n    lagrange : number, optional (default 2)\n        Lagrange parameter to weight constraints (no edges within set)\n        versus objective (largest set possible).\n\n    weight : optional (default 'weight')\n        The name of the edge attribute containing the weight.\n\n    Returns\n    -------\n    QUBO : dict\n       The QUBO with ground states corresponding to a minimum travelling\n       salesperson route.\n\n    \"\"\"\n    N = G.number_of_nodes()\n\n    # some input checking\n    if N in (1, 2) or len(G.edges) != N*(N-1)//2:\n        msg = \"graph must be a complete graph with at least 3 nodes or empty\"\n        raise ValueError(msg)\n\n    # Creating the QUBO\n    Q = defaultdict(float)\n\n    # Constraint that each row has exactly one 1\n    for node in G:\n        for pos_1 in range(N):\n            Q[((node, pos_1), (node, pos_1))] -= lagrange\n            for pos_2 in range(pos_1+1, N):\n                Q[((node, pos_1), (node, pos_2))] += 2.0*lagrange\n\n    # Constraint that each col has exactly one 1\n    for pos in range(N):\n        for node_1 in G:\n            Q[((node_1, pos), (node_1, pos))] -= lagrange\n            for node_2 in set(G)-{node_1}:\n                Q[((node_1, pos), (node_2, pos))] += 2.0*lagrange\n\n    # Objective that minimizes distance\n    for u, v in itertools.combinations(G.nodes, 2):\n        for pos in range(N):\n            nextpos = (pos + 1) % N\n\n            # going from u -> v\n            Q[((u, pos), (v, nextpos))] += G[u][v][weight]\n\n            # going from v -> u\n            Q[((v, pos), (u, nextpos))] += G[u][v][weight]\n\n    return Q"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Markov Network from a dictionary of potentials.", "response": "def markov_network(potentials):\n    \"\"\"Creates a Markov Network from potentials.\n\n    A Markov Network is also knows as a `Markov Random Field`_\n\n    Parameters\n    ----------\n    potentials : dict[tuple, dict]\n        A dict where the keys are either nodes or edges and the values are a\n        dictionary of potentials. The potential dict should map each possible\n        assignment of the nodes/edges to their energy.\n\n    Returns\n    -------\n    MN : :obj:`networkx.Graph`\n        A markov network as a graph where each node/edge stores its potential\n        dict as above.\n\n    Examples\n    --------\n    >>> potentials = {('a', 'b'): {(0, 0): -1,\n    ...                            (0, 1): .5,\n    ...                            (1, 0): .5,\n    ...                            (1, 1): 2}}\n    >>> MN = dnx.markov_network(potentials)\n    >>> MN['a']['b']['potential'][(0, 0)]\n    -1\n\n    .. _Markov Random Field: https://en.wikipedia.org/wiki/Markov_random_field\n\n    \"\"\"\n\n    G = nx.Graph()\n\n    G.name = 'markov_network({!r})'.format(potentials)\n\n    # we use 'clique' because the keys of potentials can be either nodes or\n    # edges, but in either case they are fully connected.\n    for clique, phis in potentials.items():\n\n        num_vars = len(clique)\n\n        # because this data potentially wont be used for a while, let's do some\n        # input checking now and save some debugging issues later\n        if not isinstance(phis, abc.Mapping):\n            raise TypeError(\"phis should be a dict\")\n        elif not all(config in phis for config in itertools.product((0, 1), repeat=num_vars)):\n            raise ValueError(\"not all potentials provided for {!r}\".format(clique))\n\n        if num_vars == 1:\n            u, = clique\n            G.add_node(u, potential=phis)\n        elif num_vars == 2:\n            u, v = clique\n            # in python<=3.5 the edge order might not be consistent so we store\n            # the relevant order of the variables relative to the potentials\n            G.add_edge(u, v, potential=phis, order=(u, v))\n        else:\n            # developer note: in principle supporting larger cliques can be done\n            # using higher-order, but it would make the use of networkx graphs\n            # far more difficult\n            raise ValueError(\"Only supports cliques up to size 2\")\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an approximate maximum cut of a Chimera graph.", "response": "def maximum_cut(G, sampler=None, **sampler_args):\n    \"\"\"Returns an approximate maximum cut.\n\n    Defines an Ising problem with ground states corresponding to\n    a maximum cut and uses the sampler to sample from it.\n\n    A maximum cut is a subset S of the vertices of G such that\n    the number of edges between S and the complementary subset\n    is as large as possible.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to find a maximum cut.\n\n    sampler\n        A binary quadratic model sampler. A sampler is a process that\n        samples from low energy states in models defined by an Ising\n        equation or a Quadratic Unconstrained Binary Optimization\n        Problem (QUBO). A sampler is expected to have a 'sample_qubo'\n        and 'sample_ising' method. A sampler is expected to return an\n        iterable of samples, in order of increasing energy. If no\n        sampler is provided, one must be provided using the\n        `set_default_sampler` function.\n\n    sampler_args\n        Additional keyword parameters are passed to the sampler.\n\n    Returns\n    -------\n    S : set\n        A maximum cut of G.\n\n    Example\n    -------\n    This example uses a sampler from\n    `dimod <https://github.com/dwavesystems/dimod>`_ to find a maximum cut\n    for a graph of a Chimera unit cell created using the `chimera_graph()`\n    function.\n\n    >>> import dimod\n    >>> import dwave_networkx as dnx\n    >>> samplerSA = dimod.SimulatedAnnealingSampler()\n    >>> G = dnx.chimera_graph(1, 1, 4)\n    >>> cut = dnx.maximum_cut(G, samplerSA)\n\n    Notes\n    -----\n    Samplers by their nature may not return the optimal solution. This\n    function does not attempt to confirm the quality of the returned\n    sample.\n\n    \"\"\"\n    # In order to form the Ising problem, we want to increase the\n    # energy by 1 for each edge between two nodes of the same color.\n    # The linear biases can all be 0.\n    h = {v: 0. for v in G}\n    J = {(u, v): 1 for u, v in G.edges}\n\n    # draw the lowest energy sample from the sampler\n    response = sampler.sample_ising(h, J, **sampler_args)\n    sample = next(iter(response))\n\n    return set(v for v in G if sample[v] >= 0)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef weighted_maximum_cut(G, sampler=None, **sampler_args):\n    # In order to form the Ising problem, we want to increase the\n    # energy by 1 for each edge between two nodes of the same color.\n    # The linear biases can all be 0.\n    h = {v: 0. for v in G}\n    try:\n        J = {(u, v): G[u][v]['weight'] for u, v in G.edges}\n    except KeyError:\n        raise DWaveNetworkXException(\"edges must have 'weight' attribute\")\n\n    # draw the lowest energy sample from the sampler\n    response = sampler.sample_ising(h, J, **sampler_args)\n    sample = next(iter(response))\n\n    return set(v for v in G if sample[v] >= 0)", "response": "Returns an approximate weighted maximum cut of a graph G."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a Pegasus graph for the given size parameter m.", "response": "def pegasus_graph(m, create_using=None, node_list=None, edge_list=None, data=True,\n                  offset_lists=None, offsets_index=None, coordinates=False, fabric_only=True,\n                  nice_coordinates=False):\n    \"\"\"\n    Creates a Pegasus graph with size parameter m.  The number of nodes and edges varies\n    according to multiple parameters, for example,\n\n        pegasus_graph(1) contains zero nodes,\n        pegasus_graph(m, fabric_only=False) contains :math:`24m(m-1)` nodes,\n        pegasus_graph(m, fabric_only=True) contains :math:`24m(m-1)-8m` nodes, and\n        pegasus_graph(m, nice_coordinates=True) contains :math:`24(m-1)^2` nodes.\n\n    The maximum degree of these graph is 15, and counting formulas are more complicated\n    for edges given most parameter settings.  Upper bounds are given below,\n\n        pegasus_graph(1, fabric_only=False) has zero edges,\n        pegasus_graph(m, fabric_only=False) has :math:`12*(15*(m-1)^2 + m - 3)` edges if m > 1\n\n    Note that the above are valid with default offset parameters.\n\n    A Pegasus lattice is a graph minor of a lattice similar to Chimera,\n    where unit tiles are completely connected.  In the most generality,\n    prelattice :math:`Q(N0,N1)` contains nodes of the form\n\n        :math:`(i, j, 0, k)` with :math:`0 <= k < 2` [vertical nodes]\n\n    and\n\n        :math:`(i, j, 1, k)` with :math:`0 <= k < 2` [horizontal nodes]\n\n    for :math:`0 <= i <= N0` and :math:`0 <= j < N1`; and edges of the form\n\n        :math:`(i, j, u, k)` ~ :math:`(i+u, j+1-u, u, k)`  [external edges]\n\n        :math:`(i, j, 0, k)` ~ :math:`(i, j, 1, k)` [internal edges]\n\n        :math:`(i, j, u, 0)` ~ :math:`(i, j, u, 1)` [odd edges]\n\n    The minor is specified by two lists of offsets; :math:`S0` and :math:`S1` of length\n    :math:`L0` and :math:`L1` (where :math:`L0` and :math:`L1`, and the entries of\n    :math:`S0` and :math:`S1`, must be divisible by 2).\n    From these offsets, we construct our minor, a Pegasus lattice, by contracting\n    the complete intervals of external edges::\n\n        I(0, w, k, z) = [(L1*w + k, L0*z + S0[k] + r, 0, k % 2) for 0 <= r < L0]\n        I(1, w, k, z) = [(L1*z + S1[k] + r, L0*w + k, 1, k % 2) for 0 <= r < L1]\n\n    and deleting the prelattice nodes of any interval not fully contained in\n    :math:`Q(N0, N1)`.\n\n    This generator is specialized to :math:`L0 = L1 = 12`; :math:`N0 = N1 = 12m`.\n\n    The notation :math:`(u, w, k, z)` is called the Pegasus index of a node in a Pegasus\n    lattice.  The entries can be interpreted as following,\n\n        :math:`u`: qubit orientation (0 = vertical, 1 = horizontal)\n\n        :math:`w`: orthogonal major offset\n\n        :math:`k`: orthogonal minor offset\n\n        :math:`z`: parallel offset\n\n    and the edges in the minor have the form\n\n        :math:`(u, w, k, z)` ~ :math:`(u, w, k, z+1)` [external edges]\n\n        :math:`(0, w0, k0, z0)` ~ :math:`(1, w1, k1, z1)` [internal edges, see below]\n\n        :math:`(u, w, 2k, z)` ~ :math:`(u, w, 2k+1, z)` [odd edges]\n\n    where internal edges only exist when::\n\n        w1 = z0 + (1 if k1 < S0[k0] else 0), and\n        z1 = w0 - (1 if k0 < S1[k1] else 0)\n\n    linear indices are computed from pegasus indices by the formula::\n\n        q = ((u * m + w) * 12 + k) * (m - 1) + z\n\n    Parameters\n    ----------\n    m : int\n        The size parameter for the Pegasus lattice.\n    create_using : Graph, optional (default None)\n        If provided, this graph is cleared of nodes and edges and filled\n        with the new graph. Usually used to set the type of the graph.\n    node_list : iterable, optional (default None)\n        Iterable of nodes in the graph. If None, calculated from m.\n        Note that this list is used to remove nodes, so any nodes specified\n        not in range(24 * m * (m-1)) will not be added.\n    edge_list : iterable, optional (default None)\n        Iterable of edges in the graph. If None, edges are generated as\n        described above. The nodes in each edge must be integer-labeled in\n        range(24 * m * (m-1)).\n    data : bool, optional (default True)\n        If True, each node has a pegasus_index attribute. The attribute\n        is a 4-tuple Pegasus index as defined above. (if coordinates = True,\n        we set a linear_index, which is an integer)\n    coordinates : bool, optional (default False)\n        If True, node labels are 4-tuple Pegasus indices.  Ignored if\n        nice_coordinates is True\n    offset_lists : pair of lists, optional (default None)\n        Used to directly control the offsets, each list in the pair should\n        have length 12, and contain even ints.  If offset_lists is not None,\n        then offsets_index must be None.\n    offsets_index : int, optional (default None)\n        A number between 0 and 7 inclusive, to select a preconfigured\n        set of topological parameters.  If both offsets_index and\n        offset_lists are None, then we set offsets_index = 0.  At least\n        one of these two parameters must be None.\n    fabric_only: bool, optional (default True)\n        The Pegasus graph, by definition, will have some disconnected\n        components.  If this True, we will only construct nodes from the\n        largest component.  Otherwise, the full disconnected graph will be\n        constructed.  Ignored if edge_lists is not None or nice_coordinates\n        is True\n    nice_coordinates: bool, optional (default False)\n        In the case that offsets_index = 0, generate the graph with a nicer\n        coordinate system which is more compatible with Chimera addressing.\n        These coordinates are 5-tuples taking the form (t, y, x, u, k) where\n        0 <= x < M-1, 0 <= y < M-1, 0 <= u < 2, 0 <= k < 4 and 0 <= t < 3.\n        For any given 0 <= t0 < 3, the subgraph of nodes with t = t0 has the\n        structure of chimera(M-1, M-1, 4) with the addition of odd couplers.\n        Supercedes both the fabric_only and coordinates parameters.\n    \"\"\"\n    if offset_lists is None:\n        offsets_descriptor = offsets_index = offsets_index or 0\n        offset_lists = [\n            [(2, 2, 2, 2, 10, 10, 10, 10, 6, 6, 6, 6,), (6, 6, 6, 6, 2, 2, 2, 2, 10, 10, 10, 10,)],\n            [(2, 2, 2, 2, 10, 10, 10, 10, 6, 6, 6, 6,), (2, 2, 2, 2, 10, 10, 10, 10, 6, 6, 6, 6,)],\n            [(2, 2, 2, 2, 10, 10, 10, 10, 6, 6, 6, 6,), (10, 10, 10, 10, 6, 6, 6, 6, 2, 2, 2, 2,)],\n            [(10, 10, 10, 10, 6, 6, 6, 6, 2, 2, 2, 2,), (10, 10, 10, 10, 6, 6, 6, 6, 2, 2, 2, 2,)],\n            [(10, 10, 10, 10, 6, 6, 6, 6, 2, 2, 2, 2,), (2, 2, 2, 2, 6, 6, 6, 6, 10, 10, 10, 10,)],\n            [(6, 6, 2, 2, 2, 2, 10, 10, 10, 10, 6, 6,), (6, 6, 2, 2, 2, 2, 10, 10, 10, 10, 6, 6,)],\n            [(6, 6, 2, 2, 2, 2, 10, 10, 10, 10, 6, 6,), (6, 6, 10, 10, 10, 10, 2, 2, 2, 2, 6, 6,)],\n            [(6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,), (6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,)],\n        ][offsets_index]\n    elif offsets_index is not None:\n        raise DWaveNetworkXException(\"provide at most one of offsets_index and offset_lists\")\n    else:\n        for ori in 0, 1:\n            for x, y in zip(offset_lists[ori][::2], offset_lists[ori][1::2]):\n                if x != y:\n                    warnings.warn(\"The offets list you've provided is possibly non-physical.  Odd-coupled qubits should have the same value.\")\n        offsets_descriptor = offset_lists\n\n    G = nx.empty_graph(0, create_using)\n\n    G.name = \"pegasus_graph(%s, %s)\" % (m, offsets_descriptor)\n\n    m1 = m - 1\n    if nice_coordinates:\n        if offsets_index != 0:\n            raise NotImplementedError(\"nice coordinate system is only implemented for offsets_index 0\")\n        labels = 'nice'\n        c2i = get_pegasus_to_nice_fn()\n    elif coordinates:\n        c2i = lambda *q: q\n        labels = 'coordinate'\n    else:\n        labels = 'int'\n        def c2i(u, w, k, z): return u * 12 * m * m1 + w * 12 * m1 + k * m1 + z\n\n    construction = ((\"family\", \"pegasus\"), (\"rows\", m), (\"columns\", m),\n                    (\"tile\", 12), (\"vertical_offsets\", offset_lists[0]),\n                    (\"horizontal_offsets\", offset_lists[1]), (\"data\", data),\n                    (\"labels\", labels))\n\n    G.graph.update(construction)\n\n    max_size = m * (m - 1) * 24  # max number of nodes G can have\n\n    if edge_list is None:\n        if nice_coordinates:\n            fabric_start = 4,8\n            fabric_end = 8, 4\n        elif fabric_only:\n            fabric_start = min(s for s in offset_lists[1]), min(s for s in offset_lists[0])\n            fabric_end = 12 - max(s for s in offset_lists[1]), 12 - max(s for s in offset_lists[0])\n        else:\n            fabric_end = fabric_start = 0, 0\n\n        G.add_edges_from((c2i(u, w, k, z), c2i(u, w, k, z + 1))\n                         for u in (0, 1)\n                         for w in range(m)\n                         for k in range(fabric_start[u] if w == 0 else 0, 12 - (fabric_end[u] if w == m1 else 0))\n                         for z in range(m1 - 1))\n\n        G.add_edges_from((c2i(u, w, k, z), c2i(u, w, k + 1, z))\n                         for u in (0, 1)\n                         for w in range(m)\n                         for k in range(fabric_start[u] if w == 0 else 0, 12 - (fabric_end[u] if w == m1 else 0), 2)\n                         for z in range(m1))\n\n        off0, off1 = offset_lists\n        def qfilter(u, w, k, z):\n            if w == 0: return k >= fabric_start[u]\n            if w == m1: return k < 12-fabric_end[u]\n            return True\n        def efilter(e): return qfilter(*e[0]) and qfilter(*e[1])\n\n        internal_couplers = (((0, w, k, z), (1, z + (kk < off0[k]), kk, w - (k < off1[kk])))\n                         for w in range(m)\n                         for kk in range(12)\n                         for k in range(0 if w else off1[kk], 12 if w < m1 else off1[kk])\n                         for z in range(m1))\n        G.add_edges_from((c2i(*e[0]), c2i(*e[1])) for e in internal_couplers if efilter(e))\n\n    else:\n        G.add_edges_from(edge_list)\n\n    if node_list is not None:\n        nodes = set(node_list)\n        G.remove_nodes_from(set(G) - nodes)\n        G.add_nodes_from(nodes)  # for singleton nodes\n\n    if data:\n        v = 0\n        for u in range(2):\n            for w in range(m):\n                for k in range(12):\n                    for z in range(m1):\n                        q = u, w, k, z\n                        if nice_coordinates:\n                            p = c2i(*q)\n                            if p in G:\n                                G.node[p]['linear_index'] = v\n                                G.node[p]['pegasus_index'] = q\n                        elif coordinates:\n                            if q in G:\n                                G.node[q]['linear_index'] = v\n                        else:\n                            if v in G:\n                                G.node[v]['pegasus_index'] = q\n                        v += 1\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pegasus_elimination_order(n, coordinates=False):\n    '''\n    Produces a variable elimination order for a pegasus P(n) graph, which provides an upper bound on the treewidth.\n\n    Simple pegasus variable elimination order rules:\n       - eliminate vertical qubits, one column at a time\n       - eliminate horizontal qubits in each column once their adjacent vertical qubits have been eliminated\n\n    Many other orderings are possible giving the same treewidth upper bound of 12n-4 for a P(n)\n    see pegasus_var_order_extra.py for a few.\n    P(n) contains cliques of size 12n-10 so we know the treewidth of P(n) is in [12n-11,12n-4].\n\n    the treewidth bound generated by a variable elimination order can be verified using\n    dwave_networkx.elimination_order_width. Example:\n    import dwave_networkx as dwnx\n    n = 6\n    P = dwnx.generators.pegasus_graph(n)\n    order = pegasus_var_order(n)\n    print(dwnx.elimination_order_width(P,order))\n    '''\n    m = n\n    l = 12\n\n    # ordering for horizontal qubits in each tile, from east to west:\n    h_order = [4, 5, 6, 7, 0, 1, 2, 3, 8, 9, 10, 11]\n    order = []\n    for n_i in range(n):  # for each tile offset\n        # eliminate vertical qubits:\n        for l_i in range(0, l, 2):\n            for l_v in range(l_i, l_i + 2):\n                for m_i in range(m - 1):  # for each column\n                    order.append((0, n_i, l_v, m_i))\n            # eliminate horizontal qubits:\n            if n_i > 0 and not(l_i % 4):\n                # a new set of horizontal qubits have had all their neighbouring vertical qubits eliminated.\n                for m_i in range(m):\n                    for l_h in range(h_order[l_i], h_order[l_i] + 4):\n                        order.append((1, m_i, l_h, n_i - 1))\n\n    if coordinates:\n        return order\n    else:\n        return pegasus_coordinates(n).ints(order)", "response": "Generates a variable elimination order for a pegasus P n."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tuple_fragmentation_fn(pegasus_graph):\n    horizontal_offsets = pegasus_graph.graph['horizontal_offsets']\n    vertical_offsets = pegasus_graph.graph['vertical_offsets']\n\n    # Note: we are returning a fragmentation function rather than fragmenting the pegasus\n    # coordinates ourselves because:\n    #   (1) We don't want the user to have to deal with Pegasus horizontal/vertical offsets directly.\n    #       (i.e. Don't want fragment_tuple(pegasus_coord, vertical_offset, horizontal_offset))\n    #   (2) We don't want the user to have to pass entire Pegasus graph each time they want to\n    #       fragment some pegasus coordinates.\n    #       (i.e. Don't want fragment_tuple(pegasus_coord, pegasus_graph))\n    def fragment_tuple(pegasus_coords):\n        fragments = []\n        for u, w, k, z in pegasus_coords:\n            # Determine offset\n            offset = horizontal_offsets if u else vertical_offsets\n            offset = offset[k]\n\n            # Find the base (i.e. zeroth) Chimera fragment of this pegasus coordinate\n            x0 = (z * 12 + offset) // 2\n            y = (w * 12 + k) // 2\n            r = k % 2\n            base = [0, 0, u, r]\n\n            # Generate the six fragments associated with this pegasus coordinate\n            for x in range(x0, x0 + 6):\n                base[u] = x\n                base[1 - u] = y\n                fragments.append(tuple(base))\n\n        return fragments\n\n    return fragment_tuple", "response": "Returns a function that returns a tuple that is specific to the given pegasus_graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tuple_defragmentation_fn(pegasus_graph):\n    horizontal_offsets = pegasus_graph.graph['horizontal_offsets']\n    vertical_offsets = pegasus_graph.graph['vertical_offsets']\n\n    # Note: we are returning a defragmentation function rather than defragmenting the chimera\n    # fragments ourselves because:\n    #   (1) We don't want the user to have to deal with Pegasus horizontal/vertical offsets directly.\n    #       (i.e. Don't want defragment_tuple(chimera_coord, vertical_offset, horizontal_offset))\n    #   (2) We don't want the user to have to pass entire Pegasus graph each time they want to\n    #       defragment some chimera coordinates.\n    #       (i.e. Don't want defragment_tuple(chimera_coord, pegasus_graph))\n    def defragment_tuple(chimera_coords):\n        pegasus_coords = []\n        for y, x, u, r in chimera_coords:\n            # Set up shifts and offsets\n            shifts = [x, y]\n            offsets = horizontal_offsets if u else vertical_offsets\n\n            # Determine number of tiles and track number\n            w, k = divmod(2 * shifts[u] + r, 12)\n\n            # Determine qubit index on track\n            x0 = shifts[1-u] * 2 - offsets[k]\n            z = x0 // 12\n\n            pegasus_coords.append((u, w, k, z))\n\n        # Several chimera coordinates may map to the same pegasus coordinate, hence, apply set(..)\n        return list(set(pegasus_coords))\n\n    return defragment_tuple", "response": "Returns a function that returns a list of tuples that can be used to defragment the Chimera coordinates of a given set of Chimera coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a function that returns a coordinate translation function from the 4 - term pegasus_index and 5 - term nice coordinates to the 5 - term nice coordinates.", "response": "def get_pegasus_to_nice_fn(*args, **kwargs):\n    \"\"\"\n    Returns a coordinate translation function from the 4-term pegasus_index\n    coordinates to the 5-term \"nice\" coordinates.\n\n    Details on the returned function, pegasus_to_nice(u,w,k,z)\n        Inputs are 4-tuples of ints, return is a 5-tuple of ints.  See\n        pegasus_graph for description of the pegasus_index and \"nice\"\n        coordinate systems.\n\n    Returns\n    -------\n    pegasus_to_nice_fn(chimera_coordinates): a function\n        A function that accepts augmented chimera coordinates and returns corresponding \n        Pegasus coordinates.\n    \"\"\"\n    if args or kwargs:\n        warnings.warn(\"Deprecation warning: get_pegasus_to_nice_fn does not need / use parameters anymore\")\n    def p2n0(u, w, k, z): return (0, w-1 if u else z, z if u else w, u, k-4 if u else k-4)\n    def p2n1(u, w, k, z): return (1, w-1 if u else z, z if u else w, u, k if u else k-8)\n    def p2n2(u, w, k, z): return (2, w if u else z, z if u else w-1, u, k-8 if u else k)\n    def p2n(u, w, k, z): return [p2n0, p2n1, p2n2][(2-u-(2*u-1)*(k//4)) % 3](u, w, k, z)\n    return p2n"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a function that converts the nice coordinates to the 4 - term pegasus coordinates.", "response": "def get_nice_to_pegasus_fn(*args, **kwargs):\n    \"\"\"\n    Returns a coordinate translation function from the 5-term \"nice\"\n    coordinates to the 4-term pegasus_index coordinates.\n\n    Details on the returned function, nice_to_pegasus(t, y, x, u, k)\n        Inputs are 5-tuples of ints, return is a 4-tuple of ints.  See\n        pegasus_graph for description of the pegasus_index and \"nice\"\n        coordinate systems.\n\n    Returns\n    -------\n    nice_to_pegasus_fn(pegasus_coordinates): a function\n        A function that accepts Pegasus coordinates and returns the corresponding \n        augmented chimera coordinates\n    \"\"\"\n    if args or kwargs:\n        warnings.warn(\"Deprecation warning: get_pegasus_to_nice_fn does not need / use parameters anymore\")\n    def c2p0(y, x, u, k): return (u, y+1 if u else x, 4+k if u else 4+k, x if u else y)\n    def c2p1(y, x, u, k): return (u, y+1 if u else x, k if u else 8+k, x if u else y)\n    def c2p2(y, x, u, k): return (u, y if u else x + 1, 8+k if u else k, x if u else y)\n    def n2p(t, y, x, u, k): return [c2p0, c2p1, c2p2][t](y, x, u, k)\n    return n2p"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting the chimera_index q into an integer representation of the linear_index node label corresponding to q .", "response": "def int(self, q):\n        \"\"\"\n        Converts the chimera_index `q` into an linear_index\n\n        Parameters\n        ----------\n        q : tuple\n            The chimera_index node label\n\n        Returns\n        -------\n        r : int\n            The linear_index node label corresponding to q\n        \"\"\"\n\n        u, w, k, z = q\n        m, m1 = self.args\n        return ((m * u + w) * 12 + k) * m1 + z"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tuple(self, r):\n\n        m, m1 = self.args\n        r, z = divmod(r, m1)\n        r, k = divmod(r, 12)\n        u, w = divmod(r, m)\n        return u, w, k, z", "response": "Converts the linear_index q into an pegasus_index u w k z"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ints(self, qlist):\n\n        m, m1 = self.args\n        return (((m * u + w) * 12 + k) * m1 + z for (u, w, k, z) in qlist)", "response": "Converts a sequence of pegasus_index node labels into a linear_index node labels preserving order\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a sequence of linear_index node labels into pegasus_index node labels preserving order", "response": "def tuples(self, rlist):\n        \"\"\"\n        Converts a sequence of linear_index node labels into\n        pegasus_index node labels, preserving order\n\n        Parameters\n        ----------\n        rlist : sequence of tuples\n            The linear_index node labels\n\n        Returns\n        -------\n        qlist : iterable of ints\n            The pegasus_index node lables corresponding to rlist\n        \"\"\"\n\n        m, m1 = self.args\n        for r in rlist:\n            r, z = divmod(r, m1)\n            r, k = divmod(r, 12)\n            u, w = divmod(r, m)\n            yield u, w, k, z"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nflattens a sequence of pairs to pass through f and then re - pairs the result.", "response": "def __pair_repack(self, f, plist):\n        \"\"\"\n        Flattens a sequence of pairs to pass through `f`, and then\n        re-pairs the result.\n\n        Parameters\n        ----------\n        f : callable\n            A function that accepts a sequence and returns a sequence\n        plist:\n            A sequence of pairs\n\n        Returns\n        -------\n        qlist : sequence\n            Equivalent to (tuple(f(p)) for p in plist)\n        \"\"\"\n        ulist = f(u for p in plist for u in p)\n        for u in ulist:\n            v = next(ulist)\n            yield u, v"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples from a Markov network using the provided sampler.", "response": "def sample_markov_network(MN, sampler=None, fixed_variables=None,\n                          return_sampleset=False,\n                          **sampler_args):\n    \"\"\"Samples from a markov network using the provided sampler.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        A Markov Network as returned by :func:`.markov_network`\n\n    sampler\n        A binary quadratic model sampler. A sampler is a process that\n        samples from low energy states in models defined by an Ising\n        equation or a Quadratic Unconstrained Binary Optimization\n        Problem (QUBO). A sampler is expected to have a 'sample_qubo'\n        and 'sample_ising' method. A sampler is expected to return an\n        iterable of samples, in order of increasing energy. If no\n        sampler is provided, one must be provided using the\n        `set_default_sampler` function.\n\n    fixed_variables : dict\n        A dictionary of variable assignments to be fixed in the markov network.\n\n    return_sampleset : bool (optional, default=False)\n        If True, returns a :obj:`dimod.SampleSet` rather than a list of samples.\n\n    **sampler_args\n        Additional keyword parameters are passed to the sampler.\n\n    Returns\n    -------\n    samples : list[dict]/:obj:`dimod.SampleSet`\n        A list of samples ordered from low-to-high energy or a sample set.\n\n    Examples\n    --------\n\n    >>> import dimod\n    ...\n    >>> potentials = {('a', 'b'): {(0, 0): -1,\n    ...                            (0, 1): .5,\n    ...                            (1, 0): .5,\n    ...                            (1, 1): 2}}\n    >>> MN = dnx.markov_network(potentials)\n    >>> sampler = dimod.ExactSolver()\n    >>> samples = dnx.sample_markov_network(MN, sampler)\n    >>> samples[0]\n    {'a': 0, 'b': 0}\n\n    >>> import dimod\n    ...\n    >>> potentials = {('a', 'b'): {(0, 0): -1,\n    ...                            (0, 1): .5,\n    ...                            (1, 0): .5,\n    ...                            (1, 1): 2}}\n    >>> MN = dnx.markov_network(potentials)\n    >>> sampler = dimod.ExactSolver()\n    >>> samples = dnx.sample_markov_network(MN, sampler, return_sampleset=True)\n    >>> samples.first\n    Sample(sample={'a': 0, 'b': 0}, energy=-1.0, num_occurrences=1)\n\n    >>> import dimod\n    ...\n    >>> potentials = {('a', 'b'): {(0, 0): -1,\n    ...                            (0, 1): .5,\n    ...                            (1, 0): .5,\n    ...                            (1, 1): 2},\n    ...               ('b', 'c'): {(0, 0): -9,\n    ...                            (0, 1): 1.2,\n    ...                            (1, 0): 7.2,\n    ...                            (1, 1): 5}}\n    >>> MN = dnx.markov_network(potentials)\n    >>> sampler = dimod.ExactSolver()\n    >>> samples = dnx.sample_markov_network(MN, sampler, fixed_variables={'b': 0})\n    >>> samples[0]\n    {'a': 0, 'c': 0, 'b': 0}\n\n    Notes\n    -----\n    Samplers by their nature may not return the optimal solution. This\n    function does not attempt to confirm the quality of the returned\n    sample.\n\n    \"\"\"\n\n    bqm = markov_network_bqm(MN)\n\n    # use the FixedVar\n    fv_sampler = dimod.FixedVariableComposite(sampler)\n\n    sampleset = fv_sampler.sample(bqm, fixed_variables=fixed_variables,\n                                  **sampler_args)\n\n    if return_sampleset:\n        return sampleset\n    else:\n        return list(map(dict, sampleset.samples()))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef markov_network_bqm(MN):\n\n    bqm = dimod.BinaryQuadraticModel.empty(dimod.BINARY)\n\n    # the variable potentials\n    for v, ddict in MN.nodes(data=True, default=None):\n        potential = ddict.get('potential', None)\n\n        if potential is None:\n            continue\n\n        # for single nodes we don't need to worry about order\n\n        phi0 = potential[(0,)]\n        phi1 = potential[(1,)]\n\n        bqm.add_variable(v, phi1 - phi0)\n        bqm.add_offset(phi0)\n\n    # the interaction potentials\n    for u, v, ddict in MN.edges(data=True, default=None):\n        potential = ddict.get('potential', None)\n\n        if potential is None:\n            continue\n\n        # in python<=3.5 the edge order might not be consistent so we use the\n        # one that was stored\n        order = ddict['order']\n        u, v = order\n\n        phi00 = potential[(0, 0)]\n        phi01 = potential[(0, 1)]\n        phi10 = potential[(1, 0)]\n        phi11 = potential[(1, 1)]\n\n        bqm.add_variable(u, phi10 - phi00)\n        bqm.add_variable(v, phi01 - phi00)\n        bqm.add_interaction(u, v, phi11 - phi10 - phi01 + phi00)\n        bqm.add_offset(phi00)\n\n    return bqm", "response": "Construct a binary quadratic model for a Markov network."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dictionary of positions of nodes in a Chimera cross topology.", "response": "def chimera_layout(G, scale=1., center=None, dim=2):\n    \"\"\"Positions the nodes of graph G in a Chimera cross topology.\n\n    NumPy (http://scipy.org) is required for this function.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        Should be a Chimera graph or a subgraph of a\n        Chimera graph. If every node in G has a `chimera_index`\n        attribute, those are used to place the nodes. Otherwise makes\n        a best-effort attempt to find positions.\n\n    scale : float (default 1.)\n        Scale factor. When scale = 1,  all positions fit within [0, 1]\n        on the x-axis and [-1, 0] on the y-axis.\n\n    center : None or array (default None)\n        Coordinates of the top left corner.\n\n    dim : int (default 2)\n        Number of dimensions. When dim > 2, all extra dimensions are\n        set to 0.\n\n    Returns\n    -------\n    pos : dict\n        A dictionary of positions keyed by node.\n\n    Examples\n    --------\n    >>> G = dnx.chimera_graph(1)\n    >>> pos = dnx.chimera_layout(G)\n\n    \"\"\"\n\n    if not isinstance(G, nx.Graph):\n        empty_graph = nx.Graph()\n        empty_graph.add_edges_from(G)\n        G = empty_graph\n\n    # now we get chimera coordinates for the translation\n    # first, check if we made it\n    if G.graph.get(\"family\") == \"chimera\":\n        m = G.graph['rows']\n        n = G.graph['columns']\n        t = G.graph['tile']\n        # get a node placement function\n        xy_coords = chimera_node_placer_2d(m, n, t, scale, center, dim)\n\n        if G.graph.get('labels') == 'coordinate':\n            pos = {v: xy_coords(*v) for v in G.nodes()}\n        elif G.graph.get('data'):\n            pos = {v: xy_coords(*dat['chimera_index']) for v, dat in G.nodes(data=True)}\n        else:\n            coord = chimera_coordinates(m, n, t)\n            pos = {v: xy_coords(*coord.tuple(v)) for v in G.nodes()}\n    else:\n        # best case scenario, each node in G has a chimera_index attribute. Otherwise\n        # we will try to determine it using the find_chimera_indices function.\n        if all('chimera_index' in dat for __, dat in G.nodes(data=True)):\n            chimera_indices = {v: dat['chimera_index'] for v, dat in G.nodes(data=True)}\n        else:\n            chimera_indices = find_chimera_indices(G)\n\n        # we could read these off of the name attribute for G, but we would want the values in\n        # the nodes to override the name in case of conflict.\n        m = max(idx[0] for idx in itervalues(chimera_indices)) + 1\n        n = max(idx[1] for idx in itervalues(chimera_indices)) + 1\n        t = max(idx[3] for idx in itervalues(chimera_indices)) + 1\n        xy_coords = chimera_node_placer_2d(m, n, t, scale, center, dim)\n\n        # compute our coordinates\n        pos = {v: xy_coords(i, j, u, k) for v, (i, j, u, k) in iteritems(chimera_indices)}\n\n    return pos"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chimera_node_placer_2d(m, n, t, scale=1., center=None, dim=2):\n    import numpy as np\n\n    tile_center = t // 2\n    tile_length = t + 3  # 1 for middle of cross, 2 for spacing between tiles\n    # want the enter plot to fill in [0, 1] when scale=1\n    scale /= max(m, n) * tile_length - 3\n\n    grid_offsets = {}\n\n    if center is None:\n        center = np.zeros(dim)\n    else:\n        center = np.asarray(center)\n\n    paddims = dim - 2\n    if paddims < 0:\n        raise ValueError(\"layout must have at least two dimensions\")\n\n    if len(center) != dim:\n        raise ValueError(\"length of center coordinates must match dimension of layout\")\n\n    def _xy_coords(i, j, u, k):\n        # row, col, shore, shore index\n\n        # first get the coordinatiates within the tile\n        if k < tile_center:\n            p = k\n        else:\n            p = k + 1\n\n        if u:\n            xy = np.array([tile_center, -1 * p])\n        else:\n            xy = np.array([p, -1 * tile_center])\n\n        # next offset the corrdinates based on the which tile\n        if i > 0 or j > 0:\n            if (i, j) in grid_offsets:\n                xy += grid_offsets[(i, j)]\n            else:\n                off = np.array([j * tile_length, -1 * i * tile_length])\n                xy += off\n                grid_offsets[(i, j)] = off\n\n        # convention for Chimera-lattice pictures is to invert the y-axis\n        return np.hstack((xy * scale, np.zeros(paddims))) + center\n\n    return _xy_coords", "response": "Generates a function that converts a Chimera index to x y coordinates for a plot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef draw_chimera_embedding(G, *args, **kwargs):\n    draw_embedding(G, chimera_layout(G), *args, **kwargs)", "response": "Draws an embedding onto the Chimera graph G according to layout."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw the given graph G with highlighted faults according to layout.", "response": "def draw_chimera_yield(G, **kwargs):\n    \"\"\"Draws the given graph G with highlighted faults, according to layout.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph to be parsed for faults\n\n    unused_color : tuple or color string (optional, default (0.9,0.9,0.9,1.0))\n        The color to use for nodes and edges of G which are not faults.\n        If unused_color is None, these nodes and edges will not be shown at all.\n\n    fault_color : tuple or color string (optional, default (1.0,0.0,0.0,1.0))\n        A color to represent nodes absent from the graph G. Colors should be\n        length-4 tuples of floats between 0 and 1 inclusive.\n\n    fault_shape : string, optional (default='x')\n        The shape of the fault nodes. Specification is as matplotlib.scatter\n        marker, one of 'so^>v<dph8'.\n\n    fault_style : string, optional (default='dashed')\n        Edge fault line style (solid|dashed|dotted,dashdot)\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n    \"\"\"\n    try:\n        assert(G.graph[\"family\"] == \"chimera\")\n        m = G.graph[\"columns\"]\n        n = G.graph[\"rows\"]\n        t = G.graph[\"tile\"]\n        coordinates = G.graph[\"labels\"] == \"coordinate\"\n    except:\n        raise ValueError(\"Target chimera graph needs to have columns, rows, \\\n        tile, and label attributes to be able to identify faulty qubits.\")\n\n    perfect_graph = chimera_graph(m,n,t, coordinates=coordinates)\n\n    draw_yield(G, chimera_layout(perfect_graph), perfect_graph, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an approximate maximum clique. A clique in an undirected graph G = (V, E) is a subset of the vertex set `C \\subseteq V` such that for every two vertices in C there exists an edge connecting the two. This is equivalent to saying that the subgraph induced by C is complete (in some cases, the term clique may also refer to the subgraph).A maximum clique is a clique of the largest possible size in a given graph. This function works by finding the maximum independent set of the compliment graph of the given graph G which is equivalent to finding maximum clique. It defines a QUBO with ground states corresponding to a maximum weighted independent set and uses the sampler to sample from it. Parameters ---------- G : NetworkX graph The graph on which to find a maximum clique. sampler A binary quadratic model sampler. A sampler is a process that samples from low energy states in models defined by an Ising equation or a Quadratic Unconstrained Binary Optimization Problem (QUBO). A sampler is expected to have a 'sample_qubo' and 'sample_ising' method. A sampler is expected to return an iterable of samples, in order of increasing energy. If no sampler is provided, one must be provided using the `set_default_sampler` function. lagrange : optional (default 2) Lagrange parameter to weight constraints (no edges within set) versus objective (largest set possible). sampler_args Additional keyword parameters are passed to the sampler. Returns ------- clique_nodes : list List of nodes that form a maximum clique, as determined by the given sampler. Notes ----- Samplers by their nature may not return the optimal solution. This function does not attempt to confirm the quality of the returned sample. References ---------- `Maximum Clique on Wikipedia <https://en.wikipedia.org/wiki/Maximum_clique(graph_theory)>`_ `Independent Set on Wikipedia <https://en.wikipedia.org/wiki/Independent_set_(graph_theory)>`_ `QUBO on Wikipedia <https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization>`_ .. [AL] Lucas, A. (2014). Ising formulations of many NP problems. Frontiers in Physics, Volume 2, Article 5.", "response": "def maximum_clique(G, sampler=None, lagrange=2.0, **sampler_args):\n    \"\"\"\n    Returns an approximate maximum clique.\n    A clique in an undirected graph G = (V, E) is a subset of the vertex set\n    `C \\subseteq V` such that for every two vertices in C there exists an edge\n    connecting the two. This is equivalent to saying that the subgraph\n    induced by C is complete (in some cases, the term clique may also refer\n    to the subgraph).A maximum clique is a clique of the largest\n    possible size in a given graph.\n\n    This function works by finding the maximum independent set of the compliment\n    graph of the given graph G which is equivalent to finding maximum clique.\n    It defines a QUBO with ground states corresponding\n    to a maximum weighted independent set and uses the sampler to sample from it.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to find a maximum clique.\n\n    sampler\n        A binary quadratic model sampler. A sampler is a process that\n        samples from low energy states in models defined by an Ising\n        equation or a Quadratic Unconstrained Binary Optimization\n        Problem (QUBO). A sampler is expected to have a 'sample_qubo'\n        and 'sample_ising' method. A sampler is expected to return an\n        iterable of samples, in order of increasing energy. If no\n        sampler is provided, one must be provided using the\n        `set_default_sampler` function.\n\n    lagrange : optional (default 2)\n        Lagrange parameter to weight constraints (no edges within set)\n        versus objective (largest set possible).\n\n    sampler_args\n        Additional keyword parameters are passed to the sampler.\n\n    Returns\n    -------\n    clique_nodes : list\n       List of nodes that form a maximum clique, as\n       determined by the given sampler.\n\n    Notes\n    -----\n    Samplers by their nature may not return the optimal solution. This\n    function does not attempt to confirm the quality of the returned\n    sample.\n\n    References\n    ----------\n\n    `Maximum Clique on Wikipedia <https://en.wikipedia.org/wiki/Maximum_clique(graph_theory)>`_\n\n    `Independent Set on Wikipedia <https://en.wikipedia.org/wiki/Independent_set_(graph_theory)>`_\n\n    `QUBO on Wikipedia <https://en.wikipedia.org/wiki/Quadratic_unconstrained_binary_optimization>`_\n\n    .. [AL] Lucas, A. (2014). Ising formulations of many NP problems.\n       Frontiers in Physics, Volume 2, Article 5.\n    \"\"\"\n    if G is None:\n        raise ValueError(\"Expected NetworkX graph!\")\n\n    # finding the maximum clique in a graph is equivalent to finding\n    # the independent set in the complementary graph\n    complement_G = nx.complement(G)\n    return dnx.maximum_independent_set(complement_G, sampler, lagrange, **sampler_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a Chimera graph with the given parameters.", "response": "def chimera_graph(m, n=None, t=None, create_using=None, node_list=None, edge_list=None, data=True, coordinates=False):\n    \"\"\"Creates a Chimera lattice of size (m, n, t).\n\n    A Chimera lattice is an m-by-n grid of Chimera tiles. Each Chimera\n    tile is itself a bipartite graph with shores of size t. The\n    connection in a Chimera lattice can be expressed using a node-indexing\n    notation (i,j,u,k) for each node.\n\n    * (i,j) indexes the (row, column) of the Chimera tile. i must be\n      between 0 and m-1, inclusive, and j must be between 0 and\n      n-1, inclusive.\n    * u=0 indicates the left-hand nodes in the tile, and u=1 indicates\n      the right-hand nodes.\n    * k=0,1,...,t-1 indexes nodes within either the left- or\n      right-hand shores of a tile.\n\n    In this notation, two nodes (i, j, u, k) and (i', j', u', k') are\n    neighbors if and only if:\n\n        (i = i' AND j = j' AND u != u') OR\n        (i = i' +/- 1 AND j = j' AND u = 0 AND u' = 0 AND k = k') OR\n        (i = i' AND j = j' +/- 1 AND u = 1 AND u' = 1 AND k = k')\n\n    The first of the three terms of the disjunction gives the\n    bipartite connections  within the tile. The second and third terms\n    give the vertical and horizontal connections between blocks\n    respectively.\n\n    Node (i, j, u, k) is labeled by:\n\n        label = i * n * 2 * t + j * 2 * t + u * t + k\n\n    Parameters\n    ----------\n    m : int\n        Number of rows in the Chimera lattice.\n    n : int (optional, default m)\n        Number of columns in the Chimera lattice.\n    t : int (optional, default 4)\n        Size of the shore within each Chimera tile.\n    create_using : Graph (optional, default None)\n        If provided, this graph is cleared of nodes and edges and filled\n        with the new graph. Usually used to set the type of the graph.\n    node_list : iterable (optional, default None)\n        Iterable of nodes in the graph. If None, calculated\n        from (m, n, t). Note that this list is used to remove nodes,\n        so any nodes specified not in `range(m * n * 2 * t)` are not added.\n    edge_list : iterable (optional, default None)\n        Iterable of edges in the graph. If None, edges are\n        generated as described above. The nodes in each edge must be\n        integer-labeled in `range(m * n * t * 2)`.\n    data : bool (optional, default True)\n        If True, each node has a\n        `chimera_index attribute`. The attribute is a 4-tuple Chimera index\n        as defined above.\n    coordinates : bool (optional, default False)\n        If True, node labels are 4-tuples, equivalent to the chimera_index\n        attribute as above.  In this case, the `data` parameter controls the\n        existence of a `linear_index attribute`, which is an int\n\n    Returns\n    -------\n    G : NetworkX Graph\n        An (m, n, t) Chimera lattice. Nodes are labeled by integers.\n\n    Examples\n    ========\n    >>> G = dnx.chimera_graph(1, 1, 2)  # a single Chimera tile\n    >>> len(G)\n    4\n    >>> list(G.nodes())\n    [0, 1, 2, 3]\n    >>> list(G.nodes(data=True))  # doctest: +SKIP\n    [(0, {'chimera_index': (0, 0, 0, 0)}),\n     (1, {'chimera_index': (0, 0, 0, 1)}),\n     (2, {'chimera_index': (0, 0, 1, 0)}),\n     (3, {'chimera_index': (0, 0, 1, 1)})]\n    >>> list(G.edges())\n    [(0, 2), (0, 3), (1, 2), (1, 3)]\n\n    \"\"\"\n    m = int(m)\n    if n is None:\n        n = m\n    else:\n        n = int(n)\n    if t is None:\n        t = 4\n    else:\n        t = int(t)\n\n    G = nx.empty_graph(0, create_using)\n\n    G.name = \"chimera_graph(%s, %s, %s)\" % (m, n, t)\n\n    construction = ((\"family\", \"chimera\"), (\"rows\", m), (\"columns\", n),\n                    (\"tile\", t), (\"data\", data),\n                    (\"labels\", \"coordinate\" if coordinates else \"int\"))\n\n    G.graph.update(construction)\n\n    max_size = m * n * 2 * t  # max number of nodes G can have\n\n    if edge_list is None:\n        if coordinates:\n            # tile edges\n            G.add_edges_from(((i, j, 0, k0), (i, j, 1, k1))\n                             for i in range(n)\n                             for j in range(m)\n                             for k0 in range(t)\n                             for k1 in range(t))\n\n            # horizontal edges\n            G.add_edges_from(((i, j, 1, k), (i, j+1, 1, k))\n                             for i in range(m)\n                             for j in range(n-1)\n                             for k in range(t))\n            # vertical edges\n            G.add_edges_from(((i, j, 0, k), (i+1, j, 0, k))\n                             for i in range(m-1)\n                             for j in range(n)\n                             for k in range(t))\n        else:\n            hoff = 2 * t\n            voff = n * hoff\n            mi = m * voff\n            ni = n * hoff\n\n            # tile edges\n            G.add_edges_from((k0, k1)\n                             for i in range(0, ni, hoff)\n                             for j in range(i, mi, voff)\n                             for k0 in range(j, j + t)\n                             for k1 in range(j + t, j + 2 * t))\n            # horizontal edges\n            G.add_edges_from((k, k + hoff)\n                             for i in range(t, 2 * t)\n                             for j in range(i, ni - hoff, hoff)\n                             for k in range(j, mi, voff))\n            # vertical edges\n            G.add_edges_from((k, k + voff)\n                             for i in range(t)\n                             for j in range(i, ni, hoff)\n                             for k in range(j, mi - voff, voff))\n    else:\n        G.add_edges_from(edge_list)\n\n    if node_list is not None:\n        nodes = set(node_list)\n        G.remove_nodes_from(set(G) - nodes)\n        G.add_nodes_from(nodes)  # for singleton nodes\n\n    if data:\n        if coordinates:\n            def checkadd(v, q):\n                if q in G:\n                    G.node[q]['linear_index'] = v\n        else:\n            def checkadd(v, q):\n                if v in G:\n                    G.node[v]['chimera_index'] = q\n\n        v = 0\n        for i in range(m):\n            for j in range(n):\n                for u in range(2):\n                    for k in range(t):\n                        checkadd(v, (i, j, u, k))\n                        v += 1\n\n    return G"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to determine the Chimera indices of the nodes in a Chimera graph.", "response": "def find_chimera_indices(G):\n    \"\"\"Attempts to determine the Chimera indices of the nodes in graph G.\n\n    See the `chimera_graph()` function for a definition of a Chimera graph and Chimera\n    indices.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        Should be a single-tile Chimera graph.\n\n    Returns\n    -------\n    chimera_indices : dict\n        A dict of the form {node: (i, j, u, k), ...} where (i, j, u, k)\n        is a 4-tuple of integer Chimera indices.\n\n    Examples\n    --------\n    >>> G = dnx.chimera_graph(1, 1, 4)\n    >>> chimera_indices = dnx.find_chimera_indices(G)\n\n    >>> G = nx.Graph()\n    >>> G.add_edges_from([(0, 2), (1, 2), (1, 3), (0, 3)])\n    >>> chimera_indices = dnx.find_chimera_indices(G)\n    >>> nx.set_node_attributes(G, chimera_indices, 'chimera_index')\n\n    \"\"\"\n\n    # if the nodes are orderable, we want the lowest-order one.\n    try:\n        nlist = sorted(G.nodes)\n    except TypeError:\n        nlist = G.nodes()\n\n    n_nodes = len(nlist)\n\n    # create the object that will store the indices\n    chimera_indices = {}\n\n    # ok, let's first check for the simple cases\n    if n_nodes == 0:\n        return chimera_indices\n    elif n_nodes == 1:\n        raise DWaveNetworkXException(\n            'Singleton graphs are not Chimera-structured')\n    elif n_nodes == 2:\n        return {nlist[0]: (0, 0, 0, 0), nlist[1]: (0, 0, 1, 0)}\n\n    # next, let's get the bicoloring of the graph; this raises an exception if the graph is\n    # not bipartite\n    coloring = color(G)\n\n    # we want the color of the node to be the u term in the Chimera-index, so we want the\n    # first node in nlist to be color 0\n    if coloring[nlist[0]] == 1:\n        coloring = {v: 1 - coloring[v] for v in coloring}\n\n    # we also want the diameter of the graph\n    # claim: diameter(G) == m + n for |G| > 2\n    dia = diameter(G)\n\n    # we have already handled the |G| <= 2 case, so we know, for diameter == 2, that the Chimera\n    # graph is a single tile\n    if dia == 2:\n        shore_indices = [0, 0]\n\n        for v in nlist:\n            u = coloring[v]\n            chimera_indices[v] = (0, 0, u, shore_indices[u])\n            shore_indices[u] += 1\n\n        return chimera_indices\n\n    # NB: max degree == shore size <==> one tile\n\n    raise Exception('not yet implemented for Chimera graphs with more than one tile')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chimera_elimination_order(m, n=None, t=None):\n    if n is None:\n        n = m\n\n    if t is None:\n        t = 4\n\n    index_flip = m > n\n    if index_flip:\n        m, n = n, m\n\n    def chimeraI(m0, n0, k0, l0):\n        if index_flip:\n            return m*2*t*n0 + 2*t*m0 + t*(1-k0) + l0\n        else:\n            return n*2*t*m0 + 2*t*n0 + t*k0 + l0\n\n    order = []\n\n    for n_i in range(n):\n        for t_i in range(t):\n            for m_i in range(m):\n                order.append(chimeraI(m_i, n_i, 0, t_i))\n\n    for n_i in range(n):\n        for m_i in range(m):\n            for t_i in range(t):\n                order.append(chimeraI(m_i, n_i, 1, t_i))\n\n    return order", "response": "This function provides a variable elimination order for a Chimera graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the chimera_index q into an integer representation of the linear_index node.", "response": "def int(self, q):\n        \"\"\"\n        Converts the chimera_index `q` into an linear_index\n\n        Parameters\n        ----------\n        q : tuple\n            The chimera_index node label    \n\n        Returns\n        -------\n        r : int\n            The linear_index node label corresponding to q            \n        \"\"\"\n\n        i, j, u, k = q\n        m, n, t = self.args\n        return ((n*i + j)*2 + u)*t + k"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tuple(self, r):\n\n        m, n, t = self.args\n        r, k = divmod(r, t)\n        r, u = divmod(r, 2)\n        i, j = divmod(r, n)\n        return i, j, u, k", "response": "Converts the linear_index q into an chimera_index_n_u_k_n_t tuple"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a sequence of chimera_index node labels into a linear_index node labels preserving order", "response": "def ints(self, qlist):\n        \"\"\"\n        Converts a sequence of chimera_index node labels into\n        linear_index node labels, preserving order\n\n        Parameters\n        ----------\n        qlist : sequence of ints\n            The chimera_index node labels\n\n        Returns\n        -------\n        rlist : iterable of tuples\n            The linear_lindex node lables corresponding to qlist\n        \"\"\"\n\n        m, n, t = self.args\n        return (((n*i + j)*2 + u)*t + k for (i, j, u, k) in qlist)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a sequence of linear_index node labels into a sequence of chimera_index node labels preserving order", "response": "def tuples(self, rlist):\n        \"\"\"\n        Converts a sequence of linear_index node labels into\n        chimera_index node labels, preserving order\n\n        Parameters\n        ----------\n        rlist : sequence of tuples\n            The linear_index node labels\n\n        Returns\n        -------\n        qlist : iterable of ints\n            The chimera_lindex node lables corresponding to rlist\n        \"\"\"\n\n        m, n, t = self.args\n        for r in rlist:\n            r, k = divmod(r, t)\n            r, u = divmod(r, 2)\n            i, j = divmod(r, n)\n            yield i, j, u, k"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef structural_imbalance(S, sampler=None, **sampler_args):\n    h, J = structural_imbalance_ising(S)\n\n    # use the sampler to find low energy states\n    response = sampler.sample_ising(h, J, **sampler_args)\n\n    # we want the lowest energy sample\n    sample = next(iter(response))\n\n    # spins determine the color\n    colors = {v: (spin + 1) // 2 for v, spin in iteritems(sample)}\n\n    # frustrated edges are the ones that are violated\n    frustrated_edges = {}\n    for u, v, data in S.edges(data=True):\n        sign = data['sign']\n\n        if sign > 0 and colors[u] != colors[v]:\n            frustrated_edges[(u, v)] = data\n        elif sign < 0 and colors[u] == colors[v]:\n            frustrated_edges[(u, v)] = data\n        # else: not frustrated or sign == 0, no relation to violate\n\n    return frustrated_edges, colors", "response": "Returns an approximate set of frustrated edges and a bicoloring."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the Ising problem to calculate the structural imbalance of a signed social network.", "response": "def structural_imbalance_ising(S):\n    \"\"\"Construct the Ising problem to calculate the structural imbalance of a signed social network.\n\n    A signed social network graph is a graph whose signed edges\n    represent friendly/hostile interactions between nodes. A\n    signed social network is considered balanced if it can be cleanly\n    divided into two factions, where all relations within a faction are\n    friendly, and all relations between factions are hostile. The measure\n    of imbalance or frustration is the minimum number of edges that\n    violate this rule.\n\n    Parameters\n    ----------\n    S : NetworkX graph\n        A social graph on which each edge has a 'sign' attribute with a numeric value.\n\n    Returns\n    -------\n    h : dict\n        The linear biases of the Ising problem. Each variable in the Ising problem represent\n        a node in the signed social network. The solution that minimized the Ising problem\n        will assign each variable a value, either -1 or 1. This bi-coloring defines the factions.\n\n    J : dict\n        The quadratic biases of the Ising problem.\n\n    Raises\n    ------\n    ValueError\n        If any edge does not have a 'sign' attribute.\n\n    Examples\n    --------\n    >>> import dimod\n    >>> from dwave_networkx.algorithms.social import structural_imbalance_ising\n    ...\n    >>> S = nx.Graph()\n    >>> S.add_edge('Alice', 'Bob', sign=1)  # Alice and Bob are friendly\n    >>> S.add_edge('Alice', 'Eve', sign=-1)  # Alice and Eve are hostile\n    >>> S.add_edge('Bob', 'Eve', sign=-1)  # Bob and Eve are hostile\n    ...\n    >>> h, J = structural_imbalance_ising(S)\n    >>> h  # doctest: +SKIP\n    {'Alice': 0.0, 'Bob': 0.0, 'Eve': 0.0}\n    >>> J  # doctest: +SKIP\n    {('Alice', 'Bob'): -1.0, ('Alice', 'Eve'): 1.0, ('Bob', 'Eve'): 1.0}\n\n    \"\"\"\n    h = {v: 0.0 for v in S}\n    J = {}\n    for u, v, data in S.edges(data=True):\n        try:\n            J[(u, v)] = -1. * data['sign']\n        except KeyError:\n            raise ValueError((\"graph should be a signed social graph,\"\n                              \"each edge should have a 'sign' attr\"))\n\n    return h, J"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine whether a node n in graph G is simplicial.", "response": "def is_simplicial(G, n):\n    \"\"\"Determines whether a node n in G is simplicial.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to check whether node n is simplicial.\n    n : node\n        A node in graph G.\n\n    Returns\n    -------\n    is_simplicial : bool\n        True if its neighbors form a clique.\n\n    Examples\n    --------\n    This example checks whether node 0 is simplicial for two graphs: G, a\n    single Chimera unit cell, which is bipartite, and K_5, the :math:`K_5`\n    complete graph.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> G = dnx.chimera_graph(1, 1, 4)\n    >>> K_5 = nx.complete_graph(5)\n    >>> dnx.is_simplicial(G, 0)\n    False\n    >>> dnx.is_simplicial(K_5, 0)\n    True\n\n    \"\"\"\n    return all(u in G[v] for u, v in itertools.combinations(G[n], 2))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether a node n in graph G is almost simplicial.", "response": "def is_almost_simplicial(G, n):\n    \"\"\"Determines whether a node n in G is almost simplicial.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to check whether node n is almost simplicial.\n    n : node\n        A node in graph G.\n\n    Returns\n    -------\n    is_almost_simplicial : bool\n        True if all but one of its neighbors induce a clique\n\n    Examples\n    --------\n    This example checks whether node 0 is simplicial or almost simplicial for\n    a :math:`K_5` complete graph with one edge removed.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> K_5 = nx.complete_graph(5)\n    >>> K_5.remove_edge(1,3)\n    >>> dnx.is_simplicial(K_5, 0)\n    False\n    >>> dnx.is_almost_simplicial(K_5, 0)\n    True\n\n    \"\"\"\n    for w in G[n]:\n        if all(u in G[v] for u, v in itertools.combinations(G[n], 2) if u != w and v != w):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef minor_min_width(G):\n    # we need only deal with the adjacency structure of G. We will also\n    # be manipulating it directly so let's go ahead and make a new one\n    adj = {v: set(G[v]) for v in G}\n\n    lb = 0  # lower bound on treewidth\n    while len(adj) > 1:\n\n        # get the node with the smallest degree\n        v = min(adj, key=lambda v: len(adj[v]))\n\n        # find the vertex u such that the degree of u is minimal in the neighborhood of v\n        neighbors = adj[v]\n\n        if not neighbors:\n            # if v is a singleton, then we can just delete it\n            del adj[v]\n            continue\n\n        def neighborhood_degree(u):\n            Gu = adj[u]\n            return sum(w in Gu for w in neighbors)\n\n        u = min(neighbors, key=neighborhood_degree)\n\n        # update the lower bound\n        new_lb = len(adj[v])\n        if new_lb > lb:\n            lb = new_lb\n\n        # contract the edge between u, v\n        adj[v] = adj[v].union(n for n in adj[u] if n != v)\n        for n in adj[v]:\n            adj[n].add(v)\n        for n in adj[u]:\n            adj[n].discard(u)\n        del adj[u]\n\n    return lb", "response": "Computes a lower bound for the treewidth of graph G."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute an upper bound on the treewidth of graph G based on the min-fill heuristic for the elimination ordering. Parameters ---------- G : NetworkX graph The graph on which to compute an upper bound for the treewidth. Returns ------- treewidth_upper_bound : int An upper bound on the treewidth of the graph G. order : list An elimination order that induces the treewidth. Examples -------- This example computes an upper bound for the treewidth of the :math:`K_4` complete graph. >>> import dwave_networkx as dnx >>> import networkx as nx >>> K_4 = nx.complete_graph(4) >>> tw, order = dnx.min_fill_heuristic(K_4) References ---------- Based on the algorithm presented in [GD]_", "response": "def min_fill_heuristic(G):\n    \"\"\"Computes an upper bound on the treewidth of graph G based on\n    the min-fill heuristic for the elimination ordering.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to compute an upper bound for the treewidth.\n\n    Returns\n    -------\n    treewidth_upper_bound : int\n        An upper bound on the treewidth of the graph G.\n\n    order : list\n        An elimination order that induces the treewidth.\n\n    Examples\n    --------\n    This example computes an upper bound for the treewidth of the :math:`K_4`\n    complete graph.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> K_4 = nx.complete_graph(4)\n    >>> tw, order = dnx.min_fill_heuristic(K_4)\n\n    References\n    ----------\n    Based on the algorithm presented in [GD]_\n\n    \"\"\"\n    # we need only deal with the adjacency structure of G. We will also\n    # be manipulating it directly so let's go ahead and make a new one\n    adj = {v: set(G[v]) for v in G}\n\n    num_nodes = len(adj)\n\n    # preallocate the return values\n    order = [0] * num_nodes\n    upper_bound = 0\n\n    for i in range(num_nodes):\n        # get the node that adds the fewest number of edges when eliminated from the graph\n        v = min(adj, key=lambda x: _min_fill_needed_edges(adj, x))\n\n        # if the number of neighbours of v is higher than upper_bound, update\n        dv = len(adj[v])\n        if dv > upper_bound:\n            upper_bound = dv\n\n        # make v simplicial by making its neighborhood a clique then remove the\n        # node\n        _elim_adj(adj, v)\n        order[i] = v\n\n    return upper_bound, order"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute an upper bound on the treewidth of graph G based on the min-width heuristic for the elimination ordering. Parameters ---------- G : NetworkX graph The graph on which to compute an upper bound for the treewidth. Returns ------- treewidth_upper_bound : int An upper bound on the treewidth of the graph G. order : list An elimination order that induces the treewidth. Examples -------- This example computes an upper bound for the treewidth of the :math:`K_4` complete graph. >>> import dwave_networkx as dnx >>> import networkx as nx >>> K_4 = nx.complete_graph(4) >>> tw, order = dnx.min_width_heuristic(K_4) References ---------- Based on the algorithm presented in [GD]_", "response": "def min_width_heuristic(G):\n    \"\"\"Computes an upper bound on the treewidth of graph G based on\n    the min-width heuristic for the elimination ordering.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to compute an upper bound for the treewidth.\n\n    Returns\n    -------\n    treewidth_upper_bound : int\n        An upper bound on the treewidth of the graph G.\n\n    order : list\n        An elimination order that induces the treewidth.\n\n    Examples\n    --------\n    This example computes an upper bound for the treewidth of the :math:`K_4`\n    complete graph.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> K_4 = nx.complete_graph(4)\n    >>> tw, order = dnx.min_width_heuristic(K_4)\n\n    References\n    ----------\n    Based on the algorithm presented in [GD]_\n\n    \"\"\"\n    # we need only deal with the adjacency structure of G. We will also\n    # be manipulating it directly so let's go ahead and make a new one\n    adj = {v: set(G[v]) for v in G}\n\n    num_nodes = len(adj)\n\n    # preallocate the return values\n    order = [0] * num_nodes\n    upper_bound = 0\n\n    for i in range(num_nodes):\n        # get the node with the smallest degree. We add random() which picks a value\n        # in the range [0., 1.). This is ok because the lens are all integers. By\n        # adding a small random value, we randomize which node is chosen without affecting\n        # correctness.\n        v = min(adj, key=lambda u: len(adj[u]) + random())\n\n        # if the number of neighbours of v is higher than upper_bound, update\n        dv = len(adj[v])\n        if dv > upper_bound:\n            upper_bound = dv\n\n        # make v simplicial by making its neighborhood a clique then remove the\n        # node\n        _elim_adj(adj, v)\n        order[i] = v\n\n    return upper_bound, order"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute an upper bound on the treewidth of graph G based on the max-cardinality heuristic for the elimination ordering. Parameters ---------- G : NetworkX graph The graph on which to compute an upper bound for the treewidth. inplace : bool If True, G will be made an empty graph in the process of running the function, otherwise the function uses a copy of G. Returns ------- treewidth_upper_bound : int An upper bound on the treewidth of the graph G. order : list An elimination order that induces the treewidth. Examples -------- This example computes an upper bound for the treewidth of the :math:`K_4` complete graph. >>> import dwave_networkx as dnx >>> import networkx as nx >>> K_4 = nx.complete_graph(4) >>> dnx.max_cardinality_heuristic(K_4) (3, [3, 1, 0, 2]) References ---------- Based on the algorithm presented in [GD]_", "response": "def max_cardinality_heuristic(G):\n    \"\"\"Computes an upper bound on the treewidth of graph G based on\n    the max-cardinality heuristic for the elimination ordering.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to compute an upper bound for the treewidth.\n\n    inplace : bool\n        If True, G will be made an empty graph in the process of\n        running the function, otherwise the function uses a copy\n        of G.\n\n    Returns\n    -------\n    treewidth_upper_bound : int\n        An upper bound on the treewidth of the graph G.\n\n    order : list\n        An elimination order that induces the treewidth.\n\n    Examples\n    --------\n    This example computes an upper bound for the treewidth of the :math:`K_4`\n    complete graph.\n\n    >>> import dwave_networkx as dnx\n    >>> import networkx as nx\n    >>> K_4 = nx.complete_graph(4)\n    >>> dnx.max_cardinality_heuristic(K_4)\n    (3, [3, 1, 0, 2])\n\n    References\n    ----------\n    Based on the algorithm presented in [GD]_\n\n    \"\"\"\n    # we need only deal with the adjacency structure of G. We will also\n    # be manipulating it directly so let's go ahead and make a new one\n    adj = {v: set(G[v]) for v in G}\n\n    num_nodes = len(adj)\n\n    # preallocate the return values\n    order = [0] * num_nodes\n    upper_bound = 0\n\n    # we will need to track the nodes and how many labelled neighbors\n    # each node has\n    labelled_neighbors = {v: 0 for v in adj}\n\n    # working backwards\n    for i in range(num_nodes):\n        # pick the node with the most labelled neighbors\n        v = max(labelled_neighbors, key=lambda u: labelled_neighbors[u] + random())\n        del labelled_neighbors[v]\n\n        # increment all of its neighbors\n        for u in adj[v]:\n            if u in labelled_neighbors:\n                labelled_neighbors[u] += 1\n\n        order[-(i + 1)] = v\n\n    for v in order:\n        # if the number of neighbours of v is higher than upper_bound, update\n        dv = len(adj[v])\n        if dv > upper_bound:\n            upper_bound = dv\n\n        # make v simplicial by making its neighborhood a clique then remove the node\n        # add v to order\n        _elim_adj(adj, v)\n\n    return upper_bound, order"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _elim_adj(adj, n):\n    neighbors = adj[n]\n    new_edges = set()\n    for u, v in itertools.combinations(neighbors, 2):\n        if v not in adj[u]:\n            adj[u].add(v)\n            adj[v].add(u)\n            new_edges.add((u, v))\n            new_edges.add((v, u))\n    for v in neighbors:\n        adj[v].discard(n)\n    del adj[n]\n    return new_edges", "response": "eliminates a variable acting on the adj matrix of G returning a set of edges that were added."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef elimination_order_width(G, order):\n    # we need only deal with the adjacency structure of G. We will also\n    # be manipulating it directly so let's go ahead and make a new one\n    adj = {v: set(G[v]) for v in G}\n\n    treewidth = 0\n\n    for v in order:\n\n        # get the degree of the eliminated variable\n        try:\n            dv = len(adj[v])\n        except KeyError:\n            raise ValueError('{} is in order but not in G'.format(v))\n\n        # the treewidth is the max of the current treewidth and the degree\n        if dv > treewidth:\n            treewidth = dv\n\n        # eliminate v by making it simplicial (acts on adj in place)\n        _elim_adj(adj, v)\n\n    # if adj is not empty, then order did not include all of the nodes in G.\n    if adj:\n        raise ValueError('not all nodes in G were in order')\n\n    return treewidth", "response": "Calculates the width of the tree decomposition induced by a given list of variables in order."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef treewidth_branch_and_bound(G, elimination_order=None, treewidth_upperbound=None):\n    # empty graphs have treewidth 0 and the nodes can be eliminated in\n    # any order\n    if not any(G[v] for v in G):\n        return 0, list(G)\n\n    # variable names are chosen to match the paper\n\n    # our order will be stored in vector x, named to be consistent with\n    # the paper\n    x = []  # the partial order\n\n    f = minor_min_width(G)  # our current lower bound guess, f(s) in the paper\n    g = 0  # g(s) in the paper\n\n    # we need the best current update we can find.\n    ub, order = min_fill_heuristic(G)\n\n    # if the user has provided an upperbound or an elimination order, check those against\n    # our current best guess\n    if elimination_order is not None:\n        upperbound = elimination_order_width(G, elimination_order)\n        if upperbound <= ub:\n            ub, order = upperbound, elimination_order\n\n    if treewidth_upperbound is not None and treewidth_upperbound < ub:\n        # in this case the order might never be found\n        ub, order = treewidth_upperbound, []\n\n    # best found encodes the ub and the order\n    best_found = ub, order\n\n    # if our upper bound is the same as f, then we are done! Otherwise begin the\n    # algorithm\n    assert f <= ub, \"Logic error\"\n    if f < ub:\n        # we need only deal with the adjacency structure of G. We will also\n        # be manipulating it directly so let's go ahead and make a new one\n        adj = {v: set(G[v]) for v in G}\n\n        best_found = _branch_and_bound(adj, x, g, f, best_found)\n\n    return best_found", "response": "This function computes the treewidth of a complete graph G and a corresponding perfect elimination ordering."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _branch_and_bound(adj, x, g, f, best_found, skipable=set(), theorem6p2=None):\n\n    # theorem6p2 checks for branches that can be pruned using Theorem 6.2\n    if theorem6p2 is None:\n        theorem6p2 = _theorem6p2()\n    prune6p2, explored6p2, finished6p2 = theorem6p2\n    # current6p2 is the list of prunable terms created during this instantiation of _branch_and_bound.\n    # These terms will only be use during this call and its successors,\n    # so they are removed before the function terminates.\n    current6p2 = list()\n\n    # theorem6p4 checks for branches that can be pruned using Theorem 6.4.\n    # These terms do not need to be passed to successive calls to _branch_and_bound,\n    # so they are simply created and deleted during this call.\n    prune6p4, explored6p4 = _theorem6p4()\n\n    # Note: theorem6p1 and theorem6p3 are a pruning strategies that are currently disabled\n    # # as they does not appear to be invoked regularly,\n    # and invoking it can require large memory allocations.\n    # This can be fixed in the future if there is evidence that it's useful.\n    # To add them in, define _branch_and_bound as follows:\n    # def _branch_and_bound(adj, x, g, f, best_found, skipable=set(), theorem6p1=None,\n    #                       theorem6p2=None, theorem6p3=None):\n\n    # if theorem6p1 is None:\n    #     theorem6p1 = _theorem6p1()\n    # prune6p1, explored6p1 = theorem6p1\n\n    # if theorem6p3 is None:\n    #     theorem6p3 = _theorem6p3()\n    # prune6p3, explored6p3 = theorem6p3\n\n    # we'll need to know our current upper bound in several places\n    ub, order = best_found\n\n    # ok, take care of the base case first\n    if len(adj) < 2:\n        # check if our current branch is better than the best we've already\n        # found and if so update our best solution accordingly.\n        if f < ub:\n            return (f, x + list(adj))\n        elif f == ub and not order:\n            return (f, x + list(adj))\n        else:\n            return best_found\n\n    # so we have not yet reached the base case\n    # Note: theorem 6.4 gives a heuristic for choosing order of n in adj.\n    # Quick_bb suggests using a min-fill or random order.\n    # We don't need to consider the neighbors of the last vertex eliminated\n    sorted_adj = sorted((n for n in adj if n not in skipable), key=lambda x: _min_fill_needed_edges(adj, x))\n    for n in sorted_adj:\n\n        g_s = max(g, len(adj[n]))\n\n        # according to Lemma 5.3, we can skip all of the neighbors of the last\n        # variable eliniated when choosing the next variable\n        # this does not get altered so we don't need a copy\n        next_skipable = adj[n]\n\n        if prune6p2(x, n, next_skipable):\n            continue\n\n        # update the state by eliminating n and adding it to the partial ordering\n        adj_s = {v: adj[v].copy() for v in adj}  # create a new object\n        edges_n = _elim_adj(adj_s, n)\n        x_s = x + [n]  # new partial ordering\n\n        # pruning (disabled):\n        # if prune6p1(x_s):\n        #     continue\n\n        if prune6p4(edges_n):\n            continue\n\n        # By Theorem 5.4, if any two vertices have ub + 1 common neighbors then\n        # we can add an edge between them\n        _theorem5p4(adj_s, ub)\n\n        # ok, let's update our values\n        f_s = max(g_s, minor_min_width(adj_s))\n\n        g_s, f_s, as_list = _graph_reduction(adj_s, x_s, g_s, f_s)\n\n        # pruning (disabled):\n        # if prune6p3(x, as_list, n):\n        #     continue\n\n        if f_s < ub:\n            best_found = _branch_and_bound(adj_s, x_s, g_s, f_s, best_found,\n                                           next_skipable, theorem6p2=theorem6p2)\n            # if theorem6p1, theorem6p3 are enabled, this should be called as:\n            # best_found = _branch_and_bound(adj_s, x_s, g_s, f_s, best_found,\n            #                                next_skipable, theorem6p1=theorem6p1,\n            #                                theorem6p2=theorem6p2,theorem6p3=theorem6p3)\n            ub, __ = best_found\n\n        # store some information for pruning (disabled):\n        # explored6p3(x, n, as_list)\n\n        prunable = explored6p2(x, n, next_skipable)\n        current6p2.append(prunable)\n\n        explored6p4(edges_n)\n\n    # store some information for pruning (disabled):\n    # explored6p1(x)\n\n    for prunable in current6p2:\n        finished6p2(prunable)\n\n    return best_found", "response": "Recursive branch and bound for computing treewidth of a subgraph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _theorem6p1():\n\n    pruning_set = set()\n\n    def _prune(x):\n        if len(x) <= 2:\n            return False\n        # this is faster than tuple(x[-3:])\n        key = (tuple(x[:-2]), x[-2], x[-1])\n        return key in pruning_set\n\n    def _explored(x):\n        if len(x) >= 3:\n            prunable = (tuple(x[:-2]), x[-1], x[-2])\n            pruning_set.add(prunable)\n\n    return _prune, _explored", "response": "See Theorem 6. 1 in paper."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsees Theorem 6. 2 in paper. Prunes ( x... a ) when ( x a has the same neighbour set in both graphs. Returns a function that returns True if theorem6p2 returns False otherwise.", "response": "def _theorem6p2():\n    \"\"\"See Theorem 6.2 in paper.\n    Prunes (x,...,a) when (x,a) is explored and a has the same neighbour set in both graphs.\n    \"\"\"\n    pruning_set2 = set()\n\n    def _prune2(x, a, nbrs_a):\n        frozen_nbrs_a = frozenset(nbrs_a)\n        for i in range(len(x)):\n            key = (tuple(x[0:i]), a, frozen_nbrs_a)\n            if key in pruning_set2:\n                return True\n        return False\n\n    def _explored2(x, a, nbrs_a):\n        prunable = (tuple(x), a, frozenset(nbrs_a))  # (s,a,N(a))\n        pruning_set2.add(prunable)\n        return prunable\n\n    def _finished2(prunable):\n        pruning_set2.remove(prunable)\n\n    return _prune2, _explored2, _finished2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nseeing Theorem 6. 3 in paper. Prunes ( s b when ( s a ) is explored and b when ( s b ) is explored and a ( almost ) simplicial in ( s a ) and b ( almost ) simplicial in ( s a ) and b ( almost ) simplicial in ( s b ).", "response": "def _theorem6p3():\n    \"\"\"See Theorem 6.3 in paper.\n    Prunes (s,b) when (s,a) is explored, b (almost) simplicial in (s,a), and a (almost) simplicial in (s,b)\n    \"\"\"\n    pruning_set3 = set()\n\n    def _prune3(x, as_list, b):\n        for a in as_list:\n            key = (tuple(x), a, b)  # (s,a,b) with (s,a) explored\n            if key in pruning_set3:\n                return True\n        return False\n\n    def _explored3(x, a, as_list):\n        for b in as_list:\n            prunable = (tuple(x), a, b)  # (s,a,b) with (s,a) explored\n            pruning_set3.add(prunable)\n\n    return _prune3, _explored3"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsees Theorem 6. 4 in paper.", "response": "def _theorem6p4():\n    \"\"\"See Theorem 6.4 in paper.\n    Let E(x) denote the edges added when eliminating x. (edges_x below).\n    Prunes (s,b) when (s,a) is explored and E(a) is a subset of E(b).\n    For this theorem we only record E(a) rather than (s,E(a))\n    because we only need to check for pruning in the same s context\n    (i.e the same level of recursion).\n    \"\"\"\n    pruning_set4 = list()\n\n    def _prune4(edges_b):\n        for edges_a in pruning_set4:\n            if edges_a.issubset(edges_b):\n                return True\n        return False\n\n    def _explored4(edges_a):\n        pruning_set4.append(edges_a)  # (s,E_a) with (s,a) explored\n\n    return _prune4, _explored4"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw a graph G according to layout.", "response": "def draw_qubit_graph(G, layout, linear_biases={}, quadratic_biases={},\n                     nodelist=None, edgelist=None, cmap=None, edge_cmap=None, vmin=None, vmax=None,\n                     edge_vmin=None, edge_vmax=None,\n                     **kwargs):\n    \"\"\"Draws graph G according to layout.\n\n    If `linear_biases` and/or `quadratic_biases` are provided, these\n    are visualized on the plot.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph to be drawn\n\n    layout : dict\n        A dict of coordinates associated with each node in G.  Should\n        be of the form {node: coordinate, ...}.  Coordinates will be\n        treated as vectors, and should all have the same length.\n\n    linear_biases : dict (optional, default {})\n        A dict of biases associated with each node in G. Should be of\n        form {node: bias, ...}. Each bias should be numeric.\n\n    quadratic_biases : dict (optional, default {})\n        A dict of biases associated with each edge in G. Should be of\n        form {edge: bias, ...}. Each bias should be numeric. Self-loop\n        edges (i.e., :math:`i=j`) are treated as linear biases.\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n\n    \"\"\"\n\n    if linear_biases or quadratic_biases:\n        # if linear biases and/or quadratic biases are provided, then color accordingly.\n\n        try:\n            import matplotlib.pyplot as plt\n            import matplotlib as mpl\n        except ImportError:\n            raise ImportError(\"Matplotlib and numpy required for draw_qubit_graph()\")\n\n        if nodelist is None:\n            nodelist = G.nodes()\n\n        if edgelist is None:\n            edgelist = G.edges()\n\n        if cmap is None:\n            cmap = plt.get_cmap('coolwarm')\n\n        if edge_cmap is None:\n            edge_cmap = plt.get_cmap('coolwarm')\n\n        # any edges or nodes with an unspecified bias default to 0\n        def edge_color(u, v):\n            c = 0.\n            if (u, v) in quadratic_biases:\n                c += quadratic_biases[(u, v)]\n            if (v, u) in quadratic_biases:\n                c += quadratic_biases[(v, u)]\n            return c\n\n        def node_color(v):\n            c = 0.\n            if v in linear_biases:\n                c += linear_biases[v]\n            if (v, v) in quadratic_biases:\n                c += quadratic_biases[(v, v)]\n            return c\n\n        node_color = [node_color(v) for v in nodelist]\n        edge_color = [edge_color(u, v) for u, v in edgelist]\n\n        kwargs['edge_color'] = edge_color\n        kwargs['node_color'] = node_color\n\n        # the range of the color map is shared for nodes/edges and is symmetric\n        # around 0.\n        vmag = max(max(abs(c) for c in node_color), max(abs(c) for c in edge_color))\n        if vmin is None:\n            vmin = -1 * vmag\n        if vmax is None:\n            vmax = vmag\n        if edge_vmin is None:\n            edge_vmin = -1 * vmag\n        if edge_vmax is None:\n            edge_vmax = vmag\n\n    draw(G, layout, nodelist=nodelist, edgelist=edgelist,\n         cmap=cmap, edge_cmap=edge_cmap, vmin=vmin, vmax=vmax, edge_vmin=edge_vmin,\n         edge_vmax=edge_vmax,\n         **kwargs)\n\n    # if the biases are provided, then add a legend explaining the color map\n    if linear_biases or quadratic_biases:\n        fig = plt.figure(1)\n        # cax = fig.add_axes([])\n        cax = fig.add_axes([.9, 0.2, 0.04, 0.6])  # left, bottom, width, height\n        mpl.colorbar.ColorbarBase(cax, cmap=cmap,\n                                  norm=mpl.colors.Normalize(vmin=-1 * vmag, vmax=vmag, clip=False),\n                                  orientation='vertical')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_embedding(G, layout, emb, embedded_graph=None, interaction_edges=None,\n                   chain_color=None, unused_color=(0.9,0.9,0.9,1.0), cmap=None,\n                   show_labels=False, **kwargs):\n    \"\"\"Draws an embedding onto the graph G, according to layout.\n\n    If interaction_edges is not None, then only display the couplers in that\n    list.  If embedded_graph is not None, the only display the couplers between\n    chains with intended couplings according to embedded_graph.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph to be drawn\n\n    layout : dict\n        A dict of coordinates associated with each node in G.  Should\n        be of the form {node: coordinate, ...}.  Coordinates will be\n        treated as vectors, and should all have the same length.\n\n    emb : dict\n        A dict of chains associated with each node in G.  Should be\n        of the form {node: chain, ...}.  Chains should be iterables\n        of qubit labels (qubits are nodes in G).\n\n    embedded_graph : NetworkX graph (optional, default None)\n        A graph which contains all keys of emb as nodes.  If specified,\n        edges of G will be considered interactions if and only if they\n        exist between two chains of emb if their keys are connected by\n        an edge in embedded_graph\n\n    interaction_edges : list (optional, default None)\n        A list of edges which will be used as interactions.\n\n    show_labels: boolean (optional, default False)\n        If show_labels is True, then each chain in emb is labelled with its key.\n\n    chain_color : dict (optional, default None)\n        A dict of colors associated with each key in emb.  Should be\n        of the form {node: rgba_color, ...}.  Colors should be length-4\n        tuples of floats between 0 and 1 inclusive. If chain_color is None,\n        each chain will be assigned a different color.\n\n    unused_color : tuple or color string (optional, default (0.9,0.9,0.9,1.0))\n        The color to use for nodes and edges of G which are not involved\n        in chains, and edges which are neither chain edges nor interactions.\n        If unused_color is None, these nodes and edges will not be shown at all.\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib as mpl\n    except ImportError:\n        raise ImportError(\"Matplotlib and numpy required for draw_chimera()\")\n\n    if nx.utils.is_string_like(unused_color):\n        from matplotlib.colors import colorConverter\n        alpha = kwargs.get('alpha',1.0)\n        unused_color = colorConverter.to_rgba(unused_color,alpha)\n\n    if chain_color is None:\n        import matplotlib.cm\n        n = max(1., len(emb) - 1.)\n        if kwargs.get(\"cmap\"):\n            color = matplotlib.cm.get_cmap(kwargs.get(\"cmap\"))\n        else:\n            color = distinguishable_color_map(int(n+1))\n        var_i = {v: i for i, v in enumerate(emb)}\n        chain_color = {v: color(i/n) for i, v in enumerate(emb)}\n\n    qlabel = {q: v for v, chain in iteritems(emb) for q in chain}\n    edgelist = []\n    edge_color = []\n    background_edgelist = []\n    background_edge_color = []\n\n    if interaction_edges is not None:\n        interactions = nx.Graph()\n        interactions.add_edges_from(interaction_edges)\n\n        def show(p, q, u, v): return interactions.has_edge(p, q)\n    elif embedded_graph is not None:\n        def show(p, q, u, v): return embedded_graph.has_edge(u, v)\n    else:\n        def show(p, q, u, v): return True\n\n    for (p, q) in G.edges():\n        u = qlabel.get(p)\n        v = qlabel.get(q)\n        if u is None or v is None:\n            ec = unused_color\n        elif u == v:\n            ec = chain_color.get(u)\n        elif show(p, q, u, v):\n            ec = (0, 0, 0, 1)\n        else:\n            ec = unused_color\n\n        if ec == unused_color:\n            background_edgelist.append((p, q))\n            background_edge_color.append(ec)\n        elif ec is not None:\n            edgelist.append((p, q))\n            edge_color.append(ec)\n\n    nodelist = []\n    node_color = []\n    for p in G.nodes():\n        u = qlabel.get(p)\n        if u is None:\n            pc = unused_color\n        else:\n            pc = chain_color.get(u)\n\n        if pc is not None:\n            nodelist.append(p)\n            node_color.append(pc)\n\n    labels = {}\n    if show_labels:\n        for v in emb.keys():\n            c = emb[v]\n            labels[list(c)[0]] = str(v)\n\n    # draw the background (unused) graph first\n    if unused_color is not None:\n        draw(G, layout, nodelist=nodelist, edgelist=background_edgelist,\n             node_color=node_color, edge_color=background_edge_color,\n             **kwargs)\n\n    draw(G, layout, nodelist=nodelist, edgelist=edgelist,\n         node_color=node_color, edge_color=edge_color, labels=labels,\n         **kwargs)", "response": "Draws an embedding onto the graph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_yield(G, layout, perfect_graph, unused_color=(0.9,0.9,0.9,1.0),\n                    fault_color=(1.0,0.0,0.0,1.0), fault_shape='x',\n                    fault_style='dashed', **kwargs):\n\n    \"\"\"Draws the given graph G with highlighted faults, according to layout.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph to be parsed for faults\n\n    layout : dict\n        A dict of coordinates associated with each node in perfect_graph. Should\n        be of the form {node: coordinate, ...}.  Coordinates will be\n        treated as vectors, and should all have the same length.\n\n    perfect_graph : NetworkX graph\n        The graph to be drawn with highlighted faults\n\n\n    unused_color : tuple or color string (optional, default (0.9,0.9,0.9,1.0))\n        The color to use for nodes and edges of G which are not faults.\n        If unused_color is None, these nodes and edges will not be shown at all.\n\n    fault_color : tuple or color string (optional, default (1.0,0.0,0.0,1.0))\n        A color to represent nodes absent from the graph G. Colors should be\n        length-4 tuples of floats between 0 and 1 inclusive.\n\n    fault_shape : string, optional (default='x')\n        The shape of the fault nodes. Specification is as matplotlib.scatter\n        marker, one of 'so^>v<dph8'.\n\n    fault_style : string, optional (default='dashed')\n        Edge fault line style (solid|dashed|dotted,dashdot)\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n    \"\"\"\n    try:\n        import matplotlib.pyplot as plt\n        import matplotlib as mpl\n    except ImportError:\n        raise ImportError(\"Matplotlib and numpy required for draw_chimera()\")\n\n    nodelist = G.nodes()\n    edgelist = G.edges()\n    faults_nodelist = perfect_graph.nodes() - nodelist\n    faults_edgelist = perfect_graph.edges() - edgelist\n\n    # To avoid matplotlib.pyplot.scatter warnings for single tuples, create\n    # lists of colors from given colors.\n    faults_node_color = [fault_color for v in faults_nodelist]\n    faults_edge_color = [fault_color for v in faults_edgelist]\n\n    # Draw faults with different style and shape\n    draw(perfect_graph, layout, nodelist=faults_nodelist, edgelist=faults_edgelist,\n        node_color=faults_node_color, edge_color=faults_edge_color,\n        style=fault_style, node_shape=fault_shape,\n        **kwargs )\n\n    # Draw rest of graph\n    if unused_color is not None:\n        if nodelist is None:\n            nodelist = G.nodes() - faults_nodelist\n        if edgelist is None:\n            edgelist = G.edges() - faults_edgelist\n\n        unused_node_color = [unused_color for v in nodelist]\n        unused_edge_color = [unused_color for v in edgelist]\n\n        draw(perfect_graph, layout, nodelist=nodelist, edgelist=edgelist,\n            node_color=unused_node_color, edge_color=unused_edge_color,\n            **kwargs)", "response": "Draws the given graph G with highlighted faults according to layout."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an approximate minimum vertex coloring for a given graph.", "response": "def min_vertex_coloring(G, sampler=None, **sampler_args):\n    \"\"\"Returns an approximate minimum vertex coloring.\n\n    Vertex coloring is the problem of assigning a color to the\n    vertices of a graph in a way that no adjacent vertices have the\n    same color. A minimum vertex coloring is the problem of solving\n    the vertex coloring problem using the smallest number of colors.\n\n    Since neighboring vertices must satisfy a constraint of having\n    different colors, the problem can be posed as a binary constraint\n    satisfaction problem.\n\n    Defines a QUBO with ground states corresponding to minimum\n    vertex colorings and uses the sampler to sample from it.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to find a minimum vertex coloring.\n\n    sampler\n        A binary quadratic model sampler. A sampler is a process that\n        samples from low energy states in models defined by an Ising\n        equation or a Quadratic Unconstrained Binary Optimization\n        Problem (QUBO). A sampler is expected to have a 'sample_qubo'\n        and 'sample_ising' method. A sampler is expected to return an\n        iterable of samples, in order of increasing energy. If no\n        sampler is provided, one must be provided using the\n        `set_default_sampler` function.\n\n    sampler_args\n        Additional keyword parameters are passed to the sampler.\n\n    Returns\n    -------\n    coloring : dict\n        A coloring for each vertex in G such that no adjacent nodes\n        share the same color. A dict of the form {node: color, ...}\n\n    Example\n    -------\n    This example colors a single Chimera unit cell. It colors the four\n    horizontal qubits one color (0) and the four vertical qubits another (1).\n\n    >>> # Set up a sampler; this example uses a sampler from dimod https://github.com/dwavesystems/dimod\n    >>> import dimod\n    >>> import dwave_networkx as dnx\n    >>> samplerSA = dimod.SimulatedAnnealingSampler()\n    >>> # Create a graph and color it\n    >>> G = dnx.chimera_graph(1, 1, 4)\n    >>> colors = dnx.min_vertex_coloring(G, sampler=samplerSA)\n    >>> colors\n    {0: 0, 1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1}\n\n    References\n    ----------\n    .. [DWMP] Dahl, E., \"Programming the D-Wave: Map Coloring Problem\",\n       https://www.dwavesys.com/sites/default/files/Map%20Coloring%20WP2.pdf\n\n    Notes\n    -----\n    Samplers by their nature may not return the optimal solution. This\n    function does not attempt to confirm the quality of the returned\n    sample.\n\n    \"\"\"\n\n    # if the given graph is not connected, apply the function to each connected component\n    # seperately.\n    if not nx.is_connected(G):\n        coloring = {}\n        for subG in (G.subgraph(c).copy() for c in nx.connected_components(G)):\n            sub_coloring = min_vertex_coloring(subG, sampler, **sampler_args)\n            coloring.update(sub_coloring)\n        return coloring\n\n    n_nodes = len(G)  # number of nodes\n    n_edges = len(G.edges)  # number of edges\n\n    # ok, first up, we can eliminate a few graph types trivially\n\n    # Graphs with no edges, have chromatic number 1\n    if not n_edges:\n        return {node: 0 for node in G}\n\n    # Complete graphs have chromatic number N\n    if n_edges == n_nodes * (n_nodes - 1) // 2:\n        return {node: color for color, node in enumerate(G)}\n\n    # The number of variables in the QUBO is approximately the number of nodes in the graph\n    # times the number of potential colors, so we want as tight an upper bound on the\n    # chromatic number (chi) as possible\n    chi_ub = _chromatic_number_upper_bound(G, n_nodes, n_edges)\n\n    # now we can start coloring. Without loss of generality, we can determine some of\n    # the node colors before trying to solve.\n    partial_coloring, possible_colors, chi_lb = _partial_precolor(G, chi_ub)\n\n    # ok, to get the rest of the coloring, we need to start building the QUBO. We do this\n    # by assigning a variable x_v_c for each node v and color c. This variable will be 1\n    # when node v is colored c, and 0 otherwise.\n\n    # let's assign an index to each of the variables\n    counter = itertools.count()\n    x_vars = {v: {c: next(counter) for c in possible_colors[v]} for v in possible_colors}\n\n    # now we have three different constraints we wish to add.\n\n    # the first constraint enforces the coloring rule, that for each pair of vertices\n    # u, v that share an edge, they should be different colors\n    Q_neighbor = _vertex_different_colors_qubo(G, x_vars)\n\n    # the second constraint enforces that each vertex has a single color assigned\n    Q_vertex = _vertex_one_color_qubo(x_vars)\n\n    # the third constraint is that we want a minimum vertex coloring, so we want to\n    # disincentivize the colors we might not need.\n    Q_min_color = _minimum_coloring_qubo(x_vars, chi_lb, chi_ub, magnitude=.75)\n\n    # combine all three constraints\n    Q = Q_neighbor\n    for (u, v), bias in iteritems(Q_vertex):\n        if (u, v) in Q:\n            Q[(u, v)] += bias\n        elif (v, u) in Q:\n            Q[(v, u)] += bias\n        else:\n            Q[(u, v)] = bias\n    for (v, v), bias in iteritems(Q_min_color):\n        if (v, v) in Q:\n            Q[(v, v)] += bias\n        else:\n            Q[(v, v)] = bias\n\n    # use the sampler to find low energy states\n    response = sampler.sample_qubo(Q, **sampler_args)\n\n    # we want the lowest energy sample\n    sample = next(iter(response))\n\n    # read off the coloring\n    for v in x_vars:\n        for c in x_vars[v]:\n            if sample[x_vars[v][c]]:\n                partial_coloring[v] = c\n\n    return partial_coloring"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _minimum_coloring_qubo(x_vars, chi_lb, chi_ub, magnitude=1.):\n    # if we already know the chromatic number, then we don't need to\n    # disincentivize any colors.\n    if chi_lb == chi_ub:\n        return {}\n\n    # we might need to use some of the colors, so we want to disincentivize\n    # them in increasing amounts, linearly.\n    scaling = magnitude / (chi_ub - chi_lb)\n\n    # build the QUBO\n    Q = {}\n    for v in x_vars:\n        for f, color in enumerate(range(chi_lb, chi_ub)):\n            idx = x_vars[v][color]\n            Q[(idx, idx)] = (f + 1) * scaling\n\n    return Q", "response": "Generates the QUBO that does that."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _vertex_different_colors_qubo(G, x_vars):\n    Q = {}\n    for u, v in G.edges:\n        if u not in x_vars or v not in x_vars:\n            continue\n        for color in x_vars[u]:\n            if color in x_vars[v]:\n                Q[(x_vars[u][color], x_vars[v][color])] = 1.\n    return Q", "response": "Generates a QUBO that enforces that each vertex has the same color as any of its neighbors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _vertex_one_color_qubo(x_vars):\n    Q = {}\n    for v in x_vars:\n        for color in x_vars[v]:\n            idx = x_vars[v][color]\n            Q[(idx, idx)] = -1\n\n        for color0, color1 in itertools.combinations(x_vars[v], 2):\n            idx0 = x_vars[v][color0]\n            idx1 = x_vars[v][color1]\n\n            Q[(idx0, idx1)] = 2\n\n    return Q", "response": "Generate a single QUBO that has exactly one color."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_cycle(G):\n    trailing, leading = next(iter(G.edges))\n    start_node = trailing\n\n    # travel around the graph, checking that each node has degree exactly two\n    # also track how many nodes were visited\n    n_visited = 1\n    while leading != start_node:\n        neighbors = G[leading]\n\n        if len(neighbors) != 2:\n            return False\n\n        node1, node2 = neighbors\n\n        if node1 == trailing:\n            trailing, leading = leading, node2\n        else:\n            trailing, leading = leading, node1\n\n        n_visited += 1\n\n    # if we haven't visited all of the nodes, then it is not a connected cycle\n    return n_visited == len(G)", "response": "Determines whether the given graph is a cycle or circular graph."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_vertex_coloring(G, coloring):\n    return all(coloring[u] != coloring[v] for u, v in G.edges)", "response": "Determines whether the given coloring is a vertex coloring of graph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef maximal_matching(G, sampler=None, **sampler_args):\n\n    # the maximum degree\n    delta = max(G.degree(node) for node in G)\n\n    # use the maximum degree to determine the infeasible gaps\n    A = 1.\n    if delta == 2:\n        B = .75\n    else:\n        B = .75 * A / (delta - 2.)  # we want A > (delta - 2) * B\n\n    # each edge in G gets a variable, so let's create those\n    edge_mapping = _edge_mapping(G)\n\n    # build the QUBO\n    Q = _maximal_matching_qubo(G, edge_mapping, magnitude=B)\n    Qm = _matching_qubo(G, edge_mapping, magnitude=A)\n    for edge, bias in Qm.items():\n        if edge not in Q:\n            Q[edge] = bias\n        else:\n            Q[edge] += bias\n\n    # use the sampler to find low energy states\n    response = sampler.sample_qubo(Q, **sampler_args)\n\n    # we want the lowest energy sample\n    sample = next(iter(response))\n\n    # the matching are the edges that are 1 in the sample\n    return set(edge for edge in G.edges if sample[edge_mapping[edge]] > 0)", "response": "Finds an approximate maximal matching of a graph G."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining whether the given set of edges is a maximal matching.", "response": "def is_maximal_matching(G, matching):\n    \"\"\"Determines whether the given set of edges is a maximal matching.\n\n    A matching is a subset of edges in which no node occurs more than\n    once. The cardinality of a matching is the number of matched edges.\n    A maximal matching is one where one cannot add any more edges\n    without violating the matching rule.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph on which to check the maximal matching.\n\n    edges : iterable\n        A iterable of edges.\n\n    Returns\n    -------\n    is_matching : bool\n        True if the given edges are a maximal matching.\n\n    Example\n    -------\n    This example checks two sets of edges, both derived from a\n    single Chimera unit cell, for a matching. The first set (a matching) is\n    a subset of the second, which was found using the `min_maximal_matching()`\n    function.\n\n    >>> import dwave_networkx as dnx\n    >>> G = dnx.chimera_graph(1, 1, 4)\n    >>> dnx.is_matching({(0, 4), (2, 7)})\n    True\n    >>> dnx.is_maximal_matching(G,{(0, 4), (2, 7)})\n    False\n    >>> dnx.is_maximal_matching(G,{(0, 4), (1, 5), (2, 7), (3, 6)})\n    True\n\n    \"\"\"\n    touched_nodes = set().union(*matching)\n\n    # first check if a matching\n    if len(touched_nodes) != len(matching) * 2:\n        return False\n\n    # now for each edge, check that at least one of its variables is\n    # already in the matching\n    for (u, v) in G.edges:\n        if u not in touched_nodes and v not in touched_nodes:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassigns a variable for each edge in G.", "response": "def _edge_mapping(G):\n    \"\"\"Assigns a variable for each edge in G.\n    (u, v) and (v, u) map to the same variable.\n    \"\"\"\n    edge_mapping = {edge: idx for idx, edge in enumerate(G.edges)}\n    edge_mapping.update({(e1, e0): idx for (e0, e1), idx in edge_mapping.items()})\n    return edge_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _maximal_matching_qubo(G, edge_mapping, magnitude=1.):\n    Q = {}\n\n    # for each node n in G, define a variable y_n to be 1 when n has a colored edge\n    # and 0 otherwise.\n    # for each edge (u, v) in the graph we want to enforce y_u OR y_v. This is because\n    # if both y_u == 0 and y_v == 0, then we could add (u, v) to the matching.\n    for (u, v) in G.edges:\n        # 1 - y_v - y_u + y_v*y_u\n\n        # for each edge connected to u\n        for edge in G.edges(u):\n            x = edge_mapping[edge]\n            if (x, x) not in Q:\n                Q[(x, x)] = -1 * magnitude\n            else:\n                Q[(x, x)] -= magnitude\n\n        # for each edge connected to v\n        for edge in G.edges(v):\n            x = edge_mapping[edge]\n            if (x, x) not in Q:\n                Q[(x, x)] = -1 * magnitude\n            else:\n                Q[(x, x)] -= magnitude\n\n        for e0 in G.edges(v):\n            x0 = edge_mapping[e0]\n            for e1 in G.edges(u):\n                x1 = edge_mapping[e1]\n\n                if x0 < x1:\n                    if (x0, x1) not in Q:\n                        Q[(x0, x1)] = magnitude\n                    else:\n                        Q[(x0, x1)] += magnitude\n                else:\n                    if (x1, x0) not in Q:\n                        Q[(x1, x0)] = magnitude\n                    else:\n                        Q[(x1, x0)] += magnitude\n\n    return Q", "response": "Generates a QUBO that is combined with one as generated by _matching_qubo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a QUBO that induces a matching on the given graph G.", "response": "def _matching_qubo(G, edge_mapping, magnitude=1.):\n    \"\"\"Generates a QUBO that induces a matching on the given graph G.\n    The variables in the QUBO are the edges, as given my edge_mapping.\n\n    ground_energy = 0\n    infeasible_gap = magnitude\n    \"\"\"\n    Q = {}\n\n    # We wish to enforce the behavior that no node has two colored edges\n    for node in G:\n\n        # for each pair of edges that contain node\n        for edge0, edge1 in itertools.combinations(G.edges(node), 2):\n\n            v0 = edge_mapping[edge0]\n            v1 = edge_mapping[edge1]\n\n            # penalize both being True\n            Q[(v0, v1)] = magnitude\n\n    return Q"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef canonical_chimera_labeling(G, t=None):\n    adj = G.adj\n\n    if t is None:\n        if hasattr(G, 'edges'):\n            num_edges = len(G.edges)\n        else:\n            num_edges = len(G.quadratic)\n        t = _chimera_shore_size(adj, num_edges)\n\n    chimera_indices = {}\n\n    row = col = 0\n    root = min(adj, key=lambda v: len(adj[v]))\n    horiz, verti = rooted_tile(adj, root, t)\n    while len(chimera_indices) < len(adj):\n\n        new_indices = {}\n\n        if row == 0:\n            # if we're in the 0th row, we can assign the horizontal randomly\n            for si, v in enumerate(horiz):\n                new_indices[v] = (row, col, 0, si)\n        else:\n            # we need to match the row above\n            for v in horiz:\n                north = [u for u in adj[v] if u in chimera_indices]\n                assert len(north) == 1\n                i, j, u, si = chimera_indices[north[0]]\n                assert i == row - 1 and j == col and u == 0\n                new_indices[v] = (row, col, 0, si)\n\n        if col == 0:\n            # if we're in the 0th col, we can assign the vertical randomly\n            for si, v in enumerate(verti):\n                new_indices[v] = (row, col, 1, si)\n        else:\n            # we need to match the column to the east\n            for v in verti:\n                east = [u for u in adj[v] if u in chimera_indices]\n                assert len(east) == 1\n                i, j, u, si = chimera_indices[east[0]]\n                assert i == row and j == col - 1 and u == 1\n                new_indices[v] = (row, col, 1, si)\n\n        chimera_indices.update(new_indices)\n\n        # get the next root\n        root_neighbours = [v for v in adj[root] if v not in chimera_indices]\n        if len(root_neighbours) == 1:\n            # we can increment the row\n            root = root_neighbours[0]\n            horiz, verti = rooted_tile(adj, root, t)\n\n            row += 1\n        else:\n            # need to go back to row 0, and increment the column\n            assert not root_neighbours  # should be empty\n\n            # we want (0, col, 1, 0), we could cache this, but for now let's just go look for it\n            # the slow way\n            vert_root = [v for v in chimera_indices if chimera_indices[v] == (0, col, 1, 0)][0]\n\n            vert_root_neighbours = [v for v in adj[vert_root] if v not in chimera_indices]\n\n            if vert_root_neighbours:\n\n                verti, horiz = rooted_tile(adj, vert_root_neighbours[0], t)\n                root = next(iter(horiz))\n\n                row = 0\n                col += 1\n\n    return chimera_indices", "response": "Returns a dict mapping from the labels of G to Chimera - indexed labeling."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maximum_weighted_independent_set(G, weight=None, sampler=None, lagrange=2.0, **sampler_args):\n    # Get a QUBO representation of the problem\n    Q = maximum_weighted_independent_set_qubo(G, weight, lagrange)\n\n    # use the sampler to find low energy states\n    response = sampler.sample_qubo(Q, **sampler_args)\n\n    # we want the lowest energy sample\n    sample = next(iter(response))\n\n    # nodes that are spin up or true are exactly the ones in S.\n    return [node for node in sample if sample[node] > 0]", "response": "Returns an approximate maximum weighted independent set."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef maximum_independent_set(G, sampler=None, lagrange=2.0, **sampler_args):\n    return maximum_weighted_independent_set(G, None, sampler, lagrange, **sampler_args)", "response": "Returns an approximate maximum independent set of a given graph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef maximum_weighted_independent_set_qubo(G, weight=None, lagrange=2.0):\n\n    # empty QUBO for an empty graph\n    if not G:\n        return {}\n\n    # We assume that the sampler can handle an unstructured QUBO problem, so let's set one up.\n    # Let us define the largest independent set to be S.\n    # For each node n in the graph, we assign a boolean variable v_n, where v_n = 1 when n\n    # is in S and v_n = 0 otherwise.\n    # We call the matrix defining our QUBO problem Q.\n    # On the diagnonal, we assign the linear bias for each node to be the negative of its weight.\n    # This means that each node is biased towards being in S. Weights are scaled to a maximum of 1.\n    # Negative weights are considered 0.\n    # On the off diagnonal, we assign the off-diagonal terms of Q to be 2. Thus, if both\n    # nodes are in S, the overall energy is increased by 2.\n    cost = dict(G.nodes(data=weight, default=1))\n    scale = max(cost.values())\n    Q = {(node, node): min(-cost[node] / scale, 0.0) for node in G}\n    Q.update({edge: lagrange for edge in G.edges})\n\n    return Q", "response": "Returns the QUBO with ground states corresponding to a maximum weighted independent set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef min_weighted_vertex_cover(G, weight=None, sampler=None, **sampler_args):\n    indep_nodes = set(maximum_weighted_independent_set(G, weight, sampler, **sampler_args))\n    return [v for v in G if v not in indep_nodes]", "response": "Returns an approximate minimum weighted vertex cover of a network."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_vertex_cover(G, vertex_cover):\n    cover = set(vertex_cover)\n    return all(u in cover or v in cover for u, v in G.edges)", "response": "Determines whether the given set of vertices is a vertex cover of graph G."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of positions of nodes in a Pegasus graph.", "response": "def pegasus_layout(G, scale=1., center=None, dim=2, crosses=False):\n    \"\"\"Positions the nodes of graph G in a Pegasus topology.\n\n    NumPy (http://scipy.org) is required for this function.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        Should be a Pegasus graph or a subgraph of a Pegasus graph.\n        This should be the product of dwave_networkx.pegasus_graph\n\n    scale : float (default 1.)\n        Scale factor. When scale = 1,  all positions fit within [0, 1]\n        on the x-axis and [-1, 0] on the y-axis.\n\n    center : None or array (default None)\n        Coordinates of the top left corner.\n\n    dim : int (default 2)\n        Number of dimensions. When dim > 2, all extra dimensions are\n        set to 0.\n\n    crosses: boolean (optional, default False)\n        If crosses is True, K_4,4 subgraphs are shown in a cross\n        rather than L configuration. Ignored if G was defined with\n        nice_coordinates=True.\n\n    Returns\n    -------\n    pos : dict\n        A dictionary of positions keyed by node.\n\n    Examples\n    --------\n    >>> G = dnx.pegasus_graph(1)\n    >>> pos = dnx.pegasus_layout(G)\n\n    \"\"\"\n\n    if not isinstance(G, nx.Graph) or G.graph.get(\"family\") != \"pegasus\":\n        raise ValueError(\"G must be generated by dwave_networkx.pegasus_graph\")\n\n    if G.graph.get('labels') == 'nice':\n        m = 3*(G.graph['rows']-1)\n        c_coords = chimera_node_placer_2d(m, m, 4, scale=scale, center=center, dim=dim)\n        def xy_coords(t, y, x, u, k): return c_coords(3*y+2-t, 3*x+t, u, k)\n        pos = {v: xy_coords(*v) for v in G.nodes()}\n    else:\n        xy_coords = pegasus_node_placer_2d(G, scale, center, dim, crosses=crosses)\n\n        if G.graph.get('labels') == 'coordinate':\n            pos = {v: xy_coords(*v) for v in G.nodes()}\n        elif G.graph.get('data'):\n            pos = {v: xy_coords(*dat['pegasus_index']) for v, dat in G.nodes(data=True)}\n        else:\n            m = G.graph.get('rows')\n            coord = pegasus_coordinates(m)\n            pos = {v: xy_coords(*coord.tuple(v)) for v in G.nodes()}\n\n    return pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pegasus_node_placer_2d(G, scale=1., center=None, dim=2, crosses=False):\n    import numpy as np\n\n    m = G.graph.get('rows')\n    h_offsets = G.graph.get(\"horizontal_offsets\")\n    v_offsets = G.graph.get(\"vertical_offsets\")\n    tile_width = G.graph.get(\"tile\")\n    tile_center = tile_width / 2 - .5\n\n    # want the enter plot to fill in [0, 1] when scale=1\n    scale /= m * tile_width\n\n    if center is None:\n        center = np.zeros(dim)\n    else:\n        center = np.asarray(center)\n\n    paddims = dim - 2\n    if paddims < 0:\n        raise ValueError(\"layout must have at least two dimensions\")\n\n    if len(center) != dim:\n        raise ValueError(\"length of center coordinates must match dimension of layout\")\n\n    if crosses:\n        # adjustment for crosses\n        cross_shift = 2.\n    else:\n        cross_shift = 0.\n\n    def _xy_coords(u, w, k, z):\n        # orientation, major perpendicular offset, minor perpendicular offset, parallel offset\n\n        if k % 2:\n            p = -.1\n        else:\n            p = .1\n\n        if u:\n            xy = np.array([z*tile_width+h_offsets[k] + tile_center, -tile_width*w-k-p+cross_shift])\n        else:\n            xy = np.array([tile_width*w+k+p+cross_shift, -z*tile_width-v_offsets[k]-tile_center])\n\n        # convention for Pegasus-lattice pictures is to invert the y-axis\n        return np.hstack((xy * scale, np.zeros(paddims))) + center\n\n    return _xy_coords", "response": "Generates a function that converts a Pegasus index to x y coordinates for a plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_pegasus(G, crosses=False, **kwargs):\n\n    draw_qubit_graph(G, pegasus_layout(G, crosses=crosses), **kwargs)", "response": "Draws graph G in a Pegasus topology."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndraws an embedding onto the pegasus graph G according to layout.", "response": "def draw_pegasus_embedding(G, *args, **kwargs):\n    \"\"\"Draws an embedding onto the pegasus graph G, according to layout.\n\n    If interaction_edges is not None, then only display the couplers in that\n    list.  If embedded_graph is not None, the only display the couplers between\n    chains with intended couplings according to embedded_graph.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        Should be a Pegasus graph or a subgraph of a Pegasus graph.\n        This should be the product of dwave_networkx.pegasus_graph\n\n    emb : dict\n        A dict of chains associated with each node in G.  Should be\n        of the form {node: chain, ...}.  Chains should be iterables\n        of qubit labels (qubits are nodes in G).\n\n    embedded_graph : NetworkX graph (optional, default None)\n        A graph which contains all keys of emb as nodes.  If specified,\n        edges of G will be considered interactions if and only if they\n        exist between two chains of emb if their keys are connected by\n        an edge in embedded_graph\n\n    interaction_edges : list (optional, default None)\n        A list of edges which will be used as interactions.\n\n    show_labels: boolean (optional, default False)\n        If show_labels is True, then each chain in emb is labelled with its key.\n\n    chain_color : dict (optional, default None)\n        A dict of colors associated with each key in emb.  Should be\n        of the form {node: rgba_color, ...}.  Colors should be length-4\n        tuples of floats between 0 and 1 inclusive. If chain_color is None,\n        each chain will be assigned a different color.\n\n    unused_color : tuple (optional, default (0.9,0.9,0.9,1.0))\n        The color to use for nodes and edges of G which are not involved\n        in chains, and edges which are neither chain edges nor interactions.\n        If unused_color is None, these nodes and edges will not be shown at all.\n\n    crosses: boolean (optional, default False)\n        If crosses is True, K_4,4 subgraphs are shown in a cross\n        rather than L configuration. Ignored if G was defined with\n        nice_coordinates=True.\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n    \"\"\"\n    crosses = kwargs.pop(\"crosses\", False)\n    draw_embedding(G, pegasus_layout(G, crosses=crosses), *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws the given graph G with highlighted faults according to layout.", "response": "def draw_pegasus_yield(G, **kwargs):\n    \"\"\"Draws the given graph G with highlighted faults, according to layout.\n\n    Parameters\n    ----------\n    G : NetworkX graph\n        The graph to be parsed for faults\n\n    unused_color : tuple or color string (optional, default (0.9,0.9,0.9,1.0))\n        The color to use for nodes and edges of G which are not faults.\n        If unused_color is None, these nodes and edges will not be shown at all.\n\n    fault_color : tuple or color string (optional, default (1.0,0.0,0.0,1.0))\n        A color to represent nodes absent from the graph G. Colors should be\n        length-4 tuples of floats between 0 and 1 inclusive.\n\n    fault_shape : string, optional (default='x')\n        The shape of the fault nodes. Specification is as matplotlib.scatter\n        marker, one of 'so^>v<dph8'.\n\n    fault_style : string, optional (default='dashed')\n        Edge fault line style (solid|dashed|dotted,dashdot)\n\n    kwargs : optional keywords\n       See networkx.draw_networkx() for a description of optional keywords,\n       with the exception of the `pos` parameter which is not used by this\n       function. If `linear_biases` or `quadratic_biases` are provided,\n       any provided `node_color` or `edge_color` arguments are ignored.\n    \"\"\"\n    try:\n        assert(G.graph[\"family\"] == \"pegasus\")\n        m = G.graph['columns']\n        offset_lists = (G.graph['vertical_offsets'], G.graph['horizontal_offsets'])\n        coordinates = G.graph[\"labels\"] == \"coordinate\"\n        # Can't interpret fabric_only from graph attributes\n    except:\n        raise ValueError(\"Target pegasus graph needs to have columns, rows, \\\n        tile, and label attributes to be able to identify faulty qubits.\")\n\n\n    perfect_graph = pegasus_graph(m, offset_lists=offset_lists, coordinates=coordinates)\n\n    draw_yield(G, pegasus_layout(perfect_graph), perfect_graph, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef duration_to_number(duration, units='seconds'):\n    if isinstance(duration, (int, float, long)):\n        return duration\n    elif isinstance(duration, (datetime.timedelta,)):\n        if units == 'seconds':\n            return duration.total_seconds()\n        else:\n            msg = 'unit \"%s\" is not supported' % units\n            raise NotImplementedError(msg)\n    elif duration == inf or duration == -inf:\n        msg = \"Can't convert infinite duration to number\"\n        raise ValueError(msg)\n    else:\n        msg = 'duration is an unknown type (%s)' % duration\n        raise TypeError(msg)", "response": "Convert a duration to a numeric type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_args_to_list(args):\n    list_of_pairs = []\n    if len(args) == 0:\n        return []\n\n    if any(isinstance(arg, (list, tuple)) for arg in args):\n        # Domain([[1, 4]])\n        # Domain([(1, 4)])\n        # Domain([(1, 4), (5, 8)])\n        # Domain([[1, 4], [5, 8]])\n        if len(args) == 1 and \\\n                any(isinstance(arg, (list, tuple)) for arg in args[0]):\n            for item in args[0]:\n                list_of_pairs.append(list(item))\n        else:\n            # Domain([1, 4])\n            # Domain((1, 4))\n            # Domain((1, 4), (5, 8))\n            # Domain([1, 4], [5, 8])\n            for item in args:\n                list_of_pairs.append(list(item))\n    else:\n        # Domain(1, 2)\n        if len(args) == 2:\n            list_of_pairs.append(list(args))\n        else:\n            msg = \"The argument type is invalid. \".format(args)\n            raise TypeError(msg)\n\n    return list_of_pairs", "response": "Convert all iterable pairs of inputs into a list of list of lists"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datetime_range(start_dt, end_dt, unit,\n                   n_units=1, inclusive_end=False):\n    \"\"\"A range of datetimes/dates.\"\"\"\n\n    def done(a, b, inclusive_end):\n        if inclusive_end:\n            return a <= b\n        else:\n            return a < b\n\n    current = start_dt\n    while done(current, end_dt, inclusive_end):\n        yield current\n        current += datetime.timedelta(**{unit: n_units})", "response": "A range of datetimes and dates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfloor a datetime to nearest n units.", "response": "def floor_datetime(dt, unit, n_units=1):\n    \"\"\"Floor a datetime to nearest n units. For example, if we want to\n    floor to nearest three months, starting with 2016-05-06-yadda, it\n    will go to 2016-04-01. Or, if starting with 2016-05-06-11:45:06\n    and rounding to nearest fifteen minutes, it will result in\n    2016-05-06-11:45:00.\n    \"\"\"\n    if unit == 'years':\n        new_year = dt.year - (dt.year - 1) % n_units\n        return datetime.datetime(new_year, 1, 1, 0, 0, 0)\n    elif unit == 'months':\n        new_month = dt.month - (dt.month - 1) % n_units\n        return datetime.datetime(dt.year, new_month, 1, 0, 0, 0)\n    elif unit == 'weeks':\n        _, isoweek, _ = dt.isocalendar()\n        new_week = isoweek - (isoweek - 1) % n_units\n        return datetime.datetime.strptime(\n            \"%d %02d 1\" % (dt.year, new_week), \"%Y %W %w\")\n    elif unit == 'days':\n        new_day = dt.day - dt.day % n_units\n        return datetime.datetime(dt.year, dt.month, new_day, 0, 0, 0)\n    elif unit == 'hours':\n        new_hour = dt.hour - dt.hour % n_units\n        return datetime.datetime(dt.year, dt.month, dt.day, new_hour, 0, 0)\n    elif unit == 'minutes':\n        new_minute = dt.minute - dt.minute % n_units\n        return datetime.datetime(dt.year, dt.month, dt.day,\n                                 dt.hour, new_minute, 0)\n    elif unit == 'seconds':\n        new_second = dt.second - dt.second % n_units\n        return datetime.datetime(dt.year, dt.month, dt.day,\n                                 dt.hour, dt.minute, new_second)\n    else:\n        msg = 'Unknown unit type {}'.format(unit)\n        raise ValueError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an iterable p1 p2 p3...", "response": "def pairwise(iterable):\n    \"\"\" given an interable `p1, p2, p3, ...`\n    it iterates through pairwise tuples `(p0, p1), (p1, p2), ...`\"\"\"\n    it = iter(iterable)\n    a = next(it, None)\n\n    for b in it:\n        yield (a, b)\n        a = b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmeaning of the distribution.", "response": "def mean(self):\n        \"\"\"Mean of the distribution.\"\"\"\n        _self = self._discard_value(None)\n        if not _self.total():\n            return None\n        weighted_sum = sum(\n            key * value for key, value in iteritems(_self)\n        )\n        return weighted_sum / float(_self.total())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normalized(self):\n        total = self.total()\n        result = Histogram()\n        for value, count in iteritems(self):\n            try:\n                result[value] = count / float(total)\n            except UnorderableElements as e:\n                result = Histogram.from_dict(dict(result), key=hash)\n                result[value] = count / float(total)\n        return result", "response": "Return a normalized version of the histogram where the values sum\n        to one."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a function that returns the quantile values for thisCOOKIE.", "response": "def _quantile_function(self, alpha=0.5, smallest_count=None):\n        \"\"\"Return a function that returns the quantile values for this\n        histogram.\n\n        \"\"\"\n        total = float(self.total())\n\n        smallest_observed_count = min(itervalues(self))\n        if smallest_count is None:\n            smallest_count = smallest_observed_count\n        else:\n            smallest_count = min(smallest_count, smallest_observed_count)\n\n        beta = alpha * smallest_count\n\n        debug_plot = []\n        cumulative_sum = 0.0\n        inverse = sortedcontainers.SortedDict()\n        for value, count in iteritems(self):\n            debug_plot.append((cumulative_sum / total, value))\n            inverse[(cumulative_sum + beta) / total] = value\n            cumulative_sum += count\n            inverse[(cumulative_sum - beta) / total] = value\n            debug_plot.append((cumulative_sum / total, value))\n\n        # get maximum and minumum q values\n        q_min = inverse.iloc[0]\n        q_max = inverse.iloc[-1]\n\n        # this stuff if helpful for debugging -- keep it in here\n        # for i, j in debug_plot:\n        #     print i, j\n        # print ''\n        # for i, j in inverse.iteritems():\n        #     print i, j\n        # print ''\n\n        def function(q):\n\n            if q < 0.0 or q > 1.0:\n                msg = 'invalid quantile %s, need `0 <= q <= 1`' % q\n                raise ValueError(msg)\n            elif q < q_min:\n                q = q_min\n            elif q > q_max:\n                q = q_max\n\n            # if beta is\n            if beta > 0:\n                if q in inverse:\n                    result = inverse[q]\n                else:\n                    previous_index = inverse.bisect_left(q) - 1\n                    x1 = inverse.iloc[previous_index]\n                    x2 = inverse.iloc[previous_index + 1]\n                    y1 = inverse[x1]\n                    y2 = inverse[x2]\n                    result = (y2 - y1) * (q - x1) / float(x2 - x1) + y1\n\n            else:\n                if q in inverse:\n                    previous_index = inverse.bisect_left(q) - 1\n                    x1 = inverse.iloc[previous_index]\n                    x2 = inverse.iloc[previous_index + 1]\n                    y1 = inverse[x1]\n                    y2 = inverse[x2]\n                    result = 0.5 * (y1 + y2)\n                else:\n                    previous_index = inverse.bisect_left(q) - 1\n                    x1 = inverse.iloc[previous_index]\n                    result = inverse[x1]\n\n            return float(result)\n\n        return function"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of the time series even in - between measured values.", "response": "def get(self, time, interpolate='previous'):\n        \"\"\"Get the value of the time series, even in-between measured values.\n\n        \"\"\"\n        try:\n            getter = self.getter_functions[interpolate]\n        except KeyError:\n            msg = (\n                \"unknown value '{}' for interpolate, \"\n                \"valid values are in [{}]\"\n            ).format(interpolate, ', '.join(self.getter_functions))\n            raise ValueError(msg)\n        else:\n            return getter(time)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the value for the time series.", "response": "def set(self, time, value, compact=False):\n        \"\"\"Set the value for the time series. If compact is True, only set the\n        value if it's different from what it would be anyway.\n\n        \"\"\"\n        if (len(self) == 0) or (not compact) or \\\n                (compact and self.get(time) != value):\n            self._d[time] = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_interval(self, start, end, value, compact=False):\n        # for each interval to render\n        for i, (s, e, v) in enumerate(self.iterperiods(start, end)):\n            # look at all intervals included in the current interval\n            # (always at least 1)\n            if i == 0:\n                # if the first, set initial value to new value of range\n                self.set(s, value, compact)\n            else:\n                # otherwise, remove intermediate key\n                del self[s]\n\n        # finish by setting the end of the interval to the previous value\n        self.set(end, v, compact)", "response": "Set the value for the time series on an interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting this instance to a compact version of the value.", "response": "def compact(self):\n        \"\"\"Convert this instance to a compact version: the value will be the\n        same at all times, but repeated measurements are discarded.\n\n        \"\"\"\n        previous_value = object()\n        redundant = []\n        for time, value in self:\n            if value == previous_value:\n                redundant.append(time)\n            previous_value = value\n        for time in redundant:\n            del self[time]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef exists(self):\n        result = TimeSeries(default=False if self.default is None else True)\n        for t, v in self:\n            result[t] = False if v is None else True\n        return result", "response": "returns False when the timeseries has a None value True otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nallowing removal of all points from the time series within a given interval", "response": "def remove_points_from_interval(self, start, end):\n        \"\"\"Allow removal of all points from the time series within a interval\n        [start:end].\n\n        \"\"\"\n        for s, e, v in self.iterperiods(start, end):\n            try:\n                del self._d[s]\n            except KeyError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iterintervals(self, n=2):\n        # tee the original iterator into n identical iterators\n        streams = tee(iter(self), n)\n\n        # advance the \"cursor\" on each iterator by an increasing\n        # offset, e.g. if n=3:\n        #\n        #                   [a, b, c, d, e, f, ..., w, x, y, z]\n        #  first cursor -->  *\n        # second cursor -->     *\n        #  third cursor -->        *\n        for stream_index, stream in enumerate(streams):\n            for i in range(stream_index):\n                next(stream)\n\n        # now, zip the offset streams back together to yield tuples,\n        # in the n=3 example it would yield:\n        # (a, b, c), (b, c, d), ..., (w, x, y), (x, y, z)\n        for intervals in zip(*streams):\n            yield intervals", "response": "Iterate over consecutive measurement points in the time series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iterperiods(self, start=None, end=None, value=None):\n        start, end, mask = \\\n            self._check_boundaries(start, end, allow_infinite=False)\n\n        value_function = self._value_function(value)\n\n        # get start index and value\n        start_index = self._d.bisect_right(start)\n        if start_index:\n            start_value = self._d[self._d.iloc[start_index - 1]]\n        else:\n            start_value = self.default\n\n        # get last index before end of time span\n        end_index = self._d.bisect_right(end)\n\n        interval_t0, interval_value = start, start_value\n\n        for interval_t1 in self._d.islice(start_index, end_index):\n\n            if value_function(interval_t0, interval_t1, interval_value):\n                yield interval_t0, interval_t1, interval_value\n\n            # set start point to the end of this interval for next\n            # iteration\n            interval_t0 = interval_t1\n            interval_value = self[interval_t0]\n\n        # yield the time, duration, and value of the final period\n        if interval_t0 < end:\n            if value_function(interval_t0, end, interval_value):\n                yield interval_t0, end, interval_value", "response": "This method iterates over the periods of a given time - range."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an equivalent TimeSeries that only has points between start and end", "response": "def slice(self, start, end):\n        \"\"\"Return an equivalent TimeSeries that only has points between\n        `start` and `end` (always starting at `start`)\n\n        \"\"\"\n        start, end, mask = \\\n            self._check_boundaries(start, end, allow_infinite=True)\n\n        result = TimeSeries(default=self.default)\n        for t0, t1, value in self.iterperiods(start, end):\n            result[t0] = value\n\n        result[t1] = self[t1]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, sampling_period, start=None, end=None,\n               interpolate='previous'):\n        \"\"\"Sampling at regular time periods.\n\n        \"\"\"\n        start, end, mask = self._check_boundaries(start, end)\n\n        sampling_period = \\\n            self._check_regularization(start, end, sampling_period)\n\n        result = []\n        current_time = start\n        while current_time <= end:\n            value = self.get(current_time, interpolate=interpolate)\n            result.append((current_time, value))\n            current_time += sampling_period\n        return result", "response": "Sampling at regular time periods."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naverages over regular intervals and averaging over a given period.", "response": "def moving_average(self, sampling_period,\n                       window_size=None,\n                       start=None, end=None,\n                       placement='center',\n                       pandas=False):\n        \"\"\"Averaging over regular intervals\n        \"\"\"\n        start, end, mask = self._check_boundaries(start, end)\n\n        # default to sampling_period if not given\n        if window_size is None:\n            window_size = sampling_period\n\n        sampling_period = \\\n            self._check_regularization(start, end, sampling_period)\n\n        # convert to datetime if the times are datetimes\n        full_window = window_size * 1.  # convert to float if int or do nothing\n        half_window = full_window / 2.  # divide by 2\n        if (isinstance(start, datetime.datetime) and\n                not isinstance(full_window, datetime.timedelta)):\n            half_window = datetime.timedelta(seconds=half_window)\n            full_window = datetime.timedelta(seconds=full_window)\n\n        result = []\n        current_time = start\n        while current_time <= end:\n\n            if placement == 'center':\n                window_start = current_time - half_window\n                window_end = current_time + half_window\n            elif placement == 'left':\n                window_start = current_time\n                window_end = current_time + full_window\n            elif placement == 'right':\n                window_start = current_time - full_window\n                window_end = current_time\n            else:\n                msg = 'unknown placement \"{}\"'.format(placement)\n                raise ValueError(msg)\n\n            # calculate mean over window and add (t, v) tuple to list\n            try:\n                mean = self.mean(window_start, window_end)\n            except TypeError as e:\n                if 'NoneType' in str(e):\n                    mean = None\n                else:\n                    raise e\n            result.append((current_time, mean))\n\n            current_time += sampling_period\n\n        # convert to pandas Series if pandas=True\n        if pandas:\n\n            try:\n                import pandas as pd\n            except ImportError:\n                msg = \"can't have pandas=True if pandas is not installed\"\n                raise ImportError(msg)\n\n            result = pd.Series(\n                [v for t, v in result],\n                index=[t for t, v in result],\n            )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mean(self, start=None, end=None, mask=None):\n        return self.distribution(start=start, end=end, mask=mask).mean()", "response": "This calculated the average value of the time series over the given time range from start to end when mask is truthy."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the distribution of values over the given time range from start to end.", "response": "def distribution(self, start=None, end=None, normalized=True, mask=None):\n        \"\"\"Calculate the distribution of values over the given time range from\n        `start` to `end`.\n\n        Args:\n\n            start (orderable, optional): The lower time bound of\n                when to calculate the distribution. By default, the\n                first time point will be used.\n\n            end (orderable, optional): The upper time bound of\n                when to calculate the distribution. By default, the\n                last time point will be used.\n\n            normalized (bool): If True, distribution will sum to\n                one. If False and the time values of the TimeSeries\n                are datetimes, the units will be seconds.\n\n            mask (:obj:`TimeSeries`, optional): A\n                domain on which to calculate the distribution.\n\n        Returns:\n\n            :obj:`Histogram` with the results.\n\n        \"\"\"\n\n        start, end, mask = self._check_boundaries(start, end, mask=mask)\n\n        counter = histogram.Histogram()\n        for start, end, _ in mask.iterperiods(value=True):\n            for t0, t1, value in self.iterperiods(start, end):\n                duration = utils.duration_to_number(\n                    t1 - t0,\n                    units='seconds',\n                )\n                try:\n                    counter[value] += duration\n                except histogram.UnorderableElements as e:\n\n                    counter = histogram.Histogram.from_dict(\n                        dict(counter), key=hash)\n                    counter[value] += duration\n\n        # divide by total duration if result needs to be normalized\n        if normalized:\n            return counter.normalized()\n        else:\n            return counter"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the number of points over the given time range from start to end.", "response": "def n_points(self, start=-inf, end=+inf, mask=None,\n                 include_start=True, include_end=False, normalized=False):\n        \"\"\"Calculate the number of points over the given time range from\n        `start` to `end`.\n\n        Args:\n\n            start (orderable, optional): The lower time bound of when\n                to calculate the distribution. By default, start is\n                -infinity.\n\n            end (orderable, optional): The upper time bound of when to\n                calculate the distribution. By default, the end is\n                +infinity.\n\n            mask (:obj:`TimeSeries`, optional): A\n                domain on which to calculate the distribution.\n\n        Returns:\n\n             `int` with the result\n\n        \"\"\"\n        # just go ahead and return 0 if we already know it regarless\n        # of boundaries\n        if not self.n_measurements():\n            return 0\n\n        start, end, mask = self._check_boundaries(start, end, mask=mask)\n\n        count = 0\n        for start, end, _ in mask.iterperiods(value=True):\n\n            if include_end:\n                end_count = self._d.bisect_right(end)\n            else:\n                end_count = self._d.bisect_left(end)\n\n            if include_start:\n                start_count = self._d.bisect_left(start)\n            else:\n                start_count = self._d.bisect_right(start)\n\n            count += (end_count - start_count)\n\n        if normalized:\n            count /= float(self.n_measurements())\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the argument is a TimeSeries and raises an error if it s not.", "response": "def _check_time_series(self, other):\n        \"\"\"Function used to check the type of the argument and raise an\n        informative error message if it's not a TimeSeries.\n\n        \"\"\"\n        if not isinstance(other, TimeSeries):\n            msg = \"unsupported operand types(s) for +: %s and %s\" % \\\n                  (type(self), type(other))\n            raise TypeError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_merge(cls, timeseries_list):\n        # using return without an argument is the way to say \"the\n        # iterator is empty\" when there is nothing to iterate over\n        # (the more you know...)\n        if not timeseries_list:\n            return\n\n        # for ts in timeseries_list:\n        #     if ts.is_floating():\n        #         msg = \"can't merge empty TimeSeries with no default value\"\n        #         raise KeyError(msg)\n\n        # This function mostly wraps _iter_merge, the main point of\n        # this is to deal with the case of tied times, where we only\n        # want to yield the last list of values that occurs for any\n        # group of tied times.\n        index, previous_t, previous_state = -1, object(), object()\n        for index, (t, state) in enumerate(cls._iter_merge(timeseries_list)):\n            if index > 0 and t != previous_t:\n                yield previous_t, previous_state\n            previous_t, previous_state = t, state\n\n        # only yield final thing if there was at least one element\n        # yielded by _iter_merge\n        if index > -1:\n            yield previous_t, previous_state", "response": "Iterate through several time series in order yielding ( time list ) where list is the values of each individual TimeSeries\n        in the list at time t."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging several time series in order yielding ( time value ) where value is the list of each individual time series at time t.", "response": "def merge(cls, ts_list, compact=True, operation=None):\n        \"\"\"Iterate through several time series in order, yielding (time,\n        `value`) where `value` is the either the list of each\n        individual TimeSeries in the list at time t (in the same order\n        as in ts_list) or the result of the optional `operation` on\n        that list of values.\n        \"\"\"\n        # If operation is not given then the default is the list\n        # of defaults of all time series\n        # If operation is given, then the default is the result of\n        # the operation over the list of all defaults\n        default = [ts.default for ts in ts_list]\n        if operation:\n            default = operation(default)\n\n        result = cls(default=default)\n        for t, merged in cls.iter_merge(ts_list):\n            if operation is None:\n                value = merged\n            else:\n                value = operation(merged)\n            result.set(t, value, compact=compact)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef operation(self, other, function, **kwargs):\n        result = TimeSeries(**kwargs)\n        if isinstance(other, TimeSeries):\n            for time, value in self:\n                result[time] = function(value, other[time])\n            for time, value in other:\n                result[time] = function(self[time], value)\n        else:\n            for time, value in self:\n                result[time] = function(value, other)\n        return result", "response": "Calculate elementwise operation between this time series and another time series."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_bool(self, invert=False):\n        if invert:\n            def function(x, y):\n                return False if x else True\n        else:\n            def function(x, y):\n                return True if x else False\n        return self.operation(None, function)", "response": "Return the truth value of each element."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef threshold(self, value, inclusive=False):\n        if inclusive:\n            def function(x, y):\n                return True if x >= y else False\n        else:\n            def function(x, y):\n                return True if x > y else False\n        return self.operation(value, function)", "response": "Return True if value is > than threshold value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsum the values of two Timeseries.", "response": "def sum(self, other):\n        \"\"\"sum(x, y) = x(t) + y(t).\"\"\"\n        return TimeSeries.merge(\n            [self, other], operation=operations.ignorant_sum\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multiply(self, other):\n        return self.operation(other, lambda x, y: x * y)", "response": "multiply - computes the product of two sets of time items."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef logical_and(self, other):\n        return self.operation(other, lambda x, y: int(x and y))", "response": "logical_and returns the logical AND of two sets of data structures."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef logical_or(self, other):\n        return self.operation(other, lambda x, y: int(x or y))", "response": "logical_or - computes the logical OR of two sets of attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef logical_xor(self, other):\n        return self.operation(other, lambda x, y: int(bool(x) ^ bool(y)))", "response": "logical_xor - XOR operation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_all(pattern='data/lightbulb-*.csv'):\n    result = []\n    for filename in glob.iglob(pattern):\n        print('reading', filename, file=sys.stderr)\n        ts = traces.TimeSeries.from_csv(\n            filename,\n            time_column=0,\n            time_transform=parse_iso_datetime,\n            value_column=1,\n            value_transform=int,\n            default=0,\n        )\n        ts.compact()\n        result.append(ts)\n    return result", "response": "Read all of the CSVs in a directory matching the pattern\n    as TimeSeries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_init(key):\n    filename = os.path.join(\"traces\", \"__init__.py\")\n    result = None\n    with open(filename) as stream:\n        for line in stream:\n            if key in line:\n                result = line.split('=')[-1].strip().replace(\"'\", \"\")\n\n    # throw error if version isn't in __init__ file\n    if result is None:\n        raise ValueError('must define %s in %s' % (key, filename))\n\n    return result", "response": "Parse the package __init__ file to find a variable so that it s not\n    in multiple places."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_dependencies(filename):\n    dependencies = []\n    filepath = os.path.join('requirements', filename)\n    with open(filepath, 'r') as stream:\n        for line in stream:\n            package = line.strip().split('#')[0].strip()\n            if package and package.split(' ')[0] != '-r':\n                dependencies.append(package)\n    return dependencies", "response": "Read in the dependencies from the virtualenv requirements file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self, plugin, instance=None, action=None):\n\n        plugin_obj = self.__plugins[plugin[\"id\"]]\n        instance_obj = (self.__instances[instance[\"id\"]]\n                        if instance is not None else None)\n\n        result = pyblish.plugin.process(\n            plugin=plugin_obj,\n            context=self._context,\n            instance=instance_obj,\n            action=action)\n\n        return formatting.format_result(result)", "response": "Given JSON objects from client perform actual processing of the related objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef emit(self, signal, kwargs):\n\n        if \"context\" in kwargs:\n            kwargs[\"context\"] = self._context\n\n        if \"instance\" in kwargs:\n            kwargs[\"instance\"] = self.__instances[kwargs[\"instance\"]]\n\n        if \"plugin\" in kwargs:\n            kwargs[\"plugin\"] = self.__plugins[kwargs[\"plugin\"]]\n\n        pyblish.api.emit(signal, **kwargs)", "response": "Trigger registered callbacks\n\nAttributeNames This method is triggered remotely and run locally."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validates(version):\n\n    def _validates(cls):\n        validators[version] = cls\n        if u\"id\" in cls.META_SCHEMA:\n            meta_schemas[cls.META_SCHEMA[u\"id\"]] = cls\n        return cls\n    return _validates", "response": "Decorator for validating a version of the base specification."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate(instance, schema, cls=None, *args, **kwargs):\n    if cls is None:\n        cls = validator_for(schema)\n    cls.check_schema(schema)\n    cls(schema, *args, **kwargs).validate(instance)", "response": "Validate an instance under the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_schema(cls, schema, *args, **kwargs):\n\n        return cls(schema.get(u\"id\", u\"\"), schema, *args, **kwargs)", "response": "Construct a new resolver from a JSON schema object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve a fragment within the referenced document.", "response": "def resolve_fragment(self, document, fragment):\n        \"\"\"\n        Resolve a ``fragment`` within the referenced ``document``.\n\n        :argument document: the referrant document\n        :argument str fragment: a URI fragment to resolve within it\n\n        \"\"\"\n\n        fragment = fragment.lstrip(u\"/\")\n        parts = unquote(fragment).split(u\"/\") if fragment else []\n\n        for part in parts:\n            part = part.replace(u\"~1\", u\"/\").replace(u\"~0\", u\"~\")\n\n            if isinstance(document, Sequence):\n                # Array indexes should be turned into integers\n                try:\n                    part = int(part)\n                except ValueError:\n                    pass\n            try:\n                document = document[part]\n            except (TypeError, LookupError):\n                raise RefResolutionError(\n                    \"Unresolvable JSON pointer: %r\" % fragment\n                )\n\n        return document"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_remote(self, uri):\n\n        scheme = urlsplit(uri).scheme\n\n        if scheme in self.handlers:\n            result = self.handlers[scheme](uri)\n        elif (\n            scheme in [u\"http\", u\"https\"] and\n            requests and\n            getattr(requests.Response, \"json\", None) is not None\n        ):\n            # Requests has support for detecting the correct encoding of\n            # json over http\n            if callable(requests.Response.json):\n                result = requests.get(uri).json()\n            else:\n                result = requests.get(uri).json\n        else:\n            # Otherwise, pass off to urllib and assume utf-8\n            result = json.loads(urlopen(uri).read().decode(\"utf-8\"))\n\n        if self.cache_remote:\n            self.store[uri] = result\n        return result", "response": "Resolve a remote uri."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate():\n\n    errors = dict()\n    for test in (test_architecture,\n                 test_pyqt_availability,\n                 test_pyblish_availability,\n                 test_qtconf_availability,\n                 test_qtconf_correctness,\n                 test_qt_availability):\n        try:\n            test()\n        except Exception as e:\n            errors[test] = e\n\n    if not errors:\n        print(\"=\" * 78)\n        print()\n        print(\"\"\" - Success!\n\n{exe} is well suited to run Pyblish QML\"\"\".format(exe=sys.executable))\n        print()\n        print(\"=\" * 78)\n\n        return True\n\n    print(\"=\" * 78)\n    print()\n    print(\" - Failed\")\n    print()\n    for test, error in errors.iteritems():\n        print(test.__name__)\n        print(\"    %s\" % error)\n    print()\n    print(\"=\" * 78)\n\n    return False", "response": "Validate compatibility with Pyblish QML"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_safemode_windows():\n\n    try:\n        import pyblish\n        import pyblish_qml\n        import PyQt5\n\n    except ImportError:\n        return sys.stderr.write(\n            \"Run this in a terminal with access to \"\n            \"the Pyblish libraries and PyQt5.\\n\")\n\n    template = r\"\"\"@echo off\n\n    :: Clear all environment variables\n\n    @echo off\n    if exist \".\\backup_env.bat\" del \".\\backup_env.bat\"\n    for /f \"tokens=1* delims==\" %%a in ('set') do (\n    echo set %%a=%%b>> .\\backup_env.bat\n    set %%a=\n    )\n\n    :: Set only the bare essentials\n\n    set PATH={PyQt5}\n    set PATH=%PATH%;{python}\n    set PYTHONPATH={pyblish}\n    set PYTHONPATH=%PYTHONPATH%;{pyblish_qml}\n    set PYTHONPATH=%PYTHONPATH%;{PyQt5}\n\n    set SystemRoot=C:\\Windows\n\n    :: Run Pyblish\n\n    python -m pyblish_qml\n\n    :: Restore environment\n    backup_env.bat\n\n    \"\"\"\n\n    values = {}\n    for lib in (pyblish, pyblish_qml, PyQt5):\n        values[lib.__name__] = os.path.dirname(os.path.dirname(lib.__file__))\n\n    values[\"python\"] = os.path.dirname(sys.executable)\n\n    with open(\"run.bat\", \"w\") as f:\n        print(\"Writing %s\" % template.format(**values))\n        f.write(template.format(**values))", "response": "Generate a batch file to run QML in safe - mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_python():\n    python = (\n        _state.get(\"pythonExecutable\") or\n\n        # Support for multiple executables.\n        next((\n            exe for exe in\n            os.getenv(\"PYBLISH_QML_PYTHON_EXECUTABLE\", \"\").split(os.pathsep)\n            if os.path.isfile(exe)), None\n        ) or\n\n        # Search PATH for executables.\n        which(\"python\") or\n        which(\"python3\")\n    )\n\n    if not python or not os.path.isfile(python):\n        raise ValueError(\"Could not locate Python executable.\")\n\n    return python", "response": "Search for Python automatically."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_pyqt5(python):\n    pyqt5 = (\n        _state.get(\"pyqt5\") or\n        os.getenv(\"PYBLISH_QML_PYQT5\")\n    )\n\n    # If not registered, ask Python for it explicitly\n    # This avoids having to expose PyQt5 on PYTHONPATH\n    # where it may otherwise get picked up by bystanders\n    # such as Python 2.\n    if not pyqt5:\n        try:\n            path = subprocess.check_output([\n                python, \"-c\",\n                \"import PyQt5, sys;\"\n                \"sys.stdout.write(PyQt5.__file__)\"\n\n                # Normally, the output is bytes.\n            ], universal_newlines=True)\n\n            pyqt5 = os.path.dirname(os.path.dirname(path))\n\n        except subprocess.CalledProcessError:\n            pass\n\n    return pyqt5", "response": "Search for PyQt5 automatically and return the path of the PyQt5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlocates program in PATH and return absolute path", "response": "def which(program):\n    \"\"\"Locate `program` in PATH\n\n    Arguments:\n        program (str): Name of program, e.g. \"python\"\n\n    \"\"\"\n\n    def is_exe(fpath):\n        if os.path.isfile(fpath) and os.access(fpath, os.X_OK):\n            return True\n        return False\n\n    for path in os.environ[\"PATH\"].split(os.pathsep):\n        for ext in os.getenv(\"PATHEXT\", \"\").split(os.pathsep):\n            fname = program + ext.lower()\n            abspath = os.path.join(path.strip('\"'), fname)\n\n            if is_exe(abspath):\n                return abspath\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlisten to stdout and stderr and dispatch the appropriate command.", "response": "def listen(self):\n        \"\"\"Listen to both stdout and stderr\n\n        We'll want messages of a particular origin and format to\n        cause QML to perform some action. Other messages are simply\n        forwarded, as they are expected to be plain print or error messages.\n\n        \"\"\"\n\n        def _listen():\n            \"\"\"This runs in a thread\"\"\"\n            HEADER = \"pyblish-qml:popen.request\"\n\n            for line in iter(self.popen.stdout.readline, b\"\"):\n\n                if six.PY3:\n                    line = line.decode(\"utf8\")\n\n                try:\n                    response = json.loads(line)\n\n                except Exception:\n                    # This must be a regular message.\n                    sys.stdout.write(line)\n\n                else:\n                    if (hasattr(response, \"get\") and\n                            response.get(\"header\") == HEADER):\n\n                        payload = response[\"payload\"]\n                        args = payload[\"args\"]\n\n                        func_name = payload[\"name\"]\n\n                        wrapper = _state.get(\"dispatchWrapper\",\n                                             default_wrapper)\n\n                        func = getattr(self.service, func_name)\n                        result = wrapper(func, *args)  # block..\n\n                        # Note(marcus): This is where we wait for the host to\n                        # finish. Technically, we could kill the GUI at this\n                        # point which would make the following commands throw\n                        # an exception. However, no host is capable of kill\n                        # the GUI whilst running a command. The host is locked\n                        # until finished, which means we are guaranteed to\n                        # always respond.\n\n                        data = json.dumps({\n                            \"header\": \"pyblish-qml:popen.response\",\n                            \"payload\": result\n                        })\n\n                        if six.PY3:\n                            data = data.encode(\"ascii\")\n\n                        self.popen.stdin.write(data + b\"\\n\")\n                        self.popen.stdin.flush()\n\n                    else:\n                        # In the off chance that a message\n                        # was successfully decoded as JSON,\n                        # but *wasn't* a request, just print it.\n                        sys.stdout.write(line)\n\n        if not self.listening:\n            self._start_pulse()\n\n            if self.modal:\n                _listen()\n            else:\n                thread = threading.Thread(target=_listen)\n                thread.daemon = True\n                thread.start()\n\n            self.listening = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends pulse to child process", "response": "def _start_pulse(self):\n        \"\"\"Send pulse to child process\n\n        Child process will run forever if parent process encounter such\n        failure that not able to kill child process.\n\n        This inform child process that server is still running and child\n        process will auto kill itself after server stop sending pulse\n        message.\n\n        \"\"\"\n\n        def _pulse():\n            start_time = time.time()\n\n            while True:\n                data = json.dumps({\"header\": \"pyblish-qml:server.pulse\"})\n\n                if six.PY3:\n                    data = data.encode(\"ascii\")\n\n                try:\n                    self.popen.stdin.write(data + b\"\\n\")\n                    self.popen.stdin.flush()\n                except IOError:\n                    break\n\n                # Send pulse every 5 seconds\n                time.sleep(5.0 - ((time.time() - start_time) % 5.0))\n\n        thread = threading.Thread(target=_pulse)\n        thread.daemon = True\n        thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the Qt - runtime and show the window", "response": "def main(demo=False, aschild=False, targets=[]):\n    \"\"\"Start the Qt-runtime and show the window\n\n    Arguments:\n        aschild (bool, optional): Run as child of parent process\n\n    \"\"\"\n\n    if aschild:\n        print(\"Starting pyblish-qml\")\n        compat.main()\n        app = Application(APP_PATH, targets)\n        app.listen()\n\n        print(\"Done, don't forget to call `show()`\")\n        return app.exec_()\n\n    else:\n        print(\"Starting pyblish-qml server..\")\n        service = ipc.service.MockService() if demo else ipc.service.Service()\n        server = ipc.server.Server(service, targets=targets)\n\n        proxy = ipc.server.Proxy(server)\n        proxy.show(settings.to_dict())\n\n        server.listen()\n        server.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef event(self, event):\n        if event.type() == QtCore.QEvent.Close:\n            modifiers = self.app.queryKeyboardModifiers()\n            shift_pressed = QtCore.Qt.ShiftModifier & modifiers\n            states = self.app.controller.states\n\n            if shift_pressed:\n                print(\"Force quitted..\")\n                self.app.controller.host.emit(\"pyblishQmlCloseForced\")\n                event.accept()\n\n            elif any(state in states for state in (\"ready\", \"finished\")):\n                self.app.controller.host.emit(\"pyblishQmlClose\")\n                event.accept()\n\n            else:\n                print(\"Not ready, hold SHIFT to force an exit\")\n                event.ignore()\n\n        return super(Window, self).event(event)", "response": "Allow GUI to be closed upon holding Shift"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay the current state machine.", "response": "def show(self, client_settings=None):\n        \"\"\"Display GUI\n\n        Once the QML interface has been loaded, use this\n        to display it.\n\n        Arguments:\n            port (int): Client asking to show GUI.\n            client_settings (dict, optional): Visual settings, see settings.py\n\n        \"\"\"\n        window = self.window\n\n        if client_settings:\n            # Apply client-side settings\n            settings.from_dict(client_settings)\n            window.setWidth(client_settings[\"WindowSize\"][0])\n            window.setHeight(client_settings[\"WindowSize\"][1])\n            window.setTitle(client_settings[\"WindowTitle\"])\n            window.setFramePosition(\n                QtCore.QPoint(\n                    client_settings[\"WindowPosition\"][0],\n                    client_settings[\"WindowPosition\"][1]\n                )\n            )\n\n        message = list()\n        message.append(\"Settings: \")\n        for key, value in settings.to_dict().items():\n            message.append(\"  %s = %s\" % (key, value))\n\n        print(\"\\n\".join(message))\n\n        window.requestActivate()\n        window.showNormal()\n\n        # Work-around for window appearing behind\n        # other windows upon being shown once hidden.\n        previous_flags = window.flags()\n        window.setFlags(previous_flags | QtCore.Qt.WindowStaysOnTopHint)\n        window.setFlags(previous_flags)\n\n        # Give statemachine enough time to boot up\n        if not any(state in self.controller.states\n                   for state in [\"ready\", \"finished\"]):\n            util.timer(\"ready\")\n\n            ready = QtTest.QSignalSpy(self.controller.ready)\n\n            count = len(ready)\n            ready.wait(1000)\n            if len(ready) != count + 1:\n                print(\"Warning: Could not enter ready state\")\n\n            util.timer_end(\"ready\", \"Awaited statemachine for %.2f ms\")\n\n        if client_settings:\n            self.controller.data['autoValidate'] = client_settings.get('autoValidate', False)\n            self.controller.data['autoPublish'] = client_settings.get('autoPublish', False)\n\n        self.controller.show.emit()\n\n        # Allow time for QML to initialise\n        util.schedule(self.controller.reset, 500, channel=\"main\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef inFocus(self):\n        previous_flags = self.window.flags()\n        self.window.setFlags(previous_flags |\n                             QtCore.Qt.WindowStaysOnTopHint)", "response": "Set GUI on - top flag"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef listen(self):\n\n        def _listen():\n            while True:\n                line = self.host.channels[\"parent\"].get()\n                payload = json.loads(line)[\"payload\"]\n\n                # We can't call methods directly, as we are running\n                # in a thread. Instead, we emit signals that do the\n                # job for us.\n                signal = {\n\n                    \"show\": \"shown\",\n                    \"hide\": \"hidden\",\n                    \"quit\": \"quitted\",\n                    \"publish\": \"published\",\n                    \"validate\": \"validated\",\n\n                    \"rise\": \"risen\",\n                    \"inFocus\": \"inFocused\",\n                    \"outFocus\": \"outFocused\",\n\n                }.get(payload[\"name\"])\n\n                if not signal:\n                    print(\"'{name}' was unavailable.\".format(\n                        **payload))\n                else:\n                    try:\n                        getattr(self, signal).emit(\n                            *payload.get(\"args\", []))\n                    except Exception:\n                        traceback.print_exc()\n\n        thread = threading.Thread(target=_listen)\n        thread.daemon = True\n        thread.start()", "response": "Listen on incoming messages from host\n            through client. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms operation in thread with callback Taxonomy", "response": "def defer(target, args=None, kwargs=None, callback=None):\n    \"\"\"Perform operation in thread with callback\n\n    Instances are cached until finished, at which point\n    they are garbage collected. If we didn't do this,\n    Python would step in and garbage collect the thread\n    before having had time to finish, resulting in an\n    exception.\n\n    Arguments:\n        target (callable): Method or function to call\n        callback (callable, optional): Method or function to call\n            once `target` has finished.\n\n    Returns:\n        None\n\n    \"\"\"\n\n    obj = _defer(target, args, kwargs, callback)\n    obj.finished.connect(lambda: _defer_cleanup(obj))\n    obj.start()\n    _defer_threads.append(obj)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscheduling a function to be run at a later time.", "response": "def schedule(func, time, channel=\"default\"):\n    \"\"\"Run `func` at a later `time` in a dedicated `channel`\n\n    Given an arbitrary function, call this function after a given\n    timeout. It will ensure that only one \"job\" is running within\n    the given channel at any one time and cancel any currently\n    running job if a new job is submitted before the timeout.\n\n    \"\"\"\n\n    try:\n        _jobs[channel].stop()\n    except (AttributeError, KeyError):\n        pass\n\n    timer = QtCore.QTimer()\n    timer.setSingleShot(True)\n    timer.timeout.connect(func)\n    timer.start(time)\n\n    _jobs[channel] = timer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nformat text into a sequence of text.", "response": "def format_text(text):\n    \"\"\"Remove newlines, but preserve paragraphs\"\"\"\n    result = \"\"\n    for paragraph in text.split(\"\\n\\n\"):\n        result += \" \".join(paragraph.split()) + \"\\n\\n\"\n\n    result = result.rstrip(\"\\n\")  # Remove last newlines\n\n    # converting links to HTML\n    pattern = r\"(https?:\\/\\/(?:w{1,3}.)?[^\\s]*?(?:\\.[a-z]+)+)\"\n    pattern += r\"(?![^<]*?(?:<\\/\\w+>|\\/?>))\"\n    if re.search(pattern, result):\n        html = r\"<a href='\\1'><font color='FF00CC'>\\1</font></a>\"\n        result = re.sub(pattern, html, result)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides exception handling for all slots", "response": "def SlotSentinel(*args):\n    \"\"\"Provides exception handling for all slots\"\"\"\n\n    # (NOTE) davidlatwe\n    # Thanks to this answer\n    # https://stackoverflow.com/questions/18740884\n\n    if len(args) == 0 or isinstance(args[0], types.FunctionType):\n        args = []\n\n    @QtCore.pyqtSlot(*args)\n    def slotdecorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                func(*args)\n            except Exception:\n                traceback.print_exc()\n        return wrapper\n\n    return slotdecorator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _byteify(data):\n\n    # Unicode\n    if isinstance(data, six.text_type):\n        return data.encode(\"utf-8\")\n\n    # Members of lists\n    if isinstance(data, list):\n        return [_byteify(item) for item in data]\n\n    # Members of dicts\n    if isinstance(data, dict):\n        return {\n            _byteify(key): _byteify(value) for key, value in data.items()\n        }\n\n    # Anything else, return the original form\n    return data", "response": "Convert unicode to bytes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process(self, plugin, context, instance=None, action=None):\n\n        plugin = plugin.to_json()\n        instance = instance.to_json() if instance is not None else None\n        return self._dispatch(\"process\", args=[plugin, instance, action])", "response": "Transmit a process request to host\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _self_destruct(self):\n        # This will give parent process 15 seconds to reset.\n        self._kill = threading.Timer(15, lambda: os._exit(0))\n        self._kill.start()", "response": "Auto quit exec if parent process failed\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _listen(self):\n\n        def _listen():\n            \"\"\"This runs in a thread\"\"\"\n            for line in iter(sys.stdin.readline, b\"\"):\n                try:\n                    response = json.loads(line)\n\n                except Exception as e:\n                    # The parent has passed on a message that\n                    # isn't formatted in any particular way.\n                    # This is likely a bug.\n                    raise e\n\n                else:\n                    if response.get(\"header\") == \"pyblish-qml:popen.response\":\n                        self.channels[\"response\"].put(line)\n\n                    elif response.get(\"header\") == \"pyblish-qml:popen.parent\":\n                        self.channels[\"parent\"].put(line)\n\n                    elif response.get(\"header\") == \"pyblish-qml:server.pulse\":\n                        self._kill.cancel()  # reset timer\n                        self._self_destruct()\n\n                    else:\n                        # The parent has passed on a message that\n                        # is JSON, but not in any format we recognise.\n                        # This is likely a bug.\n                        raise Exception(\"Unhandled message \"\n                                        \"passed to Popen, '%s'\" % line)\n\n        thread = threading.Thread(target=_listen)\n        thread.daemon = True\n        thread.start()", "response": "Listen for messages received from parent\n        and dispatch them to their corresponding handlers."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dispatch(self, func, args=None):\n\n        data = json.dumps(\n            {\n                \"header\": \"pyblish-qml:popen.request\",\n                \"payload\": {\n                    \"name\": func,\n                    \"args\": args or list(),\n                }\n            }\n        )\n\n        # This should never happen. Each request is immediately\n        # responded to, always. If it isn't the next line will block.\n        # If multiple responses were made, then this will fail.\n        # Both scenarios are bugs.\n        assert self.channels[\"response\"].empty(), (\n            \"There were pending messages in the response channel\")\n\n        sys.stdout.write(data + \"\\n\")\n        sys.stdout.flush()\n\n        try:\n            message = self.channels[\"response\"].get()\n\n            if six.PY3:\n                response = json.loads(message)\n            else:\n                response = _byteify(json.loads(message, object_hook=_byteify))\n\n        except TypeError as e:\n            raise e\n\n        else:\n            assert response[\"header\"] == \"pyblish-qml:popen.response\", response\n            return response[\"payload\"]", "response": "Send a message to the parent process\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a PluginProxy object from a dictionary.", "response": "def from_json(cls, plugin):\n        \"\"\"Build PluginProxy object from incoming dictionary\n\n        Emulate a plug-in by providing access to attributes\n        in the same way they are accessed using the remote object.\n        This allows for it to be used by members of :mod:`pyblish.logic`.\n\n        \"\"\"\n\n        process = None\n        repair = None\n\n        name = plugin[\"name\"] + \"Proxy\"\n        cls = type(name, (cls,), plugin)\n\n        # Emulate function\n        for name in (\"process\", \"repair\"):\n            args = \", \".join(plugin[\"process\"][\"args\"])\n            func = \"def {name}({args}): pass\".format(name=name,\n                                                     args=args)\n            exec(func)\n\n        cls.process = process\n        cls.repair = repair\n\n        cls.__orig__ = plugin\n\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Item(**kwargs):\n\n    parent = kwargs.pop(\"parent\", None)\n    cls = type(\"Item\", (AbstractItem,), kwargs.copy())\n\n    self = cls(parent)\n    self.json = kwargs  # Store as json\n\n    for key, value in kwargs.items():\n        if hasattr(self, key):\n            key = PropertyType.prefix + key\n        setattr(self, key, value)\n\n    return self", "response": "Factory function for QAbstractListModel items\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a new item to the model", "response": "def add_item(self, item):\n        \"\"\"Add new item to model\n\n        Each keyword argument is passed to the :func:Item\n        factory function.\n\n        \"\"\"\n\n        self.beginInsertRows(QtCore.QModelIndex(),\n                             self.rowCount(),\n                             self.rowCount())\n\n        item[\"parent\"] = self\n        item = Item(**item)\n        self.items.append(item)\n        self.endInsertRows()\n\n        item.__datachanged__.connect(self._dataChanged)\n\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_item(self, item):\n        index = self.items.index(item)\n        self.beginRemoveRows(QtCore.QModelIndex(), index, index)\n        self.items.remove(item)\n        self.endRemoveRows()", "response": "Removes the item from the model"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nappend a plugin to the internal model.", "response": "def add_plugin(self, plugin):\n        \"\"\"Append `plugin` to model\n\n        Arguments:\n            plugin (dict): Serialised plug-in from pyblish-rpc\n\n        Schema:\n            plugin.json\n\n        \"\"\"\n\n        item = {}\n        item.update(defaults[\"common\"])\n        item.update(defaults[\"plugin\"])\n\n        for member in [\"pre11\",\n                       \"name\",\n                       \"label\",\n                       \"optional\",\n                       \"category\",\n                       \"actions\",\n                       \"id\",\n                       \"order\",\n                       \"doc\",\n                       \"type\",\n                       \"module\",\n                       \"match\",\n                       \"hasRepair\",\n                       \"families\",\n                       \"contextEnabled\",\n                       \"instanceEnabled\",\n                       \"__instanceEnabled__\",\n                       \"path\"]:\n            item[member] = plugin[member]\n\n        # Visualised in Perspective\n        item[\"familiesConcatenated\"] = \", \".join(plugin[\"families\"])\n\n        # converting links to HTML\n        pattern = r\"(https?:\\/\\/(?:w{1,3}.)?[^\\s]*?(?:\\.[a-z]+)+)\"\n        pattern += r\"(?![^<]*?(?:<\\/\\w+>|\\/?>))\"\n        if item[\"doc\"] and re.search(pattern, item[\"doc\"]):\n            html = r\"<a href='\\1'><font color='FF00CC'>\\1</font></a>\"\n            item[\"doc\"] = re.sub(pattern, html, item[\"doc\"])\n\n        # Append GUI-only data\n        item[\"itemType\"] = \"plugin\"\n        item[\"hasCompatible\"] = True\n        item[\"isToggled\"] = plugin.get(\"active\", True)\n        item[\"verb\"] = {\n            \"Selector\": \"Collect\",\n            \"Collector\": \"Collect\",\n            \"Validator\": \"Validate\",\n            \"Extractor\": \"Extract\",\n            \"Integrator\": \"Integrate\",\n            \"Conformer\": \"Integrate\",\n        }.get(item[\"type\"], \"Other\")\n\n        for action in item[\"actions\"]:\n            if action[\"on\"] == \"all\":\n                item[\"actionsIconVisible\"] = True\n\n        self.add_section(item[\"verb\"])\n\n        item = self.add_item(item)\n        self.plugins.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_instance(self, instance):\n\n        assert isinstance(instance, dict)\n\n        item = defaults[\"common\"].copy()\n        item.update(defaults[\"instance\"])\n\n        item.update(instance[\"data\"])\n        item.update(instance)\n\n        item[\"itemType\"] = \"instance\"\n        item[\"isToggled\"] = instance[\"data\"].get(\"publish\", True)\n        item[\"hasCompatible\"] = True\n        item[\"category\"] = item[\"category\"] or item[\"family\"]\n\n        self.add_section(item[\"category\"])\n\n        # Visualised in Perspective\n        families = [instance[\"data\"][\"family\"]]\n        families.extend(instance[\"data\"].get(\"families\", []))\n        item[\"familiesConcatenated\"] += \", \".join(families)\n\n        item = self.add_item(item)\n        self.instances.append(item)", "response": "Append instance to the list of related items"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove instance from model", "response": "def remove_instance(self, item):\n        \"\"\"Remove `instance` from model\"\"\"\n        self.instances.remove(item)\n        self.remove_item(item)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_section(self, name):\n\n        assert isinstance(name, str)\n\n        # Skip existing sections\n        for section in self.sections:\n            if section.name == name:\n                return section\n\n        item = defaults[\"common\"].copy()\n        item[\"name\"] = name\n\n        item[\"itemType\"] = \"section\"\n\n        item = self.add_item(item)\n        self.sections.append(item)\n\n        return item", "response": "Append a section to the model"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nappends a context to the internal list of the related items.", "response": "def add_context(self, context, label=None):\n        \"\"\"Append `context` to model\n\n        Arguments:\n            context (dict): Serialised to add\n\n        Schema:\n            context.json\n\n        \"\"\"\n\n        assert isinstance(context, dict)\n\n        item = defaults[\"common\"].copy()\n        item.update(defaults[\"instance\"])\n        item.update(context)\n\n        item[\"family\"] = None\n        item[\"label\"] = context[\"data\"].get(\"label\") or settings.ContextLabel\n        item[\"itemType\"] = \"instance\"\n        item[\"isToggled\"] = True\n        item[\"optional\"] = False\n        item[\"hasCompatible\"] = True\n\n        item = self.add_item(item)\n        self.instances.append(item)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating item - model with result from host", "response": "def update_with_result(self, result):\n        \"\"\"Update item-model with result from host\n\n        State is sent from host after processing had taken place\n        and represents the events that took place; including\n        log messages and completion status.\n\n        Arguments:\n            result (dict): Dictionary following the Result schema\n\n        \"\"\"\n\n        assert isinstance(result, dict), \"%s is not a dictionary\" % result\n\n        for type in (\"instance\", \"plugin\"):\n            id = (result[type] or {}).get(\"id\")\n\n            is_context = not id\n            if is_context:\n                item = self.instances[0]\n            else:\n                item = self.items.get(id)\n\n            if item is None:\n                # If an item isn't there yet\n                # no worries. It's probably because\n                # reset is still running and the\n                # item in question is a new instance\n                # not yet added to the model.\n                continue\n\n            item.isProcessing = False\n            item.currentProgress = 1\n            item.processed = True\n            item.hasWarning = item.hasWarning or any([\n                record[\"levelno\"] == logging.WARNING\n                for record in result[\"records\"]\n            ])\n\n            if result.get(\"error\"):\n                item.hasError = True\n                item.amountFailed += 1\n\n            else:\n                item.succeeded = True\n                item.amountPassed += 1\n\n            item.duration += result[\"duration\"]\n            item.finishedAt = time.time()\n\n            if item.itemType == \"plugin\" and not item.actionsIconVisible:\n\n                actions = list(item.actions)\n\n                # Context specific actions\n                for action in list(actions):\n                    if action[\"on\"] == \"failed\" and not item.hasError:\n                        actions.remove(action)\n                    if action[\"on\"] == \"succeeded\" and not item.succeeded:\n                        actions.remove(action)\n                    if action[\"on\"] == \"processed\" and not item.processed:\n                        actions.remove(action)\n\n                if actions:\n                    item.actionsIconVisible = True\n\n            # Update section item\n            class DummySection(object):\n                hasWarning = False\n                hasError = False\n                succeeded = False\n\n            section_item = DummySection()\n            for section in self.sections:\n                if item.itemType == \"plugin\" and section.name == item.verb:\n                    section_item = section\n                if (item.itemType == \"instance\" and\n                        section.name == item.category):\n                    section_item = section\n\n            section_item.hasWarning = (\n                section_item.hasWarning or item.hasWarning\n            )\n            section_item.hasError = section_item.hasError or item.hasError\n            section_item.succeeded = section_item.succeeded or item.succeeded\n            section_item.isProcessing = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset status of items in the list.", "response": "def reset_status(self):\n        \"\"\"Reset progress bars\"\"\"\n        for item in self.items:\n            item.isProcessing = False\n            item.currentProgress = 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_exclusion(self, role, value):\n\n        self._add_rule(self.excludes, role, value)", "response": "Add an exclusion rule to the exclusion list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_exclusion(self, role, value=None):\n\n        self._remove_rule(self.excludes, role, value)", "response": "Removes the exclusion rule from the set of exclusion rules."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd an inclusion rule to the internal list of includes.", "response": "def add_inclusion(self, role, value):\n        \"\"\"Include item if `role` equals `value`\n\n        Attributes:\n            role (int): Qt role to compare `value` to\n            value (object): Value to exclude\n\n        \"\"\"\n\n        self._add_rule(self.includes, role, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove exclusion rule from the set of includes.", "response": "def remove_inclusion(self, role, value=None):\n        \"\"\"Remove exclusion rule\"\"\"\n        self._remove_rule(self.includes, role, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexclude items in self. includes and self. excludes", "response": "def filterAcceptsRow(self, source_row, source_parent):\n        \"\"\"Exclude items in `self.excludes`\"\"\"\n        model = self.sourceModel()\n        item = model.items[source_row]\n\n        key = getattr(item, \"filter\", None)\n        if key is not None:\n            regex = self.filterRegExp()\n            if regex.pattern():\n                match = regex.indexIn(key)\n                return False if match == -1 else True\n\n        for role, values in self.includes.items():\n            data = getattr(item, role, None)\n            if data not in values:\n                return False\n\n        for role, values in self.excludes.items():\n            data = getattr(item, role, None)\n            if data in values:\n                return False\n\n        return super(ProxyModel, self).filterAcceptsRow(\n            source_row, source_parent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_records(records):\n    formatted = list()\n    for record_ in records:\n        formatted.append(format_record(record_))\n    return formatted", "response": "Serialise multiple records into a single list of strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserialises a LogRecord instance and validate it.", "response": "def format_record(record):\n    \"\"\"Serialise LogRecord instance\"\"\"\n\n    record = dict(\n        (key, getattr(record, key, None))\n        for key in (\n            \"threadName\",\n            \"name\",\n            \"thread\",\n            \"created\",\n            \"process\",\n            \"processName\",\n            \"args\",\n            \"module\",\n            \"filename\",\n            \"levelno\",\n            \"exc_text\",\n            \"pathname\",\n            \"lineno\",\n            \"msg\",\n            \"exc_info\",\n            \"funcName\",\n            \"relativeCreated\",\n            \"levelname\",\n            \"msecs\")\n    )\n\n    # Humanise output and conform to Exceptions\n    record[\"message\"] = str(record.pop(\"msg\"))\n\n    if os.getenv(\"PYBLISH_SAFE\"):\n        schema.validate(record, \"record\")\n\n    return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserialises exception into a dictionary.", "response": "def format_error(error):\n    \"\"\"Serialise exception\"\"\"\n    formatted = {\"message\": str(error)}\n\n    if hasattr(error, \"traceback\"):\n        fname, line_no, func, exc = error.traceback\n        formatted.update({\n            \"fname\": fname,\n            \"line_number\": line_no,\n            \"func\": func,\n            \"exc\": exc\n        })\n\n    return formatted"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserialising instance into a dictionary that can be visualised and modified.", "response": "def format_instance(instance):\n    \"\"\"Serialise `instance`\n\n    For children to be visualised and modified,\n    they must provide an appropriate implementation\n    of __str__.\n\n    Data that isn't JSON compatible cannot be\n    visualised nor modified.\n\n    Attributes:\n        name (str): Name of instance\n        niceName (str, optional): Nice name of instance\n        family (str): Name of compatible family\n        data (dict, optional): Associated data\n        publish (bool): Whether or not instance should be published\n\n    Returns:\n        Dictionary of JSON-compatible instance\n\n    \"\"\"\n\n    instance = {\n        \"name\": instance.name,\n        \"id\": instance.id,\n        \"data\": format_data(instance.data),\n        \"children\": list(),\n    }\n\n    if os.getenv(\"PYBLISH_SAFE\"):\n        schema.validate(instance, \"instance\")\n\n    return instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_plugins(plugins):\n\n    formatted = []\n    for plugin_ in plugins:\n        formatted_plugin = format_plugin(plugin_)\n        formatted.append(formatted_plugin)\n\n    return formatted", "response": "Serialise multiple plug - in - free articles into JSON - compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserialising a plugin into a single object.", "response": "def format_plugin(plugin):\n    \"\"\"Serialise `plugin`\n\n    Attributes:\n        name: Name of Python class\n        id: Unique identifier\n        version: Plug-in version\n        category: Optional category\n        requires: Plug-in requirements\n        order: Plug-in order\n        optional: Is the plug-in optional?\n        doc: The plug-in documentation\n        hasRepair: Can the plug-in perform a repair?\n        hasCompatible: Does the plug-in have any compatible instances?\n        type: Which baseclass does the plug-in stem from? E.g. Validator\n        module: File in which plug-in was defined\n        contextEnabled: Does it process the Context?\n        instanceEnabled: Does it process Instance(s)?\n\n    \"\"\"\n\n    type = \"Other\"\n\n    for order, _type in {pyblish.plugin.CollectorOrder: \"Collector\",\n                         pyblish.plugin.ValidatorOrder: \"Validator\",\n                         pyblish.plugin.ExtractorOrder: \"Extractor\",\n                         pyblish.plugin.IntegratorOrder: \"Integrator\"}.items():\n        if pyblish.lib.inrange(plugin.order, base=order):\n            type = _type\n\n    module = plugin.__module__\n\n    if module == \"__main__\":\n        # Support for in-memory plug-ins.\n        path = \"mem:%s\" % plugin.__name__\n    else:\n        try:\n            path = os.path.abspath(sys.modules[module].__file__)\n        except Exception:\n            path = \"unknown\"\n\n    has_repair = False\n\n    args = inspect.getargspec(plugin.repair).args\n    if \"context\" in args or \"instance\" in args:\n        has_repair = True\n\n    # Legacy abilities\n    if hasattr(plugin, \"repair_context\") or hasattr(plugin, \"repair_instance\"):\n        has_repair = True\n\n    output = {\n        \"label\": plugin.label,\n        \"id\": plugin.id,\n        \"version\": plugin.version,\n        \"category\": getattr(plugin, \"category\", None),\n        \"requires\": plugin.requires,\n        \"order\": plugin.order,\n        \"optional\": plugin.optional,\n        \"hosts\": plugin.hosts,\n        \"families\": plugin.families,\n\n        # New in pyblish-base 1.5.2\n        \"targets\": getattr(plugin, \"targets\", list()),\n\n        \"doc\": inspect.getdoc(plugin),\n        \"active\": plugin.active,\n        \"match\": plugin.match,\n\n        # Metadata\n        \"__pre11__\": plugin.__pre11__,\n        \"__contextEnabled__\": plugin.__contextEnabled__,\n        \"__instanceEnabled__\": plugin.__instanceEnabled__,\n\n        \"path\": path,\n        \"pre11\": plugin.__pre11__,\n        \"contextEnabled\": plugin.__contextEnabled__,\n        \"instanceEnabled\": plugin.__instanceEnabled__,\n        \"name\": plugin.__name__,\n        \"type\": type,\n        \"module\": module,\n        \"hasRepair\": has_repair,\n        \"process\": {\n            \"args\": inspect.getargspec(plugin.process).args,\n        },\n        \"repair\": {\n            \"args\": inspect.getargspec(plugin.repair).args,\n        },\n\n        \"actions\": [format_action(a) for a in plugin.actions],\n    }\n\n    if os.getenv(\"PYBLISH_SAFE\"):\n        schema.validate(output, \"plugin\")\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iterator(plugins, context):\n    test = pyblish.logic.registered_test()\n    state = {\n        \"nextOrder\": None,\n        \"ordersWithError\": set()\n    }\n\n    for plugin in plugins:\n        state[\"nextOrder\"] = plugin.order\n\n        message = test(**state)\n        if message:\n            raise StopIteration(\"Stopped due to %s\" % message)\n\n        instances = pyblish.api.instances_by_plugin(context, plugin)\n        if plugin.__instanceEnabled__:\n            for instance in instances:\n                yield plugin, instance\n\n        else:\n            yield plugin, None", "response": "An iterator for plug - in and instance pairs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setup_statemachine(self):\n\n        machine = QtCore.QStateMachine()\n\n        #  _______________\n        # |               |\n        # |               |\n        # |               |\n        # |_______________|\n        #\n        group = util.QState(\"group\", QtCore.QState.ParallelStates, machine)\n\n        #  _______________\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________| - Parallell State\n        #\n        visibility = util.QState(\"visibility\", group)\n\n        hidden = util.QState(\"hidden\", visibility)\n        visible = util.QState(\"visible\", visibility)\n\n        #  _______________\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________| - Parallell State\n        #\n        operation = util.QState(\"operation\", group)\n\n        ready = util.QState(\"ready\", operation)\n\n        collecting = util.QState(\"collecting\", operation)\n        validating = util.QState(\"validating\", operation)\n        extracting = util.QState(\"extracting\", operation)\n        integrating = util.QState(\"integrating\", operation)\n\n        finished = util.QState(\"finished\", operation)\n        repairing = util.QState(\"repairing\", operation)\n        initialising = util.QState(\"initialising\", operation)\n        stopping = util.QState(\"stopping\", operation)\n        stopped = util.QState(\"stopped\", operation)\n        saving = util.QState(\"saving\", operation)\n\n        #  _______________\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________| - Parallell State\n        #\n        errored = util.QState(\"errored\", group)\n\n        clean = util.QState(\"clean\", errored)\n        dirty = util.QState(\"dirty\", errored)\n\n        #  _______________\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________| - Parallell State\n\n        # States that block the underlying GUI\n        suspended = util.QState(\"suspended\", group)\n\n        alive = util.QState(\"alive\", suspended)\n        acting = util.QState(\"acting\", suspended)\n        acted = QtCore.QHistoryState(operation)\n        acted.setDefaultState(ready)\n\n        #  _______________\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________|\n        # | ____     ____ |\n        # ||    |---|    ||\n        # ||____|---|____||\n        # |_______________|\n        #\n\n        hidden.addTransition(self.show, visible)\n        visible.addTransition(self.hide, hidden)\n\n        ready.addTransition(self.acting, acting)\n        ready.addTransition(self.validating, validating)\n        ready.addTransition(self.initialising, initialising)\n        ready.addTransition(self.repairing, repairing)\n        ready.addTransition(self.saving, saving)\n        saving.addTransition(self.saved, ready)\n\n        collecting.addTransition(self.initialised, ready)\n        collecting.addTransition(self.stopping, stopping)\n\n        validating.addTransition(self.stopping, stopping)\n        validating.addTransition(self.finished, finished)\n        validating.addTransition(self.extracting, extracting)\n\n        extracting.addTransition(self.stopping, stopping)\n        extracting.addTransition(self.finished, finished)\n        extracting.addTransition(self.integrating, integrating)\n\n        integrating.addTransition(self.stopping, stopping)\n        integrating.addTransition(self.finished, finished)\n\n        finished.addTransition(self.initialising, initialising)\n        finished.addTransition(self.acting, acting)\n        initialising.addTransition(self.collecting, collecting)\n        stopping.addTransition(self.acted, acted)\n        stopping.addTransition(self.finished, finished)\n\n        dirty.addTransition(self.initialising, clean)\n        clean.addTransition(self.changed, dirty)\n\n        alive.addTransition(self.acting, acting)\n        acting.addTransition(self.acted, acted)\n\n        # Set initial states\n        for compound, state in {machine: group,\n                                visibility: hidden,\n                                operation: ready,\n                                errored: clean,\n                                suspended: alive}.items():\n            compound.setInitialState(state)\n\n        # Make connections\n        for state in (hidden,\n                      visible,\n                      ready,\n                      collecting,\n                      validating,\n                      extracting,\n                      integrating,\n                      finished,\n                      repairing,\n                      initialising,\n                      stopping,\n                      saving,\n                      stopped,\n                      dirty,\n                      clean,\n                      acting,\n                      alive,\n                      acted):\n            state.entered.connect(\n                lambda state=state: self.state_changed.emit(state.name))\n\n        machine.start()\n        return machine", "response": "Setup and start state machine"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getPluginActions(self, index):\n\n        index = self.data[\"proxies\"][\"plugin\"].mapToSource(\n            self.data[\"proxies\"][\"plugin\"].index(\n                index, 0, QtCore.QModelIndex())).row()\n        item = self.data[\"models\"][\"item\"].items[index]\n\n        # Inject reference to the original index\n        actions = [\n            dict(action, **{\"index\": index})\n            for action in item.actions\n        ]\n\n        # Context specific actions\n        for action in list(actions):\n            if action[\"on\"] == \"failed\" and not item.hasError:\n                actions.remove(action)\n            if action[\"on\"] == \"succeeded\" and not item.succeeded:\n                actions.remove(action)\n            if action[\"on\"] == \"processed\" and not item.processed:\n                actions.remove(action)\n            if action[\"on\"] == \"notProcessed\" and item.processed:\n                actions.remove(action)\n\n        # Discard empty categories, separators\n        remaining_actions = list()\n        index = 0\n        try:\n            action = actions[index]\n        except IndexError:\n            pass\n        else:\n            while action:\n                try:\n                    action = actions[index]\n                except IndexError:\n                    break\n\n                isempty = False\n\n                if action[\"__type__\"] in (\"category\", \"separator\"):\n                    try:\n                        next_ = actions[index + 1]\n                        if next_[\"__type__\"] != \"action\":\n                            isempty = True\n                    except IndexError:\n                        isempty = True\n\n                if not isempty:\n                    remaining_actions.append(action)\n\n                index += 1\n\n        return remaining_actions", "response": "Return the actions from plug - in at index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexcluding a role of value at target", "response": "def exclude(self, target, operation, role, value):\n        \"\"\"Exclude a `role` of `value` at `target`\n\n        Arguments:\n            target (str): Destination proxy model\n            operation (str): \"add\" or \"remove\" exclusion\n            role (str): Role to exclude\n            value (str): Value of `role` to exclude\n\n        \"\"\"\n\n        target = {\"result\": self.data[\"proxies\"][\"result\"],\n                  \"instance\": self.data[\"proxies\"][\"instance\"],\n                  \"plugin\": self.data[\"proxies\"][\"plugin\"]}[target]\n\n        if operation == \"add\":\n            target.add_exclusion(role, value)\n\n        elif operation == \"remove\":\n            target.remove_exclusion(role, value)\n\n        else:\n            raise TypeError(\"operation must be either `add` or `remove`\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __item_data(self, model, index):\n        item = model.items[index]\n\n        data = {\n            \"name\": item.name,\n            \"data\": item.data,\n            \"doc\": getattr(item, \"doc\", None),\n            \"path\": getattr(item, \"path\", None),\n        }\n\n        return data", "response": "Return item data as dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating comments to host and notify subscribers", "response": "def comment_sync(self, comment):\n        \"\"\"Update comments to host and notify subscribers\"\"\"\n        self.host.update(key=\"comment\", value=comment)\n        self.host.emit(\"commented\", comment=comment)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_commenting(self, comment):\n\n        def update():\n            context = self.host.cached_context\n            context.data[\"comment\"] = comment\n            self.data[\"comment\"] = comment\n\n            # Notify subscribers of the comment\n            self.comment_sync(comment)\n\n            self.commented.emit()\n\n        # Update local cache a little later\n        util.schedule(update, 100, channel=\"commenting\")", "response": "The user is entering a comment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_about_to_process(self, plugin, instance):\n\n        if instance is None:\n            instance_item = self.data[\"models\"][\"item\"].instances[0]\n        else:\n            instance_item = self.data[\"models\"][\"item\"].instances[instance.id]\n\n        plugin_item = self.data[\"models\"][\"item\"].plugins[plugin.id]\n\n        for section in self.data[\"models\"][\"item\"].sections:\n            if section.name == plugin_item.verb:\n                section.isProcessing = True\n            if section.name == instance_item.category:\n                section.isProcessing = True\n\n        instance_item.isProcessing = True\n        plugin_item.isProcessing = True", "response": "Reflect currently running pair in GUI"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef publish(self):\n\n        def get_data():\n            model = self.data[\"models\"][\"item\"]\n\n            # Communicate with host to retrieve current plugins and instances\n            # This can potentially take a very long time; it is run\n            # asynchronously and initiates processing once complete.\n            host_plugins = dict((p.id, p) for p in self.host.cached_discover)\n            host_context = dict((i.id, i) for i in self.host.cached_context)\n\n            plugins = list()\n            instances = list()\n\n            for plugin in models.ItemIterator(model.plugins):\n\n                # Exclude Collectors\n                if pyblish.lib.inrange(\n                        number=plugin.order,\n                        base=pyblish.api.Collector.order):\n                    continue\n\n                plugins.append(host_plugins[plugin.id])\n\n            for instance in models.ItemIterator(model.instances):\n                instances.append(host_context[instance.id])\n\n            return plugins, instances\n\n        def on_data_received(args):\n            self.run(*args, callback=on_finished)\n\n        def on_finished():\n            self.host.emit(\"published\", context=None)\n\n        util.defer(get_data, callback=on_data_received)", "response": "Start asynchonous publishing of the currently available plug - in and instances."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, plugins, context, callback=None, callback_args=[]):\n\n        # if \"ready\" not in self.states:\n        #     return self.error.emit(\"Not ready\")\n\n        # Initial set-up\n        self.data[\"state\"][\"is_running\"] = True\n\n        # Setup statistics for better debugging.\n        # (To be finalised in `on_finished`)\n        util.timer(\"publishing\")\n        stats = {\"requestCount\": self.host.stats()[\"totalRequestCount\"]}\n\n        # For each completed task, update\n        # the GUI and commence next task.\n        def on_next(result):\n            if isinstance(result, StopIteration):\n                return on_finished(str(result))\n\n            self.data[\"models\"][\"item\"].update_with_result(result)\n            self.data[\"models\"][\"result\"].update_with_result(result)\n\n            # Once the main thread has finished updating\n            # the GUI, we can proceed handling of next task.\n            util.defer(self.host.context, callback=update_context)\n\n        def update_context(ctx):\n            item_model = self.data[\"models\"][\"item\"]\n            instance_items = {item.id: item for item in item_model.instances}\n            for instance in ctx:\n                id = instance.id\n                item = instance_items.get(id)\n                if item is not None:\n                    proxy = next((i for i in context if i.id == id), None)\n                    update_instance(item, proxy, instance.data)\n                    continue\n\n                context.append(instance)\n                item_model.add_instance(instance.to_json())\n\n            if len(ctx) < item_model.instance_count():\n                remove_instance(ctx, instance_items)\n\n            util.defer(lambda: next(iterator), callback=on_next)\n\n        def update_instance(item, proxy, data):\n            \"\"\"Update model and proxy for reflecting changes on instance\"\"\"\n\n            # Update instance item model data for GUI\n            item.isToggled = data.get(\"publish\", True)\n            item.optional = data.get(\"optional\", True)\n            item.category = data.get(\"category\", data[\"family\"])\n\n            families = [data[\"family\"]]\n            families.extend(data.get(\"families\", []))\n            item.familiesConcatenated = \", \".join(families)\n\n            if proxy is None:\n                return\n            # Update proxy instance data which currently being iterated in\n            # the primary iterator\n            proxy.data[\"publish\"] = data.get(\"publish\", True)\n            proxy.data[\"family\"] = data[\"family\"]\n            proxy.data[\"families\"] = data.get(\"families\", [])\n\n        def remove_instance(ctx, items):\n            \"\"\"Remove instance\"\"\"\n            instances = {i.id: i for i in context}\n            instance_ids = set(i.id for i in ctx)\n            instance_ids.add(ctx.id)\n            for id, item in items.items():\n                if id not in instance_ids:\n                    # Remove from model\n                    self.data[\"models\"][\"item\"].remove_instance(item)\n                    # Remove instance from list\n                    context.remove(instances[id])\n\n        def on_finished(message=None):\n            \"\"\"Locally running function\"\"\"\n            self.data[\"state\"][\"is_running\"] = False\n            self.finished.emit()\n\n            if message:\n                self.info.emit(message)\n\n            # Report statistics\n            stats[\"requestCount\"] -= self.host.stats()[\"totalRequestCount\"]\n            util.timer_end(\"publishing\", \"Spent %.2f ms resetting\")\n            util.echo(\"Made %i requests during publish.\"\n                      % abs(stats[\"requestCount\"]))\n\n            if callback:\n                callback(*callback_args)\n\n        # The iterator initiates processing and is\n        # executed one item at a time in a separate thread.\n        # Once the thread finishes execution, it signals\n        # the `callback`.\n        iterator = self.iterator(plugins, context)\n        util.defer(lambda: next(iterator), callback=on_next)", "response": "This method runs the asynchronous tasks in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies settings from dictionary to the current object", "response": "def from_dict(settings):\n    \"\"\"Apply settings from dictionary\n\n    Arguments:\n        settings (dict): Settings in the form of a dictionary\n\n    \"\"\"\n\n    assert isinstance(settings, dict), \"`settings` must be of type dict\"\n    for key, value in settings.items():\n        setattr(self, key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an object return True if the object is callable. For classes return True if the object is callable.", "response": "def _instance_callable(obj):\n    \"\"\"Given an object, return True if the object is callable.\n    For classes, return True if instances would be callable.\"\"\"\n    if not isinstance(obj, ClassTypes):\n        # already an instance\n        return getattr(obj, '__call__', None) is not None\n\n    klass = obj\n    # uses __bases__ instead of __mro__ so that we work with old style classes\n    if klass.__dict__.get('__call__') is not None:\n        return True\n\n    for base in klass.__bases__:\n        if _instance_callable(base):\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming multiple patches in a single call.", "response": "def _patch_multiple(target, spec=None, create=False, spec_set=None,\n                    autospec=None, new_callable=None, **kwargs):\n    \"\"\"Perform multiple patches in a single call. It takes the object to be\n    patched (either as an object or a string to fetch the object by importing)\n    and keyword arguments for the patches::\n\n        with patch.multiple(settings, FIRST_PATCH='one', SECOND_PATCH='two'):\n            ...\n\n    Use `DEFAULT` as the value if you want `patch.multiple` to create\n    mocks for you. In this case the created mocks are passed into a decorated\n    function by keyword, and a dictionary is returned when `patch.multiple` is\n    used as a context manager.\n\n    `patch.multiple` can be used as a decorator, class decorator or a context\n    manager. The arguments `spec`, `spec_set`, `create`,\n    `autospec` and `new_callable` have the same meaning as for `patch`. These\n    arguments will be applied to *all* patches done by `patch.multiple`.\n\n    When used as a class decorator `patch.multiple` honours `patch.TEST_PREFIX`\n    for choosing which methods to wrap.\n    \"\"\"\n    if type(target) in (unicode, str):\n        getter = lambda: _importer(target)\n    else:\n        getter = lambda: target\n\n    if not kwargs:\n        raise ValueError(\n            'Must supply at least one keyword argument with patch.multiple'\n        )\n    # need to wrap in a list for python 3, where items is a view\n    items = list(kwargs.items())\n    attribute, new = items[0]\n    patcher = _patch(\n        getter, attribute, new, spec, create, spec_set,\n        autospec, new_callable, {}\n    )\n    patcher.attribute_name = attribute\n    for attribute, new in items[1:]:\n        this_patcher = _patch(\n            getter, attribute, new, spec, create, spec_set,\n            autospec, new_callable, {}\n        )\n        this_patcher.attribute_name = attribute\n        patcher.additional_patchers.append(this_patcher)\n    return patcher"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef patch(\n        target, new=DEFAULT, spec=None, create=False,\n        spec_set=None, autospec=None, new_callable=None, **kwargs\n    ):\n    \"\"\"\n    `patch` acts as a function decorator, class decorator or a context\n    manager. Inside the body of the function or with statement, the `target`\n    is patched with a `new` object. When the function/with statement exits\n    the patch is undone.\n\n    If `new` is omitted, then the target is replaced with a\n    `MagicMock`. If `patch` is used as a decorator and `new` is\n    omitted, the created mock is passed in as an extra argument to the\n    decorated function. If `patch` is used as a context manager the created\n    mock is returned by the context manager.\n\n    `target` should be a string in the form `'package.module.ClassName'`. The\n    `target` is imported and the specified object replaced with the `new`\n    object, so the `target` must be importable from the environment you are\n    calling `patch` from. The target is imported when the decorated function\n    is executed, not at decoration time.\n\n    The `spec` and `spec_set` keyword arguments are passed to the `MagicMock`\n    if patch is creating one for you.\n\n    In addition you can pass `spec=True` or `spec_set=True`, which causes\n    patch to pass in the object being mocked as the spec/spec_set object.\n\n    `new_callable` allows you to specify a different class, or callable object,\n    that will be called to create the `new` object. By default `MagicMock` is\n    used.\n\n    A more powerful form of `spec` is `autospec`. If you set `autospec=True`\n    then the mock with be created with a spec from the object being replaced.\n    All attributes of the mock will also have the spec of the corresponding\n    attribute of the object being replaced. Methods and functions being\n    mocked will have their arguments checked and will raise a `TypeError` if\n    they are called with the wrong signature. For mocks replacing a class,\n    their return value (the 'instance') will have the same spec as the class.\n\n    Instead of `autospec=True` you can pass `autospec=some_object` to use an\n    arbitrary object as the spec instead of the one being replaced.\n\n    By default `patch` will fail to replace attributes that don't exist. If\n    you pass in `create=True`, and the attribute doesn't exist, patch will\n    create the attribute for you when the patched function is called, and\n    delete it again afterwards. This is useful for writing tests against\n    attributes that your production code creates at runtime. It is off by by\n    default because it can be dangerous. With it switched on you can write\n    passing tests against APIs that don't actually exist!\n\n    Patch can be used as a `TestCase` class decorator. It works by\n    decorating each test method in the class. This reduces the boilerplate\n    code when your test methods share a common patchings set. `patch` finds\n    tests by looking for method names that start with `patch.TEST_PREFIX`.\n    By default this is `test`, which matches the way `unittest` finds tests.\n    You can specify an alternative prefix by setting `patch.TEST_PREFIX`.\n\n    Patch can be used as a context manager, with the with statement. Here the\n    patching applies to the indented block after the with statement. If you\n    use \"as\" then the patched object will be bound to the name after the\n    \"as\"; very useful if `patch` is creating a mock object for you.\n\n    `patch` takes arbitrary keyword arguments. These will be passed to\n    the `Mock` (or `new_callable`) on construction.\n\n    `patch.dict(...)`, `patch.multiple(...)` and `patch.object(...)` are\n    available for alternate use-cases.\n    \"\"\"\n    getter, attribute = _get_target(target)\n    return _patch(\n        getter, attribute, new, spec, create,\n        spec_set, autospec, new_callable, kwargs\n    )", "response": "Patch the object in the specified target with the specified spec and spec_set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nturns a callable object ( like a mock ) into a real function", "response": "def _get_method(name, func):\n    \"Turns a callable object (like a mock) into a real function\"\n    def method(self, *args, **kw):\n        return func(self, *args, **kw)\n    method.__name__ = name\n    return method"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_autospec(spec, spec_set=False, instance=False, _parent=None,\n                    _name=None, **kwargs):\n    \"\"\"Create a mock object using another object as a spec. Attributes on the\n    mock will use the corresponding attribute on the `spec` object as their\n    spec.\n\n    Functions or methods being mocked will have their arguments checked\n    to check that they are called with the correct signature.\n\n    If `spec_set` is True then attempting to set attributes that don't exist\n    on the spec object will raise an `AttributeError`.\n\n    If a class is used as a spec then the return value of the mock (the\n    instance of the class) will have the same spec. You can use a class as the\n    spec for an instance object by passing `instance=True`. The returned mock\n    will only be callable if instances of the mock are callable.\n\n    `create_autospec` also takes arbitrary keyword arguments that are passed to\n    the constructor of the created mock.\"\"\"\n    if _is_list(spec):\n        # can't pass a list instance to the mock constructor as it will be\n        # interpreted as a list of strings\n        spec = type(spec)\n\n    is_type = isinstance(spec, ClassTypes)\n\n    _kwargs = {'spec': spec}\n    if spec_set:\n        _kwargs = {'spec_set': spec}\n    elif spec is None:\n        # None we mock with a normal mock without a spec\n        _kwargs = {}\n\n    _kwargs.update(kwargs)\n\n    Klass = MagicMock\n    if type(spec) in DescriptorTypes:\n        # descriptors don't have a spec\n        # because we don't know what type they return\n        _kwargs = {}\n    elif not _callable(spec):\n        Klass = NonCallableMagicMock\n    elif is_type and instance and not _instance_callable(spec):\n        Klass = NonCallableMagicMock\n\n    _new_name = _name\n    if _parent is None:\n        # for a top level object no _new_name should be set\n        _new_name = ''\n\n    mock = Klass(parent=_parent, _new_parent=_parent, _new_name=_new_name,\n                 name=_name, **_kwargs)\n\n    if isinstance(spec, FunctionTypes):\n        # should only happen at the top level because we don't\n        # recurse for functions\n        mock = _set_signature(mock, spec)\n    else:\n        _check_signature(spec, mock, is_type, instance)\n\n    if _parent is not None and not instance:\n        _parent._mock_children[_name] = mock\n\n    if is_type and not instance and 'return_value' not in kwargs:\n        mock.return_value = create_autospec(spec, spec_set, instance=True,\n                                            _name='()', _parent=mock)\n\n    for entry in dir(spec):\n        if _is_magic(entry):\n            # MagicMock already does the useful magic methods for us\n            continue\n\n        if isinstance(spec, FunctionTypes) and entry in FunctionAttributes:\n            # allow a mock to actually be a function\n            continue\n\n        # XXXX do we need a better way of getting attributes without\n        # triggering code execution (?) Probably not - we need the actual\n        # object to mock it so we would rather trigger a property than mock\n        # the property descriptor. Likewise we want to mock out dynamically\n        # provided attributes.\n        # XXXX what about attributes that raise exceptions other than\n        # AttributeError on being fetched?\n        # we could be resilient against it, or catch and propagate the\n        # exception when the attribute is fetched from the mock\n        try:\n            original = getattr(spec, entry)\n        except AttributeError:\n            continue\n\n        kwargs = {'spec': original}\n        if spec_set:\n            kwargs = {'spec_set': original}\n\n        if not isinstance(original, FunctionTypes):\n            new = _SpecState(original, spec_set, mock, entry, instance)\n            mock._mock_children[entry] = new\n        else:\n            parent = mock\n            if isinstance(spec, FunctionTypes):\n                parent = mock.mock\n\n            new = MagicMock(parent=parent, name=entry, _new_name=entry,\n                            _new_parent=parent, **kwargs)\n            mock._mock_children[entry] = new\n            skipfirst = _must_skip(spec, entry, is_type)\n            _check_signature(original, new, skipfirst=skipfirst)\n\n        # so functions created with _set_signature become instance attributes,\n        # *plus* their underlying mock exists in _mock_children of the parent\n        # mock. Adding to _mock_children may be unnecessary where we are also\n        # setting as an instance attribute?\n        if isinstance(new, FunctionTypes):\n            setattr(mock, entry, new)\n\n    return mock", "response": "Create a new object using another object as a spec."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mock_open(mock=None, read_data=''):\n    global file_spec\n    if file_spec is None:\n        # set on first use\n        if inPy3k:\n            import _io\n            file_spec = list(set(dir(_io.TextIOWrapper)).union(set(dir(_io.BytesIO))))\n        else:\n            file_spec = file\n\n    if mock is None:\n        mock = MagicMock(name='open', spec=open)\n\n    handle = MagicMock(spec=file_spec)\n    handle.write.return_value = None\n    handle.__enter__.return_value = handle\n    handle.read.return_value = read_data\n\n    mock.return_value = handle\n    return mock", "response": "A helper function to create a mock to replace the use of open. It creates a mock object for use by the context manager."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nattach a mock as an attribute of this one replacing its name and the mock parent.", "response": "def attach_mock(self, mock, attribute):\n        \"\"\"\n        Attach a mock as an attribute of this one, replacing its name and\n        parent. Calls to the attached mock will be recorded in the\n        `method_calls` and `mock_calls` attributes of this one.\"\"\"\n        mock._mock_parent = None\n        mock._mock_new_parent = None\n        mock._mock_name = ''\n        mock._mock_new_name = None\n\n        setattr(self, attribute, mock)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_mock(self):\n        \"Restore the mock object to its initial state.\"\n        self.called = False\n        self.call_args = None\n        self.call_count = 0\n        self.mock_calls = _CallList()\n        self.call_args_list = _CallList()\n        self.method_calls = _CallList()\n\n        for child in self._mock_children.values():\n            if isinstance(child, _SpecState):\n                continue\n            child.reset_mock()\n\n        ret = self._mock_return_value\n        if _is_instance_mock(ret) and ret is not self:\n            ret.reset_mock()", "response": "Restore the mock object to its initial state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure_mock(self, **kwargs):\n        for arg, val in sorted(kwargs.items(),\n                               # we sort on the number of dots so that\n                               # attributes are set before we set attributes on\n                               # attributes\n                               key=lambda entry: entry[0].count('.')):\n            args = arg.split('.')\n            final = args.pop()\n            obj = self\n            for entry in args:\n                obj = getattr(obj, entry)\n            setattr(obj, final, val)", "response": "Configure attributes on the child object for the specified child objects."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef assert_called_with(_mock_self, *args, **kwargs):\n        self = _mock_self\n        if self.call_args is None:\n            expected = self._format_mock_call_signature(args, kwargs)\n            raise AssertionError('Expected call: %s\\nNot called' % (expected,))\n\n        if self.call_args != (args, kwargs):\n            msg = self._format_mock_failure_message(args, kwargs)\n            raise AssertionError(msg)", "response": "assert that the mock was called with the specified arguments."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassert the mock has been called with the specified calls.", "response": "def assert_has_calls(self, calls, any_order=False):\n        \"\"\"assert the mock has been called with the specified calls.\n        The `mock_calls` list is checked for the calls.\n\n        If `any_order` is False (the default) then the calls must be\n        sequential. There can be extra calls before or after the\n        specified calls.\n\n        If `any_order` is True then the calls can be in any order, but\n        they must all appear in `mock_calls`.\"\"\"\n        if not any_order:\n            if calls not in self.mock_calls:\n                raise AssertionError(\n                    'Calls not found.\\nExpected: %r\\n'\n                    'Actual: %r' % (calls, self.mock_calls)\n                )\n            return\n\n        all_calls = list(self.mock_calls)\n\n        not_found = []\n        for kall in calls:\n            try:\n                all_calls.remove(kall)\n            except ValueError:\n                not_found.append(kall)\n        if not_found:\n            raise AssertionError(\n                '%r not all found in call list' % (tuple(not_found),)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nassert the mock has been called with the specified arguments.", "response": "def assert_any_call(self, *args, **kwargs):\n        \"\"\"assert the mock has been called with the specified arguments.\n\n        The assert passes if the mock has *ever* been called, unlike\n        `assert_called_with` and `assert_called_once_with` that only pass if\n        the call is the most recent one.\"\"\"\n        kall = call(*args, **kwargs)\n        if kall not in self.call_args_list:\n            expected_string = self._format_mock_call_signature(args, kwargs)\n            raise AssertionError(\n                '%s call not found' % expected_string\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the child mocks for attributes and return value.", "response": "def _get_child_mock(self, **kw):\n        \"\"\"Create the child mocks for attributes and return value.\n        By default child mocks will be the same type as the parent.\n        Subclasses of Mock may want to override this to customize the way\n        child mocks are made.\n\n        For non-callable mocks the callable variant will be used (rather than\n        any custom subclass).\"\"\"\n        _type = type(self)\n        if not issubclass(_type, CallableMixin):\n            if issubclass(_type, NonCallableMagicMock):\n                klass = MagicMock\n            elif issubclass(_type, NonCallableMock) :\n                klass = Mock\n        else:\n            klass = _type.__mro__[1]\n        return klass(**kw)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self):\n        result = self.__enter__()\n        self._active_patches.add(self)\n        return result", "response": "Activate a patch returning any created mock."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a spec to a mock.", "response": "def mock_add_spec(self, spec, spec_set=False):\n        \"\"\"Add a spec to a mock. `spec` can either be an object or a\n        list of strings. Only attributes on the `spec` can be fetched as\n        attributes from the mock.\n\n        If `spec_set` is True then only attributes on the spec can be set.\"\"\"\n        self._mock_add_spec(spec, spec_set)\n        self._mock_set_magics()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_list(self):\n        vals = []\n        thing = self\n        while thing is not None:\n            if thing.from_kall:\n                vals.append(thing)\n            thing = thing.parent\n        return _CallList(reversed(vals))", "response": "For a call object that represents multiple calls returns a list of all the intermediate calls as well as the\n        final call."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrapinstance(func, ptr, base=None):\n\n    assert isinstance(ptr, long), \"Argument 'ptr' must be of type <long>\"\n    assert (base is None) or issubclass(base, Qt.QtCore.QObject), (\n        \"Argument 'base' must be of type <QObject>\")\n\n    if base is None:\n        q_object = func(long(ptr), Qt.QtCore.QObject)\n        meta_object = q_object.metaObject()\n        class_name = meta_object.className()\n        super_class_name = meta_object.superClass().className()\n\n        if hasattr(Qt.QtWidgets, class_name):\n            base = getattr(Qt.QtWidgets, class_name)\n\n        elif hasattr(Qt.QtWidgets, super_class_name):\n            base = getattr(Qt.QtWidgets, super_class_name)\n\n        else:\n            base = Qt.QtCore.QObject\n\n    return func(long(ptr), base)", "response": "Wrap a function in a new instance of the most suitable class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _reassign_misplaced_members(binding):\n\n    for src, dst in _misplaced_members[binding].items():\n        src_module, src_member = src.split(\".\")\n        dst_module, dst_member = dst.split(\".\")\n\n        try:\n            src_object = getattr(Qt, dst_module)\n        except AttributeError:\n            # Skip reassignment of non-existing members.\n            # This can happen if a request was made to\n            # rename a member that didn't exist, for example\n            # if QtWidgets isn't available on the target platform.\n            continue\n\n        dst_value = getattr(getattr(Qt, \"_\" + src_module), src_member)\n\n        setattr(\n            src_object,\n            dst_member,\n            dst_value\n        )", "response": "Applies misplaced members from binding to Qt. py\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pyside2():\n\n    import PySide2 as module\n    _setup(module, [\"QtUiTools\"])\n\n    Qt.__binding_version__ = module.__version__\n\n    try:\n        try:\n            # Before merge of PySide and shiboken\n            import shiboken2\n        except ImportError:\n            # After merge of PySide and shiboken, May 2017\n            from PySide2 import shiboken2\n\n        Qt.QtCompat.wrapInstance = (\n            lambda ptr, base=None: _wrapinstance(\n                shiboken2.wrapInstance, ptr, base)\n        )\n        Qt.QtCompat.getCppPointer = lambda object: \\\n            shiboken2.getCppPointer(object)[0]\n\n    except ImportError:\n        pass  # Optional\n\n    if hasattr(Qt, \"_QtUiTools\"):\n        Qt.QtCompat.loadUi = _loadUi\n\n    if hasattr(Qt, \"_QtCore\"):\n        Qt.__qt_version__ = Qt._QtCore.qVersion()\n        Qt.QtCompat.qInstallMessageHandler = _qInstallMessageHandler\n        Qt.QtCompat.translate = Qt._QtCore.QCoreApplication.translate\n\n    if hasattr(Qt, \"_QtWidgets\"):\n        Qt.QtCompat.setSectionResizeMode = \\\n            Qt._QtWidgets.QHeaderView.setSectionResizeMode\n\n    _reassign_misplaced_members(\"PySide2\")\n    _build_compatibility_members(\"PySide2\")", "response": "Initialise PySide2\n\n    These functions serve to test the existence of a binding\n    along with set it up in such a way that it aligns with\n    the final step; adding members from the original binding\n    to Qt.py"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pyqt4():\n\n    import sip\n\n    # Validation of envivornment variable. Prevents an error if\n    # the variable is invalid since it's just a hint.\n    try:\n        hint = int(QT_SIP_API_HINT)\n    except TypeError:\n        hint = None  # Variable was None, i.e. not set.\n    except ValueError:\n        raise ImportError(\"QT_SIP_API_HINT=%s must be a 1 or 2\")\n\n    for api in (\"QString\",\n                \"QVariant\",\n                \"QDate\",\n                \"QDateTime\",\n                \"QTextStream\",\n                \"QTime\",\n                \"QUrl\"):\n        try:\n            sip.setapi(api, hint or 2)\n        except AttributeError:\n            raise ImportError(\"PyQt4 < 4.6 isn't supported by Qt.py\")\n        except ValueError:\n            actual = sip.getapi(api)\n            if not hint:\n                raise ImportError(\"API version already set to %d\" % actual)\n            else:\n                # Having provided a hint indicates a soft constraint, one\n                # that doesn't throw an exception.\n                sys.stderr.write(\n                    \"Warning: API '%s' has already been set to %d.\\n\"\n                    % (api, actual)\n                )\n\n    import PyQt4 as module\n    _setup(module, [\"uic\"])\n\n    try:\n        import sip\n        Qt.QtCompat.wrapInstance = (\n            lambda ptr, base=None: _wrapinstance(\n                sip.wrapinstance, ptr, base)\n        )\n        Qt.QtCompat.getCppPointer = lambda object: \\\n            sip.unwrapinstance(object)\n\n    except ImportError:\n        pass  # Optional\n\n    if hasattr(Qt, \"_uic\"):\n        Qt.QtCompat.loadUi = _loadUi\n\n    if hasattr(Qt, \"_QtGui\"):\n        setattr(Qt, \"QtWidgets\", _new_module(\"QtWidgets\"))\n        setattr(Qt, \"_QtWidgets\", Qt._QtGui)\n        if hasattr(Qt._QtGui, \"QX11Info\"):\n            setattr(Qt, \"QtX11Extras\", _new_module(\"QtX11Extras\"))\n            Qt.QtX11Extras.QX11Info = Qt._QtGui.QX11Info\n\n        Qt.QtCompat.setSectionResizeMode = \\\n            Qt._QtGui.QHeaderView.setResizeMode\n\n    if hasattr(Qt, \"_QtCore\"):\n        Qt.__binding_version__ = Qt._QtCore.PYQT_VERSION_STR\n        Qt.__qt_version__ = Qt._QtCore.QT_VERSION_STR\n        QCoreApplication = Qt._QtCore.QCoreApplication\n        Qt.QtCompat.qInstallMessageHandler = _qInstallMessageHandler\n        Qt.QtCompat.translate = (\n            lambda context, sourceText, disambiguation, n:\n            QCoreApplication.translate(\n                context,\n                sourceText,\n                disambiguation,\n                QCoreApplication.CodecForTr,\n                n)\n        )\n\n    _reassign_misplaced_members(\"PyQt4\")\n\n    # QFileDialog QtCompat decorator\n    def _standardizeQFileDialog(some_function):\n        \"\"\"Decorator that makes PyQt4 return conform to other bindings\"\"\"\n        def wrapper(*args, **kwargs):\n            ret = (some_function(*args, **kwargs))\n\n            # PyQt4 only returns the selected filename, force it to a\n            # standard return of the selected filename, and a empty string\n            # for the selected filter\n            return ret, ''\n\n        wrapper.__doc__ = some_function.__doc__\n        wrapper.__name__ = some_function.__name__\n\n        return wrapper\n\n    decorators = {\n        \"QFileDialog\": {\n            \"getOpenFileName\": _standardizeQFileDialog,\n            \"getOpenFileNames\": _standardizeQFileDialog,\n            \"getSaveFileName\": _standardizeQFileDialog,\n        }\n    }\n    _build_compatibility_members('PyQt4', decorators)", "response": "Initialise PyQt4 with the necessary modules and modules."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _loadUi(uifile, baseinstance=None):\n\n    if hasattr(Qt, \"_uic\"):\n        return Qt._uic.loadUi(uifile, baseinstance)\n\n    elif hasattr(Qt, \"_QtUiTools\"):\n        # Implement `PyQt5.uic.loadUi` for PySide(2)\n\n        class _UiLoader(Qt._QtUiTools.QUiLoader):\n            \"\"\"Create the user interface in a base instance.\n\n            Unlike `Qt._QtUiTools.QUiLoader` itself this class does not\n            create a new instance of the top-level widget, but creates the user\n            interface in an existing instance of the top-level class if needed.\n\n            This mimics the behaviour of `PyQt5.uic.loadUi`.\n\n            \"\"\"\n\n            def __init__(self, baseinstance):\n                super(_UiLoader, self).__init__(baseinstance)\n                self.baseinstance = baseinstance\n\n            def load(self, uifile, *args, **kwargs):\n                from xml.etree.ElementTree import ElementTree\n\n                # For whatever reason, if this doesn't happen then\n                # reading an invalid or non-existing .ui file throws\n                # a RuntimeError.\n                etree = ElementTree()\n                etree.parse(uifile)\n\n                widget = Qt._QtUiTools.QUiLoader.load(\n                    self, uifile, *args, **kwargs)\n\n                # Workaround for PySide 1.0.9, see issue #208\n                widget.parentWidget()\n\n                return widget\n\n            def createWidget(self, class_name, parent=None, name=\"\"):\n                \"\"\"Called for each widget defined in ui file\n\n                Overridden here to populate `baseinstance` instead.\n\n                \"\"\"\n\n                if parent is None and self.baseinstance:\n                    # Supposed to create the top-level widget,\n                    # return the base instance instead\n                    return self.baseinstance\n\n                # For some reason, Line is not in the list of available\n                # widgets, but works fine, so we have to special case it here.\n                if class_name in self.availableWidgets() + [\"Line\"]:\n                    # Create a new widget for child widgets\n                    widget = Qt._QtUiTools.QUiLoader.createWidget(self,\n                                                                  class_name,\n                                                                  parent,\n                                                                  name)\n\n                else:\n                    raise Exception(\"Custom widget '%s' not supported\"\n                                    % class_name)\n\n                if self.baseinstance:\n                    # Set an attribute for the new child widget on the base\n                    # instance, just like PyQt5.uic.loadUi does.\n                    setattr(self.baseinstance, name, widget)\n\n                return widget\n\n        widget = _UiLoader(baseinstance).load(uifile)\n        Qt.QtCore.QMetaObject.connectSlotsByName(widget)\n\n        return widget\n\n    else:\n        raise NotImplementedError(\"No implementation available for loadUi\")", "response": "Dynamically load a user interface from the given uifile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a dispatch wrapper for servers", "response": "def register_dispatch_wrapper(wrapper):\n    \"\"\"Register a dispatch wrapper for servers\n\n    The wrapper must have this exact signature:\n        (func, *args, **kwargs)\n\n    \"\"\"\n\n    signature = inspect.getargspec(wrapper)\n    if any([len(signature.args) != 1,\n            signature.varargs is None,\n            signature.keywords is None]):\n        raise TypeError(\"Wrapper signature mismatch\")\n\n    def _wrapper(func, *args, **kwargs):\n        \"\"\"Exception handling\"\"\"\n        try:\n            return wrapper(func, *args, **kwargs)\n        except Exception as e:\n            # Kill subprocess\n            _state[\"currentServer\"].stop()\n            traceback.print_exc()\n            raise e\n\n    _state[\"dispatchWrapper\"] = _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef install(modal):\n\n    if _state.get(\"installed\"):\n        sys.stdout.write(\"Already installed, uninstalling..\\n\")\n        uninstall()\n\n    use_threaded_wrapper = not modal\n\n    install_callbacks()\n    install_host(use_threaded_wrapper)\n\n    _state[\"installed\"] = True", "response": "Perform first time install"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to show a new GUI and return the server object.", "response": "def show(parent=None, targets=[], modal=None, auto_publish=False, auto_validate=False):\n    \"\"\"Attempt to show GUI\n\n    Requires install() to have been run first, and\n    a live instance of Pyblish QML in the background.\n\n    Arguments:\n        parent (None, optional): Deprecated\n        targets (list, optional): Publishing targets\n        modal (bool, optional): Block interactions to parent\n\n    \"\"\"\n\n    # Get modal mode from environment\n    if modal is None:\n        modal = bool(os.environ.get(\"PYBLISH_QML_MODAL\", False))\n\n    # Automatically install if not already installed.\n    install(modal)\n\n    show_settings = settings.to_dict()\n    show_settings['autoPublish'] = auto_publish\n    show_settings['autoValidate'] = auto_validate\n\n    # Show existing GUI\n    if _state.get(\"currentServer\"):\n        server = _state[\"currentServer\"]\n        proxy = ipc.server.Proxy(server)\n\n        try:\n            proxy.show(show_settings)\n            return server\n\n        except IOError:\n            # The running instance has already been closed.\n            _state.pop(\"currentServer\")\n\n    if not host.is_headless():\n        host.splash()\n\n    try:\n        service = ipc.service.Service()\n        server = ipc.server.Server(service, targets=targets, modal=modal)\n    except Exception:\n        # If for some reason, the GUI fails to show.\n        traceback.print_exc()\n        return host.desplash()\n\n    proxy = ipc.server.Proxy(server)\n    proxy.show(show_settings)\n\n    # Store reference to server for future calls\n    _state[\"currentServer\"] = server\n\n    log.info(\"Success. QML server available as \"\n             \"pyblish_qml.api.current_server()\")\n\n    server.listen()\n\n    return server"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install_host(use_threaded_wrapper):\n\n    for install in (_install_maya,\n                    _install_houdini,\n                    _install_nuke,\n                    _install_nukeassist,\n                    _install_hiero,\n                    _install_nukestudio,\n                    _install_blender):\n        try:\n            install(use_threaded_wrapper)\n        except ImportError:\n            pass\n        else:\n            break", "response": "Install required components into supported hosts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_googleapiclient():\n\n    keyword = \"googleapiclient\"\n\n    # reconstruct python paths\n    python_paths = os.environ[\"PYTHONPATH\"].split(os.pathsep)\n    paths = [path for path in python_paths if keyword not in path]\n    os.environ[\"PYTHONPATH\"] = os.pathsep.join(paths)", "response": "Check if the compatibility must be maintained\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _install_maya(use_threaded_wrapper):\n    from maya import utils, cmds\n\n    def threaded_wrapper(func, *args, **kwargs):\n        return utils.executeInMainThreadWithResult(\n            func, *args, **kwargs)\n\n    sys.stdout.write(\"Setting up Pyblish QML in Maya\\n\")\n\n    if cmds.about(version=True) == \"2018\":\n        _remove_googleapiclient()\n\n    _common_setup(\"Maya\", threaded_wrapper, use_threaded_wrapper)", "response": "Helper function to Autodesk Maya support"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _install_houdini(use_threaded_wrapper):\n    import hdefereval\n\n    def threaded_wrapper(func, *args, **kwargs):\n        return hdefereval.executeInMainThreadWithResult(\n            func, *args, **kwargs)\n\n    _common_setup(\"Houdini\", threaded_wrapper, use_threaded_wrapper)", "response": "Helper function to SideFx Houdini support"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _install_hiero(use_threaded_wrapper):\n    import hiero\n    import nuke\n\n    if \"--hiero\" not in nuke.rawArgs:\n        raise ImportError\n\n    def threaded_wrapper(func, *args, **kwargs):\n        return hiero.core.executeInMainThreadWithResult(\n            func, args, kwargs)\n\n    _common_setup(\"Hiero\", threaded_wrapper, use_threaded_wrapper)", "response": "Helper function to The Foundry Hiero support"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef install(self, host):\n        print(\"Installing..\")\n        if self._state[\"installed\"]:\n            return\n\n        if self.is_headless():\n            log.info(\"Headless host\")\n            return\n\n        print(\"aboutToQuit..\")\n        self.app.aboutToQuit.connect(self._on_application_quit)\n\n        if host == \"Maya\":\n            print(\"Maya host..\")\n            window = {\n                widget.objectName(): widget\n                for widget in self.app.topLevelWidgets()\n            }[\"MayaWindow\"]\n\n        else:\n            window = self.find_window()\n\n        # Install event filter\n        print(\"event filter..\")\n        event_filter = self.EventFilter(window)\n        window.installEventFilter(event_filter)\n\n        for signal in SIGNALS_TO_REMOVE_EVENT_FILTER:\n            pyblish.api.register_callback(signal, self.uninstall)\n\n        log.info(\"Installed event filter\")\n\n        self.window = window\n        self._state[\"installed\"] = True\n        self._state[\"eventFilter\"] = event_filter", "response": "Installs the given host and returns the unique ID of the host."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_window(self):\n        window = self.app.activeWindow()\n\n        while True:\n            parent_window = window.parent()\n            if parent_window:\n                window = parent_window\n            else:\n                break\n\n        return window", "response": "Find the top window in host"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tab_should_insert_whitespace():\n    b = get_app().current_buffer\n    before_cursor = b.document.current_line_before_cursor\n\n    return bool(b.text and (not before_cursor or before_cursor.isspace()))", "response": "Return True if tab key is pressed and insert whitespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads key bindings for python_input.", "response": "def load_python_bindings(python_input):\n    \"\"\"\n    Custom key bindings.\n    \"\"\"\n    bindings = KeyBindings()\n\n    sidebar_visible = Condition(lambda: python_input.show_sidebar)\n    handle = bindings.add\n\n    @handle('c-l')\n    def _(event):\n        \"\"\"\n        Clear whole screen and render again -- also when the sidebar is visible.\n        \"\"\"\n        event.app.renderer.clear()\n\n    @handle('c-z')\n    def _(event):\n        \"\"\"\n        Suspend.\n        \"\"\"\n        if python_input.enable_system_bindings:\n            event.app.suspend_to_background()\n\n    @handle('f2')\n    def _(event):\n        \"\"\"\n        Show/hide sidebar.\n        \"\"\"\n        python_input.show_sidebar = not python_input.show_sidebar\n        if python_input.show_sidebar:\n            event.app.layout.focus(python_input.ptpython_layout.sidebar)\n        else:\n            event.app.layout.focus_last()\n\n    @handle('f3')\n    def _(event):\n        \"\"\"\n        Select from the history.\n        \"\"\"\n        python_input.enter_history()\n\n    @handle('f4')\n    def _(event):\n        \"\"\"\n        Toggle between Vi and Emacs mode.\n        \"\"\"\n        python_input.vi_mode = not python_input.vi_mode\n\n    @handle('f6')\n    def _(event):\n        \"\"\"\n        Enable/Disable paste mode.\n        \"\"\"\n        python_input.paste_mode = not python_input.paste_mode\n\n    @handle('tab', filter= ~sidebar_visible & ~has_selection & tab_should_insert_whitespace)\n    def _(event):\n        \"\"\"\n        When tab should insert whitespace, do that instead of completion.\n        \"\"\"\n        event.app.current_buffer.insert_text('    ')\n\n    @Condition\n    def is_multiline():\n        return document_is_multiline_python(python_input.default_buffer.document)\n\n    @handle('enter', filter= ~sidebar_visible & ~has_selection &\n            (vi_insert_mode | emacs_insert_mode) &\n            has_focus(DEFAULT_BUFFER) & ~is_multiline)\n    @handle(Keys.Escape, Keys.Enter, filter= ~sidebar_visible & emacs_mode)\n    def _(event):\n        \"\"\"\n        Accept input (for single line input).\n        \"\"\"\n        b = event.current_buffer\n\n        if b.validate():\n            # When the cursor is at the end, and we have an empty line:\n            # drop the empty lines, but return the value.\n            b.document = Document(\n                text=b.text.rstrip(),\n                cursor_position=len(b.text.rstrip()))\n\n            b.validate_and_handle()\n\n    @handle('enter', filter= ~sidebar_visible & ~has_selection &\n            (vi_insert_mode | emacs_insert_mode) &\n            has_focus(DEFAULT_BUFFER) & is_multiline)\n    def _(event):\n        \"\"\"\n        Behaviour of the Enter key.\n\n        Auto indent after newline/Enter.\n        (When not in Vi navigaton mode, and when multiline is enabled.)\n        \"\"\"\n        b = event.current_buffer\n        empty_lines_required = python_input.accept_input_on_enter or 10000\n\n        def at_the_end(b):\n            \"\"\" we consider the cursor at the end when there is no text after\n            the cursor, or only whitespace. \"\"\"\n            text = b.document.text_after_cursor\n            return text == '' or (text.isspace() and not '\\n' in text)\n\n        if python_input.paste_mode:\n            # In paste mode, always insert text.\n            b.insert_text('\\n')\n\n        elif at_the_end(b) and b.document.text.replace(' ', '').endswith(\n                    '\\n' * (empty_lines_required - 1)):\n            # When the cursor is at the end, and we have an empty line:\n            # drop the empty lines, but return the value.\n            if b.validate():\n                b.document = Document(\n                    text=b.text.rstrip(),\n                    cursor_position=len(b.text.rstrip()))\n\n                b.validate_and_handle()\n        else:\n            auto_newline(b)\n\n    @handle('c-d', filter=~sidebar_visible &\n            has_focus(python_input.default_buffer) &\n            Condition(lambda:\n                # The current buffer is empty.\n                not get_app().current_buffer.text))\n    def _(event):\n        \"\"\"\n        Override Control-D exit, to ask for confirmation.\n        \"\"\"\n        if python_input.confirm_exit:\n            python_input.show_exit_confirmation = True\n        else:\n            event.app.exit(exception=EOFError)\n\n    @handle('c-c', filter=has_focus(python_input.default_buffer))\n    def _(event):\n        \" Abort when Control-C has been pressed. \"\n        event.app.exit(exception=KeyboardInterrupt, style='class:aborting')\n\n    return bindings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_sidebar_bindings(python_input):\n    bindings = KeyBindings()\n\n    handle = bindings.add\n    sidebar_visible = Condition(lambda: python_input.show_sidebar)\n\n    @handle('up', filter=sidebar_visible)\n    @handle('c-p', filter=sidebar_visible)\n    @handle('k', filter=sidebar_visible)\n    def _(event):\n        \" Go to previous option. \"\n        python_input.selected_option_index = (\n            (python_input.selected_option_index - 1) % python_input.option_count)\n\n    @handle('down', filter=sidebar_visible)\n    @handle('c-n', filter=sidebar_visible)\n    @handle('j', filter=sidebar_visible)\n    def _(event):\n        \" Go to next option. \"\n        python_input.selected_option_index = (\n            (python_input.selected_option_index + 1) % python_input.option_count)\n\n    @handle('right', filter=sidebar_visible)\n    @handle('l', filter=sidebar_visible)\n    @handle(' ', filter=sidebar_visible)\n    def _(event):\n        \" Select next value for current option. \"\n        option = python_input.selected_option\n        option.activate_next()\n\n    @handle('left', filter=sidebar_visible)\n    @handle('h', filter=sidebar_visible)\n    def _(event):\n        \" Select previous value for current option. \"\n        option = python_input.selected_option\n        option.activate_previous()\n\n    @handle('c-c', filter=sidebar_visible)\n    @handle('c-d', filter=sidebar_visible)\n    @handle('c-d', filter=sidebar_visible)\n    @handle('enter', filter=sidebar_visible)\n    @handle('escape', filter=sidebar_visible)\n    def _(event):\n        \" Hide sidebar. \"\n        python_input.show_sidebar = False\n        event.app.layout.focus_last()\n\n    return bindings", "response": "Load bindings for the navigation in the sidebar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_confirm_exit_bindings(python_input):\n    bindings = KeyBindings()\n\n    handle = bindings.add\n    confirmation_visible = Condition(lambda: python_input.show_exit_confirmation)\n\n    @handle('y', filter=confirmation_visible)\n    @handle('Y', filter=confirmation_visible)\n    @handle('enter', filter=confirmation_visible)\n    @handle('c-d', filter=confirmation_visible)\n    def _(event):\n        \"\"\"\n        Really quit.\n        \"\"\"\n        event.app.exit(exception=EOFError, style='class:exiting')\n\n    @handle(Keys.Any, filter=confirmation_visible)\n    def _(event):\n        \"\"\"\n        Cancel exit.\n        \"\"\"\n        python_input.show_exit_confirmation = False\n\n    return bindings", "response": "Load the bindings for the confirm exit button."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef auto_newline(buffer):\n    insert_text = buffer.insert_text\n\n    if buffer.document.current_line_after_cursor:\n        # When we are in the middle of a line. Always insert a newline.\n        insert_text('\\n')\n    else:\n        # Go to new line, but also add indentation.\n        current_line = buffer.document.current_line_before_cursor.rstrip()\n        insert_text('\\n')\n\n        # Unident if the last line ends with 'pass', remove four spaces.\n        unindent = current_line.rstrip().endswith(' pass')\n\n        # Copy whitespace from current line\n        current_line2 = current_line[4:] if unindent else current_line\n\n        for c in current_line2:\n            if c.isspace():\n                insert_text(c)\n            else:\n                break\n\n        # If the last line ends with a colon, add four extra spaces.\n        if current_line[-1:] == ':':\n            for x in range(4):\n                insert_text(' ')", "response": "r Automatically appends a new line to the current line."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the grammar for matching paths inside strings inside PythonCOOKIE code.", "response": "def _path_completer_grammar(self):\n        \"\"\"\n        Return the grammar for matching paths inside strings inside Python\n        code.\n        \"\"\"\n        # We make this lazy, because it delays startup time a little bit.\n        # This way, the grammar is build during the first completion.\n        if self._path_completer_grammar_cache is None:\n            self._path_completer_grammar_cache = self._create_path_completer_grammar()\n        return self._path_completer_grammar_cache"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_config(repl, config_file='~/.ptpython/config.py'):\n    assert isinstance(repl, PythonInput)\n    assert isinstance(config_file, six.text_type)\n\n    # Expand tildes.\n    config_file = os.path.expanduser(config_file)\n\n    def enter_to_continue():\n         six.moves.input('\\nPress ENTER to continue...')\n\n    # Check whether this file exists.\n    if not os.path.exists(config_file):\n        print('Impossible to read %r' % config_file)\n        enter_to_continue()\n        return\n\n    # Run the config file in an empty namespace.\n    try:\n        namespace = {}\n\n        with open(config_file, 'rb') as f:\n            code = compile(f.read(), config_file, 'exec')\n            six.exec_(code, namespace, namespace)\n\n        # Now we should have a 'configure' method in this namespace. We call this\n        # method with the repl as an argument.\n        if 'configure' in namespace:\n            namespace['configure'](repl)\n\n    except Exception:\n         traceback.print_exc()\n         enter_to_continue()", "response": "Execute REPL config file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nembedding a new instance of the current instance of the current instance of the current class.", "response": "def embed(globals=None, locals=None, configure=None,\n          vi_mode=False, history_filename=None, title=None,\n          startup_paths=None, patch_stdout=False, return_asyncio_coroutine=False):\n    \"\"\"\n    Call this to embed  Python shell at the current point in your program.\n    It's similar to `IPython.embed` and `bpython.embed`. ::\n\n        from prompt_toolkit.contrib.repl import embed\n        embed(globals(), locals())\n\n    :param vi_mode: Boolean. Use Vi instead of Emacs key bindings.\n    :param configure: Callable that will be called with the `PythonRepl` as a first\n                      argument, to trigger configuration.\n    :param title: Title to be displayed in the terminal titlebar. (None or string.)\n    \"\"\"\n    assert configure is None or callable(configure)\n\n    # Default globals/locals\n    if globals is None:\n        globals = {\n            '__name__': '__main__',\n            '__package__': None,\n            '__doc__': None,\n            '__builtins__': six.moves.builtins,\n        }\n\n    locals = locals or globals\n\n    def get_globals():\n        return globals\n\n    def get_locals():\n        return locals\n\n    # Create eventloop.\n    if return_asyncio_coroutine:\n        use_asyncio_event_loop()\n\n    # Create REPL.\n    repl = PythonRepl(get_globals=get_globals, get_locals=get_locals, vi_mode=vi_mode,\n                      history_filename=history_filename, startup_paths=startup_paths)\n\n    if title:\n        repl.terminal_title = title\n\n    if configure:\n        configure(repl)\n\n    app = repl.app\n\n    # Start repl.\n    patch_context = patch_stdout_context() if patch_stdout else DummyContext()\n\n    if return_asyncio_coroutine: # XXX\n        def coroutine():\n            with patch_context:\n                while True:\n                    iterator = iter(app.run_async().to_asyncio_future())\n                    try:\n                        while True:\n                            yield next(iterator)\n                    except StopIteration as exc:\n                        text = exc.args[0]\n                    repl._process_text(text)\n        return coroutine()\n    else:\n        with patch_context:\n            repl.run()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstarting the Read - Eval - Print Loop.", "response": "def _load_start_paths(self):\n        \" Start the Read-Eval-Print Loop. \"\n        if self._startup_paths:\n            for path in self._startup_paths:\n                if os.path.exists(path):\n                    with open(path, 'rb') as f:\n                        code = compile(f.read(), path, 'exec')\n                        six.exec_(code, self.get_globals(), self.get_locals())\n                else:\n                    output = self.app.output\n                    output.write('WARNING | File not found: {}\\n\\n'.format(path))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _execute(self, line):\n        output = self.app.output\n\n        # WORKAROUND: Due to a bug in Jedi, the current directory is removed\n        # from sys.path. See: https://github.com/davidhalter/jedi/issues/1148\n        if '' not in sys.path:\n            sys.path.insert(0, '')\n\n        def compile_with_flags(code, mode):\n            \" Compile code with the right compiler flags. \"\n            return compile(code, '<stdin>', mode,\n                           flags=self.get_compiler_flags(),\n                           dont_inherit=True)\n\n        if line.lstrip().startswith('\\x1a'):\n            # When the input starts with Ctrl-Z, quit the REPL.\n            self.app.exit()\n\n        elif line.lstrip().startswith('!'):\n            # Run as shell command\n            os.system(line[1:])\n        else:\n            # Try eval first\n            try:\n                code = compile_with_flags(line, 'eval')\n                result = eval(code, self.get_globals(), self.get_locals())\n\n                locals = self.get_locals()\n                locals['_'] = locals['_%i' % self.current_statement_index] = result\n\n                if result is not None:\n                    out_prompt = self.get_output_prompt()\n\n                    try:\n                        result_str = '%r\\n' % (result, )\n                    except UnicodeDecodeError:\n                        # In Python 2: `__repr__` should return a bytestring,\n                        # so to put it in a unicode context could raise an\n                        # exception that the 'ascii' codec can't decode certain\n                        # characters. Decode as utf-8 in that case.\n                        result_str = '%s\\n' % repr(result).decode('utf-8')\n\n                    # Align every line to the first one.\n                    line_sep = '\\n' + ' ' * fragment_list_width(out_prompt)\n                    result_str = line_sep.join(result_str.splitlines()) + '\\n'\n\n                    # Write output tokens.\n                    if self.enable_syntax_highlighting:\n                        formatted_output = merge_formatted_text([\n                            out_prompt,\n                            PygmentsTokens(list(_lex_python_result(result_str))),\n                        ])\n                    else:\n                        formatted_output = FormattedText(\n                                out_prompt + [('', result_str)])\n\n                    print_formatted_text(\n                        formatted_output, style=self._current_style,\n                        style_transformation=self.style_transformation,\n                        include_default_pygments_style=False)\n\n            # If not a valid `eval` expression, run using `exec` instead.\n            except SyntaxError:\n                code = compile_with_flags(line, 'exec')\n                six.exec_(code, self.get_globals(), self.get_locals())\n\n            output.flush()", "response": "Evaluate the line and print the result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating the input for Python 2. 7 and Python 3.", "response": "def validate(self, document):\n        \"\"\"\n        Check input for Python syntax errors.\n        \"\"\"\n        # When the input starts with Ctrl-Z, always accept. This means EOF in a\n        # Python REPL.\n        if document.text.startswith('\\x1a'):\n            return\n\n        try:\n            if self.get_compiler_flags:\n                flags = self.get_compiler_flags()\n            else:\n                flags = 0\n\n            compile(document.text, '<input>', 'exec', flags=flags, dont_inherit=True)\n        except SyntaxError as e:\n            # Note, the 'or 1' for offset is required because Python 2.7\n            # gives `None` as offset in case of '4=4' as input. (Looks like\n            # fixed in Python 3.)\n            index = document.translate_row_col_to_index(e.lineno - 1,  (e.offset or 1) - 1)\n            raise ValidationError(index, 'Syntax Error')\n        except TypeError as e:\n            # e.g. \"compile() expected string without null bytes\"\n            raise ValidationError(0, str(e))\n        except ValueError as e:\n            # In Python 2, compiling \"\\x9\" (an invalid escape sequence) raises\n            # ValueError instead of SyntaxError.\n            raise ValidationError(0, 'Syntax Error: %s' % e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(port=8222):\n    loop = asyncio.get_event_loop()\n\n    # Namespace exposed in the REPL.\n    environ = {'hello': 'world'}\n\n    # Start SSH server.\n    def create_server():\n        return MySSHServer(lambda: environ)\n\n    print('Listening on :%i' % port)\n    print('To connect, do \"ssh localhost -p %i\"' % port)\n\n    loop.run_until_complete(\n        asyncssh.create_server(create_server, '', port,\n                               server_host_keys=['/etc/ssh/ssh_host_dsa_key']))\n\n    # Run eventloop.\n    loop.run_forever()", "response": "Example that starts the REPL through an SSH server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_size(self):\n        if self._chan is None:\n            return Size(rows=20, columns=79)\n        else:\n            width, height, pixwidth, pixheight = self._chan.get_terminal_size()\n            return Size(rows=height, columns=width)", "response": "Get the size of the current channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connection_made(self, chan):\n        self._chan = chan\n\n        # Run REPL interface.\n        f = asyncio.ensure_future(self.cli.run_async())\n\n        # Close channel when done.\n        def done(_):\n            chan.close()\n            self._chan = None\n        f.add_done_callback(done)", "response": "Called when a connection is made."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _print(self, *data, **kw):\n        # Pop keyword-only arguments. (We cannot use the syntax from the\n        # signature. Otherwise, Python2 will give a syntax error message when\n        # installing.)\n        sep = kw.pop('sep', ' ')\n        end = kw.pop('end', '\\n')\n        _ = kw.pop('file', None)\n        assert not kw, 'Too many keyword-only arguments'\n\n        data = sep.join(map(str, data))\n        self._chan.write(data + end)", "response": "A convenience method that prints to the SSH channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_unclosed_brackets(text):\n    stack = []\n\n    # Ignore braces inside strings\n    text = re.sub(r'''('[^']*'|\"[^\"]*\")''', '', text)  # XXX: handle escaped quotes.!\n\n    for c in reversed(text):\n        if c in '])}':\n            stack.append(c)\n\n        elif c in '[({':\n            if stack:\n                if ((c == '[' and stack[-1] == ']') or\n                        (c == '{' and stack[-1] == '}') or\n                        (c == '(' and stack[-1] == ')')):\n                    stack.pop()\n            else:\n                # Opening bracket for which we didn't had a closing one.\n                return True\n\n    return False", "response": "Return True if the string contains an unclosed bracket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef document_is_multiline_python(document):\n    def ends_in_multiline_string():\n        \"\"\"\n        ``True`` if we're inside a multiline string at the end of the text.\n        \"\"\"\n        delims = _multiline_string_delims.findall(document.text)\n        opening = None\n        for delim in delims:\n            if opening is None:\n                opening = delim\n            elif delim == opening:\n                opening = None\n        return bool(opening)\n\n    if '\\n' in document.text or ends_in_multiline_string():\n        return True\n\n    def line_ends_with_colon():\n        return document.current_line.rstrip()[-1:] == ':'\n\n    # If we just typed a colon, or still have open brackets, always insert a real newline.\n    if line_ends_with_colon() or \\\n            (document.is_cursor_at_the_end and\n             has_unclosed_brackets(document.text_before_cursor)) or \\\n            document.text.startswith('@'):\n        return True\n\n    # If the character before the cursor is a backslash (line continuation\n    # char), insert a new line.\n    elif document.text_before_cursor[-1:] == '\\\\':\n        return True\n\n    return False", "response": "Determines whether the given document is a multiline Python document."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_popup_window(title, body):\n    assert isinstance(title, six.text_type)\n    assert isinstance(body, Container)\n    return Frame(body=body, title=title)", "response": "Create a Popup Window."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays / hide help.", "response": "def _toggle_help(history):\n    \" Display/hide help. \"\n    help_buffer_control = history.history_layout.help_buffer_control\n\n    if history.app.layout.current_control == help_buffer_control:\n        history.app.layout.focus_previous()\n    else:\n        history.app.layout.current_control = help_buffer_control"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntoggle focus between left / right window.", "response": "def _select_other_window(history):\n    \" Toggle focus between left/right window. \"\n    current_buffer = history.app.current_buffer\n    layout = history.history_layout.layout\n\n    if current_buffer == history.history_buffer:\n        layout.current_control = history.history_layout.default_buffer_control\n\n    elif current_buffer == history.default_buffer:\n        layout.current_control = history.history_layout.history_buffer_control"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a KeyBindings object for the given history.", "response": "def create_key_bindings(history, python_input, history_mapping):\n    \"\"\"\n    Key bindings.\n    \"\"\"\n    bindings = KeyBindings()\n    handle = bindings.add\n\n    @handle(' ', filter=has_focus(history.history_buffer))\n    def _(event):\n        \"\"\"\n        Space: select/deselect line from history pane.\n        \"\"\"\n        b = event.current_buffer\n        line_no = b.document.cursor_position_row\n\n        if not history_mapping.history_lines:\n            # If we've no history, then nothing to do\n            return\n\n        if line_no in history_mapping.selected_lines:\n            # Remove line.\n            history_mapping.selected_lines.remove(line_no)\n            history_mapping.update_default_buffer()\n        else:\n            # Add line.\n            history_mapping.selected_lines.add(line_no)\n            history_mapping.update_default_buffer()\n\n            # Update cursor position\n            default_buffer = history.default_buffer\n            default_lineno = sorted(history_mapping.selected_lines).index(line_no) + \\\n                history_mapping.result_line_offset\n            default_buffer.cursor_position = \\\n                default_buffer.document.translate_row_col_to_index(default_lineno, 0)\n\n        # Also move the cursor to the next line. (This way they can hold\n        # space to select a region.)\n        b.cursor_position = b.document.translate_row_col_to_index(line_no + 1, 0)\n\n    @handle(' ', filter=has_focus(DEFAULT_BUFFER))\n    @handle('delete', filter=has_focus(DEFAULT_BUFFER))\n    @handle('c-h', filter=has_focus(DEFAULT_BUFFER))\n    def _(event):\n        \"\"\"\n        Space: remove line from default pane.\n        \"\"\"\n        b = event.current_buffer\n        line_no = b.document.cursor_position_row - history_mapping.result_line_offset\n\n        if line_no >= 0:\n            try:\n                history_lineno = sorted(history_mapping.selected_lines)[line_no]\n            except IndexError:\n                pass  # When `selected_lines` is an empty set.\n            else:\n                history_mapping.selected_lines.remove(history_lineno)\n\n            history_mapping.update_default_buffer()\n\n    help_focussed = has_focus(history.help_buffer)\n    main_buffer_focussed = has_focus(history.history_buffer) | has_focus(history.default_buffer)\n\n    @handle('tab', filter=main_buffer_focussed)\n    @handle('c-x', filter=main_buffer_focussed, eager=True)\n        # Eager: ignore the Emacs [Ctrl-X Ctrl-X] binding.\n    @handle('c-w', filter=main_buffer_focussed)\n    def _(event):\n        \" Select other window. \"\n        _select_other_window(history)\n\n    @handle('f4')\n    def _(event):\n        \" Switch between Emacs/Vi mode. \"\n        python_input.vi_mode = not python_input.vi_mode\n\n    @handle('f1')\n    def _(event):\n        \" Display/hide help. \"\n        _toggle_help(history)\n\n    @handle('enter', filter=help_focussed)\n    @handle('c-c', filter=help_focussed)\n    @handle('c-g', filter=help_focussed)\n    @handle('escape', filter=help_focussed)\n    def _(event):\n        \" Leave help. \"\n        event.app.layout.focus_previous()\n\n    @handle('q', filter=main_buffer_focussed)\n    @handle('f3', filter=main_buffer_focussed)\n    @handle('c-c', filter=main_buffer_focussed)\n    @handle('c-g', filter=main_buffer_focussed)\n    def _(event):\n        \" Cancel and go back. \"\n        event.app.exit(result=None)\n\n    @handle('enter', filter=main_buffer_focussed)\n    def _(event):\n        \" Accept input. \"\n        event.app.exit(result=history.default_buffer.text)\n\n    enable_system_bindings = Condition(lambda: python_input.enable_system_bindings)\n\n    @handle('c-z', filter=enable_system_bindings)\n    def _(event):\n        \" Suspend to background. \"\n        event.app.suspend_to_background()\n\n    return bindings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Document instance that contains the resulting text.", "response": "def get_new_document(self, cursor_pos=None):\n        \"\"\"\n        Create a `Document` instance that contains the resulting text.\n        \"\"\"\n        lines = []\n\n        # Original text, before cursor.\n        if self.original_document.text_before_cursor:\n            lines.append(self.original_document.text_before_cursor)\n\n        # Selected entries from the history.\n        for line_no in sorted(self.selected_lines):\n            lines.append(self.history_lines[line_no])\n\n        # Original text, after cursor.\n        if self.original_document.text_after_cursor:\n            lines.append(self.original_document.text_after_cursor)\n\n        # Create `Document` with cursor at the right position.\n        text = '\\n'.join(lines)\n        if cursor_pos is not None and cursor_pos > len(text):\n            cursor_pos = len(text)\n        return Document(text, cursor_pos)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _default_buffer_pos_changed(self, _):\n        # Only when this buffer has the focus.\n        if self.app.current_buffer == self.default_buffer:\n            try:\n                line_no = self.default_buffer.document.cursor_position_row - \\\n                    self.history_mapping.result_line_offset\n\n                if line_no < 0:  # When the cursor is above the inserted region.\n                    raise IndexError\n\n                history_lineno = sorted(self.history_mapping.selected_lines)[line_no]\n            except IndexError:\n                pass\n            else:\n                self.history_buffer.cursor_position = \\\n                    self.history_buffer.document.translate_row_col_to_index(history_lineno, 0)", "response": "When the cursor changes in the default buffer. Synchronize with the history buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _history_buffer_pos_changed(self, _):\n        # Only when this buffer has the focus.\n        if self.app.current_buffer == self.history_buffer:\n            line_no = self.history_buffer.document.cursor_position_row\n\n            if line_no in self.history_mapping.selected_lines:\n                default_lineno = sorted(self.history_mapping.selected_lines).index(line_no) + \\\n                    self.history_mapping.result_line_offset\n\n                self.default_buffer.cursor_position = \\\n                    self.default_buffer.document.translate_row_col_to_index(default_lineno, 0)", "response": "When the cursor changes in the history buffer. Synchronize."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef python_sidebar(python_input):\n    def get_text_fragments():\n        tokens = []\n\n        def append_category(category):\n            tokens.extend([\n                ('class:sidebar', '  '),\n                ('class:sidebar.title', '   %-36s' % category.title),\n                ('class:sidebar', '\\n'),\n            ])\n\n        def append(index, label, status):\n            selected = index == python_input.selected_option_index\n\n            @if_mousedown\n            def select_item(mouse_event):\n                python_input.selected_option_index = index\n\n            @if_mousedown\n            def goto_next(mouse_event):\n                \" Select item and go to next value. \"\n                python_input.selected_option_index = index\n                option = python_input.selected_option\n                option.activate_next()\n\n            sel = ',selected' if selected else ''\n\n            tokens.append(('class:sidebar' + sel, ' >' if selected else '  '))\n            tokens.append(('class:sidebar.label' + sel, '%-24s' % label, select_item))\n            tokens.append(('class:sidebar.status' + sel, ' ', select_item))\n            tokens.append(('class:sidebar.status' + sel, '%s' % status, goto_next))\n\n            if selected:\n                tokens.append(('[SetCursorPosition]', ''))\n\n            tokens.append(('class:sidebar.status' + sel, ' ' * (13 - len(status)), goto_next))\n            tokens.append(('class:sidebar', '<' if selected else ''))\n            tokens.append(('class:sidebar', '\\n'))\n\n        i = 0\n        for category in python_input.options:\n            append_category(category)\n\n            for option in category.options:\n                append(i, option.title, '%s' % option.get_current_value())\n                i += 1\n\n        tokens.pop()  # Remove last newline.\n\n        return tokens\n\n    class Control(FormattedTextControl):\n        def move_cursor_down(self):\n            python_input.selected_option_index += 1\n\n        def move_cursor_up(self):\n            python_input.selected_option_index -= 1\n\n    return Window(\n        Control(get_text_fragments),\n        style='class:sidebar',\n        width=Dimension.exact(43),\n        height=Dimension(min=3),\n        scroll_offsets=ScrollOffsets(top=1, bottom=1))", "response": "Create the Layout for the sidebar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef python_sidebar_navigation(python_input):\n    def get_text_fragments():\n        tokens = []\n\n        # Show navigation info.\n        tokens.extend([\n            ('class:sidebar', '    '),\n            ('class:sidebar.key', '[Arrows]'),\n            ('class:sidebar', ' '),\n            ('class:sidebar.description', 'Navigate'),\n            ('class:sidebar', ' '),\n            ('class:sidebar.key', '[Enter]'),\n            ('class:sidebar', ' '),\n            ('class:sidebar.description', 'Hide menu'),\n        ])\n\n        return tokens\n\n    return Window(\n        FormattedTextControl(get_text_fragments),\n        style='class:sidebar',\n        width=Dimension.exact(43),\n        height=Dimension.exact(1))", "response": "Create the Layout showing the navigation information for the sidebar."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the Layout for the help text for the current item in the sidebar.", "response": "def python_sidebar_help(python_input):\n    \"\"\"\n    Create the `Layout` for the help text for the current item in the sidebar.\n    \"\"\"\n    token = 'class:sidebar.helptext'\n\n    def get_current_description():\n        \"\"\"\n        Return the description of the selected option.\n        \"\"\"\n        i = 0\n        for category in python_input.options:\n            for option in category.options:\n                if i == python_input.selected_option_index:\n                    return option.description\n                i += 1\n        return ''\n\n    def get_help_text():\n        return [(token, get_current_description())]\n\n    return ConditionalContainer(\n        content=Window(\n            FormattedTextControl(get_help_text),\n            style=token,\n            height=Dimension(min=3)),\n        filter=ShowSidebar(python_input) &\n               Condition(lambda: python_input.show_sidebar_help) & ~is_done)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the layout for the signature.", "response": "def signature_toolbar(python_input):\n    \"\"\"\n    Return the `Layout` for the signature.\n    \"\"\"\n    def get_text_fragments():\n        result = []\n        append = result.append\n        Signature = 'class:signature-toolbar'\n\n        if python_input.signatures:\n            sig = python_input.signatures[0]  # Always take the first one.\n\n            append((Signature, ' '))\n            try:\n                append((Signature, sig.full_name))\n            except IndexError:\n                # Workaround for #37: https://github.com/jonathanslenders/python-prompt-toolkit/issues/37\n                # See also: https://github.com/davidhalter/jedi/issues/490\n                return []\n\n            append((Signature + ',operator', '('))\n\n            try:\n                enumerated_params = enumerate(sig.params)\n            except AttributeError:\n                # Workaround for #136: https://github.com/jonathanslenders/ptpython/issues/136\n                # AttributeError: 'Lambda' object has no attribute 'get_subscope_by_name'\n                return []\n\n            for i, p in enumerated_params:\n                # Workaround for #47: 'p' is None when we hit the '*' in the signature.\n                #                     and sig has no 'index' attribute.\n                # See: https://github.com/jonathanslenders/ptpython/issues/47\n                #      https://github.com/davidhalter/jedi/issues/598\n                description = (p.description if p else '*') #or '*'\n                sig_index = getattr(sig, 'index', 0)\n\n                if i == sig_index:\n                    # Note: we use `_Param.description` instead of\n                    #       `_Param.name`, that way we also get the '*' before args.\n                    append((Signature + ',current-name', str(description)))\n                else:\n                    append((Signature, str(description)))\n                append((Signature + ',operator', ', '))\n\n            if sig.params:\n                # Pop last comma\n                result.pop()\n\n            append((Signature + ',operator', ')'))\n            append((Signature, ' '))\n        return result\n\n    return ConditionalContainer(\n        content=Window(\n            FormattedTextControl(get_text_fragments),\n            height=Dimension.exact(1)),\n        filter=\n            # Show only when there is a signature\n            HasSignature(python_input) &\n            # And there are no completions to be shown. (would cover signature pop-up.)\n            ~(has_completions & (show_completions_menu(python_input) |\n                                   show_multi_column_completions_menu(python_input)))\n            # Signature needs to be shown.\n            & ShowSignature(python_input) &\n            # Not done yet.\n            ~is_done)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the Layout for the status bar.", "response": "def status_bar(python_input):\n    \"\"\"\n    Create the `Layout` for the status bar.\n    \"\"\"\n    TB = 'class:status-toolbar'\n\n    @if_mousedown\n    def toggle_paste_mode(mouse_event):\n        python_input.paste_mode = not python_input.paste_mode\n\n    @if_mousedown\n    def enter_history(mouse_event):\n        python_input.enter_history()\n\n    def get_text_fragments():\n        python_buffer = python_input.default_buffer\n\n        result = []\n        append = result.append\n\n        append((TB, ' '))\n        result.extend(get_inputmode_fragments(python_input))\n        append((TB, ' '))\n\n        # Position in history.\n        append((TB, '%i/%i ' % (python_buffer.working_index + 1,\n                                len(python_buffer._working_lines))))\n\n        # Shortcuts.\n        app = get_app()\n        if not python_input.vi_mode and app.current_buffer == python_input.search_buffer:\n            append((TB, '[Ctrl-G] Cancel search [Enter] Go to this position.'))\n        elif bool(app.current_buffer.selection_state) and not python_input.vi_mode:\n            # Emacs cut/copy keys.\n            append((TB, '[Ctrl-W] Cut [Meta-W] Copy [Ctrl-Y] Paste [Ctrl-G] Cancel'))\n        else:\n            result.extend([\n                (TB + ' class:key', '[F3]', enter_history),\n                (TB, ' History ', enter_history),\n                (TB + ' class:key', '[F6]', toggle_paste_mode),\n                (TB, ' ', toggle_paste_mode),\n            ])\n\n            if python_input.paste_mode:\n                append((TB + ' class:paste-mode-on', 'Paste mode (on)', toggle_paste_mode))\n            else:\n                append((TB, 'Paste mode', toggle_paste_mode))\n\n        return result\n\n    return ConditionalContainer(\n            content=Window(content=FormattedTextControl(get_text_fragments), style=TB),\n            filter=~is_done & renderer_height_is_known &\n                 Condition(lambda: python_input.show_status_bar and\n                                      not python_input.show_exit_confirmation))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_inputmode_fragments(python_input):\n    app = get_app()\n    @if_mousedown\n    def toggle_vi_mode(mouse_event):\n        python_input.vi_mode = not python_input.vi_mode\n\n    token = 'class:status-toolbar'\n    input_mode_t = 'class:status-toolbar.input-mode'\n\n    mode = app.vi_state.input_mode\n    result = []\n    append = result.append\n\n    append((input_mode_t, '[F4] ', toggle_vi_mode))\n\n    # InputMode\n    if python_input.vi_mode:\n        recording_register = app.vi_state.recording_register\n        if recording_register:\n            append((token, ' '))\n            append((token + ' class:record', 'RECORD({})'.format(recording_register)))\n            append((token, ' - '))\n\n        if bool(app.current_buffer.selection_state):\n            if app.current_buffer.selection_state.type == SelectionType.LINES:\n                append((input_mode_t, 'Vi (VISUAL LINE)', toggle_vi_mode))\n            elif app.current_buffer.selection_state.type == SelectionType.CHARACTERS:\n                append((input_mode_t, 'Vi (VISUAL)', toggle_vi_mode))\n                append((token, ' '))\n            elif app.current_buffer.selection_state.type == 'BLOCK':\n                append((input_mode_t, 'Vi (VISUAL BLOCK)', toggle_vi_mode))\n                append((token, ' '))\n        elif mode in (InputMode.INSERT, 'vi-insert-multiple'):\n            append((input_mode_t, 'Vi (INSERT)', toggle_vi_mode))\n            append((token, '  '))\n        elif mode == InputMode.NAVIGATION:\n            append((input_mode_t, 'Vi (NAV)', toggle_vi_mode))\n            append((token, '     '))\n        elif mode == InputMode.REPLACE:\n            append((input_mode_t, 'Vi (REPLACE)', toggle_vi_mode))\n            append((token, ' '))\n    else:\n        if app.emacs_state.is_recording:\n            append((token, ' '))\n            append((token + ' class:record', 'RECORD'))\n            append((token, ' - '))\n\n        append((input_mode_t, 'Emacs', toggle_vi_mode))\n        append((token, ' '))\n\n    return result", "response": "Return current input mode as a list of tuples for use in a\n    toolbar."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_sidebar_button_info(python_input):\n    @if_mousedown\n    def toggle_sidebar(mouse_event):\n        \" Click handler for the menu. \"\n        python_input.show_sidebar = not python_input.show_sidebar\n\n    version = sys.version_info\n    tokens = [\n        ('class:status-toolbar.key', '[F2]', toggle_sidebar),\n        ('class:status-toolbar', ' Menu', toggle_sidebar),\n        ('class:status-toolbar', ' - '),\n        ('class:status-toolbar.python-version', '%s %i.%i.%i' % (platform.python_implementation(),\n                                               version[0], version[1], version[2])),\n        ('class:status-toolbar', ' '),\n    ]\n    width = fragment_list_width(tokens)\n\n    def get_text_fragments():\n        # Python version\n        return tokens\n\n    return ConditionalContainer(\n        content=Window(\n            FormattedTextControl(get_text_fragments),\n            style='class:status-toolbar',\n            height=Dimension.exact(1),\n            width=Dimension.exact(width)),\n        filter=~is_done & renderer_height_is_known &\n            Condition(lambda: python_input.show_status_bar and\n                                  not python_input.show_exit_confirmation))", "response": "Create Layout for the information in the right - bottom corner of the status bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a Layout for the exit message.", "response": "def exit_confirmation(python_input, style='class:exit-confirmation'):\n    \"\"\"\n    Create `Layout` for the exit message.\n    \"\"\"\n    def get_text_fragments():\n        # Show \"Do you really want to exit?\"\n        return [\n            (style, '\\n %s ([y]/n)' % python_input.exit_message),\n            ('[SetCursorPosition]', ''),\n            (style, '  \\n'),\n        ]\n\n    visible = ~is_done & Condition(lambda: python_input.show_exit_confirmation)\n\n    return ConditionalContainer(\n        content=Window(FormattedTextControl(get_text_fragments), style=style),   # , has_focus=visible)),\n        filter=visible)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef meta_enter_message(python_input):\n    def get_text_fragments():\n        return [('class:accept-message', ' [Meta+Enter] Execute ')]\n\n    def extra_condition():\n        \" Only show when... \"\n        b = python_input.default_buffer\n\n        return (\n            python_input.show_meta_enter_message and\n            (not b.document.is_cursor_at_the_end or\n                python_input.accept_input_on_enter is None) and\n            '\\n' in b.text)\n\n    visible = ~is_done & has_focus(DEFAULT_BUFFER) & Condition(extra_condition)\n\n    return ConditionalContainer(\n        content=Window(FormattedTextControl(get_text_fragments)),\n        filter=visible)", "response": "Create the Layout for the Meta + Enter message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the currently selected option.", "response": "def selected_option(self):\n        \" Return the currently selected option. \"\n        i = 0\n        for category in self.options:\n            for o in category.options:\n                if i == self.selected_option_index:\n                    return o\n                else:\n                    i += 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the compiler flags for the current set of modules.", "response": "def get_compiler_flags(self):\n        \"\"\"\n        Give the current compiler flags by looking for _Feature instances\n        in the globals.\n        \"\"\"\n        flags = 0\n\n        for value in self.get_globals().values():\n            if isinstance(value, __future__._Feature):\n                flags |= value.compiler_flag\n\n        return flags"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef install_code_colorscheme(self, name, style_dict):\n        assert isinstance(name, six.text_type)\n        assert isinstance(style_dict, dict)\n\n        self.code_styles[name] = style_dict", "response": "Install a new code color scheme."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef use_code_colorscheme(self, name):\n        assert name in self.code_styles\n\n        self._current_code_style_name = name\n        self._current_style = self._generate_style()", "response": "Apply new colorscheme. (By name.)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall a new UI color scheme.", "response": "def install_ui_colorscheme(self, name, style_dict):\n        \"\"\"\n        Install a new UI color scheme.\n        \"\"\"\n        assert isinstance(name, six.text_type)\n        assert isinstance(style_dict, dict)\n\n        self.ui_styles[name] = style_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply new colorscheme. (By name.)", "response": "def use_ui_colorscheme(self, name):\n        \"\"\"\n        Apply new colorscheme. (By name.)\n        \"\"\"\n        assert name in self.ui_styles\n\n        self._current_ui_style_name = name\n        self._current_style = self._generate_style()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_style(self):\n        return generate_style(self.code_styles[self._current_code_style_name],\n                              self.ui_styles[self._current_ui_style_name])", "response": "Generate a new style instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_options(self):\n        def enable(attribute, value=True):\n            setattr(self, attribute, value)\n\n            # Return `True`, to be able to chain this in the lambdas below.\n            return True\n\n        def disable(attribute):\n            setattr(self, attribute, False)\n            return True\n\n        def simple_option(title, description, field_name, values=None):\n            \" Create Simple on/of option. \"\n            values = values or ['off', 'on']\n\n            def get_current_value():\n                return values[bool(getattr(self, field_name))]\n\n            def get_values():\n                return {\n                    values[1]: lambda: enable(field_name),\n                    values[0]: lambda: disable(field_name),\n                }\n\n            return Option(title=title, description=description,\n                          get_values=get_values,\n                          get_current_value=get_current_value)\n\n        brightness_values = [1.0 / 20 * value for value in range(0, 21)]\n\n        return [\n            OptionCategory('Input', [\n                simple_option(title='Editing mode',\n                              description='Vi or emacs key bindings.',\n                              field_name='vi_mode',\n                              values=[EditingMode.EMACS, EditingMode.VI]),\n                simple_option(title='Paste mode',\n                              description=\"When enabled, don't indent automatically.\",\n                              field_name='paste_mode'),\n                Option(title='Complete while typing',\n                       description=\"Generate autocompletions automatically while typing. \"\n                                   'Don\\'t require pressing TAB. (Not compatible with \"History search\".)',\n                       get_current_value=lambda: ['off', 'on'][self.complete_while_typing],\n                       get_values=lambda: {\n                           'on': lambda: enable('complete_while_typing') and disable('enable_history_search'),\n                           'off': lambda: disable('complete_while_typing'),\n                       }),\n                Option(title='History search',\n                       description='When pressing the up-arrow, filter the history on input starting '\n                                   'with the current text. (Not compatible with \"Complete while typing\".)',\n                       get_current_value=lambda: ['off', 'on'][self.enable_history_search],\n                       get_values=lambda: {\n                           'on': lambda: enable('enable_history_search') and disable('complete_while_typing'),\n                           'off': lambda: disable('enable_history_search'),\n                       }),\n                simple_option(title='Mouse support',\n                              description='Respond to mouse clicks and scrolling for positioning the cursor, '\n                                          'selecting text and scrolling through windows.',\n                              field_name='enable_mouse_support'),\n                simple_option(title='Confirm on exit',\n                              description='Require confirmation when exiting.',\n                              field_name='confirm_exit'),\n                simple_option(title='Input validation',\n                              description='In case of syntax errors, move the cursor to the error '\n                                          'instead of showing a traceback of a SyntaxError.',\n                              field_name='enable_input_validation'),\n                simple_option(title='Auto suggestion',\n                              description='Auto suggest inputs by looking at the history. '\n                                          'Pressing right arrow or Ctrl-E will complete the entry.',\n                              field_name='enable_auto_suggest'),\n                Option(title='Accept input on enter',\n                       description='Amount of ENTER presses required to execute input when the cursor '\n                                   'is at the end of the input. (Note that META+ENTER will always execute.)',\n                       get_current_value=lambda: str(self.accept_input_on_enter or 'meta-enter'),\n                       get_values=lambda: {\n                           '2': lambda: enable('accept_input_on_enter', 2),\n                           '3': lambda: enable('accept_input_on_enter', 3),\n                           '4': lambda: enable('accept_input_on_enter', 4),\n                           'meta-enter': lambda: enable('accept_input_on_enter', None),\n                       }),\n            ]),\n            OptionCategory('Display', [\n                Option(title='Completions',\n                       description='Visualisation to use for displaying the completions. (Multiple columns, one column, a toolbar or nothing.)',\n                       get_current_value=lambda: self.completion_visualisation,\n                       get_values=lambda: {\n                           CompletionVisualisation.NONE: lambda: enable('completion_visualisation', CompletionVisualisation.NONE),\n                           CompletionVisualisation.POP_UP: lambda: enable('completion_visualisation', CompletionVisualisation.POP_UP),\n                           CompletionVisualisation.MULTI_COLUMN: lambda: enable('completion_visualisation', CompletionVisualisation.MULTI_COLUMN),\n                           CompletionVisualisation.TOOLBAR: lambda: enable('completion_visualisation', CompletionVisualisation.TOOLBAR),\n                       }),\n                Option(title='Prompt',\n                       description=\"Visualisation of the prompt. ('>>>' or 'In [1]:')\",\n                       get_current_value=lambda: self.prompt_style,\n                       get_values=lambda: dict((s, partial(enable, 'prompt_style', s)) for s in self.all_prompt_styles)),\n                simple_option(title='Blank line after output',\n                              description='Insert a blank line after the output.',\n                              field_name='insert_blank_line_after_output'),\n                simple_option(title='Show signature',\n                              description='Display function signatures.',\n                              field_name='show_signature'),\n                simple_option(title='Show docstring',\n                              description='Display function docstrings.',\n                              field_name='show_docstring'),\n                simple_option(title='Show line numbers',\n                              description='Show line numbers when the input consists of multiple lines.',\n                              field_name='show_line_numbers'),\n                simple_option(title='Show Meta+Enter message',\n                              description='Show the [Meta+Enter] message when this key combination is required to execute commands. ' +\n                                  '(This is the case when a simple [Enter] key press will insert a newline.',\n                              field_name='show_meta_enter_message'),\n                simple_option(title='Wrap lines',\n                              description='Wrap lines instead of scrolling horizontally.',\n                              field_name='wrap_lines'),\n                simple_option(title='Show status bar',\n                              description='Show the status bar at the bottom of the terminal.',\n                              field_name='show_status_bar'),\n                simple_option(title='Show sidebar help',\n                              description='When the sidebar is visible, also show this help text.',\n                              field_name='show_sidebar_help'),\n                simple_option(title='Highlight parenthesis',\n                              description='Highlight matching parenthesis, when the cursor is on or right after one.',\n                              field_name='highlight_matching_parenthesis'),\n            ]),\n            OptionCategory('Colors', [\n                simple_option(title='Syntax highlighting',\n                              description='Use colors for syntax highligthing',\n                              field_name='enable_syntax_highlighting'),\n                simple_option(title='Swap light/dark colors',\n                              description='Swap light and dark colors.',\n                              field_name='swap_light_and_dark'),\n                Option(title='Code',\n                       description='Color scheme to use for the Python code.',\n                       get_current_value=lambda: self._current_code_style_name,\n                       get_values=lambda: dict(\n                            (name, partial(self.use_code_colorscheme, name)) for name in self.code_styles)\n                       ),\n                Option(title='User interface',\n                       description='Color scheme to use for the user interface.',\n                       get_current_value=lambda: self._current_ui_style_name,\n                       get_values=lambda: dict(\n                            (name, partial(self.use_ui_colorscheme, name)) for name in self.ui_styles)\n                       ),\n                Option(title='Color depth',\n                       description='Monochrome (1 bit), 16 ANSI colors (4 bit),\\n256 colors (8 bit), or 24 bit.',\n                       get_current_value=lambda: COLOR_DEPTHS[self.color_depth],\n                       get_values=lambda: dict(\n                           (name, partial(self._use_color_depth, depth)) for depth, name in COLOR_DEPTHS.items())\n                       ),\n                Option(title='Min brightness',\n                       description='Minimum brightness for the color scheme (default=0.0).',\n                       get_current_value=lambda: '%.2f' % self.min_brightness,\n                       get_values=lambda: dict(\n                           ('%.2f' % value, partial(self._set_min_brightness, value))\n                            for value in brightness_values)\n                       ),\n                Option(title='Max brightness',\n                       description='Maximum brightness for the color scheme (default=1.0).',\n                       get_current_value=lambda: '%.2f' % self.max_brightness,\n                       get_values=lambda: dict(\n                           ('%.2f' % value, partial(self._set_max_brightness, value))\n                            for value in brightness_values)\n                       ),\n            ]),\n        ]", "response": "Create a list of Option instances for the options sidebar."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates an application instance.", "response": "def _create_application(self):\n        \"\"\"\n        Create an `Application` instance.\n        \"\"\"\n        return Application(\n            input=self.input,\n            output=self.output,\n            layout=self.ptpython_layout.layout,\n            key_bindings=merge_key_bindings([\n                load_python_bindings(self),\n                load_auto_suggest_bindings(),\n                load_sidebar_bindings(self),\n                load_confirm_exit_bindings(self),\n                ConditionalKeyBindings(\n                    load_open_in_editor_bindings(),\n                    Condition(lambda: self.enable_open_in_editor)),\n                # Extra key bindings should not be active when the sidebar is visible.\n                ConditionalKeyBindings(\n                    self.extra_key_bindings,\n                    Condition(lambda: not self.show_sidebar))\n            ]),\n            color_depth=lambda: self.color_depth,\n            paste_mode=Condition(lambda: self.paste_mode),\n            mouse_support=Condition(lambda: self.enable_mouse_support),\n            style=DynamicStyle(lambda: self._current_style),\n            style_transformation=self.style_transformation,\n            include_default_pygments_style=False,\n            reverse_vi_search_direction=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the Buffer object for the Python input.", "response": "def _create_buffer(self):\n        \"\"\"\n        Create the `Buffer` for the Python input.\n        \"\"\"\n        python_buffer = Buffer(\n            name=DEFAULT_BUFFER,\n            complete_while_typing=Condition(lambda: self.complete_while_typing),\n            enable_history_search=Condition(lambda: self.enable_history_search),\n            tempfile_suffix='.py',\n            history=self.history,\n            completer=ThreadedCompleter(self._completer),\n            validator=ConditionalValidator(\n                self._validator,\n                Condition(lambda: self.enable_input_validation)),\n            auto_suggest=ConditionalAutoSuggest(\n                ThreadedAutoSuggest(AutoSuggestFromHistory()),\n                Condition(lambda: self.enable_auto_suggest)),\n            accept_handler=self._accept_handler,\n            on_text_changed=self._on_input_timeout)\n\n        return python_buffer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _on_input_timeout(self, buff):\n        assert isinstance(buff, Buffer)\n        app = self.app\n\n        # Never run multiple get-signature threads.\n        if self._get_signatures_thread_running:\n            return\n        self._get_signatures_thread_running = True\n\n        document = buff.document\n\n        def run():\n            script = get_jedi_script_from_document(document, self.get_locals(), self.get_globals())\n\n            # Show signatures in help text.\n            if script:\n                try:\n                    signatures = script.call_signatures()\n                except ValueError:\n                    # e.g. in case of an invalid \\\\x escape.\n                    signatures = []\n                except Exception:\n                    # Sometimes we still get an exception (TypeError), because\n                    # of probably bugs in jedi. We can silence them.\n                    # See: https://github.com/davidhalter/jedi/issues/492\n                    signatures = []\n                else:\n                    # Try to access the params attribute just once. For Jedi\n                    # signatures containing the keyword-only argument star,\n                    # this will crash when retrieving it the first time with\n                    # AttributeError. Every following time it works.\n                    # See: https://github.com/jonathanslenders/ptpython/issues/47\n                    #      https://github.com/davidhalter/jedi/issues/598\n                    try:\n                        if signatures:\n                            signatures[0].params\n                    except AttributeError:\n                        pass\n            else:\n                signatures = []\n\n            self._get_signatures_thread_running = False\n\n            # Set signatures and redraw if the text didn't change in the\n            # meantime. Otherwise request new signatures.\n            if buff.text == document.text:\n                self.signatures = signatures\n\n                # Set docstring in docstring buffer.\n                if signatures:\n                    string = signatures[0].docstring()\n                    if not isinstance(string, six.text_type):\n                        string = string.decode('utf-8')\n                    self.docstring_buffer.reset(\n                        document=Document(string, cursor_position=0))\n                else:\n                    self.docstring_buffer.reset()\n\n                app.invalidate()\n            else:\n                self._on_input_timeout(buff)\n\n        get_event_loop().run_in_executor(run)", "response": "Called when the user wants to get the signature of the current code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_all_code_styles():\n    result = dict((name, style_from_pygments_cls(get_style_by_name(name))) for name in get_all_styles())\n    result['win32'] = Style.from_dict(win32_code_style)\n    return result", "response": "Return a mapping from style names to their classes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize_extensions(shell, extensions):\n    try:\n        iter(extensions)\n    except TypeError:\n        pass  # no extensions found\n    else:\n        for ext in extensions:\n            try:\n                shell.extension_manager.load_extension(ext)\n            except:\n                ipy_utils.warn.warn(\n                    \"Error in loading extension: %s\" % ext +\n                    \"\\nCheck your config files in %s\" % ipy_utils.path.get_ipython_dir())\n                shell.showtraceback()", "response": "Initialize the extensions in IPython."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncopying from `IPython/terminal/embed.py`, but using our `InteractiveShellEmbed` instead.", "response": "def embed(**kwargs):\n    \"\"\"\n    Copied from `IPython/terminal/embed.py`, but using our `InteractiveShellEmbed` instead.\n    \"\"\"\n    config = kwargs.get('config')\n    header = kwargs.pop('header', u'')\n    compile_flags = kwargs.pop('compile_flags', None)\n    if config is None:\n        config = load_default_config()\n        config.InteractiveShellEmbed = config.TerminalInteractiveShell\n        kwargs['config'] = config\n    shell = InteractiveShellEmbed.instance(**kwargs)\n    initialize_extensions(shell, config['InteractiveShellApp']['extensions'])\n    shell(header=header, stack_depth=2, compile_flags=compile_flags)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting of URLs available for downloading.", "response": "def paths_for_download(self):\n        \"\"\"List of URLs available for downloading.\"\"\"\n        if self._paths_for_download is None:\n            queries = list()\n            try:\n                for sra in self.gsm.relations['SRA']:\n                    query = sra.split(\"=\")[-1]\n                    if 'SRX' not in query:\n                        raise ValueError(\n                            \"Sample looks like it is not an SRA: %s\" % query)\n                    logger.info(\"Query: %s\" % query)\n                    queries.append(query)\n            except KeyError:\n                raise NoSRARelationException(\n                    'No relation called SRA for %s' % self.gsm.get_accession())\n\n            # Construction of DataFrame df with paths to download\n            df = DataFrame(columns=['download_path'])\n            for query in queries:\n                # retrieve IDs for given SRX\n                searchdata = Entrez.esearch(db='sra', term=query, usehistory='y',\n                                            retmode='json')\n                answer = json.loads(searchdata.read())\n                ids = answer[\"esearchresult\"][\"idlist\"]\n                if len(ids) != 1:\n                    raise ValueError(\n                        \"There should be one and only one ID per SRX\")\n\n                # using ID fetch the info\n                number_of_trials = 10\n                wait_time = 30\n                for trial in range(number_of_trials):\n                    try:\n                        results = Entrez.efetch(db=\"sra\", id=ids[0],\n                                                rettype=\"runinfo\",\n                                                retmode=\"text\").read()\n                        break\n                    except HTTPError as httperr:\n                        if \"502\" in str(httperr):\n                            logger.warn((\"%s, trial %i out of %i, waiting \"\n                                         \"for %i seconds.\") % (\n                                             str(httperr),\n                                             trial,\n                                             number_of_trials,\n                                             wait_time))\n                            time.sleep(wait_time)\n                        elif httperr.code == 429:\n                            # This means that there is too many requests\n                            try:\n                                header_wait_time = int(\n                                    httperr.headers[\"Retry-After\"])\n                            except:\n                                header_wait_time = wait_time\n                            logger.warn((\"%s, trial %i out of %i, waiting \"\n                                         \"for %i seconds.\") % (\n                                             str(httperr),\n                                             trial,\n                                             number_of_trials,\n                                             header_wait_time))\n                            time.sleep(header_wait_time)\n                        else:\n                            raise httperr\n                try:\n                    df_tmp = DataFrame([i.split(',') for i in results.split('\\n') if i != ''][1:],\n                                       columns=[i.split(',') for i in results.split('\\n') if i != ''][0])\n                except IndexError:\n                    logger.error((\"SRA is empty (ID: %s, query: %s). \"\n                                  \"Check if it is publicly available.\") %\n                                 (ids[0], query))\n                    continue\n\n                # check it first\n                try:\n                    df_tmp['download_path']\n                except KeyError as e:\n                    logger.error('KeyError: ' + str(e) + '\\n')\n                    logger.error(str(results) + '\\n')\n\n                df = concat([df, df_tmp], sort=True)\n            self._paths_for_download = [path for path in df['download_path']]\n        return self._paths_for_download"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndownloads SRA files. Returns: :obj:`list` of :obj:`str`: List of downloaded files.", "response": "def download(self):\n        \"\"\"Download SRA files.\n\n        Returns:\n            :obj:`list` of :obj:`str`: List of downloaded files.\n        \"\"\"\n        self.downloaded_paths = list()\n        for path in self.paths_for_download:\n            downloaded_path = list()\n            utils.mkdir_p(os.path.abspath(self.directory))\n\n            sra_run = path.split(\"/\")[-1]\n            logger.info(\"Analysing %s\" % sra_run)\n            url = type(self).FTP_ADDRESS_TPL.format(\n                range_subdir=sra_run[:6],\n                file_dir=sra_run)\n            logger.debug(\"URL: %s\", url)\n            filepath = os.path.abspath(\n                os.path.join(self.directory, \"%s.sra\" % sra_run))\n            utils.download_from_url(\n                url,\n                filepath,\n                aspera=self.aspera,\n                silent=self.silent,\n                force=self.force)\n\n            if self.filetype in (\"fasta\", \"fastq\"):\n                if utils.which('fastq-dump') is None:\n                    logger.error(\"fastq-dump command not found\")\n                ftype = \"\"\n                if self.filetype == \"fasta\":\n                    ftype = \" --fasta \"\n                cmd = \"fastq-dump\"\n                if utils.which('parallel-fastq-dump') is None:\n                    cmd += \" %s --outdir %s %s\"\n                else:\n                    logger.debug(\"Using parallel fastq-dump\")\n                    cmd = \" parallel-fastq-dump --threads %s\"\n                    cmd = cmd % self.threads\n                    cmd += \" %s --outdir %s -s %s\"\n                cmd = cmd % (ftype, self.directory, filepath)\n\n                for fqoption, fqvalue in iteritems(self.fastq_dump_options):\n                    if fqvalue:\n                        cmd += (\" --%s %s\" % (fqoption, fqvalue))\n                    elif fqvalue is None:\n                        cmd += (\" --%s\" % fqoption)\n                logger.debug(cmd)\n                process = sp.Popen(cmd, stdout=sp.PIPE,\n                                   stderr=sp.PIPE,\n                                   shell=True)\n                logger.info(\"Converting to %s/%s*.%s.gz\\n\" % (\n                    self.directory, sra_run, self.filetype))\n                pout, perr = process.communicate()\n                downloaded_path = glob.glob(os.path.join(\n                    self.directory,\n                    \"%s*.%s.gz\" % (sra_run, self.filetype)))\n\n            elif self.filetype == 'sra':\n                downloaded_path = glob.glob(os.path.join(\n                    self.directory,\n                    \"%s*.%s\" % (sra_run, self.filetype)))\n\n            else:\n                downloaded_path = glob.glob(os.path.join(\n                    self.directory,\n                    \"%s*\" % sra_run))\n                logger.error(\"Filetype %s not supported.\" % self.filetype)\n\n            if not self.keep_sra and self.filetype != 'sra':\n                # Delete sra file\n                os.unlink(filepath)\n\n            self.downloaded_paths += downloaded_path\n        return self.downloaded_paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_log_file(path):\n    logfile_handler = RotatingFileHandler(\n        path, maxBytes=50000, backupCount=2)\n    formatter = logging.Formatter(\n        fmt='%(asctime)s %(levelname)s %(module)s - %(message)s',\n        datefmt=\"%d-%b-%Y %H:%M:%S\")\n    logfile_handler.setFormatter(formatter)\n    geoparse_logger.addHandler(logfile_handler)", "response": "Add a log file to the geoparse logger."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _supplementary_files_download_worker(*args):\n    gsm = args[0][0]\n    download_sra = args[0][1]\n    email = args[0][2]\n    dirpath = args[0][3]\n    sra_kwargs = args[0][4]\n    return (gsm.get_accession(), gsm.download_supplementary_files(\n        directory=dirpath,\n        download_sra=download_sra,\n        email=email, **sra_kwargs))", "response": "A worker to download supplementary files.\n    To be used with multiprocessing.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the metadata attribute by the name.", "response": "def get_metadata_attribute(self, metaname):\n        \"\"\"Get the metadata attribute by the name.\n\n        Args:\n            metaname (:obj:`str`): Name of the attribute\n\n        Returns:\n            :obj:`list` or :obj:`str`: Value(s) of the requested metadata\n                attribute\n\n        Raises:\n            NoMetadataException: Attribute error\n            TypeError: Metadata should be a list\n        \"\"\"\n        metadata_value = self.metadata.get(metaname, None)\n        if metadata_value is None:\n            raise NoMetadataException(\n                \"No metadata attribute named %s\" % metaname)\n        if not isinstance(metadata_value, list):\n            raise TypeError(\"Metadata is not a list and it should be.\")\n\n        if len(metadata_value) > 1:\n            return metadata_value\n        else:\n            return metadata_value[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_metadata_as_string(self):\n        metalist = []\n        for metaname, meta in iteritems(self.metadata):\n            message = \"Single value in metadata dictionary should be a list!\"\n            assert isinstance(meta, list), message\n            for data in meta:\n                if data:\n                    metalist.append(\"!%s_%s = %s\" % (self.geotype.capitalize(),\n                                                     metaname, data))\n        return \"\\n\".join(metalist)", "response": "Get the metadata as SOFT formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the object in a SOFT format.", "response": "def to_soft(self, path_or_handle, as_gzip=False):\n        \"\"\"Save the object in a SOFT format.\n\n        Args:\n            path_or_handle (:obj:`str` or :obj:`file`): Path or handle to\n                output file\n            as_gzip (:obj:`bool`): Save as gzip\n        \"\"\"\n        if isinstance(path_or_handle, str):\n            if as_gzip:\n                with gzip.open(path_or_handle, 'wt') as outfile:\n                    outfile.write(self._get_object_as_soft())\n            else:\n                with open(path_or_handle, 'w') as outfile:\n                    outfile.write(self._get_object_as_soft())\n        else:\n            path_or_handle.write(self._get_object_as_soft())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef head(self):\n        summary = list()\n        summary.append(\"%s %s\" % (self.geotype, self.name) + \"\\n\")\n        summary.append(\" - Metadata:\" + \"\\n\")\n        summary.append(\n            \"\\n\".join(self._get_metadata_as_string().split(\"\\n\")[:5]) + \"\\n\")\n        summary.append(\"\\n\")\n        summary.append(\" - Columns:\" + \"\\n\")\n        summary.append(self.columns.to_string() + \"\\n\")\n        summary.append(\"\\n\")\n        summary.append(\" - Table:\" + \"\\n\")\n        summary.append(\n            \"\\t\".join([\"Index\"] + self.table.columns.tolist()) + \"\\n\")\n        summary.append(self.table.head().to_string(header=None) + \"\\n\")\n        summary.append(\" \" * 40 + \"...\" + \" \" * 40 + \"\\n\")\n        summary.append(\" \" * 40 + \"...\" + \" \" * 40 + \"\\n\")\n        summary.append(\" \" * 40 + \"...\" + \" \" * 40 + \"\\n\")\n        summary.append(self.table.tail().to_string(header=None) + \"\\n\")\n        return \"\\n\".join([str(s) for s in summary])", "response": "Print short description of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_object_as_soft(self):\n        soft = [\"^%s = %s\" % (self.geotype, self.name),\n                self._get_metadata_as_string(),\n                self._get_columns_as_string(),\n                self._get_table_as_string()]\n        return \"\\n\".join(soft)", "response": "Get the object as SOFT formated string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_table_as_string(self):\n        tablelist = []\n        tablelist.append(\"!%s_table_begin\" % self.geotype.lower())\n        tablelist.append(\"\\t\".join(self.table.columns))\n        for idx, row in self.table.iterrows():\n            tablelist.append(\"\\t\".join(map(str, row)))\n        tablelist.append(\"!%s_table_end\" % self.geotype.lower())\n        return \"\\n\".join(tablelist)", "response": "Get table as string."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_columns_as_string(self):\n        columnslist = []\n        for rowidx, row in self.columns.iterrows():\n            columnslist.append(\"#%s = %s\" % (rowidx, row.description))\n        return \"\\n\".join(columnslist)", "response": "Returns columns as SOFT formated string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nannotates GSM with a column in a DataFrame.", "response": "def annotate(self, gpl, annotation_column, gpl_on=\"ID\", gsm_on=\"ID_REF\",\n                 in_place=False):\n        \"\"\"Annotate GSM with provided GPL\n\n        Args:\n            gpl (:obj:`pandas.DataFrame`): A Platform or DataFrame to annotate with\n            annotation_column (str`): Column in a table for annotation\n            gpl_on (:obj:`str`): Use this column in GSM to merge. Defaults to \"ID\".\n            gsm_on (:obj:`str`): Use this column in GPL to merge.\n                Defaults to \"ID_REF\".\n            in_place (:obj:`bool`): Substitute table in GSM by new annotated\n                table. Defaults to False.\n\n        Returns:\n            :obj:`pandas.DataFrame` or :obj:`None`: Annotated table or None\n\n\n        Raises:\n            TypeError: GPL should be GPL or pandas.DataFrame\n        \"\"\"\n        if isinstance(gpl, GPL):\n            annotation_table = gpl.table\n        elif isinstance(gpl, DataFrame):\n            annotation_table = gpl\n        else:\n            raise TypeError(\"gpl should be a GPL object or a pandas.DataFrame\")\n\n        # annotate by merging\n        annotated = self.table.merge(\n            annotation_table[[gpl_on, annotation_column]], left_on=gsm_on,\n            right_on=gpl_on)\n        del annotated[gpl_on]\n        if in_place:\n            self.table = annotated\n            return None\n        else:\n            return annotated"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef annotate_and_average(self, gpl, expression_column, group_by_column,\n                             rename=True, force=False, merge_on_column=None,\n                             gsm_on=None, gpl_on=None):\n        \"\"\"Annotate GSM table with provided GPL.\n\n        Args:\n            gpl (:obj:`GEOTypes.GPL`): Platform for annotations\n            expression_column (:obj:`str`): Column name which \"expressions\"\n                are represented\n            group_by_column (:obj:`str`): The data will be grouped and averaged\n                over this column and only this column will be kept\n            rename (:obj:`bool`): Rename output column to the\n                self.name. Defaults to True.\n            force (:obj:`bool`): If the name of the GPL does not match the platform\n                name in GSM proceed anyway. Defaults to False.\n            merge_on_column (:obj:`str`): Column to merge the data\n                on. Defaults to None.\n            gsm_on (:obj:`str`): In the case columns to merge are different in GSM\n                and GPL use this column in GSM. Defaults to None.\n            gpl_on (:obj:`str`): In the case columns to merge are different in GSM\n                and GPL use this column in GPL. Defaults to None.\n\n        Returns:\n            :obj:`pandas.DataFrame`: Annotated data\n        \"\"\"\n        if gpl.name != self.metadata['platform_id'][0] and not force:\n            raise KeyError(\"Platforms from GSM (%s) and from GPL (%s)\" % (\n                gpl.name, self.metadata['platform_id']) +\n                           \" are incompatible. Use force=True to use this GPL.\")\n        if merge_on_column is None and gpl_on is None and gsm_on is None:\n            raise Exception(\"You have to provide one of the two: \"\n                            \"merge_on_column or gpl_on and gsm_on parameters\")\n        if merge_on_column:\n            logger.info(\"merge_on_column is not None. Using this option.\")\n            tmp_data = self.table.merge(gpl.table, on=merge_on_column,\n                                        how='outer')\n            tmp_data = tmp_data.groupby(group_by_column).mean()[\n                [expression_column]]\n        else:\n            if gpl_on is None or gsm_on is None:\n                raise Exception(\"Please provide both gpl_on and gsm_on or \"\n                                \"provide merge_on_column only\")\n            tmp_data = self.table.merge(gpl.table, left_on=gsm_on,\n                                        right_on=gpl_on, how='outer')\n            tmp_data = tmp_data.groupby(group_by_column).mean()[\n                [expression_column]]\n        if rename:\n            tmp_data.columns = [self.name]\n        return tmp_data", "response": "Annotate the GSM table with the data in the GPL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_supplementary_files(self, directory=\"./\", download_sra=True,\n                                     email=None, sra_kwargs=None):\n        \"\"\"Download all supplementary data available for the sample.\n\n        Args:\n            directory (:obj:`str`): Directory to download the data (in this directory\n                function will create new directory with the files).\n                Defaults to \"./\".\n            download_sra (:obj:`bool`): Indicates whether to download SRA raw\n                data too. Defaults to True.\n            email (:obj:`str`): E-mail that will be provided to the Entrez.\n                It is mandatory if download_sra=True. Defaults to None.\n            sra_kwargs (:obj:`dict`, optional): Kwargs passed to the\n                download_SRA method. Defaults to None.\n\n        Returns:\n            :obj:`dict`: A key-value pair of name taken from the metadata and\n                paths downloaded, in the case of SRA files the key is ``SRA``.\n        \"\"\"\n        directory_path = os.path.abspath(\n            os.path.join(directory, \"%s_%s_%s\" % (\n                'Supp',\n                self.get_accession(),\n                # the directory name cannot contain many of the signs\n                re.sub(r'[\\s\\*\\?\\(\\),\\.;]', '_', self.metadata['title'][0]))))\n\n        utils.mkdir_p(os.path.abspath(directory_path))\n        downloaded_paths = dict()\n        if sra_kwargs is None:\n            sra_kwargs = {}\n        # Possible erroneous values that could be identified and skipped right\n        # after\n        blacklist = ('NONE',)\n        for metakey, metavalue in iteritems(self.metadata):\n            if 'supplementary_file' in metakey:\n                assert len(metavalue) == 1 and metavalue != ''\n                if metavalue[0] in blacklist:\n                    logger.warn(\"%s value is blacklisted as '%s' - skipping\" %\n                                (metakey, metavalue[0]))\n                    continue\n                # SRA will be downloaded elsewhere\n                if 'sra' not in metavalue[0]:\n                    download_path = os.path.abspath(os.path.join(\n                        directory,\n                        os.path.join(directory_path,\n                                     metavalue[0].split(\"/\")[-1])))\n                    try:\n                        utils.download_from_url(metavalue[0], download_path)\n                        downloaded_paths[metavalue[0]] = download_path\n                    except Exception as err:\n                        logger.error(\n                            \"Cannot download %s supplementary file (%s)\" % (\n                                self.get_accession(), err))\n        if download_sra:\n            try:\n                downloaded_files = self.download_SRA(\n                    email,\n                    directory=directory,\n                    **sra_kwargs)\n                downloaded_paths.update(downloaded_files)\n            except Exception as err:\n                logger.error(\"Cannot download %s SRA file (%s)\" % (\n                    self.get_accession(), err))\n        return downloaded_paths", "response": "Download all supplementary files in the sample and store them in the directory."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_SRA(self, email, directory='./', **kwargs):\n        downloader = SRADownloader(self, email, directory, **kwargs)\n        return {\"SRA\": downloader.download()}", "response": "Download RAW data as SRA file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_object_as_soft(self):\n        soft = [\"^%s = %s\" % (self.geotype, self.name),\n                self._get_metadata_as_string()]\n        return \"\\n\".join(soft)", "response": "Get the object as SOFT formatted string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_object_as_soft(self):\n        soft = []\n        if self.database is not None:\n            soft.append(self.database._get_object_as_soft())\n        soft += [\"^%s = %s\" % (self.geotype, self.name),\n                 self._get_metadata_as_string()]\n        for subset in self.subsets.values():\n            soft.append(subset._get_object_as_soft())\n        soft += [\"^%s = %s\" % (self.geotype, self.name),\n                 self._get_columns_as_string(),\n                 self._get_table_as_string()]\n        return \"\\n\".join(soft)", "response": "Return object as SOFT formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef phenotype_data(self):\n        if self._phenotype_data is None:\n            pheno_data = {}\n            for gsm_name, gsm in iteritems(self.gsms):\n                tmp = {}\n                for key, value in iteritems(gsm.metadata):\n                    if len(value) == 0:\n                        tmp[key] = np.nan\n                    elif key.startswith(\"characteristics_\"):\n                        for i, char in enumerate(value):\n                            char = re.split(\":\\s+\", char)\n                            char_type, char_value = [char[0],\n                                                     \": \".join(char[1:])]\n                            tmp[key + \".\" + str(\n                                i) + \".\" + char_type] = char_value\n                    else:\n                        tmp[key] = \",\".join(value)\n                pheno_data[gsm_name] = tmp\n            self._phenotype_data = DataFrame(pheno_data).T\n        return self._phenotype_data", "response": "Get the phenotype data for each of the sample."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_and_average(self, platform, expression_column, group_by_column,\n                          force=False, merge_on_column=None, gsm_on=None,\n                          gpl_on=None):\n        \"\"\"Merge and average GSE samples.\n\n        For given platform prepare the DataFrame with all the samples present in\n        the GSE annotated with given column from platform and averaged over\n        the column.\n\n        Args:\n            platform (:obj:`str` or :obj:`GEOparse.GPL`): GPL platform to use.\n            expression_column (:obj:`str`): Column name in which \"expressions\"\n                are represented\n            group_by_column (:obj:`str`): The data will be grouped and averaged\n                over this column and only this column will be kept\n            force (:obj:`bool`): If the name of the GPL does not match the\n                platform name in GSM proceed anyway\n            merge_on_column (:obj:`str`): Column to merge the data on - should\n                be present in both GSM and GPL\n            gsm_on (:obj:`str`): In the case columns to merge are different in\n                GSM and GPL use this column in GSM\n            gpl_on (:obj:`str`): In the case columns to merge are different in\n                GSM and GPL use this column in GPL\n\n        Returns:\n            :obj:`pandas.DataFrame`: Merged and averaged table of results.\n\n        \"\"\"\n        if isinstance(platform, str):\n            gpl = self.gpls[platform]\n        elif isinstance(platform, GPL):\n            gpl = platform\n        else:\n            raise ValueError(\"Platform has to be of type GPL or string with \"\n                             \"key for platform in GSE\")\n\n        data = []\n        for gsm in self.gsms.values():\n            if gpl.name == gsm.metadata['platform_id'][0]:\n                data.append(gsm.annotate_and_average(\n                    gpl=gpl,\n                    merge_on_column=merge_on_column,\n                    expression_column=expression_column,\n                    group_by_column=group_by_column,\n                    force=force,\n                    gpl_on=gpl_on,\n                    gsm_on=gsm_on))\n        if len(data) == 0:\n            logger.warning(\"No samples for the platform were found\\n\")\n            return None\n        elif len(data) == 1:\n            return data[0]\n        else:\n            return data[0].join(data[1:])", "response": "Merge and average GSE samples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pivot_samples(self, values, index=\"ID_REF\"):\n        data = []\n        for gsm in self.gsms.values():\n            tmp_data = gsm.table.copy()\n            tmp_data[\"name\"] = gsm.name\n            data.append(tmp_data)\n        ndf = concat(data).pivot(index=index, values=values, columns=\"name\")\n        return ndf", "response": "Pivot samples by specified column."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npivots the samples of a GSM with a column in the annotation table.", "response": "def pivot_and_annotate(self, values, gpl, annotation_column, gpl_on=\"ID\",\n                           gsm_on=\"ID_REF\"):\n        \"\"\"Annotate GSM with provided GPL.\n\n        Args:\n            values (:obj:`str`): Column to use as values eg. \"VALUES\"\n            gpl (:obj:`pandas.DataFrame` or :obj:`GEOparse.GPL`): A Platform or\n                DataFrame to annotate with.\n            annotation_column (:obj:`str`): Column in table for annotation.\n            gpl_on (:obj:`str`, optional): Use this column in GPL to merge.\n                Defaults to \"ID\".\n            gsm_on (:obj:`str`, optional): Use this column in GSM to merge.\n                Defaults to \"ID_REF\".\n\n        Returns:\n            pandas.DataFrame: Pivoted and annotated table of results\n\n        \"\"\"\n        if isinstance(gpl, GPL):\n            annotation_table = gpl.table\n        elif isinstance(gpl, DataFrame):\n            annotation_table = gpl\n        else:\n            raise TypeError(\"gpl should be a GPL object or a pandas.DataFrame\")\n        pivoted_samples = self.pivot_samples(values=values, index=gsm_on)\n        ndf = pivoted_samples.reset_index().merge(\n            annotation_table[[gpl_on, annotation_column]],\n            left_on=gsm_on,\n            right_on=gpl_on).set_index(gsm_on)\n        del ndf[gpl_on]\n        ndf.columns.name = 'name'\n        return ndf"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload supplementary files for each GSM in the series.", "response": "def download_supplementary_files(self, directory='series',\n                                     download_sra=True, email=None,\n                                     sra_kwargs=None, nproc=1):\n        \"\"\"Download supplementary data.\n\n        .. warning::\n\n            Do not use parallel option (nproc > 1) in the interactive shell.\n            For more details see `this issue <https://stackoverflow.com/questions/23641475/multiprocessing-working-in-python-but-not-in-ipython/23641560#23641560>`_\n            on SO.\n\n        Args:\n            directory (:obj:`str`, optional): Directory to download the data\n                (in this directory function will create new directory with the\n                files), by default this will be named with the series\n                name + _Supp.\n            download_sra (:obj:`bool`, optional): Indicates whether to download\n                SRA raw data too. Defaults to True.\n            email (:obj:`str`, optional): E-mail that will be provided to the\n                Entrez. Defaults to None.\n            sra_kwargs (:obj:`dict`, optional): Kwargs passed to the\n                GSM.download_SRA method. Defaults to None.\n            nproc (:obj:`int`, optional): Number of processes for SRA download\n                (default is 1, no parallelization).\n\n        Returns:\n            :obj:`dict`: Downloaded data for each of the GSM\n        \"\"\"\n        if sra_kwargs is None:\n            sra_kwargs = dict()\n        if directory == 'series':\n            dirpath = os.path.abspath(self.get_accession() + \"_Supp\")\n            utils.mkdir_p(dirpath)\n        else:\n            dirpath = os.path.abspath(directory)\n            utils.mkdir_p(dirpath)\n        downloaded_paths = dict()\n        if nproc == 1:\n            # No need to parallelize, running ordinary download in loop\n            downloaded_paths = dict()\n            for gsm in itervalues(self.gsms):\n                logger.info(\n                    \"Downloading SRA files for %s series\\n\" % gsm.name)\n                paths = gsm.download_supplementary_files(email=email,\n                                                         download_sra=download_sra,\n                                                         directory=dirpath,\n                                                         sra_kwargs=sra_kwargs)\n                downloaded_paths[gsm.name] = paths\n        elif nproc > 1:\n            # Parallelization enabled\n            downloaders = list()\n            # Collecting params for Pool.map in a loop\n            for gsm in itervalues(self.gsms):\n                downloaders.append([\n                    gsm,\n                    download_sra,\n                    email,\n                    dirpath,\n                    sra_kwargs])\n            p = Pool(nproc)\n            results = p.map(_supplementary_files_download_worker, downloaders)\n            downloaded_paths = dict(results)\n        else:\n            raise ValueError(\"Nproc should be non-negative: %s\" % str(nproc))\n\n        return downloaded_paths"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_SRA(self, email, directory='series', filterby=None, nproc=1,\n                     **kwargs):\n        \"\"\"Download SRA files for each GSM in series.\n\n        .. warning::\n\n            Do not use parallel option (nproc > 1) in the interactive shell.\n            For more details see `this issue <https://stackoverflow.com/questions/23641475/multiprocessing-working-in-python-but-not-in-ipython/23641560#23641560>`_\n            on SO.\n\n        Args:\n            email (:obj:`str`): E-mail that will be provided to the Entrez.\n            directory (:obj:`str`, optional): Directory to save the data\n                (defaults to the 'series' which saves the data to the directory\n                with the name of the series + '_SRA' ending).\n                Defaults to \"series\".\n            filterby (:obj:`str`, optional): Filter GSM objects, argument is a\n                function that operates on GSM object  and return bool\n                eg. lambda x: \"brain\" not in x.name. Defaults to None.\n            nproc (:obj:`int`, optional): Number of processes for SRA download\n                (default is 1, no parallelization).\n            **kwargs: Any arbitrary argument passed to GSM.download_SRA\n                method. See the documentation for more details.\n\n            Returns:\n                :obj:`dict`: A dictionary containing output of ``GSM.download_SRA``\n                    method where each GSM accession ID is the key for the\n                    output.\n        \"\"\"\n        if directory == 'series':\n            dirpath = os.path.abspath(self.get_accession() + \"_SRA\")\n            utils.mkdir_p(dirpath)\n        else:\n            dirpath = os.path.abspath(directory)\n            utils.mkdir_p(dirpath)\n        if filterby is not None:\n            gsms_to_use = [gsm for gsm in self.gsms.values() if filterby(gsm)]\n        else:\n            gsms_to_use = self.gsms.values()\n\n        if nproc == 1:\n            # No need to parallelize, running ordinary download in loop\n            downloaded_paths = dict()\n            for gsm in gsms_to_use:\n                logger.info(\n                    \"Downloading SRA files for %s series\\n\" % gsm.name)\n                downloaded_paths[gsm.name] = gsm.download_SRA(\n                    email=email,\n                    directory=dirpath,\n                    **kwargs)\n        elif nproc > 1:\n            # Parallelization enabled\n            downloaders = list()\n            # Collecting params for Pool.map in a loop\n            for gsm in gsms_to_use:\n                downloaders.append([\n                    gsm,\n                    email,\n                    dirpath,\n                    kwargs])\n\n            p = Pool(nproc)\n            results = p.map(_sra_download_worker, downloaders)\n            downloaded_paths = dict(results)\n        else:\n            raise ValueError(\"Nproc should be non-negative: %s\" % str(nproc))\n\n        return downloaded_paths", "response": "Download SRA files for each GSM in series."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_object_as_soft(self):\n        soft = []\n        if self.database is not None:\n            soft.append(self.database._get_object_as_soft())\n        soft += [\"^%s = %s\" % (self.geotype, self.name),\n                 self._get_metadata_as_string()]\n        for gsm in itervalues(self.gsms):\n            soft.append(gsm._get_object_as_soft())\n        for gpl in itervalues(self.gpls):\n            soft.append(gpl._get_object_as_soft())\n\n        return \"\\n\".join(soft)", "response": "Get object as SOFT formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the destination path.", "response": "def destination(self):\n        \"\"\"Get the destination path.\n\n        This is the property should be calculated every time it is used because\n        a user could change the outdir and filename dynamically.\n        \"\"\"\n        return os.path.join(os.path.abspath(self.outdir), self.filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload file with Aspera Connect.", "response": "def download_aspera(self, user, host, silent=False):\n        \"\"\"Download file with Aspera Connect.\n\n        For details see the documentation ov Aspera Connect\n\n        Args:\n            user (:obj:`str`): FTP user.\n            host (:obj:`str`): FTP host. Defaults to \"ftp-trace.ncbi.nlm.nih.gov\".\n        \"\"\"\n        aspera_home = os.environ.get(\"ASPERA_HOME\", None)\n        if not aspera_home:\n            raise ValueError(\"environment variable $ASPERA_HOME not set\")\n        if not os.path.exists(aspera_home):\n            raise ValueError(\n                \"$ASPERA_HOME directory {} does not exist\".format(aspera_home))\n        ascp = os.path.join(aspera_home, \"connect/bin/ascp\")\n        key = os.path.join(aspera_home, \"connect/etc/asperaweb_id_dsa.openssh\")\n        if not os.path.exists(ascp):\n            raise ValueError(\"could not find ascp binary\")\n        if not os.path.exists(key):\n            raise ValueError(\"could not find openssh key\")\n\n        parsed_url = urlparse(self.url)\n\n        cmd = \"{} -i {} -k1 -T -l400m {}@{}:{} {}\".format(\n            ascp, key, user, host, parsed_url.path, self._temp_file_name)\n        logger.debug(cmd)\n        try:\n            pr = sp.Popen(cmd, shell=True, stdout=sp.PIPE, stderr=sp.PIPE)\n            stdout, stderr = pr.communicate()\n            if not silent:\n                logger.debug(\"Aspera stdout: \" + str(stdout))\n                logger.debug(\"Aspera stderr: \" + str(stderr))\n            if pr.returncode == 0:\n                logger.debug(\"Moving %s to %s\" % (\n                    self._temp_file_name,\n                    self.destination))\n                shutil.move(self._temp_file_name, self.destination)\n                logger.debug(\"Successfully downloaded %s\" % self.url)\n            else:\n                logger.error(\n                    \"Failed to download %s using Aspera Connect\" % self.url)\n        finally:\n            try:\n                os.remove(self._temp_file_name)\n            except OSError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef md5sum(filename, blocksize=8192):\n        with open(filename, 'rb') as fh:\n            m = hashlib.md5()\n            while True:\n                data = fh.read(blocksize)\n                if not data:\n                    break\n                m.update(data)\n        return m.hexdigest()", "response": "Get the MD5 checksum of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_GEO(geo=None, filepath=None, destdir=\"./\", how='full',\n            annotate_gpl=False, geotype=None, include_data=False, silent=False,\n            aspera=False, partial=None):\n    \"\"\"Get the GEO entry.\n\n    The GEO entry is taken directly from the GEO database or read it from SOFT\n    file.\n\n    Args:\n        geo (:obj:`str`): GEO database identifier.\n        filepath (:obj:`str`): Path to local SOFT file. Defaults to None.\n        destdir (:obj:`str`, optional): Directory to download data. Defaults to\n            None.\n        how (:obj:`str`, optional): GSM download mode. Defaults to \"full\".\n        annotate_gpl (:obj:`bool`, optional): Download the GPL annotation\n            instead of regular GPL. If not available, fallback to regular GPL\n            file. Defaults to False.\n        geotype (:obj:`str`, optional): Type of GEO entry. By default it is\n            inferred from the ID or the file name.\n        include_data (:obj:`bool`, optional): Full download of GPLs including\n            series and samples. Defaults to False.\n        silent (:obj:`bool`, optional): Do not print anything. Defaults to\n            False.\n        aspera (:obj:`bool`, optional): EXPERIMENTAL Download using Aspera\n            Connect. Follow Aspera instructions for further details. Defaults\n            to False.\n        partial (:obj:'iterable', optional): A list of accession IDs of GSMs\n            to be partially extracted from GPL, works only if a file/accession\n            is a GPL.\n\n\n    Returns:\n        :obj:`GEOparse.BaseGEO`: A GEO object of given type.\n\n    \"\"\"\n    if geo is None and filepath is None:\n        raise Exception(\"You have to specify filename or GEO accession!\")\n    if geo is not None and filepath is not None:\n        raise Exception(\"You can specify filename or GEO accession - not both!\")\n\n    if silent:\n        logger.setLevel(100)  # More than critical\n\n    if filepath is None:\n        filepath, geotype = get_GEO_file(geo, destdir=destdir, how=how,\n                                         annotate_gpl=annotate_gpl,\n                                         include_data=include_data,\n                                         silent=silent,\n                                         aspera=aspera)\n    else:\n        if geotype is None:\n            geotype = path.basename(filepath)[:3]\n\n    logger.info(\"Parsing %s: \" % filepath)\n    if geotype.upper() == \"GSM\":\n        return parse_GSM(filepath)\n    elif geotype.upper() == \"GSE\":\n        return parse_GSE(filepath)\n    elif geotype.upper() == 'GPL':\n        return parse_GPL(filepath, partial=partial)\n    elif geotype.upper() == 'GDS':\n        return parse_GDS(filepath)\n    else:\n        raise ValueError((\"Unknown GEO type: %s. Available types: GSM, GSE, \"\n                          \"GPL and GDS.\") % geotype.upper())", "response": "Get the GEO entry from the database or read it from the SOFT file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading corresponding SOFT file given a GEO database identifier.", "response": "def get_GEO_file(geo, destdir=None, annotate_gpl=False, how=\"full\",\n                 include_data=False, silent=False, aspera=False):\n    \"\"\"Download corresponding SOFT file given GEO accession.\n\n    Args:\n        geo (:obj:`str`): GEO database identifier.\n        destdir (:obj:`str`, optional): Directory to download data. Defaults to\n            None.\n        annotate_gpl (:obj:`bool`, optional): Download the GPL annotation\n            instead of regular GPL. If not available, fallback to regular GPL\n            file. Defaults to False.\n        how (:obj:`str`, optional): GSM download mode. Defaults to \"full\".\n        include_data (:obj:`bool`, optional): Full download of GPLs including\n            series and samples. Defaults to False.\n        silent (:obj:`bool`, optional): Do not print anything. Defaults to\n            False.\n        aspera (:obj:`bool`, optional): EXPERIMENTAL Download using Aspera\n            Connect. Follow Aspera instructions for further details. Defaults\n            to False.\n\n    Returns:\n        :obj:`2-tuple` of :obj:`str` and :obj:`str`: Path to downloaded file and\n        and the type of GEO object.\n\n    \"\"\"\n    geo = geo.upper()\n    geotype = geo[:3]\n    range_subdir = sub(r\"\\d{1,3}$\", \"nnn\", geo)\n    if destdir is None:\n        tmpdir = mkdtemp()\n        logger.info(\"No destination directory specified.\"\n                    \" Temporary files will be downloaded at %s\" % tmpdir)\n    else:\n        tmpdir = destdir\n        utils.mkdir_p(tmpdir)\n\n    if geotype == \"GDS\":\n        gseurl = (\"ftp://ftp.ncbi.nlm.nih.gov/geo/\"\n                  \"{root}/{range_subdir}/{record}/soft/{record_file}\")\n        url = gseurl.format(root=\"datasets\",\n                            range_subdir=range_subdir,\n                            record=geo,\n                            record_file=\"%s.soft.gz\" % geo)\n        filepath = path.join(tmpdir, \"{record}.soft.gz\".format(record=geo))\n    elif geotype == \"GSE\":\n        gseurl = (\"ftp://ftp.ncbi.nlm.nih.gov/geo/\"\n                  \"{root}/{range_subdir}/{record}/soft/{record_file}\")\n        url = gseurl.format(root=\"series\",\n                            range_subdir=range_subdir,\n                            record=geo,\n                            record_file=\"%s_family.soft.gz\" % geo)\n        filepath = path.join(tmpdir,\n                             \"{record}_family.soft.gz\".format(record=geo))\n    elif geotype == \"GSM\":\n        gsmurl = (\"http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi\"\n                  \"?targ=self&acc={record}&form=text&view={how}\")\n        url = gsmurl.format(record=geo, how=how)\n        filepath = path.join(tmpdir, \"{record}.txt\".format(record=geo))\n    elif geotype == \"GPL\":\n        if annotate_gpl:\n            gplurl = (\"ftp://ftp.ncbi.nlm.nih.gov/geo/\"\n                      \"{root}/{range_subdir}/{record}/annot/{record_file}\")\n            url = gplurl.format(root=\"platforms\",\n                                range_subdir=range_subdir,\n                                record=geo,\n                                record_file=\"%s.annot.gz\" % geo)\n            filepath = path.join(tmpdir, \"{record}.annot.gz\".format(record=geo))\n            if not path.isfile(filepath):\n                try:\n                    logger.info(\"Downloading %s to %s\" % (url, filepath))\n                    utils.download_from_url(url, filepath, silent=silent,\n                                            aspera=aspera)\n                    return filepath, geotype\n                except URLError:\n                    logger.info((\"Annotations for %s are not available, \"\n                                 \"trying submitter GPL\") % geo)\n            else:\n                logger.info(\"File already exist: using local version.\")\n                return filepath, geotype\n\n        if include_data:\n            url = (\"ftp://ftp.ncbi.nlm.nih.gov/geo/platforms/\"\n                   \"{0}/{1}/soft/{1}_family.soft.gz\").format(\n                range_subdir,\n                geo)\n            filepath = path.join(tmpdir,\n                                 \"{record}_family.soft.gz\".format(record=geo))\n        else:\n            gplurl = (\"http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi\"\n                      \"?targ=self&acc={record}&form=text&view={how}\")\n            url = gplurl.format(record=geo, how=how)\n            filepath = path.join(tmpdir, \"{record}.txt\".format(record=geo))\n        if not path.isfile(filepath):\n            logger.info(\"Downloading %s to %s\" % (url, filepath))\n            utils.download_from_url(url, filepath, silent=silent, aspera=aspera)\n        else:\n            logger.info(\"File already exist: using local version.\")\n        return filepath, geotype\n    else:\n        raise UnknownGEOTypeException(\"%s type is not known\" % geotype)\n\n    if not path.isfile(filepath):\n        logger.info(\"Downloading %s to %s\" % (url, filepath))\n        utils.download_from_url(url, filepath, silent=silent, aspera=aspera)\n    else:\n        logger.info(\"File already exist: using local version.\")\n\n    return filepath, geotype"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the entry name line that starts with ^ or! or '#'.", "response": "def __parse_entry(entry_line):\n    \"\"\"Parse the SOFT file entry name line that starts with '^', '!' or '#'.\n\n    Args:\n        entry_line (:obj:`str`): Line from SOFT  to be parsed.\n\n    Returns:\n        :obj:`2-tuple`: Type of entry, value of entry.\n\n    \"\"\"\n    if entry_line.startswith(\"!\"):\n        entry_line = sub(r\"!\\w*?_\", '', entry_line)\n    else:\n        entry_line = entry_line.strip()[1:]\n    try:\n        entry_type, entry_name = [i.strip() for i in entry_line.split(\"=\", 1)]\n    except ValueError:\n        entry_type = [i.strip() for i in entry_line.split(\"=\", 1)][0]\n        entry_name = ''\n    return entry_type, entry_name"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse list of lines with metadata information from SOFT file.", "response": "def parse_metadata(lines):\n    \"\"\"Parse list of lines with metadata information from SOFT file.\n\n    Args:\n        lines (:obj:`Iterable`): Iterator over the lines.\n\n    Returns:\n        :obj:`dict`: Metadata from SOFT file.\n\n    \"\"\"\n    meta = defaultdict(list)\n    for line in lines:\n        line = line.rstrip()\n        if line.startswith(\"!\"):\n            if \"_table_begin\" in line or \"_table_end\" in line:\n                continue\n            key, value = __parse_entry(line)\n            meta[key].append(value)\n\n    return dict(meta)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing list of lines with columns description from SOFT file.", "response": "def parse_columns(lines):\n    \"\"\"Parse list of lines with columns description from SOFT file.\n\n    Args:\n        lines (:obj:`Iterable`): Iterator over the lines.\n\n    Returns:\n        :obj:`pandas.DataFrame`: Columns description.\n\n    \"\"\"\n    data = []\n    index = []\n    for line in lines:\n        line = line.rstrip()\n        if line.startswith(\"#\"):\n            tmp = __parse_entry(line)\n            data.append(tmp[1])\n            index.append(tmp[0])\n\n    return DataFrame(data, index=index, columns=['description'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_GDS_columns(lines, subsets):\n    data = []\n    index = []\n    for line in lines:\n        line = line.rstrip()\n        if line.startswith(\"#\"):\n            tmp = __parse_entry(line)\n            data.append(tmp[1])\n            index.append(tmp[0])\n\n    df = DataFrame(data, index=index, columns=['description'])\n    subset_ids = defaultdict(dict)\n    for subsetname, subset in iteritems(subsets):\n        for expid in subset.metadata[\"sample_id\"][0].split(\",\"):\n            try:\n                subset_type = subset.get_type()\n                subset_ids[subset_type][expid] = \\\n                    subset.metadata['description'][0]\n            except Exception as err:\n                logger.error(\"Error processing subsets: %s for subset %s\" % (\n                    subset.get_type(), subsetname))\n\n    return df.join(DataFrame(subset_ids))", "response": "Parse list of line with columns description from SOFT file of GDS."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse list of lines from SOFT file into DataFrame.", "response": "def parse_table_data(lines):\n    \"\"\"\"Parse list of lines from SOFT file into DataFrame.\n\n    Args:\n        lines (:obj:`Iterable`): Iterator over the lines.\n\n    Returns:\n        :obj:`pandas.DataFrame`: Table data.\n\n    \"\"\"\n    # filter lines that do not start with symbols\n    data = \"\\n\".join([i.rstrip() for i in lines\n                      if not i.startswith((\"^\", \"!\", \"#\")) and i.rstrip()])\n    if data:\n        return read_csv(StringIO(data), index_col=None, sep=\"\\t\")\n    else:\n        return DataFrame()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a GSM entry from a SOFT file.", "response": "def parse_GSM(filepath, entry_name=None):\n    \"\"\"Parse GSM entry from SOFT file.\n\n    Args:\n        filepath (:obj:`str` or :obj:`Iterable`): Path to file with 1 GSM entry\n            or list of lines representing GSM from GSE file.\n        entry_name (:obj:`str`, optional): Name of the entry. By default it is\n            inferred from the data.\n\n    Returns:\n        :obj:`GEOparse.GSM`: A GSM object.\n\n    \"\"\"\n    if isinstance(filepath, str):\n        with utils.smart_open(filepath) as f:\n            soft = []\n            has_table = False\n            for line in f:\n                if \"_table_begin\" in line or (not line.startswith((\"^\", \"!\", \"#\"))):\n                    has_table = True\n                soft.append(line.rstrip())\n    else:\n        soft = []\n        has_table = False\n        for line in filepath:\n            if \"_table_begin\" in line or (not line.startswith((\"^\", \"!\", \"#\"))):\n                has_table = True\n            soft.append(line.rstrip())\n\n    if entry_name is None:\n        sets = [i for i in soft if i.startswith(\"^\")]\n        if len(sets) > 1:\n            raise Exception(\"More than one entry in GPL\")\n        if len(sets) == 0:\n            raise NoEntriesException(\n                \"No entries found. Check the if accession is correct!\")\n        entry_name = parse_entry_name(sets[0])\n\n    columns = parse_columns(soft)\n    metadata = parse_metadata(soft)\n    if has_table:\n        table_data = parse_table_data(soft)\n    else:\n        table_data = DataFrame()\n\n    gsm = GSM(name=entry_name,\n              table=table_data,\n              metadata=metadata,\n              columns=columns)\n\n    return gsm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_GPL(filepath, entry_name=None, partial=None):\n    gsms = {}\n    gses = {}\n    gpl_soft = []\n    has_table = False\n    gpl_name = entry_name\n    database = None\n    if isinstance(filepath, str):\n        with utils.smart_open(filepath) as soft:\n            groupper = groupby(soft, lambda x: x.startswith(\"^\"))\n            for is_new_entry, group in groupper:\n                if is_new_entry:\n                    entry_type, entry_name = __parse_entry(next(group))\n                    logger.debug(\"%s: %s\" % (entry_type.upper(), entry_name))\n                    if entry_type == \"SERIES\":\n                        is_data, data_group = next(groupper)\n                        gse_metadata = parse_metadata(data_group)\n\n                        gses[entry_name] = GSE(name=entry_name,\n                                               metadata=gse_metadata)\n                    elif entry_type == \"SAMPLE\":\n                        if partial and entry_name not in partial:\n                            continue\n                        is_data, data_group = next(groupper)\n                        gsms[entry_name] = parse_GSM(data_group, entry_name)\n                    elif entry_type == \"DATABASE\":\n                        is_data, data_group = next(groupper)\n                        database_metadata = parse_metadata(data_group)\n                        database = GEODatabase(name=entry_name,\n                                               metadata=database_metadata)\n\n                    elif entry_type == \"PLATFORM\" or entry_type == \"Annotation\":\n                        gpl_name = entry_name\n                        is_data, data_group = next(groupper)\n                        has_gpl_name = gpl_name or gpl_name is None\n                        for line in data_group:\n                            if (\"_table_begin\" in line or\n                                    not line.startswith((\"^\", \"!\", \"#\"))):\n                                has_table = True\n                            if not has_gpl_name:\n                                if match(\"!Annotation_platform\\s*=\\s*\", line):\n                                    gpl_name = split(\"\\s*=\\s*\", line)[-1].strip()\n                                    has_gpl_name = True\n                            gpl_soft.append(line)\n                    else:\n                        raise RuntimeError(\n                            \"Cannot parse {etype}. Unknown for GPL.\".format(\n                                etype=entry_type\n                            ))\n    else:\n        for line in filepath:\n            if \"_table_begin\" in line or (not line.startswith((\"^\", \"!\", \"#\"))):\n                has_table = True\n            gpl_soft.append(line.rstrip())\n\n    columns = None\n    try:\n        columns = parse_columns(gpl_soft)\n    except Exception:\n        pass\n    metadata = parse_metadata(gpl_soft)\n\n    if has_table:\n        table_data = parse_table_data(gpl_soft)\n    else:\n        table_data = DataFrame()\n\n    gpl = GPL(name=gpl_name,\n              gses=gses,\n              gsms=gsms,\n              table=table_data,\n              metadata=metadata,\n              columns=columns,\n              database=database\n              )\n\n    # link samples to series, if these were present in the GPL soft file\n    for gse_id, gse in gpl.gses.items():\n        for gsm_id in gse.metadata.get(\"sample_id\", []):\n            if gsm_id in gpl.gsms:\n                gpl.gses[gse_id].gsms[gsm_id] = gpl.gsms[gsm_id]\n\n    return gpl", "response": "Parse a GPL entry from a GSE file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses GSE SOFT file. Args: filepath (:obj:`str`): Path to GSE SOFT file. Returns: :obj:`GEOparse.GSE`: A GSE object.", "response": "def parse_GSE(filepath):\n    \"\"\"Parse GSE SOFT file.\n\n    Args:\n        filepath (:obj:`str`): Path to GSE SOFT file.\n\n    Returns:\n        :obj:`GEOparse.GSE`: A GSE object.\n\n    \"\"\"\n    gpls = {}\n    gsms = {}\n    series_counter = 0\n    database = None\n    metadata = {}\n    gse_name = None\n    with utils.smart_open(filepath) as soft:\n        groupper = groupby(soft, lambda x: x.startswith(\"^\"))\n        for is_new_entry, group in groupper:\n            if is_new_entry:\n                entry_type, entry_name = __parse_entry(next(group))\n                logger.debug(\"%s: %s\" % (entry_type.upper(), entry_name))\n                if entry_type == \"SERIES\":\n                    gse_name = entry_name\n                    series_counter += 1\n                    if series_counter > 1:\n                        raise Exception(\n                            \"GSE file should contain only one series entry!\")\n                    is_data, data_group = next(groupper)\n                    message = (\"The key is not False, probably there is an \"\n                               \"error in the SOFT file\")\n                    assert not is_data, message\n                    metadata = parse_metadata(data_group)\n                elif entry_type == \"SAMPLE\":\n                    is_data, data_group = next(groupper)\n                    gsms[entry_name] = parse_GSM(data_group, entry_name)\n                elif entry_type == \"PLATFORM\":\n                    is_data, data_group = next(groupper)\n                    gpls[entry_name] = parse_GPL(data_group, entry_name)\n                elif entry_type == \"DATABASE\":\n                    is_data, data_group = next(groupper)\n                    database_metadata = parse_metadata(data_group)\n                    database = GEODatabase(name=entry_name,\n                                           metadata=database_metadata)\n                else:\n                    logger.error(\"Cannot recognize type %s\" % entry_type)\n    gse = GSE(name=gse_name,\n              metadata=metadata,\n              gpls=gpls,\n              gsms=gsms,\n              database=database)\n    return gse"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse GDS SOFT file. Args: filepath (:obj:`str`): Path to GDS SOFT file. Returns: :obj:`GEOparse.GDS`: A GDS object.", "response": "def parse_GDS(filepath):\n    \"\"\"Parse GDS SOFT file.\n\n    Args:\n        filepath (:obj:`str`): Path to GDS SOFT file.\n\n    Returns:\n        :obj:`GEOparse.GDS`: A GDS object.\n\n    \"\"\"\n    dataset_lines = []\n    subsets = {}\n    database = None\n    dataset_name = None\n    with utils.smart_open(filepath) as soft:\n        groupper = groupby(soft, lambda x: x.startswith(\"^\"))\n        for is_new_entry, group in groupper:\n            if is_new_entry:\n                entry_type, entry_name = __parse_entry(next(group))\n                logger.debug(\"%s: %s\" % (entry_type.upper(), entry_name))\n                if entry_type == \"SUBSET\":\n                    is_data, data_group = next(groupper)\n                    message = (\"The key is not False, probably there is an \"\n                               \"error in the SOFT file\")\n                    assert not is_data, message\n                    subset_metadata = parse_metadata(data_group)\n                    subsets[entry_name] = GDSSubset(name=entry_name,\n                                                    metadata=subset_metadata)\n                elif entry_type == \"DATABASE\":\n\n                    is_data, data_group = next(groupper)\n                    message = (\"The key is not False, probably there is an \"\n                               \"error in the SOFT file\")\n                    assert not is_data, message\n                    database_metadata = parse_metadata(data_group)\n                    database = GEODatabase(name=entry_name,\n                                           metadata=database_metadata)\n                elif entry_type == \"DATASET\":\n                    is_data, data_group = next(groupper)\n                    dataset_name = entry_name\n                    for line in data_group:\n                        dataset_lines.append(line.rstrip())\n                else:\n                    logger.error(\"Cannot recognize type %s\" % entry_type)\n\n    metadata = parse_metadata(dataset_lines)\n    columns = parse_GDS_columns(dataset_lines, subsets)\n    table = parse_table_data(dataset_lines)\n    return GDS(name=dataset_name, metadata=metadata, columns=columns,\n               table=table, subsets=subsets, database=database)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes directory ( ies.", "response": "def mkdir_p(path_to_dir):\n    \"\"\"Make directory(ies).\n\n    This function behaves like mkdir -p.\n\n    Args:\n        path_to_dir (:obj:`str`): Path to the directory to make.\n    \"\"\"\n    try:\n        os.makedirs(path_to_dir)\n    except OSError as e:  # Python >2.5\n        if e.errno == EEXIST and os.path.isdir(path_to_dir):\n            logger.debug(\n                \"Directory %s already exists. Skipping.\" % path_to_dir)\n        else:\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_from_url(url, destination_path,\n                      force=False, aspera=False, silent=False):\n    \"\"\"Download file from remote server.\n\n    If the file is already downloaded and  ``force`` flag is on the file will\n    be removed.\n\n    Args:\n        url (:obj:`str`): Path to the file on remote server (including file\n            name)\n        destination_path (:obj:`str`): Path to the file on local machine\n            (including file name)\n        force (:obj:`bool`): If file exist force to overwrite it. Defaults to\n            False.\n        aspera (:obj:`bool`): Download with Aspera Connect. Defaults to False.\n        silent (:obj:`bool`): Do not print any message. Defaults to False.\n    \"\"\"\n    if aspera and url.startswith(\"http\"):\n        logger.warn(\"Aspera Connect allows only FTP servers - falling back to \"\n                    \"normal download\")\n        aspera = False\n\n    try:\n        fn = Downloader(\n            url,\n            outdir=os.path.dirname(destination_path))\n        if aspera:\n            fn.download_aspera(\n                user=\"anonftp\",\n                host=\"ftp-trace.ncbi.nlm.nih.gov\",\n                silent=silent)\n        else:\n            fn.download(silent=silent, force=force)\n    except URLError:\n        logger.error(\"Cannot find file %s\" % url)", "response": "Download file from remote server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smart_open(filepath):\n    if filepath[-2:] == \"gz\":\n        mode = \"rt\"\n        fopen = gzip.open\n    else:\n        mode = \"r\"\n        fopen = open\n    if sys.version_info[0] < 3:\n        fh = fopen(filepath, mode)\n    else:\n        fh = fopen(filepath, mode, errors=\"ignore\")\n    try:\n        yield fh\n    except IOError:\n        fh.close()\n    finally:\n        fh.close()", "response": "Open file intelligently depending on the source and python version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntuning a single pipeline with a single tuner", "response": "def tune_pipeline(X, y, X_val, y_val, generate_model, tuner):\n    \"\"\"\n    Tunes a specified pipeline with the\n    specified tuner for TUNING_BUDGET_PER_ITER (3) iterations.\n\n    Params:\n        X: np.array of X training data\n        y: np.array of y training data\n        X_val: np.array of X validation data\n        y_val: np.array of y validation data\n        generate_model: function that returns an slkearn model to fit\n        tuner: BTB tuner object for tuning hyperparameters\n    \"\"\"\n    print(\"Tuning with GP tuner for %s iterations\" % TUNING_BUDGET_PER_ITER)\n    for i in range(TUNING_BUDGET_PER_ITER):\n        params = tuner.propose()\n        # create model using proposed hyperparams from tuner\n        model = generate_model(params)\n        model.fit(X, y)\n        predicted = model.predict(X_val)\n        score = accuracy_score(predicted, y_val)\n        # record hyper-param combination and score for tuning\n        tuner.add(params, score)\n    print(\"Final score:\", tuner._best_score)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bandit(self, choice_rewards):\n        return max(choice_rewards, key=lambda a: np.mean(choice_rewards[a]))", "response": "This method returns the next arm in the order that they should make next."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(self, choice_scores):\n        choice_rewards = {}\n        for choice, scores in choice_scores.items():\n            if choice not in self.choices:\n                continue\n\n            choice_rewards[choice] = self.compute_rewards(scores)\n\n        return self.bandit(choice_rewards)", "response": "Select the next best choice to make\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_rewards(self, scores):\n        if len(scores) > self.k:\n            scores = np.copy(scores)\n            inds = np.argsort(scores)[:-self.k]\n            scores[inds] = np.nan\n\n        return list(scores)", "response": "Retain the K best scores and replace the rest with nans"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef compute_rewards(self, scores):\n        k = self.k\n        m = max(len(scores) - k, 0)\n        best_scores = sorted(scores)[-k - 1:]\n        velocities = np.diff(best_scores)\n        nans = np.full(m, np.nan)\n        return list(velocities) + list(nans)", "response": "Compute the velocity of the best scores\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(self, dpp_vector):\n\n        # decompose X and generate the rankings of the elements in the\n        # decomposed matrix\n        dpp_vector_decomposed = self.mf_model.transform(dpp_vector)\n        dpp_vector_ranked = stats.rankdata(\n            dpp_vector_decomposed,\n            method='dense',\n        )\n\n        max_agreement_index = None\n        max_agreement = -1  # min value of Kendall Tau agremment\n        for i in range(self.dpp_ranked.shape[0]):\n            # calculate agreement between current row and X\n            agreement, _ = stats.kendalltau(\n                dpp_vector_ranked,\n                self.dpp_ranked[i, :],\n            )\n            if agreement > max_agreement:\n                max_agreement_index = i\n                max_agreement = agreement\n\n        if max_agreement_index is None:\n            max_agreement_index = np.random.randint(self.dpp_matrix.shape[0])\n        # store the row with the highest agreement for prediction\n        self.matching_dataset = self.dpp_matrix[max_agreement_index, :]", "response": "Fits the Kendall Tau agremment to the given DPP vector."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fit(self, X, y):\n        # first, train a gaussian process like normal\n        super(GPEiVelocity, self).fit(X, y)\n\n        # probability of uniform\n        self.POU = 0\n        if len(y) >= self.r_minimum:\n            # get the best few scores so far, and compute the average distance\n            # between them.\n            top_y = sorted(y)[-self.N_BEST_Y:]\n            velocities = [top_y[i + 1] - top_y[i] for i in range(len(top_y) - 1)]\n\n            # the probability of returning random parameters scales inversely with\n            # the \"velocity\" of top scores.\n            self.POU = np.exp(self.MULTIPLIER * np.mean(velocities))", "response": "Fit the GPEiVelocity model to the data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict(self, X):\n        if np.random.random() < self.POU:\n            # choose params at random to avoid local minima\n            return Uniform(self.tunables).predict(X)\n\n        return super(GPEiVelocity, self).predict(X)", "response": "Use the POU value we computed in fit to choose randomly between GPEi and GPEi - VAT"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretaining the K most recent scores and replace the rest with zeros", "response": "def compute_rewards(self, scores):\n        \"\"\"Retain the K most recent scores, and replace the rest with zeros\"\"\"\n        for i in range(len(scores)):\n            if i >= self.k:\n                scores[i] = 0.\n        return scores"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select(self, choice_scores):\n        # if we don't have enough scores to do K-selection, fall back to UCB1\n        min_num_scores = min([len(s) for s in choice_scores.values()])\n        if min_num_scores >= K_MIN:\n            logger.info('{klass}: using Best K bandit selection'.format(klass=type(self).__name__))\n            reward_func = self.compute_rewards\n        else:\n            logger.warning(\n                '{klass}: Not enough choices to do K-selection; using plain UCB1'\n                .format(klass=type(self).__name__))\n            reward_func = super(RecentKReward, self).compute_rewards\n\n        choice_rewards = {}\n        for choice, scores in choice_scores.items():\n            if choice not in self.choices:\n                continue\n            choice_rewards[choice] = reward_func(scores)\n\n        return self.bandit(choice_rewards)", "response": "Use the top k learner s scores for usage in rewards for the bandit calculation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_rewards(self, scores):\n        # take the k + 1 most recent scores so we can get k velocities\n        recent_scores = scores[:-self.k - 2:-1]\n        velocities = [recent_scores[i] - recent_scores[i + 1] for i in\n                      range(len(recent_scores) - 1)]\n        # pad the list out with zeros, so the length of the list is\n        # maintained\n        zeros = (len(scores) - self.k) * [0]\n        return velocities + zeros", "response": "Compute the velocity of the most recent scores. Return a list with those k velocities and zeros"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rosenbrock(x, y, a=1, b=100):\n    return -1 * ((a - x)**2 + b * (y - x**2)**2)", "response": "Bigger is better ; global optimum at x = a y = b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting the best set of frozen sets based on the traditional UCB1 criteria.", "response": "def select(self, choice_scores):\n        \"\"\"\n        Groups the frozen sets by algorithm and first chooses an algorithm based\n        on the traditional UCB1 criteria.\n\n        Next, from that algorithm's frozen sets, makes the final set choice.\n        \"\"\"\n        # choose algorithm using a bandit\n        alg_scores = {}\n        for algorithm, choices in self.by_algorithm.items():\n            # only make arms for algorithms that have options\n            if not set(choices) & set(choice_scores.keys()):\n                continue\n            # sum up lists to get a list of all the scores from any run of this\n            # algorithm\n            sublists = [choice_scores.get(c, []) for c in choices]\n            alg_scores[algorithm] = sum(sublists, [])\n\n        best_algorithm = self.bandit(alg_scores)\n\n        # now use only the frozen sets from the chosen algorithm\n        best_subset = self.by_algorithm[best_algorithm]\n        normal_ucb1 = UCB1(choices=best_subset)\n        return normal_ucb1.select(choice_scores)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bandit(self, choice_rewards):\n\n        # count the larger of 1 and the total number of arm pulls\n        total_pulls = max(1, sum(len(r) for r in choice_rewards.values()))\n\n        def ucb1(choice):\n            rewards = choice_rewards[choice]\n            choice_pulls = max(len(rewards), 1)\n            average_reward = np.nanmean(rewards) if len(rewards) else 0\n            error = np.sqrt(2.0 * np.log(total_pulls) / choice_pulls)\n            return average_reward + error\n\n        return max(shuffle(choice_rewards), key=ucb1)", "response": "Multi - armed bandit method which chooses the right arm for which the upper confidence bound of expected reward is greatest."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_rewards(self, scores):\n        # get the k + 1 best scores in descending order\n        best_scores = sorted(scores, reverse=True)[:self.k + 1]\n        velocities = [best_scores[i] - best_scores[i + 1]\n                      for i in range(len(best_scores) - 1)]\n\n        # pad the list out with zeros to maintain the length of the list\n        zeros = (len(scores) - self.k) * [0]\n        return velocities + zeros", "response": "Compute the velocities of average distance between the k + 1 best\n        scores. Return a list with those k velocities padded out with zeros so\n        that the count remains the same."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_grid(self):\n        grid_axes = []\n        for _, param in self.tunables:\n            grid_axes.append(param.get_grid_axis(self.grid_width))\n\n        return grid_axes", "response": "Generate the list of grid axes for each of the tunables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(self, X, y):\n        self.X = X\n        self.y = y", "response": "Fit the object to the data X and y."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets unused candidates from the grid or parameters.", "response": "def _candidates_from_grid(self, n=1000):\n        \"\"\"Get unused candidates from the grid or parameters.\"\"\"\n        used_vectors = set(tuple(v) for v in self.X)\n\n        # if every point has been used before, gridding is done.\n        grid_size = self.grid_width ** len(self.tunables)\n        if len(used_vectors) == grid_size:\n            return None\n\n        all_vectors = set(itertools.product(*self._grid_axes))\n        remaining_vectors = all_vectors - used_vectors\n        candidates = np.array(list(map(np.array, remaining_vectors)))\n\n        np.random.shuffle(candidates)\n        return candidates[0:n]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a matrix of random parameters column by column.", "response": "def _random_candidates(self, n=1000):\n        \"\"\"Generate a matrix of random parameters, column by column.\"\"\"\n\n        candidates = np.zeros((n, len(self.tunables)))\n        for i, tunable in enumerate(self.tunables):\n            param = tunable[1]\n            lo, hi = param.range\n            if param.is_integer:\n                column = np.random.randint(lo, hi + 1, size=n)\n\n            else:\n                diff = hi - lo\n                column = lo + diff * np.random.rand(n)\n\n            candidates[:, i] = column\n\n        return candidates"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_candidates(self, n=1000):\n        # If using a grid, generate a list of previously unused grid points\n        if self.grid:\n            return self._candidates_from_grid(n)\n\n        # If not using a grid, generate a list of vectors where each parameter\n        # is chosen uniformly at random\n        else:\n            return self._random_candidates(n)", "response": "Generate random hyperparameter vectors for the current set of hyperparameter tables."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef propose(self, n=1):\n        proposed_params = []\n\n        for i in range(n):\n            # generate a list of random candidate vectors. If self.grid == True\n            # each candidate will be a vector that has not been used before.\n            candidate_params = self._create_candidates()\n\n            # create_candidates() returns None when every grid point\n            # has been tried\n            if candidate_params is None:\n                return None\n\n            # predict() returns a tuple of predicted values for each candidate\n            predictions = self.predict(candidate_params)\n\n            # acquire() evaluates the list of predictions, selects one,\n            # and returns its index.\n            idx = self._acquire(predictions)\n\n            # inverse transform acquired hyperparameters\n            # based on hyparameter type\n            params = {}\n            for i in range(candidate_params[idx, :].shape[0]):\n                inverse_transformed = self.tunables[i][1].inverse_transform(\n                    candidate_params[idx, i]\n                )\n                params[self.tunables[i][0]] = inverse_transformed\n            proposed_params.append(params)\n\n        return params if n == 1 else proposed_params", "response": "Use the trained model to propose a new set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, X, y):\n        if isinstance(X, dict):\n            X = [X]\n            y = [y]\n\n        # transform the list of dictionaries into a np array X_raw\n        for i in range(len(X)):\n            each = X[i]\n            # update best score and hyperparameters\n            if y[i] > self._best_score:\n                self._best_score = y[i]\n                self._best_hyperparams = X[i]\n\n            vectorized = []\n            for tunable in self.tunables:\n                vectorized.append(each[tunable[0]])\n\n            if self.X_raw is not None:\n                self.X_raw = np.append(\n                    self.X_raw,\n                    np.array([vectorized], dtype=object),\n                    axis=0,\n                )\n\n            else:\n                self.X_raw = np.array([vectorized], dtype=object)\n\n        self.y_raw = np.append(self.y_raw, y)\n\n        # transforms each hyperparameter based on hyperparameter type\n        x_transformed = np.array([], dtype=np.float64)\n        if len(self.X_raw.shape) > 1 and self.X_raw.shape[1] > 0:\n            x_transformed = self.tunables[0][1].fit_transform(\n                self.X_raw[:, 0],\n                self.y_raw,\n            ).astype(float)\n\n            for i in range(1, self.X_raw.shape[1]):\n                transformed = self.tunables[i][1].fit_transform(\n                    self.X_raw[:, i],\n                    self.y_raw,\n                ).astype(float)\n                x_transformed = np.column_stack((x_transformed, transformed))\n\n        self.fit(x_transformed, self.y_raw)", "response": "Adds data about known tunable hyperparameter configurations and scores to the model with all data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_candidates(self):\n        candidates = np.where(self.dpp_vector == 0)\n        return None if len(candidates[0]) == 0 else candidates[0]", "response": "Finds the pipelines that have not yet been tried on the X table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef propose(self):\n        # generate a list of all the untried candidate pipelines\n        candidates = self._get_candidates()\n\n        # get_candidates() returns None when every possibility has been tried\n        if candidates is None:\n            return None\n\n        # predict() returns a predicted values for each candidate\n        predictions = self.predict(candidates)\n\n        # acquire() evaluates the list of predictions, selects one, and returns\n        # its index.\n        idx = self._acquire(predictions)\n        return candidates[idx]", "response": "Use the trained model to propose a new pipeline."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd data about known pipeline and scores to the model.", "response": "def add(self, X):\n        \"\"\"Add data about known pipeline and scores.\n\n        Updates ``dpp_vector`` and refits model with all data.\n\n        Args:\n            X (dict): mapping of pipeline indices to scores. Keys must correspond to the index of a\n                column in ``dpp_matrix`` and values are the corresponding score for pipeline on\n                the dataset.\n        \"\"\"\n        for each in X:\n            self.dpp_vector[each] = X[each]\n        self.fit(self.dpp_vector.reshape(1, -1))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a dict of variables that are relevant for the cluster setup process.", "response": "def to_vars_dict(self):\n        \"\"\"\n        Return local state which is relevant for the cluster setup process.\n        \"\"\"\n        return {\n            'azure_client_id':        self.client_id,\n            'azure_location':         self.location,\n            'azure_secret':           self.secret,\n            'azure_subscription_id':  self.subscription_id,\n            'azure_tenant_id':        self.tenant_id,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_az_api(self):\n        with self.__lock:\n            if self._resource_client is None:\n                log.debug(\"Making Azure `ServicePrincipalcredentials` object\"\n                          \" with tenant=%r, client_id=%r, secret=%r ...\",\n                          self.tenant_id, self.client_id,\n                          ('<redacted>' if self.secret else None))\n                credentials = ServicePrincipalCredentials(\n                    tenant=self.tenant_id,\n                    client_id=self.client_id,\n                    secret=self.secret,\n                )\n                log.debug(\"Initializing Azure `ComputeManagementclient` ...\")\n                self._compute_client = ComputeManagementClient(credentials, self.subscription_id)\n                log.debug(\"Initializing Azure `NetworkManagementclient` ...\")\n                self._network_client = NetworkManagementClient(credentials, self.subscription_id)\n                log.debug(\"Initializing Azure `ResourceManagementclient` ...\")\n                self._resource_client = ResourceManagementClient(credentials, self.subscription_id)\n                log.info(\"Azure API clients initialized.\")", "response": "Initializes the client objects for talking to Azure API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting a new instance of the given properties.", "response": "def start_instance(self, key_name, public_key_path, private_key_path,\n                       security_group, flavor, image_id, image_userdata,\n                       username='root',\n                       node_name=None,\n                       boot_disk_size=30,\n                       storage_account_type='Standard_LRS',\n                       **extra):\n        \"\"\"\n        Start a new VM using the given properties.\n\n        :param str key_name:\n          **unused in Azure**, only present for interface compatibility\n        :param str public_key_path:\n          path to ssh public key to authorize on the VM (for user `username`, see below)\n        :param str private_key_path:\n          **unused in Azure**, only present for interface compatibility\n        :param str security_group:\n          network security group to attach VM to, **currently unused**\n        :param str flavor:\n          machine type to use for the instance\n        :param str image_id:\n          disk image to use for the instance;\n          has the form *publisher/offer/sku/version*\n          (e.g., ``canonical/ubuntuserver/16.04.0-LTS/latest``)\n        :param str image_userdata:\n          command to execute after startup, **currently unused**\n        :param int boot_disk_size:\n          size of boot disk to use; values are specified in gigabytes.\n        :param str username:\n          username for the given ssh key\n          (default is ``root`` as it's always guaranteed to exist,\n          but you probably don't want to use that)\n        :param str storage_account_type:\n          Type of disks to attach to the VM. For a list of valid values,\n          see: https://docs.microsoft.com/en-us/rest/api/compute/disks/createorupdate#diskstorageaccounttypes\n\n        :return: tuple[str, str] -- resource group and node name of the started VM\n        \"\"\"\n        self._init_az_api()\n\n        # Warn of unsupported parameters, if set.  We do not warn\n        # about `user_key` or `private_key_path` since they come from\n        # a `[login/*]` section and those can be shared across\n        # different cloud providers.\n        if security_group and security_group != 'default':\n            warn(\"Setting `security_group` is currently not supported\"\n                 \" in the Azure cloud; VMs will all be attached to\"\n                 \" a network security group named after the cluster name.\")\n        if image_userdata:\n            warn(\"Parameter `image_userdata` is currently not supported\"\n                 \" in the Azure cloud and will be ignored.\")\n\n        # Use the cluster name to identify the Azure resource group;\n        # however, `Node.cluster_name` is not passed down here so\n        # extract it from the node name, which always contains it as\n        # the substring before the leftmost dash (see `cluster.py`,\n        # line 1182)\n        cluster_name, _ = node_name.split('-', 1)\n        with self.__lock:\n            if cluster_name not in self._resource_groups_created:\n                self._resource_client.resource_groups.create_or_update(\n                    cluster_name, {'location': self.location})\n                self._resource_groups_created.add(cluster_name)\n\n        # read public SSH key\n        with open(public_key_path, 'r') as public_key_file:\n            public_key = public_key_file.read()\n\n        image_publisher, image_offer, \\\n            image_sku, image_version = self._split_image_id(image_id)\n\n        if not security_group:\n            security_group = (cluster_name + '-secgroup')\n\n        net_parameters = {\n            'networkSecurityGroupName': {\n                'value': security_group,\n            },\n            'subnetName':     { 'value': cluster_name },\n        }\n        net_name = net_parameters['subnetName']['value']\n        with self.__lock:\n            if net_name not in self._networks_created:\n                log.debug(\n                    \"Creating network `%s` in Azure ...\", net_name)\n                oper = self._resource_client.deployments.create_or_update(\n                    cluster_name, net_name, {\n                        'mode':       DeploymentMode.incremental,\n                        'template':   self.net_deployment_template,\n                        'parameters': net_parameters,\n                    })\n                oper.wait()\n                self._networks_created.add(net_name)\n        boot_disk_size_gb = int(boot_disk_size)\n\n        vm_parameters = {\n            'adminUserName':  { 'value': username },\n            'imagePublisher': { 'value': image_publisher },  # e.g., 'canonical'\n            'imageOffer':     { 'value': image_offer },      # e.g., ubuntuserver\n            'imageSku':       { 'value': image_sku },        # e.g., '16.04.0-LTS'\n            'imageVersion':   { 'value': image_version },    # e.g., 'latest'\n            'networkSecurityGroupName': {\n                'value': security_group,\n            },\n            'sshKeyData':     { 'value': public_key },\n            'storageAccountName': {\n                'value': self._make_storage_account_name(\n                    cluster_name, node_name)\n            },\n            'storageAccountType': { 'value': storage_account_type },\n            'subnetName':     { 'value': cluster_name },\n            'vmName':         { 'value': node_name },\n            'vmSize':         { 'value': flavor },\n            'bootDiskSize':   { 'value': boot_disk_size_gb}\n        }\n        log.debug(\n            \"Deploying `%s` VM template to Azure ...\",\n            vm_parameters['vmName']['value'])\n        oper = self._resource_client.deployments.create_or_update(\n            cluster_name, node_name, {\n                'mode':       DeploymentMode.incremental,\n                'template':   self.vm_deployment_template,\n                'parameters': vm_parameters,\n            })\n        oper.wait()\n\n        # the `instance_id` is a composite type since we need both the\n        # resource group name and the vm name to uniquely identify a VM\n        return [cluster_name, node_name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop_instance(self, instance_id):\n        self._init_az_api()\n\n        cluster_name, node_name = instance_id\n        self._init_inventory(cluster_name)\n\n        for name, api_version in [\n                # we must delete resources in a specific order: e.g.,\n                # a public IP address cannot be deleted if it's still\n                # in use by a NIC...\n                (node_name,                '2018-06-01'),\n                (node_name + '-nic',       '2018-10-01'),\n                (node_name + '-public-ip', '2018-10-01'),\n                (node_name + '-disk',      '2018-09-30'),\n            (self._make_storage_account_name(\n                    cluster_name, node_name),\n                                           '2018-07-01'),\n        ]:\n            rsc_id = self._inventory[name]\n            log.debug(\"Deleting resource %s (`%s`) ...\", name, rsc_id)\n            oper = self._resource_client.resources.delete_by_id(rsc_id, api_version)\n            oper.wait()\n            del self._inventory[name]\n\n        self._vm_details.pop(node_name, None)\n\n        # if this was the last VM to be deleted, clean up leftover resource group\n        with self.__lock:\n            if len(self._inventory) == 2:\n                log.debug(\"Cleaning up leftover resource group ...\")\n                oper = self._resource_client.resource_groups.delete(cluster_name)\n                oper.wait()\n                self._inventory = {}", "response": "Stops the instance gracefully."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve all IP addresses associated to a given instance.", "response": "def get_ips(self, instance_id):\n        \"\"\"\n        Retrieves all IP addresses associated to a given instance.\n\n        :return: tuple (IPs)\n        \"\"\"\n        self._init_az_api()\n        cluster_name, node_name = instance_id\n        # XXX: keep in sync with contents of `vm_deployment_template`\n        ip_name = ('{node_name}-public-ip'.format(node_name=node_name))\n        ip = self._network_client.public_ip_addresses.get(cluster_name, ip_name)\n        if (ip.provisioning_state == 'Succeeded' and ip.ip_address):\n            return [ip.ip_address]\n        else:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the instance is up and running.", "response": "def is_instance_running(self, instance_id):\n        \"\"\"\n        Check if the instance is up and running.\n\n        :param str instance_id: instance identifier\n\n        :return: bool - True if running, False otherwise\n        \"\"\"\n        self._init_az_api()\n        # Here, it's always better if we update the instance.\n        vm = self._get_vm(instance_id, force_reload=True)\n        # FIXME: should we rather check `vm.instance_view.statuses`\n        # and search for `.code == \"PowerState/running\"`? or\n        # `vm.instance_view.vm_agent.statuses` and search for `.code\n        # == 'ProvisioningState/suceeded'`?\n        return vm.provisioning_state == u'Succeeded'"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_vm(self, instance_id, force_reload=True):\n        self._init_az_api()\n        if force_reload:\n            # Remove from cache and get from server again\n            self._inventory = {}\n        cluster_name, node_name = instance_id\n        self._init_inventory(cluster_name)\n\n        # if instance is known, return it\n        if node_name not in self._vm_details:\n            vm_info = self._compute_client.virtual_machines.get(\n                cluster_name, node_name, 'instanceView')\n            self._vm_details[node_name] = vm_info\n\n        try:\n            return self._vm_details[node_name]\n        except KeyError:\n            raise InstanceNotFoundError(\n                \"Instance `{instance_id}` not found\"\n                .format(instance_id=instance_id))", "response": "Get details on the VM with the given instance identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_bytes(s):\n    last = -1\n    unit = s[last].lower()\n    if unit.isdigit():\n        # `s` is a integral number\n        return int(s)\n    if unit == 'b':\n        # ignore the the 'b' or 'B' suffix\n        last -= 1\n        unit = s[last].lower()\n    if unit == 'i':\n        k = 1024\n        last -= 1\n        unit = s[last].lower()\n    else:\n        k = 1000\n    # convert the substring of `s` that does not include the suffix\n    if unit.isdigit():\n        return int(s[0:(last+1)])\n    if unit == 'k':\n        return int(float(s[0:last])*k)\n    if unit == 'm':\n        return int(float(s[0:last])*k*k)\n    if unit == 'g':\n        return int(float(s[0:last])*k*k*k)\n    if unit == 't':\n        return int(float(s[0:last])*k*k*k*k)\n    if unit == 'p':\n        return int(float(s[0:last])*k*k*k*k*k)\n    if unit == 'e':\n        return int(float(s[0:last])*k*k*k*k*k*k)\n    if unit == 'z':\n        return int(float(s[0:last])*k*k*k*k*k*k*k)\n    if unit == 'y':\n        return int(float(s[0:last])*k*k*k*k*k*k*k*k)", "response": "Convert a string s to an integer number of bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inspect_node(node):\n    node_information = {}\n    ssh = node.connect()\n    if not ssh:\n        log.error(\"Unable to connect to node %s\", node.name)\n        return\n\n    (_in, _out, _err) = ssh.exec_command(\"(type >& /dev/null -a srun && echo slurm) \\\n                      || (type >& /dev/null -a qconf && echo sge) \\\n                      || (type >& /dev/null -a pbsnodes && echo pbs) \\\n                      || echo UNKNOWN\")\n    node_information['type'] = _out.read().strip()\n\n    (_in, _out, _err) = ssh.exec_command(\"arch\")\n    node_information['architecture'] = _out.read().strip()\n\n    if node_information['type'] == 'slurm':\n        inspect_slurm_cluster(ssh, node_information)\n    elif node_information['type'] == 'sge':\n        inspect_sge_cluster(ssh, node_information)\n    ssh.close()\n    return node_information", "response": "This function accept a elasticluster. cluster. Node class and returns a dictionary containing the type of batch system installed and the architecture of the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a configuration file snippet to be used with GC3Pie.", "response": "def create_gc3pie_config_snippet(cluster):\n    \"\"\"\n    Create a configuration file snippet to be used with GC3Pie.\n    \"\"\"\n    auth_section = 'auth/elasticluster_%s' % cluster.name\n    resource_section = 'resource/elasticluster_%s' % cluster.name\n\n    cfg = RawConfigParser()\n    cfg.add_section(auth_section)\n\n    frontend_node = cluster.get_ssh_to_node()\n    cfg.set(auth_section, 'type', 'ssh')\n    cfg.set(auth_section, 'username', frontend_node.image_user)\n\n    cluster_info = inspect_node(frontend_node)\n    cfg.add_section(resource_section)\n    cfg.set(resource_section, 'enabled', 'yes')\n    cfg.set(resource_section, 'transport', 'ssh')\n    cfg.set(resource_section, 'frontend', frontend_node.preferred_ip)\n    if not cluster_info:\n        log.error(\"Unable to gather enough information from the cluster. \"\n                  \"Following informatino are only partial!\")\n        cluster_info = {'architecture': 'unknown',\n                        'type': 'unknown',\n                        'max_cores': -1,\n                        'max_cores_per_job': -1,\n                        'max_memory_per_core': -1,\n                        'max_walltime': '672hours'}\n\n    cfg.set(resource_section, 'type', cluster_info['type'])\n    cfg.set(resource_section, 'architecture', cluster_info['architecture'])\n    cfg.set(resource_section, 'max_cores', cluster_info.get('max_cores', 1))\n    cfg.set(resource_section, 'max_cores_per_job', cluster_info.get('max_cores_per_job', 1))\n    cfg.set(resource_section, 'max_memory_per_core', cluster_info.get('max_memory_per_core', '2GB'))\n    cfg.set(resource_section, 'max_walltime', cluster_info.get('max_walltime', '672hours'))\n\n    cfgstring = StringIO()\n    cfg.write(cfgstring)\n\n    return cfgstring.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a dictionary of variables that can be used to set the local state of the gcloud cluster.", "response": "def to_vars_dict(self):\n        \"\"\"\n        Return local state which is relevant for the cluster setup process.\n        \"\"\"\n        return {\n            'gcloud_client_id':      self._client_id,\n            'gcloud_client_secret':  self._client_secret,\n            'gcloud_network':        self._network,\n            'gcloud_project_id':     self._project_id,\n            'gcloud_zone':           self._zone,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect to the cloud web services.", "response": "def _connect(self):\n        \"\"\"Connects to the cloud web services. If this is the first\n        authentication, a web browser will be started to authenticate\n        against google and provide access to elasticluster.\n\n        :return: A Resource object with methods for interacting with the\n                 service.\n        \"\"\"\n        # ensure only one thread runs the authentication process, if needed\n        with GoogleCloudProvider.__gce_lock:\n            # check for existing connection\n            if not self._gce:\n                version = pkg_resources.get_distribution(\"elasticluster\").version\n                http = googleapiclient.http.set_user_agent(httplib2.Http(), \"elasticluster/%s\" % version)\n                credentials = self._get_credentials()\n                self._auth_http = credentials.authorize(http)\n\n                self._gce = build(GCE_API_NAME, GCE_API_VERSION, http=http)\n\n        return self._gce"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _execute_request(self, request):\n        with GoogleCloudProvider.__gce_lock:\n            return request.execute(http=self._auth_http)", "response": "Helper method to execute a request."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wait_until_done(self, response, wait=30):\n\n        gce = self._connect()\n\n        status = response['status']\n        while status != 'DONE' and response:\n            # wait a random amount of time (up to `wait` seconds)\n            if wait:\n                time.sleep(1 + random.randrange(wait))\n\n            operation_id = response['name']\n\n            # Identify if this is a per-zone resource\n            if 'zone' in response:\n                zone_name = response['zone'].split('/')[-1]\n                request = gce.zoneOperations().get(\n                    project=self._project_id, operation=operation_id,\n                    zone=zone_name)\n            else:\n                request = gce.globalOperations().get(\n                    project=self._project_id,\n                    operation=operation_id)\n\n            response = self._execute_request(request)\n            if response:\n                status = response['status']\n        return response", "response": "Blocks until the operation status is done for the given operation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a new instance with the given properties and returns the instance id.", "response": "def start_instance(self,\n                       # these are common to any\n                       # CloudProvider.start_instance() call\n                       key_name, public_key_path, private_key_path,\n                       security_group, flavor, image_id, image_userdata,\n                       username=None,\n                       # these params are specific to the\n                       # GoogleCloudProvider\n                       node_name=None,\n                       boot_disk_type='pd-standard',\n                       boot_disk_size=10,\n                       tags=None,\n                       scheduling=None,\n                       accelerator_count=0,\n                       accelerator_type='default',\n                       allow_project_ssh_keys=True,\n                       local_ssd_count=0,\n                       local_ssd_interface='SCSI',\n                       min_cpu_platform=None,\n                       **kwargs):\n        \"\"\"\n        Starts a new instance with the given properties and returns\n        the instance id.\n\n        :param str key_name: name of the ssh key to connect\n        :param str public_key_path: path to ssh public key\n        :param str private_key_path: path to ssh private key\n        :param str security_group: firewall rule definition to apply on the\n                                   instance\n        :param str flavor: machine type to use for the instance\n        :param str image_id: image type (os) to use for the instance\n        :param str image_userdata: command to execute after startup\n        :param str username: username for the given ssh key, default None\n        :param str node_name: name of the instance\n        :param str|Sequence tags: \"Tags\" to label the instance.\n          Can be either a single string (individual tags are comma-separated),\n          or a sequence of strings (each string being a single tag).\n        :param str scheduling: scheduling option to use for the instance (\"preemptible\")\n        :param int accelerator_count: Number of accelerators (e.g., GPUs) to make available in instance\n        :param str accelerator_type: Type of accelerator to request.  Can be one of:\n\n          * Full URL specifying an accelerator type valid for the zone and project VMs are being created in.  For example, ``https://www.googleapis.com/compute/v1/projects/[PROJECT_ID]/zones/[ZONE]/acceleratorTypes/[ACCELERATOR_TYPE]``\n          * An accelerator type name (any string which is not a valid URL).  This is internally prefixed with the string ``https://www.googleapis.com/compute/v1/projects/[PROJECT_ID]/zones/[ZONE]/acceleratorTypes/`` to form a full URL.\n        :param bool allow_project_ssh_keys:\n          When ``True`` (default), SSH login is allowed to a node\n          using any of the project-wide SSH keys (if they are\n          defined).  When ``False``, only the SSH key specified by\n          ElastiCluster config's ``[login/*]`` section will be allowed\n          to log in (instance-level key).\n        :param int local_ssd_count: Number of local SSD disks (each 375GB size) to make available in instance\n        :param int local_ssd_interface: Attachment interface for local SSD disks; either ``'SCSI'`` (default) or ``'NVME'``.\n        :param str min_cpu_platform: require CPUs of this type or better (e.g., \"Intel Skylake\")\n\n          Only used if ``accelerator_count`` is > 0.\n\n        :return: str - instance id of the started instance\n        \"\"\"\n        # construct URLs\n        project_url = '%s%s' % (GCE_URL, self._project_id)\n        machine_type_url = '%s/zones/%s/machineTypes/%s' \\\n                           % (project_url, self._zone, flavor)\n        boot_disk_type_url = '%s/zones/%s/diskTypes/%s' \\\n                           % (project_url, self._zone, boot_disk_type)\n        # FIXME: `conf.py` should ensure that `boot_disk_size` has the right\n        # type, so there would be no need to convert here\n        boot_disk_size_gb = int(boot_disk_size)\n        network_url = '%s/global/networks/%s' % (project_url, self._network)\n        if image_id.startswith('http://') or image_id.startswith('https://'):\n            image_url = image_id\n        else:\n            # allow image shortcuts (see docstring for IMAGE_NAME_SHORTCUTS)\n            for prefix, os_cloud in self.IMAGE_NAME_SHORTCUTS.items():\n                if image_id.startswith(prefix + '-'):\n                    image_url = '%s%s/global/images/%s' % (\n                        GCE_URL, os_cloud, image_id)\n                    break\n            else:\n                raise InstanceError(\n                    \"Unknown image name shortcut '{0}',\"\n                    \" please use the full `https://...` self-link URL.\"\n                    .format(image_id))\n\n        scheduling_option = {}\n        if scheduling == 'preemptible':\n            scheduling_option['preemptible'] = True\n        elif scheduling is not None:\n            raise InstanceError(\"Unknown scheduling option: '%s'\" % scheduling)\n\n        if isinstance(tags, (str,)):\n            tags = tags.split(',')\n        elif isinstance(tags, collections.Sequence):\n            # ok, nothing to do\n            pass\n        elif tags is not None:\n            raise TypeError(\n                \"The `tags` argument to `gce.start_instance`\"\n                \" should be a string or a list, got {T} instead\"\n                .format(T=type(tags)))\n\n        with open(public_key_path, 'r') as f:\n            public_key_content = f.read()\n\n        compute_metadata = [\n            {\n                \"key\": \"ssh-keys\",\n                \"value\": \"%s:%s\" % (username, public_key_content),\n            },\n            {\n                \"key\": \"block-project-ssh-keys\",\n                \"value\": (not allow_project_ssh_keys),\n            },\n        ]\n        if image_userdata:\n            compute_metadata.append({\n                \"key\": \"startup-script\",\n                \"value\": image_userdata,\n            })\n\n        # construct the request body\n        if node_name:\n            instance_id = node_name.lower().replace('_', '-')  # GCE doesn't allow \"_\"\n        else:\n            instance_id = 'elasticluster-%s' % uuid.uuid4()\n\n        instance = {\n            'name': instance_id,\n            'machineType': machine_type_url,\n            'tags': {\n              'items': tags,\n            },\n            'scheduling': scheduling_option,\n            'disks': [\n                {\n                    'type': 'PERSISTENT',\n                    'boot': 'true',\n                    'initializeParams' : {\n                        'diskName': \"%s-disk\" % instance_id,\n                        'diskType': boot_disk_type_url,\n                        'diskSizeGb': boot_disk_size_gb,\n                        'sourceImage': image_url,\n                    },\n                    'autoDelete': 'true',\n                },\n            ],\n            'networkInterfaces': [\n                {'accessConfigs': [\n                    {'type': 'ONE_TO_ONE_NAT',\n                     'name': 'External NAT'\n                    }],\n                 'network': network_url\n                }],\n            'serviceAccounts': [\n                {'email': self._email,\n                 'scopes': GCE_DEFAULT_SCOPES\n                }],\n            \"metadata\": {\n                \"kind\": \"compute#metadata\",\n                \"items\": compute_metadata,\n            }\n        }\n\n        if min_cpu_platform is not None:\n            instance['minCpuPlatform'] = min_cpu_platform\n\n        # add accelerators/GPUs if requested\n        if accelerator_count > 0:\n            if (accelerator_type.startswith('https://')\n                or accelerator_type.startswith('http://')):\n                # use URL as-is\n                accelerator_type_url = accelerator_type\n            else:\n                accelerator_type_url = (\n                    'https://www.googleapis.com/compute/{api_version}/'\n                    'projects/{project_id}/zones/{zone}/'\n                    'acceleratorTypes/{accelerator_type}'\n                    .format(\n                        api_version=GCE_API_VERSION,\n                        project_id=self._project_id,\n                        zone=self._zone,\n                        accelerator_type=accelerator_type\n                    ))\n            log.debug(\n                \"VM instance `%s`:\"\n                \" Requesting %d accelerator%s of type '%s'\",\n                instance_id, accelerator_count,\n                ('s' if accelerator_count > 1 else ''),\n                accelerator_type_url)\n            instance['guestAccelerators'] = [\n             {\n                 'acceleratorCount': accelerator_count,\n                 'acceleratorType': accelerator_type_url,\n             }\n            ]\n            # no live migration with GPUs,\n            # see: https://cloud.google.com/compute/docs/gpus#restrictions\n            instance['scheduling']['onHostMaintenance'] = 'TERMINATE'\n\n        # add local SSDs if requested\n        if local_ssd_count > 0:\n            log.debug(\n                \"VM instance `%s`:\"\n                \" Requesting %d local SSD%s with %s interface\",\n                instance_id, local_ssd_count,\n                ('s' if local_ssd_count > 1 else ''),\n                local_ssd_interface)\n            for n in range(local_ssd_count):\n                instance['disks'].append({\n                    'type': 'SCRATCH',\n                    'initializeParams' : {\n                        #'diskName': (\"local-ssd-%d\" % n),\n                        'diskType': (\n                            'https://www.googleapis.com/compute/v1'\n                            '/projects/{project_id}'\n                            '/zones/{zone}'\n                            '/diskTypes/local-ssd'\n                            .format(\n                                project_id=self._project_id,\n                                zone=self._zone,\n                            )\n                        ),\n                    },\n                    'interface': local_ssd_interface,\n                    'autoDelete': 'true',\n                })\n\n        # preemptible instances cannot be restarted automatically\n        instance['scheduling']['automaticRestart'] = (\n            not instance['scheduling'].get('preemptible', False))\n\n        # create the instance\n        gce = self._connect()\n        request = gce.instances().insert(\n            project=self._project_id, body=instance, zone=self._zone)\n        try:\n            response = self._execute_request(request)\n            response = self._wait_until_done(response)\n            self._check_response(response)\n            return instance_id\n        except (HttpError, CloudProviderError) as e:\n            log.error(\"Error creating instance `%s`\" % e)\n            raise InstanceError(\"Error creating instance `%s`\" % e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npauses the instance and retaining disk and config.", "response": "def pause_instance(self, instance_id):\n        \"\"\"Pauses the instance, retaining disk and config.\n\n        :param str instance_id: instance identifier\n        :raises: `InstanceError` if instance cannot be paused\n\n        :return: dict - information needed to restart instance.\n        \"\"\"\n\n        if not instance_id:\n            log.info(\"Instance to pause has no instance id.\")\n            return\n\n        gce = self._connect()\n\n        try:\n            request = gce.instances().stop(project=self._project_id,\n                                           instance=instance_id,\n                                           zone=self._zone)\n            operation = self._execute_request(request)\n            response = self._wait_until_done(operation)\n            self._check_response(response)\n            return {\"instance_id\": instance_id}\n        except HttpError as e:\n            log.error(\"Error stopping instance: `%s\", e)\n            raise InstanceError(\"Error stopping instance `%s`\", e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrestart a paused instance.", "response": "def resume_instance(self, paused_info):\n        \"\"\"Restarts a paused instance, retaining disk and config.\n\n        :param str instance_id: instance identifier\n        :raises: `InstanceError` if instance cannot be resumed.\n\n        :return: dict - information needed to restart instance.\n        \"\"\"\n\n        if not paused_info.get(\"instance_id\"):\n            log.info(\"Instance to stop has no instance id.\")\n            return\n\n        gce = self._connect()\n\n        try:\n            request = gce.instances().start(project=self._project_id,\n                                            instance=paused_info[\"instance_id\"],\n                                            zone=self._zone)\n            operation = self._execute_request(request)\n            response = self._wait_until_done(operation)\n            self._check_response(response)\n            return\n        except HttpError as e:\n            log.error(\"Error restarting instance: `%s\", e)\n            raise InstanceError(\"Error restarting instance `%s`\", e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstop the instance gracefully.", "response": "def stop_instance(self, instance_id):\n        \"\"\"Stops the instance gracefully.\n\n        :param str instance_id: instance identifier\n        :raises: `InstanceError` if instance can not be stopped\n        \"\"\"\n        if not instance_id:\n            log.info(\"Instance to stop has no instance id\")\n            return\n\n        gce = self._connect()\n\n        try:\n            request = gce.instances().delete(project=self._project_id,\n                                        instance=instance_id, zone=self._zone)\n            response = self._execute_request(request)\n            self._check_response(response)\n        except HttpError as e:\n            # If the instance does not exist, we get a 404\n            if e.resp.status == 404:\n                raise InstanceNotFoundError(\n                    \"Instance `{instance_id}` was not found\"\n                    .format(instance_id=instance_id))\n            else:\n                raise InstanceError(\n                    \"Could not stop instance `{instance_id}`: `{e}`\"\n                    .format(instance_id=instance_id, e=e))\n        except CloudProviderError as e:\n            raise InstanceError(\n                \"Could not stop instance `{instance_id}`: `{e}`\"\n                .format(instance_id=instance_id, e=e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_instances(self, filter=None):\n        gce = self._connect()\n\n        try:\n            request = gce.instances().list(\n                project=self._project_id, filter=filter, zone=self._zone)\n            response = self._execute_request(request)\n            self._check_response(response)\n        except (HttpError, CloudProviderError) as e:\n            raise InstanceError(\"could not retrieve all instances on the \"\n                                \"cloud: ``\" % e)\n\n        if response and 'items' in response:\n            return response['items']\n        else:\n            return list()", "response": "List all instances on GCE optionally filtering the results."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ips(self, instance_id):\n        if not instance_id:\n          raise InstanceError(\"could not retrieve the ip address for node: \"\n                              \"no associated instance id\")\n        gce = self._connect()\n        instances = gce.instances()\n        try:\n            request = instances.get(instance=instance_id,\n                                    project=self._project_id, zone=self._zone)\n            response = self._execute_request(request)\n            ip_public = None\n\n            # If the instance is in status TERMINATED, then there will be\n            # no IP addresses.\n            if response and response['status'] in ('STOPPING', 'TERMINATED'):\n              log.info(\"node '%s' state is '%s'; no IP address(es)\" %\n                       (instance_id, response['status']))\n              return [None]\n\n            if response and \"networkInterfaces\" in response:\n                interfaces = response['networkInterfaces']\n                if interfaces:\n                    if \"accessConfigs\" in interfaces[0]:\n                        ip_public = interfaces[0]['accessConfigs'][0]['natIP']\n\n            if ip_public:\n                return [ip_public]\n            else:\n                raise InstanceError(\"could not retrieve the ip address for \"\n                                    \"node `%s`, please check the node \"\n                                    \"through the cloud provider interface\"\n                                    % instance_id)\n\n        except (HttpError, CloudProviderError) as e:\n            raise InstanceError('could not retrieve the ip address of `%s`: '\n                                '`%s`' % (instance_id, e))", "response": "Retrieves the ip addresses from the cloud provider by the given instance id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether the instance is up and running.", "response": "def is_instance_running(self, instance_id):\n        \"\"\"Check whether the instance is up and running.\n\n        :param str instance_id: instance identifier\n        :reutrn: True if instance is running, False otherwise\n        \"\"\"\n        items = self.list_instances(filter=('name eq \"%s\"' % instance_id))\n        for item in items:\n            if item['status'] == 'RUNNING':\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_vars_dict(self):\n        return {\n            # connection data (= what is in the \"openrc\" file)\n            'os_auth_url':             self._os_auth_url,\n            'os_cacert':               (self._os_cacert or ''),\n            'os_password':             self._os_password,\n            'os_project_domain_name':  self._os_project_domain_name,\n            'os_region_name':          self._os_region_name,\n            'os_tenant_name':          self._os_tenant_name,\n            'os_user_domain_name':     self._os_user_domain_name,\n            'os_username':             self._os_username,\n            # API versioning\n            'os_compute_api_version':   self._compute_api_version,\n            'os_identity_api_version':  self._identity_api_version,\n            'os_image_api_version':     self._image_api_version,\n            'os_network_api_version':   self._network_api_version,\n            'os_volume_api_version':    self._volume_api_version,\n        }", "response": "Return a dict of variables that can be used to set up the cluster setup process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _init_os_api(self):\n        if not self.nova_client:\n            log.debug(\"Initializing OpenStack API clients:\"\n                      \" OS_AUTH_URL='%s'\"\n                      \" OS_USERNAME='%s'\"\n                      \" OS_USER_DOMAIN_NAME='%s'\"\n                      \" OS_PROJECT_NAME='%s'\"\n                      \" OS_PROJECT_DOMAIN_NAME='%s'\"\n                      \" OS_REGION_NAME='%s'\"\n                      \" OS_CACERT='%s'\"\n                      \"\", self._os_auth_url,\n                      self._os_username, self._os_user_domain_name,\n                      self._os_tenant_name, self._os_project_domain_name,\n                      self._os_region_name,\n                      self._os_cacert)\n            sess = self.__init_keystone_session()\n            log.debug(\"Creating OpenStack Compute API (Nova) v%s client ...\", self._compute_api_version)\n            self.nova_client = nova_client.Client(\n                self._compute_api_version, session=sess,\n                region_name=self._os_region_name,\n                cacert=self._os_cacert)\n            log.debug(\"Creating OpenStack Network API (Neutron) client ...\")\n            self.neutron_client = neutron_client.Client(\n                #self._network_api_version,  ## doesn't work as of Neutron Client 2 :-(\n                session=sess, region_name=self._os_region_name,\n                ca_cert=self._os_cacert)\n            # FIXME: Glance's `Client` class does not take an explicit\n            # `cacert` parameter, instead it relies on the `session`\n            # argument being \"A keystoneauth1 session that should be\n            # used for transport\" -- I presume this means that\n            # `cacert` only needs to be set there.  Is this true of\n            # other OpenStack client classes as well?\n            log.debug(\"Creating OpenStack Image API (Glance) v%s client ...\", self._image_api_version)\n            self.glance_client = glance_client.Client(\n                self._image_api_version, session=sess,\n                region_name=self._os_region_name)\n            log.debug(\"Creating OpenStack Volume API (Cinder) v%s client ...\", self._volume_api_version)\n            self.cinder_client = cinder_client.Client(\n                self._volume_api_version, session=sess,\n                region_name=self._os_region_name,\n                cacert=self._os_cacert)", "response": "Initializes the OpenStack API client objects for talking to OpenStack API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __init_keystone_session(self):\n        api = self._identity_api_version  # for readability\n        tried = []\n        if api in ['3', None]:\n            sess = self.__init_keystone_session_v3(check=(api is None))\n            tried.append('v3')\n            if sess:\n                return sess\n        if api in ['2', None]:\n            sess = self.__init_keystone_session_v2(check=(api is None))\n            tried.append('v2')\n            if sess:\n                return sess\n        raise RuntimeError(\n            \"Cannot establish Keystone session (tried: {0}).\"\n            .format(', '.join(tried)))", "response": "Create and return a Keystone session object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef __init_keystone_session_v2(self, check=False):\n        from keystoneauth1 import loading as keystone_v2\n        loader = keystone_v2.get_plugin_loader('password')\n        auth = loader.load_from_options(\n            auth_url=self._os_auth_url,\n            username=self._os_username,\n            password=self._os_password,\n            project_name=self._os_tenant_name,\n        )\n        sess = keystoneauth1.session.Session(auth=auth, verify=self._os_cacert)\n        if check:\n            log.debug(\"Checking that Keystone API v2 session works...\")\n            try:\n                # if session is invalid, the following will raise some exception\n                nova = nova_client.Client(self._compute_api_version, session=sess, cacert=self._os_cacert)\n                nova.flavors.list()\n            except keystoneauth1.exceptions.NotFound as err:\n                log.warning(\"Creating Keystone v2 session failed: %s\", err)\n                return None\n            except keystoneauth1.exceptions.ClientException as err:\n                log.error(\"OpenStack server rejected request (likely configuration error?): %s\", err)\n                return None  # FIXME: should we be raising an error instead?\n        # if we got to this point, v2 session is valid\n        log.info(\"Using Keystone API v2 session to authenticate to OpenStack\")\n        return sess", "response": "Create and return a session object using Keystone API v2."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a new Keystone API v3 session.", "response": "def __init_keystone_session_v3(self, check=False):\n        \"\"\"\n        Return a new session object, created using Keystone API v3.\n\n        .. note::\n\n          Note that the only supported authN method is password authentication;\n          token or other plug-ins are not currently supported.\n        \"\"\"\n        try:\n            # may fail on Python 2.6?\n            from keystoneauth1.identity import v3 as keystone_v3\n        except ImportError:\n            log.warning(\"Cannot load Keystone API v3 library.\")\n            return None\n        auth = keystone_v3.Password(\n            auth_url=self._os_auth_url,\n            username=self._os_username,\n            password=self._os_password,\n            user_domain_name=self._os_user_domain_name,\n            project_domain_name=self._os_project_domain_name,\n            project_name=self._os_tenant_name,\n        )\n        sess = keystoneauth1.session.Session(auth=auth, verify=self._os_cacert)\n        if check:\n            log.debug(\"Checking that Keystone API v3 session works...\")\n            try:\n                # if session is invalid, the following will raise some exception\n                nova = nova_client.Client(self._compute_api_version, session=sess)\n                nova.flavors.list()\n            except keystoneauth1.exceptions.NotFound as err:\n                log.warning(\"Creating Keystone v3 session failed: %s\", err)\n                return None\n            except keystoneauth1.exceptions.ClientException as err:\n                log.error(\"OpenStack server rejected request (likely configuration error?): %s\", err)\n                return None  # FIXME: should we be raising an error instead?\n        # if we got to this point, v3 session is valid\n        log.info(\"Using Keystone API v3 session to authenticate to OpenStack\")\n        return sess"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetects the preferred OpenStack Identity API version.", "response": "def __detect_os_identity_api_version(self):\n        \"\"\"\n        Return preferred OpenStack Identity API version (either one of the two strings ``'2'`` or ``'3'``) or ``None``.\n\n        The following auto-detection strategies are tried (in this order):\n\n        #. Read the environmental variable `OS_IDENTITY_API_VERSION` and check if its value is one of the two strings ``'2'`` or ``'3'``;\n        #. Check if a version tag like ``/v3`` or ``/v2.0`` ends the OpenStack auth URL.\n\n        If none of the above worked, return ``None``.\n\n        For more information on ``OS_IDENTITY_API_VERSION``, please see\n        `<https://docs.openstack.org/developer/python-openstackclient/authentication.html>`_.\n        \"\"\"\n        ver = os.getenv('OS_IDENTITY_API_VERSION', '')\n        if ver == '3':\n            log.debug(\n                \"Using OpenStack Identity API v3\"\n                \" because of environmental variable setting `OS_IDENTITY_API_VERSION=3`\")\n            return '3'\n        elif ver == '2' or ver.startswith('2.'):\n            log.debug(\n                \"Using OpenStack Identity API v2\"\n                \" because of environmental variable setting `OS_IDENTITY_API_VERSION=2`\")\n            return '2'\n        elif self._os_auth_url.endswith('/v3'):\n            log.debug(\n                \"Using OpenStack Identity API v3 because of `/v3` ending in auth URL;\"\n                \" set environmental variable OS_IDENTITY_API_VERSION to force use of Identity API v2 instead.\")\n            return '3'\n        elif self._os_auth_url.endswith('/v2.0'):\n            log.debug(\n                \"Using OpenStack Identity API v2 because of `/v2.0` ending in auth URL;\"\n                \" set environmental variable OS_IDENTITY_API_VERSION to force use of Identity API v3 instead.\")\n            return '2'\n        else:\n            # auto-detection failed, need to probe\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a new instance on the cloud using the given properties.", "response": "def start_instance(self, key_name, public_key_path, private_key_path,\n                       security_group, flavor, image_id, image_userdata,\n                       username=None, node_name=None, **kwargs):\n        \"\"\"Starts a new instance on the cloud using the given properties.\n        The following tasks are done to start an instance:\n\n        * establish a connection to the cloud web service\n        * check ssh keypair and upload it if it does not yet exist. This is\n          a locked process, since this function might be called in multiple\n          threads and we only want the key to be stored once.\n        * check if the security group exists\n        * run the instance with the given properties\n\n        :param str key_name: name of the ssh key to connect\n        :param str public_key_path: path to ssh public key\n        :param str private_key_path: path to ssh private key\n        :param str security_group: firewall rule definition to apply on the\n                                   instance\n        :param str flavor: machine type to use for the instance\n        :param str image_id: image type (os) to use for the instance\n        :param str image_userdata: command to execute after startup\n        :param str username: username for the given ssh key, default None\n\n        :return: str - instance id of the started instance\n        \"\"\"\n        self._init_os_api()\n\n        vm_start_args = {}\n\n        log.debug(\"Checking keypair `%s` ...\", key_name)\n        with OpenStackCloudProvider.__node_start_lock:\n            self._check_keypair(key_name, public_key_path, private_key_path)\n        vm_start_args['key_name'] = key_name\n\n        security_groups = [sg.strip() for sg in security_group.split(',')]\n        self._check_security_groups(security_groups)\n        vm_start_args['security_groups'] = security_groups\n\n\n        # Check if the image id is present.\n        if image_id not in [img.id for img in self._get_images()]:\n            raise ImageError(\n                    \"No image found with ID `{0}` in project `{1}` of cloud {2}\"\n                    .format(image_id, self._os_tenant_name, self._os_auth_url))\n        vm_start_args['userdata'] = image_userdata\n\n        # Check if the flavor exists\n        flavors = [fl for fl in self._get_flavors() if fl.name == flavor]\n        if not flavors:\n            raise FlavorError(\n                \"No flavor found with name `{0}` in project `{1}` of cloud {2}\"\n                .format(flavor, self._os_tenant_name, self._os_auth_url))\n        flavor = flavors[0]\n\n        availability_zone = kwargs.pop('availability_zone','')\n        vm_start_args['availability_zone']=availability_zone\n\n        network_ids = [net_id.strip()\n                       for net_id in kwargs.pop('network_ids', '').split(',')]\n        if network_ids:\n            nics = [{'net-id': net_id, 'v4-fixed-ip': ''}\n                    for net_id in network_ids ]\n            log.debug(\"Specifying networks for node %s: %s\",\n                      node_name, ', '.join([nic['net-id'] for nic in nics]))\n        else:\n            nics = None\n        vm_start_args['nics'] = nics\n\n        if 'boot_disk_size' in kwargs:\n            # check if the backing volume is already there\n            volume_name = '{name}-{id}'.format(name=node_name, id=image_id)\n            if volume_name in [v.name for v in self._get_volumes()]:\n                raise ImageError(\n                    \"Volume `{0}` already exists in project `{1}` of cloud {2}\"\n                    .format(volume_name, self._os_tenant_name, self._os_auth_url))\n\n            log.info('Creating volume `%s` to use as VM disk ...', volume_name)\n            try:\n                bds = int(kwargs['boot_disk_size'])\n                if bds < 1:\n                    raise ValueError('non-positive int')\n            except (ValueError, TypeError):\n                raise ConfigurationError(\n                    \"Invalid `boot_disk_size` specified:\"\n                    \" should be a positive integer, got {0} instead\"\n                    .format(kwargs['boot_disk_size']))\n            volume = self.cinder_client.volumes.create(\n                size=bds, name=volume_name, imageRef=image_id,\n                volume_type=kwargs.pop('boot_disk_type'))\n\n            # wait for volume to come up\n            volume_available = False\n            while not volume_available:\n                for v in self._get_volumes():\n                    if v.name == volume_name and v.status == 'available':\n                        volume_available = True\n                        break\n                sleep(1)  # FIXME: hard-coded waiting time\n\n            # ok, use volume as VM disk\n            vm_start_args['block_device_mapping'] = {\n                # FIXME: is it possible that `vda` is not the boot disk? e.g. if\n                # a non-paravirtualized kernel is being used?  should we allow\n                # to set the boot device as an image parameter?\n                'vda': ('{id}:::{delete_on_terminate}'\n                        .format(id=volume.id, delete_on_terminate=1)),\n            }\n\n        # due to some `nova_client.servers.create()` implementation weirdness,\n        # the first three args need to be spelt out explicitly and cannot be\n        # conflated into `**vm_start_args`\n        vm = self.nova_client.servers.create(node_name, image_id, flavor, **vm_start_args)\n\n        # allocate and attach a floating IP, if requested\n        request_floating_ip = kwargs.get(\n            'request_floating_ip',\n            self._request_floating_ip_default)\n\n        if request_floating_ip:\n\n            # wait for server to come up (otherwise floating IP can't be associated)\n            log.info(\"Waiting for instance `%s` (%s) to come up ...\", node_name, vm.id)\n            max_wait = int(kwargs.get('max_wait', 300))\n            waited = 0\n            while waited < max_wait:\n                if vm.status == 'ACTIVE':\n                    break\n                if vm.status == 'ERROR':\n                    raise RuntimeError(\n                        \"Failed to start VM {0}:\"\n                        \" OpenStack scheduling error.\"\n                        .format(vm.id))\n                vm = self.nova_client.servers.get(vm.id)\n                # FIXME: Configurable poll interval\n                sleep(3)\n                waited += 3\n            else:\n                raise RuntimeError(\n                    \"VM {0} didn't come up in {1:d} seconds\"\n                    .format(vm.id, max_wait))\n\n            # We need to list the floating IPs for this instance\n            try:\n                # python-novaclient <8.0.0\n                floating_ips = [ip for ip in self.nova_client.floating_ips.list()\n                                if ip.instance_id == vm.id]\n            except AttributeError:\n                floating_ips = (\n                    self.neutron_client\n                    .list_floatingips(id=vm.id)\n                    .get('floating_ips', []))\n            # allocate new floating IP if none given\n            if not floating_ips:\n                if 'floating_network_id' in kwargs:\n                    floating_networks = [kwargs.pop('floating_network_id')]\n                else:\n                    floating_networks = network_ids[:]\n                ip_addr = self._allocate_address(vm, floating_networks)\n                log.debug(\"VM `%s` was allocated floating IP: %r\", vm.id, ip_addr)\n            else:\n                log.debug(\"VM `%s` already allocated floating IPs: %r\", vm.id, floating_ips)\n\n        self._instances[vm.id] = vm\n\n        return vm.id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ips(self, instance_id):\n        self._init_os_api()\n        instance = self._load_instance(instance_id)\n        try:\n            ip_addrs = set([self.floating_ip])\n        except AttributeError:\n            ip_addrs = set([])\n        for ip_addr in sum(instance.networks.values(), []):\n            ip_addrs.add(ip_addr)\n        log.debug(\"VM `%s` has IP addresses %r\", instance_id, ip_addrs)\n        return list(ip_addrs)", "response": "Retrieves all IP addresses associated to a given instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if security groups exist.", "response": "def _check_security_groups(self, names):\n        \"\"\"\n        Raise an exception if any of the named security groups does not exist.\n\n        :param List[str] groups: List of security group names\n        :raises: `SecurityGroupError` if group does not exist\n        \"\"\"\n        self._init_os_api()\n        log.debug(\"Checking existence of security group(s) %s ...\", names)\n        try:\n            # python-novaclient < 8.0.0\n            security_groups = self.nova_client.security_groups.list()\n            existing = set(sg.name for sg in security_groups)\n        except AttributeError:\n            security_groups = self.neutron_client.list_security_groups()['security_groups']\n            existing = set(sg[u'name'] for sg in security_groups)\n\n        # TODO: We should be able to create the security group if it\n        # doesn't exist and at least add a rule to accept ssh access.\n        # Also, we should be able to add new rules to a security group\n        # if needed.\n        nonexisting = set(names) - existing\n        if nonexisting:\n            raise SecurityGroupError(\n                \"Security group(s) `{0}` do not exist\"\n                .format(', '.join(nonexisting)))\n\n        # if we get to this point, all sec groups exist\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_images(self):\n        self._init_os_api()\n        try:\n            # python-novaclient < 8.0.0\n            return self.nova_client.images.list()\n        except AttributeError:\n            # ``glance_client.images.list()`` returns a generator, but callers\n            # of `._get_images()` expect a Python list\n            return list(self.glance_client.images.list())", "response": "Get available images. We cache the results in order to reduce\n        network usage."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nallocates a floating IP to the given instance and dispatching to Compute or Network API depending on the installed packages.", "response": "def _allocate_address(self, instance, network_ids):\n        \"\"\"\n        Allocates a floating/public ip address to the given instance,\n        dispatching to either the Compute or Network API depending\n        on installed packages.\n\n        :param instance: instance to assign address to\n\n        :param list network_id: List of IDs (as strings) of networks\n          where to request allocation the floating IP.\n\n        :return: public ip address\n        \"\"\"\n        log.debug(\n            \"Trying to allocate floating IP for VM `%s` on network(s) %r\",\n            instance.id, network_ids)\n        try:\n            # on python-novaclient>=8.0.0 this fails with\n            # `AttributeError` since the `Client.floating_ips`\n            # attribute has been removed\n            return self._allocate_address_nova(instance, network_ids)\n        except AttributeError:\n            return self._allocate_address_neutron(instance, network_ids)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _allocate_address_nova(self, instance, network_ids):\n        self._init_os_api()\n        with OpenStackCloudProvider.__node_start_lock:\n            # Use the `novaclient` API (works with python-novaclient <8.0.0)\n            free_ips = [ip for ip in self.nova_client.floating_ips.list() if not ip.fixed_ip]\n            if not free_ips:\n                log.debug(\"Trying to allocate a new floating IP ...\")\n                free_ips.append(self.nova_client.floating_ips.create())\n            if free_ips:\n                ip = free_ips.pop()\n            else:\n                raise RuntimeError(\n                    \"Could not allocate floating IP for VM {0}\"\n                    .format(instance_id))\n            instance.add_floating_ip(ip)\n        return ip.ip", "response": "Allocates a floating IP address to the given instance using the Neutron API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _allocate_address_neutron(self, instance, network_ids):\n        self._init_os_api()\n        with OpenStackCloudProvider.__node_start_lock:\n            # Note: to return *all* addresses, all parameters to\n            # `neutron_client.list_floatingips()` should be left out;\n            # setting them to `None` (e.g., `fixed_ip_address=None`)\n            # results in an empty list...\n            free_ips = [\n                ip for ip in\n                self.neutron_client.list_floatingips().get('floatingips')\n                if (ip['floating_network_id'] in network_ids\n                    # keep only unallocated IP addrs\n                    and ip['fixed_ip_address'] is None\n                    and ip['port_id'] is None)\n            ]\n            if free_ips:\n                floating_ip = free_ips.pop()\n                log.debug(\"Using existing floating IP %r\", floating_ip)\n            else:\n                # FIXME: OpenStack Network API v2 requires that we specify\n                # a network ID along with the request for a floating IP.\n                # However, ElastiCluster configuration allows for multiple\n                # networks to be connected to a VM, but does not give any\n                # hint as to which one(s) should be used for such requests.\n                # So we try them all, ignoring errors until one request\n                # succeeds and hope that it's OK. One can imagine\n                # scenarios where this is *not* correct, but: (1) these\n                # scenarios are unlikely, and (2) the old novaclient code\n                # above has not even had the concept of multiple networks\n                # for floating IPs and no-one has complained in 5 years...\n                for network_id in network_ids:\n                    log.debug(\n                        \"Trying to allocate floating IP on network %s ...\", network_id)\n                    try:\n                        floating_ip = self.neutron_client.create_floatingip({\n                            'floatingip': {\n                                'floating_network_id':network_id,\n                            }}).get('floatingip')\n                        log.debug(\n                            \"Allocated IP address %s on network %s\",\n                            floating_ip['floating_ip_address'], network_id)\n                        break  # stop at first network where we get a floating IP\n                    except BadNeutronRequest as err:\n                        raise RuntimeError(\n                            \"Failed allocating floating IP on network {0}: {1}\"\n                            .format(network_id, err))\n            if floating_ip.get('floating_ip_address', None) is None:\n                raise RuntimeError(\n                    \"Could not allocate floating IP for VM {0}\"\n                    .format(instance_id))\n            # wait until at least one interface is up\n            interfaces = []\n            # FIXMEE: no timeout!\n            while not interfaces:\n                interfaces = instance.interface_list()\n                sleep(2)  ## FIXME: hard-coded value\n            # get port ID\n            for interface in interfaces:\n                log.debug(\n                    \"Instance %s (ID: %s):\"\n                    \" Checking if floating IP can be attached to interface %r ...\",\n                    instance.name, instance.id, interface)\n                # if interface.net_id not in network_ids:\n                #     log.debug(\n                #         \"Instance %s (ID: %s):\"\n                #         \" Skipping interface %r:\"\n                #         \" not attached to any of the requested networks.\",\n                #         instance.name, instance.id, interface)\n                #     continue\n                port_id = interface.port_id\n                if port_id is None:\n                    log.debug(\n                        \"Instance %s (ID: %s):\"\n                        \" Skipping interface %r: no port ID!\",\n                        instance.name, instance.id, interface)\n                    continue\n                log.debug(\n                    \"Instance `%s` (ID: %s):\"\n                    \" will assign floating IP to port ID %s (state: %s),\"\n                    \" already running IP addresses %r\",\n                    instance.name, instance.id,\n                    port_id, interface.port_state,\n                    [item['ip_address'] for item in interface.fixed_ips])\n                if interface.port_state != 'ACTIVE':\n                    log.warn(\n                        \"Instance `%s` (ID: %s):\"\n                        \" port `%s` is in state %s (epected 'ACTIVE' instead)\",\n                        instance.name, instance.id,\n                        port_id, interface.port_state)\n                break\n            else:\n                raise RuntimeError(\n                    \"Could not find port on network(s) {0}\"\n                    \" for instance {1} (ID: {2}) to bind a floating IP to.\"\n                    .format(network_ids, instance.name, instance.id))\n            # assign floating IP to port\n            floating_ip = self.neutron_client.update_floatingip(\n                floating_ip['id'], {\n                    'floatingip': {\n                        'port_id': port_id,\n                    },\n                }\n            ).get('floatingip')\n            ip_address = floating_ip['floating_ip_address']\n            log.debug(\"Assigned IP address %s to port %s\", ip_address, port_id)\n\n            log.info(\"Waiting 300s until floating IP %s is ACTIVE\", ip_address)\n            for i in range(300):\n                _floating_ip = self.neutron_client.show_floatingip(floating_ip['id'])\n                if _floating_ip['floatingip']['status'] != 'DOWN':\n                    break\n                sleep(1)\n\n            # Invalidate cache for this VM, as we just assigned a new IP\n            if instance.id in self._cached_instances:\n                del self._cached_instances[instance.id]\n        return ip_address", "response": "Allocates a floating IP address to the given instance using the OpenStack Network API v2 API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_cluster(self, cluster, extra_args=tuple()):\n        return self._run_playbook(cluster, self._playbook_path, extra_args)", "response": "Configure the cluster by running an Ansible playbook."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resume_cluster(self, cluster, extra_args=tuple()):\n        if self._resume_playbook_path is not None:\n            return self._run_playbook(cluster, self._resume_playbook_path, extra_args)\n        else:\n            log.warning(\"No resume playbook is available - falling back to the setup \"\n                        \"playbook, which could be slow.\")\n            return self.setup_cluster(cluster, extra_args)", "response": "This function will run a resume playbook if one is available."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the inventory for the given cluster and returns its path.", "response": "def _build_inventory(self, cluster):\n        \"\"\"\n        Builds the inventory for the given cluster and returns its path\n\n        :param cluster: cluster to build inventory for\n        :type cluster: :py:class:`elasticluster.cluster.Cluster`\n        \"\"\"\n        inventory_data = defaultdict(list)\n\n        for node in cluster.get_all_nodes():\n            if node.preferred_ip is None:\n                log.warning(\n                    \"Ignoring node `{0}`: No IP address.\"\n                    .format(node.name))\n                continue\n            if node.kind not in self.groups:\n                # FIXME: should this raise a `ConfigurationError` instead?\n                log.warning(\n                    \"Ignoring node `{0}`:\"\n                    \" Node kind `{1}` not defined in cluster!\"\n                    .format(node.name, node.kind))\n                continue\n\n            extra_vars = ['ansible_user=%s' % node.image_user]\n\n            ip_addr, port = parse_ip_address_and_port(node.preferred_ip)\n            if port != 22:\n                extra_vars.append('ansible_port=%s' % port)\n\n            if node.kind in self.environment:\n                extra_vars.extend('%s=%s' % (k, v) for k, v in\n                                  self.environment[node.kind].items())\n            for group in self.groups[node.kind]:\n                inventory_data[group].append(\n                    (node.name, ip_addr, ' '.join(extra_vars)))\n\n        if not inventory_data:\n            log.info(\"No inventory file was created.\")\n            return None\n\n        # create a temporary file to pass to ansible, since the\n        # api is not stable yet...\n        if self._storage_path_tmp:\n            if not self._storage_path:\n                self._storage_path = tempfile.mkdtemp()\n            elasticluster.log.warning(\n                \"Writing inventory file to tmp dir `%s`\", self._storage_path)\n\n        inventory_path = os.path.join(\n            self._storage_path, (cluster.name + '.inventory'))\n        log.debug(\"Writing Ansible inventory to file `%s` ...\", inventory_path)\n        with open(inventory_path, 'w+') as inventory_file:\n            for section, hosts in inventory_data.items():\n                # Ansible throws an error \"argument of type 'NoneType' is not\n                # iterable\" if a section is empty, so ensure we have something\n                # to write in there\n                if hosts:\n                    inventory_file.write(\"\\n[\" + section + \"]\\n\")\n                    for host in hosts:\n                        hostline = \"{0} ansible_host={1} {2}\\n\".format(*host)\n                        inventory_file.write(hostline)\n        return inventory_path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main(self):\n        assert self.params.func, \"No subcommand defined in `ElastiCluster.main()\"\n        try:\n            return self.params.func()\n        except Exception as err:\n            log.error(\"Error: %s\", err)\n            if self.params.verbose > 2:\n                import traceback\n                traceback.print_exc()\n            print(\"Aborting because of errors: {err}.\".format(err=err))\n            sys.exit(1)", "response": "This is the main entry point of the ElastiCluster CLI. It is called by the command line interface to create the central configuration and then the command line interface is called."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef confirm_or_abort(prompt, exitcode=os.EX_TEMPFAIL, msg=None, **extra_args):\n    if click.confirm(prompt, **extra_args):\n        return True\n    else:\n        # abort\n        if msg:\n            sys.stderr.write(msg)\n            sys.stderr.write('\\n')\n        sys.exit(exitcode)", "response": "Prompt user for confirmation and exit on negative reply."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef environment(**kv):\n    added = []\n    changed = {}\n    for key, value in kv.items():\n        if key not in os.environ:\n            added.append(key)\n        else:\n            changed[key] = os.environ[key]\n        os.environ[key] = value\n\n    yield\n\n    # restore pristine process environment\n    for key in added:\n        del os.environ[key]\n    for key in changed:\n        os.environ[key] = changed[key]", "response": "Context manager to run Python code with modified UNIX process environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexpand a string containing a spacial digraph and a user - specified address and port.", "response": "def expand_ssh_proxy_command(command, user, addr, port=22):\n    \"\"\"\n    Expand spacial digraphs ``%h``, ``%p``, and ``%r``.\n\n    Return a copy of `command` with the following string\n    substitutions applied:\n\n    * ``%h`` is replaced by *addr*\n    * ``%p`` is replaced by *port*\n    * ``%r`` is replaced by *user*\n    * ``%%`` is replaced by ``%``.\n\n    See also: man page ``ssh_config``, section \"TOKENS\".\n    \"\"\"\n    translated = []\n    subst = {\n        'h': list(str(addr)),\n        'p': list(str(port)),\n        'r': list(str(user)),\n        '%': ['%'],\n    }\n    escaped = False\n    for char in command:\n        if char == '%':\n            escaped = True\n            continue\n        if escaped:\n           try:\n               translated.extend(subst[char])\n               escaped = False\n               continue\n           except KeyError:\n               raise ValueError(\n                   \"Unknown digraph `%{0}`\"\n                   \" in proxy command string `{1}`\"\n                   .format(char, command))\n        else:\n            translated.append(char)\n            continue\n    return ''.join(translated)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn number of online processor cores.", "response": "def get_num_processors():\n    \"\"\"\n    Return number of online processor cores.\n    \"\"\"\n    # try different strategies and use first one that succeeeds\n    try:\n        return os.cpu_count()  # Py3 only\n    except AttributeError:\n        pass\n    try:\n        import multiprocessing\n        return multiprocessing.cpu_count()\n    except ImportError:  # no multiprocessing?\n        pass\n    except NotImplementedError:\n        # multiprocessing cannot determine CPU count\n        pass\n    try:\n        from subprocess32 import check_output\n        ncpus = check_output('nproc')\n        return int(ncpus)\n    except CalledProcessError:  # no `/usr/bin/nproc`\n        pass\n    except (ValueError, TypeError):\n        # unexpected output from `nproc`\n        pass\n    except ImportError:  # no subprocess32?\n        pass\n    try:\n        from subprocess import check_output\n        ncpus = check_output('nproc')\n        return int(ncpus)\n    except CalledProcessError:  # no `/usr/bin/nproc`\n        pass\n    except (ValueError, TypeError):\n        # unexpected output from `nproc`\n        pass\n    except ImportError:  # no subprocess.check_call (Py 2.6)\n        pass\n    raise RuntimeError(\"Cannot determine number of processors\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_nested_keys(mapping, k1, *more):\n    if k1 in mapping:\n        if more:\n            return has_nested_keys(mapping[k1], *more)\n        else:\n            return True\n    else:\n        return False", "response": "Return True if mapping has nested keys k1 and k2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_ip_address_and_port(addr, default_port=22):\n    # we assume one and only one of the regexps will match\n    for regexp in _IP_ADDRESS_RE:\n        match = regexp.search(addr)\n        if not match:\n            continue\n        # can raise netaddr.AddrFormatError\n        ip_addr = netaddr.IPAddress(match.group('ip_addr'))\n        try:\n            port = match.group('port')\n        except IndexError:\n            port = None\n        if port is not None:\n            port = int(port[1:])  # skip leading `:`\n        else:\n            port = default_port\n        return ip_addr, port\n    # parse failed\n    raise netaddr.AddrFormatError(\n        \"Could not extract IP address and port from `{1}`\"\n        .format(addr))", "response": "Parse a string addr and return a pair of IP address and port."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sighandler(signum, handler):\n    prev_handler = signal.getsignal(signum)\n    signal.signal(signum, handler)\n    yield\n    signal.signal(signum, prev_handler)", "response": "Context manager to run code with UNIX signal signalum bound to handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a temporary directory and make it current for the code in this context. Delete temporary directory upon exit from the context, unless ``delete=False`` is passed in the arguments. Arguments *suffix*, *prefix* and *dir* are exactly as in :func:`tempfile.mkdtemp()` (but have different defaults).", "response": "def temporary_dir(delete=True, dir=None,\n                  prefix='elasticluster.', suffix='.d'):\n    \"\"\"\n    Make a temporary directory and make it current for the code in this context.\n\n    Delete temporary directory upon exit from the context, unless\n    ``delete=False`` is passed in the arguments.\n\n    Arguments *suffix*, *prefix* and *dir* are exactly as in\n    :func:`tempfile.mkdtemp()` (but have different defaults).\n    \"\"\"\n    cwd = os.getcwd()\n    tmpdir = tempfile.mkdtemp(suffix, prefix, dir)\n    os.chdir(tmpdir)\n    yield\n    os.chdir(cwd)\n    if delete:\n        shutil.rmtree(tmpdir, ignore_errors=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef timeout(delay, handler=None):\n    delay = int(delay)\n    if handler is None:\n        def default_handler(signum, frame):\n            raise RuntimeError(\"{:d} seconds timeout expired\".format(delay))\n        handler = default_handler\n    prev_sigalrm_handler = signal.getsignal(signal.SIGALRM)\n    signal.signal(signal.SIGALRM, handler)\n    signal.alarm(delay)\n    yield\n    signal.alarm(0)\n    signal.signal(signal.SIGALRM, prev_sigalrm_handler)", "response": "Context manager to run code and deliver a SIGALRM signal after delay seconds."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_warning_oneline(message, category, filename, lineno,\n                           file=None, line=None):\n    \"\"\"\n    Format a warning for logging.\n\n    The returned value should be a single-line string, for better\n    logging style (although this is not enforced by the code).\n\n    This methods' arguments have the same meaning of the\n    like-named arguments from `warnings.formatwarning`.\n    \"\"\"\n    # `warnings.formatwarning` produces multi-line output that does\n    # not look good in a log file, so let us replace it with something\n    # simpler...\n    return ('{category}: {message}'\n            .format(message=message, category=category.__name__))", "response": "Format a warning for better\n    logging."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nredirect all warnings to the logging package.", "response": "def redirect_warnings(capture=True, logger='py.warnings'):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        assert _warnings_showwarning is None\n        _warnings_showwarning = warnings.showwarning\n        # `warnings.showwarning` must be a function, a generic\n        # callable object is not accepted ...\n        warnings.showwarning = _WarningsLogger(logger, format_warning_oneline).__call__\n    else:\n        assert _warnings_showwarning is not None\n        warnings.showwarning = _warnings_showwarning\n        _warnings_showwarning = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a new instance on the cloud using the given properties.", "response": "def start_instance(self, key_name, public_key_path, private_key_path,\n                       security_group, flavor, image_id, image_userdata,\n                       username=None, node_name=None):\n        \"\"\"Starts a new instance on the cloud using the given properties.\n        Multiple instances might be started in different threads at the same\n        time. The implementation should handle any problems regarding this\n        itself.\n\n        :param str key_name: name of the ssh key to connect\n        :param str public_key_path: path to ssh public key\n        :param str private_key_path: path to ssh private key\n        :param str security_group: firewall rule definition to apply on the\n                                   instance\n        :param str flavor: machine type to use for the instance\n        :param str image_name: image type (os) to use for the instance\n        :param str image_userdata: command to execute after startup\n        :param str username: username for the given ssh key, default None\n\n        :return: str - instance id of the started instance\n        \"\"\"\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __get_name_or_id(values, known):\n        result = list()\n        for element in [e.strip() for e in values.split(',')]:\n            for item in [i for i in known if i.name == element or i.id == element]:\n                result.append(item)\n        return result", "response": "Return list of values that match attribute. id or. name of any object in list known."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_linux_cmdline(cmdline):\n    result = OrderedDict()\n    for kv in cmdline.strip().split():\n        if '=' in kv:\n            # limit max split to only 1, to correctly handle cases like\n            # `root=UUID=c9d37675-ef02-42f0-8900-a72ec2cd0f56`\n            k, v = kv.split('=', 1)\n            result[k] = v\n\n        else:\n            # represent \"boolean\" flags like `ro` as a key with value `None`\n            result[kv] = None\n    return result", "response": "Parse a Linux boot parameter line into key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _assemble_linux_cmdline(kv):\n    # try to be compatible with Py2.4\n    parts = []\n    for k, v in kv.items():\n        if v is None:\n            parts.append(str(k))\n        else:\n            parts.append('%s=%s' % (k, v))\n    return ' '.join(parts)", "response": "Given a dictionary assemble a Linux boot command line."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nedits a Linux command line with parameter name added replaced with value.", "response": "def _edit_linux_cmdline(cmdline, state, name, value=None):\n    \"\"\"\n    Return a new Linux command line, with parameter `name` added,\n    replaced, or removed.\n    \"\"\"\n    kv = _parse_linux_cmdline(cmdline)\n    if state == 'absent':\n        try:\n            del kv[name]\n        except KeyError:\n            pass\n    elif state == 'present':\n        kv[name] = value\n    return _assemble_linux_cmdline(kv)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef edit(self, config, state, name, value=None):\n        config = str(config)  # make a copy so we can alter it\n        matches = list(self._GRUB_KERNEL_BOOT_ENTRY.finditer(config))\n        # process matches in reverse order, so replacing one match\n        # does not alter the start/end positions of other matches\n        for match in reversed(matches):\n            start = match.end()\n            # Linux command line extends up to the newline\n            if config[start] == '\\n':\n                cmdline = ''\n            else:\n                end = config.find('\\n', start)\n                cmdline = config[start:end]\n            new_cmdline = _assemble_linux_cmdline(cmdline, state, name, value)\n            config = config[:start] + new_cmdline + config[end:]\n        return config", "response": "Edit all occurrences of name in a kernel boot line in the given config text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nedit all occurrences of name in a kernel boot line in the given config text.", "response": "def edit(self, config, state, name, value=None):\n        \"\"\"\n        Change all occurrences of `name` in a kernel boot line in the\n        given `config` text.\n\n        It is expected that `config` is the contents of a file following\n        the syntax of ``/etc/default/grub``:file:.\n\n        .. warning::\n\n          This module only does a very crude textual search and replace:\n          it is assumed that input lines in have the form ``KEY=\"value\"``\n          (quote characters can be double ``\"`` or single ``'``), and that\n          the ``value`` string spans a single line and contains all\n          relevant kernel boot parameters.\n\n          However, the GRUB docs state that :file:``/etc/default/grub``\n          \"is sourced by a shell script, and so must be valid POSIX shell\n          input; normally, it will just be a sequence of ``KEY=value``\n          lines\".  In particular, the following cases are valid POSIX\n          shell input but will be mishandled by this module:\n\n          - It is assumed that all ``KEY=value`` assignments are on a\n            single line.  Multi-line strings will make the module error\n            out.\n\n          - Variable substitutions in the ``value`` part will not be detected.\n\n          - Escaped quotes will be treated as regular quotes, i.e., there\n            is no way to embed a ``\"`` or a ``'`` character in a\n            ``KEY=value`` line with this module.\n\n          - String concatenation is not supported: whereas the POSIX shell\n            interprets a line ``KEY=\"foo\"'bar'`` as assigning the string\n            ``foobar`` to ``KEY``, this module will only operate on the\n            ``\"foo\"`` part.\n        \"\"\"\n        config = str(config)  # make a copy so we can alter it\n        pos = config.find(self._GRUB_CMDLINE_VAR)\n        while pos > -1:\n            # quote char can be `'` or `\"`\n            quote_pos = pos + len(self._GRUB_CMDLINE_VAR)\n            quote_char = config[quote_pos]\n            start = quote_pos + 1\n            # string ends with matching quote\n            end = config.index(quote_char, start)\n            cmdline = config[start:end]\n            new_cmdline = _edit_linux_cmdline(cmdline, state, name, value)\n            config = config[:start] + new_cmdline + config[end:]\n            delta = len(new_cmdline) - len(cmdline)\n            pos = config.find(self._GRUB_CMDLINE_VAR, end + delta)\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a new cluster.", "response": "def execute(self):\n        \"\"\"\n        Starts a new cluster.\n        \"\"\"\n\n        cluster_template = self.params.cluster\n        if self.params.cluster_name:\n            cluster_name = self.params.cluster_name\n        else:\n            cluster_name = self.params.cluster\n\n        creator = make_creator(self.params.config,\n                               storage_path=self.params.storage)\n\n        if cluster_template not in creator.cluster_conf:\n            raise ClusterNotFound(\n                \"No cluster template named `{0}`\"\n                .format(cluster_template))\n\n        # possibly overwrite node mix from config\n        cluster_nodes_conf = creator.cluster_conf[cluster_template]['nodes']\n        for kind, num in self.params.nodes_override.items():\n            if kind not in cluster_nodes_conf:\n                raise ConfigurationError(\n                    \"No node group `{kind}` defined\"\n                    \" in cluster template `{template}`\"\n                    .format(kind=kind, template=cluster_template))\n            cluster_nodes_conf[kind]['num'] = num\n\n        # First, check if the cluster is already created.\n        try:\n            cluster = creator.load_cluster(cluster_name)\n        except ClusterNotFound:\n            try:\n                cluster = creator.create_cluster(\n                    cluster_template, cluster_name)\n            except ConfigurationError as err:\n                log.error(\"Starting cluster %s: %s\", cluster_template, err)\n                return\n\n        try:\n            print(\"Starting cluster `{0}` with:\".format(cluster.name))\n            for cls in cluster.nodes:\n                print(\"* {0:d} {1} nodes.\".format(len(cluster.nodes[cls]), cls))\n            print(\"(This may take a while...)\")\n            min_nodes = dict((kind, cluster_nodes_conf[kind]['min_num'])\n                             for kind in cluster_nodes_conf)\n            cluster.start(min_nodes, self.params.max_concurrent_requests)\n            if self.params.no_setup:\n                print(\"NOT configuring the cluster as requested.\")\n            else:\n                print(\"Configuring the cluster ...\")\n                print(\"(this too may take a while)\")\n                ok = cluster.setup()\n                if ok:\n                    print(\n                        \"\\nYour cluster `{0}` is ready!\"\n                        .format(cluster.name))\n                else:\n                    print(\n                        \"\\nWARNING: YOUR CLUSTER `{0}` IS NOT READY YET!\"\n                        .format(cluster.name))\n            print(cluster_summary(cluster))\n        except (KeyError, ImageError, SecurityGroupError, ClusterError) as err:\n            log.error(\"Could not start cluster `%s`: %s\", cluster.name, err)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef execute(self):\n        cluster_name = self.params.cluster\n        creator = make_creator(self.params.config,\n                               storage_path=self.params.storage)\n        try:\n            cluster = creator.load_cluster(cluster_name)\n        except (ClusterNotFound, ConfigurationError) as err:\n            log.error(\"Cannot stop cluster `%s`: %s\", cluster_name, err)\n            return os.EX_NOINPUT\n\n        if not self.params.yes:\n            confirm_or_abort(\n                \"Do you want really want to stop cluster `{cluster_name}`?\"\n                .format(cluster_name=cluster_name),\n                msg=\"Aborting upon user request.\")\n        print(\"Destroying cluster `%s` ...\" % cluster_name)\n        cluster.stop(force=self.params.force, wait=self.params.wait)", "response": "Stops the cluster if it s running."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npauses the cluster if it is running.", "response": "def execute(self):\n        \"\"\"Pause the cluster if it is running.\"\"\"\n        cluster_name = self.params.cluster\n        creator = make_creator(self.params.config,\n                               storage_path=self.params.storage)\n        try:\n            cluster = creator.load_cluster(cluster_name)\n        except (ClusterNotFound, ConfigurationError) as e:\n            log.error(\"Cannot load cluster `%s`: %s\", cluster_name, e)\n            return os.EX_NOINPUT\n        if not self.params.yes:\n            confirm_or_abort(\n                \"Do you want really want to pause cluster `{cluster_name}`?\"\n                .format(cluster_name=cluster_name),\n                msg=\"Aborting upon user request.\")\n        print(\"Pausing cluster `%s` ...\" % cluster_name)\n        cluster.pause()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresumes the cluster if it is paused.", "response": "def execute(self):\n        \"\"\"Resume the cluster if it is paused.\"\"\"\n        cluster_name = self.params.cluster\n        creator = make_creator(self.params.config,\n                               storage_path=self.params.storage)\n        try:\n            cluster = creator.load_cluster(cluster_name)\n        except (ClusterNotFound, ConfigurationError) as e:\n            log.error(\"Cannot load cluster `%s`: %s\", cluster_name, e)\n            return os.EX_NOINPUT\n        print(\"Resuming cluster `%s` ...\" % cluster_name)\n        cluster.resume()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef execute(self):\n        creator = make_creator(self.params.config,\n                               storage_path=self.params.storage)\n        cluster_name = self.params.cluster\n        try:\n            cluster = creator.load_cluster(cluster_name)\n            if self.params.update:\n                cluster.update()\n        except (ClusterNotFound, ConfigurationError) as ex:\n            log.error(\"Listing nodes from cluster %s: %s\", cluster_name, ex)\n            return\n\n        if self.params.pretty_json:\n            print(json.dumps(cluster, default=dict, indent=4))\n        elif self.params.json:\n            print(json.dumps(cluster, default=dict))\n        else:\n            print(cluster_summary(cluster))\n            for cls in cluster.nodes:\n                print(\"%s nodes:\" % cls)\n                print(\"\")\n                for node in cluster.nodes[cls]:\n                    txt = [\"    \" + i for i in node.pprint().splitlines()]\n                    print('  - ' + \"\\n\".join(txt)[4:])\n                    print(\"\")", "response": "Lists all nodes within the specified cluster with certain\n        information like id and ip."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a shallow copy of self cast to a dictionary optionally omitting some key value pairs.", "response": "def to_dict(self, omit=()):\n        \"\"\"\n        Return a (shallow) copy of self cast to a dictionary,\n        optionally omitting some key/value pairs.\n        \"\"\"\n        result = self.__dict__.copy()\n        for key in omit:\n            if key in result:\n                del result[key]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keys(self):\n        keys = Struct.keys(self)\n        for key in (\n                '_cloud_provider',\n                '_naming_policy',\n                '_setup_provider',\n                'known_hosts_file',\n                'repository',\n        ):\n            if key in keys:\n                keys.remove(key)\n        return keys", "response": "Only expose some of the attributes when using as a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the current configuration of the cluster.", "response": "def update_config(self, cluster_config):\n        \"\"\"Update current configuration.\n\n        This method is usually called after loading a `Cluster`\n        instance from a persistent storage. Note that not all fields\n        are actually updated, but only those that can be safely\n        updated.\n        \"\"\"\n        for key, attr in [\n                ('ssh_to', 'ssh_to'),\n        ]:\n            oldvalue = getattr(self, attr)\n            try:\n                newvalue = cluster_config[key]\n            except KeyError:\n                continue\n            if key in cluster_config and newvalue != oldvalue:\n                setattr(self, attr, newvalue)\n                log.debug(\n                    \"Configuration attribute `%s` updated: %s -> %s\",\n                    key, oldvalue, newvalue)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_node(self, kind, image_id, image_user, flavor,\n                 security_group, image_userdata='', name=None, **extra):\n        \"\"\"\n        Adds a new node to the cluster. This factory method provides an\n        easy way to add a new node to the cluster by specifying all relevant\n        parameters. The node does not get started nor setup automatically,\n        this has to be done manually afterwards.\n\n        :param str kind: kind of node to start. this refers to the\n                         groups defined in the ansible setup provider\n                         :py:class:`elasticluster.providers.AnsibleSetupProvider`\n                         Please note that this can only contain\n                         alphanumeric characters and hyphens (and must\n                         not end with a digit), as it is used to build\n                         a valid hostname\n\n        :param str image_id: image id to use for the cloud instance (e.g.\n                             ami on amazon)\n\n        :param str image_user: user to login on given image\n\n        :param str flavor: machine type to use for cloud instance\n\n        :param str security_group: security group that defines firewall rules\n                                   to the instance\n\n        :param str image_userdata: commands to execute after instance starts\n\n        :param str name: name of this node, automatically generated if None\n\n        :raises: ValueError: `kind` argument is an invalid string.\n\n        :return: created :py:class:`Node`\n        \"\"\"\n        if not self._NODE_KIND_RE.match(kind):\n            raise ValueError(\n                \"Invalid name `{kind}`. The `kind` argument may only contain\"\n                \" alphanumeric characters, and must not end with a digit.\"\n                    .format(kind=kind))\n\n        if kind not in self.nodes:\n            self.nodes[kind] = []\n\n        # To ease json dump/load, use `extra` dictionary to\n        # instantiate Node class\n        extra.update(\n            cloud_provider=self._cloud_provider,\n            cluster_name=self.name,\n            flavor=flavor,\n            image_id=image_id,\n            image_user=image_user,\n            image_userdata=image_userdata,\n            kind=kind,\n            security_group=security_group,\n        )\n        for attr in (\n                'flavor',\n                'image_id',\n                'image_user',\n                'image_userdata',\n                'security_group',\n                'user_key_name',\n                'user_key_private',\n                'user_key_public',\n        ):\n            if attr not in extra:\n                extra[attr] = getattr(self, attr)\n\n        if not name:\n            # `extra` contains key `kind` already\n            name = self._naming_policy.new(**extra)\n        else:\n            self._naming_policy.use(kind, name)\n        node = Node(name=name, **extra)\n\n        self.nodes[kind].append(node)\n        return node", "response": "This method creates a new node in the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_nodes(self, kind, num, image_id, image_user, flavor,\n                  security_group, image_userdata='', **extra):\n        \"\"\"Helper method to add multiple nodes of the same kind to a cluster.\n\n        :param str kind: kind of node to start. this refers to the groups\n                         defined in the ansible setup provider\n                         :py:class:`elasticluster.providers.AnsibleSetupProvider`\n\n        :param int num: number of nodes to add of this kind\n\n        :param str image_id: image id to use for the cloud instance (e.g.\n                             ami on amazon)\n\n        :param str image_user: user to login on given image\n\n        :param str flavor: machine type to use for cloud instance\n\n        :param str security_group: security group that defines firewall rules\n                                   to the instance\n\n        :param str image_userdata: commands to execute after instance starts\n        \"\"\"\n        for i in range(num):\n            self.add_node(kind, image_id, image_user, flavor,\n                          security_group, image_userdata=image_userdata, **extra)", "response": "helper method to add multiple nodes of the same kind to a cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a node from the cluster.", "response": "def remove_node(self, node, stop=False):\n        \"\"\"Removes a node from the cluster.\n\n        By default, it doesn't also stop the node, just remove from\n        the known hosts of this cluster.\n\n        :param node: node to remove\n        :type node: :py:class:`Node`\n\n        :param stop: Stop the node\n        :type stop: bool\n\n        \"\"\"\n        if node.kind not in self.nodes:\n            raise NodeNotFound(\"Unable to remove node %s: invalid node type `%s`.\",\n                               node.name, node.kind)\n        else:\n            try:\n                index = self.nodes[node.kind].index(node)\n                if self.nodes[node.kind][index]:\n                    del self.nodes[node.kind][index]\n                if stop:\n                    node.stop()\n                self._naming_policy.free(node.kind, node.name)\n                self.repository.save_or_update(self)\n                remaining_nodes = self.get_all_nodes()\n                self._gather_node_ip_addresses(\n                    remaining_nodes, self.start_timeout, self.ssh_probe_timeout,\n                    remake=True)\n            except ValueError:\n                raise NodeNotFound(\"Node %s not found in cluster\" % node.name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts the cluster nodes.", "response": "def start(self, min_nodes=None, max_concurrent_requests=0):\n        \"\"\"\n        Starts up all the instances in the cloud.\n\n        To speed things up, all\n        instances are started in a seperate thread. To make sure\n        ElastiCluster is not stopped during creation of an instance, it will\n        overwrite the sigint handler. As soon as the last started instance\n        is returned and saved to the repository, sigint is executed as usual.\n\n        A VM instance is considered 'up and running' as soon as an SSH\n        connection can be established. If the startup timeout is reached before\n        all instances are started, ElastiCluster stops the cluster and\n        terminates all VM instances.\n\n        This method is blocking and might take some time depending on the\n        amount of instances to start.\n\n        :param min_nodes: minimum number of nodes to start in case the quota\n                          is reached before all instances are up\n        :type min_nodes: dict [node_kind] = number\n        :param int max_concurrent_requests:\n          Issue at most this number of requests to start\n          VMs; if 1 or less, start nodes one at a time (sequentially).\n          The special value ``0`` means run 4 threads for each available\n          processor.\n        \"\"\"\n\n        nodes = self.get_all_nodes()\n\n        log.info(\n            \"Starting cluster nodes (timeout: %d seconds) ...\",\n            self.start_timeout)\n        if max_concurrent_requests == 0:\n            try:\n                max_concurrent_requests = 4 * get_num_processors()\n            except RuntimeError:\n                log.warning(\n                    \"Cannot determine number of processors!\"\n                    \" will start nodes sequentially...\")\n                max_concurrent_requests = 1\n        if max_concurrent_requests > 1:\n            nodes = self._start_nodes_parallel(nodes, max_concurrent_requests)\n        else:\n            nodes = self._start_nodes_sequentially(nodes)\n\n        # checkpoint cluster state\n        self.repository.save_or_update(self)\n\n        not_started_nodes = self._check_starting_nodes(nodes, self.start_timeout)\n\n        # now that all nodes are up, checkpoint cluster state again\n        self.repository.save_or_update(self)\n\n        # Try to connect to each node to gather IP addresses and SSH host keys\n        started_nodes = nodes - not_started_nodes\n        if not started_nodes:\n            raise ClusterSizeError(\"No nodes could be started!\")\n        log.info(\n            \"Checking SSH connection to nodes (timeout: %d seconds) ...\",\n            self.start_timeout)\n        self._gather_node_ip_addresses(\n            started_nodes, self.start_timeout, self.ssh_probe_timeout)\n        # It's possible that the node.connect() call updated the\n        # `preferred_ip` attribute, so, let's save the cluster again.\n        self.repository.save_or_update(self)\n\n        # A lot of things could go wrong when starting the cluster.\n        # Check that the minimum number of nodes within each groups is\n        # reachable. Raise `ClusterSizeError()` if not.\n        self._check_cluster_size(self._compute_min_nodes(min_nodes))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting until all given nodes are alive and return a list of not - yet - started nodes.", "response": "def _check_starting_nodes(self, nodes, lapse):\n        \"\"\"\n        Wait until all given nodes are alive, for max `lapse` seconds.\n        \"\"\"\n        with timeout(lapse, raise_timeout_error):\n            try:\n                while nodes:\n                    nodes = set(node for node in nodes\n                                if not node.is_alive())\n                    if nodes:\n                        log.debug(\"Waiting for %d more nodes to come up ...\", len(nodes))\n                        time.sleep(self.polling_interval)\n            except TimeoutError:\n                log.error(\"Some nodes did not start correctly\"\n                          \" within the given %d-seconds timeout: %s\",\n                          lapse, ', '.join(node.name for node in nodes))\n        # return list of not-yet-started nodes,\n        # so we can exclude them from coming rounds\n        return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to each node and return set of nodes that could not be reached with lapse seconds.", "response": "def _gather_node_ip_addresses(self, nodes, lapse, ssh_timeout, remake=False):\n        \"\"\"\n        Connect via SSH to each node.\n\n        Return set of nodes that could not be reached with `lapse` seconds.\n        \"\"\"\n        # for convenience, we might set this to ``None`` if the file cannot\n        # be opened -- but we do not want to forget the cluster-wide\n        # setting in case the error is transient\n        known_hosts_path = self.known_hosts_file\n\n        # If run with remake=True, deletes known_hosts_file so that it will\n        # be recreated. Prevents \"Invalid host key\" errors\n        if remake and os.path.isfile(known_hosts_path):\n            os.remove(known_hosts_path)\n\n        # Create the file if it's not present, otherwise the\n        # following lines will raise an error\n        try:\n            fd = open(known_hosts_path, 'a')\n            fd.close()\n        except IOError as err:\n            log.warning(\"Error opening SSH 'known hosts' file `%s`: %s\",\n                        known_hosts_path, err)\n            known_hosts_path = None\n\n        keys = paramiko.hostkeys.HostKeys(known_hosts_path)\n\n        with timeout(lapse, raise_timeout_error):\n            try:\n                while nodes:\n                    for node in copy(nodes):\n                        ssh = node.connect(\n                            keyfile=known_hosts_path,\n                            timeout=ssh_timeout)\n                        if ssh:\n                            log.info(\"Connection to node `%s` successful,\"\n                                     \" using IP address %s to connect.\",\n                                     node.name, node.connection_ip())\n                            # Add host keys to the keys object.\n                            for host, key in ssh.get_host_keys().items():\n                                for keytype, keydata in key.items():\n                                    keys.add(host, keytype, keydata)\n                            self._save_keys_to_known_hosts_file(keys)\n                            nodes.remove(node)\n                    if nodes:\n                        time.sleep(self.polling_interval)\n\n            except TimeoutError:\n                log.error(\n                    \"Some nodes of the cluster were unreachable\"\n                    \" within the given %d-seconds timeout: %s\",\n                    lapse, ', '.join(node.name for node in nodes))\n\n        # return list of nodes\n        return nodes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_cluster_size(self, min_nodes):\n        # finding all node groups with an unsatisfied amount of nodes\n        unsatisfied = 0\n        for kind, required in min_nodes.items():\n            available = len(self.nodes[kind])\n            if available < required:\n                log.error(\n                    \"Not enough nodes of kind `%s`:\"\n                    \" %d required, but only %d available.\",\n                )\n                unsatisfied += 1\n\n        if unsatisfied:\n            raise ClusterSizeError()", "response": "Checks the size of the cluster to fit the minimum number of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the node corresponding with name nodename", "response": "def get_node_by_name(self, nodename):\n        \"\"\"Return the node corresponding with name `nodename`\n\n        :params nodename: Name of the node\n        :type nodename: str\n        \"\"\"\n        for kind, nodes in self.nodes.items():\n            for node in nodes:\n                if node.name == nodename:\n                    return node\n        else:\n            raise NodeNotFound(\n                \"Node `{0}` not found in cluster `{1}`\"\n                .format(nodename, self.name))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping all VMs in this cluster and delete its repository.", "response": "def stop(self, force=False, wait=False):\n        \"\"\"\n        Terminate all VMs in this cluster and delete its repository.\n\n        :param bool force:\n          remove cluster from storage even if not all nodes could be stopped.\n        \"\"\"\n        log.debug(\"Stopping cluster `%s` ...\", self.name)\n\n        failed = self._stop_all_nodes(wait)\n\n        if failed:\n            if force:\n                self._delete_saved_data()\n                log.warning(\n                    \"Not all cluster nodes have been terminated.\"\n                    \" However, as requested, data about the cluster\"\n                    \" has been removed from local storage.\")\n            else:\n                self.repository.save_or_update(self)\n                log.warning(\n                    \"Not all cluster nodes have been terminated.\"\n                    \" Fix errors above and re-run `elasticluster stop %s`\",\n                    self.name)\n        else:\n            self._delete_saved_data()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npause all VMs in this cluster and store data so that they can be restarted later.", "response": "def pause(self):\n        \"\"\"Pause all VMs in this cluster and store data so that they\n        can be restarted later.\n        \"\"\"\n        log.info(\"Pausing cluster `%s` ...\", self.name)\n        failed = self._pause_all_nodes()\n        if os.path.exists(self.known_hosts_file):\n            os.remove(self.known_hosts_file)\n        self.repository.save_or_update(self)\n        if failed:\n            log.warning(\n                    \"Not all cluster nodes have been successfully \"\n                    \"stopped.  Some nodes may still be running - \"\n                    \"check error messages above and consider \"\n                    \"re-running `elasticluster pause %s` if \"\n                    \"necessary.\", self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resume(self):\n        log.info(\"Resuming cluster `%s` ...\", self.name)\n        failed = self._resume_all_nodes()\n        for node in self.get_all_nodes():\n            node.update_ips()\n        self._gather_node_ip_addresses(\n            self.get_all_nodes(), self.start_timeout, self.ssh_probe_timeout)\n        self.repository.save_or_update(self)\n        if failed:\n            log.warning(\n                    \"Not all cluster nodes have been successfully \"\n                    \"restarted.  Check error messages above and consider \"\n                    \"re-running `elasticluster resume %s` if \"\n                    \"necessary.\", self.name)\n            return\n        if not self._setup_provider.resume_cluster(self):\n            log.warning(\"Elasticluster was not able to guarantee that the \"\n                        \"cluster restarted correctly - check the errors \"\n                        \"above and check your config.\")", "response": "Resume all paused VMs in this cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping all nodes in the cluster.", "response": "def _stop_all_nodes(self, wait=False):\n        \"\"\"\n        Terminate all cluster nodes. Return number of failures.\n        \"\"\"\n        failed = 0\n        for node in self.get_all_nodes():\n            if not node.instance_id:\n                log.warning(\n                    \"Node `%s` has no instance ID.\"\n                    \" Assuming it did not start correctly,\"\n                    \" so removing it anyway from the cluster.\", node.name)\n                self.nodes[node.kind].remove(node)\n                continue\n            # try and stop node\n            try:\n                # wait and pause for and recheck.\n                node.stop(wait)\n\n                self.nodes[node.kind].remove(node)\n                log.debug(\n                    \"Removed node `%s` from cluster `%s`\", node.name, self.name)\n            except InstanceNotFoundError as err:\n                log.info(\n                    \"Node `%s` (instance ID `%s`) was not found;\"\n                    \" assuming it has already been terminated.\",\n                    node.name, node.instance_id)\n            except Exception as err:\n                failed += 1\n                log.error(\n                    \"Could not stop node `%s` (instance ID `%s`): %s %s\",\n                    node.name, node.instance_id, err, err.__class__)\n        return failed"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pause_all_nodes(self, max_thread_pool_size=0):\n        failed = 0\n\n        def _pause_specific_node(node):\n            if not node.instance_id:\n                log.warning(\"Node `%s` has no instance id.\"\n                            \" It is either already stopped, or\"\n                            \" never created properly.  Not attempting\"\n                            \" to stop it again.\", node.name)\n                return None\n            try:\n                return node.pause()\n            except Exception as err:\n                log.error(\n                    \"Could not stop node `%s` (instance ID `%s`): %s %s\",\n                    node.name, node.instance_id, err, err.__class__)\n                node.update_ips()\n                return None\n\n        nodes = self.get_all_nodes()\n        thread_pool = self._make_thread_pool(max_thread_pool_size)\n        for node, state in zip(nodes, thread_pool.map(_pause_specific_node, nodes)):\n            if state is None:\n                failed += 1\n            else:\n                self.paused_nodes[node.name] = state\n\n        return failed", "response": "Pause all cluster nodes - ensure that we store data so that the future the nodes can be restarted."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_ssh_to_node(self, ssh_to=None):\n        if ssh_to is None:\n            ssh_to = self.ssh_to\n\n        # first try to interpret `ssh_to` as a node name\n        if ssh_to:\n            try:\n                return self.get_node_by_name(ssh_to)\n            except NodeNotFound:\n                pass\n\n        # next, ensure `ssh_to` is a class name\n        if ssh_to:\n            try:\n                parts = self._naming_policy.parse(ssh_to)\n                log.warning(\n                    \"Node `%s` not found.\"\n                    \" Trying to find other node in class `%s` ...\",\n                    ssh_to, parts['kind'])\n                ssh_to = parts['kind']\n            except ValueError:\n                # it's already a class name\n                pass\n\n        # try getting first node of kind `ssh_to`\n        if ssh_to:\n            try:\n                nodes = self.nodes[ssh_to]\n            except KeyError:\n                raise ConfigurationError(\n                    \"Invalid configuration item `ssh_to={ssh_to}` in cluster `{name}`:\"\n                    \" node class `{ssh_to}` does not exist in this cluster.\"\n                    .format(ssh_to=ssh_to, name=self.name))\n            try:\n                return nodes[0]\n            except IndexError:\n                log.warning(\n                    \"Chosen `ssh_to` class `%s` is empty: unable to \"\n                    \"get the choosen frontend node from that class.\",\n                    ssh_to)\n\n        # If we reach this point, `ssh_to` was not set or the\n        # preferred class was empty. Try \"natural\" `ssh_to` values.\n        for kind in ['ssh', 'login', 'frontend', 'master']:\n            try:\n                nodes = self.nodes[kind]\n                return nodes[0]\n            except (KeyError, IndexError):\n                pass\n\n        # ... if all else fails, return first node\n        for kind in sorted(self.nodes.keys()):\n            if self.nodes[kind]:\n                return self.nodes[kind][0]\n\n        # Uh-oh, no nodes in this cluster!\n        raise NodeNotFound(\"Unable to find a valid frontend:\"\n                           \" cluster has no nodes!\")", "response": "Return the node that is connected to the given ssh_to node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure the cluster nodes.", "response": "def setup(self, extra_args=tuple()):\n        \"\"\"\n        Configure the cluster nodes.\n\n        Actual action is delegated to the\n        :py:class:`elasticluster.providers.AbstractSetupProvider` that\n        was provided at construction time.\n\n        :param list extra_args:\n          List of additional command-line arguments\n          that are appended to each invocation of the setup program.\n\n        :return: bool - True on success, False otherwise\n        \"\"\"\n        try:\n            # setup the cluster using the setup provider\n            ret = self._setup_provider.setup_cluster(self, extra_args)\n        except Exception as err:\n            log.error(\n                \"The cluster hosts are up and running,\"\n                \" but %s failed to set the cluster up: %s\",\n                self._setup_provider.HUMAN_READABLE_NAME, err)\n            ret = False\n\n        if not ret:\n            log.warning(\n                \"Cluster `%s` not yet configured. Please, re-run \"\n                \"`elasticluster setup %s` and/or check your configuration\",\n                self.name, self.name)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self):\n        for node in self.get_all_nodes():\n            try:\n                node.update_ips()\n\n                # If we previously did not have a preferred_ip or the\n                # preferred_ip is not in the current list, then try to connect\n                # to one of the node ips and update the preferred_ip.\n                if node.ips and \\\n                        not (node.preferred_ip and \\\n                                         node.preferred_ip in node.ips):\n                    node.connect()\n            except InstanceError as ex:\n                log.warning(\"Ignoring error updating information on node %s: %s\",\n                            node, ex)\n        self.repository.save_or_update(self)", "response": "Update connection information of all nodes in this cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a node name into a dictionary of parts forming name.", "response": "def parse(name):\n        \"\"\"\n        Return dict of parts forming `name`.  Raise `ValueError` if string\n        `name` cannot be correctly parsed.\n\n        The default implementation uses\n        `NodeNamingPolicy._NODE_NAME_RE` to parse the name back into\n        constituent parts.\n\n        This is ideally the inverse of :meth:`format` -- it should be\n        able to parse a node name string into the parameter values\n        that were used to form it.\n        \"\"\"\n        match = NodeNamingPolicy._NODE_NAME_RE.match(name)\n        if match:\n            return match.groupdict()\n        else:\n            raise ValueError(\n                \"Cannot parse node name `{name}`\"\n                    .format(name=name))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the node on the cloud using the given instance properties.", "response": "def start(self):\n        \"\"\"\n        Start the node on the cloud using the given instance properties.\n\n        This method is non-blocking: as soon as the node id is returned from\n        the cloud provider, it will return. The `is_alive`:meth: and\n        `update_ips`:meth: methods should be used to further gather details\n        about the state of the node.\n        \"\"\"\n        log.info(\"Starting node `%s` from image `%s` with flavor %s ...\",\n                 self.name, self.image_id, self.flavor)\n        self.instance_id = self._cloud_provider.start_instance(\n            self.user_key_name, self.user_key_public, self.user_key_private,\n            self.security_group,\n            self.flavor, self.image_id, self.image_userdata,\n            username=self.image_user,\n            node_name=(\"%s-%s\" % (self.cluster_name, self.name)),\n            **self.extra)\n        log.debug(\"Node `%s` has instance ID `%s`\", self.name, self.instance_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop(self, wait=False):\n        if self.instance_id is not None:\n            log.info(\"Shutting down node `%s` (VM instance `%s`) ...\",\n                     self.name, self.instance_id)\n\n            self._cloud_provider.stop_instance(self.instance_id)\n            if wait:\n                while self.is_alive():\n                    time.sleep(1)\n            # When an instance is terminated, the EC2 cloud provider will\n            # basically return it as \"running\" state. Setting the\n            # `instance_id` attribute to None will force `is_alive()`\n            # method not to check with the cloud provider, and forever\n            # forgetting about the instance id.\n            self.instance_id = None", "response": "Stop the VM instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pause(self):\n        if self.instance_id is None:\n            raise ValueError(\"Trying to stop unstarted node.\")\n        resp = self._cloud_provider.pause_instance(self.instance_id)\n        self.preferred_ip = None\n        return resp", "response": "Pause the VM instance and return the info needed to restart it."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the node via SSH.", "response": "def connect(self, keyfile=None, timeout=5):\n        \"\"\"\n        Connect to the node via SSH.\n\n        :param keyfile: Path to the SSH host key.\n        :param timeout: Maximum time to wait (in seconds) for the TCP\n            connection to be established.\n\n        :return: :py:class:`paramiko.SSHClient` - ssh connection or None on\n                 failure\n        \"\"\"\n        ssh = paramiko.SSHClient()\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n        if keyfile and os.path.exists(keyfile):\n            ssh.load_host_keys(keyfile)\n\n        # Try connecting using the `preferred_ip`, if\n        # present. Otherwise, try all of them and set `preferred_ip`\n        # using the first that is working.\n        ips = self.ips[:]\n        # This is done in order to \"sort\" the IPs and put the preferred_ip first.\n        if self.preferred_ip:\n            if self.preferred_ip in ips:\n                ips.remove(self.preferred_ip)\n            else:\n                # Preferred is changed?\n                log.debug(\n                    \"IP address %s does not seem to belong to %s anymore.\"\n                    \" Ignoring it.\", self.preferred_ip, self.name)\n                self.preferred_ip = ips[0]\n\n        for ip in itertools.chain([self.preferred_ip], ips):\n            if not ip:\n                continue\n            log.debug(\n                \"Trying to connect to host %s using IP address %s ...\",\n                self.name, ip)\n            try:\n                addr, port = parse_ip_address_and_port(ip, SSH_PORT)\n                extra = {\n                    'allow_agent':   True,\n                    'key_filename':  self.user_key_private,\n                    'look_for_keys': False,\n                    'timeout':       timeout,\n                    'username':      self.image_user,\n                }\n                if self.ssh_proxy_command:\n                    proxy_command = expand_ssh_proxy_command(\n                        self.ssh_proxy_command,\n                        self.image_user, addr, port)\n                    from paramiko.proxy import ProxyCommand\n                    extra['sock'] = ProxyCommand(proxy_command)\n                    log.debug(\"Using proxy command `%s`.\", proxy_command)\n                ssh.connect(str(addr), port=port, **extra)\n                log.debug(\n                    \"Connection to %s succeeded on port %d,\"\n                    \" will use this IP address for future connections.\",\n                    ip, port)\n                if ip != self.preferred_ip:\n                    self.preferred_ip = ip\n                # Connection successful.\n                return ssh\n            except socket.error as ex:\n                log.debug(\n                    \"Host %s (%s) not reachable within %d seconds: %s -- %r\",\n                    self.name, ip, timeout, ex, type(ex))\n            except paramiko.BadHostKeyException as ex:\n                log.error(\n                    \"Invalid SSH host key for %s (%s): %s.\",\n                    self.name, ip, ex)\n            except paramiko.SSHException as ex:\n                log.debug(\n                    \"Ignoring error connecting to %s: %s -- %r\",\n                    self.name, ex, type(ex))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the public and private ip of the instance by using the cloud provider.", "response": "def update_ips(self):\n        \"\"\"Retrieves the public and private ip of the instance by using the\n        cloud provider. In some cases the public ip assignment takes some\n        time, but this method is non blocking. To check for a public ip,\n        consider calling this method multiple times during a certain timeout.\n        \"\"\"\n        self.ips = self._cloud_provider.get_ips(self.instance_id)\n        if self.preferred_ip not in self.ips:\n            self.preferred_ip = None\n        return self.ips[:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a shallow copy of self cast to a dictionary optionally omitting some key value pairs.", "response": "def to_dict(self, omit=()):\n        \"\"\"\n        Return a (shallow) copy of self cast to a dictionary,\n        optionally omitting some key/value pairs.\n        \"\"\"\n        result = dict(self)\n        for key in omit:\n            if key in result:\n                del result[key]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pprint(self):\n        ips = ', '.join(ip for ip in self.ips if ip)\n        return \"\"\"%s\nconnection IP: %s\nIPs:    %s\ninstance id:   %s\ninstance flavor: %s\"\"\" % (self.name, self.preferred_ip, ips,\n                          self.instance_id, self.flavor)", "response": "Pretty print information about the node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of variables that can be used to set the local state of the cluster setup process.", "response": "def to_vars_dict(self):\n        \"\"\"\n        Return local state which is relevant for the cluster setup process.\n        \"\"\"\n        return {\n            'aws_access_key_id':      self._access_key,\n            'aws_secret_access_key':  self._secret_key,\n            'aws_region':             self._region_name,\n            'aws_vpc_name':           (self._vpc or ''),\n            'aws_vpc_id':             (self._vpc_id or ''),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to the EC2 endpoint and return a connection object.", "response": "def _connect(self):\n        \"\"\"\n        Connect to the EC2 cloud provider.\n\n        :return: :py:class:`boto.ec2.connection.EC2Connection`\n        :raises: Generic exception on error\n        \"\"\"\n        # check for existing connection\n        if self._ec2_connection:\n            return self._ec2_connection\n\n        try:\n            log.debug(\"Connecting to EC2 endpoint %s\", self._ec2host)\n\n            # connect to webservice\n            ec2_connection = boto.ec2.connect_to_region(\n                self._region_name,\n                aws_access_key_id=self._access_key,\n                aws_secret_access_key=self._secret_key,\n                is_secure=self._secure,\n                host=self._ec2host,\n                port=self._ec2port,\n                path=self._ec2path,\n            )\n            # With the loose setting `BOTO_USE_ENDPOINT_HEURISTICS`\n            # which is necessary to work around issue #592, Boto will\n            # now accept *any* string as an AWS region name;\n            # furthermore, it *always* returns a connection object --\n            # so the only way to check that we are not going to run\n            # into trouble is to check that there *is* a valid host\n            # name on the other end of the connection.\n            if ec2_connection.host:\n                log.debug(\"EC2 connection has been successful.\")\n            else:\n                raise CloudProviderError(\n                    \"Cannot establish connection to EC2 region {0}\"\n                    .format(self._region_name))\n\n            if not self._vpc:\n                vpc_connection = None\n                self._vpc_id = None\n            else:\n                vpc_connection, self._vpc_id = self._find_vpc_by_name(self._vpc)\n\n        except Exception as err:\n            log.error(\"Error connecting to EC2: %s\", err)\n            raise\n\n        self._ec2_connection, self._vpc_connection = (\n            ec2_connection, vpc_connection)\n        return self._ec2_connection"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstarts a new instance on the cloud using the given properties.", "response": "def start_instance(self, key_name, public_key_path, private_key_path,\n                       security_group, flavor, image_id, image_userdata,\n                       username=None, node_name=None, network_ids=None,\n                       price=None, timeout=None,\n                       boot_disk_device=None,\n                       boot_disk_size=None,\n                       boot_disk_type=None,\n                       boot_disk_iops=None,\n                       placement_group=None,\n                       **kwargs):\n        \"\"\"Starts a new instance on the cloud using the given properties.\n        The following tasks are done to start an instance:\n\n        * establish a connection to the cloud web service\n        * check ssh keypair and upload it if it does not yet exist. This is\n          a locked process, since this function might be called in multiple\n          threads and we only want the key to be stored once.\n        * check if the security group exists\n        * run the instance with the given properties\n\n        :param str key_name: name of the ssh key to connect\n        :param str public_key_path: path to ssh public key\n        :param str private_key_path: path to ssh private key\n        :param str security_group: firewall rule definition to apply on the\n                                   instance\n        :param str flavor: machine type to use for the instance\n        :param str image_id: image type (os) to use for the instance\n        :param str image_userdata: command to execute after startup\n        :param str username: username for the given ssh key, default None\n        :param float price: Spot instance price (if 0, do not use spot instances).\n        :param int price: Timeout (in seconds) waiting for spot instances;\n                          only used if price > 0.\n        :param str boot_disk_device: Root volume device path if not /dev/sda1\n        :param str boot_disk_size: Target size, in GiB, for the root volume\n        :param str boot_disk_type: Type of root volume (standard, gp2, io1)\n        :param str boot_disk_iops: Provisioned IOPS for the root volume\n        :param str placement_group: Enable low-latency networking between\n                                    compute nodes.\n\n        :return: str - instance id of the started instance\n        \"\"\"\n        connection = self._connect()\n\n        log.debug(\"Checking keypair `%s`.\", key_name)\n        # the `_check_keypair` method has to be called within a lock,\n        # since it will upload the key if it does not exist and if this\n        # happens for every node at the same time ec2 will throw an error\n        # message (see issue #79)\n        with BotoCloudProvider.__node_start_lock:\n            self._check_keypair(key_name, public_key_path, private_key_path)\n\n        log.debug(\"Checking security group `%s`.\", security_group)\n        security_group_id = self._check_security_group(security_group)\n        # image_id = self._find_image_id(image_id)\n\n        if network_ids:\n            interfaces = []\n            for subnet in network_ids.split(','):\n                subnet_id = self._check_subnet(subnet)\n\n                interfaces.append(\n                    boto.ec2.networkinterface.NetworkInterfaceSpecification(\n                        subnet_id=subnet_id, groups=[security_group_id],\n                        associate_public_ip_address=self.request_floating_ip))\n            interfaces = boto.ec2.networkinterface.NetworkInterfaceCollection(*interfaces)\n\n            security_groups = []\n        else:\n            interfaces = None\n            security_groups = [security_group]\n\n        # get defaults for `price` and `timeout` from class instance\n        if price is None:\n            price = self.price\n        if timeout is None:\n            timeout = self.timeout\n\n        if boot_disk_size:\n            dev_root = boto.ec2.blockdevicemapping.BlockDeviceType()\n            dev_root.size = int(boot_disk_size)\n            dev_root.delete_on_termination = True\n            if boot_disk_type:\n                dev_root.volume_type = boot_disk_type\n            if boot_disk_iops:\n                dev_root.iops = int(boot_disk_iops)\n            bdm = boto.ec2.blockdevicemapping.BlockDeviceMapping()\n            dev_name = boot_disk_device if boot_disk_device else \"/dev/sda1\"\n            bdm[dev_name] = dev_root\n        else:\n            bdm = None\n\n        try:\n            #start spot instance if bid is specified\n            if price:\n                log.info(\"Requesting spot instance with price `%s` ...\", price)\n                request = connection.request_spot_instances(\n                                price,image_id, key_name=key_name, security_groups=security_groups,\n                                instance_type=flavor, user_data=image_userdata,\n                                network_interfaces=interfaces,\n                                placement_group=placement_group,\n                                block_device_map=bdm,\n                                instance_profile_name=self._instance_profile)[-1]\n\n                # wait until spot request is fullfilled (will wait\n                # forever if no timeout is given)\n                start_time = time.time()\n                timeout = (float(timeout) if timeout else 0)\n                log.info(\"Waiting for spot instance (will time out in %d seconds) ...\", timeout)\n                while  request.status.code != 'fulfilled':\n                    if timeout and time.time()-start_time > timeout:\n                        request.cancel()\n                        raise RuntimeError('spot instance timed out')\n                    time.sleep(self.POLL_INTERVAL)\n                    # update request status\n                    request=connection.get_all_spot_instance_requests(request_ids=request.id)[-1]\n            else:\n                reservation = connection.run_instances(\n                    image_id, key_name=key_name, security_groups=security_groups,\n                    instance_type=flavor, user_data=image_userdata,\n                    network_interfaces=interfaces,\n                    placement_group=placement_group,\n                    block_device_map=bdm,\n                    instance_profile_name=self._instance_profile)\n        except Exception as ex:\n            log.error(\"Error starting instance: %s\", ex)\n            if \"TooManyInstances\" in ex:\n                raise ClusterError(ex)\n            else:\n                raise InstanceError(ex)\n        if price:\n            vm  = connection.get_only_instances(instance_ids=[request.instance_id])[-1]\n        else:\n            vm = reservation.instances[-1]\n        vm.add_tag(\"Name\", node_name)\n\n        # cache instance object locally for faster access later on\n        self._instances[vm.id] = vm\n\n        return vm.id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nstopping the instance gracefully.", "response": "def stop_instance(self, instance_id):\n        \"\"\"Stops the instance gracefully.\n\n        :param str instance_id: instance identifier\n        \"\"\"\n        instance = self._load_instance(instance_id)\n        instance.terminate()\n        del self._instances[instance_id]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the private and public ip addresses for a given instance.", "response": "def get_ips(self, instance_id):\n        \"\"\"Retrieves the private and public ip addresses for a given instance.\n\n        :return: list (ips)\n        \"\"\"\n        self._load_instance(instance_id)\n        instance = self._load_instance(instance_id)\n        IPs = [ip for ip in (instance.private_ip_address, instance.ip_address) if ip]\n\n        # We also need to check if there is any floating IP associated\n        if self.request_floating_ip and not self._vpc:\n            # We need to list the floating IPs for this instance\n            floating_ips = [ip for ip in self._ec2_connection.get_all_addresses() if ip.instance_id == instance.id]\n            if not floating_ips:\n                log.debug(\"Public ip address has to be assigned through \"\n                          \"elasticluster.\")\n                ip = self._allocate_address(instance)\n                # This is probably the preferred IP we want to use\n                IPs.insert(0, ip)\n            else:\n                IPs = [ip.public_ip for ip in floating_ips] + IPs\n\n        return list(set(IPs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the instance is up and running.", "response": "def is_instance_running(self, instance_id):\n        \"\"\"Checks if the instance is up and running.\n\n        :param str instance_id: instance identifier\n\n        :return: bool - True if running, False otherwise\n        \"\"\"\n        instance = self._load_instance(instance_id)\n\n        if instance.update() == \"running\":\n            # If the instance is up&running, ensure it has an IP\n            # address.\n            if not instance.ip_address and self.request_floating_ip:\n                log.debug(\"Public ip address has to be assigned through \"\n                          \"elasticluster.\")\n                self._allocate_address(instance)\n                instance.update()\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nallocates a free public ip address to the given instance.", "response": "def _allocate_address(self, instance):\n        \"\"\"Allocates a free public ip address to the given instance\n\n        :param instance: instance to assign address to\n        :type instance: py:class:`boto.ec2.instance.Reservation`\n\n        :return: public ip address\n        \"\"\"\n        connection = self._connect()\n        free_addresses = [ ip for ip in connection.get_all_addresses() if not ip.instance_id]\n        if not free_addresses:\n            try:\n                address = connection.allocate_address()\n            except Exception as ex:\n                log.error(\"Unable to allocate a public IP address to instance `%s`\",\n                          instance.id)\n                return None\n\n        try:\n            address = free_addresses.pop()\n            instance.use_ip(address)\n            return address.public_ip\n        except Exception as ex:\n            log.error(\"Unable to associate IP address %s to instance `%s`\",\n                      address, instance.id)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _load_instance(self, instance_id):\n        # if instance is known, return it\n        if instance_id in self._instances:\n            return self._instances[instance_id]\n\n        # else, check (cached) list from provider\n        if instance_id not in self._cached_instances:\n            self._cached_instances = self._build_cached_instances()\n\n        if instance_id in self._cached_instances:\n            inst = self._cached_instances[instance_id]\n            self._instances[instance_id] = inst\n            return inst\n\n        # If we reached this point, the instance was not found neither\n        # in the caches nor on the website.\n        raise InstanceNotFoundError(\n            \"Instance `{instance_id}` not found\"\n            .format(instance_id=instance_id))", "response": "Load an instance from the cache or the cloud provider."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a lookup table of VM instances known to the cloud provider.", "response": "def _build_cached_instances(self):\n        \"\"\"\n        Build lookup table of VM instances known to the cloud provider.\n\n        The returned dictionary links VM id with the actual VM object.\n        \"\"\"\n        connection = self._connect()\n        reservations = connection.get_all_reservations()\n        cached_instances = {}\n        for rs in reservations:\n            for vm in rs.instances:\n                cached_instances[vm.id] = vm\n        return cached_instances"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_keypair(self, name, public_key_path, private_key_path):\n        connection = self._connect()\n        keypairs = connection.get_all_key_pairs()\n        keypairs = dict((k.name, k) for k in keypairs)\n\n        # decide if dsa or rsa key is provided\n        pkey = None\n        is_dsa_key = False\n        try:\n            pkey = DSSKey.from_private_key_file(private_key_path)\n            is_dsa_key = True\n        except PasswordRequiredException:\n            warn(\"Unable to check key file `{0}` because it is encrypted with a \"\n                 \"password. Please, ensure that you added it to the SSH agent \"\n                 \"with `ssh-add {1}`\"\n                 .format(private_key_path, private_key_path))\n        except SSHException:\n            try:\n                pkey = RSAKey.from_private_key_file(private_key_path)\n            except PasswordRequiredException:\n                warn(\"Unable to check key file `{0}` because it is encrypted with a \"\n                     \"password. Please, ensure that you added it to the SSH agent \"\n                     \"with `ssh-add {1}`\"\n                     .format(private_key_path, private_key_path))\n            except SSHException:\n                raise KeypairError('File `%s` is neither a valid DSA key '\n                                   'or RSA key.' % private_key_path)\n\n        # create keys that don't exist yet\n        if name not in keypairs:\n            log.warning(\n                \"Keypair `%s` not found on resource `%s`, Creating a new one\",\n                name, self._url)\n            with open(os.path.expanduser(public_key_path)) as f:\n                key_material = f.read()\n                try:\n                    # check for DSA on amazon\n                    if \"amazon\" in self._ec2host and is_dsa_key:\n                        log.error(\n                            \"Apparently, amazon does not support DSA keys. \"\n                            \"Please specify a valid RSA key.\")\n                        raise KeypairError(\n                            \"Apparently, amazon does not support DSA keys.\"\n                            \"Please specify a valid RSA key.\")\n\n                    connection.import_key_pair(name, key_material)\n                except Exception as ex:\n                    log.error(\n                        \"Could not import key `%s` with name `%s` to `%s`\",\n                        name, public_key_path, self._url)\n                    raise KeypairError(\n                        \"could not create keypair `%s`: %s\" % (name, ex))\n        else:\n            # check fingerprint\n            cloud_keypair = keypairs[name]\n\n            if pkey:\n                if \"amazon\" in self._ec2host:\n                    # AWS takes the MD5 hash of the key's DER representation.\n                    key = RSA.importKey(open(private_key_path).read())\n                    der = key.publickey().exportKey('DER')\n\n                    m = hashlib.md5()\n                    m.update(der)\n                    digest = m.hexdigest()\n                    fingerprint = ':'.join(digest[i:(i + 2)]\n                                           for i in range(0, len(digest), 2))\n                else:\n                    fingerprint = ':'.join(byte_to_hex(byte)\n                                           for byte in pkey.get_fingerprint())\n\n\n                if fingerprint != cloud_keypair.fingerprint:\n                    if \"amazon\" in self._ec2host:\n                        log.error(\n                            \"Apparently, Amazon does not compute the RSA key \"\n                            \"fingerprint as we do! We cannot check if the \"\n                            \"uploaded keypair is correct!\")\n                    else:\n                        raise KeypairError(\n                            \"Keypair `%s` is present but has \"\n                            \"different fingerprint. Aborting!\" % name)", "response": "Checks if the keypair is valid and creates the ssh keys if not"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the security group exists.", "response": "def _check_security_group(self, name):\n        \"\"\"Checks if the security group exists.\n\n        :param str name: name of the security group\n        :return: str - security group id of the security group\n        :raises: `SecurityGroupError` if group does not exist\n        \"\"\"\n        connection = self._connect()\n\n        filters = {}\n        if self._vpc:\n            filters = {'vpc-id': self._vpc_id}\n\n        security_groups = connection.get_all_security_groups(filters=filters)\n\n        matching_groups = [\n            group\n            for group\n             in security_groups\n             if name in [group.name, group.id]\n        ]\n        if len(matching_groups) == 0:\n            raise SecurityGroupError(\n                \"the specified security group %s does not exist\" % name)\n        elif len(matching_groups) == 1:\n            return matching_groups[0].id\n        elif self._vpc and len(matching_groups) > 1:\n            raise SecurityGroupError(\n                \"the specified security group name %s matches \"\n                \"more than one security group\" % name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_subnet(self, name):\n        # Subnets only exist in VPCs, so we don't need to worry about\n        # the EC2 Classic case here.\n        subnets = self._vpc_connection.get_all_subnets(\n            filters={'vpcId': self._vpc_id})\n\n        matching_subnets = [\n            subnet\n            for subnet\n             in subnets\n             if name in [subnet.tags.get('Name'), subnet.id]\n        ]\n        if len(matching_subnets) == 0:\n            raise SubnetError(\n                \"the specified subnet %s does not exist\" % name)\n        elif len(matching_subnets) == 1:\n            return matching_subnets[0].id\n        else:\n            raise SubnetError(\n                \"the specified subnet name %s matches more than \"\n                \"one subnet\" % name)", "response": "Checks if the subnet exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_image_id(self, image_id):\n        if not self._images:\n            connection = self._connect()\n            self._images = connection.get_all_images()\n\n        image_id_cloud = None\n        for i in self._images:\n            if i.id == image_id or i.name == image_id:\n                image_id_cloud = i.id\n                break\n\n        if image_id_cloud:\n            return image_id_cloud\n        else:\n            raise ImageError(\n                \"Could not find given image id `%s`\" % image_id)", "response": "Finds an image id to a given id or name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef migrate_cluster(cluster):\n\n    for old, new in [('_user_key_public', 'user_key_public'),\n                     ('_user_key_private', 'user_key_private'),\n                     ('_user_key_name', 'user_key_name'),]:\n        if hasattr(cluster, old):\n            setattr(cluster, new, getattr(cluster, old))\n            delattr(cluster, old)\n    for kind, nodes in cluster.nodes.items():\n        for node in nodes:\n            if hasattr(node, 'image'):\n                image_id = getattr(node, 'image_id', None) or node.image\n                setattr(node, 'image_id', image_id)\n                delattr(node, 'image')\n\n    # Possibly related to issue #129\n    if not hasattr(cluster, 'thread_pool_max_size'):\n        cluster.thread_pool_max_size = 10\n    return cluster", "response": "Called when loading a cluster when it comes from an older version\n    of elasticluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the cluster by the given name.", "response": "def get(self, name):\n        \"\"\"Retrieves the cluster by the given name.\n\n        :param str name: name of the cluster (identifier)\n        :return: instance of :py:class:`elasticluster.cluster.Cluster` that\n                 matches the given name\n        \"\"\"\n        if name not in self.clusters:\n            raise ClusterNotFound(\"Cluster %s not found.\" % name)\n        return self.clusters.get(name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, cluster):\n        if cluster.name not in self.clusters:\n            raise ClusterNotFound(\n                \"Unable to delete non-existent cluster %s\" % cluster.name)\n        del self.clusters[cluster.name]", "response": "Deletes the cluster from memory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving all clusters from the persistent state.", "response": "def get_all(self):\n        \"\"\"Retrieves all clusters from the persistent state.\n\n        :return: list of :py:class:`elasticluster.cluster.Cluster`\n        \"\"\"\n\n        clusters = []\n        cluster_files = glob.glob(\"%s/*.%s\" % (self.storage_path, self.file_ending))\n        for fname in cluster_files:\n            try:\n                name = fname[:-len(self.file_ending)-1]\n                clusters.append(self.get(name))\n            except (ImportError, AttributeError) as ex:\n                log.error(\"Unable to load cluster %s: `%s`\", fname, ex)\n                log.error(\"If cluster %s was created with a previous version of elasticluster, you may need to run `elasticluster migrate %s %s` to update it.\", cluster_file, self.storage_path, fname)\n        return clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, cluster):\n        path = self._get_cluster_storage_path(cluster.name)\n        if os.path.exists(path):\n            os.unlink(path)", "response": "Deletes the cluster from persistent state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a cluster from a file descriptor fp.", "response": "def load(self, fp):\n        \"\"\"Load cluster from file descriptor fp\"\"\"\n        cluster = pickle.load(fp)\n        cluster.repository = self\n        return cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_store_by_name(self, name):\n        for cls in self.storage_type_map.values():\n            filename = os.path.join(self.storage_path,\n                                    '{name}.{ending}'.format(\n                                        name=name,\n                                        ending=cls.file_ending))\n            if os.path.exists(filename):\n                try:\n                    return cls(self.storage_path)\n                except:\n                    continue\n        raise ClusterNotFound(\n            \"No state file for cluster `{name}` was found in directory `{path}`\"\n            .format(name=name, path=self.storage_path))", "response": "Return an instance of the correct DiskRepository based on the first file that matches the standard syntax for repository files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn server side data.", "response": "def data():\n    \"\"\"Return server side data.\"\"\"\n    # defining columns\n    columns = [\n        ColumnDT(User.id),\n        ColumnDT(User.name),\n        ColumnDT(Address.description),\n        ColumnDT(User.created_at)\n    ]\n\n    # defining the initial query depending on your purpose\n    query = db.session.query().select_from(User).join(Address).filter(\n        Address.id > 14)\n\n    # GET parameters\n    params = request.args.to_dict()\n\n    # instantiating a DataTable for the query and table needed\n    rowTable = DataTables(params, query, columns)\n\n    # returns what is needed by DataTable\n    return jsonify(rowTable.output_result())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_regex(regex):\n    # copy for return\n    ret_regex = regex\n\n    # these characters are escaped (all except alternation | and escape \\)\n    # see http://www.regular-expressions.info/refquick.html\n    escape_chars = '[^$.?*+(){}'\n\n    # remove any escape chars\n    ret_regex = ret_regex.replace('\\\\', '')\n\n    # escape any characters which are used by regex\n    # could probably concoct something incomprehensible using re.sub() but\n    # prefer to write clear code with this loop\n    # note expectation that no characters have already been escaped\n    for c in escape_chars:\n        ret_regex = ret_regex.replace(c, '\\\\' + c)\n\n    # remove any double alternations until these don't exist any more\n    while True:\n        old_regex = ret_regex\n        ret_regex = ret_regex.replace('||', '|')\n        if old_regex == ret_regex:\n            break\n\n    # if last char is alternation | remove it because this\n    # will cause operational error\n    # this can happen as user is typing in global search box\n    while len(ret_regex) >= 1 and ret_regex[-1] == '|':\n        ret_regex = ret_regex[:-1]\n\n    # and back to the caller\n    return ret_regex", "response": "Remove any special characters other than alternation and return a new regex"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef output_result(self):\n        output = {}\n        output['draw'] = str(int(self.params.get('draw', 1)))\n        output['recordsTotal'] = str(self.cardinality)\n        output['recordsFiltered'] = str(self.cardinality_filtered)\n        if self.error:\n            output['error'] = self.error\n            return output\n\n        output['data'] = self.results\n        for k, v in self.yadcf_params:\n            output[k] = v\n        return output", "response": "Output results in the format needed by DataTables."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlaunching filtering sorting and paging to output results.", "response": "def run(self):\n        \"\"\"Launch filtering, sorting and paging to output results.\"\"\"\n        query = self.query\n\n        # count before filtering\n        self.cardinality = query.add_columns(self.columns[0].sqla_expr).count()\n\n        self._set_column_filter_expressions()\n        self._set_global_filter_expression()\n        self._set_sort_expressions()\n        self._set_yadcf_data(query)\n\n        # apply filters\n        query = query.filter(\n            *[e for e in self.filter_expressions if e is not None])\n\n        self.cardinality_filtered = query.add_columns(\n            self.columns[0].sqla_expr).count()\n\n        # apply sorts\n        query = query.order_by(\n            *[e for e in self.sort_expressions if e is not None])\n\n        # add paging options\n        length = int(self.params.get('length'))\n        if length >= 0:\n            query = query.limit(length)\n        elif length == -1:\n            pass\n        else:\n            raise (ValueError(\n                'Length should be a positive integer or -1 to disable'))\n        query = query.offset(int(self.params.get('start')))\n\n        # add columns to query\n        query = query.add_columns(*[c.sqla_expr for c in self.columns])\n\n        # fetch the result of the queries\n        column_names = [\n            col.mData if col.mData else str(i)\n            for i, col in enumerate(self.columns)\n        ]\n        self.results = [{k: v\n                         for k, v in zip(column_names, row)}\n                        for row in query.all()]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the query for filtering.", "response": "def _set_column_filter_expressions(self):\n        \"\"\"Construct the query: filtering.\n\n        Add filtering when per column searching is used.\n        \"\"\"\n        # per columns filters:\n        for i in range(len(self.columns)):\n            filter_expr = None\n            value = self.params.get('columns[{:d}][search][value]'.format(i),\n                                    '')\n            if value:\n                search_func = SEARCH_METHODS[self.columns[i].search_method]\n                filter_expr = search_func(self.columns[i].sqla_expr, value)\n            self.filter_expressions.append(filter_expr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs the query for sorting.", "response": "def _set_sort_expressions(self):\n        \"\"\"Construct the query: sorting.\n\n        Add sorting(ORDER BY) on the columns needed to be applied on.\n        \"\"\"\n        sort_expressions = []\n        i = 0\n        while self.params.get('order[{:d}][column]'.format(i), False):\n            column_nr = int(self.params.get('order[{:d}][column]'.format(i)))\n            column = self.columns[column_nr]\n            direction = self.params.get('order[{:d}][dir]'.format(i))\n            sort_expr = column.sqla_expr\n            if direction == 'asc':\n                sort_expr = sort_expr.asc()\n            elif direction == 'desc':\n                sort_expr = sort_expr.desc()\n            else:\n                raise ValueError(\n                    'Invalid order direction: {}'.format(direction))\n            if column.nulls_order:\n                if column.nulls_order == 'nullsfirst':\n                    sort_expr = sort_expr.nullsfirst()\n                elif column.nulls_order == 'nullslast':\n                    sort_expr = sort_expr.nullslast()\n                else:\n                    raise ValueError(\n                        'Invalid order direction: {}'.format(direction))\n\n            sort_expressions.append(sort_expr)\n            i += 1\n        self.sort_expressions = sort_expressions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_query_value(combined_value):\n    split = len(combined_value) - len(combined_value.lstrip('<>='))\n    operator = combined_value[:split]\n    if operator == '':\n        operator = '='\n    try:\n        operator_func = search_operators[operator]\n    except KeyError:\n        raise ValueError(\n            'Numeric query should start with operator, choose from %s' %\n            ', '.join(search_operators.keys()))\n    value = combined_value[split:].strip()\n    return operator_func, value", "response": "Parse value in form of > value to a lambda and a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef home(request):\n    try:\n        DBSession.query(User).first()\n    except DBAPIError:\n        return Response(\n            conn_err_msg,\n            content_type=\"text/plain\",\n            status_int=500,\n        )\n\n    return {\"project\": \"pyramid_tut\"}", "response": "Return the current project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef data(request):\n    columns = [\n        ColumnDT(User.id),\n        ColumnDT(User.name),\n        ColumnDT(Address.description),\n        ColumnDT(func.strftime(\"%d-%m-%Y\", User.birthday)),\n        ColumnDT(User.age)\n    ]\n\n    query = DBSession.query().select_from(User).join(Address).filter(\n        Address.id > 4)\n\n    rowTable = DataTables(request.GET, query, columns)\n\n    return rowTable.output_result()", "response": "Return server side data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns server side data.", "response": "def data_advanced(request):\n    \"\"\"Return server side data.\"\"\"\n    columns = [\n        ColumnDT(User.id, search_method=\"numeric\"),\n        ColumnDT(User.name),\n        ColumnDT(Address.description),\n        ColumnDT(User.birthday, search_method=\"date\"),\n        ColumnDT(User.age, search_method=\"numeric\")\n    ]\n\n    query = DBSession.query().select_from(User).join(Address).filter(\n        Address.id > 4)\n\n    rowTable = DataTables(request.GET, query, columns)\n\n    return rowTable.output_result()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(global_config, **settings):\n    engine = engine_from_config(settings, \"sqlalchemy.\")\n\n    DBSession.configure(bind=engine)\n\n    Base.metadata.bind = engine\n\n    config = Configurator(settings=settings)\n\n    config.include(\"pyramid_jinja2\")\n\n    config.include(\"pyramid_debugtoolbar\")\n\n    config.add_route(\"home\", \"/\")\n\n    config.add_route(\"data\", \"/data\")\n\n    config.add_route(\"data_advanced\", \"/data_advanced\")\n\n    config.add_route(\"data_yadcf\", \"/data_yadcf\")\n\n    config.add_route(\"dt_110x\", \"/dt_110x\")\n\n    config.add_route(\"dt_110x_custom_column\", \"/dt_110x_custom_column\")\n\n    config.add_route(\"dt_110x_basic_column_search\",\n                     \"/dt_110x_basic_column_search\")\n\n    config.add_route(\"dt_110x_advanced_column_search\",\n                     \"/dt_110x_advanced_column_search\")\n\n    config.add_route(\"dt_110x_yadcf\", \"/dt_110x_yadcf\")\n\n    config.scan()\n\n    json_renderer = JSON()\n\n    json_renderer.add_adapter(date, date_adapter)\n\n    config.add_renderer(\"json_with_dates\", json_renderer)\n\n    config.add_jinja2_renderer('.html')\n\n    return config.make_wsgi_app()", "response": "Return a Pyramid WSGI application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npopulating database with 30 users.", "response": "def main(argv=sys.argv):\n    \"\"\"Populate database with 30 users.\"\"\"\n    if len(argv) < 2:\n        usage(argv)\n\n    config_uri = argv[1]\n\n    setup_logging(config_uri)\n\n    settings = get_appsettings(config_uri)\n\n    engine = engine_from_config(settings, \"sqlalchemy.\")\n\n    DBSession.configure(bind=engine)\n\n    Base.metadata.create_all(engine)\n\n    for i in range(30):\n        with transaction.manager:\n            address = Address(description=\"Address#2\" + str(i).rjust(2, \"0\"))\n\n            DBSession.add(address)\n\n            user = User(\n                name=\"User#1\" + str(i).rjust(2, \"0\"),\n                birthday=date(1980 + i % 8, i % 12 + 1, i % 10 + 1))\n\n            user.address = address\n\n            DBSession.add(user)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse a logline timestamp into a tuple.", "response": "def _parse_logline_timestamp(t):\n    \"\"\"Parses a logline timestamp into a tuple.\n\n    Args:\n        t: Timestamp in logline format.\n\n    Returns:\n        An iterable of date and time elements in the order of month, day, hour,\n        minute, second, microsecond.\n    \"\"\"\n    date, time = t.split(' ')\n    month, day = date.split('-')\n    h, m, s = time.split(':')\n    s, ms = s.split('.')\n    return (month, day, h, m, s, ms)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef logline_timestamp_comparator(t1, t2):\n    dt1 = _parse_logline_timestamp(t1)\n    dt2 = _parse_logline_timestamp(t2)\n    for u1, u2 in zip(dt1, dt2):\n        if u1 < u2:\n            return -1\n        elif u1 > u2:\n            return 1\n    return 0", "response": "Returns the comparator for timestamps in logline format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef epoch_to_log_line_timestamp(epoch_time, time_zone=None):\n    s, ms = divmod(epoch_time, 1000)\n    d = datetime.datetime.fromtimestamp(s, tz=time_zone)\n    return d.strftime('%m-%d %H:%M:%S.') + str(ms)", "response": "Converts an epoch timestamp in ms to log line timestamp format which can be used to read the log line timestamp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_suite(test_classes, argv=None):\n    # Parse cli args.\n    parser = argparse.ArgumentParser(description='Mobly Suite Executable.')\n    parser.add_argument(\n        '-c',\n        '--config',\n        nargs=1,\n        type=str,\n        required=True,\n        metavar='<PATH>',\n        help='Path to the test configuration file.')\n    parser.add_argument(\n        '--tests',\n        '--test_case',\n        nargs='+',\n        type=str,\n        metavar='[ClassA[.test_a] ClassB[.test_b] ...]',\n        help='A list of test classes and optional tests to execute.')\n    if not argv:\n        argv = sys.argv[1:]\n    args = parser.parse_args(argv)\n    # Load test config file.\n    test_configs = config_parser.load_test_config_file(args.config[0])\n\n    # Check the classes that were passed in\n    for test_class in test_classes:\n        if not issubclass(test_class, base_test.BaseTestClass):\n            logging.error('Test class %s does not extend '\n                          'mobly.base_test.BaseTestClass', test_class)\n            sys.exit(1)\n\n    # Find the full list of tests to execute\n    selected_tests = compute_selected_tests(test_classes, args.tests)\n\n    # Execute the suite\n    ok = True\n    for config in test_configs:\n        runner = test_runner.TestRunner(config.log_path, config.test_bed_name)\n        for (test_class, tests) in selected_tests.items():\n            runner.add_test_class(config, test_class, tests)\n        try:\n            runner.run()\n            ok = runner.results.is_all_pass and ok\n        except signals.TestAbortAll:\n            pass\n        except:\n            logging.exception('Exception when executing %s.',\n                              config.test_bed_name)\n            ok = False\n    if not ok:\n        sys.exit(1)", "response": "Executes multiple test classes as a suite."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads an AndroidDevice instance for the given serial number.", "response": "def load_device(self, serial=None):\n        \"\"\"Creates an AndroidDevice for the given serial number.\n\n        If no serial is given, it will read from the ANDROID_SERIAL\n        environmental variable. If the environmental variable is not set, then\n        it will read from 'adb devices' if there is only one.\n        \"\"\"\n        serials = android_device.list_adb_devices()\n        if not serials:\n            raise Error('No adb device found!')\n        # No serial provided, try to pick up the device automatically.\n        if not serial:\n            env_serial = os.environ.get('ANDROID_SERIAL', None)\n            if env_serial is not None:\n                serial = env_serial\n            elif len(serials) == 1:\n                serial = serials[0]\n            else:\n                raise Error(\n                    'Expected one phone, but %d found. Use the -s flag or '\n                    'specify ANDROID_SERIAL.' % len(serials))\n        if serial not in serials:\n            raise Error('Device \"%s\" is not found by adb.' % serial)\n        ads = android_device.get_instances([serial])\n        assert len(ads) == 1\n        self._ad = ads[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nopens a telnet connection to the desired AttenuatorDevice and queries basic information.", "response": "def open(self, host, port=23):\n        \"\"\"Opens a telnet connection to the desired AttenuatorDevice and\n        queries basic information.\n\n        Args:\n            host: A valid hostname (IP address or DNS-resolvable name) to an\n                MC-DAT attenuator instrument.\n            port: An optional port number (defaults to telnet default 23)\n        \"\"\"\n        self._telnet_client.open(host, port)\n        config_str = self._telnet_client.cmd(\"MN?\")\n        if config_str.startswith(\"MN=\"):\n            config_str = config_str[len(\"MN=\"):]\n        self.properties = dict(\n            zip(['model', 'max_freq', 'max_atten'], config_str.split(\"-\", 2)))\n        self.max_atten = float(self.properties['max_atten'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the attenuation value for a particular signal path in an instrument.", "response": "def set_atten(self, idx, value):\n        \"\"\"Sets the attenuation value for a particular signal path.\n\n        Args:\n            idx: Zero-based index int which is the identifier for a particular\n                signal path in an instrument. For instruments that only has one\n                channel, this is ignored by the device.\n            value: A float that is the attenuation value to set.\n\n        Raises:\n            Error: The underlying telnet connection to the instrument is not\n                open.\n            IndexError: The index of the attenuator is greater than the maximum\n                index of the underlying instrument.\n            ValueError: The requested set value is greater than the maximum\n                attenuation value.\n        \"\"\"\n        if not self.is_open:\n            raise attenuator.Error(\n                \"Connection to attenuator at %s is not open!\" %\n                self._telnet_client.host)\n        if idx + 1 > self.path_count:\n            raise IndexError(\"Attenuator index out of range!\", self.path_count,\n                             idx)\n        if value > self.max_atten:\n            raise ValueError(\"Attenuator value out of range!\", self.max_atten,\n                             value)\n        # The actual device uses one-based index for channel numbers.\n        self._telnet_client.cmd(\"CHAN:%s:SETATT:%s\" % (idx + 1, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef start_app_and_connect(self):\n        # Check that sl4a is installed\n        out = self._adb.shell('pm list package')\n        if not utils.grep('com.googlecode.android_scripting', out):\n            raise jsonrpc_client_base.AppStartError(\n                self._ad, '%s is not installed on %s' % (_APP_NAME,\n                                                         self._adb.serial))\n        self.disable_hidden_api_blacklist()\n\n        # sl4a has problems connecting after disconnection, so kill the apk and\n        # try connecting again.\n        try:\n            self.stop_app()\n        except Exception as e:\n            self.log.warning(e)\n\n        # Launch the app\n        self.device_port = _DEVICE_SIDE_PORT\n        self._adb.shell(_LAUNCH_CMD % self.device_port)\n\n        # Try to start the connection (not restore the connectivity).\n        # The function name restore_app_connection is used here is for the\n        # purpose of reusing the same code as it does when restoring the\n        # connection. And we do not want to come up with another function\n        # name to complicate the API. Change the name if necessary.\n        self.restore_app_connection()", "response": "Starts the app and connects to the device."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrestoring the sl4a after device got disconnected.", "response": "def restore_app_connection(self, port=None):\n        \"\"\"Restores the sl4a after device got disconnected.\n\n        Instead of creating new instance of the client:\n          - Uses the given port (or find a new available host_port if none is\n            given).\n          - Tries to connect to remote server with selected port.\n\n        Args:\n          port: If given, this is the host port from which to connect to remote\n              device port. If not provided, find a new available port as host\n              port.\n\n        Raises:\n            AppRestoreConnectionError: When the app was not able to be started.\n        \"\"\"\n        self.host_port = port or utils.get_available_host_port()\n        self._retry_connect()\n        self.ed = self._start_event_client()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_app(self):\n        try:\n            if self._conn:\n                # Be polite; let the dest know we're shutting down.\n                try:\n                    self.closeSl4aSession()\n                except:\n                    self.log.exception('Failed to gracefully shut down %s.',\n                                       self.app_name)\n\n                # Close the socket connection.\n                self.disconnect()\n                self.stop_event_dispatcher()\n\n            # Terminate the app\n            self._adb.shell('am force-stop com.googlecode.android_scripting')\n        finally:\n            # Always clean up the adb port\n            self.clear_host_port()", "response": "Closes the connection and closes the socket and closes the event dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart snippet apk on the device and connects to it.", "response": "def start_app_and_connect(self):\n        \"\"\"Starts snippet apk on the device and connects to it.\n\n        This wraps the main logic with safe handling\n\n        Raises:\n            AppStartPreCheckError, when pre-launch checks fail.\n        \"\"\"\n        try:\n            self._start_app_and_connect()\n        except AppStartPreCheckError:\n            # Precheck errors don't need cleanup, directly raise.\n            raise\n        except Exception as e:\n            # Log the stacktrace of `e` as re-raising doesn't preserve trace.\n            self._ad.log.exception('Failed to start app and connect.')\n            # If errors happen, make sure we clean up before raising.\n            try:\n                self.stop_app()\n            except:\n                self._ad.log.exception(\n                    'Failed to stop app after failure to start and connect.')\n            # Explicitly raise the original error from starting app.\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_app_and_connect(self):\n        self._check_app_installed()\n        self.disable_hidden_api_blacklist()\n\n        persists_shell_cmd = self._get_persist_command()\n        # Use info here so people can follow along with the snippet startup\n        # process. Starting snippets can be slow, especially if there are\n        # multiple, and this avoids the perception that the framework is hanging\n        # for a long time doing nothing.\n        self.log.info('Launching snippet apk %s with protocol %d.%d',\n                      self.package, _PROTOCOL_MAJOR_VERSION,\n                      _PROTOCOL_MINOR_VERSION)\n        cmd = _LAUNCH_CMD % (persists_shell_cmd, self.package)\n        start_time = time.time()\n        self._proc = self._do_start_app(cmd)\n\n        # Check protocol version and get the device port\n        line = self._read_protocol_line()\n        match = re.match('^SNIPPET START, PROTOCOL ([0-9]+) ([0-9]+)$', line)\n        if not match or match.group(1) != '1':\n            raise ProtocolVersionError(self._ad, line)\n\n        line = self._read_protocol_line()\n        match = re.match('^SNIPPET SERVING, PORT ([0-9]+)$', line)\n        if not match:\n            raise ProtocolVersionError(self._ad, line)\n        self.device_port = int(match.group(1))\n\n        # Forward the device port to a new host port, and connect to that port\n        self.host_port = utils.get_available_host_port()\n        self._adb.forward(\n            ['tcp:%d' % self.host_port,\n             'tcp:%d' % self.device_port])\n        self.connect()\n\n        # Yaaay! We're done!\n        self.log.debug('Snippet %s started after %.1fs on host port %s',\n                       self.package, time.time() - start_time, self.host_port)", "response": "Starts the snippet apk on the device and connects to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrestores the app after device got reconnected.", "response": "def restore_app_connection(self, port=None):\n        \"\"\"Restores the app after device got reconnected.\n\n        Instead of creating new instance of the client:\n          - Uses the given port (or find a new available host_port if none is\n            given).\n          - Tries to connect to remote server with selected port.\n\n        Args:\n          port: If given, this is the host port from which to connect to remote\n              device port. If not provided, find a new available port as host\n              port.\n\n        Raises:\n            AppRestoreConnectionError: When the app was not able to be started.\n        \"\"\"\n        self.host_port = port or utils.get_available_host_port()\n        self._adb.forward(\n            ['tcp:%d' % self.host_port,\n             'tcp:%d' % self.device_port])\n        try:\n            self.connect()\n        except:\n            # Log the original error and raise AppRestoreConnectionError.\n            self.log.exception('Failed to re-connect to app.')\n            raise jsonrpc_client_base.AppRestoreConnectionError(\n                self._ad,\n                ('Failed to restore app connection for %s at host port %s, '\n                 'device port %s') % (self.package, self.host_port,\n                                      self.device_port))\n\n        # Because the previous connection was lost, update self._proc\n        self._proc = None\n        self._restore_event_client()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart the event client.", "response": "def _start_event_client(self):\n        \"\"\"Overrides superclass.\"\"\"\n        event_client = SnippetClient(package=self.package, ad=self._ad)\n        event_client.host_port = self.host_port\n        event_client.device_port = self.device_port\n        event_client.connect(self.uid,\n                             jsonrpc_client_base.JsonRpcCommand.CONTINUE)\n        return event_client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestore previously created event client.", "response": "def _restore_event_client(self):\n        \"\"\"Restores previously created event client.\"\"\"\n        if not self._event_client:\n            self._event_client = self._start_event_client()\n            return\n        self._event_client.host_port = self.host_port\n        self._event_client.device_port = self.device_port\n        self._event_client.connect()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_protocol_line(self):\n        while True:\n            line = self._proc.stdout.readline().decode('utf-8')\n            if not line:\n                raise jsonrpc_client_base.AppStartError(\n                    self._ad, 'Unexpected EOF waiting for app to start')\n            # readline() uses an empty string to mark EOF, and a single newline\n            # to mark regular empty lines in the output. Don't move the strip()\n            # call above the truthiness check, or this method will start\n            # considering any blank output line to be EOF.\n            line = line.strip()\n            if (line.startswith('INSTRUMENTATION_RESULT:')\n                    or line.startswith('SNIPPET ')):\n                self.log.debug(\n                    'Accepted line from instrumentation output: \"%s\"', line)\n                return line\n            self.log.debug('Discarded line from instrumentation output: \"%s\"',\n                           line)", "response": "Reads the next line of instrumentation output relevant to snippets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking availability and return path of command if available.", "response": "def _get_persist_command(self):\n        \"\"\"Check availability and return path of command if available.\"\"\"\n        for command in [_SETSID_COMMAND, _NOHUP_COMMAND]:\n            try:\n                if command in self._adb.shell(['which',\n                                               command]).decode('utf-8'):\n                    return command\n            except adb.AdbError:\n                continue\n        self.log.warning(\n            'No %s and %s commands available to launch instrument '\n            'persistently, tests that depend on UiAutomator and '\n            'at the same time performs USB disconnection may fail',\n            _SETSID_COMMAND, _NOHUP_COMMAND)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef help(self, print_output=True):\n        help_text = self._rpc('help')\n        if print_output:\n            print(help_text)\n        else:\n            return help_text", "response": "Calls the help RPC which returns the list of RPC calls available."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_any_alive(self):\n        for service in self._service_objects.values():\n            if service.is_alive:\n                return True\n        return False", "response": "True if any service is alive ; False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nregisters a new service.", "response": "def register(self, alias, service_class, configs=None, start_service=True):\n        \"\"\"Registers a service.\n\n        This will create a service instance, starts the service, and adds the\n        instance to the mananger.\n\n        Args:\n            alias: string, the alias for this instance.\n            service_class: class, the service class to instantiate.\n            configs: (optional) config object to pass to the service class's\n                constructor.\n            start_service: bool, whether to start the service instance or not.\n                Default is True.\n        \"\"\"\n        if not inspect.isclass(service_class):\n            raise Error(self._device, '\"%s\" is not a class!' % service_class)\n        if not issubclass(service_class, base_service.BaseService):\n            raise Error(\n                self._device,\n                'Class %s is not a subclass of BaseService!' % service_class)\n        if alias in self._service_objects:\n            raise Error(\n                self._device,\n                'A service is already registered with alias \"%s\".' % alias)\n        service_obj = service_class(self._device, configs)\n        if start_service:\n            service_obj.start()\n        self._service_objects[alias] = service_obj"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting all inactive service instances.", "response": "def start_all(self):\n        \"\"\"Starts all inactive service instances.\"\"\"\n        for alias, service in self._service_objects.items():\n            if not service.is_alive:\n                with expects.expect_no_raises(\n                        'Failed to start service \"%s\".' % alias):\n                    service.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stop_all(self):\n        for alias, service in self._service_objects.items():\n            if service.is_alive:\n                with expects.expect_no_raises(\n                        'Failed to stop service \"%s\".' % alias):\n                    service.stop()", "response": "Stops all active service instances."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pause_all(self):\n        for alias, service in self._service_objects.items():\n            with expects.expect_no_raises(\n                    'Failed to pause service \"%s\".' % alias):\n                service.pause()", "response": "Pauses all service instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresumes all service instances.", "response": "def resume_all(self):\n        \"\"\"Resumes all service instances.\"\"\"\n        for alias, service in self._service_objects.items():\n            with expects.expect_no_raises(\n                    'Failed to pause service \"%s\".' % alias):\n                service.resume()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(configs):\n    if not configs:\n        raise Error(ANDROID_DEVICE_EMPTY_CONFIG_MSG)\n    elif configs == ANDROID_DEVICE_PICK_ALL_TOKEN:\n        ads = get_all_instances()\n    elif not isinstance(configs, list):\n        raise Error(ANDROID_DEVICE_NOT_LIST_CONFIG_MSG)\n    elif isinstance(configs[0], dict):\n        # Configs is a list of dicts.\n        ads = get_instances_with_configs(configs)\n    elif isinstance(configs[0], basestring):\n        # Configs is a list of strings representing serials.\n        ads = get_instances(configs)\n    else:\n        raise Error('No valid config found in: %s' % configs)\n    valid_ad_identifiers = list_adb_devices() + list_adb_devices_by_usb_id()\n\n    for ad in ads:\n        if ad.serial not in valid_ad_identifiers:\n            raise DeviceError(ad, 'Android device is specified in config but'\n                              ' is not attached.')\n    _start_services_on_ads(ads)\n    return ads", "response": "Creates an AndroidDevice controller object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncleans up all AndroidDevice objects.", "response": "def destroy(ads):\n    \"\"\"Cleans up AndroidDevice objects.\n\n    Args:\n        ads: A list of AndroidDevice objects.\n    \"\"\"\n    for ad in ads:\n        try:\n            ad.services.stop_all()\n        except:\n            ad.log.exception('Failed to clean up properly.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart long running services on multiple AndroidDevice objects.", "response": "def _start_services_on_ads(ads):\n    \"\"\"Starts long running services on multiple AndroidDevice objects.\n\n    If any one AndroidDevice object fails to start services, cleans up all\n    existing AndroidDevice objects and their services.\n\n    Args:\n        ads: A list of AndroidDevice objects whose services to start.\n    \"\"\"\n    running_ads = []\n    for ad in ads:\n        running_ads.append(ad)\n        start_logcat = not getattr(ad, KEY_SKIP_LOGCAT,\n                                   DEFAULT_VALUE_SKIP_LOGCAT)\n        try:\n            ad.services.register(\n                SERVICE_NAME_LOGCAT, logcat.Logcat, start_service=start_logcat)\n        except Exception:\n            is_required = getattr(ad, KEY_DEVICE_REQUIRED,\n                                  DEFAULT_VALUE_DEVICE_REQUIRED)\n            if is_required:\n                ad.log.exception('Failed to start some services, abort!')\n                destroy(running_ads)\n                raise\n            else:\n                ad.log.exception('Skipping this optional device because some '\n                                 'services failed to start.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a byte string representing a list of android devices.", "response": "def parse_device_list(device_list_str, key):\n    \"\"\"Parses a byte string representing a list of devices.\n\n    The string is generated by calling either adb or fastboot. The tokens in\n    each string is tab-separated.\n\n    Args:\n        device_list_str: Output of adb or fastboot.\n        key: The token that signifies a device in device_list_str.\n\n    Returns:\n        A list of android device serial numbers.\n    \"\"\"\n    clean_lines = new_str(device_list_str, 'utf-8').strip().split('\\n')\n    results = []\n    for line in clean_lines:\n        tokens = line.strip().split('\\t')\n        if len(tokens) == 2 and tokens[1] == key:\n            results.append(tokens[0])\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the usb id of all android devices connected to the computer that have been detected by adb.", "response": "def list_adb_devices_by_usb_id():\n    \"\"\"List the usb id of all android devices connected to the computer that\n    are detected by adb.\n\n    Returns:\n        A list of strings that are android device usb ids. Empty if there's\n        none.\n    \"\"\"\n    out = adb.AdbProxy().devices(['-l'])\n    clean_lines = new_str(out, 'utf-8').strip().split('\\n')\n    results = []\n    for line in clean_lines:\n        tokens = line.strip().split()\n        if len(tokens) > 2 and tokens[1] == 'device':\n            results.append(tokens[2])\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates AndroidDevice instances from a list of android devices.", "response": "def get_instances(serials):\n    \"\"\"Create AndroidDevice instances from a list of serials.\n\n    Args:\n        serials: A list of android device serials.\n\n    Returns:\n        A list of AndroidDevice objects.\n    \"\"\"\n    results = []\n    for s in serials:\n        results.append(AndroidDevice(s))\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates AndroidDevice instances from a list of dicts.", "response": "def get_instances_with_configs(configs):\n    \"\"\"Create AndroidDevice instances from a list of dict configs.\n\n    Each config should have the required key-value pair 'serial'.\n\n    Args:\n        configs: A list of dicts each representing the configuration of one\n            android device.\n\n    Returns:\n        A list of AndroidDevice objects.\n    \"\"\"\n    results = []\n    for c in configs:\n        try:\n            serial = c.pop('serial')\n        except KeyError:\n            raise Error(\n                'Required value \"serial\" is missing in AndroidDevice config %s.'\n                % c)\n        is_required = c.get(KEY_DEVICE_REQUIRED, True)\n        try:\n            ad = AndroidDevice(serial)\n            ad.load_config(c)\n        except Exception:\n            if is_required:\n                raise\n            ad.log.exception('Skipping this optional device due to error.')\n            continue\n        results.append(ad)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate AndroidDevice instances for all attached android devices.", "response": "def get_all_instances(include_fastboot=False):\n    \"\"\"Create AndroidDevice instances for all attached android devices.\n\n    Args:\n        include_fastboot: Whether to include devices in bootloader mode or not.\n\n    Returns:\n        A list of AndroidDevice objects each representing an android device\n        attached to the computer.\n    \"\"\"\n    if include_fastboot:\n        serial_list = list_adb_devices() + list_fastboot_devices()\n        return get_instances(serial_list)\n    return get_instances(list_adb_devices())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of AndroidDevice instances that satisfy certain conditions.", "response": "def filter_devices(ads, func):\n    \"\"\"Finds the AndroidDevice instances from a list that match certain\n    conditions.\n\n    Args:\n        ads: A list of AndroidDevice instances.\n        func: A function that takes an AndroidDevice object and returns True\n            if the device satisfies the filter condition.\n\n    Returns:\n        A list of AndroidDevice instances that satisfy the filter condition.\n    \"\"\"\n    results = []\n    for ad in ads:\n        if func(ad):\n            results.append(ad)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_devices(ads, **kwargs):\n\n    def _get_device_filter(ad):\n        for k, v in kwargs.items():\n            if not hasattr(ad, k):\n                return False\n            elif getattr(ad, k) != v:\n                return False\n        return True\n\n    filtered = filter_devices(ads, _get_device_filter)\n    if not filtered:\n        raise Error(\n            'Could not find a target device that matches condition: %s.' %\n            kwargs)\n    else:\n        return filtered", "response": "Finds a list of AndroidDevice instances from a list that has specific\n    attributes of certain values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a unique AndroidDevice instance from a list that has specific attributes of certain values.", "response": "def get_device(ads, **kwargs):\n    \"\"\"Finds a unique AndroidDevice instance from a list that has specific\n    attributes of certain values.\n\n    Deprecated, use `get_devices(ads, **kwargs)[0]` instead.\n    This method will be removed in 1.8.\n\n    Example:\n        get_device(android_devices, label='foo', phone_number='1234567890')\n        get_device(android_devices, model='angler')\n\n    Args:\n        ads: A list of AndroidDevice instances.\n        kwargs: keyword arguments used to filter AndroidDevice instances.\n\n    Returns:\n        The target AndroidDevice instance.\n\n    Raises:\n        Error: None or more than one device is matched.\n    \"\"\"\n\n    filtered = get_devices(ads, **kwargs)\n    if len(filtered) == 1:\n        return filtered[0]\n    else:\n        serials = [ad.serial for ad in filtered]\n        raise Error('More than one device matched: %s' % serials)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake bug reports on a list of AndroidDevice instances.", "response": "def take_bug_reports(ads, test_name, begin_time, destination=None):\n    \"\"\"Takes bug reports on a list of android devices.\n\n    If you want to take a bug report, call this function with a list of\n    android_device objects in on_fail. But reports will be taken on all the\n    devices in the list concurrently. Bug report takes a relative long\n    time to take, so use this cautiously.\n\n    Args:\n        ads: A list of AndroidDevice instances.\n        test_name: Name of the test method that triggered this bug report.\n        begin_time: timestamp taken when the test started, can be either\n            string or int.\n        destination: string, path to the directory where the bugreport\n            should be saved.\n    \"\"\"\n    begin_time = mobly_logger.normalize_log_line_timestamp(str(begin_time))\n\n    def take_br(test_name, begin_time, ad, destination):\n        ad.take_bug_report(test_name, begin_time, destination=destination)\n\n    args = [(test_name, begin_time, ad, destination) for ad in ads]\n    utils.concurrent_exec(take_br, args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalized_serial(self):\n        if self._serial is None:\n            return None\n        normalized_serial = self._serial.replace(' ', '_')\n        normalized_serial = normalized_serial.replace(':', '-')\n        return normalized_serial", "response": "Return a normalized serial name for usage in log filename."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef device_info(self):\n        info = {\n            'serial': self.serial,\n            'model': self.model,\n            'build_info': self.build_info,\n            'user_added_info': self._user_added_device_info\n        }\n        return info", "response": "Return a dictionary of information to be pulled into controller info."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef log_path(self, new_path):\n        if self.has_active_service:\n            raise DeviceError(\n                self,\n                'Cannot change `log_path` when there is service running.')\n        old_path = self._log_path\n        if new_path == old_path:\n            return\n        if os.listdir(new_path):\n            raise DeviceError(\n                self, 'Logs already exist at %s, cannot override.' % new_path)\n        if os.path.exists(old_path):\n            # Remove new path so copytree doesn't complain.\n            shutil.rmtree(new_path, ignore_errors=True)\n            shutil.copytree(old_path, new_path)\n            shutil.rmtree(old_path, ignore_errors=True)\n        self._log_path = new_path", "response": "Setter for `log_path`, use with caution."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_serial(self, new_serial):\n        new_serial = str(new_serial)\n        if self.has_active_service:\n            raise DeviceError(\n                self,\n                'Cannot change device serial number when there is service running.'\n            )\n        if self._debug_tag == self.serial:\n            self._debug_tag = new_serial\n        self._serial = new_serial\n        self.adb.serial = new_serial\n        self.fastboot.serial = new_serial", "response": "Updates the serial number of a specific object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_reboot(self):\n        self.services.stop_all()\n        try:\n            yield\n        finally:\n            self.wait_for_boot_completion()\n            if self.is_rootable:\n                self.root_adb()\n        self.services.start_all()", "response": "Handle a reboot event."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_info(self):\n        if self.is_bootloader:\n            self.log.error('Device is in fastboot mode, could not get build '\n                           'info.')\n            return\n        info = {}\n        info['build_id'] = self.adb.getprop('ro.build.id')\n        info['build_type'] = self.adb.getprop('ro.build.type')\n        return info", "response": "Get the build info of this Android device including build id and build type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_adb_root(self):\n        try:\n            return '0' == self.adb.shell('id -u').decode('utf-8').strip()\n        except adb.AdbError:\n            # Wait a bit and retry to work around adb flakiness for this cmd.\n            time.sleep(0.2)\n            return '0' == self.adb.shell('id -u').decode('utf-8').strip()", "response": "True if adb is running as root for this device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef model(self):\n        # If device is in bootloader mode, get mode name from fastboot.\n        if self.is_bootloader:\n            out = self.fastboot.getvar('product').strip()\n            # 'out' is never empty because of the 'total time' message fastboot\n            # writes to stderr.\n            lines = out.decode('utf-8').split('\\n', 1)\n            if lines:\n                tokens = lines[0].split(' ')\n                if len(tokens) > 1:\n                    return tokens[1].lower()\n            return None\n        model = self.adb.getprop('ro.build.product').lower()\n        if model == 'sprout':\n            return model\n        return self.adb.getprop('ro.product.name').lower()", "response": "The Android code name for the device."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the configuration of the AndroidDevice object based on the given dictionary.", "response": "def load_config(self, config):\n        \"\"\"Add attributes to the AndroidDevice object based on config.\n\n        Args:\n            config: A dictionary representing the configs.\n\n        Raises:\n            Error: The config is trying to overwrite an existing attribute.\n        \"\"\"\n        for k, v in config.items():\n            if hasattr(self, k):\n                raise DeviceError(\n                    self,\n                    ('Attribute %s already exists with value %s, cannot set '\n                     'again.') % (k, getattr(self, k)))\n            setattr(self, k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange adb to root mode for this device if allowed.", "response": "def root_adb(self):\n        \"\"\"Change adb to root mode for this device if allowed.\n\n        If executed on a production build, adb will not be switched to root\n        mode per security restrictions.\n        \"\"\"\n        self.adb.root()\n        self.adb.wait_for_device(\n            timeout=DEFAULT_TIMEOUT_BOOT_COMPLETION_SECOND)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the snippet apk with the given package name and connects to the snippet client.", "response": "def load_snippet(self, name, package):\n        \"\"\"Starts the snippet apk with the given package name and connects.\n\n        Examples:\n\n        .. code-block:: python\n\n            ad.load_snippet(\n                    name='maps', package='com.google.maps.snippets')\n            ad.maps.activateZoom('3')\n\n        Args:\n            name: string, the attribute name to which to attach the snippet\n                client. E.g. `name='maps'` attaches the snippet client to\n                `ad.maps`.\n            package: string, the package name of the snippet apk to connect to.\n\n        Raises:\n            SnippetError: Illegal load operations are attempted.\n        \"\"\"\n        # Should not load snippet with an existing attribute.\n        if hasattr(self, name):\n            raise SnippetError(\n                self,\n                'Attribute \"%s\" already exists, please use a different name.' %\n                name)\n        self.services.snippets.add_snippet_client(name, package)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a bugreport on the device and stores it in a file.", "response": "def take_bug_report(self,\n                        test_name,\n                        begin_time,\n                        timeout=300,\n                        destination=None):\n        \"\"\"Takes a bug report on the device and stores it in a file.\n\n        Args:\n            test_name: Name of the test method that triggered this bug report.\n            begin_time: Timestamp of when the test started.\n            timeout: float, the number of seconds to wait for bugreport to\n                complete, default is 5min.\n            destination: string, path to the directory where the bugreport\n                should be saved.\n        \"\"\"\n        new_br = True\n        try:\n            stdout = self.adb.shell('bugreportz -v').decode('utf-8')\n            # This check is necessary for builds before N, where adb shell's ret\n            # code and stderr are not propagated properly.\n            if 'not found' in stdout:\n                new_br = False\n        except adb.AdbError:\n            new_br = False\n        if destination:\n            br_path = utils.abs_path(destination)\n        else:\n            br_path = os.path.join(self.log_path, 'BugReports')\n        utils.create_dir(br_path)\n        base_name = ',%s,%s.txt' % (begin_time, self._normalized_serial)\n        if new_br:\n            base_name = base_name.replace('.txt', '.zip')\n        test_name_len = utils.MAX_FILENAME_LEN - len(base_name)\n        out_name = test_name[:test_name_len] + base_name\n        full_out_path = os.path.join(br_path, out_name.replace(' ', r'\\ '))\n        # in case device restarted, wait for adb interface to return\n        self.wait_for_boot_completion()\n        self.log.info('Taking bugreport for %s.', test_name)\n        if new_br:\n            out = self.adb.shell('bugreportz', timeout=timeout).decode('utf-8')\n            if not out.startswith('OK'):\n                raise DeviceError(self, 'Failed to take bugreport: %s' % out)\n            br_out_path = out.split(':')[1].strip()\n            self.adb.pull([br_out_path, full_out_path])\n        else:\n            # shell=True as this command redirects the stdout to a local file\n            # using shell redirection.\n            self.adb.bugreport(\n                ' > \"%s\"' % full_out_path, shell=True, timeout=timeout)\n        self.log.info('Bugreport for %s taken at %s.', test_name,\n                      full_out_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_iperf_client(self, server_host, extra_args=''):\n        out = self.adb.shell('iperf3 -c %s %s' % (server_host, extra_args))\n        clean_out = new_str(out, 'utf-8').strip().split('\\n')\n        if 'error' in clean_out[0].lower():\n            return False, clean_out\n        return True, clean_out", "response": "Start iperf client on the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_boot_completion(\n            self, timeout=DEFAULT_TIMEOUT_BOOT_COMPLETION_SECOND):\n        \"\"\"Waits for Android framework to broadcast ACTION_BOOT_COMPLETED.\n\n        This function times out after 15 minutes.\n\n        Args:\n            timeout: float, the number of seconds to wait before timing out.\n                If not specified, no timeout takes effect.\n        \"\"\"\n        timeout_start = time.time()\n\n        self.adb.wait_for_device(timeout=timeout)\n        while time.time() < timeout_start + timeout:\n            try:\n                if self.is_boot_completed():\n                    return\n            except adb.AdbError:\n                # adb shell calls may fail during certain period of booting\n                # process, which is normal. Ignoring these errors.\n                pass\n            time.sleep(5)\n        raise DeviceError(self, 'Booting process timed out')", "response": "Waits for Android framework to broadcast ACTION_BOOT_COMPLETED."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if device boot is completed by verifying system property.", "response": "def is_boot_completed(self):\n        \"\"\"Checks if device boot is completed by verifying system property.\"\"\"\n        completed = self.adb.getprop('sys.boot_completed')\n        if completed == '1':\n            self.log.debug('Device boot completed.')\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if USB is on and device is ready by verifying adb devices.", "response": "def is_adb_detectable(self):\n        \"\"\"Checks if USB is on and device is ready by verifying adb devices.\"\"\"\n        serials = list_adb_devices()\n        if self.serial in serials:\n            self.log.debug('Is now adb detectable.')\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef expect_true(condition, msg, extras=None):\n    try:\n        asserts.assert_true(condition, msg, extras)\n    except signals.TestSignal as e:\n        logging.exception('Expected a `True` value, got `False`.')\n        recorder.add_error(e)", "response": "Expects an expression evaluates to True."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expect_false(condition, msg, extras=None):\n    try:\n        asserts.assert_false(condition, msg, extras)\n    except signals.TestSignal as e:\n        logging.exception('Expected a `False` value, got `True`.')\n        recorder.add_error(e)", "response": "Expects an expression evaluates to False."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexpecting the equality of objects.", "response": "def expect_equal(first, second, msg=None, extras=None):\n    \"\"\"Expects the equality of objects, otherwise fail the test.\n\n    If the expectation is not met, the test is marked as fail after its\n    execution finishes.\n\n    Error message is \"first != second\" by default. Additional explanation can\n    be supplied in the message.\n\n    Args:\n        first: The first object to compare.\n        second: The second object to compare.\n        msg: A string that adds additional info about the failure.\n        extras: An optional field for extra information to be included in test\n            result.\n    \"\"\"\n    try:\n        asserts.assert_equal(first, second, msg, extras)\n    except signals.TestSignal as e:\n        logging.exception('Expected %s equals to %s, but they are not.', first,\n                          second)\n        recorder.add_error(e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexpect no exception is raised in a context.", "response": "def expect_no_raises(message=None, extras=None):\n    \"\"\"Expects no exception is raised in a context.\n\n    If the expectation is not met, the test is marked as fail after its\n    execution finishes.\n\n    A default message is added to the exception `details`.\n\n    Args:\n        message: string, custom message to add to exception's `details`.\n        extras: An optional field for extra information to be included in test\n            result.\n    \"\"\"\n    try:\n        yield\n    except Exception as e:\n        e_record = records.ExceptionRecord(e)\n        if extras:\n            e_record.extras = extras\n        msg = message or 'Got an unexpected exception'\n        details = '%s: %s' % (msg, e_record.details)\n        logging.exception(details)\n        e_record.details = details\n        recorder.add_error(e_record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset_internal_states(self, record=None):\n        self._record = None\n        self._count = 0\n        self._record = record", "response": "Resets the internal state of the recorder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an error to the expect record.", "response": "def add_error(self, error):\n        \"\"\"Record an error from expect APIs.\n\n        This method generates a position stamp for the expect. The stamp is\n        composed of a timestamp and the number of errors recorded so far.\n\n        Args:\n            error: Exception or signals.ExceptionRecord, the error to add.\n        \"\"\"\n        self._count += 1\n        self._record.add_error('expect@%s+%s' % (time.time(), self._count),\n                               error)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_services(self, console_env):\n        self._ad.services.register('sl4a', sl4a_service.Sl4aService)\n        console_env['s'] = self._ad.services.sl4a\n        console_env['sl4a'] = self._ad.sl4a\n        console_env['ed'] = self._ad.ed", "response": "Overrides superclass. Start sl4a services."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _enable_logpersist(self):\n        # Logpersist is only allowed on rootable devices because of excessive\n        # reads/writes for persisting logs.\n        if not self._ad.is_rootable:\n            return\n\n        logpersist_warning = ('%s encountered an error enabling persistent'\n                              ' logs, logs may not get saved.')\n        # Android L and older versions do not have logpersist installed,\n        # so check that the logpersist scripts exists before trying to use\n        # them.\n        if not self._ad.adb.has_shell_command('logpersist.start'):\n            logging.warning(logpersist_warning, self)\n            return\n\n        try:\n            # Disable adb log spam filter for rootable devices. Have to stop\n            # and clear settings first because 'start' doesn't support --clear\n            # option before Android N.\n            self._ad.adb.shell('logpersist.stop --clear')\n            self._ad.adb.shell('logpersist.start')\n        except adb.AdbError:\n            logging.warning(logpersist_warning, self)", "response": "Attempts to enable logpersist daemon to persist logs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear_adb_log(self):\n        try:\n            self._ad.adb.logcat('-c')\n        except adb.AdbError as e:\n            # On Android O, the clear command fails due to a known bug.\n            # Catching this so we don't crash from this Android issue.\n            if b'failed to clear' in e.stderr:\n                self._ad.log.warning(\n                    'Encountered known Android error to clear logcat.')\n            else:\n                raise", "response": "Clears cached adb content."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cat_adb_log(self, tag, begin_time):\n        if not self.adb_logcat_file_path:\n            raise Error(\n                self._ad,\n                'Attempting to cat adb log when none has been collected.')\n        end_time = mobly_logger.get_log_line_timestamp()\n        self._ad.log.debug('Extracting adb log from logcat.')\n        adb_excerpt_path = os.path.join(self._ad.log_path, 'AdbLogExcerpts')\n        utils.create_dir(adb_excerpt_path)\n        f_name = os.path.basename(self.adb_logcat_file_path)\n        out_name = f_name.replace('adblog,', '').replace('.txt', '')\n        out_name = ',%s,%s.txt' % (begin_time, out_name)\n        out_name = out_name.replace(':', '-')\n        tag_len = utils.MAX_FILENAME_LEN - len(out_name)\n        tag = tag[:tag_len]\n        out_name = tag + out_name\n        full_adblog_path = os.path.join(adb_excerpt_path, out_name)\n        with io.open(full_adblog_path, 'w', encoding='utf-8') as out:\n            in_file = self.adb_logcat_file_path\n            with io.open(\n                    in_file, 'r', encoding='utf-8', errors='replace') as f:\n                in_range = False\n                while True:\n                    line = None\n                    try:\n                        line = f.readline()\n                        if not line:\n                            break\n                    except:\n                        continue\n                    line_time = line[:mobly_logger.log_line_timestamp_len]\n                    if not mobly_logger.is_valid_logline_timestamp(line_time):\n                        continue\n                    if self._is_timestamp_in_range(line_time, begin_time,\n                                                   end_time):\n                        in_range = True\n                        if not line.endswith('\\n'):\n                            line += '\\n'\n                        out.write(line)\n                    else:\n                        if in_range:\n                            break", "response": "Takes an excerpt of the adb logcat log file and returns a new file with the contents of the excerpt."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a standing adb logcat collection.", "response": "def start(self):\n        \"\"\"Starts a standing adb logcat collection.\n\n        The collection runs in a separate subprocess and saves logs in a file.\n        \"\"\"\n        self._assert_not_running()\n        if self._configs.clear_log:\n            self.clear_adb_log()\n        self._start()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop(self):\n        if not self._adb_logcat_process:\n            return\n        try:\n            utils.stop_standing_subprocess(self._adb_logcat_process)\n        except:\n            self._ad.log.exception('Failed to stop adb logcat.')\n        self._adb_logcat_process = None", "response": "Stops the adb logcat service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a connection to a JSON - RPC server and creates a new session.", "response": "def connect(self, uid=UNKNOWN_UID, cmd=JsonRpcCommand.INIT):\n        \"\"\"Opens a connection to a JSON RPC server.\n\n        Opens a connection to a remote client. The connection attempt will time\n        out if it takes longer than _SOCKET_CONNECTION_TIMEOUT seconds. Each\n        subsequent operation over this socket will time out after\n        _SOCKET_READ_TIMEOUT seconds as well.\n\n        Args:\n            uid: int, The uid of the session to join, or UNKNOWN_UID to start a\n                new session.\n            cmd: JsonRpcCommand, The command to use for creating the connection.\n\n        Raises:\n            IOError: Raised when the socket times out from io error\n            socket.timeout: Raised when the socket waits to long for connection.\n            ProtocolError: Raised when there is an error in the protocol.\n        \"\"\"\n        self._counter = self._id_counter()\n        self._conn = socket.create_connection(('localhost', self.host_port),\n                                              _SOCKET_CONNECTION_TIMEOUT)\n        self._conn.settimeout(_SOCKET_READ_TIMEOUT)\n        self._client = self._conn.makefile(mode='brw')\n\n        resp = self._cmd(cmd, uid)\n        if not resp:\n            raise ProtocolError(self._ad,\n                                ProtocolError.NO_RESPONSE_FROM_HANDSHAKE)\n        result = json.loads(str(resp, encoding='utf8'))\n        if result['status']:\n            self.uid = result['uid']\n        else:\n            self.uid = UNKNOWN_UID"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear_host_port(self):\n        if self.host_port:\n            self._adb.forward(['--remove', 'tcp:%d' % self.host_port])\n            self.host_port = None", "response": "Stops the adb port forwarding of the host port used by this client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _client_send(self, msg):\n        try:\n            self._client.write(msg.encode(\"utf8\") + b'\\n')\n            self._client.flush()\n            self.log.debug('Snippet sent %s.', msg)\n        except socket.error as e:\n            raise Error(\n                self._ad,\n                'Encountered socket error \"%s\" sending RPC message \"%s\"' %\n                (e, msg))", "response": "Sends an Rpc message through the connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _client_receive(self):\n        try:\n            response = self._client.readline()\n            self.log.debug('Snippet received: %s', response)\n            return response\n        except socket.error as e:\n            raise Error(\n                self._ad,\n                'Encountered socket error reading RPC response \"%s\"' % e)", "response": "Receives the server s response of an Rpc message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _cmd(self, command, uid=None):\n        if not uid:\n            uid = self.uid\n        self._client_send(json.dumps({'cmd': command, 'uid': uid}))\n        return self._client_receive()", "response": "Send a command to the server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend an rpc to the app.", "response": "def _rpc(self, method, *args):\n        \"\"\"Sends an rpc to the app.\n\n        Args:\n            method: str, The name of the method to execute.\n            args: any, The args of the method.\n\n        Returns:\n            The result of the rpc.\n\n        Raises:\n            ProtocolError: Something went wrong with the protocol.\n            ApiError: The rpc went through, however executed with errors.\n        \"\"\"\n        with self._lock:\n            apiid = next(self._counter)\n            data = {'id': apiid, 'method': method, 'params': args}\n            request = json.dumps(data)\n            self._client_send(request)\n            response = self._client_receive()\n        if not response:\n            raise ProtocolError(self._ad,\n                                ProtocolError.NO_RESPONSE_FROM_SERVER)\n        result = json.loads(str(response, encoding='utf8'))\n        if result['error']:\n            raise ApiError(self._ad, result['error'])\n        if result['id'] != apiid:\n            raise ProtocolError(self._ad, ProtocolError.MISMATCHED_API_ID)\n        if result.get('callback') is not None:\n            if self._event_client is None:\n                self._event_client = self._start_event_client()\n            return callback_handler.CallbackHandler(\n                callback_id=result['callback'],\n                event_client=self._event_client,\n                ret_value=result['result'],\n                method_name=method,\n                ad=self._ad)\n        return result['result']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naveraging receiving rate in MB/s over the entire run.", "response": "def avg_rate(self):\n        \"\"\"Average receiving rate in MB/s over the entire run.\n\n        If the result is not from a success run, this property is None.\n        \"\"\"\n        if not self._has_data or 'sum' not in self.result['end']:\n            return None\n        bps = self.result['end']['sum']['bits_per_second']\n        return bps / 8 / 1024 / 1024"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef avg_receive_rate(self):\n        if not self._has_data or 'sum_received' not in self.result['end']:\n            return None\n        bps = self.result['end']['sum_received']['bits_per_second']\n        return bps / 8 / 1024 / 1024", "response": "Average receiving rate in MB/s over the entire run."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef avg_send_rate(self):\n        if not self._has_data or 'sum_sent' not in self.result['end']:\n            return None\n        bps = self.result['end']['sum_sent']['bits_per_second']\n        return bps / 8 / 1024 / 1024", "response": "Average sending rate in MB/s over the entire run."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart iperf server on specified port.", "response": "def start(self, extra_args=\"\", tag=\"\"):\n        \"\"\"Starts iperf server on specified port.\n\n        Args:\n            extra_args: A string representing extra arguments to start iperf\n                server with.\n            tag: Appended to log file name to identify logs from different\n                iperf runs.\n        \"\"\"\n        if self.started:\n            return\n        utils.create_dir(self.log_path)\n        if tag:\n            tag = tag + ','\n        out_file_name = \"IPerfServer,{},{}{}.log\".format(\n            self.port, tag, len(self.log_files))\n        full_out_path = os.path.join(self.log_path, out_file_name)\n        cmd = '%s %s > %s' % (self.iperf_str, extra_args, full_out_path)\n        self.iperf_process = utils.start_standing_subprocess(cmd, shell=True)\n        self.log_files.append(full_out_path)\n        self.started = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _pre_capture_config(self, override_configs=None):\n        final_configs = {}\n        if self._base_configs:\n            final_configs.update(self._base_configs)\n        if override_configs:\n            final_configs.update(override_configs)\n\n        if sniffer.Sniffer.CONFIG_KEY_CHANNEL in final_configs:\n            try:\n                subprocess.check_call([\n                    'iwconfig',\n                    self._interface,\n                    'channel',\n                    str(final_configs[sniffer.Sniffer.CONFIG_KEY_CHANNEL])])\n            except Exception as err:\n                raise sniffer.ExecutionError(err)", "response": "Utility function which configures the wireless interface per the the\n        specified configurations."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a new capture for the current instance of this class.", "response": "def start_capture(self, override_configs=None,\n                      additional_args=None, duration=None,\n                      packet_count=None):\n        \"\"\"See base class documentation\n        \"\"\"\n        if self._process is not None:\n            raise sniffer.InvalidOperationError(\n                    \"Trying to start a sniff while another is still running!\")\n        capture_dir = os.path.join(self._logger.log_path,\n                                   \"Sniffer-{}\".format(self._interface))\n        os.makedirs(capture_dir, exist_ok=True)\n        self._capture_file_path = os.path.join(capture_dir,\n                      \"capture_{}.pcap\".format(logger.get_log_file_timestamp()))\n\n        self._pre_capture_config(override_configs)\n        _, self._temp_capture_file_path = tempfile.mkstemp(suffix=\".pcap\")\n\n        cmd = self._get_command_line(additional_args=additional_args,\n                                duration=duration, packet_count=packet_count)\n\n        self._process = utils.start_standing_subprocess(cmd)\n        return sniffer.ActiveCaptureContext(self, duration)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop_capture(self):\n        if self._process is None:\n            raise sniffer.InvalidOperationError(\n                                      \"Trying to stop a non-started process\")\n        utils.stop_standing_subprocess(self._process, kill_signal=signal.SIGINT)\n        self._post_process()", "response": "Stop capture of the current process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait_for_capture(self, timeout=None):\n        if self._process is None:\n            raise sniffer.InvalidOperationError(\n                                  \"Trying to wait on a non-started process\")\n        try:\n            utils.wait_for_standing_subprocess(self._process, timeout)\n            self._post_process()\n        except subprocess.TimeoutExpired:\n            self.stop_capture()", "response": "Wait for the process to finish capture."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_occupied_adb_ports():\n    out = AdbProxy().forward('--list')\n    clean_lines = str(out, 'utf-8').strip().split('\\n')\n    used_ports = []\n    for line in clean_lines:\n        tokens = line.split(' tcp:')\n        if len(tokens) != 3:\n            continue\n        used_ports.append(int(tokens[1]))\n    return used_ports", "response": "Lists all the host ports occupied by adb forward."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _exec_cmd(self, args, shell, timeout, stderr):\n        if timeout and timeout <= 0:\n            raise ValueError('Timeout is not a positive value: %s' % timeout)\n        try:\n            (ret, out, err) = utils.run_command(\n                args, shell=shell, timeout=timeout)\n        except psutil.TimeoutExpired:\n            raise AdbTimeoutError(\n                cmd=args, timeout=timeout, serial=self.serial)\n\n        if stderr:\n            stderr.write(err)\n        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s',\n                      utils.cli_cmd_to_string(args), out, err, ret)\n        if ret == 0:\n            return out\n        else:\n            raise AdbError(\n                cmd=args,\n                stdout=out,\n                stderr=err,\n                ret_code=ret,\n                serial=self.serial)", "response": "Executes the adb command and returns the output."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes adb commands and processes the stdout.", "response": "def _execute_and_process_stdout(self, args, shell, handler):\n        \"\"\"Executes adb commands and processes the stdout with a handler.\n\n        Args:\n            args: string or list of strings, program arguments.\n                See subprocess.Popen() documentation.\n            shell: bool, True to run this command through the system shell,\n                False to invoke it directly. See subprocess.Popen() docs.\n            handler: func, a function to handle adb stdout line by line.\n\n        Returns:\n            The stderr of the adb command run if exit code is 0.\n\n        Raises:\n            AdbError: The adb command exit code is not 0.\n        \"\"\"\n        proc = subprocess.Popen(\n            args,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=shell,\n            bufsize=1)\n        out = '[elided, processed via handler]'\n        try:\n            # Even if the process dies, stdout.readline still works\n            # and will continue until it runs out of stdout to process.\n            while True:\n                line = proc.stdout.readline()\n                if line:\n                    handler(line)\n                else:\n                    break\n        finally:\n            # Note, communicate will not contain any buffered output.\n            (unexpected_out, err) = proc.communicate()\n            if unexpected_out:\n                out = '[unexpected stdout] %s' % unexpected_out\n                for line in unexpected_out.splitlines():\n                    handler(line)\n\n        ret = proc.returncode\n        logging.debug('cmd: %s, stdout: %s, stderr: %s, ret: %s',\n                      utils.cli_cmd_to_string(args), out, err, ret)\n        if ret == 0:\n            return err\n        else:\n            raise AdbError(cmd=args, stdout=out, stderr=err, ret_code=ret)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct an ADB command to be used for the current ADB module.", "response": "def _construct_adb_cmd(self, raw_name, args, shell):\n        \"\"\"Constructs an adb command with arguments for a subprocess call.\n\n        Args:\n            raw_name: string, the raw unsanitized name of the adb command to\n                format.\n            args: string or list of strings, arguments to the adb command.\n                See subprocess.Proc() documentation.\n            shell: bool, True to run this command through the system shell,\n                False to invoke it directly. See subprocess.Proc() docs.\n\n        Returns:\n            The adb command in a format appropriate for subprocess. If shell is\n                True, then this is a string; otherwise, this is a list of\n                strings.\n        \"\"\"\n        args = args or ''\n        name = raw_name.replace('_', '-')\n        if shell:\n            args = utils.cli_cmd_to_string(args)\n            # Add quotes around \"adb\" in case the ADB path contains spaces. This\n            # is pretty common on Windows (e.g. Program Files).\n            if self.serial:\n                adb_cmd = '\"%s\" -s \"%s\" %s %s' % (ADB, self.serial, name, args)\n            else:\n                adb_cmd = '\"%s\" %s %s' % (ADB, name, args)\n        else:\n            adb_cmd = [ADB]\n            if self.serial:\n                adb_cmd.extend(['-s', self.serial])\n            adb_cmd.append(name)\n            if args:\n                if isinstance(args, basestring):\n                    adb_cmd.append(args)\n                else:\n                    adb_cmd.extend(args)\n        return adb_cmd"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a property of the device.", "response": "def getprop(self, prop_name):\n        \"\"\"Get a property of the device.\n\n        This is a convenience wrapper for \"adb shell getprop xxx\".\n\n        Args:\n            prop_name: A string that is the name of the property to get.\n\n        Returns:\n            A string that is the value of the property, or None if the property\n            doesn't exist.\n        \"\"\"\n        return self.shell(\n            ['getprop', prop_name],\n            timeout=DEFAULT_GETPROP_TIMEOUT_SEC).decode('utf-8').strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks to see if a given command exists on the device.", "response": "def has_shell_command(self, command):\n        \"\"\"Checks to see if a given check command exists on the device.\n\n        Args:\n            command: A string that is the name of the command to check.\n\n        Returns:\n            A boolean that is True if the command exists and False otherwise.\n        \"\"\"\n        try:\n            output = self.shell(['command', '-v',\n                                 command]).decode('utf-8').strip()\n            return command in output\n        except AdbError:\n            # If the command doesn't exist, then 'command -v' can return\n            # an exit code > 1.\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun an instrumentation command on the device.", "response": "def instrument(self, package, options=None, runner=None, handler=None):\n        \"\"\"Runs an instrumentation command on the device.\n\n        This is a convenience wrapper to avoid parameter formatting.\n\n        Example:\n\n        .. code-block:: python\n\n            device.instrument(\n                'com.my.package.test',\n                options = {\n                    'class': 'com.my.package.test.TestSuite',\n                },\n            )\n\n        Args:\n            package: string, the package of the instrumentation tests.\n            options: dict, the instrumentation options including the test\n                class.\n            runner: string, the test runner name, which defaults to\n                DEFAULT_INSTRUMENTATION_RUNNER.\n            handler: optional func, when specified the function is used to parse\n                the instrumentation stdout line by line as the output is\n                generated; otherwise, the stdout is simply returned once the\n                instrumentation is finished.\n\n        Returns:\n            The stdout of instrumentation command or the stderr if the handler\n                is set.\n        \"\"\"\n        if runner is None:\n            runner = DEFAULT_INSTRUMENTATION_RUNNER\n        if options is None:\n            options = {}\n\n        options_list = []\n        for option_key, option_value in options.items():\n            options_list.append('-e %s %s' % (option_key, option_value))\n        options_string = ' '.join(options_list)\n\n        instrumentation_command = 'am instrument -r -w %s %s/%s' % (\n            options_string, package, runner)\n        logging.info('AndroidDevice|%s: Executing adb shell %s', self.serial,\n                     instrumentation_command)\n        if handler is None:\n            # Flow kept for backwards-compatibility reasons\n            self._exec_adb_cmd(\n                'shell',\n                instrumentation_command,\n                shell=False,\n                timeout=None,\n                stderr=None)\n        else:\n            return self._execute_adb_and_process_stdout(\n                'shell', instrumentation_command, shell=False, handler=handler)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assert_equal(first, second, msg=None, extras=None):\n    my_msg = None\n    try:\n        _pyunit_proxy.assertEqual(first, second)\n    except AssertionError as e:\n        my_msg = str(e)\n        if msg:\n            my_msg = \"%s %s\" % (my_msg, msg)\n\n    # This raise statement is outside of the above except statement to prevent\n    # Python3's exception message from having two tracebacks.\n    if my_msg is not None:\n        raise signals.TestFailure(my_msg, extras=extras)", "response": "Assert the equality of objects otherwise fail the test."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assert_raises(expected_exception, extras=None, *args, **kwargs):\n    context = _AssertRaisesContext(expected_exception, extras=extras)\n    return context", "response": "Assert that an exception is raised when a function is called."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nasserts that an exception is raised when a function is called.", "response": "def assert_raises_regex(expected_exception,\n                        expected_regex,\n                        extras=None,\n                        *args,\n                        **kwargs):\n    \"\"\"Assert that an exception is raised when a function is called.\n\n    If no exception is raised, test fail. If an exception is raised but not\n    of the expected type, the exception is let through. If an exception of the\n    expected type is raised but the error message does not match the\n    expected_regex, test fail.\n\n    This should only be used as a context manager:\n        with assert_raises(Exception):\n            func()\n\n    Args:\n        expected_exception: An exception class that is expected to be\n            raised.\n        extras: An optional field for extra information to be included in\n            test result.\n    \"\"\"\n    context = _AssertRaisesContext(\n        expected_exception, expected_regex, extras=extras)\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef waitAndGet(self, event_name, timeout=DEFAULT_TIMEOUT):\n        if timeout:\n            if timeout > MAX_TIMEOUT:\n                raise Error(\n                    self._ad,\n                    'Specified timeout %s is longer than max timeout %s.' %\n                    (timeout, MAX_TIMEOUT))\n        # Convert to milliseconds for java side.\n        timeout_ms = int(timeout * 1000)\n        try:\n            raw_event = self._event_client.eventWaitAndGet(\n                self._id, event_name, timeout_ms)\n        except Exception as e:\n            if 'EventSnippetException: timeout.' in str(e):\n                raise TimeoutError(\n                    self._ad,\n                    'Timed out after waiting %ss for event \"%s\" triggered by'\n                    ' %s (%s).' % (timeout, event_name, self._method_name,\n                                   self._id))\n            raise\n        return snippet_event.from_dict(raw_event)", "response": "Blocks until an event of the specified name has been received and returns the event or timeout."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits for an event that satisfies the predicate.", "response": "def waitForEvent(self, event_name, predicate, timeout=DEFAULT_TIMEOUT):\n        \"\"\"Wait for an event of a specific name that satisfies the predicate.\n\n        This call will block until the expected event has been received or time\n        out.\n\n        The predicate function defines the condition the event is expected to\n        satisfy. It takes an event and returns True if the condition is\n        satisfied, False otherwise.\n\n        Note all events of the same name that are received but don't satisfy\n        the predicate will be discarded and not be available for further\n        consumption.\n\n        Args:\n            event_name: string, the name of the event to wait for.\n            predicate: function, a function that takes an event (dictionary) and\n                returns a bool.\n            timeout: float, default is 120s.\n\n        Returns:\n            dictionary, the event that satisfies the predicate if received.\n\n        Raises:\n            TimeoutError: raised if no event that satisfies the predicate is\n                received after timeout seconds.\n        \"\"\"\n        deadline = time.time() + timeout\n        while time.time() <= deadline:\n            # Calculate the max timeout for the next event rpc call.\n            rpc_timeout = deadline - time.time()\n            if rpc_timeout < 0:\n                break\n            # A single RPC call cannot exceed MAX_TIMEOUT.\n            rpc_timeout = min(rpc_timeout, MAX_TIMEOUT)\n            try:\n                event = self.waitAndGet(event_name, rpc_timeout)\n            except TimeoutError:\n                # Ignoring TimeoutError since we need to throw one with a more\n                # specific message.\n                break\n            if predicate(event):\n                return event\n        raise TimeoutError(\n            self._ad,\n            'Timed out after %ss waiting for an \"%s\" event that satisfies the '\n            'predicate \"%s\".' % (timeout, event_name, predicate.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting all the events of a certain name that have been received so far.", "response": "def getAll(self, event_name):\n        \"\"\"Gets all the events of a certain name that have been received so\n        far. This is a non-blocking call.\n\n        Args:\n            callback_id: The id of the callback.\n            event_name: string, the name of the event to get.\n\n        Returns:\n            A list of SnippetEvent, each representing an event from the Java\n            side.\n        \"\"\"\n        raw_events = self._event_client.eventGetAll(self._id, event_name)\n        return [snippet_event.from_dict(msg) for msg in raw_events]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_config(config):\n    required_keys = [KEY_ADDRESS, KEY_MODEL, KEY_PORT, KEY_PATHS]\n    for key in required_keys:\n        if key not in config:\n            raise Error(\"Required key %s missing from config %s\",\n                        (key, config))", "response": "Verifies that a config dict for an attenuator device is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_instances(serials):\n    objs = []\n    for s in serials:\n        objs.append(Monsoon(serial=s))\n    return objs", "response": "Create Monsoon instances from a list of serials."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetStatus(self):\n        # status packet format\n        STATUS_FORMAT = \">BBBhhhHhhhHBBBxBbHBHHHHBbbHHBBBbbbbbbbbbBH\"\n        STATUS_FIELDS = [\n            \"packetType\",\n            \"firmwareVersion\",\n            \"protocolVersion\",\n            \"mainFineCurrent\",\n            \"usbFineCurrent\",\n            \"auxFineCurrent\",\n            \"voltage1\",\n            \"mainCoarseCurrent\",\n            \"usbCoarseCurrent\",\n            \"auxCoarseCurrent\",\n            \"voltage2\",\n            \"outputVoltageSetting\",\n            \"temperature\",\n            \"status\",\n            \"leds\",\n            \"mainFineResistor\",\n            \"serialNumber\",\n            \"sampleRate\",\n            \"dacCalLow\",\n            \"dacCalHigh\",\n            \"powerUpCurrentLimit\",\n            \"runTimeCurrentLimit\",\n            \"powerUpTime\",\n            \"usbFineResistor\",\n            \"auxFineResistor\",\n            \"initialUsbVoltage\",\n            \"initialAuxVoltage\",\n            \"hardwareRevision\",\n            \"temperatureLimit\",\n            \"usbPassthroughMode\",\n            \"mainCoarseResistor\",\n            \"usbCoarseResistor\",\n            \"auxCoarseResistor\",\n            \"defMainFineResistor\",\n            \"defUsbFineResistor\",\n            \"defAuxFineResistor\",\n            \"defMainCoarseResistor\",\n            \"defUsbCoarseResistor\",\n            \"defAuxCoarseResistor\",\n            \"eventCode\",\n            \"eventData\",\n        ]\n\n        self._SendStruct(\"BBB\", 0x01, 0x00, 0x00)\n        while 1:  # Keep reading, discarding non-status packets\n            read_bytes = self._ReadPacket()\n            if not read_bytes:\n                return None\n            calsize = struct.calcsize(STATUS_FORMAT)\n            if len(read_bytes) != calsize or read_bytes[0] != 0x10:\n                logging.warning(\"Wanted status, dropped type=0x%02x, len=%d\",\n                                read_bytes[0], len(read_bytes))\n                continue\n            status = dict(\n                zip(STATUS_FIELDS, struct.unpack(STATUS_FORMAT, read_bytes)))\n            p_type = status[\"packetType\"]\n            if p_type != 0x10:\n                raise MonsoonError(\"Package type %s is not 0x10.\" % p_type)\n            for k in status.keys():\n                if k.endswith(\"VoltageSetting\"):\n                    status[k] = 2.0 + status[k] * 0.01\n                elif k.endswith(\"FineCurrent\"):\n                    pass  # needs calibration data\n                elif k.endswith(\"CoarseCurrent\"):\n                    pass  # needs calibration data\n                elif k.startswith(\"voltage\") or k.endswith(\"Voltage\"):\n                    status[k] = status[k] * 0.000125\n                elif k.endswith(\"Resistor\"):\n                    status[k] = 0.05 + status[k] * 0.0001\n                    if k.startswith(\"aux\") or k.startswith(\"defAux\"):\n                        status[k] += 0.05\n                elif k.endswith(\"CurrentLimit\"):\n                    status[k] = 8 * (1023 - status[k]) / 1023.0\n            return status", "response": "Requests and waits for status."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef SetVoltage(self, v):\n        if v == 0:\n            self._SendStruct(\"BBB\", 0x01, 0x01, 0x00)\n        else:\n            self._SendStruct(\"BBB\", 0x01, 0x01, int((v - 2.0) * 100))", "response": "Set the output voltage."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the max output current.", "response": "def SetMaxCurrent(self, i):\n        \"\"\"Set the max output current.\n        \"\"\"\n        if i < 0 or i > 8:\n            raise MonsoonError((\"Target max current %sA, is out of acceptable \"\n                                \"range [0, 8].\") % i)\n        val = 1023 - int((i / 8) * 1023)\n        self._SendStruct(\"BBB\", 0x01, 0x0a, val & 0xff)\n        self._SendStruct(\"BBB\", 0x01, 0x0b, val >> 8)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SetMaxPowerUpCurrent(self, i):\n        if i < 0 or i > 8:\n            raise MonsoonError((\"Target max current %sA, is out of acceptable \"\n                                \"range [0, 8].\") % i)\n        val = 1023 - int((i / 8) * 1023)\n        self._SendStruct(\"BBB\", 0x01, 0x08, val & 0xff)\n        self._SendStruct(\"BBB\", 0x01, 0x09, val >> 8)", "response": "Sets the max power up current."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef CollectData(self):\n        while 1:  # loop until we get data or a timeout\n            _bytes = self._ReadPacket()\n            if not _bytes:\n                return None\n            if len(_bytes) < 4 + 8 + 1 or _bytes[0] < 0x20 or _bytes[0] > 0x2F:\n                logging.warning(\"Wanted data, dropped type=0x%02x, len=%d\",\n                                _bytes[0], len(_bytes))\n                continue\n\n            seq, _type, x, y = struct.unpack(\"BBBB\", _bytes[:4])\n            data = [\n                struct.unpack(\">hhhh\", _bytes[x:x + 8])\n                for x in range(4,\n                               len(_bytes) - 8, 8)\n            ]\n\n            if self._last_seq and seq & 0xF != (self._last_seq + 1) & 0xF:\n                logging.warning(\"Data sequence skipped, lost packet?\")\n            self._last_seq = seq\n\n            if _type == 0:\n                if not self._coarse_scale or not self._fine_scale:\n                    logging.warning(\n                        \"Waiting for calibration, dropped data packet.\")\n                    continue\n                out = []\n                for main, usb, aux, voltage in data:\n                    if main & 1:\n                        coarse = ((main & ~1) - self._coarse_zero)\n                        out.append(coarse * self._coarse_scale)\n                    else:\n                        out.append((main - self._fine_zero) * self._fine_scale)\n                return out\n            elif _type == 1:\n                self._fine_zero = data[0][0]\n                self._coarse_zero = data[1][0]\n            elif _type == 2:\n                self._fine_ref = data[0][0]\n                self._coarse_ref = data[1][0]\n            else:\n                logging.warning(\"Discarding data packet type=0x%02x\", _type)\n                continue\n\n            # See http://wiki/Main/MonsoonProtocol for details on these values.\n            if self._coarse_ref != self._coarse_zero:\n                self._coarse_scale = 2.88 / (\n                    self._coarse_ref - self._coarse_zero)\n            if self._fine_ref != self._fine_zero:\n                self._fine_scale = 0.0332 / (self._fine_ref - self._fine_zero)", "response": "Collect some current samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npacking a struct and send it.", "response": "def _SendStruct(self, fmt, *args):\n        \"\"\"Pack a struct (without length or checksum) and send it.\n        \"\"\"\n        data = struct.pack(fmt, *args)\n        data_len = len(data) + 1\n        checksum = (data_len + sum(bytearray(data))) % 256\n        out = struct.pack(\"B\", data_len) + data + struct.pack(\"B\", checksum)\n        self.ser.write(out)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ReadPacket(self):\n        len_char = self.ser.read(1)\n        if not len_char:\n            logging.error(\"Reading from serial port timed out.\")\n            return None\n\n        data_len = ord(len_char)\n        if not data_len:\n            return \"\"\n        result = self.ser.read(int(data_len))\n        result = bytearray(result)\n        if len(result) != data_len:\n            logging.error(\"Length mismatch, expected %d bytes, got %d bytes.\",\n                          data_len, len(result))\n            return None\n        body = result[:-1]\n        checksum = (sum(struct.unpack(\"B\" * len(body), body)) + data_len) % 256\n        if result[-1] != checksum:\n            logging.error(\n                \"Invalid checksum from serial port! Expected %s, got %s\",\n                hex(checksum), hex(result[-1]))\n            return None\n        return result[:-1]", "response": "Reads a single data record from the serial port."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _FlushInput(self):\n        self.ser.flush()\n        flushed = 0\n        while True:\n            ready_r, ready_w, ready_x = select.select([self.ser], [],\n                                                      [self.ser], 0)\n            if len(ready_x) > 0:\n                logging.error(\"Exception from serial port.\")\n                return None\n            elif len(ready_r) > 0:\n                flushed += 1\n                self.ser.read(1)  # This may cause underlying buffering.\n                self.ser.flush()  # Flush the underlying buffer too.\n            else:\n                break", "response": "Flushes all available input from the serial port."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef average_current(self):\n        len_data_pt = len(self.data_points)\n        if len_data_pt == 0:\n            return 0\n        cur = sum(self.data_points) * 1000 / len_data_pt\n        return round(cur, self.sr)", "response": "Average current in the unit of mA."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntotals charged used in the unit of mAh.", "response": "def total_charge(self):\n        \"\"\"Total charged used in the unit of mAh.\n        \"\"\"\n        charge = (sum(self.data_points) / self.hz) * 1000 / 3600\n        return round(charge, self.sr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_string(data_str):\n        lines = data_str.strip().split('\\n')\n        err_msg = (\"Invalid input string format. Is this string generated by \"\n                   \"MonsoonData class?\")\n        conditions = [\n            len(lines) <= 4, \"Average Current:\" not in lines[1],\n            \"Voltage: \" not in lines[2], \"Total Power: \" not in lines[3],\n            \"samples taken at \" not in lines[4],\n            lines[5] != \"Time\" + ' ' * 7 + \"Amp\"\n        ]\n        if any(conditions):\n            raise MonsoonError(err_msg)\n        hz_str = lines[4].split()[2]\n        hz = int(hz_str[:-2])\n        voltage_str = lines[2].split()[1]\n        voltage = int(voltage_str[:-1])\n        lines = lines[6:]\n        t = []\n        v = []\n        for l in lines:\n            try:\n                timestamp, value = l.split(' ')\n                t.append(int(timestamp))\n                v.append(float(value))\n            except ValueError:\n                raise MonsoonError(err_msg)\n        return MonsoonData(v, t, hz, voltage)", "response": "Creates a MonsoonData object from a string representation generated by the MonsoonData class by __str__."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves multiple MonsoonData objects to a text file.", "response": "def save_to_text_file(monsoon_data, file_path):\n        \"\"\"Save multiple MonsoonData objects to a text file.\n\n        Args:\n            monsoon_data: A list of MonsoonData objects to write to a text\n                file.\n            file_path: The full path of the file to save to, including the file\n                name.\n        \"\"\"\n        if not monsoon_data:\n            raise MonsoonError(\"Attempting to write empty Monsoon data to \"\n                               \"file, abort\")\n        utils.create_dir(os.path.dirname(file_path))\n        with io.open(file_path, 'w', encoding='utf-8') as f:\n            for md in monsoon_data:\n                f.write(str(md))\n                f.write(MonsoonData.delimiter)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_text_file(file_path):\n        results = []\n        with io.open(file_path, 'r', encoding='utf-8') as f:\n            data_strs = f.read().split(MonsoonData.delimiter)\n            for data_str in data_strs:\n                results.append(MonsoonData.from_string(data_str))\n        return results", "response": "Load MonsoonData objects from a text file generated by MonsoonData. save_to_text_file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_data(self):\n        msg = \"Error! Expected {} timestamps, found {}.\".format(\n            len(self._data_points), len(self._timestamps))\n        if len(self._data_points) != len(self._timestamps):\n            raise MonsoonError(msg)", "response": "Verifies that the data points contained in the class are valid."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the offset of the caculations.", "response": "def update_offset(self, new_offset):\n        \"\"\"Updates how many data points to skip in caculations.\n\n        Always use this function to update offset instead of directly setting\n        self.offset.\n\n        Args:\n            new_offset: The new offset.\n        \"\"\"\n        self.offset = new_offset\n        self.data_points = self._data_points[self.offset:]\n        self.timestamps = self._timestamps[self.offset:]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the data points with timestamps.", "response": "def get_data_with_timestamps(self):\n        \"\"\"Returns the data points with timestamps.\n\n        Returns:\n            A list of tuples in the format of (timestamp, data)\n        \"\"\"\n        result = []\n        for t, d in zip(self.timestamps, self.data_points):\n            result.append(t, round(d, self.lr))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of average current numbers each representing the average over the last n data points.", "response": "def get_average_record(self, n):\n        \"\"\"Returns a list of average current numbers, each representing the\n        average over the last n data points.\n\n        Args:\n            n: Number of data points to average over.\n\n        Returns:\n            A list of average current values.\n        \"\"\"\n        history_deque = collections.deque()\n        averages = []\n        for d in self.data_points:\n            history_deque.appendleft(d)\n            if len(history_deque) > n:\n                history_deque.pop()\n            avg = sum(history_deque) / len(history_deque)\n            averages.append(round(avg, self.lr))\n        return averages"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the output voltage of the Monsoon.", "response": "def set_voltage(self, volt, ramp=False):\n        \"\"\"Sets the output voltage of monsoon.\n\n        Args:\n            volt: Voltage to set the output to.\n            ramp: If true, the output voltage will be increased gradually to\n                prevent tripping Monsoon overvoltage.\n        \"\"\"\n        if ramp:\n            self.mon.RampVoltage(self.mon.start_voltage, volt)\n        else:\n            self.mon.SetVoltage(volt)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking samples of the current value supplied by monsoon. This is the actual measurement for power consumption. This function blocks until the number of samples requested has been fulfilled. Args: hz: Number of points to take for every second. sample_num: Number of samples to take. offset: The number of initial data points to discard in MonsoonData calculations. sample_num is extended by offset to compensate. live: Print each sample in console as measurement goes on. Returns: A MonsoonData object representing the data obtained in this sampling. None if sampling is unsuccessful.", "response": "def take_samples(self, sample_hz, sample_num, sample_offset=0, live=False):\n        \"\"\"Take samples of the current value supplied by monsoon.\n\n        This is the actual measurement for power consumption. This function\n        blocks until the number of samples requested has been fulfilled.\n\n        Args:\n            hz: Number of points to take for every second.\n            sample_num: Number of samples to take.\n            offset: The number of initial data points to discard in MonsoonData\n                calculations. sample_num is extended by offset to compensate.\n            live: Print each sample in console as measurement goes on.\n\n        Returns:\n            A MonsoonData object representing the data obtained in this\n            sampling. None if sampling is unsuccessful.\n        \"\"\"\n        sys.stdout.flush()\n        voltage = self.mon.GetVoltage()\n        self.log.info(\"Taking samples at %dhz for %ds, voltage %.2fv.\",\n                      sample_hz, (sample_num / sample_hz), voltage)\n        sample_num += sample_offset\n        # Make sure state is normal\n        self.mon.StopDataCollection()\n        status = self.mon.GetStatus()\n        native_hz = status[\"sampleRate\"] * 1000\n\n        # Collect and average samples as specified\n        self.mon.StartDataCollection()\n\n        # In case sample_hz doesn't divide native_hz exactly, use this\n        # invariant: 'offset' = (consumed samples) * sample_hz -\n        # (emitted samples) * native_hz\n        # This is the error accumulator in a variation of Bresenham's\n        # algorithm.\n        emitted = offset = 0\n        collected = []\n        # past n samples for rolling average\n        history_deque = collections.deque()\n        current_values = []\n        timestamps = []\n\n        try:\n            last_flush = time.time()\n            while emitted < sample_num or sample_num == -1:\n                # The number of raw samples to consume before emitting the next\n                # output\n                need = int((native_hz - offset + sample_hz - 1) / sample_hz)\n                if need > len(collected):  # still need more input samples\n                    samples = self.mon.CollectData()\n                    if not samples:\n                        break\n                    collected.extend(samples)\n                else:\n                    # Have enough data, generate output samples.\n                    # Adjust for consuming 'need' input samples.\n                    offset += need * sample_hz\n                    # maybe multiple, if sample_hz > native_hz\n                    while offset >= native_hz:\n                        # TODO(angli): Optimize \"collected\" operations.\n                        this_sample = sum(collected[:need]) / need\n                        this_time = int(time.time())\n                        timestamps.append(this_time)\n                        if live:\n                            self.log.info(\"%s %s\", this_time, this_sample)\n                        current_values.append(this_sample)\n                        sys.stdout.flush()\n                        offset -= native_hz\n                        emitted += 1  # adjust for emitting 1 output sample\n                    collected = collected[need:]\n                    now = time.time()\n                    if now - last_flush >= 0.99:  # flush every second\n                        sys.stdout.flush()\n                        last_flush = now\n        except Exception as e:\n            pass\n        self.mon.StopDataCollection()\n        try:\n            return MonsoonData(\n                current_values,\n                timestamps,\n                sample_hz,\n                voltage,\n                offset=sample_offset)\n        except:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the monsoon s USB passthrough mode.", "response": "def usb(self, state):\n        \"\"\"Sets the monsoon's USB passthrough mode. This is specific to the\n        USB port in front of the monsoon box which connects to the powered\n        device, NOT the USB that is used to talk to the monsoon itself.\n\n        \"Off\" means USB always off.\n        \"On\" means USB always on.\n        \"Auto\" means USB is automatically turned off when sampling is going on,\n        and turned back on when sampling finishes.\n\n        Args:\n            stats: The state to set the USB passthrough to.\n\n        Returns:\n            True if the state is legal and set. False otherwise.\n        \"\"\"\n        state_lookup = {\"off\": 0, \"on\": 1, \"auto\": 2}\n        state = state.lower()\n        if state in state_lookup:\n            current_state = self.mon.GetUsbPassthrough()\n            while (current_state != state_lookup[state]):\n                self.mon.SetUsbPassthrough(state_lookup[state])\n                time.sleep(1)\n                current_state = self.mon.GetUsbPassthrough()\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmeasuring power consumption of the attached USB device.", "response": "def measure_power(self, hz, duration, tag, offset=30):\n        \"\"\"Measure power consumption of the attached device.\n\n        Because it takes some time for the device to calm down after the usb\n        connection is cut, an offset is set for each measurement. The default\n        is 30s. The total time taken to measure will be (duration + offset).\n\n        Args:\n            hz: Number of samples to take per second.\n            duration: Number of seconds to take samples for in each step.\n            offset: The number of seconds of initial data to discard.\n            tag: A string that's the name of the collected data group.\n\n        Returns:\n            A MonsoonData object with the measured power data.\n        \"\"\"\n        num = duration * hz\n        oset = offset * hz\n        data = None\n        self.usb(\"auto\")\n        time.sleep(1)\n        with self.dut.handle_usb_disconnect():\n            time.sleep(1)\n            try:\n                data = self.take_samples(hz, num, sample_offset=oset)\n                if not data:\n                    raise MonsoonError(\n                        \"No data was collected in measurement %s.\" % tag)\n                data.tag = tag\n                self.dut.log.info(\"Measurement summary: %s\", repr(data))\n                return data\n            finally:\n                self.mon.StopDataCollection()\n                self.log.info(\"Finished taking samples, reconnecting to dut.\")\n                self.usb(\"on\")\n                self.dut.adb.wait_for_device(timeout=DEFAULT_TIMEOUT_USB_ON)\n                # Wait for device to come back online.\n                time.sleep(10)\n                self.dut.log.info(\"Dut reconnected.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize the sniffer structures based on the given JSON configuration.", "response": "def create(configs):\n    \"\"\"Initializes the sniffer structures based on the JSON configuration. The\n    expected keys are:\n\n        * Type: A first-level type of sniffer. Planned to be 'local' for\n            sniffers running on the local machine, or 'remote' for sniffers\n            running remotely.\n        * SubType: The specific sniffer type to be used.\n        * Interface: The WLAN interface used to configure the sniffer.\n        * BaseConfigs: A dictionary specifying baseline configurations of\n            the sniffer. Configurations can be overridden when starting a\n            capture. The keys must be one of the Sniffer.CONFIG_KEY_*\n            values.\n    \"\"\"\n    objs = []\n    for c in configs:\n        sniffer_type = c[\"Type\"]\n        sniffer_subtype = c[\"SubType\"]\n        interface = c[\"Interface\"]\n        base_configs = c[\"BaseConfigs\"]\n        module_name = \"mobly.controllers.sniffer_lib.{}.{}\".format(\n            sniffer_type, sniffer_subtype)\n        module = importlib.import_module(module_name)\n        objs.append(module.Sniffer(interface,\n                                   logging.getLogger(),\n                                   base_configs=base_configs))\n    return objs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the details field.", "response": "def _set_details(self, content):\n        \"\"\"Sets the `details` field.\n\n        Args:\n            content: the content to extract details from.\n        \"\"\"\n        try:\n            self.details = str(content)\n        except UnicodeEncodeError:\n            if sys.version_info < (3, 0):\n                # If Py2 threw encode error, convert to unicode.\n                self.details = unicode(content)\n            else:\n                # We should never hit this in Py3, if this happens, record\n                # an encoded version of the content for users to handle.\n                logging.error(\n                    'Unable to decode \"%s\" in Py3, encoding in utf-8.',\n                    content)\n                self.details = content.encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a test config file.", "response": "def _load_config_file(path):\n    \"\"\"Loads a test config file.\n\n    The test config file has to be in YAML format.\n\n    Args:\n        path: A string that is the full path to the config file, including the\n            file name.\n\n    Returns:\n        A dict that represents info in the config file.\n    \"\"\"\n    with io.open(utils.abs_path(path), 'r', encoding='utf-8') as f:\n        conf = yaml.load(f)\n        return conf"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a snippet client to the management.", "response": "def add_snippet_client(self, name, package):\n        \"\"\"Adds a snippet client to the management.\n\n        Args:\n            name: string, the attribute name to which to attach the snippet\n                client. E.g. `name='maps'` attaches the snippet client to\n                `ad.maps`.\n            package: string, the package name of the snippet apk to connect to.\n\n        Raises:\n            Error, if a duplicated name or package is passed in.\n        \"\"\"\n        # Should not load snippet with the same name more than once.\n        if name in self._snippet_clients:\n            raise Error(\n                self,\n                'Name \"%s\" is already registered with package \"%s\", it cannot '\n                'be used again.' %\n                (name, self._snippet_clients[name].client.package))\n        # Should not load the same snippet package more than once.\n        for snippet_name, client in self._snippet_clients.items():\n            if package == client.package:\n                raise Error(\n                    self,\n                    'Snippet package \"%s\" has already been loaded under name'\n                    ' \"%s\".' % (package, snippet_name))\n        client = snippet_client.SnippetClient(package=package, ad=self._device)\n        client.start_app_and_connect()\n        self._snippet_clients[name] = client"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_snippet_client(self, name):\n        if name not in self._snippet_clients:\n            raise Error(self._device, MISSING_SNIPPET_CLIENT_MSG % name)\n        client = self._snippet_clients.pop(name)\n        client.stop_app()", "response": "Removes a snippet client from management."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(self):\n        for client in self._snippet_clients.values():\n            if not client.is_alive:\n                self._device.log.debug('Starting SnippetClient<%s>.',\n                                       client.package)\n                client.start_app_and_connect()\n            else:\n                self._device.log.debug(\n                    'Not startng SnippetClient<%s> because it is already alive.',\n                    client.package)", "response": "Starts all the snippet clients under management."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop all the snippet clients under management.", "response": "def stop(self):\n        \"\"\"Stops all the snippet clients under management.\"\"\"\n        for client in self._snippet_clients.values():\n            if client.is_alive:\n                self._device.log.debug('Stopping SnippetClient<%s>.',\n                                       client.package)\n                client.stop_app()\n            else:\n                self._device.log.debug(\n                    'Not stopping SnippetClient<%s> because it is not alive.',\n                    client.package)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pause(self):\n        for client in self._snippet_clients.values():\n            self._device.log.debug(\n                'Clearing host port %d of SnippetClient<%s>.',\n                client.host_port, client.package)\n            client.clear_host_port()", "response": "Pauses all the snippet clients under management."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresumes all paused snippet clients.", "response": "def resume(self):\n        \"\"\"Resumes all paused snippet clients.\"\"\"\n        for client in self._snippet_clients.values():\n            # Resume is only applicable if a client is alive and does not have\n            # a host port.\n            if client.is_alive and client.host_port is None:\n                self._device.log.debug('Resuming SnippetClient<%s>.',\n                                       client.package)\n                client.restore_app_connection()\n            else:\n                self._device.log.debug('Not resuming SnippetClient<%s>.',\n                                       client.package)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef poll_events(self):\n        while self.started:\n            event_obj = None\n            event_name = None\n            try:\n                event_obj = self._sl4a.eventWait(50000)\n            except:\n                if self.started:\n                    print(\"Exception happened during polling.\")\n                    print(traceback.format_exc())\n                    raise\n            if not event_obj:\n                continue\n            elif 'name' not in event_obj:\n                print(\"Received Malformed event {}\".format(event_obj))\n                continue\n            else:\n                event_name = event_obj['name']\n            # if handler registered, process event\n            if event_name in self.handlers:\n                self.handle_subscribed_event(event_obj, event_name)\n            if event_name == \"EventDispatcherShutdown\":\n                self._sl4a.closeSl4aSession()\n                break\n            else:\n                self.lock.acquire()\n                if event_name in self.event_dict:  # otherwise, cache event\n                    self.event_dict[event_name].put(event_obj)\n                else:\n                    q = queue.Queue()\n                    q.put(event_obj)\n                    self.event_dict[event_name] = q\n                self.lock.release()", "response": "Continuously polls all types of events from sl4a and stores them in a separate queue."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a handler function for a specific event.", "response": "def register_handler(self, handler, event_name, args):\n        \"\"\"Registers an event handler.\n\n        One type of event can only have one event handler associated with it.\n\n        Args:\n            handler: The event handler function to be registered.\n            event_name: Name of the event the handler is for.\n            args: User arguments to be passed to the handler when it's called.\n\n        Raises:\n            IllegalStateError: Raised if attempts to register a handler after\n                the dispatcher starts running.\n            DuplicateError: Raised if attempts to register more than one\n                handler for one type of event.\n        \"\"\"\n        if self.started:\n            raise IllegalStateError((\"Can't register service after polling is\"\n                                     \" started\"))\n        self.lock.acquire()\n        try:\n            if event_name in self.handlers:\n                raise DuplicateError('A handler for {} already exists'.format(\n                    event_name))\n            self.handlers[event_name] = (handler, args)\n        finally:\n            self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the event dispatcher.", "response": "def start(self):\n        \"\"\"Starts the event dispatcher.\n\n        Initiates executor and start polling events.\n\n        Raises:\n            IllegalStateError: Can't start a dispatcher again when it's already\n                running.\n        \"\"\"\n        if not self.started:\n            self.started = True\n            self.executor = ThreadPoolExecutor(max_workers=32)\n            self.poller = self.executor.submit(self.poll_events)\n        else:\n            raise IllegalStateError(\"Dispatcher is already started.\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncleaning up and release resources after the event dispatcher loop has been broken.", "response": "def clean_up(self):\n        \"\"\"Clean up and release resources after the event dispatcher polling\n        loop has been broken.\n\n        The following things happen:\n        1. Clear all events and flags.\n        2. Close the sl4a client the event_dispatcher object holds.\n        3. Shut down executor without waiting.\n        \"\"\"\n        if not self.started:\n            return\n        self.started = False\n        self.clear_all_events()\n        # At this point, the sl4a apk is destroyed and nothing is listening on\n        # the socket. Avoid sending any sl4a commands; just clean up the socket\n        # and return.\n        self._sl4a.disconnect()\n        self.poller.set_result(\"Done\")\n        # The polling thread is guaranteed to finish after a max of 60 seconds,\n        # so we don't wait here.\n        self.executor.shutdown(wait=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pop_event(self, event_name, timeout=DEFAULT_TIMEOUT):\n        if not self.started:\n            raise IllegalStateError(\n                \"Dispatcher needs to be started before popping.\")\n\n        e_queue = self.get_event_q(event_name)\n\n        if not e_queue:\n            raise TypeError(\"Failed to get an event queue for {}\".format(\n                event_name))\n\n        try:\n            # Block for timeout\n            if timeout:\n                return e_queue.get(True, timeout)\n            # Non-blocking poll for event\n            elif timeout == 0:\n                return e_queue.get(False)\n            else:\n                # Block forever on event wait\n                return e_queue.get(True)\n        except queue.Empty:\n            raise queue.Empty('Timeout after {}s waiting for event: {}'.format(\n                timeout, event_name))", "response": "Pop an event from its queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting for an event that satisfies a predicate to appear.", "response": "def wait_for_event(self,\n                       event_name,\n                       predicate,\n                       timeout=DEFAULT_TIMEOUT,\n                       *args,\n                       **kwargs):\n        \"\"\"Wait for an event that satisfies a predicate to appear.\n\n        Continuously pop events of a particular name and check against the\n        predicate until an event that satisfies the predicate is popped or\n        timed out. Note this will remove all the events of the same name that\n        do not satisfy the predicate in the process.\n\n        Args:\n            event_name: Name of the event to be popped.\n            predicate: A function that takes an event and returns True if the\n                predicate is satisfied, False otherwise.\n            timeout: Number of seconds to wait.\n            *args: Optional positional args passed to predicate().\n            **kwargs: Optional keyword args passed to predicate().\n\n        Returns:\n            The event that satisfies the predicate.\n\n        Raises:\n            queue.Empty: Raised if no event that satisfies the predicate was\n                found before time out.\n        \"\"\"\n        deadline = time.time() + timeout\n\n        while True:\n            event = None\n            try:\n                event = self.pop_event(event_name, 1)\n            except queue.Empty:\n                pass\n\n            if event and predicate(event, *args, **kwargs):\n                return event\n\n            if time.time() > deadline:\n                raise queue.Empty(\n                    'Timeout after {}s waiting for event: {}'.format(\n                        timeout, event_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pop_events(self, regex_pattern, timeout):\n        if not self.started:\n            raise IllegalStateError(\n                \"Dispatcher needs to be started before popping.\")\n        deadline = time.time() + timeout\n        while True:\n            #TODO: fix the sleep loop\n            results = self._match_and_pop(regex_pattern)\n            if len(results) != 0 or time.time() > deadline:\n                break\n            time.sleep(1)\n        if len(results) == 0:\n            raise queue.Empty('Timeout after {}s waiting for event: {}'.format(\n                timeout, regex_pattern))\n\n        return sorted(results, key=lambda event: event['time'])", "response": "Pop events whose names match a regular expression pattern."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all the events whose names match regex_pattern.", "response": "def _match_and_pop(self, regex_pattern):\n        \"\"\"Pop one event from each of the event queues whose names\n        match (in a sense of regular expression) regex_pattern.\n        \"\"\"\n        results = []\n        self.lock.acquire()\n        for name in self.event_dict.keys():\n            if re.match(regex_pattern, name):\n                q = self.event_dict[name]\n                if q:\n                    try:\n                        results.append(q.get(False))\n                    except:\n                        pass\n        self.lock.release()\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nobtain the queue storing all the events of the specified name.", "response": "def get_event_q(self, event_name):\n        \"\"\"Obtain the queue storing events of the specified name.\n\n        If no event of this name has been polled, wait for one to.\n\n        Returns:\n            A queue storing all the events of the specified name.\n            None if timed out.\n\n        Raises:\n            queue.Empty: Raised if the queue does not exist and timeout has\n                passed.\n        \"\"\"\n        self.lock.acquire()\n        if not event_name in self.event_dict or self.event_dict[\n                event_name] is None:\n            self.event_dict[event_name] = queue.Queue()\n        self.lock.release()\n\n        event_queue = self.event_dict[event_name]\n        return event_queue"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_subscribed_event(self, event_obj, event_name):\n        handler, args = self.handlers[event_name]\n        self.executor.submit(handler, event_obj, *args)", "response": "Execute the registered handler of an event."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling an event from the cache.", "response": "def _handle(self, event_handler, event_name, user_args, event_timeout,\n                cond, cond_timeout):\n        \"\"\"Pop an event of specified type and calls its handler on it. If\n        condition is not None, block until condition is met or timeout.\n        \"\"\"\n        if cond:\n            cond.wait(cond_timeout)\n        event = self.pop_event(event_name, event_timeout)\n        return event_handler(event, *user_args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_event(self,\n                     event_handler,\n                     event_name,\n                     user_args,\n                     event_timeout=None,\n                     cond=None,\n                     cond_timeout=None):\n        \"\"\"Handle events that don't have registered handlers\n\n        In a new thread, poll one event of specified type from its queue and\n        execute its handler. If no such event exists, the thread waits until\n        one appears.\n\n        Args:\n            event_handler: Handler for the event, which should take at least\n                one argument - the event json object.\n            event_name: Name of the event to be handled.\n            user_args: User arguments for the handler; to be passed in after\n                the event json.\n            event_timeout: Number of seconds to wait for the event to come.\n            cond: A condition to wait on before executing the handler. Should\n                be a threading.Event object.\n            cond_timeout: Number of seconds to wait before the condition times\n                out. Never times out if None.\n\n        Returns:\n            A concurrent.Future object associated with the handler.\n            If blocking call worker.result() is triggered, the handler\n            needs to return something to unblock.\n        \"\"\"\n        worker = self.executor.submit(self._handle, event_handler, event_name,\n                                      user_args, event_timeout, cond,\n                                      cond_timeout)\n        return worker", "response": "Handles an event in a thread and executes its handler."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning and removes all stored events of a specified name.", "response": "def pop_all(self, event_name):\n        \"\"\"Return and remove all stored events of a specified name.\n\n        Pops all events from their queue. May miss the latest ones.\n        If no event is available, return immediately.\n\n        Args:\n            event_name: Name of the events to be popped.\n\n        Returns:\n            List of the desired events.\n\n        Raises:\n            IllegalStateError: Raised if pop is called before the dispatcher\n                starts polling.\n        \"\"\"\n        if not self.started:\n            raise IllegalStateError((\"Dispatcher needs to be started before \"\n                                     \"popping.\"))\n        results = []\n        try:\n            self.lock.acquire()\n            while True:\n                e = self.event_dict[event_name].get(block=False)\n                results.append(e)\n        except (queue.Empty, KeyError):\n            return results\n        finally:\n            self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear all events of a particular name.", "response": "def clear_events(self, event_name):\n        \"\"\"Clear all events of a particular name.\n\n        Args:\n            event_name: Name of the events to be popped.\n        \"\"\"\n        self.lock.acquire()\n        try:\n            q = self.get_event_q(event_name)\n            q.queue.clear()\n        except queue.Empty:\n            return\n        finally:\n            self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears all event queues and their cached events.", "response": "def clear_all_events(self):\n        \"\"\"Clear all event queues and their cached events.\"\"\"\n        self.lock.acquire()\n        self.event_dict.clear()\n        self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_dict(event_dict):\n    return SnippetEvent(\n        callback_id=event_dict['callbackId'],\n        name=event_dict['name'],\n        creation_time=event_dict['time'],\n        data=event_dict['data'])", "response": "Create a SnippetEvent object from a dictionary representing an event."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_dir(path):\n    full_path = abs_path(path)\n    if not os.path.exists(full_path):\n        try:\n            os.makedirs(full_path)\n        except OSError as e:\n            # ignore the error for dir already exist.\n            if e.errno != os.errno.EEXIST:\n                raise", "response": "Creates a directory if it does not exist already."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_alias(target_path, alias_path):\n    if platform.system() == 'Windows' and not alias_path.endswith('.lnk'):\n        alias_path += '.lnk'\n    if os.path.lexists(alias_path):\n        os.remove(alias_path)\n    if platform.system() == 'Windows':\n        from win32com import client\n        shell = client.Dispatch('WScript.Shell')\n        shortcut = shell.CreateShortCut(alias_path)\n        shortcut.Targetpath = target_path\n        shortcut.save()\n    else:\n        os.symlink(target_path, alias_path)", "response": "Creates an alias at alias_path pointing to the target_path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef epoch_to_human_time(epoch_time):\n    if isinstance(epoch_time, int):\n        try:\n            d = datetime.datetime.fromtimestamp(epoch_time / 1000)\n            return d.strftime(\"%m-%d-%Y %H:%M:%S \")\n        except ValueError:\n            return None", "response": "Converts an epoch timestamp to human readable time."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the Olson ID of the local timezone.", "response": "def get_timezone_olson_id():\n    \"\"\"Return the Olson ID of the local (non-DST) timezone.\n\n    Returns:\n        A string representing one of the Olson IDs of the local (non-DST)\n        timezone.\n    \"\"\"\n    tzoffset = int(time.timezone / 3600)\n    gmt = None\n    if tzoffset <= 0:\n        gmt = \"GMT+{}\".format(-tzoffset)\n    else:\n        gmt = \"GMT-{}\".format(tzoffset)\n    return GMT_to_olson[gmt]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlocate files in the specified directories and return a list of tuples where the names and extensions are desired.", "response": "def find_files(paths, file_predicate):\n    \"\"\"Locate files whose names and extensions match the given predicate in\n    the specified directories.\n\n    Args:\n        paths: A list of directory paths where to find the files.\n        file_predicate: A function that returns True if the file name and\n            extension are desired.\n\n    Returns:\n        A list of files that match the predicate.\n    \"\"\"\n    file_list = []\n    for path in paths:\n        p = abs_path(path)\n        for dirPath, _, fileList in os.walk(p):\n            for fname in fileList:\n                name, ext = os.path.splitext(fname)\n                if file_predicate(name, ext):\n                    file_list.append((dirPath, name, ext))\n    return file_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the content of a file into a base64 string.", "response": "def load_file_to_base64_str(f_path):\n    \"\"\"Loads the content of a file into a base64 string.\n\n    Args:\n        f_path: full path to the file including the file name.\n\n    Returns:\n        A base64 string representing the content of the file in utf-8 encoding.\n    \"\"\"\n    path = abs_path(f_path)\n    with io.open(path, 'rb') as f:\n        f_bytes = f.read()\n        base64_str = base64.b64encode(f_bytes).decode(\"utf-8\")\n        return base64_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the value of a field in a list that satisfies certain conditions.", "response": "def find_field(item_list, cond, comparator, target_field):\n    \"\"\"Finds the value of a field in a dict object that satisfies certain\n    conditions.\n\n    Args:\n        item_list: A list of dict objects.\n        cond: A param that defines the condition.\n        comparator: A function that checks if an dict satisfies the condition.\n        target_field: Name of the field whose value to be returned if an item\n            satisfies the condition.\n\n    Returns:\n        Target value or None if no item satisfies the condition.\n    \"\"\"\n    for item in item_list:\n        if comparator(item, cond) and target_field in item:\n            return item[target_field]\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a random string of specified length composed of ascii letters and digits.", "response": "def rand_ascii_str(length):\n    \"\"\"Generates a random string of specified length, composed of ascii letters\n    and digits.\n\n    Args:\n        length: The number of characters in the string.\n\n    Returns:\n        The random string generated.\n    \"\"\"\n    letters = [random.choice(ascii_letters_and_digits) for _ in range(length)]\n    return ''.join(letters)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concurrent_exec(func, param_list):\n    with concurrent.futures.ThreadPoolExecutor(max_workers=30) as executor:\n        # Start the load operations and mark each future with its params\n        future_to_params = {executor.submit(func, *p): p for p in param_list}\n        return_vals = []\n        for future in concurrent.futures.as_completed(future_to_params):\n            params = future_to_params[future]\n            try:\n                return_vals.append(future.result())\n            except Exception as exc:\n                logging.exception(\"{} generated an exception: {}\".format(\n                    params, traceback.format_exc()))\n                return_vals.append(exc)\n        return return_vals", "response": "Executes a function with different parameters pseudo - concurrently."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a command in a subprocess.", "response": "def run_command(cmd,\n                stdout=None,\n                stderr=None,\n                shell=False,\n                timeout=None,\n                cwd=None,\n                env=None):\n    \"\"\"Runs a command in a subprocess.\n\n    This function is very similar to subprocess.check_output. The main\n    difference is that it returns the return code and std error output as well\n    as supporting a timeout parameter.\n\n    Args:\n        cmd: string or list of strings, the command to run.\n            See subprocess.Popen() documentation.\n        stdout: file handle, the file handle to write std out to. If None is\n            given, then subprocess.PIPE is used. See subprocess.Popen()\n            documentation.\n        stdee: file handle, the file handle to write std err to. If None is\n            given, then subprocess.PIPE is used. See subprocess.Popen()\n            documentation.\n        shell: bool, True to run this command through the system shell,\n            False to invoke it directly. See subprocess.Popen() docs.\n        timeout: float, the number of seconds to wait before timing out.\n            If not specified, no timeout takes effect.\n        cwd: string, the path to change the child's current directory to before\n            it is executed. Note that this directory is not considered when\n            searching the executable, so you can't specify the program's path\n            relative to cwd.\n        env: dict, a mapping that defines the environment variables for the\n            new process. Default behavior is inheriting the current process'\n            environment.\n\n    Returns:\n        A 3-tuple of the consisting of the return code, the std output, and the\n            std error.\n\n    Raises:\n        psutil.TimeoutExpired: The command timed out.\n    \"\"\"\n    # Only import psutil when actually needed.\n    # psutil may cause import error in certain env. This way the utils module\n    # doesn't crash upon import.\n    import psutil\n    if stdout is None:\n        stdout = subprocess.PIPE\n    if stderr is None:\n        stderr = subprocess.PIPE\n    process = psutil.Popen(\n        cmd, stdout=stdout, stderr=stderr, shell=shell, cwd=cwd, env=env)\n    timer = None\n    timer_triggered = threading.Event()\n    if timeout and timeout > 0:\n        # The wait method on process will hang when used with PIPEs with large\n        # outputs, so use a timer thread instead.\n\n        def timeout_expired():\n            timer_triggered.set()\n            process.terminate()\n\n        timer = threading.Timer(timeout, timeout_expired)\n        timer.start()\n    # If the command takes longer than the timeout, then the timer thread\n    # will kill the subprocess, which will make it terminate.\n    (out, err) = process.communicate()\n    if timer is not None:\n        timer.cancel()\n    if timer_triggered.is_set():\n        raise psutil.TimeoutExpired(timeout, pid=process.pid)\n    return (process.returncode, out, err)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a long - running subprocess with the given command.", "response": "def start_standing_subprocess(cmd, shell=False, env=None):\n    \"\"\"Starts a long-running subprocess.\n\n    This is not a blocking call and the subprocess started by it should be\n    explicitly terminated with stop_standing_subprocess.\n\n    For short-running commands, you should use subprocess.check_call, which\n    blocks.\n\n    Args:\n        cmd: string, the command to start the subprocess with.\n        shell: bool, True to run this command through the system shell,\n            False to invoke it directly. See subprocess.Proc() docs.\n        env: dict, a custom environment to run the standing subprocess. If not\n            specified, inherits the current environment. See subprocess.Popen()\n            docs.\n\n    Returns:\n        The subprocess that was started.\n    \"\"\"\n    logging.debug('Starting standing subprocess with: %s', cmd)\n    proc = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        shell=shell,\n        env=env)\n    # Leaving stdin open causes problems for input, e.g. breaking the\n    # code.inspect() shell (http://stackoverflow.com/a/25512460/1612937), so\n    # explicitly close it assuming it is not needed for standing subprocesses.\n    proc.stdin.close()\n    proc.stdin = None\n    logging.debug('Started standing subprocess %d', proc.pid)\n    return proc"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstops a subprocess started by start_standing_subprocess.", "response": "def stop_standing_subprocess(proc):\n    \"\"\"Stops a subprocess started by start_standing_subprocess.\n\n    Before killing the process, we check if the process is running, if it has\n    terminated, Error is raised.\n\n    Catches and ignores the PermissionError which only happens on Macs.\n\n    Args:\n        proc: Subprocess to terminate.\n\n    Raises:\n        Error: if the subprocess could not be stopped.\n    \"\"\"\n    # Only import psutil when actually needed.\n    # psutil may cause import error in certain env. This way the utils module\n    # doesn't crash upon import.\n    import psutil\n    pid = proc.pid\n    logging.debug('Stopping standing subprocess %d', pid)\n    process = psutil.Process(pid)\n    failed = []\n    try:\n        children = process.children(recursive=True)\n    except AttributeError:\n        # Handle versions <3.0.0 of psutil.\n        children = process.get_children(recursive=True)\n    for child in children:\n        try:\n            child.kill()\n            child.wait(timeout=10)\n        except psutil.NoSuchProcess:\n            # Ignore if the child process has already terminated.\n            pass\n        except:\n            failed.append(child.pid)\n            logging.exception('Failed to kill standing subprocess %d',\n                              child.pid)\n    try:\n        process.kill()\n        process.wait(timeout=10)\n    except psutil.NoSuchProcess:\n        # Ignore if the process has already terminated.\n        pass\n    except:\n        failed.append(pid)\n        logging.exception('Failed to kill standing subprocess %d', pid)\n    if failed:\n        raise Error('Failed to kill standing subprocesses: %s' % failed)\n    # Call wait and close pipes on the original Python object so we don't get\n    # runtime warnings.\n    if proc.stdout:\n        proc.stdout.close()\n    if proc.stderr:\n        proc.stderr.close()\n    proc.wait()\n    logging.debug('Stopped standing subprocess %d', pid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a host port number on the host available for adb forward.", "response": "def get_available_host_port():\n    \"\"\"Gets a host port number available for adb forward.\n\n    Returns:\n        An integer representing a port number on the host available for adb\n        forward.\n\n    Raises:\n        Error: when no port is found after MAX_PORT_ALLOCATION_RETRY times.\n    \"\"\"\n    # Only import adb module if needed.\n    from mobly.controllers.android_device_lib import adb\n    for _ in range(MAX_PORT_ALLOCATION_RETRY):\n        port = portpicker.PickUnusedPort()\n        # Make sure adb is not using this port so we don't accidentally\n        # interrupt ongoing runs by trying to bind to the port.\n        if port not in adb.list_occupied_adb_ports():\n            return port\n    raise Error('Failed to find available port after {} retries'.format(\n        MAX_PORT_ALLOCATION_RETRY))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grep(regex, output):\n    lines = output.decode('utf-8').strip().splitlines()\n    results = []\n    for line in lines:\n        if re.search(regex, line):\n            results.append(line.strip())\n    return results", "response": "Similar to linux s grep this returns the line in an output stream that matches a given regex pattern."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli_cmd_to_string(args):\n    if isinstance(args, basestring):\n        # Return directly if it's already a string.\n        return args\n    return ' '.join([pipes.quote(arg) for arg in args])", "response": "Converts a cmd arg list to string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exe_cmd(*cmds):\n    cmd = ' '.join(cmds)\n    proc = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=True)\n    (out, err) = proc.communicate()\n    if not err:\n        return out\n    return err", "response": "Executes a sequence of commands in a new shell."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nverifies that a module object follows the required controller interface for Mobly controller objects.", "response": "def verify_controller_module(module):\n    \"\"\"Verifies a module object follows the required interface for\n    controllers.\n\n    The interface is explained in the docstring of\n    `base_test.BaseTestClass.register_controller`.\n\n    Args:\n        module: An object that is a controller module. This is usually\n            imported with import statements or loaded by importlib.\n\n    Raises:\n        ControllerError: if the module does not match the Mobly controller\n            interface, or one of the required members is null.\n    \"\"\"\n    required_attributes = ('create', 'destroy', 'MOBLY_CONTROLLER_CONFIG_NAME')\n    for attr in required_attributes:\n        if not hasattr(module, attr):\n            raise signals.ControllerError(\n                'Module %s missing required controller module attribute'\n                ' %s.' % (module.__name__, attr))\n        if not getattr(module, attr):\n            raise signals.ControllerError(\n                'Controller interface %s in %s cannot be null.' %\n                (attr, module.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register_controller(self, module, required=True, min_number=1):\n        verify_controller_module(module)\n        # Use the module's name as the ref name\n        module_ref_name = module.__name__.split('.')[-1]\n        if module_ref_name in self._controller_objects:\n            raise signals.ControllerError(\n                'Controller module %s has already been registered. It cannot '\n                'be registered again.' % module_ref_name)\n        # Create controller objects.\n        module_config_name = module.MOBLY_CONTROLLER_CONFIG_NAME\n        if module_config_name not in self.controller_configs:\n            if required:\n                raise signals.ControllerError(\n                    'No corresponding config found for %s' %\n                    module_config_name)\n            logging.warning(\n                'No corresponding config found for optional controller %s',\n                module_config_name)\n            return None\n        try:\n            # Make a deep copy of the config to pass to the controller module,\n            # in case the controller module modifies the config internally.\n            original_config = self.controller_configs[module_config_name]\n            controller_config = copy.deepcopy(original_config)\n            objects = module.create(controller_config)\n        except:\n            logging.exception(\n                'Failed to initialize objects for controller %s, abort!',\n                module_config_name)\n            raise\n        if not isinstance(objects, list):\n            raise signals.ControllerError(\n                'Controller module %s did not return a list of objects, abort.'\n                % module_ref_name)\n        # Check we got enough controller objects to continue.\n        actual_number = len(objects)\n        if actual_number < min_number:\n            module.destroy(objects)\n            raise signals.ControllerError(\n                'Expected to get at least %d controller objects, got %d.' %\n                (min_number, actual_number))\n        # Save a shallow copy of the list for internal usage, so tests can't\n        # affect internal registry by manipulating the object list.\n        self._controller_objects[module_ref_name] = copy.copy(objects)\n        logging.debug('Found %d objects for controller %s', len(objects),\n                      module_config_name)\n        self._controller_modules[module_ref_name] = module\n        return objects", "response": "Loads a controller module and returns its loaded devices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndestroy all controllers and clear internal registry.", "response": "def unregister_controllers(self):\n        \"\"\"Destroy controller objects and clear internal registry.\n\n        This will be called after each test class.\n        \"\"\"\n        # TODO(xpconanfan): actually record these errors instead of just\n        # logging them.\n        for name, module in self._controller_modules.items():\n            logging.debug('Destroying %s.', name)\n            with expects.expect_no_raises(\n                    'Exception occurred destroying %s.' % name):\n                module.destroy(self._controller_objects[name])\n        self._controller_objects = collections.OrderedDict()\n        self._controller_modules = {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new controller info record for a particular controller type.", "response": "def _create_controller_info_record(self, controller_module_name):\n        \"\"\"Creates controller info record for a particular controller type.\n\n        Info is retrieved from all the controller objects spawned from the\n        specified module, using the controller module's `get_info` function.\n\n        Args:\n            controller_module_name: string, the name of the controller module\n                to retrieve info from.\n\n        Returns:\n            A records.ControllerInfoRecord object.\n        \"\"\"\n        module = self._controller_modules[controller_module_name]\n        controller_info = None\n        try:\n            controller_info = module.get_info(\n                copy.copy(self._controller_objects[controller_module_name]))\n        except AttributeError:\n            logging.warning('No optional debug info found for controller '\n                            '%s. To provide it, implement `get_info`.',\n                            controller_module_name)\n        try:\n            yaml.dump(controller_info)\n        except TypeError:\n            logging.warning('The info of controller %s in class \"%s\" is not '\n                            'YAML serializable! Coercing it to string.',\n                            controller_module_name, self._class_name)\n            controller_info = str(controller_info)\n        return records.ControllerInfoRecord(\n            self._class_name, module.MOBLY_CONTROLLER_CONFIG_NAME,\n            controller_info)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the info records for all the controller objects in the manager.", "response": "def get_controller_info_records(self):\n        \"\"\"Get the info records for all the controller objects in the manager.\n\n        New info records for each controller object are created for every call\n        so the latest info is included.\n\n        Returns:\n            List of records.ControllerInfoRecord objects. Each opject conatins\n            the info of a type of controller\n        \"\"\"\n        info_records = []\n        for controller_module_name in self._controller_objects.keys():\n            with expects.expect_no_raises(\n                    'Failed to collect controller info from %s' %\n                    controller_module_name):\n                record = self._create_controller_info_record(\n                    controller_module_name)\n                if record:\n                    info_records.append(record)\n        return info_records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_to_op(d, index_name, doc_type, op_type='index'):\n    if d is None:\n        return d\n\n    op_types = ('create', 'delete', 'index', 'update')\n    if op_type not in op_types:\n        msg = 'Unknown operation type \"{}\", must be one of: {}'\n        raise Exception(msg.format(op_type, ', '.join(op_types)))\n\n    if 'id' not in d:\n        raise Exception('\"id\" key not found')\n\n    operation = {\n        '_op_type': op_type,\n        '_index': index_name,\n        '_type': doc_type,\n        '_id': d.pop('id'),\n    }\n    operation.update(d)\n\n    return operation", "response": "Create a bulk - indexing operation from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_dict(obj):\n    if not isinstance(obj.test, str):\n        # TODO: can we handle this in the DB?\n        # Reftests used to use tuple indicies, which we can't support.\n        # This is fixed upstream, but we also need to handle it here to allow\n        # for older branches.\n        return\n\n    keys = [\n        'id',\n        'job_guid',\n        'test',\n        'subtest',\n        'status',\n        'expected',\n        'message',\n        'best_classification',\n        'best_is_verified',\n    ]\n\n    all_fields = obj.to_dict()\n    return {k: v for k, v in all_fields.items() if k in keys}", "response": "Create a filtered dict from the given object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a list of jobs to remove and list of revisions that are not in the same treeherder.", "response": "def build_removals(active_jobs, failures, target):\n    \"\"\"\n    active_jobs - all possible desktop & android jobs on Treeherder (no PGO)\n    failures - list of all failures\n    target - percentage of failures we're going to process\n\n    Return list of jobs to remove and list of revisions that are regressed\n    \"\"\"\n    # Determine the number of failures we're going to process\n    # A failure is a revision + all of the jobs that were fixed by it\n    number_of_failures = int((target / 100) * len(failures))\n    low_value_jobs = []\n\n    for jobtype in active_jobs:\n        # Determine if removing an active job will reduce the number of failures we would catch\n        # or stay the same\n        remaining_failures = check_removal(failures, [jobtype])\n\n        if len(remaining_failures) >= number_of_failures:\n            low_value_jobs.append(jobtype)\n            failures = remaining_failures\n        else:\n            failed_revisions = []\n            for revision in failures:\n                if revision not in remaining_failures:\n                    failed_revisions.append(revision)\n\n            logger.info(\"jobtype: %s is the root failure(s) of these %s revisions\",\n                        jobtype, failed_revisions)\n\n    return low_value_jobs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_high_value_jobs(fixed_by_commit_jobs, target=100):\n    total = len(fixed_by_commit_jobs)\n    logger.info(\"Processing %s revision(s)\", total)\n    active_jobs = job_priorities_to_jobtypes()\n\n    low_value_jobs = build_removals(\n        active_jobs=active_jobs,\n        failures=fixed_by_commit_jobs,\n        target=target)\n\n    # Only return high value jobs\n    for low_value_job in low_value_jobs:\n        try:\n            active_jobs.remove(low_value_job)\n        except ValueError:\n            logger.warning(\"%s is missing from the job list\", low_value_job)\n\n    total = len(fixed_by_commit_jobs)\n    total_detected = check_removal(fixed_by_commit_jobs, low_value_jobs)\n    percent_detected = 100 * len(total_detected) / total\n    logger.info(\"We will detect %.2f%% (%s) of the %s failures\",\n                percent_detected, len(total_detected), total)\n\n    return active_jobs", "response": "Get the high value jobs for a given set of jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a single line of the log into a set of Taskcluster log entries.", "response": "def parse_line(self, line, lineno):\n        \"\"\"Parse a single line of the log.\n\n        We have to handle both buildbot style logs as well as Taskcluster logs. The latter\n        attempt to emulate the buildbot logs, but don't accurately do so, partly due\n        to the way logs are generated in Taskcluster (ie: on the workers themselves).\n\n        Buildbot logs:\n\n            builder: ...\n            slave: ...\n            starttime: ...\n            results: ...\n            buildid: ...\n            builduid: ...\n            revision: ...\n\n            ======= <step START marker> =======\n            <step log output>\n            ======= <step FINISH marker> =======\n\n            ======= <step START marker> =======\n            <step log output>\n            ======= <step FINISH marker> =======\n\n        Taskcluster logs (a worst-case example):\n\n            <log output outside a step>\n            ======= <step START marker> =======\n            <step log output>\n            ======= <step FINISH marker> =======\n            <log output outside a step>\n            ======= <step START marker> =======\n            <step log output with no following finish marker>\n\n        As can be seen above, Taskcluster logs can have (a) log output that falls between\n        step markers, and (b) content at the end of the log, that is not followed by a\n        final finish step marker. We handle this by creating generic placeholder steps to\n        hold the log output that is not enclosed by step markers, and then by cleaning up\n        the final step in finish_parse() once all lines have been parsed.\n        \"\"\"\n        if not line.strip():\n            # Skip whitespace-only lines, since they will never contain an error line,\n            # so are not of interest. This also avoids creating spurious unnamed steps\n            # (which occurs when we find content outside of step markers) for the\n            # newlines that separate the steps in Buildbot logs.\n            return\n\n        if self.state == self.STATES['awaiting_first_step'] and self.RE_HEADER_LINE.match(line):\n            # The \"key: value\" job metadata header lines that appear at the top of\n            # Buildbot logs would result in the creation of an unnamed step at the\n            # start of the job, unless we skip them. (Which is not desired, since\n            # the lines are metadata and not test/build output.)\n            return\n\n        step_marker_match = self.RE_STEP_MARKER.match(line)\n\n        if not step_marker_match:\n            # This is a normal log line, rather than a step marker. (The common case.)\n            if self.state != self.STATES['step_in_progress']:\n                # We don't have an in-progress step, so need to start one, even though this\n                # isn't a \"step started\" marker line. We therefore create a new generic step,\n                # since we have no way of finding out the step metadata. This case occurs\n                # for the Taskcluster logs where content can fall between step markers.\n                self.start_step(lineno)\n            # Parse the line for errors, which if found, will be associated with the current step.\n            self.sub_parser.parse_line(line, lineno)\n            return\n\n        # This is either a \"step started\" or \"step finished\" marker line, eg:\n        # ========= Started foo (results: 0, elapsed: 0 secs) (at 2015-08-17 02:33:56.353866) =========\n        # ========= Finished foo (results: 0, elapsed: 0 secs) (at 2015-08-17 02:33:56.354301) =========\n\n        if step_marker_match.group('marker_type') == 'Started':\n            if self.state == self.STATES['step_in_progress']:\n                # We're partway through a step (ie: haven't seen a \"step finished\" marker line),\n                # but have now reached the \"step started\" marker for the next step. Before we\n                # can start the new step, we have to clean up the previous one - albeit using\n                # generic step metadata, since there was no \"step finished\" marker. This occurs\n                # in Taskcluster's logs when content falls between the step marker lines.\n                self.end_step(lineno)\n            # Start a new step using the extracted step metadata.\n            self.start_step(lineno,\n                            name=step_marker_match.group('name'),\n                            timestamp=step_marker_match.group('timestamp'))\n            return\n\n        # This is a \"step finished\" marker line.\n\n        if self.state != self.STATES['step_in_progress']:\n            # We're not in the middle of a step, so can't finish one. Just ignore the marker line.\n            return\n\n        # Close out the current step using the extracted step metadata.\n        self.end_step(lineno,\n                      timestamp=step_marker_match.group('timestamp'),\n                      result_code=int(step_marker_match.group('result_code')))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new step and update the state to reflect we re now in the middle of a step.", "response": "def start_step(self, lineno, name=\"Unnamed step\", timestamp=None):\n        \"\"\"Create a new step and update the state to reflect we're now in the middle of a step.\"\"\"\n        self.state = self.STATES['step_in_progress']\n        self.stepnum += 1\n        self.steps.append({\n            \"name\": name,\n            \"started\": timestamp,\n            \"started_linenumber\": lineno,\n            \"errors\": [],\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef end_step(self, lineno, timestamp=None, result_code=None):\n        self.state = self.STATES['step_finished']\n        step_errors = self.sub_parser.get_artifact()\n        step_error_count = len(step_errors)\n        if step_error_count > settings.PARSER_MAX_STEP_ERROR_LINES:\n            step_errors = step_errors[:settings.PARSER_MAX_STEP_ERROR_LINES]\n            self.artifact[\"errors_truncated\"] = True\n        self.current_step.update({\n            \"finished\": timestamp,\n            \"finished_linenumber\": lineno,\n            # Whilst the result code is present on both the start and end buildbot-style step\n            # markers, for Taskcluster logs the start marker line lies about the result, since\n            # the log output is unbuffered, so Taskcluster does not know the real result at\n            # that point. As such, we only set the result when ending a step.\n            \"result\": self.RESULT_DICT.get(result_code, \"unknown\"),\n            \"errors\": step_errors\n        })\n        # reset the sub_parser for the next step\n        self.sub_parser.clear()", "response": "Fill in the current step s summary and update the state to show the current step has ended."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef finish_parse(self, last_lineno_seen):\n        if self.state == self.STATES['step_in_progress']:\n            # We've reached the end of the log without seeing the final \"step finish\"\n            # marker, which would normally have triggered updating the step. As such we\n            # must manually close out the current step, so things like result, finish\n            # time are set for it. This ensures that the error summary for Taskcluster\n            # infra failures actually lists the error that occurs at the\n            # end of the log.\n            self.end_step(last_lineno_seen)", "response": "Clean - up and summary tasks run at the end of parsing."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a single line of the log into a dictionary of artifact data.", "response": "def parse_line(self, line, lineno):\n        \"\"\"Parse a single line of the log\"\"\"\n        match = self.RE_TINDERBOXPRINT.match(line) if line else None\n        if match:\n            line = match.group('line')\n\n            for regexp_item in self.TINDERBOX_REGEXP_TUPLE:\n                match = regexp_item['re'].match(line)\n                if match:\n                    artifact = match.groupdict()\n                    # handle duplicate fields\n                    for to_field, from_field in regexp_item['duplicates_fields'].items():\n                        # if to_field not present or None copy form from_field\n                        if to_field not in artifact or artifact[to_field] is None:\n                            artifact[to_field] = artifact[from_field]\n                    artifact.update(regexp_item['base_dict'])\n                    self.artifact.append(artifact)\n                    return\n\n            # default case: consider it html content\n            # try to detect title/value splitting on <br/>\n            artifact = {\"content_type\": \"raw_html\", }\n            if \"<br/>\" in line:\n                title, value = line.split(\"<br/>\", 1)\n                artifact[\"title\"] = title\n                artifact[\"value\"] = value\n            # or similar long lines if they contain a url\n            elif \"href\" in line and \"title\" in line:\n                def parse_url_line(line_data):\n                    class TpLineParser(HTMLParser):\n\n                        def handle_starttag(self, tag, attrs):\n                            d = dict(attrs)\n                            artifact[\"url\"] = d['href']\n                            artifact[\"title\"] = d['title']\n\n                        def handle_data(self, data):\n                            artifact[\"value\"] = data\n\n                    p = TpLineParser()\n                    p.feed(line_data)\n                    p.close()\n\n                # strip ^M returns on windows lines otherwise\n                # handle_data will yield no data 'value'\n                parse_url_line(line.replace('\\r', ''))\n            else:\n                artifact[\"value\"] = line\n            self.artifact.append(artifact)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a single line of TaskCluster output into a set of attributes.", "response": "def parse_line(self, line, lineno):\n        \"\"\"Check a single line for an error.  Keeps track of the linenumber\"\"\"\n        # TaskCluster logs are a bit wonky.\n        #\n        # TaskCluster logs begin with output coming from TaskCluster itself,\n        # before it has transitioned control of the task to the configured\n        # process. These \"internal\" logs look like the following:\n        #\n        #   [taskcluster 2016-09-09 17:41:43.544Z] Worker Group: us-west-2b\n        #\n        # If an error occurs during this \"setup\" phase, TaskCluster may emit\n        # lines beginning with ``[taskcluster:error]``.\n        #\n        # Once control has transitioned from TaskCluster to the configured\n        # task process, lines can be whatever the configured process emits.\n        # The popular ``run-task`` wrapper prefixes output to emulate\n        # TaskCluster's \"internal\" logs. e.g.\n        #\n        #   [vcs 2016-09-09T17:45:02.842230Z] adding changesets\n        #\n        # This prefixing can confuse error parsing. So, we strip it.\n        #\n        # Because regular expression matching and string manipulation can be\n        # expensive when performed on every line, we only strip the TaskCluster\n        # log prefix if we know we're in a TaskCluster log.\n\n        # First line of TaskCluster logs almost certainly has this.\n        if line.startswith('[taskcluster '):\n            self.is_taskcluster = True\n\n        # For performance reasons, only do this if we have identified as\n        # a TC task.\n        if self.is_taskcluster:\n            line = re.sub(self.RE_TASKCLUSTER_NORMAL_PREFIX, \"\", line)\n\n        if self.is_error_line(line):\n            self.add(line, lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef retrieve(self, request, project, pk=None):\n        log = JobLog.objects.get(id=pk)\n        return Response(self._log_as_dict(log))", "response": "Returns a job_log_url object given its ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets method implementation for list view", "response": "def list(self, request, project):\n        \"\"\"\n        GET method implementation for list view\n        job_id -- Mandatory filter indicating which job these log belongs to.\n        \"\"\"\n        job_ids = request.query_params.getlist('job_id')\n        if not job_ids:\n            raise ParseError(\n                detail=\"The job_id parameter is mandatory for this endpoint\")\n        try:\n            job_ids = [int(job_id) for job_id in job_ids]\n        except ValueError:\n            raise ParseError(detail=\"The job_id parameter(s) must be integers\")\n\n        logs = JobLog.objects.filter(job__repository__name=project,\n                                     job_id__in=job_ids)\n\n        return Response([self._log_as_dict(log) for log in logs])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef analyze(revision_data, weight_fn=None):\n    if weight_fn is None:\n        weight_fn = default_weights\n\n    # get a weighted average for the full set of data -- this is complicated\n    # by the fact that we might have multiple data points from each revision\n    # which we would want to weight equally -- do this by creating a set of\n    # weights only for each bucket containing (potentially) multiple results\n    # for each value\n    num_revisions = len(revision_data)\n    weights = [weight_fn(i, num_revisions) for i in range(num_revisions)]\n    weighted_sum = 0\n    sum_of_weights = 0\n    for i in range(num_revisions):\n        weighted_sum += sum(value * weights[i] for value in\n                            revision_data[i].values)\n        sum_of_weights += weights[i] * len(revision_data[i].values)\n    weighted_avg = weighted_sum / sum_of_weights if num_revisions > 0 else 0.0\n\n    # now that we have a weighted average, we can calculate the variance of the\n    # whole series\n    all_data = [v for datum in revision_data for v in datum.values]\n    variance = (sum(pow(d-weighted_avg, 2) for d in all_data) / (len(all_data)-1)) if len(all_data) > 1 else 0.0\n\n    return {\"avg\": weighted_avg, \"n\": len(all_data), \"variance\": variance}", "response": "Returns the average and sample variance of a list of floats."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linear_weights(i, n):\n    if i >= n:\n        return 0.0\n    return float(n - i) / float(n)", "response": "A window function that falls off arithmetically."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform a Students t - test on the two sets of revision data.", "response": "def calc_t(w1, w2, weight_fn=None):\n    \"\"\"Perform a Students t-test on the two sets of revision data.\n\n    See the analyze() function for a description of the `weight_fn` argument.\n    \"\"\"\n    if not w1 or not w2:\n        return 0\n\n    s1 = analyze(w1, weight_fn)\n    s2 = analyze(w2, weight_fn)\n    delta_s = s2['avg'] - s1['avg']\n\n    if delta_s == 0:\n        return 0\n    if s1['variance'] == 0 and s2['variance'] == 0:\n        return float('inf')\n\n    return delta_s / (((s1['variance'] / s1['n']) + (s2['variance'] / s2['n'])) ** 0.5)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_job(repository, job_datum, push_id):\n    build_platform, _ = BuildPlatform.objects.get_or_create(\n        os_name=job_datum.get('build_platform', {}).get('os_name', 'unknown'),\n        platform=job_datum.get('build_platform', {}).get('platform', 'unknown'),\n        architecture=job_datum.get('build_platform', {}).get('architecture',\n                                                             'unknown'))\n\n    machine_platform, _ = MachinePlatform.objects.get_or_create(\n        os_name=job_datum.get('machine_platform', {}).get('os_name', 'unknown'),\n        platform=job_datum.get('machine_platform', {}).get('platform', 'unknown'),\n        architecture=job_datum.get('machine_platform', {}).get('architecture',\n                                                               'unknown'))\n\n    option_names = job_datum.get('option_collection', [])\n    option_collection_hash = OptionCollection.calculate_hash(\n        option_names)\n    if not OptionCollection.objects.filter(\n            option_collection_hash=option_collection_hash).exists():\n        # in the unlikely event that we haven't seen this set of options\n        # before, add the appropriate database rows\n        options = []\n        for option_name in option_names:\n            option, _ = Option.objects.get_or_create(name=option_name)\n            options.append(option)\n        for option in options:\n            OptionCollection.objects.create(\n                option_collection_hash=option_collection_hash,\n                option=option)\n\n    machine, _ = Machine.objects.get_or_create(\n        name=job_datum.get('machine', 'unknown'))\n\n    job_type, _ = JobType.objects.get_or_create(\n        symbol=job_datum.get('job_symbol') or 'unknown',\n        name=job_datum.get('name') or 'unknown')\n\n    job_group, _ = JobGroup.objects.get_or_create(\n        name=job_datum.get('group_name') or 'unknown',\n        symbol=job_datum.get('group_symbol') or 'unknown')\n\n    product_name = job_datum.get('product_name', 'unknown')\n    if not product_name.strip():\n        product_name = 'unknown'\n    product, _ = Product.objects.get_or_create(name=product_name)\n\n    job_guid = job_datum['job_guid']\n    job_guid = job_guid[0:50]\n\n    who = job_datum.get('who') or 'unknown'\n    who = who[0:50]\n\n    reason = job_datum.get('reason') or 'unknown'\n    reason = reason[0:125]\n\n    state = job_datum.get('state') or 'unknown'\n    state = state[0:25]\n\n    build_system_type = job_datum.get('build_system_type', 'buildbot')\n\n    reference_data_name = job_datum.get('reference_data_name', None)\n\n    default_failure_classification = FailureClassification.objects.get(\n        name='not classified')\n\n    sh = sha1()\n    sh.update(''.join(\n        map(str,\n            [build_system_type, repository.name, build_platform.os_name,\n             build_platform.platform, build_platform.architecture,\n             machine_platform.os_name, machine_platform.platform,\n             machine_platform.architecture,\n             job_group.name, job_group.symbol, job_type.name,\n             job_type.symbol, option_collection_hash,\n             reference_data_name])).encode('utf-8'))\n    signature_hash = sh.hexdigest()\n\n    # Should be the buildername in the case of buildbot (if not provided\n    # default to using the signature hash)\n    if not reference_data_name:\n        reference_data_name = signature_hash\n\n    signature, _ = ReferenceDataSignatures.objects.get_or_create(\n        name=reference_data_name,\n        signature=signature_hash,\n        build_system_type=build_system_type,\n        repository=repository.name, defaults={\n            'first_submission_timestamp': time.time(),\n            'build_os_name': build_platform.os_name,\n            'build_platform': build_platform.platform,\n            'build_architecture': build_platform.architecture,\n            'machine_os_name': machine_platform.os_name,\n            'machine_platform': machine_platform.platform,\n            'machine_architecture': machine_platform.architecture,\n            'job_group_name': job_group.name,\n            'job_group_symbol': job_group.symbol,\n            'job_type_name': job_type.name,\n            'job_type_symbol': job_type.symbol,\n            'option_collection_hash': option_collection_hash\n        })\n\n    tier = job_datum.get('tier') or 1\n\n    result = job_datum.get('result', 'unknown')\n\n    submit_time = datetime.fromtimestamp(\n        _get_number(job_datum.get('submit_timestamp')))\n    start_time = datetime.fromtimestamp(\n        _get_number(job_datum.get('start_timestamp')))\n    end_time = datetime.fromtimestamp(\n        _get_number(job_datum.get('end_timestamp')))\n\n    # first, try to create the job with the given guid (if it doesn't\n    # exist yet)\n    job_guid_root = get_guid_root(job_guid)\n    if not Job.objects.filter(guid__in=[job_guid, job_guid_root]).exists():\n        # This could theoretically already have been created by another process\n        # that is running updates simultaneously.  So just attempt to create\n        # it, but allow it to skip if it's the same guid.  The odds are\n        # extremely high that this is a pending and running job that came in\n        # quick succession and are being processed by two different workers.\n        Job.objects.get_or_create(\n            guid=job_guid,\n            defaults={\n                \"repository\": repository,\n                \"signature\": signature,\n                \"build_platform\": build_platform,\n                \"machine_platform\": machine_platform,\n                \"machine\": machine,\n                \"option_collection_hash\": option_collection_hash,\n                \"job_type\": job_type,\n                \"job_group\": job_group,\n                \"product\": product,\n                \"failure_classification\": default_failure_classification,\n                \"who\": who,\n                \"reason\": reason,\n                \"result\": result,\n                \"state\": state,\n                \"tier\": tier,\n                \"submit_time\": submit_time,\n                \"start_time\": start_time,\n                \"end_time\": end_time,\n                \"last_modified\": datetime.now(),\n                \"push_id\": push_id\n            }\n        )\n    # Can't just use the ``job`` we would get from the ``get_or_create``\n    # because we need to try the job_guid_root instance first for update,\n    # rather than a possible retry job instance.\n    try:\n        job = Job.objects.get(guid=job_guid_root)\n    except ObjectDoesNotExist:\n        job = Job.objects.get(guid=job_guid)\n\n    # add taskcluster metadata if applicable\n    if all([k in job_datum for k in ['taskcluster_task_id', 'taskcluster_retry_id']]):\n        try:\n            TaskclusterMetadata.objects.create(\n                job=job,\n                task_id=job_datum['taskcluster_task_id'],\n                retry_id=job_datum['taskcluster_retry_id'])\n        except IntegrityError:\n            pass\n\n    # Update job with any data that would have changed\n    Job.objects.filter(id=job.id).update(\n        guid=job_guid,\n        signature=signature,\n        build_platform=build_platform,\n        machine_platform=machine_platform,\n        machine=machine,\n        option_collection_hash=option_collection_hash,\n        job_type=job_type,\n        job_group=job_group,\n        product=product,\n        failure_classification=default_failure_classification,\n        who=who,\n        reason=reason,\n        result=result,\n        state=state,\n        tier=tier,\n        submit_time=submit_time,\n        start_time=start_time,\n        end_time=end_time,\n        last_modified=datetime.now(),\n        push_id=push_id)\n\n    artifacts = job_datum.get('artifacts', [])\n\n    has_text_log_summary = any(x for x in artifacts\n                               if x['name'] == 'text_log_summary')\n    if artifacts:\n        artifacts = serialize_artifact_json_blobs(artifacts)\n\n        # need to add job guid to artifacts, since they likely weren't\n        # present in the beginning\n        for artifact in artifacts:\n            if not all(k in artifact for k in (\"name\", \"type\", \"blob\")):\n                raise ValueError(\n                    \"Artifact missing properties: {}\".format(artifact))\n            # Ensure every artifact has a ``job_guid`` value.\n            # It is legal to submit an artifact that doesn't have a\n            # ``job_guid`` value.  But, if missing, it should inherit that\n            # value from the job itself.\n            if \"job_guid\" not in artifact:\n                artifact[\"job_guid\"] = job_guid\n\n        store_job_artifacts(artifacts)\n\n    log_refs = job_datum.get('log_references', [])\n    job_logs = []\n    if log_refs:\n        for log in log_refs:\n            name = log.get('name') or 'unknown'\n            name = name[0:50]\n\n            url = log.get('url') or 'unknown'\n            url = url[0:255]\n\n            # this indicates that a summary artifact was submitted with\n            # this job that corresponds to the buildbot_text log url.\n            # Therefore, the log does not need parsing.  So we should\n            # ensure that it's marked as already parsed.\n            if has_text_log_summary and name == 'buildbot_text':\n                parse_status = JobLog.PARSED\n            else:\n                parse_status_map = dict([(k, v) for (v, k) in\n                                         JobLog.STATUSES])\n                mapped_status = parse_status_map.get(\n                    log.get('parse_status'))\n                if mapped_status:\n                    parse_status = mapped_status\n                else:\n                    parse_status = JobLog.PENDING\n\n            jl, _ = JobLog.objects.get_or_create(\n                job=job, name=name, url=url, defaults={\n                    'status': parse_status\n                })\n\n            job_logs.append(jl)\n\n        _schedule_log_parsing(job, job_logs, result)\n\n    return job_guid", "response": "Load a job into the treeherder database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscheduling the initial task that parses the log data.", "response": "def _schedule_log_parsing(job, job_logs, result):\n    \"\"\"Kick off the initial task that parses the log data.\n\n    log_data is a list of job log objects and the result for that job\n    \"\"\"\n\n    # importing here to avoid an import loop\n    from treeherder.log_parser.tasks import parse_logs\n\n    task_types = {\n        \"errorsummary_json\",\n        \"buildbot_text\",\n        \"builds-4h\"\n    }\n\n    job_log_ids = []\n    for job_log in job_logs:\n        # a log can be submitted already parsed.  So only schedule\n        # a parsing task if it's ``pending``\n        # the submitter is then responsible for submitting the\n        # text_log_summary artifact\n        if job_log.status != JobLog.PENDING:\n            continue\n\n        # if this is not a known type of log, abort parse\n        if job_log.name not in task_types:\n            continue\n\n        job_log_ids.append(job_log.id)\n\n    # TODO: Replace the use of different queues for failures vs not with the\n    # RabbitMQ priority feature (since the idea behind separate queues was\n    # only to ensure failures are dealt with first if there is a backlog).\n    if result != 'success':\n        queue = 'log_parser_fail'\n        priority = 'failures'\n    else:\n        queue = 'log_parser'\n        priority = \"normal\"\n\n    parse_logs.apply_async(queue=queue,\n                           args=[job.id, job_log_ids, priority])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef store_job_data(repository, data):\n    # Ensure that we have job data to process\n    if not data:\n        return\n\n    # remove any existing jobs that already have the same state\n    data = _remove_existing_jobs(data)\n    if not data:\n        return\n\n    superseded_job_guid_placeholders = []\n\n    # TODO: Refactor this now that store_job_data() is only over called with one job at a time.\n    for datum in data:\n        try:\n            # TODO: this might be a good place to check the datum against\n            # a JSON schema to ensure all the fields are valid.  Then\n            # the exception we caught would be much more informative.  That\n            # being said, if/when we transition to only using the pulse\n            # job consumer, then the data will always be vetted with a\n            # JSON schema before we get to this point.\n            job = datum['job']\n            revision = datum['revision']\n            superseded = datum.get('superseded', [])\n\n            revision_field = 'revision__startswith' if len(revision) < 40 else 'revision'\n            filter_kwargs = {'repository': repository, revision_field: revision}\n            push_id = Push.objects.values_list('id', flat=True).get(**filter_kwargs)\n\n            # load job\n            job_guid = _load_job(repository, job, push_id)\n\n            for superseded_guid in superseded:\n                superseded_job_guid_placeholders.append(\n                    # superseded by guid, superseded guid\n                    [job_guid, superseded_guid]\n                )\n        except Exception as e:\n            # Surface the error immediately unless running in production, where we'd\n            # rather report it on New Relic and not block storing the remaining jobs.\n            # TODO: Once buildbot support is removed, remove this as part of\n            # refactoring this method to process just one job at a time.\n            if 'DYNO' not in os.environ:\n                raise\n\n            logger.exception(e)\n            # make more fields visible in new relic for the job\n            # where we encountered the error\n            datum.update(datum.get(\"job\", {}))\n            newrelic.agent.record_exception(params=datum)\n\n            # skip any jobs that hit errors in these stages.\n            continue\n\n    # Update the result/state of any jobs that were superseded by those ingested above.\n    if superseded_job_guid_placeholders:\n        for (job_guid, superseded_by_guid) in superseded_job_guid_placeholders:\n            Job.objects.filter(guid=superseded_by_guid).update(\n                result='superseded',\n                state='completed')", "response": "Stores data into jobs db"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a Kombu Exchange object using the passed in name.", "response": "def get_exchange(connection, name, create=False):\n    \"\"\"\n    Get a Kombu Exchange object using the passed in name.\n\n    Can create an Exchange but this is typically not wanted in production-like\n    environments and only useful for testing.\n    \"\"\"\n    exchange = Exchange(name, type=\"topic\", passive=not create)\n\n    # bind the exchange to our connection so operations can be performed on it\n    bound_exchange = exchange(connection)\n\n    # ensure the exchange exists.  Throw an error if it was created with\n    # passive=True and it doesn't exist.\n    bound_exchange.declare()\n\n    return bound_exchange"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process(self, project, build_system, job_priorities):\n        '''Return list of ref_data_name for job_priorities'''\n        jobs = []\n\n        # we cache the reference data names in order to reduce API calls\n        cache_key = '{}-{}-ref_data_names_cache'.format(project, build_system)\n        ref_data_names_map = cache.get(cache_key)\n        if not ref_data_names_map:\n            # cache expired so re-build the reference data names map; the map\n            # contains the ref_data_name of every treeherder *test* job for this project\n            ref_data_names_map = self._build_ref_data_names(project, build_system)\n            # update the cache\n            cache.set(cache_key, ref_data_names_map, SETA_REF_DATA_NAMES_CACHE_TIMEOUT)\n\n        # now check the JobPriority table against the list of valid runnable\n        for jp in job_priorities:\n            # if this JobPriority entry is no longer supported in SETA then ignore it\n            if not valid_platform(jp.platform):\n                continue\n            if is_job_blacklisted(jp.testtype):\n                continue\n\n            key = jp.unique_identifier()\n            if key in ref_data_names_map:\n                # e.g. desktop-test-linux64-pgo/opt-reftest-13 or builder name\n                jobs.append(ref_data_names_map[key])\n            else:\n                logger.warning('Job priority (%s) not found in accepted jobs list', jp)\n\n        return jobs", "response": "Return list of ref_data_name for job_priorities"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a dictionary of reference data names for every task that runs on a specific project.", "response": "def _build_ref_data_names(self, project, build_system):\n        '''\n        We want all reference data names for every task that runs on a specific project.\n\n        For example:\n            * Buildbot - \"Windows 8 64-bit mozilla-inbound debug test web-platform-tests-1\"\n            * TaskCluster = \"test-linux64/opt-mochitest-webgl-e10s-1\"\n        '''\n        ignored_jobs = []\n        ref_data_names = {}\n\n        runnable_jobs = list_runnable_jobs(project)\n\n        for job in runnable_jobs:\n            # get testtype e.g. web-platform-tests-4\n            testtype = parse_testtype(\n                build_system_type=job['build_system_type'],\n                job_type_name=job['job_type_name'],\n                platform_option=job['platform_option'],\n                ref_data_name=job['ref_data_name']\n            )\n\n            if not valid_platform(job['platform']):\n                continue\n\n            if is_job_blacklisted(testtype):\n                ignored_jobs.append(job['ref_data_name'])\n                continue\n\n            key = unique_key(testtype=testtype,\n                             buildtype=job['platform_option'],\n                             platform=job['platform'])\n\n            if build_system == '*':\n                ref_data_names[key] = job['ref_data_name']\n            elif job['build_system_type'] == build_system:\n                ref_data_names[key] = job['ref_data_name']\n\n        for ref_data_name in sorted(ignored_jobs):\n            logger.info('Ignoring %s', ref_data_name)\n\n        return ref_data_names"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs in the user and returns a Response object.", "response": "def login(self, request):\n        \"\"\"\n        Verify credentials\n        \"\"\"\n        try:\n            user = authenticate(request)\n\n            if not user:\n                raise AuthenticationFailed(\"User not authenticated.\")\n\n            if not user.is_active:\n                raise AuthenticationFailed(\"This user has been disabled.\")\n\n            login(request, user)\n\n            return Response(UserSerializer(user).data)\n        except AuthError as ex:\n            # This indicates an error that may require attention by the\n            # Treeherder or Taskcluster teams.  Logging this to New Relic to\n            # increase visibility.\n            newrelic.agent.record_exception()\n            logger.exception(\"Error\", exc_info=ex)\n            raise AuthenticationFailed(str(ex))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect all unique field names that are eligible for optimization", "response": "def collect_fields(node):\n    \"\"\"\n    Get all the unique field names that are eligible for optimization\n\n    Requested a function like this be added to the ``info`` object\n    upstream in graphene_django:\n    https://github.com/graphql-python/graphene-django/issues/230\n    \"\"\"\n    fields = set()\n    for leaf in node:\n        if leaf.get('kind', None) == \"Field\":\n            fields.add(leaf[\"name\"][\"value\"])\n\n        if leaf.get(\"selection_set\", None):\n            fields = fields.union(collect_fields(leaf[\"selection_set\"][\"selections\"]))\n\n    return fields"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef optimize(qs, info_dict, field_map):\n    fields = collect_fields(info_dict)\n    for field in fields:\n        if field in field_map:\n            field_name, opt = field_map[field]\n            qs = (qs.prefetch_related(field_name)\n                  if opt == \"prefetch\" else qs.select_related(field_name))\n\n    return qs", "response": "Optimize the qs by adding either select_related or prefetch_related to fields of the qs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_connection(url):\n    username = os.environ.get('ELASTICSEARCH_USERNAME')\n    password = os.environ.get('ELASTICSEARCH_PASSWORD')\n\n    if username and password:\n        return Elasticsearch(url, http_auth=(username, password))\n\n    return Elasticsearch(url)", "response": "Build an Elasticsearch connection with the given url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_line(self, line):\n        # The parser may only need to run until it has seen a specific line.\n        # Once that's occurred, it can mark itself as complete, to save\n        # being run against later log lines.\n        if self.parser.complete:\n            return\n\n        # Perf data is stored in a json structure contained in a single line,\n        # if the MAX_LINE_LENGTH is applied the data structure could be\n        # truncated, preventing it from being ingested.\n        if 'PERFHERDER_DATA' not in line:\n            line = line[:self.MAX_LINE_LENGTH]\n\n        self.parser.parse_line(line, self.lineno)\n        self.lineno += 1", "response": "Parse a single line of the log."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the job artifact built by the parser.", "response": "def get_artifact(self):\n        \"\"\"Return the job artifact built by the parser.\"\"\"\n        self.artifact[self.parser.name] = self.parser.get_artifact()\n        return self.artifact"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef precise_matcher(text_log_error):\n    failure_line = text_log_error.metadata.failure_line\n    logger.debug(\"Looking for test match in failure %d\", failure_line.id)\n\n    if failure_line.action != \"test_result\" or failure_line.message is None:\n        return\n\n    f = {\n        'text_log_error___metadata__failure_line__action': 'test_result',\n        'text_log_error___metadata__failure_line__test': failure_line.test,\n        'text_log_error___metadata__failure_line__subtest': failure_line.subtest,\n        'text_log_error___metadata__failure_line__status': failure_line.status,\n        'text_log_error___metadata__failure_line__expected': failure_line.expected,\n        'text_log_error___metadata__failure_line__message': failure_line.message\n    }\n    qwargs = (\n        Q(text_log_error___metadata__best_classification=None)\n        & (Q(text_log_error___metadata__best_is_verified=True)\n           | Q(text_log_error__step__job=text_log_error.step.job))\n    )\n    qs = (TextLogErrorMatch.objects.filter(**f)\n                                   .exclude(qwargs)\n                                   .order_by('-score', '-classified_failure'))\n\n    if not qs:\n        return\n\n    # chunk through the QuerySet because it could potentially be very large\n    # time bound each call to the scoring function to avoid job timeouts\n    # returns an iterable of (score, classified_failure_id) tuples\n    chunks = chunked_qs_reverse(qs, chunk_size=20000)\n    return chain.from_iterable(time_boxed(score_matches, chunks, time_budget=500))", "response": "Query for TextLogErrorMatches identical to matches of the given TextLogError."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef elasticsearch_matcher(text_log_error):\n    # Note: Elasticsearch is currently disabled in all environments (see bug 1527868).\n    if not settings.ELASTICSEARCH_URL:\n        return []\n\n    failure_line = text_log_error.metadata.failure_line\n\n    if failure_line.action != \"test_result\" or not failure_line.message:\n        logger.debug(\"Skipped elasticsearch matching\")\n        return\n\n    filters = [\n        {'term': {'test': failure_line.test}},\n        {'term': {'status': failure_line.status}},\n        {'term': {'expected': failure_line.expected}},\n        {'exists': {'field': 'best_classification'}}\n    ]\n    if failure_line.subtest:\n        query = filters.append({'term': {'subtest': failure_line.subtest}})\n\n    query = {\n        'query': {\n            'bool': {\n                'filter': filters,\n                'must': [{\n                    'match_phrase': {\n                        'message': failure_line.message[:1024],\n                    },\n                }],\n            },\n        },\n    }\n\n    try:\n        results = search(query)\n    except Exception:\n        logger.error(\"Elasticsearch lookup failed: %s %s %s %s %s\",\n                     failure_line.test, failure_line.subtest, failure_line.status,\n                     failure_line.expected, failure_line.message)\n        raise\n\n    if len(results) > 1:\n        args = (\n            text_log_error.id,\n            failure_line.id,\n            len(results),\n        )\n        logger.info('text_log_error=%i failure_line=%i Elasticsearch produced %i results' % args)\n        newrelic.agent.record_custom_event('es_matches', {\n            'num_results': len(results),\n            'text_log_error_id': text_log_error.id,\n            'failure_line_id': failure_line.id,\n        })\n\n    scorer = MatchScorer(failure_line.message)\n    matches = [(item, item['message']) for item in results]\n    best_match = scorer.best_match(matches)\n    if not best_match:\n        return\n\n    score, es_result = best_match\n    # TODO: score all results and return\n    # TODO: just return results with score above cut off?\n    return [(score, es_result['best_classification'])]", "response": "Query Elasticsearch and score the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new list of TextLogErrorMatches that match the given crash signature.", "response": "def crash_signature_matcher(text_log_error):\n    \"\"\"\n    Query for TextLogErrorMatches with the same crash signature.\n\n    Produces two queries, first checking if the same test produces matches\n    and secondly checking without the same test but lowering the produced\n    scores.\n    \"\"\"\n    failure_line = text_log_error.metadata.failure_line\n\n    if (failure_line.action != \"crash\" or\n        failure_line.signature is None or\n        failure_line.signature == \"None\"):\n        return\n\n    f = {\n        'text_log_error___metadata__failure_line__action': 'crash',\n        'text_log_error___metadata__failure_line__signature': failure_line.signature,\n    }\n    qwargs = (\n        Q(text_log_error___metadata__best_classification=None)\n        & (Q(text_log_error___metadata__best_is_verified=True)\n           | Q(text_log_error__step__job=text_log_error.step.job))\n    )\n    qs = (TextLogErrorMatch.objects.filter(**f)\n                                   .exclude(qwargs)\n                                   .select_related('text_log_error', 'text_log_error___metadata')\n                                   .order_by('-score', '-classified_failure'))\n\n    size = 20000\n    time_budget = 500\n\n    # See if we can get any matches when filtering by the same test\n    first_attempt = qs.filter(text_log_error___metadata__failure_line__test=failure_line.test)\n    chunks = chunked_qs_reverse(first_attempt, chunk_size=size)\n    scored_matches = chain.from_iterable(time_boxed(score_matches, chunks, time_budget))\n    if scored_matches:\n        return scored_matches\n\n    # try again without filtering to the test but applying a .8 score multiplyer\n    chunks = chunked_qs_reverse(qs, chunk_size=size)\n    scored_matches = chain.from_iterable(time_boxed(\n        score_matches,\n        chunks,\n        time_budget,\n        score_multiplier=(8, 10),\n    ))\n    return scored_matches"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a list of candidate strings find the most similar string to self. target and return the best match with a score indicating closeness of match.", "response": "def best_match(self, matches):\n        \"\"\"\n        Find the most similar string to self.target.\n\n        Given a list of candidate strings find the closest match to\n        self.target, returning the best match with a score indicating closeness\n        of match.\n\n        :param matches: A list of candidate matches\n        :returns: A tuple of (score, best_match)\n        \"\"\"\n        best_match = None\n        for match, message in matches:\n            self.matcher.set_seq1(message)\n            ratio = self.matcher.quick_ratio()\n            if best_match is None or ratio >= best_match[0]:\n                new_ratio = self.matcher.ratio()\n                if best_match is None or new_ratio > best_match[0]:\n                    best_match = (new_ratio, match)\n        return best_match"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring the push data in the treeherder database.", "response": "def store_push_data(repository, pushes):\n    \"\"\"\n    Stores push data in the treeherder database\n\n    pushes = [\n        {\n         \"revision\": \"8afdb7debc82a8b6e0d56449dfdf916c77a7bf80\",\n         \"push_timestamp\": 1378293517,\n         \"author\": \"some-sheriff@mozilla.com\",\n         \"revisions\": [\n            {\n                \"comment\": \"Bug 911954 - Add forward declaration of JSScript to TraceLogging.h, r=h4writer\",\n                \"author\": \"John Doe <jdoe@mozilla.com>\",\n                \"revision\": \"8afdb7debc82a8b6e0d56449dfdf916c77a7bf80\"\n                },\n            ...\n            ]\n            },\n        ...\n        ]\n\n    returns = {\n\n        }\n    \"\"\"\n\n    if not pushes:\n        logger.info(\"No new pushes to store\")\n        return\n\n    for push in pushes:\n        store_push(repository, push)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cycle_data(self, repository, cycle_interval, chunk_size, sleep_time):\n\n        max_timestamp = datetime.datetime.now() - cycle_interval\n\n        # seperate datums into chunks\n        while True:\n            perf_datums_to_cycle = list(self.filter(\n                repository=repository,\n                push_timestamp__lt=max_timestamp).values_list('id', flat=True)[:chunk_size])\n            if not perf_datums_to_cycle:\n                # we're done!\n                break\n            self.filter(id__in=perf_datums_to_cycle).delete()\n            if sleep_time:\n                # Allow some time for other queries to get through\n                time.sleep(sleep_time)\n\n        # also remove any signatures which are (no longer) associated with\n        # a job\n        for signature in PerformanceSignature.objects.filter(\n                repository=repository):\n            if not self.filter(signature=signature).exists():\n                signature.delete()", "response": "Delete data older than cycle_interval splitting the target data into chunks of chunk_size size."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_bug_changes(self, startday, endday, alt_startday, alt_endday):\n\n        bug_stats, bug_ids = self.get_bug_stats(startday, endday)\n        alt_date_bug_totals = self.get_alt_date_bug_totals(alt_startday, alt_endday, bug_ids)\n        test_run_count = self.get_test_runs(startday, endday)\n\n        # if fetch_bug_details fails, None is returned\n        bug_info = self.fetch_all_bug_details(bug_ids)\n\n        all_bug_changes = []\n        template = Template(self.open_file('comment.template', False))\n\n        if self.weekly_mode:\n            top_bugs = [bug[0] for bug in sorted(bug_stats.items(), key=lambda x: x[1]['total'],\n                        reverse=True)][:50]\n\n        for bug_id, counts in bug_stats.items():\n            change_priority = None\n            change_whiteboard = None\n            priority = 0\n            rank = top_bugs.index(bug_id)+1 if self.weekly_mode and bug_id in top_bugs else None\n\n            if bug_info and bug_id in bug_info:\n                if self.weekly_mode:\n                    priority = self.assign_priority(counts)\n                    if priority == 2:\n                        change_priority, change_whiteboard = self.check_needswork_owner(bug_info[bug_id])\n\n                    # change [stockwell needswork] to [stockwell unknown] when failures drop below 20 failures/week\n                    # if this block is true, it implies a priority of 0 (mutually exclusive to previous block)\n                    if (counts['total'] < 20):\n                        change_whiteboard = self.check_needswork(bug_info[bug_id]['whiteboard'])\n\n                else:\n                    change_priority, change_whiteboard = self.check_needswork_owner(bug_info[bug_id])\n\n                # recommend disabling when more than 150 failures tracked over 21 days and\n                # takes precedence over any prevous change_whiteboard assignments\n                if (bug_id in alt_date_bug_totals and not self.check_whiteboard_status(bug_info[bug_id]['whiteboard'])):\n                    priority = 3\n                    change_whiteboard = self.update_whiteboard(bug_info[bug_id]['whiteboard'], '[stockwell disable-recommended]')\n\n            comment = template.render(bug_id=bug_id,\n                                      total=counts['total'],\n                                      test_run_count=test_run_count,\n                                      rank=rank,\n                                      priority=priority,\n                                      failure_rate=round(counts['total']/float(test_run_count), 3),\n                                      repositories=counts['per_repository'],\n                                      platforms=counts['per_platform'],\n                                      startday=startday,\n                                      endday=endday.split()[0],\n                                      weekly_mode=self.weekly_mode)\n\n            bug_changes = {'bug_id': bug_id,\n                           'changes': {\n                              'comment': {'body': comment}\n                            }\n                           }\n\n            if change_whiteboard:\n                bug_changes['changes']['whiteboard'] = change_whiteboard\n\n            if change_priority:\n                bug_changes['changes']['priority'] = change_priority\n\n            all_bug_changes.append(bug_changes)\n\n        return all_bug_changes", "response": "Generate a list of dicts containing a bug id a comment and priority status and a priority status and a whiteboard status or priority status."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate start and end date strings for a given number of days.", "response": "def calculate_date_strings(self, mode, numDays):\n        \"\"\"Returns a tuple of start (in YYYY-MM-DD format) and end date\n           strings (in YYYY-MM-DD HH:MM:SS format for an inclusive day).\"\"\"\n\n        yesterday = date.today() - timedelta(days=1)\n        endday = datetime(yesterday.year, yesterday.month, yesterday.day, 23, 59, 59, 999)\n        if mode:\n            startday = yesterday - timedelta(days=numDays)\n        else:\n            # daily mode\n            startday = yesterday\n\n        return startday.isoformat(), endday.strftime('%Y-%m-%d %H:%M:%S.%f')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting stockwell text from a bug s whiteboard status to determine whether it matches specified stockwell text ; returns a boolean.", "response": "def check_whiteboard_status(self, whiteboard):\n        \"\"\"Extracts stockwell text from a bug's whiteboard status to\n           determine whether it matches specified stockwell text;\n           returns a boolean.\"\"\"\n\n        stockwell_text = re.search(r'\\[stockwell (.+?)\\]', whiteboard)\n        if stockwell_text is not None:\n            text = stockwell_text.group(1).split(':')[0]\n            if text == 'fixed' or text == 'disable-recommended' or text == 'infra' or text == 'disabled':\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch bug metadata from bugzilla and returns an encoded dict if successful otherwise returns None.", "response": "def fetch_bug_details(self, bug_ids):\n        \"\"\"Fetches bug metadata from bugzilla and returns an encoded\n           dict if successful, otherwise returns None.\"\"\"\n\n        params = {'include_fields': 'product, component, priority, whiteboard, id'}\n        params['id'] = bug_ids\n        try:\n            response = self.session.get(settings.BZ_API_URL + '/rest/bug', headers=self.session.headers,\n                                        params=params, timeout=30)\n            response.raise_for_status()\n        except RequestException as e:\n            logger.warning('error fetching bugzilla metadata for bugs due to {}'.format(e))\n            return None\n\n        if response.headers['Content-Type'] == 'text/html; charset=UTF-8':\n            return None\n\n        data = response.json()\n        if 'bugs' not in data:\n            return None\n\n        return data['bugs']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all intermittent failures per specified date range and repository and platform totals returning a dict of bug_id s with total repository and platform totals and total total.", "response": "def get_bug_stats(self, startday, endday):\n        \"\"\"Get all intermittent failures per specified date range and repository,\n           returning a dict of bug_id's with total, repository and platform totals\n           if totals are greater than or equal to the threshold.\n        eg:\n        {\n            \"1206327\": {\n                \"total\": 5,\n                \"per_repository\": {\n                    \"fx-team\": 2,\n                    \"mozilla-inbound\": 3\n                },\n                \"per_platform\": {\n                    \"osx-10-10\": 4,\n                    \"b2g-emu-ics\": 1\n                }\n            },\n            ...\n        }\n        \"\"\"\n        # Min required failures per bug in order to post a comment\n        threshold = 1 if self.weekly_mode else 15\n        bug_ids = (BugJobMap.failures.by_date(startday, endday)\n                                     .values('bug_id')\n                                     .annotate(total=Count('bug_id'))\n                                     .filter(total__gte=threshold)\n                                     .values_list('bug_id', flat=True))\n\n        bugs = (BugJobMap.failures.by_date(startday, endday)\n                                  .filter(bug_id__in=bug_ids)\n                                  .values('job__repository__name', 'job__machine_platform__platform',\n                                          'bug_id'))\n\n        bug_map = dict()\n        for bug in bugs:\n            platform = bug['job__machine_platform__platform']\n            repo = bug['job__repository__name']\n            bug_id = bug['bug_id']\n            if bug_id in bug_map:\n                bug_map[bug_id]['total'] += 1\n                bug_map[bug_id]['per_platform'][platform] += 1\n                bug_map[bug_id]['per_repository'][repo] += 1\n            else:\n                bug_map[bug_id] = {}\n                bug_map[bug_id]['total'] = 1\n                bug_map[bug_id]['per_platform'] = Counter([platform])\n                bug_map[bug_id]['per_repository'] = Counter([repo])\n\n        return bug_map, bug_ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_alt_date_bug_totals(self, startday, endday, bug_ids):\n        bugs = (BugJobMap.failures.by_date(startday, endday)\n                                  .filter(bug_id__in=bug_ids)\n                                  .values('bug_id')\n                                  .annotate(total=Count('id'))\n                                  .values('bug_id', 'total'))\n\n        return {bug['bug_id']: bug['total'] for bug in bugs if bug['total'] >= 150}", "response": "use previously fetched bug_ids to check for total failures exceeding 150 in 21 days"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch_all_bug_details(self, bug_ids):\n        min = 0\n        max = 600\n        bugs_list = []\n        bug_ids_length = len(bug_ids)\n\n        while bug_ids_length >= min and bug_ids_length > 0:\n            data = self.fetch_bug_details(bug_ids[min:max])\n            if data:\n                bugs_list += data\n            min = max\n            max = max + 600\n\n        return {bug['id']: bug for bug in bugs_list} if len(bugs_list) else None", "response": "fetch all bug details from bugzilla"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming a Treeherder testtype to a Treeherder task type.", "response": "def transform(testtype):\n    '''\n    A lot of these transformations are from tasks before task labels and some of them are if we\n    grab data directly from Treeherder jobs endpoint instead of runnable jobs API.\n    '''\n    # XXX: Evaluate which of these transformations are still valid\n    if testtype.startswith('[funsize'):\n        return None\n\n    testtype = testtype.split('/opt-')[-1]\n    testtype = testtype.split('/debug-')[-1]\n\n    # this is plain-reftests for android\n    testtype = testtype.replace('plain-', '')\n\n    testtype = testtype.strip()\n\n    # https://bugzilla.mozilla.org/show_bug.cgi?id=1313844\n    testtype = testtype.replace('browser-chrome-e10s', 'e10s-browser-chrome')\n    testtype = testtype.replace('devtools-chrome-e10s', 'e10s-devtools-chrome')\n    testtype = testtype.replace('[TC] Android 4.3 API15+ ', '')\n\n    # mochitest-gl-1 <-- Android 4.3 armv7 API 15+ mozilla-inbound opt test mochitest-gl-1\n    # mochitest-webgl-9  <-- test-android-4.3-arm7-api-15/opt-mochitest-webgl-9\n    testtype = testtype.replace('webgl-', 'gl-')\n\n    return testtype"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_username_from_userinfo(self, user_info):\n\n        subject = user_info['sub']\n        email = user_info['email']\n\n        if \"Mozilla-LDAP\" in subject:\n            return \"mozilla-ldap/\" + email\n        elif \"email\" in subject:\n            return \"email/\" + email\n        elif \"github\" in subject:\n            return \"github/\" + email\n        elif \"google\" in subject:\n            return \"google/\" + email\n        # Firefox account\n        elif \"oauth2\" in subject:\n            return \"oauth2/\" + email\n        else:\n            raise AuthenticationFailed(\"Unrecognized identity\")", "response": "Get the user s username from the jwt subject and email properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the user info from the Id Token.", "response": "def _get_user_info(self, access_token, id_token):\n        \"\"\"\n        Extracts the user info payload from the Id Token.\n\n        Example return value:\n\n        {\n            \"at_hash\": \"<HASH>\",\n            \"aud\": \"<HASH>\",\n            \"email_verified\": true,\n            \"email\": \"fsurname@mozilla.com\",\n            \"exp\": 1551259495,\n            \"family_name\": \"Surname\",\n            \"given_name\": \"Firstname\",\n            \"https://sso.mozilla.com/claim/groups\": [\n                \"all_scm_level_1\",\n                \"all_scm_level_2\",\n                \"all_scm_level_3\",\n                # ...\n            ],\n            \"iat\": 1550654695,\n            \"iss\": \"https://auth.mozilla.auth0.com/\",\n            \"name\": \"Firstname Surname\",\n            \"nickname\": \"Firstname Surname\",\n            \"nonce\": \"<HASH>\",\n            \"picture\": \"<GRAVATAR_URL>\",\n            \"sub\": \"ad|Mozilla-LDAP|fsurname\",\n            \"updated_at\": \"2019-02-20T09:24:55.449Z\",\n        }\n        \"\"\"\n\n        # JWT Validator\n        # Per https://auth0.com/docs/quickstart/backend/python/01-authorization#create-the-jwt-validation-decorator\n\n        try:\n            unverified_header = jwt.get_unverified_header(id_token)\n        except jwt.JWTError:\n            raise AuthError('Unable to decode the Id token header')\n\n        if 'kid' not in unverified_header:\n            raise AuthError('Id token header missing RSA key ID')\n\n        rsa_key = None\n        for key in jwks[\"keys\"]:\n            if key[\"kid\"] == unverified_header[\"kid\"]:\n                rsa_key = {\n                    \"kty\": key[\"kty\"],\n                    \"kid\": key[\"kid\"],\n                    \"use\": key[\"use\"],\n                    \"n\": key[\"n\"],\n                    \"e\": key[\"e\"]\n                }\n                break\n\n        if not rsa_key:\n            raise AuthError('Id token using unrecognised RSA key ID')\n\n        try:\n            # https://python-jose.readthedocs.io/en/latest/jwt/api.html#jose.jwt.decode\n            user_info = jwt.decode(\n                id_token,\n                rsa_key,\n                algorithms=['RS256'],\n                audience=AUTH0_CLIENTID,\n                access_token=access_token,\n                issuer=\"https://\"+AUTH0_DOMAIN+\"/\"\n            )\n            return user_info\n        except jwt.ExpiredSignatureError:\n            raise AuthError('Id token is expired')\n        except jwt.JWTClaimsError:\n            raise AuthError(\"Incorrect claims: please check the audience and issuer\")\n        except jwt.JWTError:\n            raise AuthError(\"Invalid header: Unable to parse authentication\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calculate_session_expiry(self, request, user_info):\n        access_token_expiry_timestamp = self._get_access_token_expiry(request)\n        id_token_expiry_timestamp = self._get_id_token_expiry(user_info)\n        now_in_seconds = int(time.time())\n\n        # The session length is set to match whichever token expiration time is closer.\n        earliest_expiration_timestamp = min(access_token_expiry_timestamp, id_token_expiry_timestamp)\n        seconds_until_expiry = earliest_expiration_timestamp - now_in_seconds\n\n        if seconds_until_expiry <= 0:\n            raise AuthError('Session expiry time has already passed!')\n\n        return seconds_until_expiry", "response": "Calculates the number of seconds until the Django session should expire."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a key to query our uniqueness mapping system.", "response": "def _unique_key(job):\n    \"\"\"Return a key to query our uniqueness mapping system.\n\n    This makes sure that we use a consistent key between our code and selecting jobs from the\n    table.\n    \"\"\"\n    return unique_key(testtype=str(job['testtype']),\n                      buildtype=str(job['platform_option']),\n                      platform=str(job['platform']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the priority table if required.", "response": "def _update_table(data):\n    \"\"\"Add new jobs to the priority table and update the build system if required.\n    data - it is a list of dictionaries that describe a job type\n\n    returns the number of new, failed and updated jobs\n    \"\"\"\n    jp_index, priority, expiration_date = _initialize_values()\n\n    total_jobs = len(data)\n    new_jobs, failed_changes, updated_jobs = 0, 0, 0\n    # Loop through sanitized jobs, add new jobs and update the build system if needed\n    for job in data:\n        key = _unique_key(job)\n        if key in jp_index:\n            # We already know about this job, we might need to update the build system\n            # We're seeing the job again with another build system (e.g. buildbot vs\n            # taskcluster). We need to change it to '*'\n            if jp_index[key]['build_system_type'] != '*' and jp_index[key]['build_system_type'] != job[\"build_system_type\"]:\n                db_job = JobPriority.objects.get(pk=jp_index[key]['pk'])\n                db_job.buildsystem = '*'\n                db_job.save()\n\n                logger.info('Updated %s/%s from %s to %s',\n                            db_job.testtype, db_job.buildtype,\n                            job['build_system_type'], db_job.buildsystem)\n                updated_jobs += 1\n\n        else:\n            # We have a new job from runnablejobs to add to our master list\n            try:\n                jobpriority = JobPriority(\n                    testtype=str(job[\"testtype\"]),\n                    buildtype=str(job[\"platform_option\"]),\n                    platform=str(job[\"platform\"]),\n                    priority=priority,\n                    expiration_date=expiration_date,\n                    buildsystem=job[\"build_system_type\"]\n                )\n                jobpriority.save()\n                logger.info('New job was found (%s,%s,%s,%s)',\n                            job['testtype'], job['platform_option'], job['platform'],\n                            job[\"build_system_type\"])\n                new_jobs += 1\n            except Exception as error:\n                logger.warning(str(error))\n                failed_changes += 1\n\n    logger.info('We have %s new jobs and %s updated jobs out of %s total jobs processed.',\n                new_jobs, updated_jobs, total_jobs)\n\n    if failed_changes != 0:\n        logger.warning('We have failed %s changes out of %s total jobs processed.',\n                       failed_changes, total_jobs)\n\n    return new_jobs, failed_changes, updated_jobs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_preseed():\n\n    if not JobPriority.objects.exists():\n        return\n    preseed = preseed_data()\n    for job in preseed:\n        queryset = JobPriority.objects.all()\n\n        for field in ('testtype', 'buildtype', 'platform'):\n            if job[field] != '*':\n                queryset = queryset.filter(**{field: job[field]})\n\n        # Deal with the case where we have a new entry in preseed\n        if not queryset:\n            create_new_entry(job)\n        else:\n            # We can have wildcards, so loop on all returned values in data\n            for jp in queryset:\n                process_job_priority(jp, job)", "response": "Load JobPriority information from preseed. json file and update the table with the information from the preseed. json file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all_valid_time_intervals():\n        '''\n        Helper method to return all possible valid time intervals for data\n        stored by Perfherder\n        '''\n        return [PerformanceTimeInterval.DAY,\n                PerformanceTimeInterval.WEEK,\n                PerformanceTimeInterval.TWO_WEEKS,\n                PerformanceTimeInterval.SIXTY_DAYS,\n                PerformanceTimeInterval.NINETY_DAYS,\n                PerformanceTimeInterval.ONE_YEAR]", "response": "Helper method to return all possible valid time intervals for a given data set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a filtered subset of this collection of signatures based on a set of key - value tuples.", "response": "def filter(self, *args):\n        '''\n        Returns a filtered subset of this collection of signatures, based on\n        a set of key/value tuples\n\n        This is useful when you only want a subset of the signatures in a project.\n\n        Example usage:\n\n        ::\n            pc = PerfherderClient()\n            signatures = pc.get_signatures('mozilla-central')\n            signatures = signatures.filter(('suite', 'tp5o'), ('machine_platform', 'windowsxp'))\n        '''\n        filtered_signatures = {}\n        for (signature, signature_value) in self.items():\n            skip = False\n            for (key, val) in args:\n                if signature_value.get(key) != val:\n                    skip = True\n                    break\n            if not skip:\n                filtered_signatures[signature] = signature_value\n\n        return PerformanceSignatureCollection(filtered_signatures)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_property_names(self):\n        '''\n        Returns all property names in this collection of signatures\n        '''\n        property_names = set()\n        for signature_value in self.values():\n            for property_name in signature_value.keys():\n                property_names.add(property_name)\n        return property_names", "response": "Returns all property names in this collection of signatures\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_property_values(self, property_name):\n        '''\n        Returns all property values for a particular property name in this collection\n        '''\n        property_values = set()\n        for signature_value in self.values():\n            if signature_value.get(property_name):\n                property_values.add(signature_value[property_name])\n        return property_values", "response": "Returns all property values for a particular property name in this collection"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_performance_signatures(self, project, **params):\n        '''\n        Gets a set of performance signatures associated with a project and time range\n        '''\n        results = self._get_json(self.PERFORMANCE_SIGNATURES_ENDPOINT, project, **params)\n        return PerformanceSignatureCollection(results)", "response": "Gets a set of performance signatures associated with a project and time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_performance_data(self, project, **params):\n        '''\n        Gets a dictionary of PerformanceSeries objects\n\n        You can specify which signatures to get by passing signature to this function\n        '''\n        results = self._get_json(self.PERFORMANCE_DATA_ENDPOINT, project, **params)\n\n        return {k: PerformanceSeries(v) for k, v in results.items()}", "response": "Gets a dictionary of PerformanceSeries objects holding information about the objects in the specified project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the list of functions that are available as matchers for the treeherder.", "response": "def get_matchers():\n    \"\"\"\n    Get matcher functions from treeherder.autoclassify.matchers\n\n    We classify matchers as any function treeherder.autoclassify.matchers with\n    a name ending in _matcher.  This is currently overkill but protects against\n    the unwarey engineer adding new functions to the matchers module that\n    shouldn't be treated as matchers.\n    \"\"\"\n    from . import matchers\n\n    def is_matcher_func(member):\n        return inspect.isfunction(member) and member.__name__.endswith(\"_matcher\")\n\n    members = inspect.getmembers(matchers, is_matcher_func)\n\n    for name, func in members:\n        yield func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the best match for each error in errors.", "response": "def find_best_matches(errors, matchers):\n    \"\"\"\n    Find the best match for each error\n\n    We use the Good Enough\u2122 ratio as a watershed level for match scores.\n    \"\"\"\n    for text_log_error in errors:\n        matches = find_all_matches(text_log_error, matchers)  # TextLogErrorMatch instances, unsaved!\n\n        best_match = first(matches, key=lambda m: (-m.score, -m.classified_failure_id))\n        if not best_match:\n            continue\n\n        newrelic.agent.record_custom_event('highest_scored_matcher', {\n            'matcher': best_match.matcher_name,\n            'score': best_match.score,\n            'text_log_error': best_match.text_log_error_id,\n        })\n\n        yield best_match"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_all_matches(text_log_error, matchers):\n    for matcher_func in matchers:\n        matches = matcher_func(text_log_error)\n        # matches: iterator of (score, ClassifiedFailure.id)\n        if not matches:\n            continue\n\n        for score, classified_failure_id in matches:\n            yield TextLogErrorMatch(\n                score=score,\n                matcher_name=matcher_func.__name__,\n                classified_failure_id=classified_failure_id,\n                text_log_error=text_log_error,\n            )", "response": "Find matches for the given error using the given matcher classes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_best_match(text_log_error):\n    score_cut_off = 0.7\n    return (text_log_error.matches.filter(score__gt=score_cut_off)\n                                  .order_by(\"-score\", \"-classified_failure_id\")\n                                  .select_related('classified_failure')\n                                  .first())", "response": "Get the best TextLogErrorMatch for a given TextLogErrorMatch."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the given ClassifiedFailure as best_classification for the given TextLogError and FailureLine.", "response": "def mark_best_classification(text_log_error, classified_failure):\n    \"\"\"\n    Wrapper for setting best_classification on both TextLogError and FailureLine.\n\n    Set the given ClassifiedFailure as best_classification for the given\n    TextLogError. Handles the duplication of best_classification on FailureLine\n    so you don't have to!\n    \"\"\"\n    text_log_error.metadata.best_classification = classified_failure\n    text_log_error.metadata.save(update_fields=['best_classification'])\n    text_log_error.metadata.failure_line.elastic_search_insert()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mark_best_classifications(errors):\n    for text_log_error in errors:\n        best_match = get_best_match(text_log_error)\n        if not best_match:\n            continue\n\n        mark_best_classification(text_log_error, best_match.classified_failure)", "response": "Given a list of TextLogErrors and a list of TextLogErrors mark the best classification for each of them."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave the given list of matches to the DB.", "response": "def update_db(matches):\n    \"\"\"\n    Save TextLogErrorMatch instances to the DB\n\n    We loop each Match instance instead of calling bulk_create() so we can\n    catch any potential IntegrityErrors and continue.\n    \"\"\"\n    for match in matches:\n        try:\n            match.save()\n        except IntegrityError:\n            args = (match.text_log_error_id, match.matcher_name, match.classified_failure_id)\n            logger.warning(\n                \"Tried to create duplicate match for TextLogError %i with matcher %s and classified_failure %i\",\n                args,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a JSON Schema by filename.", "response": "def get_json_schema(filename):\n    \"\"\"\n    Get a JSON Schema by filename.\n\n    \"\"\"\n    file_path = os.path.join(\"schemas\", filename)\n    with open(file_path) as f:\n        schema = yaml.load(f)\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_request(url, method='GET', headers=None, timeout=30, **kwargs):\n    headers = headers or {}\n    headers['User-Agent'] = 'treeherder/{}'.format(settings.SITE_HOSTNAME)\n    # Work around bug 1305768.\n    if 'queue.taskcluster.net' in url:\n        headers['x-taskcluster-skip-cache'] = 'true'\n    response = requests.request(method,\n                                url,\n                                headers=headers,\n                                timeout=timeout,\n                                **kwargs)\n\n    if response.history:\n        params = {\n            'url': url,\n            'redirects': len(response.history),\n            'duration': sum(r.elapsed.total_seconds() for r in response.history)\n        }\n        newrelic.agent.record_custom_event('RedirectedRequest', params=params)\n\n    response.raise_for_status()\n    return response", "response": "A wrapper around requests. request to set defaults & call raise_for_status"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_job_info_artifact(job, job_info_artifact):\n    job_details = json.loads(job_info_artifact['blob'])['job_details']\n    for job_detail in job_details:\n        job_detail_dict = {\n            'title': job_detail.get('title'),\n            'value': job_detail['value'],\n            'url': job_detail.get('url')\n        }\n        for (k, v) in job_detail_dict.items():\n            max_field_length = JobDetail._meta.get_field(k).max_length\n            if v is not None and len(v) > max_field_length:\n                logger.warning(\"Job detail '%s' for job_guid %s too long, truncating\",\n                               v[:max_field_length], job.guid)\n                job_detail_dict[k] = v[:max_field_length]\n\n        # move the url field to be updated in defaults now that it's\n        # had its size trimmed, if necessary\n        job_detail_dict['defaults'] = {'url': job_detail_dict['url']}\n        del job_detail_dict['url']\n\n        JobDetail.objects.update_or_create(\n            job=job,\n            **job_detail_dict)", "response": "Stores the contents of the job info artifact in the job details tree node node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_text_log_summary_artifact(job, text_log_summary_artifact):\n    step_data = json.loads(\n        text_log_summary_artifact['blob'])['step_data']\n    result_map = {v: k for (k, v) in TextLogStep.RESULTS}\n    with transaction.atomic():\n        for step in step_data['steps']:\n            name = step['name'][:TextLogStep._meta.get_field('name').max_length]\n            # process start/end times if we have them\n            # we currently don't support timezones in treeherder, so\n            # just ignore that when importing/updating the bug to avoid\n            # a ValueError (though by default the text log summaries\n            # we produce should have time expressed in UTC anyway)\n            time_kwargs = {}\n            for tkey in ('started', 'finished'):\n                if step.get(tkey):\n                    time_kwargs[tkey] = dateutil.parser.parse(\n                        step[tkey], ignoretz=True)\n\n            log_step = TextLogStep.objects.create(\n                job=job,\n                started_line_number=step['started_linenumber'],\n                finished_line_number=step['finished_linenumber'],\n                name=name,\n                result=result_map[step['result']],\n                **time_kwargs)\n\n            if step.get('errors'):\n                for error in step['errors']:\n                    TextLogError.objects.create(\n                        step=log_step,\n                        line_number=error['linenumber'],\n                        line=astral_filter(error['line']))\n\n    # get error summary immediately (to warm the cache)\n    error_summary.get_error_summary(job)", "response": "Stores the contents of the text log summary artifact in the treeherder s internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef store_job_artifacts(artifact_data):\n    for artifact in artifact_data:\n        # Determine what type of artifact we have received\n        if artifact:\n            artifact_name = artifact.get('name')\n            if not artifact_name:\n                logger.error(\"load_job_artifacts: Unnamed job artifact, skipping\")\n                continue\n            job_guid = artifact.get('job_guid')\n            if not job_guid:\n                logger.error(\"load_job_artifacts: Artifact '%s' with no \"\n                             \"job guid set, skipping\", artifact_name)\n                continue\n\n            try:\n                job = Job.objects.get(guid=job_guid)\n            except Job.DoesNotExist:\n                logger.error('load_job_artifacts: No job_id for guid %s', job_guid)\n                continue\n\n            if artifact_name == 'performance_data':\n                store_performance_artifact(job, artifact)\n            elif artifact_name == 'Job Info':\n                store_job_info_artifact(job, artifact)\n            elif artifact_name == 'text_log_summary':\n                try:\n                    store_text_log_summary_artifact(job, artifact)\n                except IntegrityError:\n                    logger.warning(\"Couldn't insert text log information \"\n                                   \"for job with guid %s, this probably \"\n                                   \"means the job was already parsed\",\n                                   job_guid)\n            else:\n                logger.warning(\"Unknown artifact type: %s submitted with job %s\",\n                               artifact_name, job.guid)\n        else:\n            logger.error('store_job_artifacts: artifact type %s not understood', artifact_name)", "response": "Stores the artifacts in the job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that JSON artifact blobs passed as dicts are converted to JSON", "response": "def serialize_artifact_json_blobs(artifacts):\n    \"\"\"\n    Ensure that JSON artifact blobs passed as dicts are converted to JSON\n    \"\"\"\n    for artifact in artifacts:\n        blob = artifact['blob']\n        if (artifact['type'].lower() == 'json' and\n                not isinstance(blob, str)):\n            artifact['blob'] = json.dumps(blob)\n\n    return artifacts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_option_collection_hash(self):\n        resp = self._get_json(self.OPTION_COLLECTION_HASH_ENDPOINT)\n        ret = {}\n        for result in resp:\n            ret[result['option_collection_hash']] = result['options']\n\n        return ret", "response": "Gets option collection hash a mapping of hash values to build properties\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_pushes(self, project, **params):\n        return self._get_json_list(self.PUSH_ENDPOINT, project, **params)", "response": "Get all the push data for a project filtered by parameters\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget jobs from project filtered by parameters", "response": "def get_jobs(self, project, **params):\n        \"\"\"\n        Gets jobs from project, filtered by parameters\n\n        :param project: project (repository name) to query data for\n        :param params: keyword arguments to filter results\n        \"\"\"\n        return self._get_json_list(self.JOBS_ENDPOINT, project, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the job log url for a project filtered by parameters asetk.", "response": "def get_job_log_url(self, project, **params):\n        \"\"\"\n        Gets job log url, filtered by parameters\n\n        :param project: project (repository name) to query data for\n        :param params: keyword arguments to filter results\n        \"\"\"\n        return self._get_json(self.JOB_LOG_URL_ENDPOINT, project,\n                              **params)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_documents(index=INDEX_NAME):\n    query = {\n        'query': {\n            'match_all': {}\n        }\n    }\n    for result in raw_query(query, index=index):\n        yield result", "response": "Get all documents from the given index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bulk(iterable, index=INDEX_NAME, doc_type=DOC_TYPE, action='index'):\n    actions = compact(dict_to_op(\n        to_dict(model),\n        index_name=INDEX_NAME,\n        doc_type=DOC_TYPE,\n        op_type=action,\n    ) for model in iterable)\n\n    # fail fast if there are no actions\n    if not actions:\n        return 0\n\n    items, _ = es_bulk(es_conn, actions, doc_type=doc_type, index=index)\n\n    return items", "response": "Wrapper of elasticsearch s bulk method"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_index(index=INDEX_NAME):\n    refresh_index()  # Refresh the index so we can get a correct count\n\n    query = {\n        'query': {\n            'match_all': {}\n        }\n    }\n    result = es_conn.count(index=index, doc_type=DOC_TYPE, body=query)\n    return result['count']", "response": "Return a document count for the given index"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a single document by ID", "response": "def get_document(id, index=INDEX_NAME, doc_type=DOC_TYPE, **kwargs):\n    \"\"\"\n    Thin wrapper to get a single document by ID.\n\n    https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.get\n    https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html\n    \"\"\"\n    result = es_conn.get(index=index, doc_type=doc_type, id=id, **kwargs)\n    return result['_source']"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef index(obj, index=INDEX_NAME, doc_type=DOC_TYPE):\n    doc = to_dict(obj)\n\n    if doc is None:\n        return\n\n    id = doc.pop('id')\n\n    return es_conn.index(index, doc_type, doc, id=id)", "response": "Index the given document."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping of the search function to provide useful defaults", "response": "def raw_query(query, index=INDEX_NAME, doc_type=DOC_TYPE):\n    \"\"\"\n    Thin wrapper of the search function to provide useful defaults\n\n    https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.search\n    http://www.elastic.co/guide/en/elasticsearch/reference/current/search-search.html\n    \"\"\"\n    result = es_conn.search(index=index, doc_type=DOC_TYPE, body=query)\n    return result['hits']['hits']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reinit_index(index=INDEX_NAME):\n    es_conn.indices.delete(index, ignore=404)\n\n    try:\n        es_conn.indices.create(index, INDEX_SETTINGS.get(index, None))\n    except TransportError as e:\n        raise Exception('Failed to created index, got: {}'.format(e.error))", "response": "Delete and then initialise the given index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search(query, index=INDEX_NAME, doc_type=DOC_TYPE):\n    results = raw_query(query, index=index, doc_type=doc_type)\n    return [r['_source'] for r in results]", "response": "Wrapper of the main query function to provide just the resulting objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_expiration_field_for_expired_jobs(self):\n        '''Set the expiration date of every job that has expired.'''\n        # Only select rows where there is an expiration date set\n        for job in JobPriority.objects.filter(expiration_date__isnull=False):\n            if job.has_expired():\n                job.expiration_date = None\n                job.save()", "response": "Set the expiration date of every job that has expired."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adjust_jobs_priority(self, high_value_jobs, priority=1):\n        # Only job priorities that don't have an expiration date (2 weeks for new jobs or year 2100\n        # for jobs update via load_preseed) are updated\n        for jp in JobPriority.objects.filter(expiration_date__isnull=True):\n            if jp.unique_identifier() not in high_value_jobs:\n                if jp.priority != SETA_LOW_VALUE_PRIORITY:\n                    logger.warning('Decreasing priority of %s', jp.unique_identifier())\n                    jp.priority = SETA_LOW_VALUE_PRIORITY\n                    jp.save(update_fields=['priority'])\n            elif jp.priority != priority:\n                logger.warning('Increasing priority of %s', jp.unique_identifier())\n                jp.priority = priority\n                jp.save(update_fields=['priority'])", "response": "Adjusts the priority of all jobs in the given high value jobs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef csp_report_collector(request):\n    try:\n        report = json.loads(request.body)['csp-report']\n    except (KeyError, TypeError, ValueError):\n        return HttpResponseBadRequest('Invalid CSP violation report')\n\n    logger.warning('CSP violation: %s', report)\n    newrelic.agent.record_custom_event('CSP violation', report)\n    return HttpResponse()", "response": "This function is used to collect the CSP violation reports from the request body."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists all the available job - related information.", "response": "def list(self, request, project):\n        ''' Routing to /api/project/{project}/seta/job-priorities/\n\n        This API can potentially have these consumers:\n            * Buildbot\n              * build_system_type=buildbot\n              * priority=5\n              * format=json\n            * TaskCluster (Gecko decision task)\n              * build_system_type=taskcluster\n              * format=json\n        '''\n        build_system_type = request.query_params.get('build_system_type', '*')\n        priority = request.query_params.get('priority')\n        try:\n            return Response(seta_job_scheduling(project, build_system_type, priority))\n        except SetaError as e:\n            return Response(str(e), status=status.HTTP_400_BAD_REQUEST)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_job_list_response(self, job_qs, offset, count, return_type):\n        '''\n        custom method to serialize + format jobs information\n\n        It's worth doing this big ugly thing (as opposed to using\n        the django rest framework serializer or whatever) as\n        this function is often in the critical path\n        '''\n        option_collection_map = OptionCollection.objects.get_option_collection_map()\n        results = []\n        for values in job_qs[offset:(offset+count)].values_list(\n                *[pq[1] for pq in self._property_query_mapping]):\n            platform_option = option_collection_map.get(\n                values[self._option_collection_hash_idx],\n                \"\")\n            # some values need to be transformed\n            values = list(values)\n            for (i, _) in enumerate(values):\n                func = self._property_query_mapping[i][2]\n                if func:\n                    values[i] = func(values[i])\n            # append results differently depending on if we are returning\n            # a dictionary or a list\n            if return_type == 'dict':\n                results.append(dict(zip(\n                    [pq[0] for pq in self._property_query_mapping] +\n                    ['platform_option'],\n                    values + [platform_option])))\n            else:\n                results.append(values + [platform_option])\n\n        response_dict = {\n            'results': results\n        }\n        if return_type == 'list':\n            response_dict.update({\n                'job_property_names': [pq[0] for pq in self._property_query_mapping] + ['platform_option']\n            })\n\n        return response_dict", "response": "This method is used to serialize and format jobs information as a dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef retrieve(self, request, project, pk=None):\n        try:\n            job = Job.objects.select_related(\n                *self._default_select_related + ['taskcluster_metadata']).get(\n                    repository__name=project, id=pk)\n        except Job.DoesNotExist:\n            return Response(\"No job with id: {0}\".format(pk), status=HTTP_404_NOT_FOUND)\n\n        resp = serializers.JobSerializer(job, read_only=True).data\n\n        resp[\"resource_uri\"] = reverse(\"jobs-detail\",\n                                       kwargs={\"project\": project, \"pk\": pk})\n        resp[\"logs\"] = []\n        for (name, url) in JobLog.objects.filter(job=job).values_list(\n                'name', 'url'):\n            resp[\"logs\"].append({'name': name, 'url': url})\n\n        platform_option = job.get_platform_option()\n        if platform_option:\n            resp[\"platform_option\"] = platform_option\n\n        try:\n            resp['taskcluster_metadata'] = {\n                'task_id': job.taskcluster_metadata.task_id,\n                'retry_id': job.taskcluster_metadata.retry_id\n            }\n        except ObjectDoesNotExist:\n            pass\n\n        status_map = {k: v for k, v in Job.AUTOCLASSIFY_STATUSES}\n        resp[\"autoclassify_status\"] = status_map[job.autoclassify_status]\n\n        return Response(resp)", "response": "GET method implementation for detail view\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget method implementation for list view of all the items in the database", "response": "def list(self, request, project):\n        \"\"\"\n        GET method implementation for list view\n        Optional parameters (default):\n        - offset (0)\n        - count (10)\n        - return_type (dict)\n        \"\"\"\n        MAX_JOBS_COUNT = 2000\n\n        # make a mutable copy of these params\n        filter_params = request.query_params.copy()\n\n        # various hacks to ensure API backwards compatibility\n        for param_key in filter_params.keys():\n            # replace `result_set_id` with `push_id`\n            if param_key.startswith('result_set_id'):\n                new_param_key = param_key.replace('result_set_id', 'push_id')\n                filter_params[new_param_key] = filter_params[param_key]\n                del filter_params[param_key]\n            # convert legacy timestamp parameters to time ones\n            elif param_key in ['submit_timestamp', 'start_timestamp',\n                               'end_timestamp']:\n                new_param_key = param_key.replace('timestamp', 'time')\n                filter_params[new_param_key] = datetime.datetime.fromtimestamp(\n                    float(filter_params[param_key]))\n                del filter_params[param_key]\n            # sanity check 'last modified'\n            elif param_key.startswith('last_modified'):\n                datestr = filter_params[param_key]\n                try:\n                    parser.parse(datestr)\n                except ValueError:\n                    return Response(\n                        \"Invalid date value for `last_modified`: {}\".format(datestr),\n                        status=HTTP_400_BAD_REQUEST)\n\n        try:\n            offset = int(filter_params.get(\"offset\", 0))\n            count = int(filter_params.get(\"count\", 10))\n        except ValueError:\n            return Response(\n                \"Invalid value for offset or count\",\n                status=HTTP_400_BAD_REQUEST)\n        return_type = filter_params.get(\"return_type\", \"dict\").lower()\n\n        if count > MAX_JOBS_COUNT:\n            msg = \"Specified count exceeds API MAX_JOBS_COUNT value: {}\".format(MAX_JOBS_COUNT)\n            return Response({\"detail\": msg}, status=HTTP_400_BAD_REQUEST)\n\n        try:\n            repository = Repository.objects.get(name=project)\n        except Repository.DoesNotExist:\n            return Response({\n                \"detail\": \"No project with name {}\".format(project)\n            }, status=HTTP_404_NOT_FOUND)\n        jobs = JobFilter({k: v for (k, v) in filter_params.items()},\n                         queryset=Job.objects.filter(\n                             repository=repository).select_related(\n                                 *self._default_select_related)).qs\n\n        response_body = self._get_job_list_response(jobs, offset, count,\n                                                    return_type)\n        response_body[\"meta\"] = dict(repository=project, offset=offset,\n                                     count=count)\n\n        return Response(response_body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of text log steps associated with this job.", "response": "def text_log_steps(self, request, project, pk=None):\n        \"\"\"\n        Gets a list of steps associated with this job\n        \"\"\"\n        try:\n            job = Job.objects.get(repository__name=project,\n                                  id=pk)\n        except ObjectDoesNotExist:\n            return Response(\"No job with id: {0}\".format(pk), status=HTTP_404_NOT_FOUND)\n\n        textlog_steps = TextLogStep.objects.filter(job=job).order_by(\n            'started_line_number').prefetch_related('errors')\n        return Response(serializers.TextLogStepSerializer(textlog_steps,\n                                                          many=True,\n                                                          read_only=True).data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of TextLogError objects associated with this job.", "response": "def text_log_errors(self, request, project, pk=None):\n        \"\"\"\n        Gets a list of steps associated with this job\n        \"\"\"\n        try:\n            job = Job.objects.get(repository__name=project,\n                                  id=pk)\n        except Job.DoesNotExist:\n            return Response(\"No job with id: {0}\".format(pk),\n                            status=HTTP_404_NOT_FOUND)\n        textlog_errors = (TextLogError.objects\n                          .filter(step__job=job)\n                          .select_related(\"_metadata\",\n                                          \"_metadata__failure_line\")\n                          .prefetch_related(\"classified_failures\", \"matches\")\n                          .order_by('id'))\n        return Response(serializers.TextLogErrorSerializer(textlog_errors,\n                                                           many=True,\n                                                           read_only=True).data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bug_suggestions(self, request, project, pk=None):\n        try:\n            job = Job.objects.get(repository__name=project, id=pk)\n        except ObjectDoesNotExist:\n            return Response(\"No job with id: {0}\".format(pk), status=HTTP_404_NOT_FOUND)\n\n        return Response(get_error_summary(job))", "response": "Returns a list of bug suggestions for this job"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef similar_jobs(self, request, project, pk=None):\n        try:\n            repository = Repository.objects.get(name=project)\n        except Repository.DoesNotExist:\n            return Response({\n                \"detail\": \"No project with name {}\".format(project)\n            }, status=HTTP_404_NOT_FOUND)\n\n        try:\n            job = Job.objects.get(repository=repository, id=pk)\n        except ObjectDoesNotExist:\n            return Response(\"No job with id: {0}\".format(pk),\n                            status=HTTP_404_NOT_FOUND)\n\n        filter_params = request.query_params.copy()\n\n        try:\n            offset = int(filter_params.get(\"offset\", 0))\n            # we don't need a big page size on this endoint,\n            # let's cap it to 50 elements\n            count = int(filter_params.get(\"count\", 50))\n        except ValueError:\n            return Response(\"Invalid value for offset or count\",\n                            status=HTTP_400_BAD_REQUEST)\n\n        return_type = filter_params.get(\"return_type\", \"dict\").lower()\n\n        jobs = JobFilter({k: v for (k, v) in filter_params.items()},\n                         queryset=Job.objects.filter(\n                             job_type_id=job.job_type_id,\n                             repository=repository).exclude(\n                                 id=job.id).select_related(\n                                     *self._default_select_related)).qs\n\n        # similar jobs we want in descending order from most recent\n        jobs = jobs.order_by('-start_time')\n\n        response_body = self._get_job_list_response(jobs, offset, count,\n                                                    return_type)\n        response_body[\"meta\"] = dict(offset=offset, count=count,\n                                     repository=project)\n\n        return Response(response_body)", "response": "Get a list of jobs similar to the one selected."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntry to crossreference the unstructured error lines with the corresponding structured error lines.", "response": "def crossreference_job(job):\n    \"\"\"Try to match the unstructured error lines with the corresponding structured error\n    lines, relying on the fact that serialization of mozlog (and hence errorsummary files)\n    is determinisic so we can reserialize each structured error line and perform an in-order\n    textual match.\n\n    :job: - Job for which to perform the crossreferencing\n    \"\"\"\n    logger.debug(\"Crossreference %s: started\", job.id)\n\n    if job.autoclassify_status >= Job.CROSSREFERENCED:\n        logger.info(\"Job %i already crossreferenced\", job.id)\n        return False\n    try:\n        rv = _crossreference(job)\n    except IntegrityError:\n        job.autoclassify_status = Job.FAILED\n        job.save(update_fields=['autoclassify_status'])\n        logger.warning(\"IntegrityError crossreferencing error lines for job %s\", job.id)\n        return False\n\n    job.autoclassify_status = Job.CROSSREFERENCED\n    job.save(update_fields=['autoclassify_status'])\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating FailureLine Tbpl - formatted - string tuples.", "response": "def structured_iterator(failure_lines):\n    \"\"\"Create FailureLine, Tbpl-formatted-string tuples.\"\"\"\n    summary = partial(failure_line_summary, TbplFormatter())\n    for failure_line in failure_lines:\n        repr_str = summary(failure_line)\n        if repr_str:\n            yield failure_line, repr_str\n\n    while True:\n        yield None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a mozlog formatted error summary string from the given failure line.", "response": "def failure_line_summary(formatter, failure_line):\n    \"\"\"\n    Create a mozlog formatted error summary string from the given failure_line.\n\n    Create a string which can be compared to a TextLogError.line string to see\n    if they match.\n    \"\"\"\n    if failure_line.action == \"test_result\":\n        action = \"test_status\" if failure_line.subtest is not None else \"test_end\"\n    elif failure_line.action == \"truncated\":\n        return\n    else:\n        action = failure_line.action\n\n    try:\n        mozlog_func = getattr(formatter, action)\n    except AttributeError:\n        logger.warning('Unknown mozlog function \"%s\"', action)\n        return\n\n    formatted_log = mozlog_func(failure_line.to_mozlog_format())\n    split_log = first(formatted_log.split(\"\\n\", 1))\n\n    if not split_log:\n        logger.debug('Failed to split log', formatted_log)\n        return\n\n    return split_log.strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the TLS version of a Heroku REDIS_URL string.", "response": "def get_tls_redis_url(redis_url):\n    \"\"\"\n    Returns the TLS version of a Heroku REDIS_URL string.\n\n    Whilst Redis server (like memcached) doesn't natively support TLS, Heroku runs an stunnel\n    daemon on their Redis instances, which can be connected to directly by Redis clients that\n    support TLS (avoiding the need for stunnel on the client). The stunnel port is one higher\n    than the Redis server port, and the informal `rediss://` scheme used to instruct clients\n    to wrap the connection with TLS.\n\n    Will convert 'redis://h:PASSWORD@INSTANCE.compute-1.amazonaws.com:8409'\n          ...to: 'rediss://h:PASSWORD@INSTANCE.compute-1.amazonaws.com:8410?ssl_cert_reqs=none'\n\n    See:\n    https://devcenter.heroku.com/articles/securing-heroku-redis#connecting-directly-to-stunnel\n    \"\"\"\n    url = furl(redis_url)\n    url.port += 1\n    url.scheme += 's'\n    # Disable TLS certificate validation (restoring the behaviour of the older redis-py 2.x),\n    # since for now Heroku Redis uses self-signed certificates:\n    # https://bugzilla.mozilla.org/show_bug.cgi?id=1510000\n    url.args['ssl_cert_reqs'] = 'none'\n    return str(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the log file and return a dict of the log level and the artifact.", "response": "def parse(self):\n        \"\"\"\n        Iterate over each line of the log, running each parser against it.\n\n        Stream lines from the gzip file and run each parser against it,\n        building the ``artifact`` as we go.\n        \"\"\"\n        with make_request(self.url, stream=True) as response:\n            download_size_in_bytes = int(response.headers.get('Content-Length', -1))\n\n            # Temporary annotation of log size to help set thresholds in bug 1295997.\n            newrelic.agent.add_custom_parameter(\n                'unstructured_log_size',\n                download_size_in_bytes\n            )\n            newrelic.agent.add_custom_parameter(\n                'unstructured_log_encoding',\n                response.headers.get('Content-Encoding', 'None')\n            )\n\n            if download_size_in_bytes > MAX_DOWNLOAD_SIZE_IN_BYTES:\n                raise LogSizeException('Download size of %i bytes exceeds limit' % download_size_in_bytes)\n\n            # Lines must be explicitly decoded since `iter_lines()`` returns bytes by default\n            # and we cannot use its `decode_unicode=True` mode, since otherwise Unicode newline\n            # characters such as `\\u0085` (which can appear in test output) are treated the same\n            # as `\\n` or `\\r`, and so split into unwanted additional lines by `iter_lines()`.\n            for line in response.iter_lines():\n                for builder in self.builders:\n                    # Using `replace` to prevent malformed unicode (which might possibly exist\n                    # in test message output) from breaking parsing of the rest of the log.\n                    builder.parse_line(line.decode('utf-8', 'replace'))\n\n        # gather the artifacts from all builders\n        for builder in self.builders:\n            # Run end-of-parsing actions for this parser,\n            # in case the artifact needs clean-up/summarising.\n            builder.finish_parse()\n            name = builder.name\n            artifact = builder.get_artifact()\n            if name == 'performance_data' and not artifact[name]:\n                continue\n            self.artifacts[name] = artifact"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_status(self):\n        '''\n        Gets a summary of what passed/failed for the push\n        '''\n        jobs = Job.objects.filter(push=self).filter(\n            Q(failure_classification__isnull=True) |\n            Q(failure_classification__name='not classified')).exclude(tier=3)\n\n        status_dict = {}\n        for (state, result, total) in jobs.values_list(\n                'state', 'result').annotate(\n                    total=Count('result')):\n            if state == 'completed':\n                status_dict[result] = total\n            else:\n                status_dict[state] = total\n        if 'superseded' in status_dict:\n            # backward compatability for API consumers\n            status_dict['coalesced'] = status_dict['superseded']\n\n        return status_dict", "response": "Gets a summary of what passed or failed for the push\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an option_collection_hash given a list of options", "response": "def calculate_hash(options):\n        \"\"\"returns an option_collection_hash given a list of options\"\"\"\n        options = sorted(list(options))\n        sha_hash = sha1()\n        # equivalent to loop over the options and call sha_hash.update()\n        sha_hash.update(''.join(options).encode('utf-8'))\n        return sha_hash.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes data older than cycle_interval.", "response": "def cycle_data(self, repository, cycle_interval, chunk_size, sleep_time):\n        \"\"\"\n        Delete data older than cycle_interval, splitting the target data into\n        chunks of chunk_size size. Returns the number of result sets deleted\n        \"\"\"\n\n        # Retrieve list of jobs to delete\n        jobs_max_timestamp = datetime.datetime.now() - cycle_interval\n\n        jobs_cycled = 0\n        while True:\n            jobs_chunk = list(self.filter(repository=repository, submit_time__lt=jobs_max_timestamp)\n                                  .values_list('guid', flat=True)[:chunk_size])\n\n            if not jobs_chunk:\n                # no more jobs to cycle, we're done!\n                return jobs_cycled\n\n            # Remove ORM entries for these jobs that don't currently have a\n            # foreign key relation\n            lines = FailureLine.objects.filter(job_guid__in=jobs_chunk)\n\n            if settings.ELASTICSEARCH_URL:\n                # To delete the data from elasticsearch we need the document\n                # id.  However selecting all this data can be rather slow, so\n                # split the job into multiple smaller chunks.\n\n                failures = itertools.chain.from_iterable(\n                    chunked_qs(\n                        lines,\n                        chunk_size=chunk_size,\n                        fields=['id', 'test'],\n                    ),\n                )\n                bulk(failures, action='delete')\n\n            lines.delete()\n\n            # cycle jobs *after* related data has been deleted, to be sure\n            # we don't have any orphan data\n            try:\n                self.filter(guid__in=jobs_chunk).delete()\n            except UnicodeDecodeError as e:\n                # Some TextLogError `line` fields contain invalid Unicode, which causes a\n                # UnicodeDecodeError since Django's .delete() fetches all fields (even those\n                # not required for the delete). As such we delete the offending `TextLogError`s\n                # separately (using only() to prevent pulling in `line`), before trying again.\n                # This can likely be removed once all pre-Python 3 migration `TextLogError`s\n                # have expired (check New Relic Insights at that point to confirm). See:\n                # https://bugzilla.mozilla.org/show_bug.cgi?id=1528710\n                newrelic.agent.record_custom_event('cycle_data UnicodeDecodeError workaround', {\n                    'exception': str(e),\n                })\n                TextLogError.objects.filter(step__job__guid__in=jobs_chunk).only('id').delete()\n                self.filter(guid__in=jobs_chunk).delete()\n\n            jobs_cycled += len(jobs_chunk)\n\n            if sleep_time:\n                # Allow some time for other queries to get through\n                time.sleep(sleep_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_fully_autoclassified(self):\n        if FailureLine.objects.filter(job_guid=self.guid,\n                                      action=\"truncated\").count() > 0:\n            return False\n\n        classified_error_count = TextLogError.objects.filter(\n            _metadata__best_classification__isnull=False,\n            step__job=self).count()\n\n        if classified_error_count == 0:\n            return False\n\n        from treeherder.model.error_summary import get_useful_search_results\n\n        return classified_error_count == len(get_useful_search_results(self))", "response": "Returns whether a job is fully autoclassified."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_fully_verified(self):\n        unverified_errors = TextLogError.objects.filter(\n            _metadata__best_is_verified=False,\n            step__job=self).count()\n\n        if unverified_errors:\n            logger.error(\"Job %r has unverified TextLogErrors\", self)\n            return False\n\n        logger.info(\"Job %r is fully verified\", self)\n        return True", "response": "Determines if this Job is fully verified based on the state of its Errors and TextLogErrorMetadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_after_verification(self, user):\n        if not self.is_fully_verified():\n            return\n\n        classification = 'autoclassified intermittent'\n\n        already_classified = (JobNote.objects.filter(job=self)\n                                             .exclude(failure_classification__name=classification)\n                                             .exists())\n        if already_classified:\n            # Don't add an autoclassification note if a Human already\n            # classified this job.\n            return\n\n        JobNote.create_autoclassify_job_note(job=self, user=user)", "response": "Updates a job s state after being verified by a sheriffJob."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_manual_classification_line(self):\n        try:\n            text_log_error = TextLogError.objects.get(step__job=self)\n        except (TextLogError.DoesNotExist, TextLogError.MultipleObjectsReturned):\n            return None\n\n        # Can this TextLogError be converted into a single \"useful search\"?\n        # FIXME: what is the significance of only one search result here?\n        from treeherder.model.error_summary import get_useful_search_results\n        search_results = get_useful_search_results(self)\n        if len(search_results) != 1:\n            return None\n\n        # Check that we have a related FailureLine\n        failure_line = text_log_error.get_failure_line()\n        if failure_line is None:\n            return None\n\n        # Check our FailureLine is in a state we expect for\n        # auto-classification.\n        if not (failure_line.action == \"test_result\" and\n                failure_line.test and\n                failure_line.status and\n                failure_line.expected):\n            return None\n\n        return text_log_error", "response": "Returns the manual classification line for this Job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the failure type of this Note s Job.", "response": "def _update_failure_type(self):\n        \"\"\"\n        Updates the failure type of this Note's Job.\n\n        Set the linked Job's failure type to that of the most recent JobNote or\n        set to Not Classified if there are no JobNotes.\n\n        This is called when JobNotes are created (via .save()) and deleted (via\n        .delete()) and is used to resolved the FailureClassification which has\n        been denormalised onto Job.\n        \"\"\"\n        # update the job classification\n        note = JobNote.objects.filter(job=self.job).order_by('-created').first()\n        if note:\n            self.job.failure_classification_id = note.failure_classification.id\n        else:\n            self.job.failure_classification_id = FailureClassification.objects.get(name='not classified').id\n        self.job.save()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ensure_classification(self):\n        # if this note was automatically filed, don't update the auto-classification information\n        if not self.user:\n            return\n\n        # if the failure type isn't intermittent, ignore\n        if self.failure_classification.name not in [\"intermittent\", \"intermittent needs filing\"]:\n            return\n\n        # if the linked Job has more than one TextLogError, ignore\n        text_log_error = self.job.get_manual_classification_line()\n        if not text_log_error:\n            return\n\n        # evaluate the QuerySet here so it can be used when creating new_bugs below\n        existing_bugs = list(ClassifiedFailure.objects.filter(error_matches__text_log_error=text_log_error)\n                                                      .values_list('bug_number', flat=True))\n\n        new_bugs = (self.job.bugjobmap_set.exclude(bug_id__in=existing_bugs)\n                                          .values_list('bug_id', flat=True))\n\n        if not new_bugs:\n            return\n\n        # Create Match instances for each new bug\n        for bug_number in new_bugs:\n            classification, _ = ClassifiedFailure.objects.get_or_create(bug_number=bug_number)\n            text_log_error.create_match(\"ManualDetector\", classification)\n\n        # if there's only one new bug and no existing ones, verify it\n        if len(new_bugs) == 1 and not existing_bugs:\n            text_log_error.verify_classification(classification)", "response": "Ensures that a single TextLogError has related bugs and if so creates Match instances for each new bug."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_autoclassify_job_note(self, job, user=None):\n        # Only insert bugs for verified failures since these are automatically\n        # mirrored to ES and the mirroring can't be undone\n        # TODO: Decide whether this should change now that we're no longer mirroring.\n        bug_numbers = set(ClassifiedFailure.objects\n                          .filter(best_for_errors__text_log_error__step__job=job,\n                                  best_for_errors__best_is_verified=True)\n                          .exclude(bug_number=None)\n                          .exclude(bug_number=0)\n                          .values_list('bug_number', flat=True))\n\n        existing_maps = set(BugJobMap.objects.filter(bug_id__in=bug_numbers)\n                                             .values_list('bug_id'))\n\n        for bug_number in (bug_numbers - existing_maps):\n            BugJobMap.objects.create(job_id=job.id, bug_id=bug_number, user=user)\n\n        # if user is not specified, then this is an autoclassified job note and\n        # we should mark it as such\n        classification_name = 'intermittent' if user else 'autoclassified intermittent'\n        classification = FailureClassification.objects.get(name=classification_name)\n\n        return JobNote.objects.create(job=job,\n                                      failure_classification=classification,\n                                      user=user,\n                                      text=\"\")", "response": "Create a JobNote that may be automatically classified for the given Job."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unstructured_bugs(self):\n        components = self._serialized_components()\n        if not components:\n            return []\n\n        from treeherder.model.error_summary import get_useful_search_results\n        job = Job.objects.get(guid=self.job_guid)\n        rv = []\n        ids_seen = set()\n        for item in get_useful_search_results(job):\n            if all(component in item[\"search\"] for component in components):\n                for suggestion in itertools.chain(item[\"bugs\"][\"open_recent\"],\n                                                  item[\"bugs\"][\"all_others\"]):\n                    if suggestion[\"id\"] not in ids_seen:\n                        ids_seen.add(suggestion[\"id\"])\n                        rv.append(suggestion)\n\n        return rv", "response": "Get the bugs that match this line in the Bug Suggestions artifact for this job."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_mozlog_format(self):\n        data = {\n            \"action\": self.action,\n            \"line_number\": self.line,\n            \"test\": self.test,\n            \"subtest\": self.subtest,\n            \"status\": self.status,\n            \"expected\": self.expected,\n            \"message\": self.message,\n            \"signature\": self.signature,\n            \"level\": self.level,\n            \"stack\": self.stack,\n            \"stackwalk_stdout\": self.stackwalk_stdout,\n            \"stackwalk_stderr\": self.stackwalk_stderr,\n        }\n\n        # Remove empty values\n        data = {k: v for k, v in data.items() if v}\n\n        return data", "response": "Convert a FailureLine into a mozlog formatted dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the bug number of this Classified Failure.", "response": "def set_bug(self, bug_number):\n        \"\"\"\n        Set the bug number of this Classified Failure\n\n        If an existing ClassifiedFailure exists with the same bug number\n        replace this instance with the existing one.\n        \"\"\"\n        if bug_number == self.bug_number:\n            return self\n\n        other = ClassifiedFailure.objects.filter(bug_number=bug_number).first()\n        if not other:\n            self.bug_number = bug_number\n            self.save(update_fields=['bug_number'])\n            return self\n\n        self.replace_with(other)\n\n        return other"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreplaces this instance with the given other.", "response": "def replace_with(self, other):\n        \"\"\"\n        Replace this instance with the given other.\n\n        Deletes stale Match objects and updates related TextLogErrorMetadatas'\n        best_classifications to point to the given other.\n        \"\"\"\n        match_ids_to_delete = list(self.update_matches(other))\n        TextLogErrorMatch.objects.filter(id__in=match_ids_to_delete).delete()\n\n        # Update best classifications\n        self.best_for_errors.update(best_classification=other)\n\n        self.delete()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate this instance s Matches to point to the given other s Matches.", "response": "def update_matches(self, other):\n        \"\"\"\n        Update this instance's Matches to point to the given other's Matches.\n\n        Find Matches with the same TextLogError as our Matches, updating their\n        score if less than ours and mark our matches for deletion.\n\n        If there are no other matches, update ours to point to the other\n        ClassifiedFailure.\n        \"\"\"\n        for match in self.error_matches.all():\n            other_matches = TextLogErrorMatch.objects.filter(\n                classified_failure=other,\n                text_log_error=match.text_log_error,\n            )\n\n            if not other_matches:\n                match.classified_failure = other\n                match.save(update_fields=['classified_failure'])\n                continue\n\n            # if any of our matches have higher scores than other's matches,\n            # overwrite with our score.\n            other_matches.filter(score__lt=match.score).update(score=match.score)\n\n            yield match.id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a TextLogErrorMatch instance for this error.", "response": "def create_match(self, matcher_name, classification):\n        \"\"\"\n        Create a TextLogErrorMatch instance\n\n        Typically used for manual \"matches\" or tests.\n        \"\"\"\n        if classification is None:\n            classification = ClassifiedFailure.objects.create()\n\n        TextLogErrorMatch.objects.create(\n            text_log_error=self,\n            classified_failure=classification,\n            matcher_name=matcher_name,\n            score=1,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verify_classification(self, classification):\n        if classification not in self.classified_failures.all():\n            self.create_match(\"ManualDetector\", classification)\n\n        # create a TextLogErrorMetadata instance for this TextLogError if it\n        # doesn't exist.  We can't use update_or_create here since OneToOne\n        # relations don't use an object manager so a missing relation is simply\n        # None as opposed to RelatedManager.\n        if self.metadata is None:\n            TextLogErrorMetadata.objects.create(text_log_error=self,\n                                                best_classification=classification,\n                                                best_is_verified=True)\n        else:\n            self.metadata.best_classification = classification\n            self.metadata.best_is_verified = True\n            self.metadata.save(update_fields=['best_classification', 'best_is_verified'])\n\n        self.metadata.failure_line.elastic_search_insert()\n\n        # Send event to NewRelic when a verifing an autoclassified failure.\n        match = self.matches.filter(classified_failure=classification).first()\n        if not match:\n            return\n\n        newrelic.agent.record_custom_event('user_verified_classification', {\n            'matcher': match.matcher_name,\n            'job_id': self.id,\n        })", "response": "Verify the given ClassifiedFailure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a bugzilla bug with passed params", "response": "def create_bug(self, request):\n        \"\"\"\n        Create a bugzilla bug with passed params\n        \"\"\"\n        if settings.BUGFILER_API_KEY is None:\n            return Response({\"failure\": \"Bugzilla API key not set!\"},\n                            status=HTTP_400_BAD_REQUEST)\n\n        params = request.data\n\n        # Arbitrarily cap crash signatures at 2048 characters to prevent perf issues on bmo\n        crash_signature = params.get(\"crash_signature\")\n        if crash_signature and len(crash_signature) > 2048:\n            return Response({\"failure\": \"Crash signature can't be more than 2048 characters.\"},\n                            status=HTTP_400_BAD_REQUEST)\n\n        description = u\"**Filed by:** {}\\n{}\".format(\n            request.user.email.replace('@', \" [at] \"),\n            params.get(\"comment\", \"\")\n        ).encode(\"utf-8\")\n        summary = params.get(\"summary\").encode(\"utf-8\").strip()\n        url = settings.BUGFILER_API_URL + \"/rest/bug\"\n        headers = {\n            'x-bugzilla-api-key': settings.BUGFILER_API_KEY,\n            'Accept': 'application/json'\n        }\n        data = {\n            'product': params.get(\"product\"),\n            'component': params.get(\"component\"),\n            'summary': summary,\n            'keywords': params.get(\"keywords\"),\n            'blocks': params.get(\"blocks\"),\n            'depends_on': params.get(\"depends_on\"),\n            'see_also': params.get(\"see_also\"),\n            'version': params.get(\"version\"),\n            'cf_crash_signature': params.get(\"crash_signature\"),\n            'severity': params.get(\"severity\"),\n            'priority': params.get(\"priority\"),\n            'description': description,\n            'comment_tags': \"treeherder\",\n        }\n\n        try:\n            response = make_request(url, method='POST', headers=headers, json=data)\n        except requests.exceptions.HTTPError as e:\n            try:\n                message = e.response.json()['message']\n            except (ValueError, KeyError):\n                message = e.response.text\n            return Response({\"failure\": message}, status=HTTP_400_BAD_REQUEST)\n\n        return Response({\"success\": response.json()[\"id\"]})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chunked_qs(qs, chunk_size=10000, fields=None):\n    min_id = 0\n\n    while True:\n        chunk = qs.filter(id__gt=min_id).order_by('id')\n\n        if fields is not None:\n            chunk = chunk.only(*fields)\n\n        # Cast to a list to execute the QuerySet here and allow us to get the\n        # last ID when updating min_id.  We can't use .last() later as it\n        # ignores the slicing we do.\n        rows = list(chunk[:chunk_size])\n\n        total = len(rows)\n\n        if total < 1:\n            break\n\n        yield rows\n\n        # update the minimum ID for next iteration\n        min_id = rows[-1].id", "response": "Generator to iterate over the given QuerySet in chunks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new relation between a job and a bug.", "response": "def create(self, request, project):\n        \"\"\"Add a new relation between a job and a bug.\"\"\"\n        job_id = int(request.data['job_id'])\n        bug_id = int(request.data['bug_id'])\n\n        try:\n            BugJobMap.create(\n                job_id=job_id,\n                bug_id=bug_id,\n                user=request.user,\n            )\n            message = \"Bug job map saved\"\n        except IntegrityError:\n            message = \"Bug job map skipped: mapping already exists\"\n\n        return Response({\"message\": message})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef destroy(self, request, project, pk=None):\n        job_id, bug_id = map(int, pk.split(\"-\"))\n        job = Job.objects.get(repository__name=project, id=job_id)\n        BugJobMap.objects.filter(job=job, bug_id=bug_id).delete()\n\n        return Response({\"message\": \"Bug job map deleted\"})", "response": "Delete bug - job - map entry. pk is a composite key in the formBugJobMap pk - job_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a bug - job - map entry. pk is a composite key in the form BugJobMap_id - bug_id - job_id", "response": "def retrieve(self, request, project, pk=None):\n        \"\"\"\n        Retrieve a bug-job-map entry. pk is a composite key in the form\n        bug_id-job_id\n        \"\"\"\n        job_id, bug_id = map(int, pk.split(\"-\"))\n        job = Job.objects.get(repository__name=project, id=job_id)\n        try:\n            bug_job_map = BugJobMap.objects.get(job=job, bug_id=bug_id)\n            serializer = BugJobMapSerializer(bug_job_map)\n\n            return Response(serializer.data)\n        except BugJobMap.DoesNotExist:\n            return Response(\"Object not found\", status=HTTP_404_NOT_FOUND)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_unicode_character_to_ascii_repr(match_obj):\n    match = match_obj.group(0)\n    code_point = ord(match)\n\n    hex_repr = hex(code_point)\n    hex_code_point = hex_repr[2:]\n\n    hex_value = hex_code_point.zfill(6).upper()\n\n    return '<U+{}>'.format(hex_value)", "response": "Converts a matched pattern from a unicode character to an ASCII representation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_push_logs():\n    for repo in Repository.objects.filter(dvcs_type='hg',\n                                          active_status=\"active\"):\n        fetch_hg_push_log.apply_async(\n            args=(repo.name, repo.url),\n            queue='pushlog'\n        )", "response": "Run several fetch_hg_push_log subtasks one per repository"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch_hg_push_log(repo_name, repo_url):\n    newrelic.agent.add_custom_parameter(\"repo_name\", repo_name)\n    process = HgPushlogProcess()\n    process.run(repo_url + '/json-pushes/?full=1&version=2', repo_name)", "response": "Run a HgPushlog etl process\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstoring the jobs pending from pulse exchanges and loads them into JobLoader.", "response": "def store_pulse_jobs(pulse_job, exchange, routing_key):\n    \"\"\"\n    Fetches the jobs pending from pulse exchanges and loads them.\n    \"\"\"\n    newrelic.agent.add_custom_parameter(\"exchange\", exchange)\n    newrelic.agent.add_custom_parameter(\"routing_key\", routing_key)\n\n    JobLoader().process_job(pulse_job)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_pulse_pushes(body, exchange, routing_key):\n    newrelic.agent.add_custom_parameter(\"exchange\", exchange)\n    newrelic.agent.add_custom_parameter(\"routing_key\", routing_key)\n\n    PushLoader().process(body, exchange)", "response": "Loads the push data from pulse exchanges."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstore the failure lines from a structured errorsummary file.", "response": "def store_failure_lines(job_log):\n    \"\"\"Store the failure lines from a log corresponding to the structured\n    errorsummary file.\"\"\"\n    logger.debug('Running store_failure_lines for job %s', job_log.job.id)\n    failureline.store_failure_lines(job_log)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a specific note from the database.", "response": "def retrieve(self, request, project, pk=None):\n        \"\"\"\n        GET method implementation for a note detail\n\n        \"\"\"\n        try:\n            serializer = JobNoteSerializer(JobNote.objects.get(id=pk))\n            return Response(serializer.data)\n        except JobNote.DoesNotExist:\n            return Response(\"No note with id: {0}\".format(pk), status=HTTP_404_NOT_FOUND)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting method implementation for list view of all job notes in a project", "response": "def list(self, request, project):\n        \"\"\"\n        GET method implementation for list view\n        job_id -- Mandatory filter indicating which job these notes belong to.\n        \"\"\"\n\n        job_id = request.query_params.get('job_id')\n        if not job_id:\n            raise ParseError(detail=\"The job_id parameter is mandatory for this endpoint\")\n        try:\n            job_id = int(job_id)\n        except ValueError:\n            raise ParseError(detail=\"The job_id parameter must be an integer\")\n\n        job = Job.objects.get(repository__name=project, id=job_id)\n        serializer = JobNoteSerializer(JobNote.objects.filter(job=job),\n                                       many=True)\n        return Response(serializer.data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nposting method implementation TracingEnabled", "response": "def create(self, request, project):\n        \"\"\"\n        POST method implementation\n        \"\"\"\n        JobNote.objects.create(\n            job=Job.objects.get(repository__name=project,\n                                id=int(request.data['job_id'])),\n            failure_classification_id=int(request.data['failure_classification_id']),\n            user=request.user,\n            text=request.data.get('text', ''))\n\n        return Response(\n            {'message': 'note stored for job {0}'.format(\n                request.data['job_id']\n            )}\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef destroy(self, request, project, pk=None):\n        try:\n            note = JobNote.objects.get(id=pk)\n            note.delete()\n            return Response({\"message\": \"Note deleted\"})\n        except JobNote.DoesNotExist:\n            return Response(\"No note with id: {0}\".format(pk),\n                            status=HTTP_404_NOT_FOUND)", "response": "Delete a note entry"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_text_log_artifacts(job_log):\n\n    # parse a log given its url\n    artifact_bc = ArtifactBuilderCollection(job_log.url)\n    artifact_bc.parse()\n\n    artifact_list = []\n    for name, artifact in artifact_bc.artifacts.items():\n        artifact_list.append({\n            \"job_guid\": job_log.job.guid,\n            \"name\": name,\n            \"type\": 'json',\n            \"blob\": json.dumps(artifact)\n        })\n\n    return artifact_list", "response": "Generate a set of artifacts by parsing from the raw text log."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_log_artifacts(job_log):\n    logger.debug(\"Downloading/parsing log for log %s\", job_log.id)\n\n    try:\n        artifact_list = extract_text_log_artifacts(job_log)\n    except LogSizeException as e:\n        job_log.update_status(JobLog.SKIPPED_SIZE)\n        logger.warning('Skipping parsing log for %s: %s', job_log.id, e)\n        return\n    except Exception as e:\n        job_log.update_status(JobLog.FAILED)\n\n        # Unrecoverable http error (doesn't exist or permission denied).\n        # Apparently this can happen somewhat often with taskcluster if\n        # the job fails (bug 1154248), so just warn rather than raising,\n        # to prevent the noise/load from retrying.\n        if isinstance(e, HTTPError) and e.response.status_code in (403, 404):\n            logger.warning(\"Unable to retrieve log for %s: %s\", job_log.id, e)\n            return\n\n        logger.error(\"Failed to download/parse log for %s: %s\", job_log.id, e)\n        raise\n\n    try:\n        serialized_artifacts = serialize_artifact_json_blobs(artifact_list)\n        store_job_artifacts(serialized_artifacts)\n        job_log.update_status(JobLog.PARSED)\n        logger.debug(\"Stored artifact for %s %s\", job_log.job.repository.name,\n                     job_log.job.id)\n    except Exception as e:\n        logger.error(\"Failed to store parsed artifact for %s: %s\", job_log.id, e)\n        raise", "response": "Post a list of artifacts to a job."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a list of bug suggestions for a job.", "response": "def get_error_summary(job):\n    \"\"\"\n    Create a list of bug suggestions for a job.\n\n    Caches the results if there are any.\n    \"\"\"\n    cache_key = 'error-summary-{}'.format(job.id)\n    cached_error_summary = cache.get(cache_key)\n    if cached_error_summary is not None:\n        return cached_error_summary\n\n    # don't cache or do anything if we have no text log errors to get\n    # results for\n    errors = TextLogError.objects.filter(step__job=job)\n    if not errors:\n        return []\n\n    # cache terms generated from error line to save excessive querying\n    term_cache = {}\n\n    error_summary = [bug_suggestions_line(err, term_cache) for err in errors]\n    cache.set(cache_key, error_summary, BUG_SUGGESTION_CACHE_TIMEOUT)\n\n    return error_summary"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets Bug suggestions for a given error line.", "response": "def bug_suggestions_line(err, term_cache=None):\n    \"\"\"\n    Get Bug suggestions for a given TextLogError (err).\n\n    Tries to extract a search term from a clean version of the given error's\n    line.  We build a search term from the cleaned line and use that to search\n    for bugs.  Returns a dictionary with the cleaned line, the generated search\n    term, and any bugs found with said search term.\n    \"\"\"\n    if term_cache is None:\n        term_cache = {}\n\n    # remove the mozharness prefix\n    clean_line = get_mozharness_substring(err.line)\n\n    # get a meaningful search term out of the error line\n    search_term = get_error_search_term(clean_line)\n    bugs = dict(open_recent=[], all_others=[])\n\n    # collect open recent and all other bugs suggestions\n    search_terms = []\n    if search_term:\n        search_terms.append(search_term)\n        if search_term not in term_cache:\n            term_cache[search_term] = Bugscache.search(search_term)\n        bugs = term_cache[search_term]\n\n    if not bugs or not (bugs['open_recent'] or\n                        bugs['all_others']):\n        # no suggestions, try to use\n        # the crash signature as search term\n        crash_signature = get_crash_signature(clean_line)\n        if crash_signature:\n            search_terms.append(crash_signature)\n            if crash_signature not in term_cache:\n                term_cache[crash_signature] = Bugscache.search(crash_signature)\n            bugs = term_cache[crash_signature]\n\n    # TODO: Rename 'search' to 'error_text' or similar, since that's\n    # closer to what it actually represents (bug 1091060).\n    return {\n        \"search\": clean_line,\n        \"search_terms\": search_terms,\n        \"bugs\": bugs,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a search term from the given error_line string.", "response": "def get_error_search_term(error_line):\n    \"\"\"\n    Generate a search term from the given error_line string.\n\n    Attempt to build a search term that will yield meaningful results when used\n    in a MySQL FTS query.\n    \"\"\"\n    if not error_line:\n        return None\n\n    # This is strongly inspired by\n    # https://hg.mozilla.org/webtools/tbpl/file/tip/php/inc/AnnotatedSummaryGenerator.php#l73\n\n    tokens = error_line.split(\" | \")\n    search_term = None\n\n    if len(tokens) >= 3:\n        # If this is process output then discard the token with the PID\n        if len(tokens) > 3 and OUTPUT_RE.match(tokens[0]):\n            tokens = tokens[1:]\n        # it's in the \"FAILURE-TYPE | testNameOrFilePath | message\" type format.\n        test_name_or_path = tokens[1]\n        message = tokens[2]\n        # Leak failure messages are of the form:\n        # leakcheck | .*\\d+ bytes leaked (Object-1, Object-2, Object-3, ...)\n        match = LEAK_RE.search(message)\n        if match:\n            search_term = match.group(1) if match.group(1) is not None else match.group(2)\n        else:\n            # For reftests, remove the reference path from the tokens as this is\n            # not very unique\n            test_name_or_path = REFTEST_RE.sub(\"\", test_name_or_path)\n            for splitter in (\"/\", \"\\\\\"):\n                # if this is a path, we are interested in the last part\n                test_name_or_path = test_name_or_path.split(splitter)[-1]\n            search_term = test_name_or_path\n\n    # If the failure line was not in the pipe symbol delimited format or the search term\n    # will likely return too many (or irrelevant) results (eg: too short or matches terms\n    # on the blacklist), then we fall back to searching for the entire failure line if\n    # it is suitable.\n    if not (search_term and is_helpful_search_term(search_term)):\n        if is_helpful_search_term(error_line):\n            search_term = error_line\n        else:\n            search_term = None\n\n    # Searching for extremely long search terms is undesirable, since:\n    # a) Bugzilla's max summary length is 256 characters, and once \"Intermittent \"\n    # and platform/suite information is prefixed, there are even fewer characters\n    # left for us to use for the failure string against which we need to match.\n    # b) For long search terms, the additional length does little to prevent against\n    # false positives, but means we're more susceptible to false negatives due to\n    # run-to-run variances in the error messages (eg paths, process IDs).\n    if search_term:\n        search_term = search_term[:100]\n\n    return search_term"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries to get a crash signature from the given error_line string.", "response": "def get_crash_signature(error_line):\n    \"\"\"Try to get a crash signature from the given error_line string.\"\"\"\n    search_term = None\n    match = CRASH_RE.match(error_line)\n    if match and is_helpful_search_term(match.group(1)):\n        search_term = match.group(1)\n    return search_term"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_helpful_search_term(search_term):\n    # Search terms that will match too many bug summaries\n    # and so not result in useful suggestions.\n    search_term = search_term.strip()\n\n    blacklist = [\n        'automation.py',\n        'remoteautomation.py',\n        'Shutdown',\n        'undefined',\n        'Main app process exited normally',\n        'Traceback (most recent call last):',\n        'Return code: 0',\n        'Return code: 1',\n        'Return code: 2',\n        'Return code: 9',\n        'Return code: 10',\n        'mozalloc_abort(char const*)',\n        'mozalloc_abort',\n        'Exiting 1',\n        'Exiting 9',\n        'CrashingThread(void *)',\n        'libSystem.B.dylib + 0xd7a',\n        'linux-gate.so + 0x424',\n        'TypeError: content is null',\n        'leakcheck',\n        'ImportError: No module named pygtk',\n        '# TBPL FAILURE #'\n    ]\n\n    return len(search_term) > 4 and search_term not in blacklist", "response": "Decide if the given search term string is helpful or not."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all job failures annotated with fixed by commit.", "response": "def get_failures_fixed_by_commit():\n    \"\"\" Return all job failures annotated with \"fixed by commit\" grouped by reason given for annotation.\n\n        It returns a dictionary with a revision or bug ID as the key (bug ID is used for\n        intermittent failures and the revision is used for real failures). For SETA's purposes\n        we only care about revisions (real failures).\n        The failures for *real failures* will contain all jobs that have been starred as \"fixed by commit\".\n\n        Notice that the data does not tell you on which repository a root failure was fixed.\n\n        For instance, in the raw data you might see a reference to 9fa614d8310d which is a back out\n        and it is referenced by 12 starred jobs:\n            https://treeherder.mozilla.org/#/jobs?repo=autoland&filter-searchStr=android%20debug%20cpp&tochange=9fa614d8310db9aabe85cc3c3cff6281fe1edb0c\n        The raw data will show those 12 jobs.\n\n        The returned data will look like this:\n        {\n           \"44d29bac3654\": [\n              [\"android-4-0-armv7-api15\", \"opt\", \"android-lint\"],\n              [\"android-4-0-armv7-api15\", \"opt\", \"android-api-15-gradle-dependencies\"],\n            ]\n        }\n    \"\"\"\n    failures = defaultdict(list)\n    option_collection_map = models.OptionCollection.objects.get_option_collection_map()\n\n    fixed_by_commit_data_set = models.JobNote.objects.filter(\n            failure_classification=2,\n            created__gt=timezone.now() - timedelta(days=SETA_FIXED_BY_COMMIT_DAYS),\n            text__isnull=False,\n            job__repository__name__in=SETA_FIXED_BY_COMMIT_REPOS\n        ).exclude(\n            job__signature__build_platform__in=SETA_UNSUPPORTED_PLATFORMS\n        ).exclude(\n            text=\"\"\n        ).select_related('job', 'job__signature', 'job__job_type')\n\n    # check if at least one fixed by commit job meets our requirements without populating queryset\n    if not fixed_by_commit_data_set.exists():\n        logger.warning(\"We couldn't find any fixed-by-commit jobs\")\n        return failures\n\n    # now process the fixed by commit jobs in batches using django's queryset iterator\n    for job_note in fixed_by_commit_data_set.iterator():\n        # if we have http://hg.mozilla.org/rev/<rev> and <rev>, we will only use <rev>\n        revision_id = job_note.text.strip('/')\n        revision_id = revision_id.split('/')[-1]\n\n        # This prevents the empty string case and ignores bug ids\n        if not revision_id or len(revision_id) < 12:\n            continue\n\n        # We currently don't guarantee that text is actually a revision\n        # Even if not perfect the main idea is that a bunch of jobs were annotated with\n        # a unique identifier. The assumption is that the text is unique\n        #\n        # I've seen these values being used:\n        #  * 12 char revision\n        #  * 40 char revision\n        #  * link to revision on hg\n        #  * revisionA & revisionB\n        #  * should be fixed by <revision>\n        #  * bug id\n        #\n        # Note that if some jobs are annotated with the 12char revision and others with the\n        # 40char revision we will have two disjunct set of failures\n        #\n        # Some of this will be improved in https://bugzilla.mozilla.org/show_bug.cgi?id=1323536\n\n        try:\n            # check if jobtype is supported by SETA (see treeherder/seta/settings.py)\n            if job_note.job.signature.build_system_type != 'buildbot':\n                if not job_note.job.job_type.name.startswith(tuple(SETA_SUPPORTED_TC_JOBTYPES)):\n                    continue\n\n            testtype = parse_testtype(\n                build_system_type=job_note.job.signature.build_system_type,  # e.g. taskcluster\n                job_type_name=job_note.job.job_type.name,  # e.g. Mochitest\n                platform_option=job_note.job.get_platform_option(option_collection_map),  # e.g. 'opt'\n                ref_data_name=job_note.job.signature.name,  # buildername or task label\n            )\n\n            if testtype:\n                if is_job_blacklisted(testtype):\n                    continue\n            else:\n                logger.warning('We were unable to parse %s/%s',\n                               job_note.job.job_type.name, job_note.job.signature.name)\n                continue\n\n            # we now have a legit fixed-by-commit job failure\n            failures[revision_id].append(unique_key(\n                testtype=testtype,\n                buildtype=job_note.job.get_platform_option(option_collection_map),  # e.g. 'opt'\n                platform=job_note.job.signature.build_platform\n            ))\n        except models.Job.DoesNotExist:\n            logger.warning('job_note %s has no job associated to it', job_note.id)\n            continue\n\n    logger.warning(\"Number of fixed_by_commit revisions: %s\", len(failures))\n    return failures"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef score_matches(matches, score_multiplier=(1, 1)):\n    for match in matches:\n        # generate a new score from the current match\n        dividend, divisor = score_multiplier\n        score = match.score * dividend / divisor\n\n        yield (score, match.classified_failure_id)", "response": "Generates a score for each match in a given set of TextLogErrorMatches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding items from an iterable within a given time budget.", "response": "def time_boxed(func, iterable, time_budget, *args):\n    \"\"\"\n    Apply a function to the items of an iterable within a given time budget.\n\n    Loop the given iterable, calling the given function on each item. The expended\n    time is compared to the given time budget after each iteration.\n    \"\"\"\n    time_budget = time_budget / 1000  # budget in milliseconds\n    start = time.time()\n\n    for thing in iterable:\n        yield func(thing, *args)\n\n        end = time.time() - start\n        if end > time_budget:\n            # Putting the condition at the end of the loop ensures that we\n            # always run it once, which is useful for testing\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming a pulse job into a log entry that can be written to disk.", "response": "def transform(self, pulse_job):\n        \"\"\"\n        Transform a pulse job into a job that can be written to disk.  Log\n        References and artifacts will also be transformed and loaded with the\n        job.\n\n        We can rely on the structure of ``pulse_job`` because it will\n        already have been validated against the JSON Schema at this point.\n        \"\"\"\n        job_guid = pulse_job[\"taskId\"]\n\n        x = {\n            \"job\": {\n                \"job_guid\": job_guid,\n                \"name\": pulse_job[\"display\"].get(\"jobName\", \"unknown\"),\n                \"job_symbol\": self._get_job_symbol(pulse_job),\n                \"group_name\": pulse_job[\"display\"].get(\"groupName\", \"unknown\"),\n                \"group_symbol\": pulse_job[\"display\"].get(\"groupSymbol\"),\n                \"product_name\": pulse_job.get(\"productName\", \"unknown\"),\n                \"state\": pulse_job[\"state\"],\n                \"result\": self._get_result(pulse_job),\n                \"reason\": pulse_job.get(\"reason\", \"unknown\"),\n                \"who\": pulse_job.get(\"owner\", \"unknown\"),\n                \"build_system_type\": pulse_job[\"buildSystem\"],\n                \"tier\": pulse_job.get(\"tier\", 1),\n                \"machine\": self._get_machine(pulse_job),\n                \"option_collection\": self._get_option_collection(pulse_job),\n                \"log_references\": self._get_log_references(pulse_job),\n                \"artifacts\": self._get_artifacts(pulse_job, job_guid),\n            },\n            \"superseded\": pulse_job.get(\"coalesced\", []),\n            \"revision\": pulse_job[\"origin\"][\"revision\"]\n        }\n\n        # some or all the time fields may not be present in some cases\n        for k, v in self.TIME_FIELD_MAP.items():\n            if v in pulse_job:\n                x[\"job\"][k] = to_timestamp(pulse_job[v])\n\n        # if only one platform is given, use it.\n        default_platform = pulse_job.get(\n            \"buildMachine\",\n            pulse_job.get(\"runMachine\", {}))\n\n        for k, v in self.PLATFORM_FIELD_MAP.items():\n            platform_src = pulse_job[v] if v in pulse_job else default_platform\n            x[\"job\"][k] = self._get_platform(platform_src)\n\n        # add some taskcluster metadata if it's available\n        # currently taskcluster doesn't pass the taskId directly, so we'll\n        # derive it from the guid, where it is stored in uncompressed\n        # guid form of a slug (see: https://github.com/taskcluster/slugid)\n        # FIXME: add support for processing the taskcluster information\n        # properly, when it's available:\n        # https://bugzilla.mozilla.org/show_bug.cgi?id=1323110#c7\n        try:\n            (decoded_task_id, retry_id) = job_guid.split('/')\n            # As of slugid v2, slugid.encode() returns a string not bytestring under Python 3.\n            real_task_id = slugid.encode(uuid.UUID(decoded_task_id))\n            x[\"job\"].update({\n                \"taskcluster_task_id\": real_task_id,\n                \"taskcluster_retry_id\": int(retry_id)\n            })\n        # TODO: Figure out what exception types we actually expect here.\n        except Exception:\n            pass\n\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets method for list of push records with revisions", "response": "def list(self, request, project):\n        \"\"\"\n        GET method for list of ``push`` records with revisions\n        \"\"\"\n        # What is the upper limit on the number of pushes returned by the api\n        MAX_PUSH_COUNT = 1000\n\n        # make a mutable copy of these params\n        filter_params = request.query_params.copy()\n\n        # This will contain some meta data about the request and results\n        meta = {}\n\n        # support ranges for date as well as revisions(changes) like old tbpl\n        for param in [\"fromchange\", \"tochange\", \"startdate\", \"enddate\", \"revision\", \"commit_revision\"]:\n            v = filter_params.get(param, None)\n            if v:\n                del filter_params[param]\n                meta[param] = v\n\n        try:\n            repository = Repository.objects.get(name=project)\n        except Repository.DoesNotExist:\n            return Response({\n                \"detail\": \"No project with name {}\".format(project)\n            }, status=HTTP_404_NOT_FOUND)\n\n        pushes = Push.objects.filter(repository=repository).order_by('-time')\n\n        for (param, value) in meta.items():\n            if param == 'fromchange':\n                revision_field = 'revision__startswith' if len(value) < 40 else 'revision'\n                filter_kwargs = {revision_field: value, 'repository': repository}\n                frompush_time = Push.objects.values_list('time', flat=True).get(\n                    **filter_kwargs)\n                pushes = pushes.filter(time__gte=frompush_time)\n                filter_params.update({\n                    \"push_timestamp__gte\": to_timestamp(frompush_time)\n                })\n                self.report_if_short_revision(param, value)\n\n            elif param == 'tochange':\n                revision_field = 'revision__startswith' if len(value) < 40 else 'revision'\n                filter_kwargs = {revision_field: value, 'repository': repository}\n                topush_time = Push.objects.values_list('time', flat=True).get(\n                    **filter_kwargs)\n                pushes = pushes.filter(time__lte=topush_time)\n                filter_params.update({\n                    \"push_timestamp__lte\": to_timestamp(topush_time)\n                })\n                self.report_if_short_revision(param, value)\n\n            elif param == 'startdate':\n                pushes = pushes.filter(time__gte=to_datetime(value))\n                filter_params.update({\n                    \"push_timestamp__gte\": to_timestamp(to_datetime(value))\n                })\n            elif param == 'enddate':\n                real_end_date = to_datetime(value) + datetime.timedelta(days=1)\n                pushes = pushes.filter(time__lte=real_end_date)\n                filter_params.update({\n                    \"push_timestamp__lt\": to_timestamp(real_end_date)\n                })\n            elif param == 'revision':\n                # revision must be the tip revision of the push itself\n                revision_field = 'revision__startswith' if len(value) < 40 else 'revision'\n                filter_kwargs = {revision_field: value}\n                pushes = pushes.filter(**filter_kwargs)\n                rev_key = \"revisions_long_revision\" \\\n                          if len(meta['revision']) == 40 else \"revisions_short_revision\"\n                filter_params.update({rev_key: meta['revision']})\n                self.report_if_short_revision(param, value)\n            elif param == 'commit_revision':\n                # revision can be either the revision of the push itself, or\n                # any of the commits it refers to\n                pushes = pushes.filter(commits__revision=value)\n                self.report_if_short_revision(param, value)\n\n        for param in ['push_timestamp__lt', 'push_timestamp__lte',\n                      'push_timestamp__gt', 'push_timestamp__gte']:\n            if filter_params.get(param):\n                # translate push timestamp directly into a filter\n                try:\n                    value = datetime.datetime.fromtimestamp(\n                        float(filter_params.get(param)))\n                except ValueError:\n                    return Response({\n                        \"detail\": \"Invalid timestamp specified for {}\".format(\n                            param)\n                    }, status=HTTP_400_BAD_REQUEST)\n                pushes = pushes.filter(**{\n                    param.replace('push_timestamp', 'time'): value\n                })\n\n        for param in ['id__lt', 'id__lte', 'id__gt', 'id__gte', 'id']:\n            try:\n                value = int(filter_params.get(param, 0))\n            except ValueError:\n                return Response({\n                    \"detail\": \"Invalid timestamp specified for {}\".format(\n                        param)\n                }, status=HTTP_400_BAD_REQUEST)\n            if value:\n                pushes = pushes.filter(**{param: value})\n\n        id_in = filter_params.get(\"id__in\")\n        if id_in:\n            try:\n                id_in_list = [int(id) for id in id_in.split(',')]\n            except ValueError:\n                return Response({\"detail\": \"Invalid id__in specification\"},\n                                status=HTTP_400_BAD_REQUEST)\n            pushes = pushes.filter(id__in=id_in_list)\n\n        author = filter_params.get(\"author\")\n        if author:\n            pushes = pushes.filter(author=author)\n\n        try:\n            count = int(filter_params.get(\"count\", 10))\n        except ValueError:\n            return Response({\"detail\": \"Valid count value required\"},\n                            status=HTTP_400_BAD_REQUEST)\n\n        if count > MAX_PUSH_COUNT:\n            msg = \"Specified count exceeds api limit: {}\".format(MAX_PUSH_COUNT)\n            return Response({\"detail\": msg}, status=HTTP_400_BAD_REQUEST)\n\n        # we used to have a \"full\" parameter for this endpoint so you could\n        # specify to not fetch the revision information if it was set to\n        # false. however AFAIK no one ever used it (default was to fetch\n        # everything), so let's just leave it out. it doesn't break\n        # anything to send extra data when not required.\n        pushes = pushes.select_related('repository').prefetch_related('commits')[:count]\n        serializer = PushSerializer(pushes, many=True)\n\n        meta['count'] = len(pushes)\n        meta['repository'] = project\n        meta['filter_params'] = filter_params\n\n        resp = {\n            'meta': meta,\n            'results': serializer.data\n        }\n\n        return Response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting method implementation for detail view of push", "response": "def retrieve(self, request, project, pk=None):\n        \"\"\"\n        GET method implementation for detail view of ``push``\n        \"\"\"\n        try:\n            push = Push.objects.get(repository__name=project,\n                                    id=pk)\n            serializer = PushSerializer(push)\n            return Response(serializer.data)\n        except Push.DoesNotExist:\n            return Response(\"No push with id: {0}\".format(pk),\n                            status=HTTP_404_NOT_FOUND)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a count of the jobs belonging to this push.", "response": "def status(self, request, project, pk=None):\n        \"\"\"\n        Return a count of the jobs belonging to this push\n        grouped by job status.\n        \"\"\"\n        try:\n            push = Push.objects.get(id=pk)\n        except Push.DoesNotExist:\n            return Response(\"No push with id: {0}\".format(pk),\n                            status=HTTP_404_NOT_FOUND)\n        return Response(push.get_status())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef health(self, request, project):\n        revision = request.query_params.get('revision')\n\n        try:\n            push = Push.objects.get(revision=revision, repository__name=project)\n        except Push.DoesNotExist:\n            return Response(\"No push with revision: {0}\".format(revision),\n                            status=HTTP_404_NOT_FOUND)\n        push_health_test_failures = get_push_health_test_failures(push, REPO_GROUPS['trunk'])\n        test_result = 'fail' if len(push_health_test_failures['needInvestigation']) else 'pass'\n\n        return Response({\n            'revision': revision,\n            'id': push.id,\n            'result': test_result,\n            'metrics': [\n                {\n                    'name': 'Tests',\n                    'result': test_result,\n                    'failures': push_health_test_failures,\n                },\n                {\n                    'name': 'Builds (Not yet implemented)',\n                    'result': 'pass',\n                    'details': ['Wow, everything passed!'],\n                },\n                {\n                    'name': 'Linting (Not yet implemented)',\n                    'result': 'pass',\n                    'details': ['Gosh, this code is really nicely formatted.'],\n                },\n                {\n                    'name': 'Coverage (Not yet implemented)',\n                    'result': 'pass',\n                    'details': [\n                        'Covered 42% of the tests that are needed for feature ``foo``.',\n                        'Covered 100% of the tests that are needed for feature ``bar``.',\n                        'The ratio of people to cake is too many...',\n                    ],\n                },\n                {\n                    'name': 'Performance (Not yet implemented)',\n                    'result': 'pass',\n                    'details': ['Ludicrous Speed'],\n                },\n            ],\n        })", "response": "Return a calculated assessment of the health of this push."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decisiontask(self, request, project):\n        push_ids = request.query_params.getlist('push_ids')\n        job_type = JobType.objects.get(name='Gecko Decision Task')\n        decision_jobs = Job.objects.filter(\n            push_id__in=push_ids,\n            job_type=job_type\n        ).select_related('taskcluster_metadata')\n\n        if decision_jobs:\n            return Response(\n                {job.push_id: job.taskcluster_metadata.task_id for job in decision_jobs}\n            )\n        else:\n            return Response(\"No decision tasks found for pushes: {}\".format(push_ids),\n                            status=HTTP_404_NOT_FOUND)", "response": "Return the decision task ids for the pushes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if TinyMCE has a language file for the specified lang code.", "response": "def language_file_exists(language_code):\n    \"\"\"\n    Check if TinyMCE has a language file for the specified lang code\n\n    :param language_code: language code\n    :type language_code: str\n    :return: check result\n    :rtype: bool\n    \"\"\"\n    filename = '{0}.js'.format(language_code)\n    path = os.path.join('tinymce', 'js', 'tinymce', 'langs', filename)\n    return finders.find(path) is not None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dictionary with language - related parameters for TinyMCE4", "response": "def get_language_config():\n    \"\"\"\n    Creates a language configuration for TinyMCE4 based on Django project settings\n\n    :return: language- and locale-related parameters for TinyMCE 4\n    :rtype: dict\n    \"\"\"\n    language_code = convert_language_code(get_language() or settings.LANGUAGE_CODE)\n    if not language_file_exists(language_code):\n        language_code = language_code[:2]\n        if not language_file_exists(language_code):\n            # Fall back to English if Tiny MCE 4 does not have required translation\n            language_code = 'en'\n    config = {'language': language_code}\n    if get_language_bidi():\n        config['directionality'] = 'rtl'\n    else:\n        config['directionality'] = 'ltr'\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate TinyMCE spellchecker config based on Django settings", "response": "def get_spellcheck_config():\n    \"\"\"\n    Create TinyMCE spellchecker config based on Django settings\n\n    :return: spellchecker parameters for TinyMCE\n    :rtype: dict\n    \"\"\"\n    config = {}\n    if mce_settings.USE_SPELLCHECKER:\n        from enchant import list_languages\n        enchant_languages = list_languages()\n        if settings.DEBUG:\n            logger.info('Enchant languages: {0}'.format(enchant_languages))\n        lang_names = []\n        for lang, name in settings.LANGUAGES:\n            lang = convert_language_code(lang)\n            if lang not in enchant_languages:\n                lang = lang[:2]\n            if lang not in enchant_languages:\n                logger.warning('Missing {0} spellchecker dictionary!'.format(lang))\n                continue\n            if config.get('spellchecker_language') is None:\n                config['spellchecker_language'] = lang\n            lang_names.append('{0}={1}'.format(name, lang))\n        config['spellchecker_languages'] = ','.join(lang_names)\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_language_code(django_lang):\n    lang_and_country = django_lang.split('-')\n    try:\n        return '_'.join((lang_and_country[0], lang_and_country[1].upper()))\n    except IndexError:\n        return lang_and_country[0]", "response": "Converts a Django language code into ISO codes ll_CC or ll"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders TinyMCE. init JavaScript code", "response": "def render_tinymce_init_js(mce_config, callbacks, id_):\n    \"\"\"\n    Renders TinyMCE.init() JavaScript code\n\n    :param mce_config: TinyMCE 4 configuration\n    :type mce_config: dict\n    :param callbacks: TinyMCE callbacks\n    :type callbacks: dict\n    :param id_: HTML element's ID to which TinyMCE is attached.\n    :type id_: str\n    :return: TinyMCE.init() code\n    :rtype: str\n    \"\"\"\n    if mce_settings.USE_FILEBROWSER and 'file_browser_callback' not in callbacks:\n        callbacks['file_browser_callback'] = 'djangoFileBrowser'\n    if mce_settings.USE_SPELLCHECKER and 'spellchecker_callback' not in callbacks:\n        callbacks['spellchecker_callback'] = 'tinymce4_spellcheck'\n\n    if id_:\n        mce_config['selector'] = mce_config.get('selector', 'textarea') + '#{0}'.format(id_)\n\n    config = json.dumps(mce_config, cls=DjangoJSONEncoder)[1:-1]\n\n    return render_to_string('tinymce/tinymce_init.js',\n                            context={'callbacks': callbacks,\n                                     'tinymce_config': config,\n                                     'is_admin_inline': '__prefix__' in id_})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_managed():\n    for item in sys.argv:\n        if re.search(r'manage.py|django-admin|django', item) is not None:\n            return True\n    return False", "response": "Check if a Django project is being managed with manage. py or django - admin scripts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimplementing the TinyMCE 4 spellchecker protocol :param request: Django http request with JSON-RPC payload from TinyMCE 4 containing a language code and a text to check for errors. :type request: django.http.request.HttpRequest :return: Django http response containing JSON-RPC payload with spellcheck results for TinyMCE 4 :rtype: django.http.JsonResponse", "response": "def spell_check(request):\n    \"\"\"\n    Implements the TinyMCE 4 spellchecker protocol\n\n    :param request: Django http request with JSON-RPC payload from TinyMCE 4\n        containing a language code and a text to check for errors.\n    :type request: django.http.request.HttpRequest\n    :return: Django http response containing JSON-RPC payload\n        with spellcheck results for TinyMCE 4\n    :rtype: django.http.JsonResponse\n    \"\"\"\n    data = json.loads(request.body.decode('utf-8'))\n    output = {'id': data['id']}\n    error = None\n    status = 200\n    try:\n        if data['params']['lang'] not in list_languages():\n            error = 'Missing {0} dictionary!'.format(data['params']['lang'])\n            raise LookupError(error)\n        spell_checker = checker.SpellChecker(data['params']['lang'])\n        spell_checker.set_text(strip_tags(data['params']['text']))\n        output['result'] = {spell_checker.word: spell_checker.suggest()\n                            for err in spell_checker}\n    except NameError:\n        error = 'The pyenchant package is not installed!'\n        logger.exception(error)\n    except LookupError:\n        logger.exception(error)\n    except Exception:\n        error = 'Unknown error!'\n        logger.exception(error)\n    if error is not None:\n        output['error'] = error\n        status = 500\n    return JsonResponse(output, status=status)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(text):\n    err = \"glaad.offensive_terms\"\n    msg = \"Offensive term. Remove it or consider the context.\"\n\n    list = [\n        \"fag\",\n        \"faggot\",\n        \"dyke\",\n        \"sodomite\",\n        \"homosexual agenda\",\n        \"gay agenda\",\n        \"transvestite\",\n        \"homosexual lifestyle\",\n        \"gay lifestyle\"\n        # homo - may create false positives without additional context\n        # FIXME use topic detetor to decide whether \"homo\" is offensive\n    ]\n\n    return existence_check(text, list, err, msg, join=True, ignore_case=False)", "response": "Flag offensive words based on GLAAD reference guide."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef score(check=None):\n    tp = 0\n    fp = 0\n\n    parent_directory = os.path.dirname(proselint_path)\n    path_to_corpus = os.path.join(parent_directory, \"corpora\", \"0.1.0\")\n    for root, _, files in os.walk(path_to_corpus):\n        files = [f for f in files if f.endswith(\".md\")]\n        for f in files:\n            fullpath = os.path.join(root, f)\n\n            # Run the linter.\n            print(\"Linting {}\".format(f))\n            out = subprocess.check_output([\"proselint\", fullpath])\n\n            # Determine the number of errors.\n            regex = r\".+?:(?P<line>\\d+):(?P<col>\\d+): (?P<message>.+)\"\n            num_errors = len(tuple(re.finditer(regex, out)))\n            print(\"Found {} errors.\".format(num_errors))\n\n            # Open the document.\n            subprocess.call([\"open\", fullpath])\n\n            # Ask the scorer how many of the errors were false alarms?\n            input_val = None\n            while not isinstance(input_val, int):\n                try:\n                    input_val = input(\"# of false alarms? \")\n                    if input_val == \"exit\":\n                        return\n                    else:\n                        input_val = int(input_val)\n                        fp += input_val\n                        tp += (num_errors - input_val)\n                except ValueError:\n                    pass\n\n            print(\"Currently {} hits and {} false alarms\\n---\".format(tp, fp))\n\n    if (tp + fp) > 0:\n        return tp * (1.0 * tp / (tp + fp)) ** 2\n    else:\n        return 0", "response": "Compute the score on the corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"oxford.venery_terms\"\n    msg = \"The venery term is '{}'.\"\n\n    term_list = [\n        [\"alligators\",   \"congregation\"],\n        [\"antelopes\",    \"herd\"],\n        [\"baboons\",      \"troop\"],\n        [\"badgers\",      \"cete\"],\n        [\"bats\",         \"colony\"],\n        [\"bears\",        \"sloth\"],\n        [\"buffalo\",      \"herd\"],\n        [\"bullfinches\",  \"bellowing\"],\n        [\"caribou\",      \"herd\"],\n        [\"cats\",         \"glaring\"],\n        [\"caterpillars\", \"army\"],\n        [\"cockroaches\",  \"intrusion\"],\n        [\"coyotes\",      \"pack\"],\n        [\"crows\",        \"murder\"],\n        [\"dogs\",         \"pack\"],\n        [\"eagles\",       \"convocation\"],\n        [\"emus\",         \"mob\"],\n        [\"flamingos\",    \"stand\"],\n        [\"frogs\",        \"army\"],\n        [\"goldfinches\",  \"charm\"],\n        [\"gorillas\",     \"band\"],\n        [\"guineafowl\",   \"rasp\"],\n        [\"hedgehogs\",    \"array\"],\n        [\"herons\",       \"siege\"],\n        [\"hogs\",         \"parcel\"],\n        [\"hyenas\",       \"cackle\"],\n        [\"ibex\",         \"herd\"],\n        [\"iguanas\",      \"mess\"],\n        [\"lions\",        \"pride\"],\n        [\"locusts\",      \"plague\"],\n        [\"mackerel\",     \"shoal\"],\n        [\"mares\",        \"stud\"],\n        [\"minnows\",      \"shoal\"],\n        [\"moose\",        \"herd\"],\n        [\"mosquitoes\",   \"scourge\"],\n        [\"nightingales\", \"watch\"],\n        [\"oysters\",      \"bed\"],\n        [\"partridges\",   \"covey\"],\n        [\"pelicans\",     \"pod\"],\n        [\"raccoons\",     \"gaze\"],\n        [\"ravens\",       \"unkindness\"],\n        [\"rhinoceroses\", \"crash\"],\n        [\"sea urchins\",  \"sea\"],\n        [\"starlings\",    \"murmuration\"],\n        [\"toads\",        \"knot\"],\n        [\"wombats\",      \"wisdom\"],\n        [\"woodcocks\",    \"fall\"],\n        [\"woodpeckers\",  \"descent\"],\n        [\"wrens\",        \"herd\"],\n    ]\n\n    generic_terms = [\n        \"group\",\n        \"bunch\",\n    ]\n\n    list = []\n    for term_pair in term_list:\n        for generic in generic_terms:\n            wrong = \"a {} of {}\".format(generic, term_pair[0])\n            right = \"a {} of {}\".format(term_pair[1], term_pair[0])\n            list += [[right, [wrong]]]\n\n    return preferred_forms_check(text, list, err, msg)", "response": "Check the venery terms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"links.valid\"\n    msg = u\"Broken link: {}\"\n\n    regex = re.compile(\n        r\"\"\"(?i)\\b((?:https?://|www\\d{0,3}[.]\n        |[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+\n        |(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)\n        |[^\\s`!()\\[\\]{};:\\'\".,<>?\\xab\\xbb\\u201c\\u201d\\u2018\\u2019\\u21a9]))\"\"\",\n        re.U | re.X)\n\n    errors = []\n    for m in re.finditer(regex, text):\n        url = m.group(0).strip()\n\n        if \"http://\" not in url and \"https://\" not in url:\n            url = \"http://\" + url\n\n        if is_broken_link(url):\n            errors.append((m.start(), m.end(), err, msg.format(url), None))\n\n    return errors", "response": "Check the text for broken links."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_broken_link(url):\n    try:\n        request = urllib_request.Request(\n            url, headers={'User-Agent': 'Mozilla/5.0'})\n        urllib_request.urlopen(request).read()\n        return False\n    except urllib_request.URLError:\n        return True\n    except SocketError:\n        return True", "response": "Determine whether the link returns a 404 error."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check(text):\n    err = \"security.credit_card\"\n    msg = u\"Don't put credit card numbers in plain text.\"\n\n    credit_card_numbers = [\n        \"4\\d{15}\",\n        \"5[1-5]\\d{14}\",\n        \"3[4,7]\\d{13}\",\n        \"3[0,6,8]\\d{12}\",\n        \"6011\\d{12}\",\n    ]\n\n    return existence_check(text, credit_card_numbers, err, msg)", "response": "Check the text for credit card numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"misc.annotations\"\n    msg = u\"Annotation left in text.\"\n\n    annotations = [\n        \"FIXME\",\n        \"FIX ME\",\n        \"TODO\",\n        \"todo\",\n        \"ERASE THIS\",\n        \"FIX THIS\",\n    ]\n\n    return existence_check(\n        text, annotations, err, msg, ignore_case=False, join=True)", "response": "Check the text for existence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for p = 0. 0.", "response": "def check_p_equals_zero(text):\n    \"\"\"Check for p = 0.000.\"\"\"\n    err = \"psychology.p_equals_zero\"\n    msg = \"Unless p really equals zero, you should use more decimal places.\"\n\n    list = [\n        \"p = 0.00\",\n        \"p = 0.000\",\n        \"p = 0.0000\",\n    ]\n\n    return existence_check(text, list, err, msg, join=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the text for the ly adjective.", "response": "def check_ly(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"garner.phrasal_adjectives.ly\"\n    msg = u\"\"\"No hyphen is necessary in phrasal adjectives with an adverb\n              ending in -ly, unless the -ly adverb is part of a longer\n              phrase\"\"\"\n\n    regex = \"\\s[^\\s-]+ly-\"\n\n    return existence_check(text, [regex], err, msg,\n                           require_padding=False, offset=-1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a username and password combination is valid.", "response": "def check_auth(username, password):\n    \"\"\"Check if a username / password combination is valid.\"\"\"\n    legal_hashes = [\n        \"15a7fdade5fa58d38c6d400770e5c0e948fbc03ba365b704a6d205687738ae46\",\n        \"057b24043181523e3c3717071953c575bd13862517a8ce228601582a9cbd9dae\",\n        \"c8d79ae7d388b6da21cb982a819065b18941925179f88041b87de2be9bcde79c\",\n        \"bb7082271060c91122de8a3bbac5c7d6dcfe1a02902d57b27422f0062f602f72\",\n        \"90e4d9e4ec680ff40ce00e712b24a67659539e06d50b538ed4a3d960b4f3bda5\",\n        \"9a9a241b05eeaa0ca2b4323c5a756851c9cd15371a4d71a326749abc47062bf0\",\n        \"0643786903dab7cbb079796ea4b27a81fb38442381773759dd52ac8615eb6ab2\",\n        \"886078097635635c1450cf52ca0ec13a887ea4f8cd4b799fdedc650ec1f08781\",\n        \"d4c4d2d16c7fec2d0d60f0a110eb4fbd9f1bb463033298e01b3141a7e4ca10bc\",\n        \"83dfe687131d285a22c5379e19f4ebabcdfe8a19bade46a5cdcdc9e9c36b52a2\",\n        \"7c4000e5d6948055553eb84fc2188ccad068caa1b584303f88dc0c582a0ecd42\",\n        \"43c693fa32545b7d4106e337fe6edf7db92282795d5bdb80705ef8a0ac7e8030\",\n        \"ebb17f7f9050e3c1b18f84cbd6333178d575d4baf3aca6dfa0587cc2a48e02d0\",\n        \"ce910c4368092bf0886e59dc5df0b0ad11f40067b685505c2195463d32fa0418\",\n        \"86fc704debb389a73775e02f8f0423ffbbb787a1033e531b2e47d40f71ad5560\",\n        \"308af1914cb90aeb8913548cc37c9b55320875a2c0d2ecfe6afe1bfc02c64326\",\n        \"bd3486100f2bb29762100b93b1f1cd41655ab05767f78fb1fc4adfe040ebe953\",\n        \"29f56ee67dd218276984d723b6b105678faa1868a9644f0d9c49109c8322e1d8\",\n        \"704c3ddde0b5fd3c6971a6ef16991ddff3e241c170ed539094ee668861e01764\",\n        \"aaebc3ca0fe041a3a595170b8efda22308cd7d843510bf01263f05a1851cb173\",\n    ]\n    return hashlib.sha256(username + password).hexdigest() in legal_hashes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset rate limits for authenticated and nonauthenticated users.", "response": "def rate():\n    \"\"\"Set rate limits for authenticated and nonauthenticated users.\"\"\"\n    auth = request.authorization\n\n    if not auth or not check_auth(auth.username, auth.password):\n        return \"60/minute\"\n    else:\n        return \"600/minute\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning linter on the provided text and return the results.", "response": "def lint():\n    \"\"\"Run linter on the provided text and return the results.\"\"\"\n    if 'text' in request.values:\n        text = unquote(request.values['text'])\n        job = q.enqueue(worker_function, text)\n\n        return jsonify(job_id=job.id), 202\n\n    elif 'job_id' in request.values:\n        job = q.fetch_job(request.values['job_id'])\n\n        if not job:\n            return jsonify(\n                status=\"error\",\n                message=\"No job with requested job_id.\"), 404\n\n        elif job.result is None:\n            return jsonify(\n                status=\"error\",\n                message=\"Job is not yet ready.\"), 202\n\n        else:\n            errors = []\n            for i, e in enumerate(job.result):\n                app.logger.debug(e)\n                errors.append({\n                    \"check\": e[0],\n                    \"message\": e[1],\n                    \"line\": e[2],\n                    \"column\": e[3],\n                    \"start\": e[4],\n                    \"end\": e[5],\n                    \"extent\": e[5] - e[4],\n                    \"severity\": e[7],\n                    \"replacements\": e[8],\n                    \"source_name\": \"\",\n                    \"source_url\": \"\",\n                })\n            return jsonify(\n                status=\"success\",\n                data={\"errors\": errors})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsuggesting the preferred forms.", "response": "def check_months(text):\n    \"\"\"Suggest the preferred forms.\"\"\"\n    err = \"MAU102\"\n    msg = \"Months should be capitalized. '{}' is the preferred form.\"\n\n    list = [\n\n        [\"January\",         [\"january\"]],\n        [\"February\",        [\"february\"]],\n        # [\"March\",           [\"march\"]],\n        [\"April\",           [\"april\"]],\n        # [\"May\",             [\"may\"]],\n        [\"June\",            [\"june\"]],\n        [\"July\",            [\"july\"]],\n        [\"August\",          [\"august\"]],\n        [\"September\",       [\"september\"]],\n        [\"October\",         [\"october\"]],\n        [\"November\",        [\"november\"]],\n        [\"December\",        [\"december\"]],\n    ]\n\n    return preferred_forms_check(text, list, err, msg, ignore_case=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsuggests the preferred forms.", "response": "def check_days(text):\n    \"\"\"Suggest the preferred forms.\"\"\"\n    err = \"MAU102\"\n    msg = \"Days of the week should be capitalized. '{}' is the preferred form.\"\n\n    list = [\n\n        [\"Monday\",       [\"monday\"]],\n        [\"Tuesday\",      [\"tuesday\"]],\n        [\"Wednesday\",    [\"wednesday\"]],\n        [\"Thursday\",     [\"thursday\"]],\n        [\"Friday\",       [\"friday\"]],\n        [\"Saturday\",     [\"saturday\"]],\n        [\"Sunday\",       [\"sunday\"]],\n    ]\n\n    return preferred_forms_check(text, list, err, msg, ignore_case=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the text for skunked terms.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"skunked_terms.misc\"\n    msg = u\"\"\"'{}' is a bit of a skunked term \u2014 impossible to use without issue.\n             Find some other way to say it.\"\"\"\n\n    skunked_terms = [\n        \"bona fides\",\n        \"deceptively\",\n        \"decimate\",\n        \"effete\",\n        \"fulsome\",\n        \"hopefully\",\n        \"impassionate\",\n        \"Thankfully,\",\n    ]\n\n    return existence_check(text, skunked_terms, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the text for existence.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"misc.illogic\"\n    msg = u\"'{}' is illogical.\"\n\n    illogics = [\n        \"preplan\",\n        \"more than .{1,10} all\",\n        \"appraisal valuations?\",\n        \"(?:i|you|he|she|it|y'all|all y'all|you all|they) could care less\",\n        \"least worst\",\n        \"much-needed gaps?\",\n        \"much-needed voids?\",\n        \"no longer requires oxygen\",\n        \"without scarcely\",\n    ]\n\n    return existence_check(text, illogics, err, msg, offset=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_coin_a_phrase_from(text):\n    err = \"misc.illogic.coin\"\n    msg = \"You can't coin an existing phrase. Did you mean 'borrow'?\"\n\n    regex = \"to coin a phrase from\"\n\n    return existence_check(text, [regex], err, msg, offset=1)", "response": "Check the text to see if it s a coinaion a phrase from another phrase."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the textself without your collusion.", "response": "def check_without_your_collusion(text):\n    \"\"\"Check the textself.\"\"\"\n    err = \"misc.illogic.collusion\"\n    msg = \"It's impossible to defraud yourself. Try 'aquiescence'.\"\n\n    regex = \"without your collusion\"\n\n    return existence_check(\n        text, [regex], err, msg, require_padding=False, offset=-1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck that the text contains exclamation points.", "response": "def check_repeated_exclamations(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"leonard.exclamation.multiple\"\n    msg = u\"Stop yelling. Keep your exclamation points under control.\"\n\n    regex = r\"[\\!]\\s*?[\\!]{1,}\"\n\n    return existence_check(\n        text, [regex], err, msg, require_padding=False, ignore_case=False,\n        max_errors=1, dotall=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes sure that the exclamation ppm is under 30.", "response": "def check_exclamations_ppm(text):\n    \"\"\"Make sure that the exclamation ppm is under 30.\"\"\"\n    err = \"leonard.exclamation.30ppm\"\n    msg = u\"More than 30 ppm of exclamations. Keep them under control.\"\n\n    regex = r\"\\w!\"\n\n    count = len(re.findall(regex, text))\n    num_words = len(text.split(\" \"))\n\n    ppm = (count*1.0 / num_words) * 1e6\n\n    if ppm > 30 and count > 1:\n        loc = re.search(regex, text).start() + 1\n        return [(loc, loc+1, err, msg, \".\")]\n    else:\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(text):\n    err = \"strunk_white.composition\"\n    msg = \"Try '{}' instead of '{}'.\"\n\n    bad_forms = [\n        # Put statements in positive form\n        [\"dishonest\",               [\"not honest\"]],\n        [\"trifling\",                [\"not important\"]],\n        [\"forgot\",                  [\"did not remember\"]],\n        [\"ignored\",                 [\"did not pay (any )?attention to\"]],\n        [\"distrusted\",              [\"did not have much confidence in\"]],\n\n        # Omit needless words\n        [\"whether\",                 [\"the question as to whether\"]],\n        [\"no doubt\",                [\"there is no doubt but that\"]],\n        [\"used for fuel\",           [\"used for fuel purposes\"]],\n        [\"he\",                      [\"he is a man who\"]],\n        [\"hastily\",                 [\"in a hasty manner\"]],\n        [\"this subject\",            [\"this is a subject that\"]],\n        [\"Her story is strange.\",   [\"Her story is a strange one.\"]],\n        [\"because\",                 [\"the reason why is that\"]],\n        [\"because / since\",         [\"owing to the fact that\"]],\n        [\"although / though\",       [\"in spite of the fact that\"]],\n        [\"remind you / notify you\",\n            [\"call your attention to the fact that\"]],\n        [\"I did not know that / I was unaware that\",\n            [\"I was unaware of the fact that\"]],\n        [\"his failure\",             [\"the fact that he had not succeeded\"]],\n        [\"my arrival\",              [\"the fact that i had arrived\"]]\n    ]\n\n    return preferred_forms_check(text, bad_forms, err, msg)", "response": "Suggest the preferred forms."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"consistency.spelling\"\n    msg = \"Inconsistent spelling of '{}' (vs. '{}').\"\n\n    word_pairs = [\n        [\"advisor\", \"adviser\"],\n        # [\"analyse\", \"analyze\"],\n        [\"centre\", \"center\"],\n        [\"colour\", \"color\"],\n        [\"emphasise\", \"emphasize\"],\n        [\"finalise\", \"finalize\"],\n        [\"focussed\", \"focused\"],\n        [\"labour\", \"labor\"],\n        [\"learnt\", \"learned\"],\n        [\"organise\", \"organize\"],\n        [\"organised\", \"organized\"],\n        [\"organising\", \"organizing\"],\n        [\"recognise\", \"recognize\"],\n    ]\n\n    return consistency_check(text, word_pairs, err, msg)", "response": "Check the spelling of the text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the text for uncomparables.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"uncomparables.misc\"\n    msg = \"Comparison of an uncomparable: '{}' is not comparable.\"\n\n    comparators = [\n        \"most\",\n        \"more\",\n        \"less\",\n        \"least\",\n        \"very\",\n        \"quite\",\n        \"largely\",\n        \"extremely\",\n        \"increasingly\",\n        \"kind of\",\n        \"mildly\"\n    ]\n\n    uncomparables = [\n        \"absolute\",\n        \"adequate\",\n        \"chief\",\n        \"complete\",\n        \"correct\",\n        \"devoid\",\n        \"entire\",\n        \"false\",\n        \"fatal\",\n        \"favorite\",\n        \"final\",\n        \"ideal\",\n        \"impossible\",\n        \"inevitable\",\n        \"infinite\",\n        \"irrevocable\",\n        \"main\",\n        \"manifest\",\n        \"only\",\n        \"paramount\",\n        \"perfect\",\n        \"perpetual\",\n        \"possible\",\n        \"preferable\",\n        \"principal\",\n        \"singular\",\n        \"stationary\",\n        \"sufficient\",\n        \"true\",\n        \"unanimous\",\n        \"unavoidable\",\n        \"unbroken\",\n        \"uniform\",\n        \"unique\",\n        \"universal\",\n        \"void\",\n        \"whole\",\n    ]\n\n    exceptions = [\n        (\"more\", \"perfect\"),\n        (\"more\", \"possible\")  # FIXME\n    ]\n\n    all = [\"\\\\b\" + i[0] + \"\\s\" + i[1] + \"[\\W$]\" for i in itertools.product(\n           comparators, uncomparables) if i not in exceptions]\n\n    occ = re.finditer(\"|\".join(all), text.lower())\n    return [(o.start(), o.end(), err, msg.format(o.group(0)), None)\n            for o in occ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(text):\n    err = \"consistency.spacing\"\n    msg = \"Inconsistent spacing after period (1 vs. 2 spaces).\"\n\n    regex = [\"[\\.\\?!] [A-Z]\", \"[\\.\\?!]  [A-Z]\"]\n    return consistency_check(text, [regex], err, msg)", "response": "Check the text for consistency."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check(text):\n    err = \"pinker.latin\"\n    msg = \"Use English. '{}' is the preferred form.\"\n\n    list = [\n        [\"other things being equal\",          [\"ceteris paribus\"]],\n        [\"among other things\",                [\"inter alia\"]],\n        [\"in and of itself\",                  [\"simpliciter\"]],\n        [\"having made the necessary changes\", [\"mutatis mutandis\"]],\n    ]\n\n    return preferred_forms_check(text, list, err, msg)", "response": "Suggest the preferred forms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the text for hyperbolic.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"hyperbolic.misc\"\n    msg = u\"'{}' is hyperbolic.\"\n\n    words = [\n        \"[a-z]*[!]{2,}\",\n        \"[a-z]*\\?{2,}\"\n    ]\n\n    return existence_check(text, words, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"glaad.terms\"\n    msg = \"Possibly offensive term. Consider using '{}' instead of '{}'.\"\n\n    list = [\n        [\"gay man\",            [\"homosexual man\"]],\n        [\"gay men\",            [\"homosexual men\"]],\n        [\"lesbian\",            [\"homosexual woman\"]],\n        [\"lesbians\",           [\"homosexual women\"]],\n        [\"gay people\",         [\"homosexual people\"]],\n        [\"gay couple\",         [\"homosexual couple\"]],\n        [\"sexual orientation\", [\"sexual preference\"]],\n        [\"openly gay\",         [\"admitted homosexual\", \"avowed homosexual\"]],\n        [\"equal rights\",       [\"special rights\"]]\n        ]\n\n    return preferred_forms_check(text, list, err, msg, ignore_case=False)", "response": "Suggest preferred forms given the reference document."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking the text for invalid roman numeral abbreviations.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"MSC104\"\n    msg = u\"Don't fail to capitalize roman numeral abbreviations.\"\n\n    pwd_regex = \" (I(i*)|i*)\"\n\n    password = [\n        \"World War{}\".format(pwd_regex),\n    ]\n\n    return blacklist(text, password, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove compiled python files.", "response": "def _delete_compiled_python_files():\n    \"\"\"Remove files with a 'pyc' extension.\"\"\"\n    for path, _, files in os.walk(os.getcwd()):\n        for fname in [f for f in files if os.path.splitext(f)[1] == \".pyc\"]:\n            try:\n                os.remove(os.path.join(path, fname))\n            except OSError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint the errors resulting from lint for filename.", "response": "def print_errors(filename, errors, output_json=False, compact=False):\n    \"\"\"Print the errors, resulting from lint, for filename.\"\"\"\n    if output_json:\n        click.echo(errors_to_json(errors))\n\n    else:\n        for error in errors:\n\n            (check, message, line, column, start, end,\n             extent, severity, replacements) = error\n\n            if compact:\n                filename = \"-\"\n\n            click.echo(\n                filename + \":\" +\n                str(1 + line) + \":\" +\n                str(1 + column) + \": \" +\n                check + \" \" +\n                message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef proselint(paths=None, version=None, clean=None, debug=None,\n              output_json=None, time=None, demo=None, compact=None):\n    \"\"\"A CLI for proselint, a linter for prose.\"\"\"\n    if time:\n        click.echo(timing_test())\n        return\n\n    # In debug or clean mode, delete cache & *.pyc files before running.\n    if debug or clean:\n        clear_cache()\n\n    # Use the demo file by default.\n    if demo:\n        paths = [demo_file]\n\n    # Expand the list of directories and files.\n    filepaths = extract_files(list(paths))\n\n    # Lint the files\n    num_errors = 0\n\n    # Use stdin if no paths were specified\n    if len(paths) == 0:\n        filepaths.append('-')\n\n    for fp in filepaths:\n        try:\n            if fp == '-':\n                fp = '<stdin>'\n                f = sys.stdin\n            else:\n                f = click.open_file(\n                    fp, 'r', encoding=\"utf-8\", errors=\"replace\")\n            errors = lint(f, debug=debug)\n            num_errors += len(errors)\n            print_errors(fp, errors, output_json, compact=compact)\n        except Exception:\n            traceback.print_exc()\n\n    # Return an exit code\n    close_cache_shelves()\n    if num_errors > 0:\n        sys.exit(1)\n    else:\n        sys.exit(0)", "response": "A CLI for proselint a linter for prose."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_files(files):\n    expanded_files = []\n    legal_extensions = [\".md\", \".txt\", \".rtf\", \".html\", \".tex\", \".markdown\"]\n\n    for f in files:\n        # If it's a directory, recursively walk through it and find the files.\n        if os.path.isdir(f):\n            for dir_, _, filenames in os.walk(f):\n                for filename in filenames:\n                    fn, file_extension = os.path.splitext(filename)\n                    if file_extension in legal_extensions:\n                        joined_file = os.path.join(dir_, filename)\n                        expanded_files.append(joined_file)\n\n        # Otherwise add the file directly.\n        else:\n            expanded_files.append(f)\n\n    return expanded_files", "response": "Expand list of paths to include all text files matching the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the text for a reasonable doubt.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"misc.not_guilty\"\n    msg = u\"'not guilty beyond a reasonable doubt' is an ambiguous phrasing.\"\n    regex = r\"not guilty beyond (a |any )?reasonable doubt\"\n\n    return existence_check(text, [regex], err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check(text):\n    err = \"hedging.misc\"\n    msg = \"Hedging. Just say it.\"\n\n    narcissism = [\n        \"I would argue that\",\n        \", so to speak\",\n        \"to a certain degree\",\n    ]\n\n    return existence_check(text, narcissism, err, msg)", "response": "Suggest the preferred forms."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the text for the nationale present.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"misc.tense_present\"\n    msg = u\"'{}'.\"\n\n    illogics = [\n        u\"up to \\d{1,3}% ?[-\\u2014\\u2013]{0,3} ?(?:or|and) more\\W?\",\n        \"between you and I\",\n        \"on accident\",\n        \"somewhat of a\",\n        \"all it's own\",\n        \"reason is because\",\n        \"audible to the ear\",\n        \"in regards to\",\n        \"would of\",\n        # \"and so\",\n        \"i ?(?:feel|am feeling|am|'m|'m feeling) nauseous\",\n    ]\n\n    errors = []\n    for i in illogics:\n        for m in re.finditer(u\"\\s{}\\s\".format(i), text, flags=re.U | re.I):\n            txt = m.group(0).strip()\n            errors.append((\n                m.start() + 1,\n                m.end(),\n                err,\n                msg.format(txt),\n                None))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsuggest the preferred forms.", "response": "def check(text):\n    \"\"\"Suggest the preferred forms.\n\n    source:     Garner's Modern American Usage\n    source_url: http://bit.ly/1T4alrY\n    \"\"\"\n    err = \"terms.denizen_labels.garner\"\n    msg = \"'{}' is the preferred denizen label.\"\n\n    preferences = [\n\n        [\"Afrikaner\",         [\"Afrikaaner\"]],\n        [\"Afrikaner\",         [\"Afrikander\"]],\n        [\"Alabamian\",         [\"Alabaman\"]],\n        [\"Albuquerquean\",     [\"Albuquerquian\"]],\n        [\"Anchorageite\",      [\"Anchoragite\"]],\n        [\"Angeleno\",          [\"Los Angelean\"]],\n        [\"Arizonan\",          [\"Arizonian\"]],\n        [\"Arkansan\",          [\"Arkansawyer\"]],\n        [\"Belarusian\",        [\"Belarusan\"]],\n        [\"Caymanian\",         [\"Cayman Islander\"]],\n        [\"Coloradan\",         [\"Coloradoan\"]],\n        [\"Fairbanksan\",       [\"Fairbanksian\"]],\n        [\"Fort Worthian\",     [\"Fort Worther\"]],\n        [\"Grenadan\",          [\"Grenadian\"]],\n        [\"Hong Konger\",       [\"Hong Kongite\", \"Hong Kongian\"]],\n        [\"Hoosier\",           [\"Indianan\", \"Indianian\"]],\n        [\"Illinoisan\",        [\"Illinoisian\"]],\n        [\"Iowan\",             [\"Iowegian\"]],\n        [\"Louisianian\",       [\"Louisianan\"]],\n        [\"Michigander\",       [\"Michiganite\", \"Michiganian\"]],\n        [\"Missourian\",        [\"Missouran\"]],\n        [\"Monegasque\",        [\"Monacan\"]],\n        [\"Neapolitan\",        [\"Neopolitan\"]],\n        [\"New Hampshirite\",   [\"New Hampshireite\", \"New Hampshireman\"]],\n        [\"New Jerseyan\",      [\"New Jerseyite\"]],\n        [\"New Orleanian\",     [\"New Orleansian\"]],\n        [\"Nutmegger\",         [\"Connecticuter\"]],\n        [\"Oklahoma Cityan\",   [\"Oklahoma Citian\"]],\n        [\"Oklahoman\",         [\"Oklahomian\"]],\n        [\"Seattleite\",        [\"Seattlite\"]],\n        [\"Surinamese\",        [\"Surinamer\"]],\n        [\"Tallahasseean\",     [\"Tallahassean\"]],\n        [\"Tennessean\",        [\"Tennesseean\"]],\n        [\"Tusconan\",          [\"Tusconian\", \"Tusconite\"]],\n        [\"Utahn\",             [\"Utahan\"]],\n        [\"Saudi\",             [\"Saudi Arabian\"]],\n    ]\n\n    return preferred_forms_check(text, preferences, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsuggesting the preferred forms.", "response": "def check_denizen_labels_norris(text):\n    \"\"\"Suggest the preferred forms.\n\n    source:     Mary Norris\n    source_url: http://nyr.kr/1rGienj\n    \"\"\"\n    err = \"terms.denizen_labels.norris\"\n    msg = \"Would you like '{}'?\"\n\n    preferences = [\n        [\"Mancunian\",         [\"Manchesterian\"]],\n        [\"Mancunians\",        [\"Manchesterians\"]],\n        [\"Vallisoletano\",     [\"Valladolidian\"]],\n        [\"Wulfrunian\",        [\"Wolverhamptonian\", \"Wolverhamptonite\"]],\n        [\"Novocastrian\",      [\"Newcastleite\", \"Newcastlite\"]],\n        [\"Trifluvian\",        [u\"Trois-Rivi\u00e8rester\"]],\n        [\"Leodenisian\",       [\"Leedsian\"]],\n        [\"Minneapolitan\",     [\"Minneapolisian\"]],\n        [\"Hartlepudlian\",     [\"Hartlepoolian\"]],\n        [\"Liverpudlian\",      [\"Liverpoolian\"]],\n        [\"Haligonian\",        [\"Halifaxer\"]],\n        [\"Varsovian\",         [\"Warsawer\", \"Warsawian\"]],\n        [\"Providentian\",      [\"Providencian\", \"Providencer\"]],\n        [\"Tridentine\",        [\"Trentian\", \"Trentonian\"]],\n    ]\n\n    return preferred_forms_check(text, preferences, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(text):\n    err = \"typography.diacritical_marks\"\n    msg = u\"Use diacritical marks in '{}'.\"\n\n    list = [\n        # French loanwords\n        [u\"beau id\u00e9al\",         [\"beau ideal\"]],\n        [u\"boutonni\u00e8re\",        [\"boutonniere\"]],\n        [u\"bric-\u00e0-brac\",        [\"bric-a-brac\"]],\n        [u\"caf\u00e9\",               [\"cafe\"]],\n        [u\"cause c\u00e9l\u00e8bre\",      [\"cause celebre\"]],\n        [u\"ch\u00e8vre\",             [\"chevre\"]],\n        [u\"clich\u00e9\",             [\"cliche\"]],\n        [u\"comme ci comme \u00e7a\",  [\"comme ci comme ca\", \"comsi comsa\"]],\n        [u\"consomm\u00e9\",           [\"consomme\"]],\n        [u\"coup d'\u00e9tat\",        [\"coup d'etat\"]],\n        [u\"coup de gr\u00e2ce\",      [\"coup de grace\"]],\n        [u\"crudit\u00e9s\",           [\"crudites\"]],\n        [u\"cr\u00e8me br\u00fbl\u00e9e\",       [\"creme brulee\"]],\n        [u\"cr\u00e8me de menthe\",    [\"creme de menthe\"]],\n        [u\"cr\u00e8me fra\u00eeche\",      [\"creme fraice\"]],\n        [u\"cr\u00e8me fra\u00eeche\",      [\"creme fresh\"]],\n        [u\"cr\u00eape\",              [\"crepe\"]],\n        [u\"d\u00e9butante\",          [\"debutante\"]],\n        [u\"d\u00e9cor\",              [\"decor\"]],\n        [u\"d\u00e9j\u00e0 vu\",            [\"deja vu\"]],\n        [u\"d\u00e9nouement\",         [\"denouement\"]],\n        [u\"fa\u00e7ade\",             [\"facade\"]],\n        [u\"fianc\u00e9\",             [\"fiance\"]],\n        [u\"fianc\u00e9e\",            [\"fiancee\"]],\n        [u\"flamb\u00e9\",             [\"flambe\"]],\n        [u\"gar\u00e7on\",             [\"garcon\"]],\n        [u\"lyc\u00e9e\",              [\"lycee\"]],\n        [u\"ma\u00eetre d\",           [\"maitre d\"]],\n        [u\"m\u00e9nage \u00e0 trois\",     [\"menage a trois\"]],\n        [u\"n\u00e9glig\u00e9e\",           [\"negligee\"]],\n        [u\"papier-m\u00e2ch\u00e9\", [\"papier-mache\", \"paper mache\", \"paper-mache\"]],\n        [u\"prot\u00e9g\u00e9\",            [\"protege\"]],\n        [u\"prot\u00e9g\u00e9e\",           [\"protegee\"]],\n        [u\"pur\u00e9e\",              [\"puree\"]],\n        [u\"raison d'\u00eatre\",      [\"raison d'etre\"]],\n        [u\"my r\u00e9sum\u00e9\",          [\"my resume\"]],\n        [u\"your r\u00e9sum\u00e9\",        [\"your resume\"]],\n        [u\"his r\u00e9sum\u00e9\",         [\"his resume\"]],\n        [u\"her r\u00e9sum\u00e9\",         [\"her resume\"]],\n        [u\"a r\u00e9sum\u00e9\",           [\"a resume\"]],\n        [u\"the r\u00e9sum\u00e9\",         [\"the resume\"]],\n        [u\"risqu\u00e9\",             [\"risque\"]],\n        [u\"rou\u00e9\",               [\"roue\"]],\n        [u\"soir\u00e9e\",             [\"soiree\"]],\n        [u\"souffl\u00e9\",            [\"souffle\"]],\n        [u\"soup\u00e7on\",            [\"soupcon\"]],\n        [u\"touch\u00e9\",             [\"touche\"]],\n        [u\"t\u00eate-\u00e0-t\u00eate\",        [\"tete-a-tete\"]],\n        [u\"voil\u00e0\",              [\"voila\"]],\n        [u\"\u00e0 la carte\",         [\"a la carte\"]],\n        [u\"\u00e0 la mode\",          [\"a la mode\"]],\n        [u\"\u00e9migr\u00e9\",             [\"emigre\"]],\n\n        # Spanish loanwords\n        [u\"El Ni\u00f1o\",            [\"El Nino\"]],\n        [u\"jalape\u00f1o\",           [\"jalapeno\"]],\n        [u\"La Ni\u00f1a\",            [\"La Nina\"]],\n        [u\"pi\u00f1a colada\",        [\"pina colada\"]],\n        [u\"se\u00f1or\",              [\"senor\"]],\n        [u\"se\u00f1ora\",             [\"senora\"]],\n        [u\"se\u00f1orita\",           [\"senorita\"]],\n\n        # Portuguese loanwords\n        [u\"a\u00e7a\u00ed\",               [\"acai\"]],\n\n        # German loanwords\n        [u\"doppelg\u00e4nger\",       [\"doppelganger\"]],\n        [u\"F\u00fchrer\",             [\"Fuhrer\"]],\n        [u\"Gew\u00fcrztraminer\",     [\"Gewurztraminer\"]],\n        [u\"vis-\u00e0-vis\",          [\"vis-a-vis\"]],\n        [u\"\u00dcbermensch\",         [\"Ubermensch\"]],\n\n        # Swedish loanwords\n        [u\"filmj\u00f6lk\",           [\"filmjolk\"]],\n        [u\"sm\u00f6rg\u00e5sbord\",        [\"smorgasbord\"]],\n\n        # Names, places, and companies\n        [u\"Beyonc\u00e9\",            [\"Beyonce\"]],\n        [u\"Bront\u00eb\",             [\"Bronte\"]],\n        [u\"Bront\u00eb\",             [\"Bronte\"]],\n        [u\"Champs-\u00c9lys\u00e9es\",     [\"Champs-Elysees\"]],\n        [u\"Citro\u00ebn\",            [\"Citroen\"]],\n        [u\"Cura\u00e7ao\",            [\"Curacao\"]],\n        [u\"H\u00e4agen-Dazs\",        [\"Haagen-Dazs\", \"Haagen Dazs\"]],\n        [u\"L\u00f6wenbr\u00e4u\",          [\"Lowenbrau\"]],\n        [u\"Mon\u00e9gasque\",         [\"Monegasque\"]],\n        [u\"M\u00f6tley Cr\u00fce\",        [\"Motley Crue\"]],\n        [u\"Nescaf\u00e9\",            [\"Nescafe\"]],\n        [u\"Queensr\u00ffche\",        [\"Queensryche\"]],\n        [u\"Qu\u00e9bec\",             [\"Quebec\"]],\n        [u\"Qu\u00e9b\u00e9cois\",          [\"Quebecois\"]],\n        [u\"\u00c5ngstr\u00f6m\",           [\"Angstrom\"]],\n        [u\"\u00e5ngstr\u00f6m\",           [\"angstrom\"]],\n        [u\"\u0160koda\",              [\"Skoda\"]],\n    ]\n\n    return preferred_forms_check(text, list, err, msg)", "response": "Suggest the preferred forms in the given text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsuggests the correct name.", "response": "def check_vtech(text):\n    \"\"\"Suggest the correct name.\n\n    source: Virginia Tech Division of Student Affairs\n    source_url: http://bit.ly/2en1zbv\n    \"\"\"\n    err = \"institution.vtech\"\n    msg = \"Incorrect name. Use '{}' instead of '{}'.\"\n\n    institution = [\n        [\"Virginia Polytechnic Institute and State University\",\n         [\"Virginia Polytechnic and State University\"]],\n    ]\n    return preferred_forms_check(text, institution, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check(text):\n    err = \"malapropisms.misc\"\n    msg = u\"'{}' is a malapropism.\"\n\n    illogics = [\n        \"the infinitesimal universe\",\n        \"a serial experience\",\n        \"attack my voracity\",\n    ]\n\n    return existence_check(text, illogics, err, msg, offset=1)", "response": "Check the text for malapropisms."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_redundancy(text):\n    err = \"dates_times.am_pm.midnight_noon\"\n    msg = (u\"'a.m.' is always morning; 'p.m.' is always night.\")\n\n    list = [\n        \"\\d{1,2} ?a\\.?m\\.? in the morning\",\n        \"\\d{1,2} ?p\\.?m\\.? in the evening\",\n        \"\\d{1,2} ?p\\.?m\\.? at night\",\n        \"\\d{1,2} ?p\\.?m\\.? in the afternoon\",\n    ]\n\n    return existence_check(text, list, err, msg, join=True)", "response": "Check the text for redundancy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check(text):\n    err = \"sexism.misc\"\n    msg = \"Gender bias. Use '{}' instead of '{}'.\"\n\n    sexism = [\n        [\"anchor\",           [\"anchorman\", \"anchorwoman\"]],\n        [\"chair\",            [\"chairman\", \"chairwoman\"]],\n        [\"drafter\",          [\"draftman\", \"draftwoman\"]],\n        [\"ombuds\",           [\"ombudsman\", \"ombudswoman\"]],\n        [\"tribe member\",     [\"tribesman\", \"tribeswoman\"]],\n        [\"police officer\",   [\"policeman\", \"policewoman\"]],\n        [\"firefighter\",      [\"fireman\", \"firewoman\"]],\n        [\"mail carrier\",     [\"mailman\", \"mailwoman\"]],\n        [\"history\",          [\"herstory\"]],\n        [\"women\",            [\"womyn\"]],\n        [\"poet\",             [\"poetess\"]],\n        [\"author\",           [\"authoress\"]],\n        [\"waiter\",           [\"waitress\"]],\n        [\"lawyer\",           [\"lady lawyer\"]],\n        [\"doctor\",           [\"woman doctor\"]],\n        [\"bookseller\",       [\"female booksalesman\"]],\n        [\"air pilot\",        [\"female airman\"]],\n        [\"executor\",         [\"executrix\"]],\n        [\"prosecutor\",       [\"prosecutrix\"]],\n        [\"testator\",         [\"testatrix\"]],\n        [\"husband and wife\", [\"man and wife\"]],\n        [\"chairs\",           [\"chairmen and chairs\"]],\n        [\"men and women\",    [\"men and girls\"]],\n        [\"comedian\",         [\"comedienne\"]],\n        [\"confidant\",        [\"confidante\"]],\n        [\"scientist\",        [\"woman scientist\"]],\n        [\"scientists\",       [\"women scientists\"]]\n        # [\"hero\",             [\"heroine\"]]\n    ]\n\n    errors = preferred_forms_check(text, sexism, err, msg, ignore_case=False)\n\n    msg = \"Not a preferred form. Use '{}' instead of '{}'.\"\n    pref = [\n            [\"anchor\",           [\"anchorperson\"]],\n            [\"chair\",            [\"chairperson\"]],\n            [\"drafter\",          [\"draftperson\"]],\n            [\"ombuds\",           [\"ombudsperson\"]],\n            [\"tribe member\",     [\"tribesperson\"]],\n            [\"police officer\",   [\"policeperson\"]],\n            [\"firefighter\",      [\"fireperson\"]],\n            [\"mail carrier\",     [\"mailperson\"]],\n    ]\n    for x in preferred_forms_check(text, pref, err, msg, ignore_case=False):\n        errors.append(x)\n\n    return errors", "response": "Suggest the preferred forms."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_decade_apostrophes_short(text):\n    err = \"dates_times.dates\"\n    msg = u\"Apostrophes aren't needed for decades.\"\n\n    regex = \"\\d0\\'s\"\n\n    return existence_check(\n        text, [regex], err, msg, excluded_topics=[\"50 Cent\"])", "response": "Check the text for dates of the form X0 s."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the text for dates of the form XXX0 s.", "response": "def check_decade_apostrophes_long(text):\n    \"\"\"Check the text for dates of the form XXX0's.\"\"\"\n    err = \"dates_times.dates\"\n    msg = u\"Apostrophes aren't needed for decades.\"\n\n    regex = \"\\d\\d\\d0\\'s\"\n    return existence_check(text, [regex], err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the text for a dash and from.", "response": "def check_dash_and_from(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"dates_times.dates\"\n    msg = u\"When specifying a date range, write 'from X to Y'.\"\n\n    regex = \"[fF]rom \\d+[^ \\t\\n\\r\\f\\va-zA-Z0-9_\\.]\\d+\"\n    return existence_check(text, [regex], err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the text for month and year comma - separated.", "response": "def check_month_year_comma(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"dates_times.dates\"\n    msg = u\"When specifying a month and year, no comma is needed.\"\n\n    regex = \"(?:\" + \"|\".join(calendar.month_name[1:]) + \"), \\d{3,}\"\n    return existence_check(text, [regex], err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the text for a month of year.", "response": "def check_month_of_year(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"dates_times.dates\"\n    msg = u\"When specifying a month and year, 'of' is unnecessary.\"\n\n    regex = \"(?:\" + \"|\".join(calendar.month_name[1:]) + \") of \\d{3,}\"\n    return existence_check(text, [regex], err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsuggesting the preferred forms.", "response": "def check(text):\n    \"\"\"Suggest the preferred forms.\"\"\"\n    err = \"redundancy.wallace\"\n    msg = \"Redundancy. Use '{}' instead of '{}'.\"\n\n    redundancies = [\n        [\"rectangular\",        [\"rectangular in shape\"]],\n        [\"audible\",            [\"audible to the ear\"]],\n    ]\n    return preferred_forms_check(text, redundancies, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsuggesting the preferred forms of Garner.", "response": "def check_garner(text):\n    \"\"\"Suggest the preferred forms.\n\n    source:     Garner's Modern American Usage\n    source_url: http://bit.ly/1T4alrY\n    \"\"\"\n    err = \"redundancy.garner\"\n    msg = \"Redundancy. Use '{}' instead of '{}'.\"\n\n    redundancies = [\n        [\"adequate\",          [\"adequate enough\"]],\n        [\"admitted\",          [\"self-admitted\"]],\n        [\"affidavit\",         [\"sworn affidavit\"]],\n        [\"agreement\",         [\"mutual agreement\"]],\n        [\"alumnus\",           [\"former alumnus\"]],\n        [\"antithetical\",      [\"directly antithetical\"]],\n        [\"approximately\",     [\"approximately about\"]],\n        [\"associate\",         [\"associate together(?: in groups)?\"]],\n        [\"bivouac\",           [\"temporary bivouac\", \"bivouac camp\"]],\n        [\"blend\",             [\"blend together\"]],\n        [\"but\",               [\"but nevertheless\"]],\n        [\"charged with...\",   [\"accused of a charge\"]],\n        [\"circumstances of\",  [\"circumstances surrounding\"]],\n        [\"circumstances\",     [\"surrounding circumstances\"]],\n        [\"close\",             [\"close proximity\"]],\n        [\"collaborate\",       [\"collaborate together\"]],\n        [\"collaborator\",      [\"fellow collaborator\"]],\n        [\"collaborators\",     [\"fellow collaborators\"]],\n        [\"collocated\",        [\"collocated together\"]],\n        [\"colleagues\",        [\"fellow colleagues\"]],\n        [\"combine\",           [\"combine together\"]],\n        [\"complacent\",        [\"self-complacent\"]],\n        [\"confessed\",         [\"self-confessed\"]],\n        [\"connect\",           [\"connect together\"]],\n        [\"consensus\",         [\"(?:general )?consensus of opinion\"]],\n        [\"consolidate\",       [\"consolidate together\"]],\n        [\"continues to\",      [\"still continues to\"]],\n        [\"contradictory\",     [\"mutually contradictory\"]],\n        [\"cooperation\",       [\"mutual cooperation\"]],\n        [\"couple\",            [\"couple together\"]],\n        [\"crisis\",            [\"serious crisis\"]],\n        [\"eliminate\",         [\"entirely eliminate\"]],\n        [\"especially\",        [\"most especially\"]],\n        [\"fact\",              [\"actual fact\"]],\n        [\"facts\",             [\"true facts\"]],\n        [\"forecast\",          [\"future forecast\"]],\n        [\"founding fathers\",  [\"founding forefathers\"]],\n        [\"free\",              [\"free and gratis\"]],\n        [\"free\",              [\"free gratis\"]],\n        [\"full\",              [\"completely full\"]],\n        [\"fundamentals\",      [\"basic fundamentals\"]],\n        [\"gift\",              [\"free gift\"]],\n        [\"innovation\",        [\"new innovation\"]],\n        [\"interact\",          [\"interact with each other\"]],\n        [\"large\",             [\"large-size\"]],\n        [\"meld\",              [\"meld together\"]],\n        [\"merge\",             [\"merge together\"]],\n        [\"mingle\",            [\"mingle together\"]],\n        [\"mix\",               [\"mix together\"]],\n        [\"mutual feelings\",   [\"mutual feelings for eachother\"]],\n        [\"mutual respect\",    [\"mutual respect for each other\"]],\n        [\"native citizen\",    [\"native-born citizen\"]],\n        [\"necessity\",         [\"absolute necessity\"]],\n        [\"obvious\",           [\"blatantly obvious\"]],\n        [\"pause\",             [\"pause for a moment\"]],\n        [\"planning\",          [\"advance planning\"]],\n        [\"plans\",             [\"future plans\"]],\n        [\"pooled\",            [\"pooled together\"]],\n        [\"potable water\",     [\"potable drinking water\"]],\n        [\"potable water\",     [\"potable drinking water\"]],\n        [\"recruit\",           [\"new recruit\"]],\n        [\"reelected\",         [\"reelected for another term\"]],\n        [\"refer\",             [\"refer back\"]],\n        [\"regress\",           [\"regress back\"]],\n        [\"repay them\",        [\"repay them back\"]],\n        [\"repay\",             [\"repay back\"]],\n        [\"repeat\",            [\"repeat again\"]],\n        [\"repeat\",            [\"repeat back\"]],\n        [\"repeat\",            [\"repeat the same\"]],\n        [\"repeated\",          [\"repeated the same\"]],\n        [\"reprieve\",          [\"temporary reprieve\"]],\n        [\"respite\",           [\"brief respite\"]],\n        [\"retirement\",        [\"retiral\", \"retiracy\"]],\n        [\"retreat\",           [\"retreat back\"]],\n        [\"return\",            [\"return back\"]],\n        [\"scrutinize\",        [\"closely scrutinize\"]],\n        [\"software\",          [\"software program\"]],\n        [\"surrounded\",        [\"surrounded on all sides\"]],\n        [\"the nation\",        [\"the whole entire nation\"]],\n        [\"throughout the\",    [\"throughout the entire\"]],\n        [\"timpani\",           [\"timpani drum\"]],\n        [\"twins\",             [\"pair of twins\"]],\n        [\"vacancy\",           [\"unfilled vacancy\"]],\n        [\"various\",           [\"various different\"]],\n        [\"veteran\",           [\"former veteran\"]],\n        [\"visible\",           [\"visible to the eye\"]],\n        [\"vocation\",          [\"professional vocation\"]],\n        [\"while\",             [\"while at the same time\"]],\n    ]\n\n    return preferred_forms_check(text, redundancies, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsuggest the preferred forms.", "response": "def check_nordquist(text):\n    \"\"\"Suggest the preferred forms.\n\n    source:     Richard Nordquist\n    source_url: http://grammar.about.com/bio/Richard-Nordquist-22176.htm\n    \"\"\"\n    err = \"redundancy.nordquist\"\n    msg = \"Redundancy. Use '{}' instead of '{}'.\"\n\n    redundancies = [\n        [\"essential\",          [\"absolutely essential\"]],\n        [\"necessary\",          [\"absolutely necessary\"]],\n        [\"a.m.\",               [\"a.m. in the morning\"]],\n        [\"p.m.\",               [\"p.m. at night\"]],\n    ]\n    return preferred_forms_check(text, redundancies, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_atd(text):\n    err = \"after_the_deadline.redundancy\"\n    msg = \"Redundancy. Use '{}' instead of '{}'.\"\n\n    redundancies = [\n        [u\"B\u014d\",               [\"Bo Staff\"]],\n        [\"Challah\",           [\"Challah bread\"]],\n        [\"Hallah\",            [\"Hallah bread\"]],\n        [\"Challah\",           [\"Challah bread\"]],\n        [\"I\",                 [\"I myself\", \"I personally\"]],\n        [\"Mount Fuji\",        [\"Mount Fujiyama\"]],\n        [\"Milky Way\",         [\"Milky Way galaxy\"]],\n        [\"Rio Grande\",        [\"Rio Grande river\"]],\n        [\"adage\",             [\"old adage\"]],\n        [\"add\",               [\"add a further\", \"add an additional\"]],\n        [\"advance\",           [\"advance forward\"]],\n        [\"alternative\",       [\"alternative choice\"]],\n        [\"amaretto\",          [\"amaretto almond\"]],\n        [\"annihilate\",        [\"completely annihilate\"]],\n        [\"anniversary\",       [\"annual anniversary\"]],\n        [\"anonymous\",         [\"unnamed anonymous\"]],\n        [\"as\",                [\"equally as\"]],\n        [\"ascend\",            [\"ascend up\"]],\n        [\"ask\",               [\"ask the question\"]],\n        [\"assemble\",          [\"assemble together\"]],\n        [\"at present the\",    [\"at the present time the\"]],\n        [\"at this point\",     [\"at this point in time\"]],\n        [\"attach\",            [\"attach together\"]],\n        [\"autumn\",            [\"autumn season\"]],\n        [\"bald\",              [\"bald-headed\"]],\n        [\"balsa\",             [\"balsa wood\"]],\n        [\"belongings\",        [\"personal belongings\"]],\n        [\"benefits\",          [\"desirable benefits\"]],\n        [\"bento\",             [\"bento box\"]],\n        [\"best\",              [\"best ever\"]],\n        [\"bit\",               [\"tiny bit\"]],\n        [\"blend\",             [\"blend together\"]],\n        [\"bond\",              [\"common bond\"]],\n        [\"bonus\",             [\"added bonus\", \"extra bonus\"]],\n        [\"bouquet\",           [\"bouquet of flowers\"]],\n        [\"breakthrough\",      [\"major breakthrough\"]],\n        [\"bride\",             [\"new bride\"]],\n        [\"brief\",             [\"brief in duration\"]],\n        [\"bruin\",             [\"bruin bear\"]],\n        [\"hot\",               [\"burning hot\"]],\n        [\"cacophony\",         [\"cacophony of sound\"]],\n        [\"cameo\",             [\"brief cameo\", \"cameo appearance\"]],\n        [\"cancel\",            [\"cancel out\"]],\n        [\"cash\",              [\"cash money\"]],\n        [\"chai\",              [\"chai tea\"]],\n        [\"chance\",            [\"random chance\"]],\n        [\"charm\",             [\"personal charm\"]],\n        [\"circle\",            [\"circle around\", \"round circle\"]],\n        [\"circulate\",         [\"circulate around\"]],\n        [\"classify\",          [\"classify into groups\"]],\n        [\"classmates\",        [\"fellow classmates\"]],\n        [\"cliche\",            [\"old cliche\", \"overused cliche\"]],\n        [\"climb\",             [\"climb up\"]],\n        [\"clock\",             [\"time clock\"]],\n        [\"collaborate\",       [\"collaborate together\"]],\n        [\"collaboration\",     [\"joint collaboration\"]],\n        [\"colleague\",         [\"fellow colleague\"]],\n        [\"combine\",           [\"combine together\"]],\n        [\"commute\",           [\"commute back and forth\"]],\n        [\"compete\",           [\"compete with each other\"]],\n        [\"comprise\",          [\"comprise of\"]],\n        [\"comprises\",         [\"comprises of\"]],\n        [\"conceived\",         [\"first conceived\"]],\n        [\"conclusion\",        [\"final conclusion\"]],\n        [\"confer\",            [\"confer together\"]],\n        [\"confrontation\",     [\"direct confrontation\"]],\n        # [\"confused\",          [\"confused state\"]],\n        [\"connect\",           [\"connect together\", \"connect up\"]],\n        [\"consensus\",         [\"consensus of opinion\", \"general consensus\"]],\n        [\"consult\",           [\"consult with\"]],\n        [\"conversation\",      [\"oral conversation\"]],\n        [\"cool\",              [\"cool down\"]],\n        [\"cooperate\",         [\"cooperate together\"]],\n        [\"cooperation\",       [\"mutual cooperation\"]],\n        [\"copy\",              [\"duplicate copy\"]],\n        [\"core\",              [\"inner core\"]],\n        [\"cost\",              [\"cost the sum of\"]],\n        [\"could\",             [\"could possibly\"]],\n        [\"coupon\",            [\"money-saving coupon\"]],\n        [\"created\",           [\"originally created\"]],\n        [\"crisis\",            [\"crisis situation\"]],\n        [\"crouch\",            [\"crouch down\"]],\n        [\"currently\",         [\"now currently\"]],\n        [\"custom\",            [\"old custom\", \"usual custom\"]],\n        [\"danger\",            [\"serious danger\"]],\n        [\"dates\",             [\"dates back\"]],\n        [\"decision\",          [\"definite decision\"]],\n        [\"depreciate\",        [\"depreciate in value\"]],\n        [\"descend\",           [\"descend down\"]],\n        [\"destroy\",           [\"totally destroy\"]],\n        [\"destroyed\",         [\"completely destroyed\"]],\n        [\"destruction\",       [\"total destruction\"]],\n        [\"details\",           [\"specific details\"]],\n        [\"dilemma\",           [\"difficult dilemma\"]],\n        [\"disappear\",         [\"disappear from sight\"]],\n        [\"discovered\",        [\"originally discovered\"]],\n        [\"dive\",              [\"dive down\"]],\n        [\"done\",              [\"over and done with\"]],\n        [\"drawing\",           [\"illustrated drawing\"]],\n        [\"drop\",              [\"drop down\"]],\n        [\"dune\",              [\"sand dune\"]],\n        [\"during\",            [\"during the course of\"]],\n        [\"dwindle\",           [\"dwindle down\"]],\n        [\"dwindled\",          [\"dwindled down\"]],\n        [\"every\",             [\"each and every\"]],\n        [\"earlier\",           [\"earlier in time\"]],\n        [\"eliminate\",         [\"completely eliminate\", \"eliminate altogether\",\n                               \"entirely eliminate\"]],\n        [\"ember\",             [\"glowing ember\"]],\n        [\"embers\",            [\"burning embers\"]],\n        [\"emergency\",         [\"emergency situation\", \"unexpected emergency\"]],\n        [\"empty\",             [\"empty out\"]],\n        [\"enclosed\",          [\"enclosed herein\"]],\n        [\"end\",               [\"final end\"]],\n        [\"engulfed\",          [\"completely engulfed\"]],\n        [\"enter\",             [\"enter in\", \"enter into\"]],\n        [\"equal\",             [\"equal to one another\"]],\n        [\"eradicate\",         [\"eradicate completely\"]],\n        [\"essential\",         [\"absolutely essential\"]],\n        [\"estimated at\",      [\"estimated at about\",\n                               \"estimated at approximately\",\n                               \"estimated at around\"]],\n        [\"etc.\",              [\"and etc.\"]],\n        [\"evolve\",            [\"evolve over time\"]],\n        [\"exaggerate\",        [\"over exaggerate\"]],\n        [\"exited\",            [\"exited from\"]],\n        [\"experience\",        [\"actual experience\", \"past experience\"]],\n        [\"experts\",           [\"knowledgeable experts\"]],\n        [\"extradite\",         [\"extradite back\"]],\n        [\"face the consequences\", [\"face up to the consequences\"]],\n        [\"face the fact\",     [\"face up to the fact\"]],\n        [\"face the challenge\", [\"face up to the challenge\"]],\n        [\"face the problem\",  [\"face up to the problem\"]],\n        [\"facilitate\",        [\"facilitate easier\"]],\n        [\"fact\",              [\"established fact\"]],\n        [\"facts\",             [\"actual facts\", \"hard facts\", \"true facts\"]],\n        [\"fad\",               [\"passing fad\"]],\n        [\"fall\",              [\"fall down\"]],\n        [\"fall\",              [\"fall season\"]],\n        [\"feat\",              [\"major feat\"]],\n        [\"feel\",              [\"feel inside\"]],\n        [\"feelings\",          [\"inner feelings\"]],\n        [\"few\",               [\"few in number\"]],\n        [\"filled\",            [\"completely filled\", \"filled to capacity\"]],\n        [\"first\",             [\"first of all\"]],\n        [\"first time\",        [\"first time ever\"]],\n        [\"fist\",              [\"closed fist\"]],\n        [\"fly\",               [\"fly through the air\"]],\n        [\"focus\",             [\"focus in\", \"main focus\"]],\n        [\"follow\",            [\"follow after\"]],\n        [\"for example\",       [\"as for example\"]],\n        # [\"foremost\",          [\"first and foremost\"]],\n        [\"forever\",           [\"forever and ever\"]],\n        [\"free\",              [\"for free\"]],\n        [\"friend\",            [\"personal friend\"]],\n        [\"friendship\",        [\"personal friendship\"]],\n        [\"full\",              [\"full to capacity\"]],\n        [\"fundamentals\",      [\"basic fundamentals\"]],\n        [\"fuse\",              [\"fuse together\"]],\n        [\"gather\",            [\"gather together\", \"gather up\"]],\n        [\"get up\",            [\"get up on his feet\", \"get up on your feet\"]],\n        [\"gift\",              [\"free gift\"]],\n        [\"gifts\",             [\"free gifts\"]],\n        [\"goal\",              [\"ultimate goal\"]],\n        # [\"graduate\",          [\"former graduate\"]],\n        [\"grow\",              [\"grow in size\"]],\n        [\"guarantee\",         [\"absolute guarantee\"]],\n        [\"gunman\",            [\"armed gunman\"]],\n        [\"gunmen\",            [\"armed gunmen\"]],\n        [\"habitat\",           [\"native habitat\"]],\n        [\"had done\",          [\"had done previously\"]],\n        [\"halves\",            [\"two equal halves\"]],\n        # [\"has\",               [\"has got\"]],\n        # [\"have\",              [\"have got\"]],\n        [\"haven\",             [\"safe haven\"]],\n        # [\"he\",                [\"he himself\"]],\n        [\"heat\",              [\"heat up\"]],\n        [\"history\",           [\"past history\"]],\n        [\"hoist\",             [\"hoist up\"]],\n        [\"hole\",              [\"empty hole\"]],\n        [\"honcho\",            [\"head honcho\"]],\n        [\"ice\",               [\"frozen ice\"]],\n        [\"ideal\",             [\"perfect ideal\"]],\n        [\"identical\",         [\"same identical\"]],\n        [\"identification\",    [\"positive identification\"]],\n        [\"imports\",           [\"foreign imports\"]],\n        [\"impulse\",           [\"sudden impulse\"]],\n        [\"in fact\",           [\"in actual fact\"]],\n        [\"in the yard\",       [\"outside in the yard\"]],\n        [\"inclusive\",         [\"all inclusive\"]],\n        [\"incredible\",        [\"incredible to believe\"]],\n        [\"incumbent\",         [\"present incumbent\"]],\n        # [\"indicted\",          [\"indicted on a charge\"]],\n        [\"industry\",          [\"private industry\"]],\n        [\"injuries\",          [\"harmful injuries\"]],\n        [\"innovation\",        [\"new innovation\"]],\n        [\"innovative\",        [\"innovative new\", \"new innovative\"]],\n        # [\"input\",             [\"input into\"]],\n        [\"instinct\",          [\"natural instinct\", \"naturally instinct\"]],\n        [\"integrate\",         [\"integrate together\",\n                               \"integrate with each other\"]],\n        [\"interdependent\",    [\"interdependent on each other\",\n                               \"mutually interdependent\"]],\n        [\"introduced\",        [\"introduced for the first time\"]],\n        [\"invention\",         [\"new invention\"]],\n        [\"kneel\",             [\"kneel down\"]],\n        [\"knots\",             [\"knots per hour\"]],\n        # [\"last\",              [\"last of all\"]],\n        # [\"later\",             [\"later time\"]],\n        [\"lift\",              [\"lift up\"]],\n        [\"lingers\",           [\"still lingers\"]],\n        [\"look to the future\", [\"look ahead to the future\"]],\n        [\"love triangle\",     [\"three-way love triangle\"]],\n        [\"maintained\",        [\"constantly maintained\"]],\n        [\"manually\",          [\"manually by hand\"]],\n        [\"marina\",            [\"boat marina\"]],\n        [\"may\",               [\"may possibly\"]],\n        [\"meet\",              [\"meet together\", \"meet with each other\"]],\n        [\"memories\",          [\"past memories\"]],\n        [\"merge\",             [\"merge together\"]],\n        [\"merged\",            [\"merged together\"]],\n        [\"meshed\",            [\"meshed together\"]],\n        [\"midnight\",          [\"twelve midnight\"]],\n        [\"migraine\",          [\"migraine headache\"]],\n        [\"minestrone\",        [\"minestrone soup\"]],\n        [\"mix\",               [\"mix together\"]],\n        [\"moment\",            [\"brief moment\", \"moment in time\"]],\n        [\"monopoly\",          [\"complete monopoly\"]],\n        [\"mural\",             [\"wall mural\"]],\n        [\"mutual respect\",    [\"mutual respect for each other\"]],\n        [\"mutually dependent\", [\"mutually dependent on each other\"]],\n        [\"mystery\",           [\"unsolved mystery\"]],\n        # [\"naked\",             [\"bare naked\"]],\n        [\"nape\",              [\"nape of her neck\"]],\n        [\"necessary\",         [\"absolutely necessary\"]],\n        [\"never\",             [\"never at any time\"]],\n        [\"noon\",              [\"12 noon\", \"12 o'clock noon\", \"high noon\",\n                               \"twelve noon\"]],\n        [\"nostalgia\",         [\"nostalgia for the past\"]],\n        [\"number of\",         [\"number of different\"]],\n        [\"opening\",           [\"exposed opening\"]],\n        [\"my opinion\",        [\"my personal opinion\"]],\n        [\"opposites\",         [\"exact opposites\", \"polar opposites\"]],\n        [\"opposite\",          [\"exact opposite\", \"polar opposite\"]],\n        [\"orbits\",            [\"orbits around\"]],\n        [\"outcome\",           [\"final outcome\"]],\n        [\"panacea\",           [\"universal panacea\"]],\n        [\"pending\",           [\"now pending\"]],\n        [\"penetrate\",         [\"penetrate through\"]],\n        [\"persists\",          [\"still persists\"]],\n        [\"pioneer\",           [\"old pioneer\"]],\n        [\"plan\",              [\"plan ahead\", \"plan in advance\",\n                               \"proposed plan\"]],\n        [\"planning\",          [\"advance planning\", \"forward planning\"]],\n        [\"plans\",             [\"future plans\"]],\n        [\"plan\",              [\"future plan\"]],\n        [\"point\",             [\"point in time\"]],\n        [\"point\",             [\"sharp point\"]],\n        [\"postpone\",          [\"postpone until later\"]],\n        [\"pouring rain\",      [\"pouring down rain\"]],\n        [\"preview\",           [\"advance preview\"]],\n        [\"previously listed\", [\"previously listed above\"]],\n        [\"probed\",            [\"probed into\"]],\n        [\"proceed\",           [\"proceed ahead\"]],\n        [\"prosthesis\",        [\"artificial prosthesis\"]],\n        # [\"protrude\",          [\"protrude out\"]],\n        [\"proverb\",           [\"old proverb\"]],\n        # [\"proximity\",         [\"close proximity\"]],\n        [\"put off\",           [\"put off until later\"]],\n        # [\"raise\",             [\"raise up\"]],\n        [\"re-elect\",          [\"re-elect for another term\"]],\n        [\"reason is\",         [\"reason is because\"]],\n        [\"recur\",             [\"recur again\"]],\n        [\"recurrence\",        [\"future recurrence\"]],\n        [\"refer\",             [\"refer back\"]],\n        [\"reflect\",           [\"reflect back\"]],\n        # [\"relevant\",          [\"highly relevant\"]],\n        [\"remain\",            [\"continue to remain\"]],\n        [\"remains\",           [\"still remains\"]],\n        [\"replica\",           [\"exact replica\"]],\n        [\"reply\",             [\"reply back\"]],\n        # [\"requirements\",      [\"necessary requirements\"]],\n        [\"reservations\",      [\"advance reservations\"]],\n        [\"retreat\",           [\"retreat back\"]],\n        [\"revert\",            [\"revert back\"]],\n        [\"round\",             [\"round in shape\"]],\n        [\"rule of thumb\",     [\"rough rule of thumb\"]],\n        [\"rumor\",             [\"unconfirmed rumor\"]],\n        [\"rustic\",            [\"rustic country\"]],\n        [\"same\",              [\"exact same\", \"precise same\", \"same exact\"]],\n        [\"sanctuary\",         [\"safe sanctuary\"]],\n        [\"satisfaction\",      [\"full satisfaction\"]],\n        [\"scrutinize\",        [\"scrutinize in detail\"]],\n        [\"scrutiny\",          [\"careful scrutiny\", \"close scrutiny\"]],\n        [\"secret\",            [\"secret that cannot be told\"]],\n        [\"seek\",              [\"seek to find\"]],\n        [\"separated\",         [\"separated apart from each other\"]],\n        [\"share\",             [\"share together\"]],\n        [\"shiny\",             [\"shiny in appearance\"]],\n        [\"sincere\",           [\"truly sincere\"]],\n        [\"sink\",              [\"sink down\"]],\n        [\"skipped\",           [\"skipped over\"]],\n        # [\"slow\",              [\"slow speed\"]],\n        # [\"small\",             [\"small size\"]],\n        [\"soft\",              [\"soft in texture\", \"soft to the touch\"]],\n        [\"sole\",              [\"sole of the foot\"]],\n        [\"some time\",         [\"some time to come\"]],\n        [\"speck\",             [\"small speck\"]],\n        [\"speed\",             [\"rate of speed\"]],\n        [\"spell out\",         [\"spell out in detail\"]],\n        [\"spiked\",            [\"spiked upward\", \"spiked upwards\"]],\n        [\"spring\",            [\"spring season\"]],\n        [\"stranger\",          [\"anonymous stranger\"]],\n        [\"studio audience\",   [\"live studio audience\"]],\n        [\"subway\",            [\"underground subway\"]],\n        [\"sufficient\",        [\"sufficient enough\"]],\n        [\"summer\",            [\"summer season\"]],\n        [\"sure\",              [\"absolutely sure\"]],\n        [\"surprise\",          [\"unexpected surprise\"]],\n        [\"surround\",          [\"completely surround\"]],\n        [\"surrounded\",        [\"surrounded on all sides\"]],\n        [\"tall\",              [\"tall in height\", \"tall in stature\"]],\n        [\"telepathy\",         [\"mental telepathy\"]],\n        [\"ten\",               [\"ten in number\"]],\n        [\"these\",             [\"these ones\"]],\n        # [\"they\",              [\"they themselves\"]],\n        [\"those\",             [\"those ones\"]],\n        [\"trench\",            [\"open trench\"]],\n        [\"truth\",             [\"honest truth\"]],\n        [\"tundra\",            [\"frozen tundra\"]],\n        [\"ultimatum\",         [\"final ultimatum\"]],\n        # [\"undeniable\",        [\"undeniable truth\"]],\n        [\"undergraduate\",     [\"undergraduate student\"]],\n        # [\"unintentional\",     [\"unintentional mistake\"]],\n        [\"vacillate\",         [\"vacillate back and forth\"]],\n        [\"veteran\",           [\"former veteran\"]],\n        [\"visible\",           [\"visible to the eye\"]],\n        [\"warn\",              [\"warn in advance\"]],\n        [\"warning\",           [\"advance warning\"]],\n        [\"water heater\",      [\"hot water heater\"]],\n        [\"in which we live\",  [\"in which we live in\"]],\n        [\"winter\",            [\"winter season\"]],\n        [\"witness\",           [\"live witness\"]],\n        [\"yakitori\",          [\"yakitori chicken\"]],\n        [\"yerba mate\",        [\"yerba mate tea\"]],\n        [\"yes\",               [\"affirmative yes\"]],\n    ]\n\n    return preferred_forms_check(text, redundancies, err, msg)", "response": "Check for redundancies from After the Deadline."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncaches results of computations on disk.", "response": "def memoize(f):\n    \"\"\"Cache results of computations on disk.\"\"\"\n    # Determine the location of the cache.\n    cache_dirname = os.path.join(_get_xdg_cache_home(), 'proselint')\n    legacy_cache_dirname = os.path.join(os.path.expanduser(\"~\"), \".proselint\")\n\n    if not os.path.isdir(cache_dirname):\n        # Migrate the cache from the legacy path to XDG complaint location.\n        if os.path.isdir(legacy_cache_dirname):\n            os.rename(legacy_cache_dirname, cache_dirname)\n        # Create the cache if it does not already exist.\n        else:\n            os.makedirs(cache_dirname)\n\n    cache_filename = f.__module__ + \".\" + f.__name__\n    cachepath = os.path.join(cache_dirname, cache_filename)\n\n    @functools.wraps(f)\n    def wrapped(*args, **kwargs):\n\n        # handle instance methods\n        if hasattr(f, '__self__'):\n            args = args[1:]\n\n        signature = (f.__module__ + '.' + f.__name__).encode(\"utf-8\")\n\n        tempargdict = inspect.getcallargs(f, *args, **kwargs)\n\n        for item in list(tempargdict.items()):\n            signature += item[1].encode(\"utf-8\")\n\n        key = hashlib.sha256(signature).hexdigest()\n\n        try:\n            cache = _get_cache(cachepath)\n            return cache[key]\n        except KeyError:\n            value = f(*args, **kwargs)\n            cache[key] = value\n            cache.sync()\n            return value\n        except TypeError:\n            call_to = f.__module__ + '.' + f.__name__\n            print('Warning: could not disk cache call to %s;'\n                  'it probably has unhashable args. Error: %s' %\n                  (call_to, traceback.format_exc()))\n            return f(*args, **kwargs)\n\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads various proselintrc files allowing user overrides.", "response": "def load_options():\n    \"\"\"Read various proselintrc files, allowing user overrides.\"\"\"\n    possible_defaults = (\n        '/etc/proselintrc',\n        os.path.join(proselint_path, '.proselintrc'),\n    )\n    options = {}\n    has_overrides = False\n\n    for filename in possible_defaults:\n        try:\n            options = json.load(open(filename))\n            break\n        except IOError:\n            pass\n\n    try:\n        user_options = json.load(\n            open(os.path.join(_get_xdg_config_home(), 'proselint', 'config')))\n        has_overrides = True\n    except IOError:\n        pass\n\n    # Read user configuration from the legacy path.\n    if not has_overrides:\n        try:\n            user_options = json.load(\n                open(os.path.join(os.path.expanduser('~'), '.proselintrc')))\n            has_overrides = True\n        except IOError:\n            pass\n\n    if has_overrides:\n        if 'max_errors' in user_options:\n            options['max_errors'] = user_options['max_errors']\n        if 'checks' in user_options:\n            for (key, value) in user_options['checks'].items():\n                try:\n                    options['checks'][key] = value\n                except KeyError:\n                    pass\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef errors_to_json(errors):\n    out = []\n    for e in errors:\n        out.append({\n            \"check\": e[0],\n            \"message\": e[1],\n            \"line\": 1 + e[2],\n            \"column\": 1 + e[3],\n            \"start\": 1 + e[4],\n            \"end\": 1 + e[5],\n            \"extent\": e[6],\n            \"severity\": e[7],\n            \"replacements\": e[8],\n        })\n\n    return json.dumps(\n        dict(status=\"success\", data={\"errors\": out}), sort_keys=True)", "response": "Convert the errors to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef line_and_column(text, position):\n    position_counter = 0\n    for idx_line, line in enumerate(text.splitlines(True)):\n        if (position_counter + len(line.rstrip())) >= position:\n            return (idx_line, position - position_counter)\n        else:\n            position_counter += len(line)", "response": "Return the line number and column of a position in a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the linter on the input file.", "response": "def lint(input_file, debug=False):\n    \"\"\"Run the linter on the input file.\"\"\"\n    options = load_options()\n\n    if isinstance(input_file, string_types):\n        text = input_file\n    else:\n        text = input_file.read()\n\n    # Get the checks.\n    checks = get_checks(options)\n\n    # Apply all the checks.\n    errors = []\n    for check in checks:\n\n        result = check(text)\n\n        for error in result:\n            (start, end, check, message, replacements) = error\n            (line, column) = line_and_column(text, start)\n            if not is_quoted(start, text):\n                errors += [(check, message, line, column, start, end,\n                           end - start, \"warning\", replacements)]\n\n        if len(errors) > options[\"max_errors\"]:\n            break\n\n    # Sort the errors by line and column number.\n    errors = sorted(errors[:options[\"max_errors\"]], key=lambda e: (e[2], e[3]))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nassert that text has n errors of type check.", "response": "def assert_error(text, check, n=1):\n    \"\"\"Assert that text has n errors of type check.\"\"\"\n    assert_error.description = \"No {} error for '{}'\".format(check, text)\n    assert(check in [error[0] for error in lint(text)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a consistency checker for the given word_pairs.", "response": "def consistency_check(text, word_pairs, err, msg, offset=0):\n    \"\"\"Build a consistency checker for the given word_pairs.\"\"\"\n    errors = []\n\n    msg = \" \".join(msg.split())\n\n    for w in word_pairs:\n        matches = [\n            [m for m in re.finditer(w[0], text)],\n            [m for m in re.finditer(w[1], text)]\n        ]\n\n        if len(matches[0]) > 0 and len(matches[1]) > 0:\n\n            idx_minority = len(matches[0]) > len(matches[1])\n\n            for m in matches[idx_minority]:\n                errors.append((\n                    m.start() + offset,\n                    m.end() + offset,\n                    err,\n                    msg.format(w[~idx_minority], m.group(0)),\n                    w[~idx_minority]))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a checker that suggests the preferred form.", "response": "def preferred_forms_check(text, list, err, msg, ignore_case=True, offset=0,\n                          max_errors=float(\"inf\")):\n    \"\"\"Build a checker that suggests the preferred form.\"\"\"\n    if ignore_case:\n        flags = re.IGNORECASE\n    else:\n        flags = 0\n\n    msg = \" \".join(msg.split())\n\n    errors = []\n    regex = u\"[\\W^]{}[\\W$]\"\n    for p in list:\n        for r in p[1]:\n            for m in re.finditer(regex.format(r), text, flags=flags):\n                txt = m.group(0).strip()\n                errors.append((\n                    m.start() + 1 + offset,\n                    m.end() + offset,\n                    err,\n                    msg.format(p[0], txt),\n                    p[0]))\n\n    errors = truncate_to_max(errors, max_errors)\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef existence_check(text, list, err, msg, ignore_case=True,\n                    str=False, max_errors=float(\"inf\"), offset=0,\n                    require_padding=True, dotall=False,\n                    excluded_topics=None, join=False):\n    \"\"\"Build a checker that blacklists certain words.\"\"\"\n    flags = 0\n\n    msg = \" \".join(msg.split())\n\n    if ignore_case:\n        flags = flags | re.IGNORECASE\n\n    if str:\n        flags = flags | re.UNICODE\n\n    if dotall:\n        flags = flags | re.DOTALL\n\n    if require_padding:\n        regex = u\"(?:^|\\W){}[\\W$]\"\n    else:\n        regex = u\"{}\"\n\n    errors = []\n\n    # If the topic of the text is in the excluded list, return immediately.\n    if excluded_topics:\n        tps = topics(text)\n        if any([t in excluded_topics for t in tps]):\n            return errors\n\n    rx = \"|\".join(regex.format(w) for w in list)\n    for m in re.finditer(rx, text, flags=flags):\n        txt = m.group(0).strip()\n        errors.append((\n            m.start() + 1 + offset,\n            m.end() + offset,\n            err,\n            msg.format(txt),\n            None))\n\n    errors = truncate_to_max(errors, max_errors)\n\n    return errors", "response": "Build a checker that blacklists certain words."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef truncate_to_max(errors, max_errors):\n    if len(errors) > max_errors:\n        start1, end1, err1, msg1, replacements = errors[0]\n\n        if len(errors) == (max_errors + 1):\n            msg1 += \" Found once elsewhere.\"\n        else:\n            msg1 += \" Found {} times elsewhere.\".format(len(errors))\n\n        errors = errors[1:max_errors]\n        errors = [(start1, end1, err1, msg1, replacements)] + errors\n\n    return errors", "response": "Truncate the list of errors to max_errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if the position in the text falls within a quote.", "response": "def is_quoted(position, text):\n    \"\"\"Determine if the position in the text falls within a quote.\"\"\"\n    def matching(quotemark1, quotemark2):\n        straight = '\\\"\\''\n        curly = '\u201c\u201d'\n        if quotemark1 in straight and quotemark2 in straight:\n            return True\n        if quotemark1 in curly and quotemark2 in curly:\n            return True\n        else:\n            return False\n\n    def find_ranges(text):\n        s = 0\n        q = pc = ''\n        start = None\n        ranges = []\n        seps = \" .,:;-\\r\\n\"\n        quotes = ['\\\"', '\u201c', '\u201d', \"'\"]\n        for i, c in enumerate(text + \"\\n\"):\n            if s == 0 and c in quotes and pc in seps:\n                start = i\n                s = 1\n                q = c\n            elif s == 1 and matching(c, q):\n                s = 2\n            elif s == 2:\n                if c in seps:\n                    ranges.append((start+1, i-1))\n                    start = None\n                    s = 0\n                else:\n                    s = 1\n            pc = c\n        return ranges\n\n    def position_in_ranges(ranges, position):\n        for start, end in ranges:\n            if start <= position < end:\n                return True\n        return False\n\n    return position_in_ranges(find_ranges(text), position)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining whether 50 Cent is a topic.", "response": "def detector_50_Cent(text):\n    \"\"\"Determine whether 50 Cent is a topic.\"\"\"\n    keywords = [\n        \"50 Cent\",\n        \"rap\",\n        \"hip hop\",\n        \"Curtis James Jackson III\",\n        \"Curtis Jackson\",\n        \"Eminem\",\n        \"Dre\",\n        \"Get Rich or Die Tryin'\",\n        \"G-Unit\",\n        \"Street King Immortal\",\n        \"In da Club\",\n        \"Interscope\",\n    ]\n    num_keywords = sum(word in text for word in keywords)\n    return (\"50 Cent\", float(num_keywords > 2))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of topics.", "response": "def topics(text):\n    \"\"\"Return a list of topics.\"\"\"\n    detectors = [\n        detector_50_Cent\n    ]\n    ts = []\n    for detector in detectors:\n        ts.append(detector(text))\n\n    return [t[0] for t in ts if t[1] > 0.95]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsuggesting the preferred forms.", "response": "def check(text):\n    \"\"\"Suggest the preferred forms.\"\"\"\n    err = \"pinker.metadiscourse\"\n    msg = \"Excessive metadiscourse.\"\n\n    metadiscourse = [\n        \"The preceeding discussion\",\n        \"The rest of this article\",\n        \"This chapter discusses\",\n        \"The preceding paragraph demonstrated\",\n        \"The previous section analyzed\",\n    ]\n\n    return existence_check(text, metadiscourse, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the text for existence.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"cursing.filth\"\n    msg = u\"\"\"Nobody ever tells you this as a kid, but you're supposed to avoid\n        this word.\"\"\"\n\n    list = [\n        \"shit\",\n        \"piss\",\n        \"fuck\",\n        \"cunt\",\n        \"cocksucker\",\n        \"motherfucker\",\n        \"tits\",\n        \"fart\",\n        \"turd\",\n        \"twat\",\n    ]\n\n    return existence_check(text, list, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check(text):\n    err = \"oxymorons.misc\"\n    msg = u\"'{}' is an oxymoron.\"\n\n    oxymorons = [\n        \"amateur expert\",\n        \"increasingly less\",\n        \"advancing backwards?\",\n        \"alludes explicitly to\",\n        \"explicitly alludes to\",\n        \"totally obsolescent\",\n        \"completely obsolescent\",\n        \"generally always\",\n        \"usually always\",\n        \"increasingly less\",\n        \"build down\",\n        \"conspicuous absence\",\n        \"exact estimate\",\n        \"found missing\",\n        \"intense apathy\",\n        \"mandatory choice\",\n        \"nonworking mother\",\n        \"organized mess\",\n        # \"pretty ugly\",\n        # \"sure bet\",\n        # \"executive secretary\",\n    ]\n\n    return existence_check(text, oxymorons, err, msg, offset=1, join=True)", "response": "Check the text for oxymorons."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check(text):\n    err = \"misc.waxed\"\n    msg = u\"The modifier following 'waxed' must be an adj.: '{}' is correct\"\n\n    waxes = [\"wax\", \"waxes\", \"waxed\", \"waxing\"]\n    modifiers = [(\"ebullient\", \"ebulliently\"),\n                 (\"ecstatic\", \"ecstatically\"),\n                 (\"eloquent\", \"eloquently\"),\n                 (\"enthusiastic\", \"enthusiastically\"),\n                 (\"euphoric\", \"euphorically\"),\n                 (\"indignant\", \"indignantly\"),\n                 (\"lyrical\", \"lyrically\"),\n                 (\"melancholic\", \"melancholically\"),\n                 (\"metaphorical\", \"metaphorically\"),\n                 (\"nostalgic\", \"nostalgically\"),\n                 (\"patriotic\", \"patriotically\"),\n                 (\"philosophical\", \"philosophically\"),\n                 (\"poetic\", \"poetically\"),\n                 (\"rhapsodic\", \"rhapsodically\"),\n                 (\"romantic\", \"romantically\"),\n                 (\"sentimental\", \"sentimentally\")\n                 ]\n\n    def pairs(word):\n        return [[word + ' ' + pair[0], [word + ' ' + pair[1]]]\n                for pair in modifiers]\n\n    preferred = []\n    for word in waxes:\n        preferred += pairs(word)\n\n    return preferred_forms_check(text, preferred, err, msg)", "response": "Suggest the preferred forms."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the text for lexical illusion.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"lexical_illusions.misc\"\n    msg = u\"There's a lexical illusion here: a word is repeated.\"\n\n    list = [\n        \"the\\sthe\",\n        \"am\\sam\",\n        \"has\\shas\"\n    ]\n\n    return existence_check(text, list, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks the text for password existence.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"security.password\"\n    msg = u\"Don't put passwords in plain text.\"\n\n    pwd_regex = \"[:]? [\\S]{6,30}\"\n\n    password = [\n        \"the password is{}\".format(pwd_regex),\n        \"my password is{}\".format(pwd_regex),\n        \"the password's{}\".format(pwd_regex),\n        \"my password's{}\".format(pwd_regex),\n        \"^[pP]assword{}\".format(pwd_regex),\n    ]\n\n    return existence_check(text, password, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the given text contains an ellipsis.", "response": "def check_ellipsis(text):\n    \"\"\"Use an ellipsis instead of three dots.\"\"\"\n    err = \"typography.symbols.ellipsis\"\n    msg = u\"'...' is an approximation, use the ellipsis symbol '\u2026'.\"\n    regex = \"\\.\\.\\.\"\n\n    return existence_check(text, [regex], err, msg, max_errors=3,\n                           require_padding=False, offset=0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the copyright symbol is a goofy alphabetic approximation.", "response": "def check_copyright_symbol(text):\n    \"\"\"Use the copyright symbol instead of (c).\"\"\"\n    err = \"typography.symbols.copyright\"\n    msg = u\"(c) is a goofy alphabetic approximation, use the symbol \u00a9.\"\n    regex = \"\\([cC]\\)\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=1, require_padding=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks that the symbol is a goofy alphabetic approximation.", "response": "def check_trademark_symbol(text):\n    \"\"\"Use the trademark symbol instead of (TM).\"\"\"\n    err = \"typography.symbols.trademark\"\n    msg = u\"(TM) is a goofy alphabetic approximation, use the symbol \u2122.\"\n    regex = \"\\(TM\\)\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=3, require_padding=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_registered_trademark_symbol(text):\n    err = \"typography.symbols.trademark\"\n    msg = u\"(R) is a goofy alphabetic approximation, use the symbol \u00ae.\"\n    regex = \"\\([rR]\\)\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=3, require_padding=False)", "response": "Check that the symbol is a goofy alphabetic approximation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck that the text contains at least two sentence spacing.", "response": "def check_sentence_spacing(text):\n    \"\"\"Use no more than two spaces after a period.\"\"\"\n    err = \"typography.symbols.sentence_spacing\"\n    msg = u\"More than two spaces after the period; use 1 or 2.\"\n    regex = \"\\. {3}\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=3, require_padding=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that multiplication symbol is valid.", "response": "def check_multiplication_symbol(text):\n    u\"\"\"Use the multiplication symbol \u00d7, not the lowercase letter x.\"\"\"\n    err = \"typography.symbols.multiplication_symbol\"\n    msg = u\"Use the multiplication symbol \u00d7, not the letter x.\"\n    regex = \"[0-9]+ ?x ?[0-9]+\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=3, require_padding=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_curly_quotes(text):\n    err = \"typography.symbols.curly_quotes\"\n    msg = u'Use curly quotes \u201c\u201d, not straight quotes \"\".'\n\n    list = [\n        [u\"\u201c or \u201d\", ['\"']],\n    ]\n\n    return preferred_forms_check(\n        text, list, err, msg, ignore_case=False, max_errors=2)", "response": "Check that the text is valid curly quotes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the text for bad words and return a list of errors.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"strunk_white.greylist\"\n    msg = \"Use of '{}'. {}\"\n\n    bad_words = [\n        \"obviously\",\n        \"utilize\"\n    ]\n\n    explanations = {\n        \"obviously\":\n        \"This is obviously an inadvisable word to use.\",\n        \"utilize\":\n        r\"Do you know anyone who *needs* to utilize the word utilize?\"\n    }\n\n    errors = []\n    for word in bad_words:\n        occ = [m for m in re.finditer(word, text.lower())]\n        for o in occ:\n            errors.append((\n                o.start(),\n                o.end(),\n                err,\n                msg.format(word, explanations[word]),\n                None))\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the mail account and lint new mail.", "response": "def check_email():\n    \"\"\"Check the mail account and lint new mail.\"\"\"\n    server = smtplib.SMTP(\"smtp.gmail.com\", 587)\n    server.ehlo()\n    server.starttls()\n    server.ehlo()\n    server.login(user, password)\n\n    g = gmail.login(user, password)\n\n    # Check for unread messages.\n    unread = g.inbox().mail(unread=True)\n\n    # Submit a job to lint each email sent to editor@proselint.com. Record the\n    # resulting job_ids somewhere (in Redis, I suppose), keyed by a hash of the\n    # email.\n    for u in unread:\n\n        u.fetch()\n\n        signature = (u.fr.decode('utf-8') +\n                     u.subject.decode('utf-8') +\n                     u.body.decode('utf-8'))\n\n        hash = hashlib.sha256(signature.encode('utf-8')).hexdigest()\n\n        if user_to in u.to or user_to in u.headers.get('Cc', []):\n\n            job_id = conn.get(hash)\n\n            if not job_id:\n                # If the email hasn't been sent for processing, send it.\n                r = requests.post(api_url, data={\"text\": u.body})\n                conn.set(hash, r.json()[\"job_id\"])\n                print(\"Email {} sent for processing.\".format(hash))\n\n            else:\n                # Otherwise, check whether the results are ready, and if so,\n                # reply with them.\n                r = requests.get(api_url, params={\"job_id\": job_id})\n\n                if r.json()[\"status\"] == \"success\":\n\n                    reply = quoted(u.body)\n                    errors = r.json()['data']['errors']\n                    reply += \"\\r\\n\\r\\n\".join([json.dumps(e) for e in errors])\n\n                    msg = MIMEMultipart()\n                    msg[\"From\"] = \"{} <{}>\".format(name, user)\n                    msg[\"To\"] = u.fr\n                    msg[\"Subject\"] = \"Re: \" + u.subject\n\n                    if u.headers.get('Message-ID'):\n                        msg.add_header(\"In-Reply-To\", u.headers['Message-ID'])\n                        msg.add_header(\"References\", u.headers['Message-ID'])\n\n                    body = reply + \"\\r\\n\\r\\n--\\r\\n\" + tagline + \"\\r\\n\" + url\n                    msg.attach(MIMEText(body, \"plain\"))\n\n                    text = msg.as_string()\n                    server.sendmail(user, u.fr, text)\n\n                    # Mark the email as read.\n                    u.read()\n                    u.archive()\n\n                    print(\"Email {} has been replied to.\".format(hash))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check(text):\n    err = \"spelling.athletes\"\n    msg = \"Misspelling of athlete's name. '{}' is the preferred form.\"\n\n    misspellings = [\n        [\"Dwyane Wade\",         [\"Dwayne Wade\"]],\n        [\"Miikka Kiprusoff\",    [\"Mikka Kiprusoff\"]],\n        [\"Mark Buehrle\",        [\"Mark Buerhle\"]],\n        [\"Skylar Diggins\",      [\"Skyler Diggins\"]],\n        [\"Agnieszka Radwanska\", [\"Agnieska Radwanska\"]],\n        [\"J.J. Redick\",         [\"J.J. Reddick\"]],\n        [\"Manny Pacquiao\",      [\"Manny Packquaio\"]],\n        [\"Antawn Jamison\",      [\"Antwan Jamison\"]],\n        [\"Cal Ripken\",          [\"Cal Ripkin\"]],\n        [\"Jhonny Peralta\",      [\"Johnny Peralta\"]],\n        [\"Monta Ellis\",         [\"Monte Ellis\"]],\n        [\"Alex Rodriguez\",      [\"Alex Rodriquez\"]],\n        [\"Mark Teixeira\",       [\"Mark Texeira\"]],\n        [\"Brett Favre\",         [\"Brett Farve\"]],\n        [\"Torii Hunter\",        [\"Tori Hunter\"]],\n        [\"Stephen Curry\",       [\"Stephon Curry\"]],\n        [\"Mike Krzyzewski\",     [\"Mike Kryzewski\"]],\n    ]\n\n    return preferred_forms_check(text, misspellings, err, msg)", "response": "Suggest the preferred forms of athletes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_repeated_exclamations(text):\n    err = \"leonard.hell\"\n    msg = u\"Never use the words 'all hell broke loose'.\"\n\n    regex = r\"all hell broke loose\"\n\n    return existence_check(\n        text, [regex], err, msg, max_errors=1)", "response": "Check the text for repeated exclusion."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_cliches_garner(text):\n    err = \"cliches.garner\"\n    msg = u\"'{}' is clich\u00e9.\"\n\n    cliches = [\n        \"a fate worse than death\",\n        \"alas and alack\",\n        \"at the end of the day\",\n        \"bald-faced lie\",\n        \"between a rock and a hard place\",\n        \"between Scylla and Charybdis\",\n        \"between the devil and the deep blue sea\",\n        \"betwixt and between\",\n        \"blissful ignorance\",\n        \"blow a fuse\",\n        \"bulk large\",\n        \"but that's another story\",\n        \"cast aspersions\",\n        \"chase a red herring\",\n        \"comparing apples and oranges\",\n        \"compleat\",\n        \"conspicuous by its absence\",\n        \"crystal clear\",\n        \"cutting edge\",\n        \"decision-making process\",\n        \"dubious distinction\",\n        \"duly authorized\",\n        \"eyes peeled\",\n        \"far be it from me\",\n        \"fast and loose\",\n        \"fills the bill\",\n        \"first and foremost\",\n        \"for free\",\n        \"get with the program\",\n        \"gilding the lily\",\n        \"have a short fuse\",\n        \"he's got his hands full\",\n        \"his own worst enemy\",\n        \"his work cut out for him\",\n        \"hither and yon\",\n        \"Hobson's choice\",\n        \"horns of a dilemma\",\n        \"if you catch my drift\",\n        \"in light of\",\n        \"in the final analysis\",\n        \"in the last analysis\",\n        \"innocent bystander\",\n        \"it's not what you know, it's who you know\",\n        \"last but not least\",\n        \"make a mockery of\",\n        \"male chauvinism\",\n        \"moment of truth\",\n        \"more in sorrow than in anger\",\n        \"more sinned against than sinning\",\n        \"my better half\",\n        \"nip in the bud\",\n        \"olden days\",\n        \"on the same page\",\n        \"presidential timber\",\n        \"pulled no punches\",\n        \"quantum jump\",\n        \"quantum leap\",\n        \"redound to one's credit\",\n        \"redound to the benefit of\",\n        \"sea change\",\n        \"shirked his duties\",\n        \"six of one, half a dozen of the other\",\n        \"stretched to the breaking point\",\n        \"than you can shake a stick at\",\n        \"the cream of the crop\",\n        \"the cream rises to the top\",\n        \"the straw that broke the camel's back\",\n        \"thick as thieves\",\n        \"thinking outside the box\",\n        \"thought leaders?\",\n        \"throw the baby out with the bathwater\",\n        \"various and sundry\",\n        \"viable alternative\",\n        \"wax eloquent\",\n        \"wax poetic\",\n        \"we've got a situation here\",\n        \"whet (?:the|your) appetite\",\n        \"wool pulled over our eyes\",\n        \"writ large\",\n    ]\n    return existence_check(text, cliches, err, msg, join=True)", "response": "Check the text for cliches."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the text for cliches.", "response": "def check_cliches_write_good(text):\n    \"\"\"Check the text.\n\n    source:     write-good\n    source_url: https://github.com/btford/write-good\n    \"\"\"\n    err = \"cliches.write_good\"\n    msg = u\"'{}' is a clich\u00e9.\"\n\n    cliches = [\n        \"a chip off the old block\",\n        \"a clean slate\",\n        \"a dark and stormy night\",\n        \"a far cry\",\n        \"a fine kettle of fish\",\n        \"a loose cannon\",\n        \"a penny saved is a penny earned\",\n        \"a tough row to hoe\",\n        \"a word to the wise\",\n        \"ace in the hole\",\n        \"acid test\",\n        \"add insult to injury\",\n        \"against all odds\",\n        \"air your dirty laundry\",\n        \"all fun and games\",\n        \"all in a day's work\",\n        \"all talk, no action\",\n        \"all thumbs\",\n        \"all your eggs in one basket\",\n        \"all's fair in love and war\",\n        \"all's well that ends well\",\n        \"almighty dollar\",\n        \"American as apple pie\",\n        \"an axe to grind\",\n        \"another day, another dollar\",\n        \"armed to the teeth\",\n        \"as luck would have it\",\n        \"as old as time\",\n        \"as the crow flies\",\n        \"at loose ends\",\n        \"at my wits end\",\n        \"avoid like the plague\",\n        \"babe in the woods\",\n        \"back against the wall\",\n        \"back in the saddle\",\n        \"back to square one\",\n        \"back to the drawing board\",\n        \"bad to the bone\",\n        \"badge of honor\",\n        \"bald faced liar\",\n        \"ballpark figure\",\n        \"banging your head against a brick wall\",\n        \"baptism by fire\",\n        \"barking up the wrong tree\",\n        \"bat out of hell\",\n        \"be all and end all\",\n        \"beat a dead horse\",\n        \"beat around the bush\",\n        \"been there, done that\",\n        \"beggars can't be choosers\",\n        \"behind the eight ball\",\n        \"bend over backwards\",\n        \"benefit of the doubt\",\n        \"bent out of shape\",\n        \"best thing since sliced bread\",\n        \"bet your bottom dollar\",\n        \"better half\",\n        \"better late than never\",\n        \"better mousetrap\",\n        \"better safe than sorry\",\n        \"between a rock and a hard place\",\n        \"beyond the pale\",\n        \"bide your time\",\n        \"big as life\",\n        \"big cheese\",\n        \"big fish in a small pond\",\n        \"big man on campus\",\n        \"bigger they are the harder they fall\",\n        \"bird in the hand\",\n        \"bird's eye view\",\n        \"birds and the bees\",\n        \"birds of a feather flock together\",\n        \"bit the hand that feeds you\",\n        \"bite the bullet\",\n        \"bite the dust\",\n        \"bitten off more than he can chew\",\n        \"black as coal\",\n        \"black as pitch\",\n        \"black as the ace of spades\",\n        \"blast from the past\",\n        \"bleeding heart\",\n        \"blessing in disguise\",\n        \"blind ambition\",\n        \"blind as a bat\",\n        \"blind leading the blind\",\n        \"blood is thicker than water\",\n        \"blood sweat and tears\",\n        \"blow off steam\",\n        \"blow your own horn\",\n        \"blushing bride\",\n        \"boils down to\",\n        \"bolt from the blue\",\n        \"bone to pick\",\n        \"bored stiff\",\n        \"bored to tears\",\n        \"bottomless pit\",\n        \"boys will be boys\",\n        \"bright and early\",\n        \"brings home the bacon\",\n        \"broad across the beam\",\n        \"broken record\",\n        \"brought back to reality\",\n        \"bull by the horns\",\n        \"bull in a china shop\",\n        \"burn the midnight oil\",\n        \"burning question\",\n        \"burning the candle at both ends\",\n        \"burst your bubble\",\n        \"bury the hatchet\",\n        \"busy as a bee\",\n        \"by hook or by crook\",\n        \"call a spade a spade\",\n        \"called onto the carpet\",\n        \"calm before the storm\",\n        \"can of worms\",\n        \"can't cut the mustard\",\n        \"can't hold a candle to\",\n        \"case of mistaken identity\",\n        \"cat got your tongue\",\n        \"cat's meow\",\n        \"caught in the crossfire\",\n        \"caught red-handed\",\n        \"checkered past\",\n        \"chomping at the bit\",\n        \"cleanliness is next to godliness\",\n        \"clear as a bell\",\n        \"clear as mud\",\n        \"close to the vest\",\n        \"cock and bull story\",\n        \"cold shoulder\",\n        \"come hell or high water\",\n        \"cool as a cucumber\",\n        \"cool, calm, and collected\",\n        \"cost a king's ransom\",\n        \"count your blessings\",\n        \"crack of dawn\",\n        \"crash course\",\n        \"creature comforts\",\n        \"cross that bridge when you come to it\",\n        \"crushing blow\",\n        \"cry like a baby\",\n        \"cry me a river\",\n        \"cry over spilt milk\",\n        \"crystal clear\",\n        \"curiosity killed the cat\",\n        \"cut and dried\",\n        \"cut through the red tape\",\n        \"cut to the chase\",\n        \"cute as a bugs ear\",\n        \"cute as a button\",\n        \"cute as a puppy\",\n        \"cuts to the quick\",\n        \"dark before the dawn\",\n        \"day in, day out\",\n        \"dead as a doornail\",\n        \"devil is in the details\",\n        \"dime a dozen\",\n        \"divide and conquer\",\n        \"dog and pony show\",\n        \"dog days\",\n        \"dog eat dog\",\n        \"dog tired\",\n        \"don't burn your bridges\",\n        \"don't count your chickens\",\n        \"don't look a gift horse in the mouth\",\n        \"don't rock the boat\",\n        \"don't step on anyone's toes\",\n        \"don't take any wooden nickels\",\n        \"down and out\",\n        \"down at the heels\",\n        \"down in the dumps\",\n        \"down the hatch\",\n        \"down to earth\",\n        \"draw the line\",\n        \"dressed to kill\",\n        \"dressed to the nines\",\n        \"drives me up the wall\",\n        \"dull as dishwater\",\n        \"dyed in the wool\",\n        \"eagle eye\",\n        \"ear to the ground\",\n        \"early bird catches the worm\",\n        \"easier said than done\",\n        \"easy as pie\",\n        \"eat your heart out\",\n        \"eat your words\",\n        \"eleventh hour\",\n        \"even the playing field\",\n        \"every dog has its day\",\n        \"every fiber of my being\",\n        \"everything but the kitchen sink\",\n        \"eye for an eye\",\n        \"face the music\",\n        \"facts of life\",\n        \"fair weather friend\",\n        \"fall by the wayside\",\n        \"fan the flames\",\n        \"feast or famine\",\n        \"feather your nest\",\n        \"feathered friends\",\n        \"few and far between\",\n        \"fifteen minutes of fame\",\n        \"filthy vermin\",\n        \"fine kettle of fish\",\n        \"fish out of water\",\n        \"fishing for a compliment\",\n        \"fit as a fiddle\",\n        \"fit the bill\",\n        \"fit to be tied\",\n        \"flash in the pan\",\n        \"flat as a pancake\",\n        \"flip your lid\",\n        \"flog a dead horse\",\n        \"fly by night\",\n        \"fly the coop\",\n        \"follow your heart\",\n        \"for all intents and purposes\",\n        \"for the birds\",\n        \"for what it's worth\",\n        \"force of nature\",\n        \"force to be reckoned with\",\n        \"forgive and forget\",\n        \"fox in the henhouse\",\n        \"free and easy\",\n        \"free as a bird\",\n        \"fresh as a daisy\",\n        \"full steam ahead\",\n        \"fun in the sun\",\n        \"garbage in, garbage out\",\n        \"gentle as a lamb\",\n        \"get a kick out of\",\n        \"get a leg up\",\n        \"get down and dirty\",\n        \"get the lead out\",\n        \"get to the bottom of\",\n        \"get your feet wet\",\n        \"gets my goat\",\n        \"gilding the lily\",\n        \"give and take\",\n        \"go against the grain\",\n        \"go at it tooth and nail\",\n        \"go for broke\",\n        \"go him one better\",\n        \"go the extra mile\",\n        \"go with the flow\",\n        \"goes without saying\",\n        \"good as gold\",\n        \"good deed for the day\",\n        \"good things come to those who wait\",\n        \"good time was had by all\",\n        \"good times were had by all\",\n        \"greased lightning\",\n        \"greek to me\",\n        \"green thumb\",\n        \"green-eyed monster\",\n        \"grist for the mill\",\n        \"growing like a weed\",\n        \"hair of the dog\",\n        \"hand to mouth\",\n        \"happy as a clam\",\n        \"happy as a lark\",\n        \"hasn't a clue\",\n        \"have a nice day\",\n        \"have high hopes\",\n        \"have the last laugh\",\n        \"haven't got a row to hoe\",\n        \"head honcho\",\n        \"head over heels\",\n        \"hear a pin drop\",\n        \"heard it through the grapevine\",\n        \"heart's content\",\n        \"heavy as lead\",\n        \"hem and haw\",\n        \"high and dry\",\n        \"high and mighty\",\n        \"high as a kite\",\n        \"hit paydirt\",\n        \"hold your head up high\",\n        \"hold your horses\",\n        \"hold your own\",\n        \"hold your tongue\",\n        \"honest as the day is long\",\n        \"horns of a dilemma\",\n        \"horse of a different color\",\n        \"hot under the collar\",\n        \"hour of need\",\n        \"I beg to differ\",\n        \"icing on the cake\",\n        \"if the shoe fits\",\n        \"if the shoe were on the other foot\",\n        \"in a jam\",\n        \"in a jiffy\",\n        \"in a nutshell\",\n        \"in a pig's eye\",\n        \"in a pinch\",\n        \"in a word\",\n        \"in hot water\",\n        \"in the gutter\",\n        \"in the nick of time\",\n        \"in the thick of it\",\n        \"in your dreams\",\n        \"it ain't over till the fat lady sings\",\n        \"it goes without saying\",\n        \"it takes all kinds\",\n        \"it takes one to know one\",\n        \"it's a small world\",\n        \"it's only a matter of time\",\n        \"ivory tower\",\n        \"Jack of all trades\",\n        \"jockey for position\",\n        \"jog your memory\",\n        \"joined at the hip\",\n        \"judge a book by its cover\",\n        \"jump down your throat\",\n        \"jump in with both feet\",\n        \"jump on the bandwagon\",\n        \"jump the gun\",\n        \"jump to conclusions\",\n        \"just a hop, skip, and a jump\",\n        \"just the ticket\",\n        \"justice is blind\",\n        \"keep a stiff upper lip\",\n        \"keep an eye on\",\n        \"keep it simple, stupid\",\n        \"keep the home fires burning\",\n        \"keep up with the Joneses\",\n        \"keep your chin up\",\n        \"keep your fingers crossed\",\n        \"kick the bucket\",\n        \"kick up your heels\",\n        \"kick your feet up\",\n        \"kid in a candy store\",\n        \"kill two birds with one stone\",\n        \"kiss of death\",\n        \"knock it out of the park\",\n        \"knock on wood\",\n        \"knock your socks off\",\n        \"know him from Adam\",\n        \"know the ropes\",\n        \"know the score\",\n        \"knuckle down\",\n        \"knuckle sandwich\",\n        \"knuckle under\",\n        \"labor of love\",\n        \"ladder of success\",\n        \"land on your feet\",\n        \"lap of luxury\",\n        \"last but not least\",\n        \"last hurrah\",\n        \"last-ditch effort\",\n        \"law of the jungle\",\n        \"law of the land\",\n        \"lay down the law\",\n        \"leaps and bounds\",\n        \"let sleeping dogs lie\",\n        \"let the cat out of the bag\",\n        \"let the good times roll\",\n        \"let your hair down\",\n        \"let's talk turkey\",\n        \"letter perfect\",\n        \"lick your wounds\",\n        \"lies like a rug\",\n        \"life's a bitch\",\n        \"life's a grind\",\n        \"light at the end of the tunnel\",\n        \"lighter than a feather\",\n        \"lighter than air\",\n        \"like clockwork\",\n        \"like father like son\",\n        \"like taking candy from a baby\",\n        \"like there's no tomorrow\",\n        \"lion's share\",\n        \"live and learn\",\n        \"live and let live\",\n        \"long and short of it\",\n        \"long lost love\",\n        \"look before you leap\",\n        \"look down your nose\",\n        \"look what the cat dragged in\",\n        \"looking a gift horse in the mouth\",\n        \"looks like death warmed over\",\n        \"loose cannon\",\n        \"lose your head\",\n        \"lose your temper\",\n        \"loud as a horn\",\n        \"lounge lizard\",\n        \"loved and lost\",\n        \"low man on the totem pole\",\n        \"luck of the draw\",\n        \"luck of the Irish\",\n        \"make hay while the sun shines\",\n        \"make money hand over fist\",\n        \"make my day\",\n        \"make the best of a bad situation\",\n        \"make the best of it\",\n        \"make your blood boil\",\n        \"man of few words\",\n        \"man's best friend\",\n        \"mark my words\",\n        \"meaningful dialogue\",\n        \"missed the boat on that one\",\n        \"moment in the sun\",\n        \"moment of glory\",\n        \"moment of truth\",\n        \"money to burn\",\n        \"more power to you\",\n        \"more than one way to skin a cat\",\n        \"movers and shakers\",\n        \"moving experience\",\n        \"naked as a jaybird\",\n        \"naked truth\",\n        \"neat as a pin\",\n        \"needle in a haystack\",\n        \"needless to say\",\n        \"neither here nor there\",\n        \"never look back\",\n        \"never say never\",\n        \"nip and tuck\",\n        \"nip it in the bud\",\n        \"no guts, no glory\",\n        \"no love lost\",\n        \"no pain, no gain\",\n        \"no skin off my back\",\n        \"no stone unturned\",\n        \"no time like the present\",\n        \"no use crying over spilled milk\",\n        \"nose to the grindstone\",\n        \"not a hope in hell\",\n        \"not a minute's peace\",\n        \"not in my backyard\",\n        \"not playing with a full deck\",\n        \"not the end of the world\",\n        \"not written in stone\",\n        \"nothing to sneeze at\",\n        \"nothing ventured nothing gained\",\n        \"now we're cooking\",\n        \"off the top of my head\",\n        \"off the wagon\",\n        \"off the wall\",\n        \"old hat\",\n        \"older and wiser\",\n        \"older than dirt\",\n        \"older than Methuselah\",\n        \"on a roll\",\n        \"on cloud nine\",\n        \"on pins and needles\",\n        \"on the bandwagon\",\n        \"on the money\",\n        \"on the nose\",\n        \"on the rocks\",\n        \"on the spot\",\n        \"on the tip of my tongue\",\n        \"on the wagon\",\n        \"on thin ice\",\n        \"once bitten, twice shy\",\n        \"one bad apple doesn't spoil the bushel\",\n        \"one born every minute\",\n        \"one brick short\",\n        \"one foot in the grave\",\n        \"one in a million\",\n        \"one red cent\",\n        \"only game in town\",\n        \"open a can of worms\",\n        \"open and shut case\",\n        \"open the flood gates\",\n        \"opportunity doesn't knock twice\",\n        \"out of pocket\",\n        \"out of sight, out of mind\",\n        \"out of the frying pan into the fire\",\n        \"out of the woods\",\n        \"out on a limb\",\n        \"over a barrel\",\n        \"over the hump\",\n        \"pain and suffering\",\n        \"pain in the\",\n        \"panic button\",\n        \"par for the course\",\n        \"part and parcel\",\n        \"party pooper\",\n        \"pass the buck\",\n        \"patience is a virtue\",\n        \"pay through the nose\",\n        \"penny pincher\",\n        \"perfect storm\",\n        \"pig in a poke\",\n        \"pile it on\",\n        \"pillar of the community\",\n        \"pin your hopes on\",\n        \"pitter patter of little feet\",\n        \"plain as day\",\n        \"plain as the nose on your face\",\n        \"play by the rules\",\n        \"play your cards right\",\n        \"playing the field\",\n        \"playing with fire\",\n        \"pleased as punch\",\n        \"plenty of fish in the sea\",\n        \"point with pride\",\n        \"poor as a church mouse\",\n        \"pot calling the kettle black\",\n        \"pretty as a picture\",\n        \"pull a fast one\",\n        \"pull your punches\",\n        \"pulling your leg\",\n        \"pure as the driven snow\",\n        \"put it in a nutshell\",\n        \"put one over on you\",\n        \"put the cart before the horse\",\n        \"put the pedal to the metal\",\n        \"put your best foot forward\",\n        \"put your foot down\",\n        \"quick as a bunny\",\n        \"quick as a lick\",\n        \"quick as a wink\",\n        \"quick as lightning\",\n        \"quiet as a dormouse\",\n        \"rags to riches\",\n        \"raining buckets\",\n        \"raining cats and dogs\",\n        \"rank and file\",\n        \"rat race\",\n        \"reap what you sow\",\n        \"red as a beet\",\n        \"red herring\",\n        \"reinvent the wheel\",\n        \"rich and famous\",\n        \"rings a bell\",\n        \"ripe old age\",\n        \"ripped me off\",\n        \"rise and shine\",\n        \"road to hell is paved with good intentions\",\n        \"rob Peter to pay Paul\",\n        \"roll over in the grave\",\n        \"rub the wrong way\",\n        \"ruled the roost\",\n        \"running in circles\",\n        \"sad but true\",\n        \"sadder but wiser\",\n        \"salt of the earth\",\n        \"scared stiff\",\n        \"scared to death\",\n        \"sealed with a kiss\",\n        \"second to none\",\n        \"see eye to eye\",\n        \"seen the light\",\n        \"seize the day\",\n        \"set the record straight\",\n        \"set the world on fire\",\n        \"set your teeth on edge\",\n        \"sharp as a tack\",\n        \"shoot for the moon\",\n        \"shoot the breeze\",\n        \"shot in the dark\",\n        \"shoulder to the wheel\",\n        \"sick as a dog\",\n        \"sigh of relief\",\n        \"signed, sealed, and delivered\",\n        \"sink or swim\",\n        \"six of one, half a dozen of another\",\n        \"skating on thin ice\",\n        \"slept like a log\",\n        \"slinging mud\",\n        \"slippery as an eel\",\n        \"slow as molasses\",\n        \"smart as a whip\",\n        \"smooth as a baby's bottom\",\n        \"sneaking suspicion\",\n        \"snug as a bug in a rug\",\n        \"sow wild oats\",\n        \"spare the rod, spoil the child\",\n        \"speak of the devil\",\n        \"spilled the beans\",\n        \"spinning your wheels\",\n        \"spitting image of\",\n        \"spoke with relish\",\n        \"spread like wildfire\",\n        \"spring to life\",\n        \"squeaky wheel gets the grease\",\n        \"stands out like a sore thumb\",\n        \"start from scratch\",\n        \"stick in the mud\",\n        \"still waters run deep\",\n        \"stitch in time\",\n        \"stop and smell the roses\",\n        \"straight as an arrow\",\n        \"straw that broke the camel's back\",\n        \"strong as an ox\",\n        \"stubborn as a mule\",\n        \"stuff that dreams are made of\",\n        \"stuffed shirt\",\n        \"sweating blood\",\n        \"sweating bullets\",\n        \"take a load off\",\n        \"take one for the team\",\n        \"take the bait\",\n        \"take the bull by the horns\",\n        \"take the plunge\",\n        \"takes one to know one\",\n        \"takes two to tango\",\n        \"the more the merrier\",\n        \"the real deal\",\n        \"the real McCoy\",\n        \"the red carpet treatment\",\n        \"the same old story\",\n        \"there is no accounting for taste\",\n        \"thick as a brick\",\n        \"thick as thieves\",\n        \"thin as a rail\",\n        \"think outside of the box\",\n        \"third time's the charm\",\n        \"this day and age\",\n        \"this hurts me worse than it hurts you\",\n        \"this point in time\",\n        \"three sheets to the wind\",\n        \"through thick and thin\",\n        \"throw in the towel\",\n        \"tie one on\",\n        \"tighter than a drum\",\n        \"time and time again\",\n        \"time is of the essence\",\n        \"tip of the iceberg\",\n        \"tired but happy\",\n        \"to coin a phrase\",\n        \"to each his own\",\n        \"to make a long story short\",\n        \"to the best of my knowledge\",\n        \"toe the line\",\n        \"tongue in cheek\",\n        \"too good to be true\",\n        \"too hot to handle\",\n        \"too numerous to mention\",\n        \"touch with a ten foot pole\",\n        \"tough as nails\",\n        \"trial and error\",\n        \"trials and tribulations\",\n        \"tried and true\",\n        \"trip down memory lane\",\n        \"twist of fate\",\n        \"two cents worth\",\n        \"two peas in a pod\",\n        \"ugly as sin\",\n        \"under the counter\",\n        \"under the gun\",\n        \"under the same roof\",\n        \"under the weather\",\n        \"until the cows come home\",\n        \"unvarnished truth\",\n        \"up the creek\",\n        \"uphill battle\",\n        \"upper crust\",\n        \"upset the applecart\",\n        \"vain attempt\",\n        \"vain effort\",\n        \"vanquish the enemy\",\n        \"vested interest\",\n        \"waiting for the other shoe to drop\",\n        \"wakeup call\",\n        \"warm welcome\",\n        \"watch your p's and q's\",\n        \"watch your tongue\",\n        \"watching the clock\",\n        \"water under the bridge\",\n        \"weather the storm\",\n        \"weed them out\",\n        \"week of Sundays\",\n        \"went belly up\",\n        \"wet behind the ears\",\n        \"what goes around comes around\",\n        \"what you see is what you get\",\n        \"when it rains, it pours\",\n        \"when push comes to shove\",\n        \"when the cat's away\",\n        \"when the going gets tough, the tough get going\",\n        \"white as a sheet\",\n        \"whole ball of wax\",\n        \"whole hog\",\n        \"whole nine yards\",\n        \"wild goose chase\",\n        \"will wonders never cease?\",\n        \"wisdom of the ages\",\n        \"wise as an owl\",\n        \"wolf at the door\",\n        \"words fail me\",\n        \"work like a dog\",\n        \"world weary\",\n        \"worst nightmare\",\n        \"worth its weight in gold\",\n        \"wrong side of the bed\",\n        \"yanking your chain\",\n        \"yappy as a dog\",\n        \"years young\",\n        \"you are what you eat\",\n        \"you can run but you can't hide\",\n        \"you only live once\",\n        \"you're the boss \",\n        \"young and foolish\",\n        \"young and vibrant\",\n    ]\n\n    return existence_check(text, cliches, err, msg, join=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks the text for cliches. GNU diction.", "response": "def check_cliches_gnu_diction(text):\n    \"\"\"Check the text.\n\n    source:     GNU diction\n    source_url: https://directory.fsf.org/wiki/Diction\n    \"\"\"\n    err = \"cliches.gnu_diction\"\n    msg = u\"'{}' is a clich\u00e9.\"\n\n    list = [\n        \"a matter of concern\",\n        \"all things being equal\",\n        \"as a last resort\",\n        \"attached hereto\",\n        \"by no means\",\n        \"conspicuous by its absence\",\n        \"easier said than done\",\n        \"enclosed herewith\",\n        \"if and when\",\n        \"in reference to\",\n        \"in short supply\",\n        \"in the foreseeable future\",\n        \"in the long run\",\n        \"in the matter of\",\n        \"it stands to reason\",\n        \"many and diverse\",\n        \"on the right track\",\n        \"par for the course\",\n        \"please feel free to\",\n        \"pursuant to your request\",\n        \"regarding the matter of\",\n        \"slowly but surely\",\n        \"this will acknowledge\",\n        \"we are pleased to advice\",\n        \"we regret to inform you\",\n        \"we wish to state\",\n        \"you are hereby advised that\",\n    ]\n\n    return existence_check(text, list, err, msg, join=True, ignore_case=True)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking the text for airlinese.", "response": "def check(text):\n    \"\"\"Check the text.\"\"\"\n    err = \"airlinese.misc\"\n    msg = u\"'{}' is airlinese.\"\n\n    airlinese = [\n        \"enplan(?:e|ed|ing|ement)\",\n        \"deplan(?:e|ed|ing|ement)\",\n        \"taking off momentarily\",\n    ]\n\n    return existence_check(text, airlinese, err, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(text):\n    err = \"misc.currency\"\n    msg = u\"Incorrect use of symbols in {}.\"\n\n    symbols = [\n        \"\\$[\\d]* ?(?:dollars|usd|us dollars)\"\n    ]\n\n    return existence_check(text, symbols, err, msg)", "response": "Check the text for correct use of symbols."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\navoids mixing metaphors about bottles with big necks.", "response": "def check_bottleneck(text):\n    \"\"\"Avoid mixing metaphors about bottles and their necks.\n\n    source:     Sir Ernest Gowers\n    source_url: http://bit.ly/1CQPH61\n    \"\"\"\n    err = \"mixed_metaphors.misc.bottleneck\"\n    msg = u\"Mixed metaphor \u2014 bottles with big necks are easy to pass through.\"\n    list = [\n        \"biggest bottleneck\",\n        \"big bottleneck\",\n        \"large bottleneck\",\n        \"largest bottleneck\",\n        \"world-wide bottleneck\",\n        \"huge bottleneck\",\n        \"massive bottleneck\",\n    ]\n\n    return existence_check(text, list, err, msg, max_errors=1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_misc(text):\n    err = \"mixed_metaphors.misc.misc\"\n    msg = u\"Mixed metaphor. Try '{}'.\"\n\n    preferences = [\n\n        [\"cream rises to the top\",    [\"cream rises to the crop\"]],\n        [\"fasten your seatbelts\",     [\"button your seatbelts\"]],\n        [\"a minute to decompress\",    [\"a minute to decompose\"]],\n        [\"sharpest tool in the shed\", [\"sharpest marble in the (shed|box)\"]],\n        [\"not rocket science\",        [\"not rocket surgery\"]],\n    ]\n\n    return preferred_forms_check(text, preferences, err, msg)", "response": "Avoid mixing metaphors.\n\n    source:     Garner's Modern American Usage\n    source_url: http://bit.ly/1T4alrY"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the list of revisions.", "response": "def revisions(self, path, max_revisions):\n        \"\"\"\n        Get the list of revisions.\n\n        :param path: the path to target.\n        :type  path: ``str``\n\n        :param max_revisions: the maximum number of revisions.\n        :type  max_revisions: ``int``\n\n        :return: A list of revisions.\n        :rtype: ``list`` of :class:`Revision`\n        \"\"\"\n        if self.repo.is_dirty():\n            raise DirtyGitRepositoryError(self.repo.untracked_files)\n\n        revisions = []\n        for commit in self.repo.iter_commits(\n            self.current_branch, max_count=max_revisions\n        ):\n            rev = Revision(\n                key=commit.name_rev.split(\" \")[0],\n                author_name=commit.author.name,\n                author_email=commit.author.email,\n                date=commit.committed_date,\n                message=commit.message,\n            )\n            revisions.append(rev)\n        return revisions"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef checkout(self, revision, options):\n        rev = revision.key\n        self.repo.git.checkout(rev)", "response": "Checkout a specific revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef finish(self):\n        self.repo.git.checkout(self.current_branch)\n        self.repo.close()", "response": "Clean up any state if processing completed and failed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_cache_path(path):\n    logger.debug(f\"Generating cache for {path}\")\n    sha = hashlib.sha1(str(path).encode()).hexdigest()[:9]\n    HOME = pathlib.Path.home()\n    cache_path = str(HOME / \".wily\" / sha)\n    logger.debug(f\"Cache path is {cache_path}\")\n    return cache_path", "response": "Generate a reusable path to cache results."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the configuration file and set values to defaults where no present.", "response": "def load(config_path=DEFAULT_CONFIG_PATH):\n    \"\"\"\n    Load config file and set values to defaults where no present.\n\n    :return: The configuration ``WilyConfig``\n    :rtype: :class:`wily.config.WilyConfig`\n    \"\"\"\n    if not pathlib.Path(config_path).exists():\n        logger.debug(f\"Could not locate {config_path}, using default config.\")\n        return DEFAULT_CONFIG\n\n    config = configparser.ConfigParser(default_section=DEFAULT_CONFIG_SECTION)\n    config.read(config_path)\n\n    operators = config.get(\n        section=DEFAULT_CONFIG_SECTION, option=\"operators\", fallback=DEFAULT_OPERATORS\n    )\n    archiver = config.get(\n        section=DEFAULT_CONFIG_SECTION, option=\"archiver\", fallback=DEFAULT_ARCHIVER\n    )\n    path = config.get(section=DEFAULT_CONFIG_SECTION, option=\"path\", fallback=\".\")\n    max_revisions = int(\n        config.get(\n            section=DEFAULT_CONFIG_SECTION,\n            option=\"max_revisions\",\n            fallback=DEFAULT_MAX_REVISIONS,\n        )\n    )\n\n    return WilyConfig(\n        operators=operators, archiver=archiver, path=path, max_revisions=max_revisions\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreport the information about the cache and runtime.", "response": "def report(\n    config, path, metrics, n, output, include_message=False, format=ReportFormat.CONSOLE, console_format=None,\n):\n    \"\"\"\n    Show information about the cache and runtime.\n\n    :param config: The configuration\n    :type  config: :class:`wily.config.WilyConfig`\n\n    :param path: The path to the file\n    :type  path: ``str``\n\n    :param metrics: Name of the metric to report on\n    :type  metrics: ``str``\n\n    :param n: Number of items to list\n    :type  n: ``int``\n\n    :param output: Output path\n    :type  output: ``Path``\n\n    :param include_message: Include revision messages\n    :type  include_message: ``bool``\n\n    :param format: Output format\n    :type  format: ``ReportFormat``\n\n    :param console_format: Grid format style for tabulate\n    :type  console_format: ``str``\n    \"\"\"\n    logger.debug(\"Running report command\")\n    logger.info(f\"-----------History for {metrics}------------\")\n\n    data = []\n    metric_metas = []\n\n    for metric in metrics:\n        operator, metric = resolve_metric_as_tuple(metric)\n        key = metric.name\n        operator = operator.name\n        # Set the delta colors depending on the metric type\n        if metric.measure == MetricType.AimHigh:\n            good_color = 32\n            bad_color = 31\n        elif metric.measure == MetricType.AimLow:\n            good_color = 31\n            bad_color = 32\n        elif metric.measure == MetricType.Informational:\n            good_color = 33\n            bad_color = 33\n        metric_meta = {\n            \"key\": key,\n            \"operator\": operator,\n            \"good_color\": good_color,\n            \"bad_color\": bad_color,\n            \"title\": metric.description,\n            \"type\": metric.type,\n        }\n        metric_metas.append(metric_meta)\n\n    state = State(config)\n    for archiver in state.archivers:\n        # We have to do it backwards to get the deltas between releases\n        history = state.index[archiver].revisions[:n][::-1]\n        last = {}\n        for rev in history:\n            vals = []\n            for meta in metric_metas:\n                try:\n                    logger.debug(\n                        f\"Fetching metric {meta['key']} for {meta['operator']} in {path}\"\n                    )\n                    val = rev.get(config, archiver, meta[\"operator\"], path, meta[\"key\"])\n\n                    last_val = last.get(meta[\"key\"], None)\n                    # Measure the difference between this value and the last\n                    if meta[\"type\"] in (int, float):\n                        if last_val:\n                            delta = val - last_val\n                        else:\n                            delta = 0\n                        last[meta[\"key\"]] = val\n                    else:\n                        # TODO : Measure ranking increases/decreases for str types?\n                        delta = 0\n\n                    if delta == 0:\n                        delta_col = delta\n                    elif delta < 0:\n                        delta_col = f\"\\u001b[{meta['good_color']}m{delta:n}\\u001b[0m\"\n                    else:\n                        delta_col = f\"\\u001b[{meta['bad_color']}m+{delta:n}\\u001b[0m\"\n\n                    if meta[\"type\"] in (int, float):\n                        k = f\"{val:n} ({delta_col})\"\n                    else:\n                        k = f\"{val}\"\n                except KeyError as e:\n                    k = f\"Not found {e}\"\n                vals.append(k)\n            if include_message:\n                data.append(\n                    (\n                        format_revision(rev.revision.key),\n                        rev.revision.message[:MAX_MESSAGE_WIDTH],\n                        rev.revision.author_name,\n                        format_date(rev.revision.date),\n                        *vals,\n                    )\n                )\n            else:\n                data.append(\n                    (\n                        format_revision(rev.revision.key),\n                        rev.revision.author_name,\n                        format_date(rev.revision.date),\n                        *vals,\n                    )\n                )\n    descriptions = [meta[\"title\"] for meta in metric_metas]\n    if include_message:\n        headers = (\"Revision\", \"Message\", \"Author\", \"Date\", *descriptions)\n    else:\n        headers = (\"Revision\", \"Author\", \"Date\", *descriptions)\n\n    if format == ReportFormat.HTML:\n        if output.is_file and output.suffix == \".html\":\n            report_path = output.parents[0]\n            report_output = output\n        else:\n            report_path = output\n            report_output = output.joinpath(\"index.html\")\n\n        report_path.mkdir(exist_ok=True, parents=True)\n\n        templates_dir = (Path(__file__).parents[1] / \"templates\").resolve()\n        report_template = Template((templates_dir / \"report_template.html\").read_text())\n\n        table_headers = \"\".join([f\"<th>{header}</th>\" for header in headers])\n        table_content = \"\"\n        for line in data[::-1]:\n            table_content += \"<tr>\"\n            for element in line:\n                element = element.replace(\"[32m\", \"<span class='green-color'>\")\n                element = element.replace(\"[31m\", \"<span class='red-color'>\")\n                element = element.replace(\"[33m\", \"<span class='orange-color'>\")\n                element = element.replace(\"[0m\", \"</span>\")\n                table_content += f\"<td>{element}</td>\"\n            table_content += \"</tr>\"\n\n        report_template = report_template.safe_substitute(\n            headers=table_headers, content=table_content\n        )\n\n        with report_output.open(\"w\") as output:\n            output.write(report_template)\n\n        try:\n            copytree(str(templates_dir / \"css\"), str(report_path / \"css\"))\n        except FileExistsError:\n            pass\n\n        logger.info(f\"wily report was saved to {report_path}\")\n    else:\n        print(\n            # But it still makes more sense to show the newest at the top, so reverse again\n            tabulate.tabulate(\n                headers=headers, tabular_data=data[::-1], tablefmt=console_format\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing the differences in metrics for each of the files.", "response": "def diff(config, files, metrics, changes_only=True, detail=True):\n    \"\"\"\n    Show the differences in metrics for each of the files.\n\n    :param config: The wily configuration\n    :type  config: :namedtuple:`wily.config.WilyConfig`\n\n    :param files: The files to compare.\n    :type  files: ``list`` of ``str``\n\n    :param metrics: The metrics to measure.\n    :type  metrics: ``list`` of ``str``\n\n    :param changes_only: Only include changes files in output.\n    :type  changes_only: ``bool``\n\n    :param detail: Show details (function-level)\n    :type  detail: ``bool``\n    \"\"\"\n    config.targets = files\n    files = list(files)\n    state = State(config)\n    last_revision = state.index[state.default_archiver].revisions[0]\n\n    # Convert the list of metrics to a list of metric instances\n    operators = {resolve_operator(metric.split(\".\")[0]) for metric in metrics}\n    metrics = [(metric.split(\".\")[0], resolve_metric(metric)) for metric in metrics]\n    data = {}\n    results = []\n\n    # Build a set of operators\n    _operators = [operator.cls(config) for operator in operators]\n\n    cwd = os.getcwd()\n    os.chdir(config.path)\n    for operator in _operators:\n        logger.debug(f\"Running {operator.name} operator\")\n        data[operator.name] = operator.run(None, config)\n    os.chdir(cwd)\n\n    # Write a summary table..\n    extra = []\n    for operator, metric in metrics:\n        if detail and resolve_operator(operator).level == OperatorLevel.Object:\n            for file in files:\n                try:\n                    extra.extend(\n                        [\n                            f\"{file}:{k}\"\n                            for k in data[operator][file].keys()\n                            if k != metric.name\n                            and isinstance(data[operator][file][k], dict)\n                        ]\n                    )\n                except KeyError:\n                    logger.debug(f\"File {file} not in cache\")\n                    logger.debug(\"Cache follows -- \")\n                    logger.debug(data[operator])\n    files.extend(extra)\n    logger.debug(files)\n    for file in files:\n        metrics_data = []\n        has_changes = False\n        for operator, metric in metrics:\n            try:\n                current = last_revision.get(\n                    config, state.default_archiver, operator, file, metric.name\n                )\n            except KeyError as e:\n                current = \"-\"\n            try:\n                new = get_metric(data, operator, file, metric.name)\n            except KeyError as e:\n                new = \"-\"\n            if new != current:\n                has_changes = True\n            if metric.type in (int, float) and new != \"-\" and current != \"-\":\n                if current > new:\n                    metrics_data.append(\n                        \"{0:n} -> \\u001b[{2}m{1:n}\\u001b[0m\".format(\n                            current, new, BAD_COLORS[metric.measure]\n                        )\n                    )\n                elif current < new:\n                    metrics_data.append(\n                        \"{0:n} -> \\u001b[{2}m{1:n}\\u001b[0m\".format(\n                            current, new, GOOD_COLORS[metric.measure]\n                        )\n                    )\n                else:\n                    metrics_data.append(\"{0:n} -> {1:n}\".format(current, new))\n            else:\n                if current == \"-\" and new == \"-\":\n                    metrics_data.append(\"-\")\n                else:\n                    metrics_data.append(\"{0} -> {1}\".format(current, new))\n        if has_changes or not changes_only:\n            results.append((file, *metrics_data))\n        else:\n            logger.debug(metrics_data)\n\n    descriptions = [metric.description for operator, metric in metrics]\n    headers = (\"File\", *descriptions)\n    if len(results) > 0:\n        print(\n            # But it still makes more sense to show the newest at the top, so reverse again\n            tabulate.tabulate(\n                headers=headers, tabular_data=results, tablefmt=DEFAULT_GRID_STYLE\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli(ctx, debug, config, path, cache):\n    ctx.ensure_object(dict)\n    ctx.obj[\"DEBUG\"] = debug\n    if debug:\n        logger.setLevel(\"DEBUG\")\n    else:\n        logger.setLevel(\"INFO\")\n\n    ctx.obj[\"CONFIG\"] = load_config(config)\n    if path:\n        logger.debug(f\"Fixing path to {path}\")\n        ctx.obj[\"CONFIG\"].path = path\n    if cache:\n        logger.debug(f\"Fixing cache to {cache}\")\n        ctx.obj[\"CONFIG\"].cache_path = cache\n    logger.debug(f\"Loaded configuration from {config}\")", "response": "A basic wily graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build(ctx, max_revisions, targets, operators, archiver):\n    config = ctx.obj[\"CONFIG\"]\n\n    from wily.commands.build import build\n\n    if max_revisions:\n        logger.debug(f\"Fixing revisions to {max_revisions}\")\n        config.max_revisions = max_revisions\n\n    if operators:\n        logger.debug(f\"Fixing operators to {operators}\")\n        config.operators = operators.strip().split(\",\")\n\n    if archiver:\n        logger.debug(f\"Fixing archiver to {archiver}\")\n        config.archiver = archiver\n\n    if targets:\n        logger.debug(f\"Fixing targets to {targets}\")\n        config.targets = targets\n\n    build(\n        config=config,\n        archiver=resolve_archiver(config.archiver),\n        operators=resolve_operators(config.operators),\n    )\n    logger.info(\n        \"Completed building wily history, run `wily report <file>` or `wily index` to see more.\"\n    )", "response": "Build the wily cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshow the history archive in the. wily folder.", "response": "def index(ctx, message):\n    \"\"\"Show the history archive in the .wily/ folder.\"\"\"\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    from wily.commands.index import index\n\n    index(config=config, include_message=message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report(ctx, file, metrics, number, message, format, console_format, output):\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    if not metrics:\n        metrics = get_default_metrics(config)\n        logger.info(f\"Using default metrics {metrics}\")\n\n    new_output = Path().cwd()\n    if output:\n        new_output = new_output / Path(output)\n    else:\n        new_output = new_output / \"wily_report\" / \"index.html\"\n\n    from wily.commands.report import report\n\n    logger.debug(f\"Running report on {file} for metric {metrics}\")\n    logger.debug(f\"Output format is {format}\")\n\n    report(\n        config=config,\n        path=file,\n        metrics=metrics,\n        n=number,\n        output=new_output,\n        include_message=message,\n        format=ReportFormat[format],\n        console_format=console_format,\n    )", "response": "Show metrics for a given file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow the differences in metrics for each file.", "response": "def diff(ctx, files, metrics, all, detail):\n    \"\"\"Show the differences in metrics for each file.\"\"\"\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    if not metrics:\n        metrics = get_default_metrics(config)\n        logger.info(f\"Using default metrics {metrics}\")\n    else:\n        metrics = metrics.split(\",\")\n        logger.info(f\"Using specified metrics {metrics}\")\n\n    from wily.commands.diff import diff\n\n    logger.debug(f\"Running diff on {files} for metric {metrics}\")\n    diff(\n        config=config, files=files, metrics=metrics, changes_only=not all, detail=detail\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngraph a specific metric for a given file.", "response": "def graph(ctx, path, metrics, output, x_axis, changes):\n    \"\"\"\n    Graph a specific metric for a given file, if a path is given, all files within path will be graphed.\n\n    Some common examples:\n\n    Graph all .py files within src/ for the raw.loc metric\n\n        $ wily graph src/ raw.loc\n\n    Graph test.py against raw.loc and cyclomatic.complexity metrics\n\n        $ wily graph src/test.py raw.loc cyclomatic.complexity\n\n    Graph test.py against raw.loc and raw.sloc on the x-axis\n\n        $ wily graph src/test.py raw.loc --x-axis raw.sloc\n    \"\"\"\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    from wily.commands.graph import graph\n\n    logger.debug(f\"Running report on {path} for metrics {metrics}\")\n    graph(\n        config=config,\n        path=path,\n        metrics=metrics,\n        output=output,\n        x_axis=x_axis,\n        changes=changes,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the. wily folder.", "response": "def clean(ctx, yes):\n    \"\"\"Clear the .wily/ folder.\"\"\"\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    if not yes:\n        p = input(\"Are you sure you want to delete wily cache? [y/N]\")\n        if p.lower() != \"y\":\n            exit(0)\n\n    from wily.cache import clean\n\n    clean(config)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting the available metrics.", "response": "def list_metrics(ctx):\n    \"\"\"List the available metrics.\"\"\"\n    config = ctx.obj[\"CONFIG\"]\n\n    if not exists(config):\n        handle_no_cache(ctx)\n\n    from wily.commands.list_metrics import list_metrics\n\n    list_metrics()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_no_cache(context):\n    logger.error(\n        f\"Could not locate wily cache, the cache is required to provide insights.\"\n    )\n    p = input(\"Do you want to run setup and index your project now? [y/N]\")\n    if p.lower() != \"y\":\n        exit(1)\n    else:\n        revisions = input(\"How many previous git revisions do you want to index? : \")\n        revisions = int(revisions)\n        path = input(\"Path to your source files; comma-separated for multiple: \")\n        paths = path.split(\",\")\n        context.invoke(\n            build,\n            max_revisions=revisions,\n            targets=paths,\n            operators=None,\n        )", "response": "Handle lack - of - cache error prompt user for index process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef metric_parts(metric):\n    operator, met = resolve_metric_as_tuple(metric)\n    return operator.name, met.name", "response": "Convert a metric name into the operator and metric names."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef graph(config, path, metrics, output=None, x_axis=None, changes=True, text=False):\n    logger.debug(\"Running report command\")\n\n    data = []\n    state = State(config)\n    abs_path = config.path / pathlib.Path(path)\n\n    if x_axis is None:\n        x_axis = \"history\"\n    else:\n        x_operator, x_key = metric_parts(x_axis)\n\n    if abs_path.is_dir():\n        paths = [\n            p.relative_to(config.path) for p in pathlib.Path(abs_path).glob(\"**/*.py\")\n        ]\n    else:\n        paths = [path]\n\n    operator, key = metric_parts(metrics[0])\n    if len(metrics) == 1:  # only y-axis\n        z_axis = None\n    else:\n        z_axis = resolve_metric(metrics[1])\n        z_operator, z_key = metric_parts(metrics[1])\n    for path in paths:\n        x = []\n        y = []\n        z = []\n        labels = []\n        last_y = None\n        for rev in state.index[state.default_archiver].revisions:\n            labels.append(f\"{rev.revision.author_name} <br>{rev.revision.message}\")\n            try:\n                val = rev.get(config, state.default_archiver, operator, str(path), key)\n                if val != last_y or not changes:\n                    y.append(val)\n                    if z_axis:\n                        z.append(\n                            rev.get(\n                                config,\n                                state.default_archiver,\n                                z_operator,\n                                str(path),\n                                z_key,\n                            )\n                        )\n                    if x_axis == \"history\":\n                        x.append(format_datetime(rev.revision.date))\n                    else:\n                        x.append(\n                            rev.get(\n                                config,\n                                state.default_archiver,\n                                x_operator,\n                                str(path),\n                                x_key,\n                            )\n                        )\n                last_y = val\n            except KeyError:\n                # missing data\n                pass\n\n        # Create traces\n        trace = go.Scatter(\n            x=x,\n            y=y,\n            mode=\"lines+markers+text\" if text else \"lines+markers\",\n            name=f\"{path}\",\n            ids=state.index[state.default_archiver].revision_keys,\n            text=labels,\n            marker=dict(\n                size=0 if z_axis is None else z,\n                color=list(range(len(y))),\n                # colorscale='Viridis',\n            ),\n            xcalendar=\"gregorian\",\n            hoveron=\"points+fills\",\n        )\n        data.append(trace)\n    if output:\n        filename = output\n        auto_open = False\n    else:\n        filename = \"wily-report.html\"\n        auto_open = True\n    y_metric = resolve_metric(metrics[0])\n    title = f\"{x_axis.capitalize()} of {y_metric.description} for {path}\"\n    plotly.offline.plot(\n        {\n            \"data\": data,\n            \"layout\": go.Layout(\n                title=title,\n                xaxis={\"title\": x_axis},\n                yaxis={\"title\": y_metric.description},\n            ),\n        },\n        auto_open=auto_open,\n        filename=filename,\n    )", "response": "Graph information about the cache and runtime."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists metrics available for all the available operators.", "response": "def list_metrics():\n    \"\"\"List metrics available.\"\"\"\n    for name, operator in ALL_OPERATORS.items():\n        print(f\"{name} operator:\")\n        if len(operator.cls.metrics) > 0:\n            print(\n                tabulate.tabulate(\n                    headers=(\"Name\", \"Description\", \"Type\"),\n                    tabular_data=operator.cls.metrics,\n                    tablefmt=DEFAULT_GRID_STYLE,\n                )\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshowing information about the cache and runtime.", "response": "def index(config, include_message=False):\n    \"\"\"\n    Show information about the cache and runtime.\n\n    :param config: The wily configuration\n    :type  config: :namedtuple:`wily.config.WilyConfig`\n\n    :param include_message: Include revision messages\n    :type  include_message: ``bool``\n    \"\"\"\n    state = State(config=config)\n    logger.debug(\"Running show command\")\n    logger.info(\"--------Configuration---------\")\n    logger.info(f\"Path: {config.path}\")\n    logger.info(f\"Archiver: {config.archiver}\")\n    logger.info(f\"Operators: {config.operators}\")\n    logger.info(\"\")\n    logger.info(\"-----------History------------\")\n\n    data = []\n    for archiver in state.archivers:\n        for rev in state.index[archiver].revisions:\n            if include_message:\n                data.append(\n                    (\n                        format_revision(rev.revision.key),\n                        rev.revision.author_name,\n                        rev.revision.message[:MAX_MESSAGE_WIDTH],\n                        format_date(rev.revision.date),\n                    )\n                )\n            else:\n                data.append(\n                    (\n                        format_revision(rev.revision.key),\n                        rev.revision.author_name,\n                        format_date(rev.revision.date),\n                    )\n                )\n\n    if include_message:\n        headers = (\"Revision\", \"Author\", \"Message\", \"Date\")\n    else:\n        headers = (\"Revision\", \"Author\", \"Date\")\n    print(\n        tabulate.tabulate(\n            headers=headers, tabular_data=data, tablefmt=DEFAULT_GRID_STYLE\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_operator(operator, revision, config):\n    instance = operator.cls(config)\n    logger.debug(f\"Running {operator.name} operator on {revision.key}\")\n    return operator.name, instance.run(revision, config)", "response": "Run an operator for the multiprocessing pool. Not called directly."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the history given a wily configuration archiver and a collection of operators.", "response": "def build(config, archiver, operators):\n    \"\"\"\n    Build the history given a archiver and collection of operators.\n\n    :param config: The wily configuration\n    :type  config: :namedtuple:`wily.config.WilyConfig`\n\n    :param archiver: The archiver to use\n    :type  archiver: :namedtuple:`wily.archivers.Archiver`\n\n    :param operators: The list of operators to execute\n    :type operators: `list` of :namedtuple:`wily.operators.Operator`\n    \"\"\"\n    try:\n        logger.debug(f\"Using {archiver.name} archiver module\")\n        archiver = archiver.cls(config)\n        revisions = archiver.revisions(config.path, config.max_revisions)\n    except InvalidGitRepositoryError:\n        # TODO: This logic shouldn't really be here (SoC)\n        logger.info(f\"Defaulting back to the filesystem archiver, not a valid git repo\")\n        archiver = FilesystemArchiver(config)\n        revisions = archiver.revisions(config.path, config.max_revisions)\n    except Exception as e:\n        if hasattr(e, \"message\"):\n            logger.error(f\"Failed to setup archiver: '{e.message}'\")\n        else:\n            logger.error(f\"Failed to setup archiver: '{type(e)} - {e}'\")\n        exit(1)\n\n    state = State(config, archiver=archiver)\n    # Check for existence of cache, else provision\n    state.ensure_exists()\n\n    index = state.index[archiver.name]\n\n    # remove existing revisions from the list\n    revisions = [revision for revision in revisions if revision not in index]\n\n    logger.info(\n        f\"Found {len(revisions)} revisions from '{archiver.name}' archiver in '{config.path}'.\"\n    )\n\n    _op_desc = \",\".join([operator.name for operator in operators])\n    logger.info(f\"Running operators - {_op_desc}\")\n\n    bar = Bar(\"Processing\", max=len(revisions) * len(operators))\n    state.operators = operators\n    try:\n        with multiprocessing.Pool(processes=len(operators)) as pool:\n            for revision in revisions:\n                # Checkout target revision\n                archiver.checkout(revision, config.checkout_options)\n                stats = {\"operator_data\": {}}\n\n                # Run each operator as a seperate process\n                data = pool.starmap(\n                    run_operator,\n                    [(operator, revision, config) for operator in operators],\n                )\n\n                # Map the data back into a dictionary\n                for operator_name, result in data:\n                    # aggregate values to directories\n                    roots = []\n\n                    # find all unique directories in the results\n                    for entry in result.keys():\n                        parent = pathlib.Path(entry).parents[0]\n                        if parent not in roots:\n                            roots.append(parent)\n\n                    for root in roots:\n                        # find all matching entries recursively\n                        aggregates = [\n                            path\n                            for path in result.keys()\n                            if root in pathlib.Path(path).parents\n                        ]\n                        result[str(root)] = {}\n                        # aggregate values\n                        for metric in resolve_operator(operator_name).cls.metrics:\n                            func = metric.aggregate\n                            values = [\n                                result[aggregate][metric.name]\n                                for aggregate in aggregates\n                                if aggregate in result\n                                and metric.name in result[aggregate]\n                            ]\n                            if len(values) > 0:\n                                result[str(root)][metric.name] = func(values)\n\n                    stats[\"operator_data\"][operator_name] = result\n                    bar.next()\n\n                ir = index.add(revision, operators=operators)\n                ir.store(config, archiver, stats)\n        index.save()\n        bar.finish()\n    except Exception as e:\n        logger.error(f\"Failed to build cache: '{e}'\")\n        raise e\n    finally:\n        # Reset the archive after every run back to the head of the branch\n        archiver.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a revision to the index.", "response": "def add(self, revision, operators):\n        \"\"\"\n        Add a revision to the index.\n\n        :param revision: The revision.\n        :type  revision: :class:`Revision` or :class:`LazyRevision`\n        \"\"\"\n        ir = IndexedRevision(\n            revision=revision, operators=[operator.name for operator in operators]\n        )\n        self._revisions[revision.key] = ir\n        return ir"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the index data back to the wily cache.", "response": "def save(self):\n        \"\"\"Save the index data back to the wily cache.\"\"\"\n        data = [i.asdict() for i in self._revisions.values()]\n        logger.debug(\"Saving data\")\n        cache.store_archiver_index(self.config, self.archiver, data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_exists(self):\n        if not cache.exists(self.config):\n            logger.debug(\"Wily cache not found, creating.\")\n            cache.create(self.config)\n            logger.debug(\"Created wily cache\")\n        else:\n            logger.debug(f\"Cache {self.config.cache_path} exists\")", "response": "Ensure that cache directory exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of revisions.", "response": "def revisions(self, path, max_revisions):\n        \"\"\"\n        Get the list of revisions.\n\n        :param path: the path to target.\n        :type  path: ``str``\n\n        :param max_revisions: the maximum number of revisions.\n        :type  max_revisions: ``int``\n\n        :return: A list of revisions.\n        :rtype: ``list`` of :class:`Revision`\n        \"\"\"\n        mtime = os.path.getmtime(path)\n        key = hashlib.sha1(str(mtime).encode()).hexdigest()[:7]\n        return [\n            Revision(\n                key=key,\n                author_name=\"Local User\",  # Don't want to leak local data\n                author_email=\"-\",  # as above\n                date=int(mtime),\n                message=\"None\",\n            )\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_version(f):\n    doc = f.__doc__\n    f.__doc__ = \"Version: \" + __version__ + \"\\n\\n\" + doc\n    return f", "response": "Decorator to add the version of wily to the help heading."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_operator(name):\n    if name.lower() in ALL_OPERATORS:\n        return ALL_OPERATORS[name.lower()]\n    else:\n        raise ValueError(f\"Operator {name} not recognised.\")", "response": "Resolve an operator name to a specific operator type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving metric key to a given target.", "response": "def resolve_metric_as_tuple(metric):\n    \"\"\"\n    Resolve metric key to a given target.\n\n    :param metric: the metric name.\n    :type  metric: ``str``\n\n    :rtype: :class:`Metric`\n    \"\"\"\n    if \".\" in metric:\n        _, metric = metric.split(\".\")\n\n    r = [\n        (operator, match) for operator, match in ALL_METRICS if match[0] == metric\n    ]\n    if not r or len(r) == 0:\n        raise ValueError(f\"Metric {metric} not recognised.\")\n    else:\n        return r[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_metric(revision, operator, path, key):\n    if \":\" in path:\n        part, entry = path.split(\":\")\n        val = revision[operator][part][entry][key]\n    else:\n        val = revision[operator][path][key]\n    return val", "response": "Get a metric from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the halstead harvester.", "response": "def run(self, module, options):\n        \"\"\"\n        Run the operator.\n\n        :param module: The target module path.\n        :type  module: ``str``\n\n        :param options: Any runtime options.\n        :type  options: ``dict``\n\n        :return: The operator results.\n        :rtype: ``dict``\n        \"\"\"\n        logger.debug(\"Running halstead harvester\")\n        results = {}\n        for filename, details in dict(self.harvester.results).items():\n            results[filename] = {}\n            for instance in details:\n                if isinstance(instance, list):\n                    for item in instance:\n                        function, report = item\n                        results[filename][function] = self._report_to_dict(report)\n                else:\n                    if isinstance(instance, str) and instance == \"error\":\n                        logger.warning(\n                            f\"Failed to run Halstead harvester on {filename} : {details['error']}\"\n                        )\n                        continue\n                    results[filename] = self._report_to_dict(instance)\n        return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef exists(config):\n    exists = (\n        pathlib.Path(config.cache_path).exists()\n        and pathlib.Path(config.cache_path).is_dir()\n    )\n    if not exists:\n        return False\n    index_path = pathlib.Path(config.cache_path) / \"index.json\"\n    if index_path.exists():\n        with open(index_path, \"r\") as out:\n            index = json.load(out)\n        if index[\"version\"] != __version__:\n            # TODO: Inspect the versions properly.\n            logger.warning(\n                \"Wily cache is old, you may incur errors until you rebuild the cache.\"\n            )\n    else:\n        logger.warning(\n            \"Wily cache was not versioned, you may incur errors until you rebuild the cache.\"\n        )\n        create_index(config)\n    return True", "response": "Check whether the. wily directory exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the root index.", "response": "def create_index(config):\n    \"\"\"Create the root index.\"\"\"\n    filename = pathlib.Path(config.cache_path) / \"index.json\"\n    index = {\"version\": __version__}\n    with open(filename, \"w\") as out:\n        out.write(json.dumps(index, indent=2))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a wily cache.", "response": "def create(config):\n    \"\"\"\n    Create a wily cache.\n\n    :param config: The configuration\n    :type  config: :class:`wily.config.WilyConfig`\n\n    :return: The path to the cache\n    :rtype: ``str``\n    \"\"\"\n    if exists(config):\n        logger.debug(\"Wily cache exists, skipping\")\n        return config.cache_path\n    logger.debug(f\"Creating wily cache {config.cache_path}\")\n    pathlib.Path(config.cache_path).mkdir(parents=True, exist_ok=True)\n    create_index(config)\n    return config.cache_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a wily cache.", "response": "def clean(config):\n    \"\"\"\n    Delete a wily cache.\n\n    :param config: The configuration\n    :type  config: :class:`wily.config.WilyConfig`\n\n    \"\"\"\n    if not exists(config):\n        logger.debug(\"Wily cache does not exist, skipping\")\n        return\n    shutil.rmtree(config.cache_path)\n    logger.debug(\"Deleted wily cache\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store(config, archiver, revision, stats):\n    root = pathlib.Path(config.cache_path) / archiver.name\n\n    if not root.exists():\n        logger.debug(\"Creating wily cache\")\n        root.mkdir()\n\n    # fix absolute path references.\n    if config.path != \".\":\n        for operator, operator_data in list(stats[\"operator_data\"].items()):\n            if operator_data:\n                new_operator_data = operator_data.copy()\n                for k, v in list(operator_data.items()):\n                    new_key = os.path.relpath(str(k), str(config.path))\n                    del new_operator_data[k]\n                    new_operator_data[new_key] = v\n                del stats[\"operator_data\"][operator]\n                stats[\"operator_data\"][operator] = new_operator_data\n\n    logger.debug(f\"Creating {revision.key} output\")\n    filename = root / (revision.key + \".json\")\n    if filename.exists():\n        raise RuntimeError(f\"File {filename} already exists, index may be corrupt.\")\n    with open(filename, \"w\") as out:\n        out.write(json.dumps(stats, indent=2))\n    return filename", "response": "Store a new record in the wily cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef store_archiver_index(config, archiver, index):\n    root = pathlib.Path(config.cache_path) / archiver.name\n\n    if not root.exists():\n        root.mkdir()\n        logger.debug(\"Created archiver directory\")\n\n    index = sorted(index, key=lambda k: k[\"date\"], reverse=True)\n\n    filename = root / \"index.json\"\n    with open(filename, \"w\") as out:\n        out.write(json.dumps(index, indent=2))\n    logger.debug(f\"Created index output\")\n    return filename", "response": "Store an archiver s index record for faster search."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the names of archivers with data.", "response": "def list_archivers(config):\n    \"\"\"\n    List the names of archivers with data.\n\n    :param config: The configuration\n    :type  config: :class:`wily.config.WilyConfig`\n\n    :return: A list of archiver names\n    :rtype: ``list`` of ``str``\n    \"\"\"\n    root = pathlib.Path(config.cache_path)\n    result = []\n    for name in ALL_ARCHIVERS.keys():\n        if (root / name).exists():\n            result.append(name)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_metrics(config):\n    archivers = list_archivers(config)\n    default_metrics = []\n\n    for archiver in archivers:\n        index = get_archiver_index(config, archiver)\n\n        if len(index) == 0:\n            logger.warning(\"No records found in the index, no metrics available\")\n            return []\n\n        operators = index[0][\"operators\"]\n        for operator in operators:\n            o = resolve_operator(operator)\n            if o.cls.default_metric_index is not None:\n                metric = o.cls.metrics[o.cls.default_metric_index]\n                default_metrics.append(\"{0}.{1}\".format(o.cls.name, metric.name))\n    return default_metrics", "response": "Get the default metrics for a configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_archiver_index(config, archiver):\n    root = pathlib.Path(config.cache_path) / archiver / \"index.json\"\n    return root.exists()", "response": "Check if this archiver has an index file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_archiver_index(config, archiver):\n    root = pathlib.Path(config.cache_path) / archiver\n    with (root / \"index.json\").open(\"r\") as index_f:\n        index = json.load(index_f)\n    return index", "response": "Get the contents of the archiver index file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(config, archiver, revision):\n    root = pathlib.Path(config.cache_path) / archiver\n    # TODO : string escaping!!!\n    with (root / f\"{revision}.json\").open(\"r\") as rev_f:\n        index = json.load(rev_f)\n    return index", "response": "Get the data record for a given revision."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mode(data):\n    c = Counter(data)\n    mode, freq = c.most_common(1)[0]\n    return mode", "response": "Returns the modal value of a iterable with discrete values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the maintainability harvester.", "response": "def run(self, module, options):\n        \"\"\"\n        Run the operator.\n\n        :param module: The target module path.\n        :type  module: ``str``\n\n        :param options: Any runtime options.\n        :type  options: ``dict``\n\n        :return: The operator results.\n        :rtype: ``dict``\n        \"\"\"\n        logger.debug(\"Running maintainability harvester\")\n        return dict(self.harvester.results)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, module, options):\n        logger.debug(\"Running CC harvester\")\n        results = {}\n        for filename, details in dict(self.harvester.results).items():\n            results[filename] = {}\n            total = 0  # running CC total\n            for instance in details:\n                if isinstance(instance, Class):\n                    i = self._dict_from_class(instance)\n                elif isinstance(instance, Function):\n                    i = self._dict_from_function(instance)\n                else:\n                    if isinstance(instance, str) and instance == \"error\":\n                        logger.warning(\n                            f\"Failed to run CC harvester on {filename} : {details['error']}\"\n                        )\n                        continue\n                    else:\n                        logger.warning(\n                            f\"Unexpected result from Radon : {instance} of {type(instance)}. Please report on Github.\"\n                        )\n                        continue\n                results[filename][i[\"fullname\"]] = i\n                del i[\"fullname\"]\n                total += i[\"complexity\"]\n            results[filename][\"complexity\"] = total\n        return results", "response": "Run the operator.\n\n        :param module: The target module path.\n        :type  module: ``str``\n\n        :param options: Any runtime options.\n        :type  options: ``dict``\n\n        :return: The operator results.\n        :rtype: ``dict``"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_values(s):\n    '''(INTERNAL) Split a line into a list of values'''\n    if not _RE_NONTRIVIAL_DATA.search(s):\n        # Fast path for trivial cases (unfortunately we have to handle missing\n        # values because of the empty string case :(.)\n        return [None if s in ('?', '') else s\n                for s in next(csv.reader([s]))]\n\n    # _RE_DENSE_VALUES tokenizes despite quoting, whitespace, etc.\n    values, errors = zip(*_RE_DENSE_VALUES.findall(',' + s))\n    if not any(errors):\n        return [_unquote(v) for v in values]\n    if _RE_SPARSE_LINE.match(s):\n        try:\n            return {int(k): _unquote(v)\n                    for k, v in _RE_SPARSE_KEY_VALUES.findall(s)}\n        except ValueError as exc:\n            # an ARFF syntax error in sparse data\n            for match in _RE_SPARSE_KEY_VALUES.finditer(s):\n                if not match.group(1):\n                    raise BadLayout('Error parsing %r' % match.group())\n            raise BadLayout('Unknown parsing error')\n    else:\n        # an ARFF syntax error\n        for match in _RE_DENSE_VALUES.finditer(s):\n            if match.group(2):\n                raise BadLayout('Error parsing %r' % match.group())\n        raise BadLayout('Unknown parsing error')", "response": "Internal method to parse a line into a list of values"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a file - like object containing the ARFF document and convert it into a Python object.", "response": "def load(fp, encode_nominal=False, return_type=DENSE):\n    '''Load a file-like object containing the ARFF document and convert it into\n    a Python object.\n\n    :param fp: a file-like object.\n    :param encode_nominal: boolean, if True perform a label encoding\n        while reading the .arff file.\n    :param return_type: determines the data structure used to store the\n        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n        `arff.DENSE_GEN` or `arff.LOD_GEN`.\n        Consult the sections on `working with sparse data`_ and `loading\n        progressively`_.\n    :return: a dictionary.\n     '''\n    decoder = ArffDecoder()\n    return decoder.decode(fp, encode_nominal=encode_nominal,\n                          return_type=return_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a string instance containing the ARFF document into a Python object.", "response": "def loads(s, encode_nominal=False, return_type=DENSE):\n    '''Convert a string instance containing the ARFF document into a Python\n    object.\n\n    :param s: a string object.\n    :param encode_nominal: boolean, if True perform a label encoding\n        while reading the .arff file.\n    :param return_type: determines the data structure used to store the\n        dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n        `arff.DENSE_GEN` or `arff.LOD_GEN`.\n        Consult the sections on `working with sparse data`_ and `loading\n        progressively`_.\n    :return: a dictionary.\n    '''\n    decoder = ArffDecoder()\n    return decoder.decode(s, encode_nominal=encode_nominal,\n                          return_type=return_type)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(obj, fp):\n    '''Serialize an object representing the ARFF document to a given file-like\n    object.\n\n    :param obj: a dictionary.\n    :param fp: a file-like object.\n    '''\n    encoder = ArffEncoder()\n    generator = encoder.iter_encode(obj)\n\n    last_row = next(generator)\n    for row in generator:\n        fp.write(last_row + u'\\n')\n        last_row = row\n    fp.write(last_row)\n\n    return fp", "response": "Serialize an object representing the ARFF document to a given file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_data(self, data, attributes):\n        '''(INTERNAL) Encodes a line of data.\n\n        Data instances follow the csv format, i.e, attribute values are\n        delimited by commas. After converted from csv.\n\n        :param data: a list of values.\n        :param attributes: a list of attributes. Used to check if data is valid.\n        :return: a string with the encoded data line.\n        '''\n        current_row = 0\n\n        for inst in data:\n            if len(inst) != len(attributes):\n                raise BadObject(\n                    'Instance %d has %d attributes, expected %d' %\n                     (current_row, len(inst), len(attributes))\n                )\n\n            new_data = []\n            for value in inst:\n                if value is None or value == u'' or value != value:\n                    s = '?'\n                else:\n                    s = encode_string(unicode(value))\n                new_data.append(s)\n\n            current_row += 1\n            yield u','.join(new_data)", "response": "Encodes a line of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _decode_relation(self, s):\n        '''(INTERNAL) Decodes a relation line.\n\n        The relation declaration is a line with the format ``@RELATION\n        <relation-name>``, where ``relation-name`` is a string. The string must\n        start with alphabetic character and must be quoted if the name includes\n        spaces, otherwise this method will raise a `BadRelationFormat` exception.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\\n\" characters.\n\n        :param s: a normalized string.\n        :return: a string with the decoded relation name.\n        '''\n        _, v = s.split(' ', 1)\n        v = v.strip()\n\n        if not _RE_RELATION.match(v):\n            raise BadRelationFormat()\n\n        res = unicode(v.strip('\"\\''))\n        return res", "response": "Internal method to decode a relation line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _decode_attribute(self, s):\n        '''(INTERNAL) Decodes an attribute line.\n\n        The attribute is the most complex declaration in an arff file. All\n        attributes must follow the template::\n\n             @attribute <attribute-name> <datatype>\n\n        where ``attribute-name`` is a string, quoted if the name contains any\n        whitespace, and ``datatype`` can be:\n\n        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.\n        - Strings as ``STRING``.\n        - Dates (NOT IMPLEMENTED).\n        - Nominal attributes with format:\n\n            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}\n\n        The nominal names follow the rules for the attribute names, i.e., they\n        must be quoted if the name contains whitespaces.\n\n        This method must receive a normalized string, i.e., a string without\n        padding, including the \"\\r\\n\" characters.\n\n        :param s: a normalized string.\n        :return: a tuple (ATTRIBUTE_NAME, TYPE_OR_VALUES).\n        '''\n        _, v = s.split(' ', 1)\n        v = v.strip()\n\n        # Verify the general structure of declaration\n        m = _RE_ATTRIBUTE.match(v)\n        if not m:\n            raise BadAttributeFormat()\n\n        # Extracts the raw name and type\n        name, type_ = m.groups()\n\n        # Extracts the final name\n        name = unicode(name.strip('\"\\''))\n\n        # Extracts the final type\n        if _RE_TYPE_NOMINAL.match(type_):\n            try:\n                type_ = _parse_values(type_.strip('{} '))\n            except Exception:\n                raise BadAttributeType()\n            if isinstance(type_, dict):\n                raise BadAttributeType()\n\n        else:\n            # If not nominal, verify the type name\n            type_ = unicode(type_).upper()\n            if type_ not in ['NUMERIC', 'REAL', 'INTEGER', 'STRING']:\n                raise BadAttributeType()\n\n        return (name, type_)", "response": "Internal method that decodes an attribute line into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndoing the job the decode.", "response": "def _decode(self, s, encode_nominal=False, matrix_type=DENSE):\n        '''Do the job the ``encode``.'''\n\n        # Make sure this method is idempotent\n        self._current_line = 0\n\n        # If string, convert to a list of lines\n        if isinstance(s, basestring):\n            s = s.strip('\\r\\n ').replace('\\r\\n', '\\n').split('\\n')\n\n        # Create the return object\n        obj = {\n            u'description': u'',\n            u'relation': u'',\n            u'attributes': [],\n            u'data': []\n        }\n        attribute_names = {}\n\n        # Create the data helper object\n        data = _get_data_object_for_decoding(matrix_type)\n\n        # Read all lines\n        STATE = _TK_DESCRIPTION\n        s = iter(s)\n        for row in s:\n            self._current_line += 1\n            # Ignore empty lines\n            row = row.strip(' \\r\\n')\n            if not row: continue\n\n            u_row = row.upper()\n\n            # DESCRIPTION -----------------------------------------------------\n            if u_row.startswith(_TK_DESCRIPTION) and STATE == _TK_DESCRIPTION:\n                obj['description'] += self._decode_comment(row) + '\\n'\n            # -----------------------------------------------------------------\n\n            # RELATION --------------------------------------------------------\n            elif u_row.startswith(_TK_RELATION):\n                if STATE != _TK_DESCRIPTION:\n                    raise BadLayout()\n\n                STATE = _TK_RELATION\n                obj['relation'] = self._decode_relation(row)\n            # -----------------------------------------------------------------\n\n            # ATTRIBUTE -------------------------------------------------------\n            elif u_row.startswith(_TK_ATTRIBUTE):\n                if STATE != _TK_RELATION and STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n\n                STATE = _TK_ATTRIBUTE\n\n                attr = self._decode_attribute(row)\n                if attr[0] in attribute_names:\n                    raise BadAttributeName(attr[0], attribute_names[attr[0]])\n                else:\n                    attribute_names[attr[0]] = self._current_line\n                obj['attributes'].append(attr)\n\n                if isinstance(attr[1], (list, tuple)):\n                    if encode_nominal:\n                        conversor = EncodedNominalConversor(attr[1])\n                    else:\n                        conversor = NominalConversor(attr[1])\n                else:\n                    CONVERSOR_MAP = {'STRING': unicode,\n                                     'INTEGER': lambda x: int(float(x)),\n                                     'NUMERIC': float,\n                                     'REAL': float}\n                    conversor = CONVERSOR_MAP[attr[1]]\n\n                self._conversors.append(conversor)\n            # -----------------------------------------------------------------\n\n            # DATA ------------------------------------------------------------\n            elif u_row.startswith(_TK_DATA):\n                if STATE != _TK_ATTRIBUTE:\n                    raise BadLayout()\n\n                break\n            # -----------------------------------------------------------------\n\n            # COMMENT ---------------------------------------------------------\n            elif u_row.startswith(_TK_COMMENT):\n                pass\n            # -----------------------------------------------------------------\n        else:\n            # Never found @DATA\n            raise BadLayout()\n\n        def stream():\n            for row in s:\n                self._current_line += 1\n                row = row.strip()\n                # Ignore empty lines and comment lines.\n                if row and not row.startswith(_TK_COMMENT):\n                    yield row\n\n        # Alter the data object\n        obj['data'] = data.decode_rows(stream(), self._conversors)\n        if obj['description'].endswith('\\n'):\n            obj['description'] = obj['description'][:-1]\n\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the Python representation of a given ARFF file.", "response": "def decode(self, s, encode_nominal=False, return_type=DENSE):\n        '''Returns the Python representation of a given ARFF file.\n\n        When a file object is passed as an argument, this method reads lines\n        iteratively, avoiding to load unnecessary information to the memory.\n\n        :param s: a string or file object with the ARFF file.\n        :param encode_nominal: boolean, if True perform a label encoding\n            while reading the .arff file.\n        :param return_type: determines the data structure used to store the\n            dataset. Can be one of `arff.DENSE`, `arff.COO`, `arff.LOD`,\n            `arff.DENSE_GEN` or `arff.LOD_GEN`.\n            Consult the sections on `working with sparse data`_ and `loading\n            progressively`_.\n        '''\n        try:\n            return self._decode(s, encode_nominal=encode_nominal,\n                                matrix_type=return_type)\n        except ArffException as e:\n            e.line = self._current_line\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _encode_relation(self, name):\n        '''(INTERNAL) Decodes a relation line.\n\n        The relation declaration is a line with the format ``@RELATION\n        <relation-name>``, where ``relation-name`` is a string.\n\n        :param name: a string.\n        :return: a string with the encoded relation declaration.\n        '''\n        for char in ' %{},':\n            if char in name:\n                name = '\"%s\"'%name\n                break\n\n        return u'%s %s'%(_TK_RELATION, name)", "response": "Encodes a relation line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nencoding an attribute line.", "response": "def _encode_attribute(self, name, type_):\n        '''(INTERNAL) Encodes an attribute line.\n\n        The attribute follow the template::\n\n             @attribute <attribute-name> <datatype>\n\n        where ``attribute-name`` is a string, and ``datatype`` can be:\n\n        - Numerical attributes as ``NUMERIC``, ``INTEGER`` or ``REAL``.\n        - Strings as ``STRING``.\n        - Dates (NOT IMPLEMENTED).\n        - Nominal attributes with format:\n\n            {<nominal-name1>, <nominal-name2>, <nominal-name3>, ...}\n\n        This method must receive a the name of the attribute and its type, if\n        the attribute type is nominal, ``type`` must be a list of values.\n\n        :param name: a string.\n        :param type_: a string or a list of string.\n        :return: a string with the encoded attribute declaration.\n        '''\n        for char in ' %{},':\n            if char in name:\n                name = '\"%s\"'%name\n                break\n\n        if isinstance(type_, (tuple, list)):\n            type_tmp = [u'%s' % encode_string(type_k) for type_k in type_]\n            type_ = u'{%s}'%(u', '.join(type_tmp))\n\n        return u'%s %s %s'%(_TK_ATTRIBUTE, name, type_)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode(self, obj):\n        '''Encodes a given object to an ARFF file.\n\n        :param obj: the object containing the ARFF information.\n        :return: the ARFF file as an unicode string.\n        '''\n        data = [row for row in self.iter_encode(obj)]\n\n        return u'\\n'.join(data)", "response": "Encodes a given object to an ARFF file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef begin(self):\n\n        resp = ws.ws2811_init(self._leds)\n        if resp != 0:\n            str_resp = ws.ws2811_get_return_t_str(resp)\n            raise RuntimeError('ws2811_init failed with code {0} ({1})'.format(resp, str_resp))", "response": "Initialize the library and initialize the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show(self):\n        resp = ws.ws2811_render(self._leds)\n        if resp != 0:\n            str_resp = ws.ws2811_get_return_t_str(resp)\n            raise RuntimeError('ws2811_render failed with code {0} ({1})'.format(resp, str_resp))", "response": "Update the display with the data from the LED buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setPixelColorRGB(self, n, red, green, blue):\n        self.setPixelColor(n, Color(red, green, blue))", "response": "Set the RGB color of the LED at position n."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef startService(self):\n        Service.startService(self)\n        self._thread = threading.Thread(target=self._writer)\n        self._thread.start()\n        addDestination(self)", "response": "Start the writer thread."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stopService(self):\n        Service.stopService(self)\n        removeDestination(self)\n        self._reactor.callFromThread(self._reactor.stop)\n        return deferToThreadPool(\n            self._mainReactor,\n            self._mainReactor.getThreadPool(), self._thread.join)", "response": "Stop the writer thread and wait for it to finish."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bind(self, **fields):\n        contents = self._contents.copy()\n        contents.update(fields)\n        return Message(contents, self._serializer)", "response": "Returns a new message with this message s contents plus the additional given bindings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfreezes this message for logging registering it with action.", "response": "def _freeze(self, action=None):\n        \"\"\"\n        Freeze this message for logging, registering it with C{action}.\n\n        @param action: The L{Action} which is the context for this message. If\n            C{None}, the L{Action} will be deduced from the current call\n            stack.\n\n        @return: A L{PMap} with added C{timestamp}, C{task_uuid}, and\n            C{task_level} entries.\n        \"\"\"\n        if action is None:\n            action = current_action()\n        if action is None:\n            task_uuid = unicode(uuid4())\n            task_level = [1]\n        else:\n            task_uuid = action._identification[TASK_UUID_FIELD]\n            task_level = action._nextTaskLevel().as_list()\n        timestamp = self._timestamp()\n        new_values = {\n            TIMESTAMP_FIELD: timestamp,\n            TASK_UUID_FIELD: task_uuid,\n            TASK_LEVEL_FIELD: task_level\n        }\n        if \"action_type\" not in self._contents and (\n            \"message_type\" not in self._contents\n        ):\n            new_values[\"message_type\"] = \"\"\n        new_values.update(self._contents)\n        return new_values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the message to the given logger.", "response": "def write(self, logger=None, action=None):\n        \"\"\"\n        Write the message to the given logger.\n\n        This will additionally include a timestamp, the action context if any,\n        and any other fields.\n\n        Byte field names will be converted to Unicode.\n\n        @type logger: L{eliot.ILogger} or C{None} indicating the default one.\n\n        @param action: The L{Action} which is the context for this message. If\n            C{None}, the L{Action} will be deduced from the current call\n            stack.\n        \"\"\"\n        if logger is None:\n            logger = _output._DEFAULT_LOGGER\n        logged_dict = self._freeze(action=action)\n        logger.write(logged_dict, self._serializer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes Dask compute but with added Eliot tracing.", "response": "def compute_with_trace(*args):\n    \"\"\"Do Dask compute(), but with added Eliot tracing.\n\n    Dask is a graph of tasks, but Eliot logs trees.  So we need to emulate a\n    graph using a tree.  We do this by making Eliot action for each task, but\n    having it list the tasks it depends on.\n\n    We use the following algorithm:\n\n        1. Create a top-level action.\n\n        2. For each entry in the dask graph, create a child with\n           serialize_task_id.  Do this in likely order of execution, so that\n           if B depends on A the task level of B is higher than the task Ievel\n           of A.\n\n        3. Replace each function with a wrapper that uses the corresponding\n           task ID (with Action.continue_task), and while it's at it also\n           records which other things this function depends on.\n\n    Known issues:\n\n        1. Retries will confuse Eliot.  Probably need different\n           distributed-tree mechanism within Eliot to solve that.\n    \"\"\"\n    # 1. Create top-level Eliot Action:\n    with start_action(action_type=\"dask:compute\"):\n        # In order to reduce logging verbosity, add logging to the already\n        # optimized graph:\n        optimized = optimize(*args, optimizations=[_add_logging])\n        return compute(*optimized, optimize_graph=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds logging to a Dask graph.", "response": "def _add_logging(dsk, ignore=None):\n    \"\"\"\n    Add logging to a Dask graph.\n\n    @param dsk: The Dask graph.\n\n    @return: New Dask graph.\n    \"\"\"\n    ctx = current_action()\n    result = {}\n\n    # Use topological sort to ensure Eliot actions are in logical order of\n    # execution in Dask:\n    keys = toposort(dsk)\n\n    # Give each key a string name. Some keys are just aliases to other\n    # keys, so make sure we have underlying key available. Later on might\n    # want to shorten them as well.\n    def simplify(k):\n        if isinstance(k, str):\n            return k\n        return \"-\".join(str(o) for o in k)\n\n    key_names = {}\n    for key in keys:\n        value = dsk[key]\n        if not callable(value) and value in keys:\n            # It's an alias for another key:\n            key_names[key] = key_names[value]\n        else:\n            key_names[key] = simplify(key)\n\n    # 2. Create Eliot child Actions for each key, in topological order:\n    key_to_action_id = {\n        key: str(ctx.serialize_task_id(), \"utf-8\")\n        for key in keys\n    }\n\n    # 3. Replace function with wrapper that logs appropriate Action:\n    for key in keys:\n        func = dsk[key][0]\n        args = dsk[key][1:]\n        if not callable(func):\n            # This key is just an alias for another key, no need to add\n            # logging:\n            result[key] = dsk[key]\n            continue\n        wrapped_func = _RunWithEliotContext(\n            task_id=key_to_action_id[key],\n            func=func,\n            key=key_names[key],\n            dependencies=[key_names[k] for k in get_dependencies(dsk, key)],\n        )\n        result[key] = (wrapped_func, ) + tuple(args)\n\n    assert result.keys() == dsk.keys()\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert a new action into the tree.", "response": "def _insert_action(self, node):\n        \"\"\"\n        Add a L{WrittenAction} to the tree.\n\n        Parent actions will be created as necessary.\n\n        @param child: A L{WrittenAction} to add to the tree.\n\n        @return: Updated L{Task}.\n        \"\"\"\n        task = self\n        if (\n            node.end_message and node.start_message and\n            (len(node.children) == node.end_message.task_level.level[-1] - 2)):\n            # Possibly this action is complete, make sure all sub-actions\n            # are complete:\n            completed = True\n            for child in node.children:\n                if (\n                    isinstance(child, WrittenAction)\n                    and child.task_level not in self._completed):\n                    completed = False\n                    break\n            if completed:\n                task = task.transform([\"_completed\"],\n                                      lambda s: s.add(node.task_level))\n        task = task.transform([\"_nodes\", node.task_level], node)\n        return task._ensure_node_parents(node)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure the node is referenced by parent Nodes.", "response": "def _ensure_node_parents(self, child):\n        \"\"\"\n        Ensure the node (WrittenAction/WrittenMessage) is referenced by parent\n        nodes.\n\n        Parent actions will be created as necessary.\n\n        @param child: A L{WrittenMessage} or L{WrittenAction} which is\n            being added to the tree.\n\n        @return: Updated L{Task}.\n        \"\"\"\n        task_level = child.task_level\n        if task_level.parent() is None:\n            return self\n\n        parent = self._nodes.get(task_level.parent())\n        if parent is None:\n            parent = WrittenAction(\n                task_level=task_level.parent(), task_uuid=child.task_uuid)\n        parent = parent._add_child(child)\n        return self._insert_action(parent)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a new entry to the tree.", "response": "def add(self, message_dict):\n        \"\"\"\n        Update the L{Task} with a dictionary containing a serialized Eliot\n        message.\n\n        @param message_dict: Dictionary whose task UUID matches this one.\n\n        @return: Updated L{Task}.\n        \"\"\"\n        is_action = message_dict.get(ACTION_TYPE_FIELD) is not None\n        written_message = WrittenMessage.from_dict(message_dict)\n        if is_action:\n            action_level = written_message.task_level.parent()\n            action = self._nodes.get(action_level)\n            if action is None:\n                action = WrittenAction(\n                    task_level=action_level,\n                    task_uuid=message_dict[TASK_UUID_FIELD])\n            if message_dict[ACTION_STATUS_FIELD] == STARTED_STATUS:\n                # Either newly created MissingAction, or one created by\n                # previously added descendant of the action.\n                action = action._start(written_message)\n            else:\n                action = action._end(written_message)\n            return self._insert_action(action)\n        else:\n            # Special case where there is no action:\n            if written_message.task_level.level == [1]:\n                return self.transform([\n                    \"_nodes\", self._root_level], written_message, [\n                        \"_completed\"], lambda s: s.add(self._root_level))\n            else:\n                return self._ensure_node_parents(written_message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the L { Parser } with a dictionary containing a serialized EliotMessage.", "response": "def add(self, message_dict):\n        \"\"\"\n        Update the L{Parser} with a dictionary containing a serialized Eliot\n        message.\n\n        @param message_dict: Dictionary of serialized Eliot message.\n\n        @return: Tuple of (list of completed L{Task} instances, updated\n            L{Parser}).\n        \"\"\"\n        uuid = message_dict[TASK_UUID_FIELD]\n        if uuid in self._tasks:\n            task = self._tasks[uuid]\n        else:\n            task = Task()\n        task = task.add(message_dict)\n        if task.is_complete():\n            parser = self.transform([\"_tasks\", uuid], discard)\n            return [task], parser\n        else:\n            parser = self.transform([\"_tasks\", uuid], task)\n            return [], parser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a stream of Eliot messages into a stream of L { Task instances.", "response": "def parse_stream(cls, iterable):\n        \"\"\"\n        Parse a stream of messages into a stream of L{Task} instances.\n\n        :param iterable: An iterable of serialized Eliot message dictionaries.\n\n        :return: An iterable of parsed L{Task} instances. Remaining\n            incomplete L{Task} will be returned when the input stream is\n            exhausted.\n        \"\"\"\n        parser = Parser()\n        for message_dict in iterable:\n            completed, parser = parser.add(message_dict)\n            for task in completed:\n                yield task\n        for task in parser.incomplete_tasks():\n            yield task"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eliot_friendly_generator_function(original):\n    @wraps(original)\n    def wrapper(*a, **kw):\n        # Keep track of whether the next value to deliver to the generator is\n        # a non-exception or an exception.\n        ok = True\n\n        # Keep track of the next value to deliver to the generator.\n        value_in = None\n\n        # Create the generator with a call to the generator function.  This\n        # happens with whatever Eliot action context happens to be active,\n        # which is fine and correct and also irrelevant because no code in the\n        # generator function can run until we call send or throw on it.\n        gen = original(*a, **kw)\n\n        # Initialize the per-generator context to a copy of the current context.\n        context = copy_context()\n        while True:\n            try:\n                # Whichever way we invoke the generator, we will do it\n                # with the Eliot action context stack we've saved for it.\n                # Then the context manager will re-save it and restore the\n                # \"outside\" stack for us.\n                #\n                # Regarding the support of Twisted's inlineCallbacks-like\n                # functionality (see eliot.twisted.inline_callbacks):\n                #\n                # The invocation may raise the inlineCallbacks internal\n                # control flow exception _DefGen_Return.  It is not wrong to\n                # just let that propagate upwards here but inlineCallbacks\n                # does think it is wrong.  The behavior triggers a\n                # DeprecationWarning to try to get us to fix our code.  We\n                # could explicitly handle and re-raise the _DefGen_Return but\n                # only at the expense of depending on a private Twisted API.\n                # For now, I'm opting to try to encourage Twisted to fix the\n                # situation (or at least not worsen it):\n                # https://twistedmatrix.com/trac/ticket/9590\n                #\n                # Alternatively, _DefGen_Return is only required on Python 2.\n                # When Python 2 support is dropped, this concern can be\n                # eliminated by always using `return value` instead of\n                # `returnValue(value)` (and adding the necessary logic to the\n                # StopIteration handler below).\n                def go():\n                    if ok:\n                        value_out = gen.send(value_in)\n                    else:\n                        value_out = gen.throw(*value_in)\n                    # We have obtained a value from the generator.  In\n                    # giving it to us, it has given up control.  Note this\n                    # fact here.  Importantly, this is within the\n                    # generator's action context so that we get a good\n                    # indication of where the yield occurred.\n                    #\n                    # This is noisy, enable only for debugging:\n                    if wrapper.debug:\n                        Message.log(message_type=u\"yielded\")\n                    return value_out\n\n                value_out = context.run(go)\n            except StopIteration:\n                # When the generator raises this, it is signaling\n                # completion.  Leave the loop.\n                break\n            else:\n                try:\n                    # Pass the generator's result along to whoever is\n                    # driving.  Capture the result as the next value to\n                    # send inward.\n                    value_in = yield value_out\n                except:\n                    # Or capture the exception if that's the flavor of the\n                    # next value.  This could possibly include GeneratorExit\n                    # which turns out to be just fine because throwing it into\n                    # the inner generator effectively propagates the close\n                    # (and with the right context!) just as you would want.\n                    # True, the GeneratorExit does get re-throwing out of the\n                    # gen.throw call and hits _the_generator_context's\n                    # contextmanager.  But @contextmanager extremely\n                    # conveniently eats it for us!  Thanks, @contextmanager!\n                    ok = False\n                    value_in = exc_info()\n                else:\n                    ok = True\n\n    wrapper.debug = False\n    return wrapper", "response": "Decorator for the eliot - friendly generator function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef in_generator(self, generator):\n        previous_generator = self._current_generator\n        try:\n            self._current_generator = generator\n            yield\n        finally:\n            self._current_generator = previous_generator", "response": "Context manager that yields the given generator as the current generator."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a message to the journald log.", "response": "def sd_journal_send(**kwargs):\n    \"\"\"\n    Send a message to the journald log.\n\n    @param kwargs: Mapping between field names to values, both as bytes.\n\n    @raise IOError: If the operation failed.\n    \"\"\"\n    # The function uses printf formatting, so we need to quote\n    # percentages.\n    fields = [\n        _ffi.new(\n            \"char[]\", key.encode(\"ascii\") + b'=' + value.replace(b\"%\", b\"%%\"))\n        for key, value in kwargs.items()]\n    fields.append(_ffi.NULL)\n    result = _journald.sd_journal_send(*fields)\n    if result != 0:\n        raise IOError(-result, strerror(-result))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecorating a function like inlineCallbacks but in a more Eliot - friendly way.", "response": "def inline_callbacks(original, debug=False):\n    \"\"\"\n    Decorate a function like ``inlineCallbacks`` would but in a more\n    Eliot-friendly way.  Use it just like ``inlineCallbacks`` but where you\n    want Eliot action contexts to Do The Right Thing inside the decorated\n    function.\n    \"\"\"\n    f = eliot_friendly_generator_function(original)\n    if debug:\n        f.debug = True\n    return inlineCallbacks(f)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addCallbacks(\n        self,\n        callback,\n        errback=None,\n        callbackArgs=None,\n        callbackKeywords=None,\n        errbackArgs=None,\n        errbackKeywords=None\n    ):\n        \"\"\"\n        Add a pair of callbacks that will be run in the context of an eliot\n        action.\n\n        @return: C{self}\n        @rtype: L{DeferredContext}\n\n        @raises AlreadyFinished: L{DeferredContext.addActionFinish} has been\n            called. This indicates a programmer error.\n        \"\"\"\n        if self._finishAdded:\n            raise AlreadyFinished()\n\n        if errback is None:\n            errback = _passthrough\n\n        def callbackWithContext(*args, **kwargs):\n            return self._action.run(callback, *args, **kwargs)\n\n        def errbackWithContext(*args, **kwargs):\n            return self._action.run(errback, *args, **kwargs)\n\n        self.result.addCallbacks(\n            callbackWithContext, errbackWithContext, callbackArgs,\n            callbackKeywords, errbackArgs, errbackKeywords)\n        return self", "response": "Adds a callback that will be run in the context of an eliot\n            action."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a success callback that will be run in the context of an eliot action.", "response": "def addCallback(self, callback, *args, **kw):\n        \"\"\"\n        Add a success callback that will be run in the context of an eliot\n        action.\n\n        @return: C{self}\n        @rtype: L{DeferredContext}\n\n        @raises AlreadyFinished: L{DeferredContext.addActionFinish} has been\n            called. This indicates a programmer error.\n        \"\"\"\n        return self.addCallbacks(\n            callback, _passthrough, callbackArgs=args, callbackKeywords=kw)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addErrback(self, errback, *args, **kw):\n        return self.addCallbacks(\n            _passthrough, errback, errbackArgs=args, errbackKeywords=kw)", "response": "Add a failure callback that will be run in the context of an eliot\n            action."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addBoth(self, callback, *args, **kw):\n        return self.addCallbacks(callback, callback, args, kw, args, kw)", "response": "Add a single callback as both success and failure callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef addActionFinish(self):\n        if self._finishAdded:\n            raise AlreadyFinished()\n        self._finishAdded = True\n\n        def done(result):\n            if isinstance(result, Failure):\n                exception = result.value\n            else:\n                exception = None\n            self._action.finish(exception)\n            return result\n\n        self.result.addBoth(done)\n        return self.result", "response": "A method that marks the action as finished."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _dumps(obj, cls=pyjson.JSONEncoder):\n\n    class WithBytes(cls):\n        \"\"\"\n        JSON encoder that supports L{bytes}.\n        \"\"\"\n\n        def default(self, o):\n            if isinstance(o, bytes):\n                warnings.warn(\n                    \"Eliot will soon stop supporting encoding bytes in JSON\"\n                    \" on Python 3\", DeprecationWarning\n                )\n                return o.decode(\"utf-8\")\n            return cls.default(self, o)\n\n    return pyjson.dumps(obj, cls=WithBytes).encode(\"utf-8\")", "response": "Encode to bytes and presume bytes in inputs are UTF - 8 encoded strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a copy of a module and distinct from what you ve imported.", "response": "def load_module(name, original_module):\n    \"\"\"\n    Load a copy of a module, distinct from what you'd get if you imported\n    it directly.\n\n    @param str name: The name of the new module.\n    @param original_module: The original module we're recreating.\n\n    @return: A new, distinct module.\n    \"\"\"\n    module = ModuleType(name)\n    if PY3:\n        import importlib.util\n        spec = importlib.util.find_spec(original_module.__name__)\n        source = spec.loader.get_code(original_module.__name__)\n    else:\n        if getattr(sys, \"frozen\", False):\n            raise NotImplementedError(\"Can't load modules on Python 2 with PyInstaller\")\n        path = original_module.__file__\n        if path.endswith(\".pyc\") or path.endswith(\".pyo\"):\n            path = path[:-1]\n        with open(path) as f:\n            source = f.read()\n    exec_(source, module.__dict__, module.__dict__)\n    return module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _writeTracebackMessage(logger, typ, exception, traceback):\n    msg = TRACEBACK_MESSAGE(\n        reason=exception, traceback=traceback, exception=typ)\n    msg = msg.bind(\n        **_error_extraction.get_fields_for_exception(logger, exception))\n    msg.write(logger)", "response": "Write a traceback to the log."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_traceback_no_io():\n    try:\n        module = load_module(str(\"_traceback_no_io\"), traceback)\n    except NotImplementedError:\n        # Can't fix the I/O problem, oh well:\n        return traceback\n\n    class FakeLineCache(object):\n        def checkcache(self, *args, **kwargs):\n            None\n\n        def getline(self, *args, **kwargs):\n            return \"\"\n\n        def lazycache(self, *args, **kwargs):\n            return None\n\n    module.linecache = FakeLineCache()\n    return module", "response": "Return a version of L{ traceback that doesn t do I / O."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the latest traceback to the log.", "response": "def write_traceback(logger=None, exc_info=None):\n    \"\"\"\n    Write the latest traceback to the log.\n\n    This should be used inside an C{except} block. For example:\n\n         try:\n             dostuff()\n         except:\n             write_traceback(logger)\n\n    Or you can pass the result of C{sys.exc_info()} to the C{exc_info}\n    parameter.\n    \"\"\"\n    if exc_info is None:\n        exc_info = sys.exc_info()\n    typ, exception, tb = exc_info\n    traceback = \"\".join(_traceback_no_io.format_exception(typ, exception, tb))\n    _writeTracebackMessage(logger, typ, exception, traceback)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a failure to the log.", "response": "def writeFailure(failure, logger=None):\n    \"\"\"\n    Write a L{twisted.python.failure.Failure} to the log.\n\n    This is for situations where you got an unexpected exception and want to\n    log a traceback. For example, if you have C{Deferred} that might error,\n    you'll want to wrap it with a L{eliot.twisted.DeferredContext} and then add\n    C{writeFailure} as the error handler to get the traceback logged:\n\n        d = DeferredContext(dostuff())\n        d.addCallback(process)\n        # Final error handler.\n        d.addErrback(writeFailure)\n\n    @param failure: L{Failure} to write to the log.\n\n    @type logger: L{eliot.ILogger}. Will be deprecated at some point, so just\n        ignore it.\n\n    @return: None\n    \"\"\"\n    # Failure.getBriefTraceback does not include source code, so does not do\n    # I/O.\n    _writeTracebackMessage(\n        logger, failure.value.__class__, failure.value,\n        failure.getBriefTraceback())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exclusively(f):\n    @wraps(f)\n    def exclusively_f(self, *a, **kw):\n        with self._lock:\n            return f(self, *a, **kw)\n    return exclusively_f", "response": "Decorator to make it thread - safe by serializing invocations\n            using a per - instance lock."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_file(output_file, encoder=EliotJSONEncoder):\n    Logger._destinations.add(\n        FileDestination(file=output_file, encoder=encoder)\n    )", "response": "Add a destination that writes a JSON message per line to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send(self, message):\n        message.update(self._globalFields)\n        errors = []\n        for dest in self._destinations:\n            try:\n                dest(message)\n            except:\n                errors.append(sys.exc_info())\n        if errors:\n            raise _DestinationsSendError(errors)", "response": "Sends a message to all destinations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds new destinations to the dictionary.", "response": "def add(self, *destinations):\n        \"\"\"\n        Adds new destinations.\n\n        A destination should never ever throw an exception. Seriously.\n        A destination should not mutate the dictionary it is given.\n\n        @param destinations: A list of callables that takes message\n            dictionaries.\n        \"\"\"\n        buffered_messages = None\n        if not self._any_added:\n            # These are first set of messages added, so we need to clear\n            # BufferingDestination:\n            self._any_added = True\n            buffered_messages = self._destinations[0].messages\n            self._destinations = []\n        self._destinations.extend(destinations)\n        if buffered_messages:\n            # Re-deliver buffered messages:\n            for message in buffered_messages:\n                self.send(message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode(timestamp):\n    seconds = int(timestamp)\n    nanoseconds = int((timestamp - seconds) * 1000000000)\n    seconds = seconds + _OFFSET\n    encoded = b2a_hex(struct.pack(_STRUCTURE, seconds, nanoseconds))\n    return \"@\" + encoded.decode(\"ascii\")", "response": "Convert seconds since UTC Unix epoch as TAI64N string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef decode(tai64n):\n    seconds, nanoseconds = struct.unpack(_STRUCTURE, a2b_hex(tai64n[1:]))\n    seconds -= _OFFSET\n    return seconds + (nanoseconds / 1000000000.0)", "response": "Convert TAI64N string to seconds since UTC Unix epoch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new child L{Action and add the start message to the log.", "response": "def start_action(logger=None, action_type=\"\", _serializers=None, **fields):\n    \"\"\"\n    Create a child L{Action}, figuring out the parent L{Action} from execution\n    context, and log the start message.\n\n    You can use the result as a Python context manager, or use the\n    L{Action.finish} API to explicitly finish it.\n\n         with start_action(logger, \"yourapp:subsystem:dosomething\",\n                          entry=x) as action:\n              do(x)\n              result = something(x * 2)\n              action.addSuccessFields(result=result)\n\n    Or alternatively:\n\n         action = start_action(logger, \"yourapp:subsystem:dosomething\",\n                              entry=x)\n         with action.context():\n              do(x)\n              result = something(x * 2)\n              action.addSuccessFields(result=result)\n         action.finish()\n\n    @param logger: The L{eliot.ILogger} to which to write messages, or\n        C{None} to use the default one.\n\n    @param action_type: The type of this action,\n        e.g. C{\"yourapp:subsystem:dosomething\"}.\n\n    @param _serializers: Either a L{eliot._validation._ActionSerializers}\n        instance or C{None}. In the latter case no validation or serialization\n        will be done for messages generated by the L{Action}.\n\n    @param fields: Additional fields to add to the start message.\n\n    @return: A new L{Action}.\n    \"\"\"\n    parent = current_action()\n    if parent is None:\n        return startTask(logger, action_type, _serializers, **fields)\n    else:\n        action = parent.child(logger, action_type, _serializers)\n        action._start(fields)\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef startTask(logger=None, action_type=u\"\", _serializers=None, **fields):\n    action = Action(\n        logger,\n        unicode(uuid4()),\n        TaskLevel(level=[]),\n        action_type,\n        _serializers)\n    action._start(fields)\n    return action", "response": "Like action but creates a new top - level L{Action with no parent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef preserve_context(f):\n    action = current_action()\n    if action is None:\n        return f\n    task_id = action.serialize_task_id()\n    called = threading.Lock()\n\n    def restore_eliot_context(*args, **kwargs):\n        # Make sure the function has not already been called:\n        if not called.acquire(False):\n            raise TooManyCalls(f)\n\n        with Action.continue_task(task_id=task_id):\n            return f(*args, **kwargs)\n\n    return restore_eliot_context", "response": "Return a function that returns a copy of the current Eliot context and then calls given function with the current Eliot context and then returns the original Eliot context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_call(\n        wrapped_function=None, action_type=None, include_args=None,\n        include_result=True\n):\n    \"\"\"Decorator/decorator factory that logs inputs and the return result.\n\n    If used with inputs (i.e. as a decorator factory), it accepts the following\n    parameters:\n\n    @param action_type: The action type to use.  If not given the function name\n        will be used.\n    @param include_args: If given, should be a list of strings, the arguments to log.\n    @param include_result: True by default. If False, the return result isn't logged.\n    \"\"\"\n    if wrapped_function is None:\n        return partial(log_call, action_type=action_type, include_args=include_args,\n                       include_result=include_result)\n\n    if action_type is None:\n        if PY3:\n            action_type = \"{}.{}\".format(wrapped_function.__module__,\n                                         wrapped_function.__qualname__)\n        else:\n            action_type = wrapped_function.__name__\n\n    if PY3 and include_args is not None:\n        from inspect import signature\n        sig = signature(wrapped_function)\n        if set(include_args) - set(sig.parameters):\n            raise ValueError(\n                (\"include_args ({}) lists arguments not in the \"\n                 \"wrapped function\").format(include_args)\n            )\n\n    @wraps(wrapped_function)\n    def logging_wrapper(*args, **kwargs):\n        callargs = getcallargs(wrapped_function, *args, **kwargs)\n\n        # Remove self is it's included:\n        if \"self\" in callargs:\n            callargs.pop(\"self\")\n\n        # Filter arguments to log, if necessary:\n        if include_args is not None:\n            callargs = {k: callargs[k] for k in include_args}\n\n        with start_action(action_type=action_type, **callargs) as ctx:\n            result = wrapped_function(*args, **kwargs)\n            if include_result:\n                ctx.add_success_fields(result=result)\n            return result\n\n    return logging_wrapper", "response": "A decorator that logs the given function and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a serialized Unicode string to a L { TaskLevel } object.", "response": "def fromString(cls, string):\n        \"\"\"\n        Convert a serialized Unicode string to a L{TaskLevel}.\n\n        @param string: Output of L{TaskLevel.toString}.\n\n        @return: L{TaskLevel} parsed from the string.\n        \"\"\"\n        return cls(level=[int(i) for i in string.split(\"/\") if i])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a child of this L { TaskLevel }.", "response": "def child(self):\n        \"\"\"\n        Return a child of this L{TaskLevel}.\n\n        @return: L{TaskLevel} which is the first child of this one.\n        \"\"\"\n        new_level = self._level[:]\n        new_level.append(1)\n        return TaskLevel(level=new_level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef serialize_task_id(self):\n        return \"{}@{}\".format(\n            self._identification[TASK_UUID_FIELD],\n            self._nextTaskLevel().toString()).encode(\"ascii\")", "response": "Create a unique identifier for the current location within the task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a new action which is part of a serialized task.", "response": "def continue_task(cls, logger=None, task_id=_TASK_ID_NOT_SUPPLIED):\n        \"\"\"\n        Start a new action which is part of a serialized task.\n\n        @param logger: The L{eliot.ILogger} to which to write\n            messages, or C{None} if the default one should be used.\n\n        @param task_id: A serialized task identifier, the output of\n            L{Action.serialize_task_id}, either ASCII-encoded bytes or unicode\n            string. Required.\n\n        @return: The new L{Action} instance.\n        \"\"\"\n        if task_id is _TASK_ID_NOT_SUPPLIED:\n            raise RuntimeError(\"You must supply a task_id keyword argument.\")\n        if isinstance(task_id, bytes):\n            task_id = task_id.decode(\"ascii\")\n        uuid, task_level = task_id.split(\"@\")\n        action = cls(\n            logger, uuid, TaskLevel.fromString(task_level),\n            \"eliot:remote_task\")\n        action._start({})\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _nextTaskLevel(self):\n        if not self._last_child:\n            self._last_child = self._task_level.child()\n        else:\n            self._last_child = self._last_child.next_sibling()\n        return self._last_child", "response": "Returns the next C { task_level } for messages within this action."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging the start message.", "response": "def _start(self, fields):\n        \"\"\"\n        Log the start message.\n\n        The action identification fields, and any additional given fields,\n        will be logged.\n\n        In general you shouldn't call this yourself, instead using a C{with}\n        block or L{Action.finish}.\n        \"\"\"\n        fields[ACTION_STATUS_FIELD] = STARTED_STATUS\n        fields.update(self._identification)\n        if self._serializers is None:\n            serializer = None\n        else:\n            serializer = self._serializers.start\n        Message(fields, serializer).write(self._logger, self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef finish(self, exception=None):\n        if self._finished:\n            return\n        self._finished = True\n        serializer = None\n        if exception is None:\n            fields = self._successFields\n            fields[ACTION_STATUS_FIELD] = SUCCEEDED_STATUS\n            if self._serializers is not None:\n                serializer = self._serializers.success\n        else:\n            fields = _error_extraction.get_fields_for_exception(\n                self._logger, exception)\n            fields[EXCEPTION_FIELD] = \"%s.%s\" % (\n                exception.__class__.__module__, exception.__class__.__name__)\n            fields[REASON_FIELD] = safeunicode(exception)\n            fields[ACTION_STATUS_FIELD] = FAILED_STATUS\n            if self._serializers is not None:\n                serializer = self._serializers.failure\n\n        fields.update(self._identification)\n        Message(fields, serializer).write(self._logger, self)", "response": "Log the finish message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a child of this action.", "response": "def child(self, logger, action_type, serializers=None):\n        \"\"\"\n        Create a child L{Action}.\n\n        Rather than calling this directly, you can use L{start_action} to\n        create child L{Action} using the execution context.\n\n        @param logger: The L{eliot.ILogger} to which to write\n            messages.\n\n        @param action_type: The type of this action,\n            e.g. C{\"yourapp:subsystem:dosomething\"}.\n\n        @param serializers: Either a L{eliot._validation._ActionSerializers}\n            instance or C{None}. In the latter case no validation or\n            serialization will be done for messages generated by the\n            L{Action}.\n        \"\"\"\n        newLevel = self._nextTaskLevel()\n        return self.__class__(\n            logger, self._identification[TASK_UUID_FIELD], newLevel,\n            action_type, serializers)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the given function with this L { Action } as its execution context.", "response": "def run(self, f, *args, **kwargs):\n        \"\"\"\n        Run the given function with this L{Action} as its execution context.\n        \"\"\"\n        parent = _ACTION_CONTEXT.set(self)\n        try:\n            return f(*args, **kwargs)\n        finally:\n            _ACTION_CONTEXT.reset(parent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef context(self):\n        parent = _ACTION_CONTEXT.set(self)\n        try:\n            yield self\n        finally:\n            _ACTION_CONTEXT.reset(parent)", "response": "Context manager that ensures code runs within action s context."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new object from a list of WrittenMessages.", "response": "def from_messages(\n        cls, start_message=None, children=pvector(), end_message=None):\n        \"\"\"\n        Create a C{WrittenAction} from C{WrittenMessage}s and other\n        C{WrittenAction}s.\n\n        @param WrittenMessage start_message: A message that has\n            C{ACTION_STATUS_FIELD}, C{ACTION_TYPE_FIELD}, and a C{task_level}\n            that ends in C{1}, or C{None} if unavailable.\n        @param children: An iterable of C{WrittenMessage} and C{WrittenAction}\n        @param WrittenMessage end_message: A message that has the same\n            C{action_type} as this action.\n\n        @raise WrongTask: If C{end_message} has a C{task_uuid} that differs\n            from C{start_message.task_uuid}.\n        @raise WrongTaskLevel: If any child message or C{end_message} has a\n            C{task_level} that means it is not a direct child.\n        @raise WrongActionType: If C{end_message} has an C{ACTION_TYPE_FIELD}\n            that differs from the C{ACTION_TYPE_FIELD} of C{start_message}.\n        @raise InvalidStatus: If C{end_message} doesn't have an\n            C{action_status}, or has one that is not C{SUCCEEDED_STATUS} or\n            C{FAILED_STATUS}.\n        @raise InvalidStartMessage: If C{start_message} does not have a\n            C{ACTION_STATUS_FIELD} of C{STARTED_STATUS}, or if it has a\n            C{task_level} indicating that it is not the first message of an\n            action.\n\n        @return: A new C{WrittenAction}.\n        \"\"\"\n        actual_message = [\n            message\n            for message in [start_message, end_message] + list(children)\n            if message][0]\n        action = cls(\n            task_level=actual_message.task_level.parent(),\n            task_uuid=actual_message.task_uuid, )\n        if start_message:\n            action = action._start(start_message)\n        for child in children:\n            if action._children.get(child.task_level, child) != child:\n                raise DuplicateChild(action, child)\n            action = action._add_child(child)\n        if end_message:\n            action = action._end(end_message)\n        return action"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef action_type(self):\n        if self.start_message:\n            return self.start_message.contents[ACTION_TYPE_FIELD]\n        elif self.end_message:\n            return self.end_message.contents[ACTION_TYPE_FIELD]\n        else:\n            return None", "response": "Returns the type of this action in the ISO format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef status(self):\n        message = self.end_message if self.end_message else self.start_message\n        if message:\n            return message.contents[ACTION_STATUS_FIELD]\n        else:\n            return None", "response": "Return the status of the action for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the list of child messages and actions sorted by task level excluding the start and end messages.", "response": "def children(self):\n        \"\"\"\n        The list of child messages and actions sorted by task level, excluding the\n        start and end messages.\n        \"\"\"\n        return pvector(\n            sorted(self._children.values(), key=lambda m: m.task_level))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the given message is a valid direct child of this action.", "response": "def _validate_message(self, message):\n        \"\"\"\n        Is C{message} a valid direct child of this action?\n\n        @param message: Either a C{WrittenAction} or a C{WrittenMessage}.\n\n        @raise WrongTask: If C{message} has a C{task_uuid} that differs from the\n            action's C{task_uuid}.\n        @raise WrongTaskLevel: If C{message} has a C{task_level} that means\n            it's not a direct child.\n        \"\"\"\n        if message.task_uuid != self.task_uuid:\n            raise WrongTask(self, message)\n        if not message.task_level.parent() == self.task_level:\n            raise WrongTaskLevel(self, message)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a child to the action.", "response": "def _add_child(self, message):\n        \"\"\"\n        Return a new action with C{message} added as a child.\n\n        Assumes C{message} is not an end message.\n\n        @param message: Either a C{WrittenAction} or a C{WrittenMessage}.\n\n        @raise WrongTask: If C{message} has a C{task_uuid} that differs from the\n            action's C{task_uuid}.\n        @raise WrongTaskLevel: If C{message} has a C{task_level} that means\n            it's not a direct child.\n\n        @return: A new C{WrittenAction}.\n        \"\"\"\n        self._validate_message(message)\n        level = message.task_level\n        return self.transform(('_children', level), message)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start(self, start_message):\n        if start_message.contents.get(\n            ACTION_STATUS_FIELD, None) != STARTED_STATUS:\n            raise InvalidStartMessage.wrong_status(start_message)\n        if start_message.task_level.level[-1] != 1:\n            raise InvalidStartMessage.wrong_task_level(start_message)\n        return self.set(start_message=start_message)", "response": "Start this action given its start message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nending this action with the given end message.", "response": "def _end(self, end_message):\n        \"\"\"\n        End this action with C{end_message}.\n\n        Assumes that the action has not already been ended.\n\n        @param WrittenMessage end_message: An end message that has the\n            same level as this action.\n\n        @raise WrongTask: If C{end_message} has a C{task_uuid} that differs\n            from the action's C{task_uuid}.\n        @raise WrongTaskLevel: If C{end_message} has a C{task_level} that means\n            it's not a direct child.\n        @raise InvalidStatus: If C{end_message} doesn't have an\n            C{action_status}, or has one that is not C{SUCCEEDED_STATUS} or\n            C{FAILED_STATUS}.\n\n        @return: A new, completed C{WrittenAction}.\n        \"\"\"\n        action_type = end_message.contents.get(ACTION_TYPE_FIELD, None)\n        if self.action_type not in (None, action_type):\n            raise WrongActionType(self, end_message)\n        self._validate_message(end_message)\n        status = end_message.contents.get(ACTION_STATUS_FIELD, None)\n        if status not in (FAILED_STATUS, SUCCEEDED_STATUS):\n            raise InvalidStatus(self, end_message)\n        return self.set(end_message=end_message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of fields for the .", "response": "def fields(*fields, **keys):\n    \"\"\"\n    Factory for for L{MessageType} and L{ActionType} field definitions.\n\n    @param *fields: A L{tuple} of L{Field} instances.\n\n    @param **keys: A L{dict} mapping key names to the expected type of the\n        field's values.\n\n    @return: A L{list} of L{Field} instances.\n    \"\"\"\n    return list(fields) + [\n        Field.forTypes(key, [value], \"\") for key, value in keys.items()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate the given input value against this field definition.", "response": "def validate(self, input):\n        \"\"\"\n        Validate the given input value against this L{Field} definition.\n\n        @param input: An input value supposedly serializable by this L{Field}.\n\n        @raises ValidationError: If the value is not serializable or fails to\n            be validated by the additional validator.\n        \"\"\"\n        # Make sure the input serializes:\n        self._serializer(input)\n        # Use extra validator, if given:\n        if self._extraValidator is not None:\n            self._extraValidator(input)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new L { Field } that can only have a single value.", "response": "def forValue(klass, key, value, description):\n        \"\"\"\n        Create a L{Field} that can only have a single value.\n\n        @param key: The name of the field, the key which refers to it,\n            e.g. C{\"path\"}.\n\n        @param value: The allowed value for the field.\n\n        @param description: A description of what this field contains.\n        @type description: C{unicode}\n\n        @return: A L{Field}.\n        \"\"\"\n\n        def validate(checked):\n            if checked != value:\n                raise ValidationError(\n                    checked, \"Field %r must be %r\" % (key, value))\n\n        return klass(key, lambda _: value, description, validate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forTypes(klass, key, classes, description, extraValidator=None):\n        fixedClasses = []\n        for k in classes:\n            if k is None:\n                k = type(None)\n            if k not in _JSON_TYPES:\n                raise TypeError(\"%s is not JSON-encodeable\" % (k, ))\n            fixedClasses.append(k)\n        fixedClasses = tuple(fixedClasses)\n\n        def validate(value):\n            if not isinstance(value, fixedClasses):\n                raise ValidationError(\n                    value,\n                    \"Field %r requires type to be one of %s\" % (key, classes))\n            if extraValidator is not None:\n                extraValidator(value)\n\n        return klass(key, lambda v: v, description, extraValidator=validate)", "response": "Create a new L { Field } that must be an instance of one of the given types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nserializes the given message in - place converting inputs to outputs.", "response": "def serialize(self, message):\n        \"\"\"\n        Serialize the given message in-place, converting inputs to outputs.\n\n        We do this in-place for performance reasons. There are more fields in\n        a message than there are L{Field} objects because of the timestamp,\n        task_level and task_uuid fields. By only iterating over our L{Fields}\n        we therefore reduce the number of function calls in a critical code\n        path.\n\n        @param message: A C{dict}.\n        \"\"\"\n        for key, field in self.fields.items():\n            message[key] = field.serialize(message[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, message):\n        for key, field in self.fields.items():\n            if key not in message:\n                raise ValidationError(message, \"Field %r is missing\" % (key, ))\n            field.validate(message[key])\n\n        if self.allow_additional_fields:\n            return\n        # Otherwise, additional fields are not allowed:\n        fieldSet = set(self.fields) | set(RESERVED_FIELDS)\n        for key in message:\n            if key not in fieldSet:\n                raise ValidationError(message, \"Unexpected field %r\" % (key, ))", "response": "Validate the given message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting a new action of this type as a task.", "response": "def as_task(self, logger=None, **fields):\n        \"\"\"\n        Start a new L{eliot.Action} of this type as a task (i.e. top-level\n        action) with the given start fields.\n\n        See L{ActionType.__call__} for example of usage.\n\n        @param logger: A L{eliot.ILogger} provider to which the action's\n            messages will be written, or C{None} to use the default one.\n\n        @param fields: Extra fields to add to the message.\n\n        @rtype: L{eliot.Action}\n        \"\"\"\n        return self._startTask(\n            logger, self.action_type, self._serializers, **fields)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pretty_format(message):\n    skip = {\n        TIMESTAMP_FIELD, TASK_UUID_FIELD, TASK_LEVEL_FIELD, MESSAGE_TYPE_FIELD,\n        ACTION_TYPE_FIELD, ACTION_STATUS_FIELD}\n\n    def add_field(previous, key, value):\n        value = unicode(pprint.pformat(value, width=40)).replace(\n            \"\\\\n\", \"\\n \").replace(\"\\\\t\", \"\\t\")\n        # Reindent second line and later to match up with first line's\n        # indentation:\n        lines = value.split(\"\\n\")\n        # indent lines are \"  <key length>|  <value>\"\n        indent = \"{}| \".format(\" \" * (2 + len(key)))\n        value = \"\\n\".join([lines[0]] + [indent + l for l in lines[1:]])\n        return \"  %s: %s\\n\" % (key, value)\n\n    remaining = \"\"\n    for field in [ACTION_TYPE_FIELD, MESSAGE_TYPE_FIELD, ACTION_STATUS_FIELD]:\n        if field in message:\n            remaining += add_field(remaining, field, message[field])\n    for (key, value) in sorted(message.items()):\n        if key not in skip:\n            remaining += add_field(remaining, key, value)\n\n    level = \"/\" + \"/\".join(map(unicode, message[TASK_LEVEL_FIELD]))\n    return \"%s -> %s\\n%sZ\\n%s\" % (\n        message[TASK_UUID_FIELD],\n        level,\n        # If we were returning or storing the datetime we'd want to use an\n        # explicit timezone instead of a naive datetime, but since we're\n        # just using it for formatting we needn't bother.\n        datetime.utcfromtimestamp(message[TIMESTAMP_FIELD]).isoformat(\n            sep=str(\" \")),\n        remaining, )", "response": "Convert a message dictionary into a human - readable string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncommands - line program that reads in JSON from stdin and writes out out AttributeNames pretty - printed messages to stdout.", "response": "def _main():\n    \"\"\"\n    Command-line program that reads in JSON from stdin and writes out\n    pretty-printed messages to stdout.\n    \"\"\"\n    if argv[1:]:\n        stdout.write(_CLI_HELP)\n        raise SystemExit()\n    for line in stdin:\n        try:\n            message = loads(line)\n        except ValueError:\n            stdout.write(\"Not JSON: {}\\n\\n\".format(line.rstrip(b\"\\n\")))\n            continue\n        if REQUIRED_FIELDS - set(message.keys()):\n            stdout.write(\n                \"Not an Eliot message: {}\\n\\n\".format(line.rstrip(b\"\\n\")))\n            continue\n        result = pretty_format(message) + \"\\n\"\n        if PY2:\n            result = result.encode(\"utf-8\")\n        stdout.write(result)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_fields_for_exception(self, logger, exception):\n        for klass in getmro(exception.__class__):\n            if klass in self.registry:\n                extractor = self.registry[klass]\n                try:\n                    return extractor(exception)\n                except:\n                    from ._traceback import write_traceback\n                    write_traceback(logger)\n                    return {}\n        return {}", "response": "Given an exception instance return the fields to add to the failed action\n        message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the program. Accept arguments from L{sys.argv}, read from L{sys.stdin}, write to L{sys.stdout}. @param sys: An object with same interface and defaulting to the L{sys} module.", "response": "def main(sys=sys):\n    \"\"\"\n    Run the program.\n\n    Accept arguments from L{sys.argv}, read from L{sys.stdin}, write to\n    L{sys.stdout}.\n\n    @param sys: An object with same interface and defaulting to the L{sys}\n        module.\n    \"\"\"\n    if len(sys.argv) != 2:\n        sys.stderr.write(USAGE)\n        return 1\n    EliotFilter(sys.argv[1], sys.stdin, sys.stdout).run()\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self):\n        for line in self.incoming:\n            message = loads(line)\n            result = self._evaluate(message)\n            if result is self._SKIP:\n                continue\n            self.output.write(dumps(result, cls=_DatetimeJSONEncoder) + b\"\\n\")", "response": "Iterate through incoming messages and evaluate the expression and write the result to the output file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nevaluates the expression with the given Python object in its locals.", "response": "def _evaluate(self, message):\n        \"\"\"\n        Evaluate the expression with the given Python object in its locals.\n\n        @param message: A decoded JSON input.\n\n        @return: The resulting object.\n        \"\"\"\n        return eval(\n            self.code,\n            globals(), {\n                \"J\": message,\n                \"timedelta\": timedelta,\n                \"datetime\": datetime,\n                \"SKIP\": self._SKIP})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the current request and write the response headers.", "response": "def respond(self):\n        \"\"\"Process the current request.\n\n        From :pep:`333`:\n\n            The start_response callable must not actually transmit\n            the response headers. Instead, it must store them for the\n            server or gateway to transmit only after the first\n            iteration of the application return value that yields\n            a NON-EMPTY string, or upon the application's first\n            invocation of the write() callable.\n        \"\"\"\n        response = self.req.server.wsgi_app(self.env, self.start_response)\n        try:\n            for chunk in filter(None, response):\n                if not isinstance(chunk, six.binary_type):\n                    raise ValueError('WSGI Applications must yield bytes')\n                self.write(chunk)\n        finally:\n            # Send headers if not already sent\n            self.req.ensure_headers_sent()\n            if hasattr(response, 'close'):\n                response.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncasts status to bytes representation of current Python version.", "response": "def _encode_status(status):\n        \"\"\"Cast status to bytes representation of current Python version.\n\n        According to :pep:`3333`, when using Python 3, the response status\n        and headers must be bytes masquerading as unicode; that is, they\n        must be of type \"str\" but are restricted to code points in the\n        \"latin-1\" set.\n        \"\"\"\n        if six.PY2:\n            return status\n        if not isinstance(status, str):\n            raise TypeError('WSGI response status is not of type str.')\n        return status.encode('ISO-8859-1')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_environ(self):\n        req = self.req\n        req_conn = req.conn\n        env = {\n            # set a non-standard environ entry so the WSGI app can know what\n            # the *real* server protocol is (and what features to support).\n            # See http://www.faqs.org/rfcs/rfc2145.html.\n            'ACTUAL_SERVER_PROTOCOL': req.server.protocol,\n            'PATH_INFO': bton(req.path),\n            'QUERY_STRING': bton(req.qs),\n            'REMOTE_ADDR': req_conn.remote_addr or '',\n            'REMOTE_PORT': str(req_conn.remote_port or ''),\n            'REQUEST_METHOD': bton(req.method),\n            'REQUEST_URI': bton(req.uri),\n            'SCRIPT_NAME': '',\n            'SERVER_NAME': req.server.server_name,\n            # Bah. \"SERVER_PROTOCOL\" is actually the REQUEST protocol.\n            'SERVER_PROTOCOL': bton(req.request_protocol),\n            'SERVER_SOFTWARE': req.server.software,\n            'wsgi.errors': sys.stderr,\n            'wsgi.input': req.rfile,\n            'wsgi.input_terminated': bool(req.chunked_read),\n            'wsgi.multiprocess': False,\n            'wsgi.multithread': True,\n            'wsgi.run_once': False,\n            'wsgi.url_scheme': bton(req.scheme),\n            'wsgi.version': self.version,\n        }\n\n        if isinstance(req.server.bind_addr, six.string_types):\n            # AF_UNIX. This isn't really allowed by WSGI, which doesn't\n            # address unix domain sockets. But it's better than nothing.\n            env['SERVER_PORT'] = ''\n            try:\n                env['X_REMOTE_PID'] = str(req_conn.peer_pid)\n                env['X_REMOTE_UID'] = str(req_conn.peer_uid)\n                env['X_REMOTE_GID'] = str(req_conn.peer_gid)\n\n                env['X_REMOTE_USER'] = str(req_conn.peer_user)\n                env['X_REMOTE_GROUP'] = str(req_conn.peer_group)\n\n                env['REMOTE_USER'] = env['X_REMOTE_USER']\n            except RuntimeError:\n                \"\"\"Unable to retrieve peer creds data.\n\n                Unsupported by current kernel or socket error happened, or\n                unsupported socket type, or disabled.\n                \"\"\"\n        else:\n            env['SERVER_PORT'] = str(req.server.bind_addr[1])\n\n        # Request headers\n        env.update(\n            ('HTTP_' + bton(k).upper().replace('-', '_'), bton(v))\n            for k, v in req.inheaders.items()\n        )\n\n        # CONTENT_TYPE/CONTENT_LENGTH\n        ct = env.pop('HTTP_CONTENT_TYPE', None)\n        if ct is not None:\n            env['CONTENT_TYPE'] = ct\n        cl = env.pop('HTTP_CONTENT_LENGTH', None)\n        if cl is not None:\n            env['CONTENT_LENGTH'] = cl\n\n        if req.conn.ssl_env:\n            env.update(req.conn.ssl_env)\n\n        return env", "response": "Return a new environ dict targeting the given wsgi. version."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new environ dict targeting the given wsgi. version.", "response": "def get_environ(self):\n        \"\"\"Return a new environ dict targeting the given wsgi.version.\"\"\"\n        req = self.req\n        env_10 = super(Gateway_u0, self).get_environ()\n        env = dict(map(self._decode_key, env_10.items()))\n\n        # Request-URI\n        enc = env.setdefault(six.u('wsgi.url_encoding'), six.u('utf-8'))\n        try:\n            env['PATH_INFO'] = req.path.decode(enc)\n            env['QUERY_STRING'] = req.qs.decode(enc)\n        except UnicodeDecodeError:\n            # Fall back to latin 1 so apps can transcode if needed.\n            env['wsgi.url_encoding'] = 'ISO-8859-1'\n            env['PATH_INFO'] = env_10['PATH_INFO']\n            env['QUERY_STRING'] = env_10['QUERY_STRING']\n\n        env.update(map(self._decode_value, env.items()))\n\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, b):\n        self._checkClosed()\n        if isinstance(b, str):\n            raise TypeError(\"can't write str to binary stream\")\n\n        with self._write_lock:\n            self._write_buf.extend(b)\n            self._flush_unlocked()\n            return len(b)", "response": "Write bytes to the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending all data to the socket.", "response": "def write(self, data):\n        \"\"\"Sendall for non-blocking sockets.\"\"\"\n        bytes_sent = 0\n        data_mv = memoryview(data)\n        payload_size = len(data_mv)\n        while bytes_sent < payload_size:\n            try:\n                bytes_sent += self.send(\n                    data_mv[bytes_sent:bytes_sent + SOCK_WRITE_BLOCKSIZE],\n                )\n            except socket.error as e:\n                if e.args[0] not in errors.socket_errors_nonblocking:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend some part of message to the socket.", "response": "def send(self, data):\n        \"\"\"Send some part of message to the socket.\"\"\"\n        bytes_sent = self._sock.send(extract_bytes(data))\n        self.bytes_written += bytes_sent\n        return bytes_sent"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flush(self):\n        if self._wbuf:\n            buffer = ''.join(self._wbuf)\n            self._wbuf = []\n            self.write(buffer)", "response": "Write all data from buffer to socket and reset write buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recv(self, size):\n        while True:\n            try:\n                data = self._sock.recv(size)\n                self.bytes_read += len(data)\n                return data\n            except socket.error as e:\n                what = (\n                    e.args[0] not in errors.socket_errors_nonblocking\n                    and e.args[0] not in errors.socket_error_eintr\n                )\n                if what:\n                    raise", "response": "Receive a message of a size from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a WSGI bind address string to a BindLocation object.", "response": "def parse_wsgi_bind_location(bind_addr_string):\n    \"\"\"Convert bind address string to a BindLocation.\"\"\"\n    # try and match for an IP/hostname and port\n    match = six.moves.urllib.parse.urlparse('//{}'.format(bind_addr_string))\n    try:\n        addr = match.hostname\n        port = match.port\n        if addr is not None or port is not None:\n            return TCPSocket(addr, port)\n    except ValueError:\n        pass\n\n    # else, assume a UNIX socket path\n    # if the string begins with an @ symbol, use an abstract socket\n    if bind_addr_string.startswith('@'):\n        return AbstractSocket(bind_addr_string[1:])\n    return UnixSocket(path=bind_addr_string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Cheroot instance with arguments from the command line.", "response": "def main():\n    \"\"\"Create a new Cheroot instance with arguments from the command line.\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Start an instance of the Cheroot WSGI/HTTP server.',\n    )\n    for arg, spec in _arg_spec.items():\n        parser.add_argument(arg, **spec)\n    raw_args = parser.parse_args()\n\n    # ensure cwd in sys.path\n    '' in sys.path or sys.path.insert(0, '')\n\n    # create a server based on the arguments provided\n    raw_args._wsgi_app.server(raw_args).safe_start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads WSGI app / Gateway path string and import application module.", "response": "def resolve(cls, full_path):\n        \"\"\"Read WSGI app/Gateway path string and import application module.\"\"\"\n        mod_path, _, app_path = full_path.partition(':')\n        app = getattr(import_module(mod_path), app_path or 'application')\n\n        with contextlib.suppress(TypeError):\n            if issubclass(app, server.Gateway):\n                return GatewayYo(app)\n\n        return cls(app)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns keyword args for Server class.", "response": "def server_args(self, parsed_args):\n        \"\"\"Return keyword args for Server class.\"\"\"\n        args = {\n            arg: value\n            for arg, value in vars(parsed_args).items()\n            if not arg.startswith('_') and value is not None\n        }\n        args.update(vars(self))\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef server(self, parsed_args):\n        server_args = vars(self)\n        server_args['bind_addr'] = parsed_args['bind_addr']\n        if parsed_args.max is not None:\n            server_args['maxthreads'] = parsed_args.max\n        if parsed_args.numthreads is not None:\n            server_args['minthreads'] = parsed_args.numthreads\n        return server.HTTPServer(**server_args)", "response": "Return an instance of the HTTP server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn error numbers for all errors in errnames on this platform.", "response": "def plat_specific_errors(*errnames):\n    \"\"\"Return error numbers for all errors in errnames on this platform.\n\n    The 'errno' module contains different global constants depending on\n    the specific platform (OS). This function will return the list of\n    numeric values for a given list of potential names.\n    \"\"\"\n    missing_attr = set([None, ])\n    unique_nums = set(getattr(errno, k, None) for k in errnames)\n    return list(unique_nums - missing_attr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether SSL exception contains either of the provided messages.", "response": "def _assert_ssl_exc_contains(exc, *msgs):\n    \"\"\"Check whether SSL exception contains either of messages provided.\"\"\"\n    if len(msgs) < 1:\n        raise TypeError(\n            '_assert_ssl_exc_contains() requires '\n            'at least one message to be passed.',\n        )\n    err_msg_lower = str(exc).lower()\n    return any(m.lower() in err_msg_lower for m in msgs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wrap(self, sock):\n        EMPTY_RESULT = None, {}\n        try:\n            s = self.context.wrap_socket(\n                sock, do_handshake_on_connect=True, server_side=True,\n            )\n        except ssl.SSLError as ex:\n            if ex.errno == ssl.SSL_ERROR_EOF:\n                # This is almost certainly due to the cherrypy engine\n                # 'pinging' the socket to assert it's connectable;\n                # the 'ping' isn't SSL.\n                return EMPTY_RESULT\n            elif ex.errno == ssl.SSL_ERROR_SSL:\n                if _assert_ssl_exc_contains(ex, 'http request'):\n                    # The client is speaking HTTP to an HTTPS server.\n                    raise errors.NoSSLError\n\n                # Check if it's one of the known errors\n                # Errors that are caught by PyOpenSSL, but thrown by\n                # built-in ssl\n                _block_errors = (\n                    'unknown protocol', 'unknown ca', 'unknown_ca',\n                    'unknown error',\n                    'https proxy request', 'inappropriate fallback',\n                    'wrong version number',\n                    'no shared cipher', 'certificate unknown',\n                    'ccs received early',\n                    'certificate verify failed',  # client cert w/o trusted CA\n                )\n                if _assert_ssl_exc_contains(ex, *_block_errors):\n                    # Accepted error, let's pass\n                    return EMPTY_RESULT\n            elif _assert_ssl_exc_contains(ex, 'handshake operation timed out'):\n                # This error is thrown by builtin SSL after a timeout\n                # when client is speaking HTTP to an HTTPS server.\n                # The connection can safely be dropped.\n                return EMPTY_RESULT\n            raise\n        except generic_socket_error as exc:\n            \"\"\"It is unclear why exactly this happens.\n\n            It's reproducible only with openssl>1.0 and stdlib ``ssl`` wrapper.\n            In CherryPy it's triggered by Checker plugin, which connects\n            to the app listening to the socket port in TLS mode via plain\n            HTTP during startup (from the same process).\n\n\n            Ref: https://github.com/cherrypy/cherrypy/issues/1618\n            \"\"\"\n            is_error0 = exc.args == (0, 'Error')\n\n            if is_error0 and IS_ABOVE_OPENSSL10:\n                return EMPTY_RESULT\n            raise\n        return s, self.get_environ(s)", "response": "Wrap and return the given socket plus WSGI environ entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates WSGI environ entries to be merged into each request.", "response": "def get_environ(self, sock):\n        \"\"\"Create WSGI environ entries to be merged into each request.\"\"\"\n        cipher = sock.cipher()\n        ssl_environ = {\n            'wsgi.url_scheme': 'https',\n            'HTTPS': 'on',\n            'SSL_PROTOCOL': cipher[1],\n            'SSL_CIPHER': cipher[0],\n            # SSL_VERSION_INTERFACE     string  The mod_ssl program version\n            # SSL_VERSION_LIBRARY   string  The OpenSSL program version\n        }\n\n        if self.context and self.context.verify_mode != ssl.CERT_NONE:\n            client_cert = sock.getpeercert()\n            if client_cert:\n                for cert_key, env_var in self.CERT_KEY_TO_ENV.items():\n                    ssl_environ.update(\n                        self.env_dn_dict(env_var, client_cert.get(cert_key)),\n                    )\n\n        return ssl_environ"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict of WSGI environment variables for a client cert DN.", "response": "def env_dn_dict(self, env_prefix, cert_value):\n        \"\"\"Return a dict of WSGI environment variables for a client cert DN.\n\n        E.g. SSL_CLIENT_S_DN_CN, SSL_CLIENT_S_DN_C, etc.\n        See SSL_CLIENT_S_DN_x509 at\n        https://httpd.apache.org/docs/2.4/mod/mod_ssl.html#envvars.\n        \"\"\"\n        if not cert_value:\n            return {}\n\n        env = {}\n        for rdn in cert_value:\n            for attr_name, val in rdn:\n                attr_code = self.CERT_KEY_TO_LDAP_CODE.get(attr_name)\n                if attr_code:\n                    env['%s_%s' % (env_prefix, attr_code)] = val\n        return env"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef makefile(self, sock, mode='r', bufsize=DEFAULT_BUFFER_SIZE):\n        cls = StreamReader if 'r' in mode else StreamWriter\n        return cls(sock, mode, bufsize)", "response": "Return socket file object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap the given call with SSL error - trapping.", "response": "def _safe_call(self, is_reader, call, *args, **kwargs):\n        \"\"\"Wrap the given call with SSL error-trapping.\n\n        is_reader: if False EOF errors will be raised. If True, EOF errors\n        will return \"\" (to emulate normal sockets).\n        \"\"\"\n        start = time.time()\n        while True:\n            try:\n                return call(*args, **kwargs)\n            except SSL.WantReadError:\n                # Sleep and try again. This is dangerous, because it means\n                # the rest of the stack has no way of differentiating\n                # between a \"new handshake\" error and \"client dropped\".\n                # Note this isn't an endless loop: there's a timeout below.\n                # Ref: https://stackoverflow.com/a/5133568/595220\n                time.sleep(self.ssl_retry)\n            except SSL.WantWriteError:\n                time.sleep(self.ssl_retry)\n            except SSL.SysCallError as e:\n                if is_reader and e.args == (-1, 'Unexpected EOF'):\n                    return b''\n\n                errnum = e.args[0]\n                if is_reader and errnum in errors.socket_errors_to_ignore:\n                    return b''\n                raise socket.error(errnum)\n            except SSL.Error as e:\n                if is_reader and e.args == (-1, 'Unexpected EOF'):\n                    return b''\n\n                thirdarg = None\n                try:\n                    thirdarg = e.args[0][0][2]\n                except IndexError:\n                    pass\n\n                if thirdarg == 'http request':\n                    # The client is talking HTTP to an HTTPS server.\n                    raise errors.NoSSLError()\n\n                raise errors.FatalSSLAlert(*e.args)\n\n            if time.time() - start > self.ssl_timeout:\n                raise socket.timeout('timed out')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recv(self, size):\n        return self._safe_call(\n            True,\n            super(SSLFileobjectMixin, self).recv,\n            size,\n        )", "response": "Receive a message of a size from the socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreceive a message of a size from the socket.", "response": "def readline(self, size=-1):\n        \"\"\"Receive message of a size from the socket.\n\n        Matches the following interface:\n        https://docs.python.org/3/library/io.html#io.IOBase.readline\n        \"\"\"\n        return self._safe_call(\n            True,\n            super(SSLFileobjectMixin, self).readline,\n            size,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend a message to the socket.", "response": "def sendall(self, *args, **kwargs):\n        \"\"\"Send whole message to the socket.\"\"\"\n        return self._safe_call(\n            False,\n            super(SSLFileobjectMixin, self).sendall,\n            *args, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends some part of message to the socket.", "response": "def send(self, *args, **kwargs):\n        \"\"\"Send some part of message to the socket.\"\"\"\n        return self._safe_call(\n            False,\n            super(SSLFileobjectMixin, self).send,\n            *args, **kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind(self, sock):\n        if self.context is None:\n            self.context = self.get_context()\n        conn = SSLConnection(self.context, sock)\n        self._environ = self.get_environ()\n        return conn", "response": "Wrap and return the given socket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_context(self):\n        # See https://code.activestate.com/recipes/442473/\n        c = SSL.Context(SSL.SSLv23_METHOD)\n        c.use_privatekey_file(self.private_key)\n        if self.certificate_chain:\n            c.load_verify_locations(self.certificate_chain)\n        c.use_certificate_file(self.certificate)\n        return c", "response": "Return an SSL. Context from self attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary of WSGI environ entries to be merged into each request.", "response": "def get_environ(self):\n        \"\"\"Return WSGI environ entries to be merged into each request.\"\"\"\n        ssl_environ = {\n            'HTTPS': 'on',\n            # pyOpenSSL doesn't provide access to any of these AFAICT\n            # 'SSL_PROTOCOL': 'SSLv2',\n            # SSL_CIPHER    string  The cipher specification name\n            # SSL_VERSION_INTERFACE     string  The mod_ssl program version\n            # SSL_VERSION_LIBRARY   string  The OpenSSL program version\n        }\n\n        if self.certificate:\n            # Server certificate attributes\n            cert = open(self.certificate, 'rb').read()\n            cert = crypto.load_certificate(crypto.FILETYPE_PEM, cert)\n            ssl_environ.update({\n                'SSL_SERVER_M_VERSION': cert.get_version(),\n                'SSL_SERVER_M_SERIAL': cert.get_serial_number(),\n                # 'SSL_SERVER_V_START':\n                #   Validity of server's certificate (start time),\n                # 'SSL_SERVER_V_END':\n                #   Validity of server's certificate (end time),\n            })\n\n            for prefix, dn in [\n                ('I', cert.get_issuer()),\n                ('S', cert.get_subject()),\n            ]:\n                # X509Name objects don't seem to have a way to get the\n                # complete DN string. Use str() and slice it instead,\n                # because str(dn) == \"<X509Name object '/C=US/ST=...'>\"\n                dnstr = str(dn)[18:-2]\n\n                wsgikey = 'SSL_SERVER_%s_DN' % prefix\n                ssl_environ[wsgikey] = dnstr\n\n                # The DN should be of the form: /k1=v1/k2=v2, but we must allow\n                # for any value to contain slashes itself (in a URL).\n                while dnstr:\n                    pos = dnstr.rfind('=')\n                    dnstr, value = dnstr[:pos], dnstr[pos + 1:]\n                    pos = dnstr.rfind('/')\n                    dnstr, key = dnstr[:pos], dnstr[pos + 1:]\n                    if key and value:\n                        wsgikey = 'SSL_SERVER_%s_DN_%s' % (prefix, key)\n                        ssl_environ[wsgikey] = value\n\n        return ssl_environ"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef makefile(self, sock, mode='r', bufsize=-1):\n        cls = (\n            SSLFileobjectStreamReader\n            if 'r' in mode else\n            SSLFileobjectStreamWriter\n        )\n        if SSL and isinstance(sock, ssl_conn_type):\n            wrapped_socket = cls(sock, mode, bufsize)\n            wrapped_socket.ssl_timeout = sock.gettimeout()\n            return wrapped_socket\n        # This is from past:\n        # TODO: figure out what it's meant for\n        else:\n            return cheroot_server.CP_fileobject(sock, mode, bufsize)", "response": "Return a file object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef extract_bytes(mv):\n    if isinstance(mv, memoryview):\n        return mv.tobytes() if six.PY3 else bytes(mv)\n\n    if isinstance(mv, bytes):\n        return mv\n\n    raise ValueError", "response": "Retrieve bytes out of memoryview or bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an SSL adapter class for the given name.", "response": "def get_ssl_adapter_class(name='builtin'):\n    \"\"\"Return an SSL adapter class for the given name.\"\"\"\n    adapter = ssl_adapters[name.lower()]\n    if isinstance(adapter, six.string_types):\n        last_dot = adapter.rfind('.')\n        attr_name = adapter[last_dot + 1:]\n        mod_path = adapter[:last_dot]\n\n        try:\n            mod = sys.modules[mod_path]\n            if mod is None:\n                raise KeyError()\n        except KeyError:\n            # The last [''] is important.\n            mod = __import__(mod_path, globals(), locals(), [''])\n\n        # Let an AttributeError propagate outward.\n        try:\n            adapter = getattr(mod, attr_name)\n        except AttributeError:\n            raise AttributeError(\"'%s' object has no attribute '%s'\"\n                                 % (mod_path, attr_name))\n\n    return adapter"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a chunk from rfile and return it.", "response": "def read(self, size=None):\n        \"\"\"Read a chunk from rfile buffer and return it.\n\n        Args:\n            size (int): amount of data to read\n\n        Returns:\n            bytes: Chunk from rfile, limited by size if specified.\n\n        \"\"\"\n        data = self.rfile.read(size)\n        self.bytes_read += len(data)\n        self._check_length()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a single line from rfile buffer and return it.", "response": "def readline(self, size=None):\n        \"\"\"Read a single line from rfile buffer and return it.\n\n        Args:\n            size (int): minimum amount of data to read\n\n        Returns:\n            bytes: One line from rfile.\n\n        \"\"\"\n        if size is not None:\n            data = self.rfile.readline(size)\n            self.bytes_read += len(data)\n            self._check_length()\n            return data\n\n        # User didn't specify a size ...\n        # We read the line in chunks to make sure it's not a 100MB line !\n        res = []\n        while True:\n            data = self.rfile.readline(256)\n            self.bytes_read += len(data)\n            self._check_length()\n            res.append(data)\n            # See https://github.com/cherrypy/cherrypy/issues/421\n            if len(data) < 256 or data[-1:] == LF:\n                return EMPTY.join(res)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(self, size=None):\n        data = EMPTY\n\n        if size == 0:\n            return data\n\n        while True:\n            if size and len(data) >= size:\n                return data\n\n            if not self.buffer:\n                self._fetch()\n                if not self.buffer:\n                    # EOF\n                    return data\n\n            if size:\n                remaining = size - len(data)\n                data += self.buffer[:remaining]\n                self.buffer = self.buffer[remaining:]\n            else:\n                data += self.buffer\n                self.buffer = EMPTY", "response": "Read a chunk from the rfile buffer and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads a single line from the rfile buffer and return it.", "response": "def readline(self, size=None):\n        \"\"\"Read a single line from rfile buffer and return it.\n\n        Args:\n            size (int): minimum amount of data to read\n\n        Returns:\n            bytes: One line from rfile.\n\n        \"\"\"\n        data = EMPTY\n\n        if size == 0:\n            return data\n\n        while True:\n            if size and len(data) >= size:\n                return data\n\n            if not self.buffer:\n                self._fetch()\n                if not self.buffer:\n                    # EOF\n                    return data\n\n            newline_pos = self.buffer.find(LF)\n            if size:\n                if newline_pos == -1:\n                    remaining = size - len(data)\n                    data += self.buffer[:remaining]\n                    self.buffer = self.buffer[remaining:]\n                else:\n                    remaining = min(size - len(data), newline_pos)\n                    data += self.buffer[:remaining]\n                    self.buffer = self.buffer[remaining:]\n            else:\n                if newline_pos == -1:\n                    data += self.buffer\n                    self.buffer = EMPTY\n                else:\n                    data += self.buffer[:newline_pos]\n                    self.buffer = self.buffer[newline_pos:]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading HTTP headers and yield them.", "response": "def read_trailer_lines(self):\n        \"\"\"Read HTTP headers and yield them.\n\n        Returns:\n            Generator: yields CRLF separated lines.\n\n        \"\"\"\n        if not self.closed:\n            raise ValueError(\n                'Cannot read trailers until the request body has been read.',\n            )\n\n        while True:\n            line = self.rfile.readline()\n            if not line:\n                # No more data--illegal end of headers\n                raise ValueError('Illegal end of headers.')\n\n            self.bytes_read += len(line)\n            if self.maxlen and self.bytes_read > self.maxlen:\n                raise IOError('Request Entity Too Large')\n\n            if line == CRLF:\n                # Normal end of headers\n                break\n            if not line.endswith(CRLF):\n                raise ValueError('HTTP requires CRLF terminators')\n\n            yield line"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_request_line(self):\n        # HTTP/1.1 connections are persistent by default. If a client\n        # requests a page, then idles (leaves the connection open),\n        # then rfile.readline() will raise socket.error(\"timed out\").\n        # Note that it does this based on the value given to settimeout(),\n        # and doesn't need the client to request or acknowledge the close\n        # (although your TCP stack might suffer for it: cf Apache's history\n        # with FIN_WAIT_2).\n        request_line = self.rfile.readline()\n\n        # Set started_request to True so communicate() knows to send 408\n        # from here on out.\n        self.started_request = True\n        if not request_line:\n            return False\n\n        if request_line == CRLF:\n            # RFC 2616 sec 4.1: \"...if the server is reading the protocol\n            # stream at the beginning of a message and receives a CRLF\n            # first, it should ignore the CRLF.\"\n            # But only ignore one leading line! else we enable a DoS.\n            request_line = self.rfile.readline()\n            if not request_line:\n                return False\n\n        if not request_line.endswith(CRLF):\n            self.simple_response(\n                '400 Bad Request', 'HTTP requires CRLF terminators',\n            )\n            return False\n\n        try:\n            method, uri, req_protocol = request_line.strip().split(SPACE, 2)\n            if not req_protocol.startswith(b'HTTP/'):\n                self.simple_response(\n                    '400 Bad Request', 'Malformed Request-Line: bad protocol',\n                )\n                return False\n            rp = req_protocol[5:].split(b'.', 1)\n            rp = tuple(map(int, rp))  # Minor.Major must be threat as integers\n            if rp > (1, 1):\n                self.simple_response(\n                    '505 HTTP Version Not Supported', 'Cannot fulfill request',\n                )\n                return False\n        except (ValueError, IndexError):\n            self.simple_response('400 Bad Request', 'Malformed Request-Line')\n            return False\n\n        self.uri = uri\n        self.method = method.upper()\n\n        if self.strict_mode and method != self.method:\n            resp = (\n                'Malformed method name: According to RFC 2616 '\n                '(section 5.1.1) and its successors '\n                'RFC 7230 (section 3.1.1) and RFC 7231 (section 4.1) '\n                'method names are case-sensitive and uppercase.'\n            )\n            self.simple_response('400 Bad Request', resp)\n            return False\n\n        try:\n            if six.PY2:  # FIXME: Figure out better way to do this\n                # Ref: https://stackoverflow.com/a/196392/595220 (like this?)\n                \"\"\"This is a dummy check for unicode in URI.\"\"\"\n                ntou(bton(uri, 'ascii'), 'ascii')\n            scheme, authority, path, qs, fragment = urllib.parse.urlsplit(uri)\n        except UnicodeError:\n            self.simple_response('400 Bad Request', 'Malformed Request-URI')\n            return False\n\n        if self.method == b'OPTIONS':\n            # TODO: cover this branch with tests\n            path = (\n                uri\n                # https://tools.ietf.org/html/rfc7230#section-5.3.4\n                if self.proxy_mode or uri == ASTERISK\n                else path\n            )\n        elif self.method == b'CONNECT':\n            # TODO: cover this branch with tests\n            if not self.proxy_mode:\n                self.simple_response('405 Method Not Allowed')\n                return False\n\n            # `urlsplit()` above parses \"example.com:3128\" as path part of URI.\n            # this is a workaround, which makes it detect netloc correctly\n            uri_split = urllib.parse.urlsplit(b'//' + uri)\n            _scheme, _authority, _path, _qs, _fragment = uri_split\n            _port = EMPTY\n            try:\n                _port = uri_split.port\n            except ValueError:\n                pass\n\n            # FIXME: use third-party validation to make checks against RFC\n            # the validation doesn't take into account, that urllib parses\n            # invalid URIs without raising errors\n            # https://tools.ietf.org/html/rfc7230#section-5.3.3\n            invalid_path = (\n                _authority != uri\n                or not _port\n                or any((_scheme, _path, _qs, _fragment))\n            )\n            if invalid_path:\n                self.simple_response(\n                    '400 Bad Request',\n                    'Invalid path in Request-URI: request-'\n                    'target must match authority-form.',\n                )\n                return False\n\n            authority = path = _authority\n            scheme = qs = fragment = EMPTY\n        else:\n            uri_is_absolute_form = (scheme or authority)\n\n            disallowed_absolute = (\n                self.strict_mode\n                and not self.proxy_mode\n                and uri_is_absolute_form\n            )\n            if disallowed_absolute:\n                # https://tools.ietf.org/html/rfc7230#section-5.3.2\n                # (absolute form)\n                \"\"\"Absolute URI is only allowed within proxies.\"\"\"\n                self.simple_response(\n                    '400 Bad Request',\n                    'Absolute URI not allowed if server is not a proxy.',\n                )\n                return False\n\n            invalid_path = (\n                self.strict_mode\n                and not uri.startswith(FORWARD_SLASH)\n                and not uri_is_absolute_form\n            )\n            if invalid_path:\n                # https://tools.ietf.org/html/rfc7230#section-5.3.1\n                # (origin_form) and\n                \"\"\"Path should start with a forward slash.\"\"\"\n                resp = (\n                    'Invalid path in Request-URI: request-target must contain '\n                    'origin-form which starts with absolute-path (URI '\n                    'starting with a slash \"/\").'\n                )\n                self.simple_response('400 Bad Request', resp)\n                return False\n\n            if fragment:\n                self.simple_response(\n                    '400 Bad Request',\n                    'Illegal #fragment in Request-URI.',\n                )\n                return False\n\n            if path is None:\n                # FIXME: It looks like this case cannot happen\n                self.simple_response(\n                    '400 Bad Request',\n                    'Invalid path in Request-URI.',\n                )\n                return False\n\n            # Unquote the path+params (e.g. \"/this%20path\" -> \"/this path\").\n            # https://www.w3.org/Protocols/rfc2616/rfc2616-sec5.html#sec5.1.2\n            #\n            # But note that \"...a URI must be separated into its components\n            # before the escaped characters within those components can be\n            # safely decoded.\" https://www.ietf.org/rfc/rfc2396.txt, sec 2.4.2\n            # Therefore, \"/this%2Fpath\" becomes \"/this%2Fpath\", not\n            # \"/this/path\".\n            try:\n                # TODO: Figure out whether exception can really happen here.\n                # It looks like it's caught on urlsplit() call above.\n                atoms = [\n                    urllib.parse.unquote_to_bytes(x)\n                    for x in QUOTED_SLASH_REGEX.split(path)\n                ]\n            except ValueError as ex:\n                self.simple_response('400 Bad Request', ex.args[0])\n                return False\n            path = QUOTED_SLASH.join(atoms)\n\n        if not path.startswith(FORWARD_SLASH):\n            path = FORWARD_SLASH + path\n\n        if scheme is not EMPTY:\n            self.scheme = scheme\n        self.authority = authority\n        self.path = path\n\n        # Note that, like wsgiref and most other HTTP servers,\n        # we \"% HEX HEX\"-unquote the path but not the query string.\n        self.qs = qs\n\n        # Compare request and server HTTP protocol versions, in case our\n        # server does not support the requested protocol. Limit our output\n        # to min(req, server). We want the following output:\n        #     request    server     actual written   supported response\n        #     protocol   protocol  response protocol    feature set\n        # a     1.0        1.0           1.0                1.0\n        # b     1.0        1.1           1.1                1.0\n        # c     1.1        1.0           1.0                1.0\n        # d     1.1        1.1           1.1                1.1\n        # Notice that, in (b), the response will be \"HTTP/1.1\" even though\n        # the client only understands 1.0. RFC 2616 10.5.6 says we should\n        # only return 505 if the _major_ version is different.\n        sp = int(self.server.protocol[5]), int(self.server.protocol[7])\n\n        if sp[0] != rp[0]:\n            self.simple_response('505 HTTP Version Not Supported')\n            return False\n\n        self.request_protocol = req_protocol\n        self.response_protocol = 'HTTP/%s.%s' % min(rp, sp)\n\n        return True", "response": "Read and parse the first line of the HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the HTTP headers into self. inheaders. Return success.", "response": "def read_request_headers(self):\n        \"\"\"Read self.rfile into self.inheaders. Return success.\"\"\"\n        # then all the http headers\n        try:\n            self.header_reader(self.rfile, self.inheaders)\n        except ValueError as ex:\n            self.simple_response('400 Bad Request', ex.args[0])\n            return False\n\n        mrbs = self.server.max_request_body_size\n\n        try:\n            cl = int(self.inheaders.get(b'Content-Length', 0))\n        except ValueError:\n            self.simple_response(\n                '400 Bad Request',\n                'Malformed Content-Length Header.',\n            )\n            return False\n\n        if mrbs and cl > mrbs:\n            self.simple_response(\n                '413 Request Entity Too Large',\n                'The entity sent with the request exceeds the maximum '\n                'allowed bytes.',\n            )\n            return False\n\n        # Persistent connection support\n        if self.response_protocol == 'HTTP/1.1':\n            # Both server and client are HTTP/1.1\n            if self.inheaders.get(b'Connection', b'') == b'close':\n                self.close_connection = True\n        else:\n            # Either the server or client (or both) are HTTP/1.0\n            if self.inheaders.get(b'Connection', b'') != b'Keep-Alive':\n                self.close_connection = True\n\n        # Transfer-Encoding support\n        te = None\n        if self.response_protocol == 'HTTP/1.1':\n            te = self.inheaders.get(b'Transfer-Encoding')\n            if te:\n                te = [x.strip().lower() for x in te.split(b',') if x.strip()]\n\n        self.chunked_read = False\n\n        if te:\n            for enc in te:\n                if enc == b'chunked':\n                    self.chunked_read = True\n                else:\n                    # Note that, even if we see \"chunked\", we must reject\n                    # if there is an extension we don't recognize.\n                    self.simple_response('501 Unimplemented')\n                    self.close_connection = True\n                    return False\n\n        # From PEP 333:\n        # \"Servers and gateways that implement HTTP 1.1 must provide\n        # transparent support for HTTP 1.1's \"expect/continue\" mechanism.\n        # This may be done in any of several ways:\n        #   1. Respond to requests containing an Expect: 100-continue request\n        #      with an immediate \"100 Continue\" response, and proceed normally.\n        #   2. Proceed with the request normally, but provide the application\n        #      with a wsgi.input stream that will send the \"100 Continue\"\n        #      response if/when the application first attempts to read from\n        #      the input stream. The read request must then remain blocked\n        #      until the client responds.\n        #   3. Wait until the client decides that the server does not support\n        #      expect/continue, and sends the request body on its own.\n        #      (This is suboptimal, and is not recommended.)\n        #\n        # We used to do 3, but are now doing 1. Maybe we'll do 2 someday,\n        # but it seems like it would be a big slowdown for such a rare case.\n        if self.inheaders.get(b'Expect', b'') == b'100-continue':\n            # Don't use simple_response here, because it emits headers\n            # we don't want. See\n            # https://github.com/cherrypy/cherrypy/issues/951\n            msg = self.server.protocol.encode('ascii')\n            msg += b' 100 Continue\\r\\n\\r\\n'\n            try:\n                self.conn.wfile.write(msg)\n            except socket.error as ex:\n                if ex.args[0] not in errors.socket_errors_to_ignore:\n                    raise\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling the gateway and write its iterable output.", "response": "def respond(self):\n        \"\"\"Call the gateway and write its iterable output.\"\"\"\n        mrbs = self.server.max_request_body_size\n        if self.chunked_read:\n            self.rfile = ChunkedRFile(self.conn.rfile, mrbs)\n        else:\n            cl = int(self.inheaders.get(b'Content-Length', 0))\n            if mrbs and mrbs < cl:\n                if not self.sent_headers:\n                    self.simple_response(\n                        '413 Request Entity Too Large',\n                        'The entity sent with the request exceeds the '\n                        'maximum allowed bytes.',\n                    )\n                return\n            self.rfile = KnownLengthRFile(self.conn.rfile, cl)\n\n        self.server.gateway(self).respond()\n        self.ready and self.ensure_headers_sent()\n\n        if self.chunked_write:\n            self.conn.wfile.write(b'0\\r\\n\\r\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a simple response back to the client.", "response": "def simple_response(self, status, msg=''):\n        \"\"\"Write a simple response back to the client.\"\"\"\n        status = str(status)\n        proto_status = '%s %s\\r\\n' % (self.server.protocol, status)\n        content_length = 'Content-Length: %s\\r\\n' % len(msg)\n        content_type = 'Content-Type: text/plain\\r\\n'\n        buf = [\n            proto_status.encode('ISO-8859-1'),\n            content_length.encode('ISO-8859-1'),\n            content_type.encode('ISO-8859-1'),\n        ]\n\n        if status[:3] in ('413', '414'):\n            # Request Entity Too Large / Request-URI Too Long\n            self.close_connection = True\n            if self.response_protocol == 'HTTP/1.1':\n                # This will not be true for 414, since read_request_line\n                # usually raises 414 before reading the whole line, and we\n                # therefore cannot know the proper response_protocol.\n                buf.append(b'Connection: close\\r\\n')\n            else:\n                # HTTP/1.0 had no 413/414 status nor Connection header.\n                # Emit 400 instead and trust the message body is enough.\n                status = '400 Bad Request'\n\n        buf.append(CRLF)\n        if msg:\n            if isinstance(msg, six.text_type):\n                msg = msg.encode('ISO-8859-1')\n            buf.append(msg)\n\n        try:\n            self.conn.wfile.write(EMPTY.join(buf))\n        except socket.error as ex:\n            if ex.args[0] not in errors.socket_errors_to_ignore:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite unbuffered data to the client.", "response": "def write(self, chunk):\n        \"\"\"Write unbuffered data to the client.\"\"\"\n        if self.chunked_write and chunk:\n            chunk_size_hex = hex(len(chunk))[2:].encode('ascii')\n            buf = [chunk_size_hex, CRLF, chunk, CRLF]\n            self.conn.wfile.write(EMPTY.join(buf))\n        else:\n            self.conn.wfile.write(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_headers(self):\n        hkeys = [key.lower() for key, value in self.outheaders]\n        status = int(self.status[:3])\n\n        if status == 413:\n            # Request Entity Too Large. Close conn to avoid garbage.\n            self.close_connection = True\n        elif b'content-length' not in hkeys:\n            # \"All 1xx (informational), 204 (no content),\n            # and 304 (not modified) responses MUST NOT\n            # include a message-body.\" So no point chunking.\n            if status < 200 or status in (204, 205, 304):\n                pass\n            else:\n                needs_chunked = (\n                    self.response_protocol == 'HTTP/1.1'\n                    and self.method != b'HEAD'\n                )\n                if needs_chunked:\n                    # Use the chunked transfer-coding\n                    self.chunked_write = True\n                    self.outheaders.append((b'Transfer-Encoding', b'chunked'))\n                else:\n                    # Closing the conn is the only way to determine len.\n                    self.close_connection = True\n\n        if b'connection' not in hkeys:\n            if self.response_protocol == 'HTTP/1.1':\n                # Both server and client are HTTP/1.1 or better\n                if self.close_connection:\n                    self.outheaders.append((b'Connection', b'close'))\n            else:\n                # Server and/or client are HTTP/1.0\n                if not self.close_connection:\n                    self.outheaders.append((b'Connection', b'Keep-Alive'))\n\n        if (not self.close_connection) and (not self.chunked_read):\n            # Read any remaining request body data on the socket.\n            # \"If an origin server receives a request that does not include an\n            # Expect request-header field with the \"100-continue\" expectation,\n            # the request includes a request body, and the server responds\n            # with a final status code before reading the entire request body\n            # from the transport connection, then the server SHOULD NOT close\n            # the transport connection until it has read the entire request,\n            # or until the client closes the connection. Otherwise, the client\n            # might not reliably receive the response message. However, this\n            # requirement is not be construed as preventing a server from\n            # defending itself against denial-of-service attacks, or from\n            # badly broken client implementations.\"\n            remaining = getattr(self.rfile, 'remaining', 0)\n            if remaining > 0:\n                self.rfile.read(remaining)\n\n        if b'date' not in hkeys:\n            self.outheaders.append((\n                b'Date',\n                email.utils.formatdate(usegmt=True).encode('ISO-8859-1'),\n            ))\n\n        if b'server' not in hkeys:\n            self.outheaders.append((\n                b'Server',\n                self.server.server_name.encode('ISO-8859-1'),\n            ))\n\n        proto = self.server.protocol.encode('ascii')\n        buf = [proto + SPACE + self.status + CRLF]\n        for k, v in self.outheaders:\n            buf.append(k + COLON + SPACE + v + CRLF)\n        buf.append(CRLF)\n        self.conn.wfile.write(EMPTY.join(buf))", "response": "Assert process and send the HTTP response message - headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading each request and respond appropriately.", "response": "def communicate(self):\n        \"\"\"Read each request and respond appropriately.\"\"\"\n        request_seen = False\n        try:\n            while True:\n                # (re)set req to None so that if something goes wrong in\n                # the RequestHandlerClass constructor, the error doesn't\n                # get written to the previous request.\n                req = None\n                req = self.RequestHandlerClass(self.server, self)\n\n                # This order of operations should guarantee correct pipelining.\n                req.parse_request()\n                if self.server.stats['Enabled']:\n                    self.requests_seen += 1\n                if not req.ready:\n                    # Something went wrong in the parsing (and the server has\n                    # probably already made a simple_response). Return and\n                    # let the conn close.\n                    return\n\n                request_seen = True\n                req.respond()\n                if req.close_connection:\n                    return\n        except socket.error as ex:\n            errnum = ex.args[0]\n            # sadly SSL sockets return a different (longer) time out string\n            timeout_errs = 'timed out', 'The read operation timed out'\n            if errnum in timeout_errs:\n                # Don't error if we're between requests; only error\n                # if 1) no request has been started at all, or 2) we're\n                # in the middle of a request.\n                # See https://github.com/cherrypy/cherrypy/issues/853\n                if (not request_seen) or (req and req.started_request):\n                    self._conditional_error(req, '408 Request Timeout')\n            elif errnum not in errors.socket_errors_to_ignore:\n                self.server.error_log(\n                    'socket.error %s' % repr(errnum),\n                    level=logging.WARNING, traceback=True,\n                )\n                self._conditional_error(req, '500 Internal Server Error')\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except errors.FatalSSLAlert:\n            pass\n        except errors.NoSSLError:\n            self._handle_no_ssl(req)\n        except Exception as ex:\n            self.server.error_log(\n                repr(ex), level=logging.ERROR, traceback=True,\n            )\n            self._conditional_error(req, '500 Internal Server Error')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresponds with an error.", "response": "def _conditional_error(self, req, response):\n        \"\"\"Respond with an error.\n\n        Don't bother writing if a response\n        has already started being written.\n        \"\"\"\n        if not req or req.sent_headers:\n            return\n\n        try:\n            req.simple_response(response)\n        except errors.FatalSSLAlert:\n            pass\n        except errors.NoSSLError:\n            self._handle_no_ssl(req)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nclose the socket underlying this connection.", "response": "def close(self):\n        \"\"\"Close the socket underlying this connection.\"\"\"\n        self.rfile.close()\n\n        if not self.linger:\n            self._close_kernel_socket()\n            self.socket.close()\n        else:\n            # On the other hand, sometimes we want to hang around for a bit\n            # to make sure the client has a chance to read our entire\n            # response. Skipping the close() calls here delays the FIN\n            # packet until the socket object is garbage-collected later.\n            # Someday, perhaps, we'll do the full lingering_close that\n            # Apache does, but not today.\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_peer_creds(self):  # LRU cached on per-instance basis, see __init__\n        PEERCRED_STRUCT_DEF = '3i'\n\n        if IS_WINDOWS or self.socket.family != socket.AF_UNIX:\n            raise NotImplementedError(\n                'SO_PEERCRED is only supported in Linux kernel and WSL',\n            )\n        elif not self.peercreds_enabled:\n            raise RuntimeError(\n                'Peer creds lookup is disabled within this server',\n            )\n\n        try:\n            peer_creds = self.socket.getsockopt(\n                # FIXME: Use LOCAL_CREDS for BSD-like OSs\n                # Ref: https://gist.github.com/LucaFilipozzi/e4f1e118202aff27af6aadebda1b5d91  # noqa\n                socket.SOL_SOCKET, socket.SO_PEERCRED,\n                struct.calcsize(PEERCRED_STRUCT_DEF),\n            )\n        except socket.error as socket_err:\n            \"\"\"Non-Linux kernels don't support SO_PEERCRED.\n\n            Refs:\n            http://welz.org.za/notes/on-peer-cred.html\n            https://github.com/daveti/tcpSockHack\n            msdn.microsoft.com/en-us/commandline/wsl/release_notes#build-15025\n            \"\"\"\n            six.raise_from(  # 3.6+: raise RuntimeError from socket_err\n                RuntimeError,\n                socket_err,\n            )\n        else:\n            pid, uid, gid = struct.unpack(PEERCRED_STRUCT_DEF, peer_creds)\n            return pid, uid, gid", "response": "Returns the PID UID and GID tuple of the peer socket."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_peer_creds(self):  # LRU cached on per-instance basis\n        if not IS_UID_GID_RESOLVABLE:\n            raise NotImplementedError(\n                'UID/GID lookup is unavailable under current platform. '\n                'It can only be done under UNIX-like OS '\n                'but not under the Google App Engine',\n            )\n        elif not self.peercreds_resolve_enabled:\n            raise RuntimeError(\n                'UID/GID lookup is disabled within this server',\n            )\n\n        user = pwd.getpwuid(self.peer_uid).pw_name  # [0]\n        group = grp.getgrgid(self.peer_gid).gr_name  # [0]\n\n        return user, group", "response": "Return the username and group tuple of the peercreds if available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _close_kernel_socket(self):\n        if six.PY2 and hasattr(self.socket, '_sock'):\n            self.socket._sock.close()", "response": "Close kernel socket in older Python versions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clear_stats(self):\n        self._start_time = None\n        self._run_time = 0\n        self.stats = {\n            'Enabled': False,\n            'Bind Address': lambda s: repr(self.bind_addr),\n            'Run time': lambda s: (not s['Enabled']) and -1 or self.runtime(),\n            'Accepts': 0,\n            'Accepts/sec': lambda s: s['Accepts'] / self.runtime(),\n            'Queue': lambda s: getattr(self.requests, 'qsize', None),\n            'Threads': lambda s: len(getattr(self.requests, '_threads', [])),\n            'Threads Idle': lambda s: getattr(self.requests, 'idle', None),\n            'Socket Errors': 0,\n            'Requests': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Requests'](w) for w in s['Worker Threads'].values()], 0,\n            ),\n            'Bytes Read': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Bytes Read'](w) for w in s['Worker Threads'].values()], 0,\n            ),\n            'Bytes Written': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Bytes Written'](w) for w in s['Worker Threads'].values()],\n                0,\n            ),\n            'Work Time': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Work Time'](w) for w in s['Worker Threads'].values()], 0,\n            ),\n            'Read Throughput': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Bytes Read'](w) / (w['Work Time'](w) or 1e-6)\n                 for w in s['Worker Threads'].values()], 0,\n            ),\n            'Write Throughput': lambda s: (not s['Enabled']) and -1 or sum(\n                [w['Bytes Written'](w) / (w['Work Time'](w) or 1e-6)\n                 for w in s['Worker Threads'].values()], 0,\n            ),\n            'Worker Threads': {},\n        }\n        logging.statistics['Cheroot HTTPServer %d' % id(self)] = self.stats", "response": "Clear server stat counters.."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bind_addr(self, value):\n        if isinstance(value, tuple) and value[0] in ('', None):\n            # Despite the socket module docs, using '' does not\n            # allow AI_PASSIVE to work. Passing None instead\n            # returns '0.0.0.0' like we want. In other words:\n            #     host    AI_PASSIVE     result\n            #      ''         Y         192.168.x.y\n            #      ''         N         192.168.x.y\n            #     None        Y         0.0.0.0\n            #     None        N         127.0.0.1\n            # But since you can get the same effect with an explicit\n            # '0.0.0.0', we deny both the empty string and None as values.\n            raise ValueError(\n                \"Host values of '' or None are not allowed. \"\n                \"Use '0.0.0.0' (IPv4) or '::' (IPv6) instead \"\n                'to listen on all active interfaces.',\n            )\n        self._bind_addr = value", "response": "Sets the bind address of the active class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the server forever and stop it cleanly on exit.", "response": "def safe_start(self):\n        \"\"\"Run the server forever, and stop it cleanly on exit.\"\"\"\n        try:\n            self.start()\n        except (KeyboardInterrupt, IOError):\n            # The time.sleep call might raise\n            # \"IOError: [Errno 4] Interrupted function call\" on KBInt.\n            self.error_log('Keyboard Interrupt: shutting down')\n            self.stop()\n            raise\n        except SystemExit:\n            self.error_log('SystemExit raised: shutting down')\n            self.stop()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(self):\n        self._interrupt = None\n\n        if self.software is None:\n            self.software = '%s Server' % self.version\n\n        # Select the appropriate socket\n        self.socket = None\n        msg = 'No socket could be created'\n        if os.getenv('LISTEN_PID', None):\n            # systemd socket activation\n            self.socket = socket.fromfd(3, socket.AF_INET, socket.SOCK_STREAM)\n        elif isinstance(self.bind_addr, six.string_types):\n            # AF_UNIX socket\n            try:\n                self.bind_unix_socket(self.bind_addr)\n            except socket.error as serr:\n                msg = '%s -- (%s: %s)' % (msg, self.bind_addr, serr)\n                six.raise_from(socket.error(msg), serr)\n        else:\n            # AF_INET or AF_INET6 socket\n            # Get the correct address family for our host (allows IPv6\n            # addresses)\n            host, port = self.bind_addr\n            try:\n                info = socket.getaddrinfo(\n                    host, port, socket.AF_UNSPEC,\n                    socket.SOCK_STREAM, 0, socket.AI_PASSIVE,\n                )\n            except socket.gaierror:\n                sock_type = socket.AF_INET\n                bind_addr = self.bind_addr\n\n                if ':' in host:\n                    sock_type = socket.AF_INET6\n                    bind_addr = bind_addr + (0, 0)\n\n                info = [(sock_type, socket.SOCK_STREAM, 0, '', bind_addr)]\n\n            for res in info:\n                af, socktype, proto, canonname, sa = res\n                try:\n                    self.bind(af, socktype, proto)\n                    break\n                except socket.error as serr:\n                    msg = '%s -- (%s: %s)' % (msg, sa, serr)\n                    if self.socket:\n                        self.socket.close()\n                    self.socket = None\n\n        if not self.socket:\n            raise socket.error(msg)\n\n        # Timeout so KeyboardInterrupt can be caught on Win32\n        self.socket.settimeout(1)\n        self.socket.listen(self.request_queue_size)\n\n        # Create worker threads\n        self.requests.start()\n\n        self.ready = True\n        self._start_time = time.time()", "response": "Prepare server to serving requests."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserves requests after invoking prepare.", "response": "def serve(self):\n        \"\"\"Serve requests, after invoking :func:`prepare()`.\"\"\"\n        while self.ready:\n            try:\n                self.tick()\n            except (KeyboardInterrupt, SystemExit):\n                raise\n            except Exception:\n                self.error_log(\n                    'Error in HTTPServer.tick', level=logging.ERROR,\n                    traceback=True,\n                )\n\n            if self.interrupt:\n                while self.interrupt is True:\n                    # Wait for self.stop() to complete. See _set_interrupt.\n                    time.sleep(0.1)\n                if self.interrupt:\n                    raise self.interrupt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef error_log(self, msg='', level=20, traceback=False):\n        # Override this in subclasses as desired\n        sys.stderr.write(msg + '\\n')\n        sys.stderr.flush()\n        if traceback:\n            tblines = traceback_.format_exc()\n            sys.stderr.write(tblines)\n            sys.stderr.flush()", "response": "Write error message to log."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bind(self, family, type, proto=0):\n        sock = self.prepare_socket(\n            self.bind_addr,\n            family, type, proto,\n            self.nodelay, self.ssl_adapter,\n        )\n        sock = self.socket = self.bind_socket(sock, self.bind_addr)\n        self.bind_addr = self.resolve_real_bind_addr(sock)\n        return sock", "response": "Create or recreate the actual socket object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bind_unix_socket(self, bind_addr):\n        if IS_WINDOWS:\n            \"\"\"\n            Trying to access socket.AF_UNIX under Windows\n            causes an AttributeError.\n            \"\"\"\n            raise ValueError(  # or RuntimeError?\n                'AF_UNIX sockets are not supported under Windows.',\n            )\n\n        fs_permissions = 0o777  # TODO: allow changing mode\n\n        try:\n            # Make possible reusing the socket...\n            os.unlink(self.bind_addr)\n        except OSError:\n            \"\"\"\n            File does not exist, which is the primary goal anyway.\n            \"\"\"\n\n        sock = self.prepare_socket(\n            bind_addr=bind_addr,\n            family=socket.AF_UNIX, type=socket.SOCK_STREAM, proto=0,\n            nodelay=self.nodelay, ssl_adapter=self.ssl_adapter,\n        )\n\n        try:\n            \"\"\"Linux way of pre-populating fs mode permissions.\"\"\"\n            # Allow everyone access the socket...\n            os.fchmod(sock.fileno(), fs_permissions)\n            FS_PERMS_SET = True\n        except OSError:\n            FS_PERMS_SET = False\n\n        try:\n            sock = self.bind_socket(sock, bind_addr)\n        except socket.error:\n            sock.close()\n            raise\n\n        bind_addr = self.resolve_real_bind_addr(sock)\n\n        try:\n            \"\"\"FreeBSD/macOS pre-populating fs mode permissions.\"\"\"\n            if not FS_PERMS_SET:\n                try:\n                    os.lchmod(bind_addr, fs_permissions)\n                except AttributeError:\n                    os.chmod(bind_addr, fs_permissions, follow_symlinks=False)\n                FS_PERMS_SET = True\n        except OSError:\n            pass\n\n        if not FS_PERMS_SET:\n            self.error_log(\n                'Failed to set socket fs mode permissions',\n                level=logging.WARNING,\n            )\n\n        self.bind_addr = bind_addr\n        self.socket = sock\n        return sock", "response": "Bind a UNIX socket to the given bind address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_socket(bind_addr, family, type, proto, nodelay, ssl_adapter):\n        sock = socket.socket(family, type, proto)\n        prevent_socket_inheritance(sock)\n\n        host, port = bind_addr[:2]\n        IS_EPHEMERAL_PORT = port == 0\n\n        if not (IS_WINDOWS or IS_EPHEMERAL_PORT):\n            \"\"\"Enable SO_REUSEADDR for the current socket.\n\n            Skip for Windows (has different semantics)\n            or ephemeral ports (can steal ports from others).\n\n            Refs:\n            * https://msdn.microsoft.com/en-us/library/ms740621(v=vs.85).aspx\n            * https://github.com/cherrypy/cheroot/issues/114\n            * https://gavv.github.io/blog/ephemeral-port-reuse/\n            \"\"\"\n            sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        if nodelay and not isinstance(bind_addr, str):\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n        if ssl_adapter is not None:\n            sock = ssl_adapter.bind(sock)\n\n        # If listening on the IPV6 any address ('::' = IN6ADDR_ANY),\n        # activate dual-stack. See\n        # https://github.com/cherrypy/cherrypy/issues/871.\n        listening_ipv6 = (\n            hasattr(socket, 'AF_INET6')\n            and family == socket.AF_INET6\n            and host in ('::', '::0', '::0.0.0.0')\n        )\n        if listening_ipv6:\n            try:\n                sock.setsockopt(\n                    socket.IPPROTO_IPV6, socket.IPV6_V6ONLY, 0,\n                )\n            except (AttributeError, socket.error):\n                # Apparently, the socket option is not available in\n                # this machine's TCP stack\n                pass\n\n        return sock", "response": "Create and prepare the socket object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves actual bind addr from bound socket.", "response": "def resolve_real_bind_addr(socket_):\n        \"\"\"Retrieve actual bind addr from bound socket.\"\"\"\n        # FIXME: keep requested bind_addr separate real bound_addr (port\n        # is different in case of ephemeral port 0)\n        bind_addr = socket_.getsockname()\n        if socket_.family in (\n            # Windows doesn't have socket.AF_UNIX, so not using it in check\n            socket.AF_INET,\n            socket.AF_INET6,\n        ):\n            \"\"\"UNIX domain sockets are strings or bytes.\n\n            In case of bytes with a leading null-byte it's an abstract socket.\n            \"\"\"\n            return bind_addr[:2]\n        return bind_addr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tick(self):\n        try:\n            s, addr = self.socket.accept()\n            if self.stats['Enabled']:\n                self.stats['Accepts'] += 1\n            if not self.ready:\n                return\n\n            prevent_socket_inheritance(s)\n            if hasattr(s, 'settimeout'):\n                s.settimeout(self.timeout)\n\n            mf = MakeFile\n            ssl_env = {}\n            # if ssl cert and key are set, we try to be a secure HTTP server\n            if self.ssl_adapter is not None:\n                try:\n                    s, ssl_env = self.ssl_adapter.wrap(s)\n                except errors.NoSSLError:\n                    msg = (\n                        'The client sent a plain HTTP request, but '\n                        'this server only speaks HTTPS on this port.'\n                    )\n                    buf = [\n                        '%s 400 Bad Request\\r\\n' % self.protocol,\n                        'Content-Length: %s\\r\\n' % len(msg),\n                        'Content-Type: text/plain\\r\\n\\r\\n',\n                        msg,\n                    ]\n\n                    sock_to_make = s if six.PY3 else s._sock\n                    wfile = mf(sock_to_make, 'wb', io.DEFAULT_BUFFER_SIZE)\n                    try:\n                        wfile.write(''.join(buf).encode('ISO-8859-1'))\n                    except socket.error as ex:\n                        if ex.args[0] not in errors.socket_errors_to_ignore:\n                            raise\n                    return\n                if not s:\n                    return\n                mf = self.ssl_adapter.makefile\n                # Re-apply our timeout since we may have a new socket object\n                if hasattr(s, 'settimeout'):\n                    s.settimeout(self.timeout)\n\n            conn = self.ConnectionClass(self, s, mf)\n\n            if not isinstance(self.bind_addr, six.string_types):\n                # optional values\n                # Until we do DNS lookups, omit REMOTE_HOST\n                if addr is None:  # sometimes this can happen\n                    # figure out if AF_INET or AF_INET6.\n                    if len(s.getsockname()) == 2:\n                        # AF_INET\n                        addr = ('0.0.0.0', 0)\n                    else:\n                        # AF_INET6\n                        addr = ('::', 0)\n                conn.remote_addr = addr[0]\n                conn.remote_port = addr[1]\n\n            conn.ssl_env = ssl_env\n\n            try:\n                self.requests.put(conn)\n            except queue.Full:\n                # Just drop the conn. TODO: write 503 back?\n                conn.close()\n                return\n        except socket.timeout:\n            # The only reason for the timeout in start() is so we can\n            # notice keyboard interrupts on Win32, which don't interrupt\n            # accept() by default\n            return\n        except socket.error as ex:\n            if self.stats['Enabled']:\n                self.stats['Socket Errors'] += 1\n            if ex.args[0] in errors.socket_error_eintr:\n                # I *think* this is right. EINTR should occur when a signal\n                # is received during the accept() call; all docs say retry\n                # the call, and I *think* I'm reading it right that Python\n                # will then go ahead and poll for and handle the signal\n                # elsewhere. See\n                # https://github.com/cherrypy/cherrypy/issues/707.\n                return\n            if ex.args[0] in errors.socket_errors_nonblocking:\n                # Just try again. See\n                # https://github.com/cherrypy/cherrypy/issues/479.\n                return\n            if ex.args[0] in errors.socket_errors_to_ignore:\n                # Our socket was closed.\n                # See https://github.com/cherrypy/cherrypy/issues/686.\n                return\n            raise", "response": "This method is called by the thread that is responsible for accepting a new connection and put it on the Queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform the shutdown of this server and save the exception.", "response": "def interrupt(self, interrupt):\n        \"\"\"Perform the shutdown of this server and save the exception.\"\"\"\n        self._interrupt = True\n        self.stop()\n        self._interrupt = interrupt"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses incoming HTTP connections.", "response": "def run(self):\n        \"\"\"Process incoming HTTP connections.\n\n        Retrieves incoming connections from thread pool.\n        \"\"\"\n        self.server.stats['Worker Threads'][self.getName()] = self.stats\n        try:\n            self.ready = True\n            while True:\n                conn = self.server.requests.get()\n                if conn is _SHUTDOWNREQUEST:\n                    return\n\n                self.conn = conn\n                if self.server.stats['Enabled']:\n                    self.start_time = time.time()\n                try:\n                    conn.communicate()\n                finally:\n                    conn.close()\n                    if self.server.stats['Enabled']:\n                        self.requests_seen += self.conn.requests_seen\n                        self.bytes_read += self.conn.rfile.bytes_read\n                        self.bytes_written += self.conn.wfile.bytes_written\n                        self.work_time += time.time() - self.start_time\n                        self.start_time = None\n                    self.conn = None\n        except (KeyboardInterrupt, SystemExit) as ex:\n            self.server.interrupt = ex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nputting a request into the queue.", "response": "def put(self, obj):\n        \"\"\"Put request into queue.\n\n        Args:\n            obj (cheroot.server.HTTPConnection): HTTP connection\n                waiting to be processed\n        \"\"\"\n        self._queue.put(obj, block=True, timeout=self._queue_put_timeout)\n        if obj is _SHUTDOWNREQUEST:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef grow(self, amount):\n        if self.max > 0:\n            budget = max(self.max - len(self._threads), 0)\n        else:\n            # self.max <= 0 indicates no maximum\n            budget = float('inf')\n\n        n_new = min(amount, budget)\n\n        workers = [self._spawn_worker() for i in range(n_new)]\n        while not all(worker.ready for worker in workers):\n            time.sleep(.1)\n        self._threads.extend(workers)", "response": "Grow the pool by amount threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shrink(self, amount):\n        # Grow/shrink the pool if necessary.\n        # Remove any dead threads from our list\n        for t in self._threads:\n            if not t.isAlive():\n                self._threads.remove(t)\n                amount -= 1\n\n        # calculate the number of threads above the minimum\n        n_extra = max(len(self._threads) - self.min, 0)\n\n        # don't remove more than amount\n        n_to_remove = min(amount, n_extra)\n\n        # put shutdown requests on the queue equal to the number of threads\n        # to remove. As each request is processed by a worker, that worker\n        # will terminate and be culled from the list.\n        for n in range(n_to_remove):\n            self._queue.put(_SHUTDOWNREQUEST)", "response": "Kill off worker threads and shrink the pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nterminates all worker threads.", "response": "def stop(self, timeout=5):\n        \"\"\"Terminate all worker threads.\n\n        Args:\n            timeout (int): time to wait for threads to stop gracefully\n        \"\"\"\n        # Must shut down threads here so the code that calls\n        # this method can know when all threads are stopped.\n        for worker in self._threads:\n            self._queue.put(_SHUTDOWNREQUEST)\n\n        # Don't join currentThread (when stop is called inside a request).\n        current = threading.currentThread()\n        if timeout is not None and timeout >= 0:\n            endtime = time.time() + timeout\n        while self._threads:\n            worker = self._threads.pop()\n            if worker is not current and worker.isAlive():\n                try:\n                    if timeout is None or timeout < 0:\n                        worker.join()\n                    else:\n                        remaining_time = endtime - time.time()\n                        if remaining_time > 0:\n                            worker.join(remaining_time)\n                        if worker.isAlive():\n                            # We exhausted the timeout.\n                            # Forcibly shut down the socket.\n                            c = worker.conn\n                            if c and not c.rfile.closed:\n                                try:\n                                    c.socket.shutdown(socket.SHUT_RD)\n                                except TypeError:\n                                    # pyOpenSSL sockets don't take an arg\n                                    c.socket.shutdown()\n                            worker.join()\n                except (\n                    AssertionError,\n                    # Ignore repeated Ctrl-C.\n                    # See\n                    # https://github.com/cherrypy/cherrypy/issues/691.\n                    KeyboardInterrupt,\n                ):\n                    pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_last_child_with_lineno(node):\n    ignored_fields = set(['ctx', 'decorator_list', 'names', 'returns'])\n    fields = node._fields\n    # The fields of ast.Call are in the wrong order.\n    if isinstance(node, ast.Call):\n        fields = ('func', 'args', 'starargs', 'keywords', 'kwargs')\n    for name in reversed(fields):\n        if name in ignored_fields:\n            continue\n\n        try:\n            last_field = getattr(node, name)\n        except AttributeError:\n            continue\n\n        # Ignore non-AST objects like \"is_async\", \"level\" and \"nl\".\n        if isinstance(last_field, ast.AST):\n            return last_field\n        elif isinstance(last_field, list) and last_field:\n            return last_field[-1]\n    return None", "response": "Return the last direct child of node that has a lineno attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_last_line_number(node):\n    max_lineno = node.lineno\n    while True:\n        last_child = _get_last_child_with_lineno(node)\n        if last_child is None:\n            return max_lineno\n        else:\n            try:\n                max_lineno = max(max_lineno, last_child.lineno)\n            except AttributeError:\n                pass\n        node = last_child", "response": "Estimate the last line number of the given AST node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_modules(paths, toplevel=True):\n    modules = []\n    for path in paths:\n        path = os.path.abspath(path)\n        if toplevel and path.endswith('.pyc'):\n            sys.exit('.pyc files are not supported: {0}'.format(path))\n        if os.path.isfile(path) and (path.endswith('.py') or toplevel):\n            modules.append(path)\n        elif os.path.isdir(path):\n            subpaths = [\n                os.path.join(path, filename)\n                for filename in sorted(os.listdir(path))]\n            modules.extend(get_modules(subpaths, toplevel=False))\n        elif toplevel:\n            sys.exit('Error: {0} could not be found.'.format(path))\n    return modules", "response": "Take files from the command line even if they don t end with. py."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ignore_variable(filename, varname):\n    return (\n        varname in IGNORED_VARIABLE_NAMES or\n        (varname.startswith('_') and not varname.startswith('__')) or\n        _is_special_name(varname))", "response": "Return True if the variable is ignored."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_unused_code(self, min_confidence=0, sort_by_size=False):\n        if not 0 <= min_confidence <= 100:\n            raise ValueError('min_confidence must be between 0 and 100.')\n\n        def by_name(item):\n            return (item.filename.lower(), item.first_lineno)\n\n        def by_size(item):\n            return (item.size,) + by_name(item)\n\n        unused_code = (self.unused_attrs + self.unused_classes +\n                       self.unused_funcs + self.unused_imports +\n                       self.unused_props + self.unused_vars +\n                       self.unreachable_code)\n\n        confidently_unused = [obj for obj in unused_code\n                              if obj.confidence >= min_confidence]\n\n        return sorted(confidently_unused,\n                      key=by_size if sort_by_size else by_name)", "response": "Return a list of unused code items."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report(self, min_confidence=0, sort_by_size=False,\n               make_whitelist=False):\n        \"\"\"\n        Print ordered list of Item objects to stdout.\n        \"\"\"\n        for item in self.get_unused_code(\n                min_confidence=min_confidence, sort_by_size=sort_by_size):\n            print(item.get_whitelist_string() if make_whitelist\n                  else item.get_report(add_size=sort_by_size))\n            self.found_dead_code_or_error = True\n        return self.found_dead_code_or_error", "response": "Print ordered list of Item objects to stdout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_aliases(self, node):\n        assert isinstance(node, (ast.Import, ast.ImportFrom))\n        for name_and_alias in node.names:\n            # Store only top-level module name (\"os.path\" -> \"os\").\n            # We can't easily detect when \"os.path\" is used.\n            name = name_and_alias.name.partition('.')[0]\n            alias = name_and_alias.asname\n            self._define(\n                self.defined_imports, alias or name, node,\n                confidence=90, ignore=_ignore_import)\n            if alias is not None:\n                self.used_names.add(name_and_alias.name)", "response": "Add aliases to the set of used names."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visit_Str(self, node):\n        # Old format strings.\n        self.used_names |= set(re.findall(r'\\%\\((\\w+)\\)', node.s))\n\n        def is_identifier(s):\n            return bool(re.match(r'[a-zA-Z_][a-zA-Z0-9_]*', s))\n\n        # New format strings.\n        parser = string.Formatter()\n        try:\n            names = [name for _, name, _, _ in parser.parse(node.s) if name]\n        except ValueError:\n            # Invalid format string.\n            names = []\n\n        for field_name in names:\n            # Remove brackets and contents: \"a[0][b].c[d].e\" -> \"a.c.e\".\n            # \"a.b.c\" -> name = \"a\", attributes = [\"b\", \"c\"]\n            name_and_attrs = re.sub(r'\\[\\w*\\]', '', field_name).split('.')\n            name = name_and_attrs[0]\n            if is_identifier(name):\n                self.used_names.add(name)\n            for attr in name_and_attrs[1:]:\n                if is_identifier(attr):\n                    self.used_attrs.add(attr)", "response": "Parse variable names in format strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_ast_list(self, ast_list):\n        for index, node in enumerate(ast_list):\n            if isinstance(node, (ast.Break, ast.Continue, ast.Raise,\n                                 ast.Return)):\n                try:\n                    first_unreachable_node = ast_list[index + 1]\n                except IndexError:\n                    continue\n                class_name = node.__class__.__name__.lower()\n                self._define(\n                    self.unreachable_code,\n                    class_name,\n                    first_unreachable_node,\n                    last_node=ast_list[-1],\n                    message=\"unreachable code after '{class_name}'\".format(\n                        **locals()),\n                    confidence=100)\n                return", "response": "Handles the ast_list of unreachable nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generic_visit(self, node):\n        for _, value in ast.iter_fields(node):\n            if isinstance(value, list):\n                self._handle_ast_list(value)\n                for item in value:\n                    if isinstance(item, ast.AST):\n                        self.visit(item)\n            elif isinstance(value, ast.AST):\n                self.visit(value)", "response": "Called by the visitor function when no explicit visitor function exists for a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_occurrences(project, resource, offset, unsure=False, resources=None,\n                     in_hierarchy=False,\n                     task_handle=taskhandle.NullTaskHandle()):\n    \"\"\"Return a list of `Location`\\s\n\n    If `unsure` is `True`, possible matches are returned, too.  You\n    can use `Location.unsure` to see which are unsure occurrences.\n    `resources` can be a list of `rope.base.resource.File`\\s that\n    should be searched for occurrences; if `None` all python files\n    in the project are searched.\n\n    \"\"\"\n    name = worder.get_name_at(resource, offset)\n    this_pymodule = project.get_pymodule(resource)\n    primary, pyname = rope.base.evaluate.eval_location2(\n        this_pymodule, offset)\n\n    def is_match(occurrence):\n        return unsure\n    finder = occurrences.create_finder(\n        project, name, pyname, unsure=is_match,\n        in_hierarchy=in_hierarchy, instance=primary)\n    if resources is None:\n        resources = project.get_python_files()\n    job_set = task_handle.create_jobset('Finding Occurrences',\n                                        count=len(resources))\n    return _find_locations(finder, resources, job_set)", "response": "Return a list of Location objects that are found at the specified offset in the specified project."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_implementations(project, resource, offset, resources=None,\n                         task_handle=taskhandle.NullTaskHandle()):\n    \"\"\"Find the places a given method is overridden.\n\n    Finds the places a method is implemented.  Returns a list of\n    `Location`\\s.\n    \"\"\"\n    name = worder.get_name_at(resource, offset)\n    this_pymodule = project.get_pymodule(resource)\n    pyname = rope.base.evaluate.eval_location(this_pymodule, offset)\n    if pyname is not None:\n        pyobject = pyname.get_object()\n        if not isinstance(pyobject, rope.base.pyobjects.PyFunction) or \\\n           pyobject.get_kind() != 'method':\n            raise exceptions.BadIdentifierError('Not a method!')\n    else:\n        raise exceptions.BadIdentifierError('Cannot resolve the identifier!')\n\n    def is_defined(occurrence):\n        if not occurrence.is_defined():\n            return False\n\n    def not_self(occurrence):\n        if occurrence.get_pyname().get_object() == pyname.get_object():\n            return False\n    filters = [is_defined, not_self,\n               occurrences.InHierarchyFilter(pyname, True)]\n    finder = occurrences.Finder(project, name, filters=filters)\n    if resources is None:\n        resources = project.get_python_files()\n    job_set = task_handle.create_jobset('Finding Implementations',\n                                        count=len(resources))\n    return _find_locations(finder, resources, job_set)", "response": "Find the places a given method is overridden."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_definition(project, code, offset, resource=None, maxfixes=1):\n    fixer = fixsyntax.FixSyntax(project, code, resource, maxfixes)\n    pyname = fixer.pyname_at(offset)\n    if pyname is not None:\n        module, lineno = pyname.get_definition_location()\n        name = rope.base.worder.Worder(code).get_word_at(offset)\n        if lineno is not None:\n            start = module.lines.get_line_start(lineno)\n\n            def check_offset(occurrence):\n                if occurrence.offset < start:\n                    return False\n            pyname_filter = occurrences.PyNameFilter(pyname)\n            finder = occurrences.Finder(project, name,\n                                        [check_offset, pyname_filter])\n            for occurrence in finder.find_occurrences(pymodule=module):\n                return Location(occurrence)", "response": "Find the definition location of the python name at offset."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind possible bad name and attribute accesses It returns a list of Error \\ s.", "response": "def find_errors(project, resource):\n    \"\"\"Find possible bad name and attribute accesses\n\n    It returns a list of `Error`\\s.\n    \"\"\"\n    pymodule = project.get_pymodule(resource)\n    finder = _BadAccessFinder(pymodule)\n    ast.walk(pymodule.get_ast(), finder)\n    return finder.errors"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self, pyobject):\n        if pyobject is None:\n            return ('none',)\n        object_type = type(pyobject)\n        try:\n            method = getattr(self, object_type.__name__ + '_to_textual')\n            return method(pyobject)\n        except AttributeError:\n            return ('unknown',)", "response": "Transform a PyObject to textual form"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(self, textual):\n        if textual is None:\n            return None\n        type = textual[0]\n        try:\n            method = getattr(self, type + '_to_pyobject')\n            return method(textual)\n        except AttributeError:\n            return None", "response": "Transform an object from textual form to PyObject"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the AST from source.", "response": "def get_patched_ast(source, sorted_children=False):\n    \"\"\"Adds ``region`` and ``sorted_children`` fields to nodes\n\n    Adds ``sorted_children`` field only if `sorted_children` is True.\n\n    \"\"\"\n    return patch_ast(ast.parse(source), source, sorted_children)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npatch the given node with the given source code.", "response": "def patch_ast(node, source, sorted_children=False):\n    \"\"\"Patches the given node\n\n    After calling, each node in `node` will have a new field named\n    `region` that is a tuple containing the start and end offsets\n    of the code that generated it.\n\n    If `sorted_children` is true, a `sorted_children` field will\n    be created for each node, too.  It is a list containing child\n    nodes as well as whitespaces and comments that occur between\n    them.\n\n    \"\"\"\n    if hasattr(node, 'region'):\n        return node\n    walker = _PatchingASTWalker(source, children=sorted_children)\n    ast.call_for_nodes(node, walker)\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts source form a patched AST node with sorted_children field", "response": "def write_ast(patched_ast_node):\n    \"\"\"Extract source form a patched AST node with `sorted_children` field\n\n    If the node is patched with sorted_children turned off you can use\n    `node_region` function for obtaining code using module source code.\n    \"\"\"\n    result = []\n    for child in patched_ast_node.sorted_children:\n        if isinstance(child, ast.AST):\n            result.append(write_ast(child))\n        else:\n            result.append(child)\n    return ''.join(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_parens(self, children, start, formats):\n        opens, closes = self._count_needed_parens(formats)\n        old_end = self.source.offset\n        new_end = None\n        for i in range(closes):\n            new_end = self.source.consume(')')[1]\n        if new_end is not None:\n            if self.children:\n                children.append(self.source[old_end:new_end])\n        new_start = start\n        for i in range(opens):\n            new_start = self.source.rfind_token('(', 0, new_start)\n        if new_start != start:\n            if self.children:\n                children.appendleft(self.source[new_start:start])\n            start = new_start\n        return start", "response": "Changes children and returns new start"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether the token is in comments and if it is in a new line", "response": "def _good_token(self, token, offset, start=None):\n        \"\"\"Checks whether consumed token is in comments\"\"\"\n        if start is None:\n            start = self.offset\n        try:\n            comment_index = self.source.rindex('#', start, offset)\n        except ValueError:\n            return True\n        try:\n            new_line_index = self.source.rindex('\\n', start, offset)\n        except ValueError:\n            return False\n        return comment_index < new_line_index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_logicals(self):\n        size = self.lines.length() + 1\n        self._starts = [None] * size\n        self._ends = [None] * size\n        for start, end in self._generate(self.lines):\n            self._starts[start] = True\n            self._ends[end] = True", "response": "Should initialize _starts and _ends attributes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Move object based on resource and offset.", "response": "def create_move(project, resource, offset=None):\n    \"\"\"A factory for creating Move objects\n\n    Based on `resource` and `offset`, return one of `MoveModule`,\n    `MoveGlobal` or `MoveMethod` for performing move refactoring.\n\n    \"\"\"\n    if offset is None:\n        return MoveModule(project, resource)\n    this_pymodule = project.get_pymodule(resource)\n    pyname = evaluate.eval_location(this_pymodule, offset)\n    if pyname is not None:\n        pyobject = pyname.get_object()\n        if isinstance(pyobject, pyobjects.PyModule) or \\\n           isinstance(pyobject, pyobjects.PyPackage):\n            return MoveModule(project, pyobject.get_resource())\n        if isinstance(pyobject, pyobjects.PyFunction) and \\\n           isinstance(pyobject.parent, pyobjects.PyClass):\n            return MoveMethod(project, resource, offset)\n        if isinstance(pyobject, pyobjects.PyDefinedObject) and \\\n           isinstance(pyobject.parent, pyobjects.PyModule) or \\\n           isinstance(pyname, pynames.AssignedName):\n            return MoveGlobal(project, resource, offset)\n    raise exceptions.RefactoringError(\n        'Move only works on global classes/functions/variables, modules and '\n        'methods.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the changes needed for this refactoring.", "response": "def get_changes(self, dest_attr, new_name=None, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Return the changes needed for this refactoring\n\n        Parameters:\n\n        - `dest_attr`: the name of the destination attribute\n        - `new_name`: the name of the new method; if `None` uses\n          the old name\n        - `resources` can be a list of `rope.base.resources.File`\\s to\n          apply this refactoring on.  If `None`, the restructuring\n          will be applied to all python files.\n\n        \"\"\"\n        changes = ChangeSet('Moving method <%s>' % self.method_name)\n        if resources is None:\n            resources = self.project.get_python_files()\n        if new_name is None:\n            new_name = self.get_method_name()\n        resource1, start1, end1, new_content1 = \\\n            self._get_changes_made_by_old_class(dest_attr, new_name)\n        collector1 = codeanalyze.ChangeCollector(resource1.read())\n        collector1.add_change(start1, end1, new_content1)\n\n        resource2, start2, end2, new_content2 = \\\n            self._get_changes_made_by_new_class(dest_attr, new_name)\n        if resource1 == resource2:\n            collector1.add_change(start2, end2, new_content2)\n        else:\n            collector2 = codeanalyze.ChangeCollector(resource2.read())\n            collector2.add_change(start2, end2, new_content2)\n            result = collector2.get_changed()\n            import_tools = importutils.ImportTools(self.project)\n            new_imports = self._get_used_imports(import_tools)\n            if new_imports:\n                goal_pymodule = libutils.get_string_module(\n                    self.project, result, resource2)\n                result = _add_imports_to_module(\n                    import_tools, goal_pymodule, new_imports)\n            if resource2 in resources:\n                changes.add_change(ChangeContents(resource2, result))\n\n        if resource1 in resources:\n            changes.add_change(ChangeContents(resource1,\n                                              collector1.get_changed()))\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget function type It returns one of function method staticmethod or classmethod", "response": "def get_kind(self):\n        \"\"\"Get function type\n\n        It returns one of 'function', 'method', 'staticmethod' or\n        'classmethod' strs.\n\n        \"\"\"\n        scope = self.parent.get_scope()\n        if isinstance(self.parent, PyClass):\n            for decorator in self.decorators:\n                pyname = rope.base.evaluate.eval_node(scope, decorator)\n                if pyname == rope.base.builtins.builtins['staticmethod']:\n                    return 'staticmethod'\n                if pyname == rope.base.builtins.builtins['classmethod']:\n                    return 'classmethod'\n            return 'method'\n        return 'function'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwalking the syntax tree", "response": "def walk(node, walker):\n    \"\"\"Walk the syntax tree\"\"\"\n    method_name = '_' + node.__class__.__name__\n    method = getattr(walker, method_name, None)\n    if method is not None:\n        if isinstance(node, _ast.ImportFrom) and node.module is None:\n            # In python < 2.7 ``node.module == ''`` for relative imports\n            # but for python 2.7 it is None. Generalizing it to ''.\n            node.module = ''\n        return method(node)\n    for child in get_child_nodes(node):\n        walk(child, walker)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef call_for_nodes(node, callback, recursive=False):\n    result = callback(node)\n    if recursive and not result:\n        for child in get_child_nodes(node):\n            call_for_nodes(child, callback, recursive)", "response": "Recursively calls callback on the node and all its children."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _realpath(path):\n    # there is a bug in cygwin for os.path.abspath() for abs paths\n    if sys.platform == 'cygwin':\n        if path[1:3] == ':\\\\':\n            return path\n        elif path[1:3] == ':/':\n            path = \"/cygdrive/\" + path[0] + path[2:]\n        return os.path.abspath(os.path.expanduser(path))\n    return os.path.realpath(os.path.abspath(os.path.expanduser(path)))", "response": "Return the real path of path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a resource in a project.", "response": "def get_resource(self, resource_name):\n        \"\"\"Get a resource in a project.\n\n        `resource_name` is the path of a resource in a project.  It is\n        the path of a resource relative to project root.  Project root\n        folder address is an empty string.  If the resource does not\n        exist a `exceptions.ResourceNotFound` exception would be\n        raised.  Use `get_file()` and `get_folder()` when you need to\n        get nonexistent `Resource`\\s.\n\n        \"\"\"\n        path = self._get_resource_path(resource_name)\n        if not os.path.exists(path):\n            raise exceptions.ResourceNotFoundError(\n                'Resource <%s> does not exist' % resource_name)\n        elif os.path.isfile(path):\n            return File(self, resource_name)\n        elif os.path.isdir(path):\n            return Folder(self, resource_name)\n        else:\n            raise exceptions.ResourceNotFoundError('Unknown resource '\n                                                   + resource_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a PyObject if the module was found.", "response": "def get_module(self, name, folder=None):\n        \"\"\"Returns a `PyObject` if the module was found.\"\"\"\n        # check if this is a builtin module\n        pymod = self.pycore.builtin_module(name)\n        if pymod is not None:\n            return pymod\n        module = self.find_module(name, folder)\n        if module is None:\n            raise ModuleNotFoundError('Module %s not found' % name)\n        return self.pycore.resource_to_pyobject(module)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning project source folders", "response": "def get_source_folders(self):\n        \"\"\"Returns project source folders\"\"\"\n        if self.root is None:\n            return []\n        result = list(self._custom_source_folders)\n        result.extend(self.pycore._find_source_folders(self.root))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates all of the files and folders contained in this folder.", "response": "def validate(self, folder):\n        \"\"\"Validate files and folders contained in this folder\n\n        It validates all of the files and folders contained in this\n        folder if some observers are interested in them.\n\n        \"\"\"\n        for observer in list(self.observers):\n            observer.validate(folder)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the changes in a ChangeSet to the current set of keys.", "response": "def do(self, changes, task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Apply the changes in a `ChangeSet`\n\n        Most of the time you call this function for committing the\n        changes for a refactoring.\n        \"\"\"\n        self.history.do(changes, task_handle=task_handle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a resource corresponding to the given module modname returns None if it can t be found", "response": "def find_module(self, modname, folder=None):\n        \"\"\"Returns a resource corresponding to the given module\n\n        returns None if it can not be found\n        \"\"\"\n        for src in self.get_source_folders():\n            module = _find_module_in_folder(src, modname)\n            if module is not None:\n                return module\n        for src in self.get_python_path_folders():\n            module = _find_module_in_folder(src, modname)\n            if module is not None:\n                return module\n        if folder is not None:\n            module = _find_module_in_folder(folder, modname)\n            if module is not None:\n                return module\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all python files available in the project", "response": "def get_python_files(self):\n        \"\"\"Returns all python files available in the project\"\"\"\n        return [resource for resource in self.get_files()\n                if self.pycore.is_python_file(resource)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a tuple of module and line number", "response": "def get_definition_location(self):\n        \"\"\"Returns a (module, lineno) tuple\"\"\"\n        if self.lineno is None and self.assignments:\n            self.lineno = self.assignments[0].get_lineno()\n        return (self.module, self.lineno)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of objects passed as this parameter", "response": "def get_objects(self):\n        \"\"\"Returns the list of objects passed as this parameter\"\"\"\n        return rope.base.oi.soi.get_passed_objects(\n            self.pyfunction, self.index)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_all_changes(self, *args, **kwds):\n        result = []\n        for project, refactoring in zip(self.projects, self.refactorings):\n            args, kwds = self._resources_for_args(project, args, kwds)\n            result.append((project, refactoring.get_changes(*args, **kwds)))\n        return result", "response": "Get a project to changes dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_changes(self, getter=None, setter=None, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get the changes this refactoring makes\n\n        If `getter` is not `None`, that will be the name of the\n        getter, otherwise ``get_${field_name}`` will be used.  The\n        same is true for `setter` and if it is None set_${field_name} is\n        used.\n\n        `resources` can be a list of `rope.base.resource.File`\\s that\n        the refactoring should be applied on; if `None` all python\n        files in the project are searched.\n\n        \"\"\"\n        if resources is None:\n            resources = self.project.get_python_files()\n        changes = ChangeSet('Encapsulate field <%s>' % self.name)\n        job_set = task_handle.create_jobset('Collecting Changes',\n                                            len(resources))\n        if getter is None:\n            getter = 'get_' + self.name\n        if setter is None:\n            setter = 'set_' + self.name\n        renamer = GetterSetterRenameInModule(\n            self.project, self.name, self.pyname, getter, setter)\n        for file in resources:\n            job_set.started_job(file.path)\n            if file == self.resource:\n                result = self._change_holding_module(changes, renamer,\n                                                     getter, setter)\n                changes.add_change(ChangeContents(self.resource, result))\n            else:\n                result = renamer.get_changed_module(file)\n                if result is not None:\n                    changes.add_change(ChangeContents(file, result))\n            job_set.finished_job()\n        return changes", "response": "Get the changes this refactoring makes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prevent_recursion(default):\n    def decorator(func):\n        name = '_calling_%s_' % func.__name__\n\n        def newfunc(self, *args, **kwds):\n            if getattr(self, name, False):\n                return default()\n            setattr(self, name, True)\n            try:\n                return func(self, *args, **kwds)\n            finally:\n                setattr(self, name, False)\n        return newfunc\n    return decorator", "response": "A decorator that returns the return value of default in recursions"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deprecated(message=None):\n    def _decorator(func, message=message):\n        if message is None:\n            message = '%s is deprecated' % func.__name__\n\n        def newfunc(*args, **kwds):\n            warnings.warn(message, DeprecationWarning, stacklevel=2)\n            return func(*args, **kwds)\n        return newfunc\n    return _decorator", "response": "A decorator for deprecated functions."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cached(size):\n    def decorator(func):\n        cached_func = _Cached(func, size)\n        return lambda *a, **kw: cached_func(*a, **kw)\n    return decorator", "response": "A caching decorator based on parameter objects"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn object from string", "response": "def resolve(str_or_obj):\n    \"\"\"Returns object from string\"\"\"\n    from rope.base.utils.pycompat import string_types\n    if not isinstance(str_or_obj, string_types):\n        return str_or_obj\n    if '.' not in str_or_obj:\n        str_or_obj += '.'\n    mod_name, obj_name = str_or_obj.rsplit('.', 1)\n    __import__(mod_name)\n    mod = sys.modules[mod_name]\n    return getattr(mod, obj_name) if obj_name else mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_primary_without_dot_start(self, offset):\n        last_atom = offset\n        offset = self._find_last_non_space_char(last_atom)\n        while offset > 0 and self.code[offset] in ')]':\n            last_atom = self._find_parens_start(offset)\n            offset = self._find_last_non_space_char(last_atom - 1)\n        if offset >= 0 and (self.code[offset] in '\"\\'})]' or\n                            self._is_id_char(offset)):\n            atom_start = self._find_atom_start(offset)\n            if not keyword.iskeyword(self.code[atom_start:offset + 1]):\n                return atom_start\n        return last_atom", "response": "This function tries to find the undotted primary start ArcGIS code entry. It tries to find the undotted primary start ArcGIS code entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn expression starting starting_offset", "response": "def get_splitted_primary_before(self, offset):\n        \"\"\"returns expression, starting, starting_offset\n\n        This function is used in `rope.codeassist.assist` function.\n        \"\"\"\n        if offset == 0:\n            return ('', '', 0)\n        end = offset - 1\n        word_start = self._find_atom_start(end)\n        real_start = self._find_primary_start(end)\n        if self.code[word_start:offset].strip() == '':\n            word_start = end\n        if self.code[end].isspace():\n            word_start = end\n        if self.code[real_start:word_start].strip() == '':\n            real_start = word_start\n        if real_start == word_start == end and not self._is_id_char(end):\n            return ('', '', offset)\n        if real_start == word_start:\n            return ('', self.raw[word_start:offset], word_start)\n        else:\n            if self.code[end] == '.':\n                return (self.raw[real_start:end], '', offset)\n            last_dot_position = word_start\n            if self.code[word_start] != '.':\n                last_dot_position = \\\n                    self._find_last_non_space_char(word_start - 1)\n            last_char_position = \\\n                self._find_last_non_space_char(last_dot_position - 1)\n            if self.code[word_start].isspace():\n                word_start = offset\n            return (self.raw[real_start:last_char_position + 1],\n                    self.raw[word_start:offset], word_start)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a refactoring object for inlining Based on resource and offset it returns an instance of InlineMethod InlineVariable or InlineParameter.", "response": "def create_inline(project, resource, offset):\n    \"\"\"Create a refactoring object for inlining\n\n    Based on `resource` and `offset` it returns an instance of\n    `InlineMethod`, `InlineVariable` or `InlineParameter`.\n\n    \"\"\"\n    pyname = _get_pyname(project, resource, offset)\n    message = 'Inline refactoring should be performed on ' \\\n              'a method, local variable or parameter.'\n    if pyname is None:\n        raise rope.base.exceptions.RefactoringError(message)\n    if isinstance(pyname, pynames.ImportedName):\n        pyname = pyname._get_imported_pyname()\n    if isinstance(pyname, pynames.AssignedName):\n        return InlineVariable(project, resource, offset)\n    if isinstance(pyname, pynames.ParameterName):\n        return InlineParameter(project, resource, offset)\n    if isinstance(pyname.get_object(), pyobjects.PyFunction):\n        return InlineMethod(project, resource, offset)\n    else:\n        raise rope.base.exceptions.RefactoringError(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_changes(self, remove=True, only_current=False, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get the changes this refactoring makes\n\n        If `remove` is `False` the definition will not be removed.  If\n        `only_current` is `True`, the the current occurrence will be\n        inlined, only.\n        \"\"\"\n        changes = ChangeSet('Inline method <%s>' % self.name)\n        if resources is None:\n            resources = self.project.get_python_files()\n        if only_current:\n            resources = [self.original]\n            if remove:\n                resources.append(self.resource)\n        job_set = task_handle.create_jobset('Collecting Changes',\n                                            len(resources))\n        for file in resources:\n            job_set.started_job(file.path)\n            if file == self.resource:\n                changes.add_change(self._defining_file_changes(\n                    changes, remove=remove, only_current=only_current))\n            else:\n                aim = None\n                if only_current and self.original == file:\n                    aim = self.offset\n                handle = _InlineFunctionCallsForModuleHandle(\n                    self.project, file, self.others_generator, aim)\n                result = move.ModuleSkipRenamer(\n                    self.occurrence_finder, file, handle).get_changed_module()\n                if result is not None:\n                    result = _add_imports(self.project, result,\n                                          file, self.imports)\n                    if remove:\n                        result = _remove_from(self.project, self.pyname,\n                                              result, file)\n                    changes.add_change(ChangeContents(file, result))\n            job_set.finished_job()\n        return changes", "response": "Get the changes this refactoring makes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the imported resource.", "response": "def get_imported_resource(self, context):\n        \"\"\"Get the imported resource\n\n        Returns `None` if module was not found.\n        \"\"\"\n        if self.level == 0:\n            return context.project.find_module(\n                self.module_name, folder=context.folder)\n        else:\n            return context.project.find_relative_module(\n                self.module_name, context.folder, self.level)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_imported_module(self, context):\n        if self.level == 0:\n            return context.project.get_module(\n                self.module_name, context.folder)\n        else:\n            return context.project.get_relative_module(\n                self.module_name, context.folder, self.level)", "response": "Get the imported PyModule."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns python code completions as a list of CodeAssistProposal objects.", "response": "def code_assist(project, source_code, offset, resource=None,\n                templates=None, maxfixes=1, later_locals=True):\n    \"\"\"Return python code completions as a list of `CodeAssistProposal`\\s\n\n    `resource` is a `rope.base.resources.Resource` object.  If\n    provided, relative imports are handled.\n\n    `maxfixes` is the maximum number of errors to fix if the code has\n    errors in it.\n\n    If `later_locals` is `False` names defined in this scope and after\n    this line is ignored.\n\n    \"\"\"\n    if templates is not None:\n        warnings.warn('Codeassist no longer supports templates',\n                      DeprecationWarning, stacklevel=2)\n    assist = _PythonCodeAssist(\n        project, source_code, offset, resource=resource,\n        maxfixes=maxfixes, later_locals=later_locals)\n    return assist()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the offset in which the completion should be inserted", "response": "def starting_offset(source_code, offset):\n    \"\"\"Return the offset in which the completion should be inserted\n\n    Usually code assist proposals should be inserted like::\n\n        completion = proposal.name\n        result = (source_code[:starting_offset] +\n                  completion + source_code[offset:])\n\n    Where starting_offset is the offset returned by this function.\n\n    \"\"\"\n    word_finder = worder.Worder(source_code, True)\n    expression, starting, starting_offset = \\\n        word_finder.get_splitted_primary_before(offset)\n    return starting_offset"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_doc(project, source_code, offset, resource=None, maxfixes=1):\n    fixer = fixsyntax.FixSyntax(project, source_code, resource, maxfixes)\n    pyname = fixer.pyname_at(offset)\n    if pyname is None:\n        return None\n    pyobject = pyname.get_object()\n    return PyDocExtractor().get_doc(pyobject)", "response": "Get the pydoc for the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the calltip of a function in a specific module.", "response": "def get_calltip(project, source_code, offset, resource=None,\n                maxfixes=1, ignore_unknown=False, remove_self=False):\n    \"\"\"Get the calltip of a function\n\n    The format of the returned string is\n    ``module_name.holding_scope_names.function_name(arguments)``.  For\n    classes `__init__()` and for normal objects `__call__()` function\n    is used.\n\n    Note that the offset is on the function itself *not* after the its\n    open parenthesis.  (Actually it used to be the other way but it\n    was easily confused when string literals were involved.  So I\n    decided it is better for it not to try to be too clever when it\n    cannot be clever enough).  You can use a simple search like::\n\n        offset = source_code.rindex('(', 0, offset) - 1\n\n    to handle simple situations.\n\n    If `ignore_unknown` is `True`, `None` is returned for functions\n    without source-code like builtins and extensions.\n\n    If `remove_self` is `True`, the first parameter whose name is self\n    will be removed for methods.\n    \"\"\"\n    fixer = fixsyntax.FixSyntax(project, source_code, resource, maxfixes)\n    pyname = fixer.pyname_at(offset)\n    if pyname is None:\n        return None\n    pyobject = pyname.get_object()\n    return PyDocExtractor().get_calltip(pyobject, ignore_unknown, remove_self)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the definition location of the python name at offset.", "response": "def get_definition_location(project, source_code, offset,\n                            resource=None, maxfixes=1):\n    \"\"\"Return the definition location of the python name at `offset`\n\n    Return a (`rope.base.resources.Resource`, lineno) tuple.  If no\n    `resource` is given and the definition is inside the same module,\n    the first element of the returned tuple would be `None`.  If the\n    location cannot be determined ``(None, None)`` is returned.\n\n    \"\"\"\n    fixer = fixsyntax.FixSyntax(project, source_code, resource, maxfixes)\n    pyname = fixer.pyname_at(offset)\n    if pyname is not None:\n        module, lineno = pyname.get_definition_location()\n        if module is not None:\n            return module.get_module().get_resource(), lineno\n    return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the canonical path to an object.", "response": "def get_canonical_path(project, resource, offset):\n    \"\"\"Get the canonical path to an object.\n\n    Given the offset of the object, this returns a list of\n    (name, name_type) tuples representing the canonical path to the\n    object. For example, the 'x' in the following code:\n\n        class Foo(object):\n            def bar(self):\n                class Qux(object):\n                    def mux(self, x):\n                        pass\n\n    we will return:\n\n        [('Foo', 'CLASS'), ('bar', 'FUNCTION'), ('Qux', 'CLASS'),\n         ('mux', 'FUNCTION'), ('x', 'PARAMETER')]\n\n    `resource` is a `rope.base.resources.Resource` object.\n\n    `offset` is the offset of the pyname you want the path to.\n\n    \"\"\"\n    # Retrieve the PyName.\n    pymod = project.get_pymodule(resource)\n    pyname = rope.base.evaluate.eval_location(pymod, offset)\n\n    # Now get the location of the definition and its containing scope.\n    defmod, lineno = pyname.get_definition_location()\n    if not defmod:\n        return None\n    scope = defmod.get_scope().get_inner_scope_for_line(lineno)\n\n    # Start with the name of the object we're interested in.\n    names = []\n    if isinstance(pyname, pynamesdef.ParameterName):\n        names = [(worder.get_name_at(pymod.get_resource(), offset),\n                  'PARAMETER') ]\n    elif isinstance(pyname, pynamesdef.AssignedName):\n        names = [(worder.get_name_at(pymod.get_resource(), offset),\n                  'VARIABLE')]\n\n    # Collect scope names.\n    while scope.parent:\n        if isinstance(scope, pyscopes.FunctionScope):\n            scope_type = 'FUNCTION'\n        elif isinstance(scope, pyscopes.ClassScope):\n            scope_type = 'CLASS'\n        else:\n            scope_type = None\n        names.append((scope.pyobject.get_name(), scope_type))\n        scope = scope.parent\n\n    names.append((defmod.get_resource().real_path, 'MODULE'))\n    names.reverse()\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort a list of CodeAssistProposal \\ s.", "response": "def sorted_proposals(proposals, scopepref=None, typepref=None):\n    \"\"\"Sort a list of proposals\n\n    Return a sorted list of the given `CodeAssistProposal`\\s.\n\n    `scopepref` can be a list of proposal scopes.  Defaults to\n    ``['parameter_keyword', 'local', 'global', 'imported',\n    'attribute', 'builtin', 'keyword']``.\n\n    `typepref` can be a list of proposal types.  Defaults to\n    ``['class', 'function', 'instance', 'module', None]``.\n    (`None` stands for completions with no type like keywords.)\n    \"\"\"\n    sorter = _ProposalSorter(proposals, scopepref, typepref)\n    return sorter.get_sorted_proposal_list()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the expression to complete", "response": "def starting_expression(source_code, offset):\n    \"\"\"Return the expression to complete\"\"\"\n    word_finder = worder.Worder(source_code, True)\n    expression, starting, starting_offset = \\\n        word_finder.get_splitted_primary_before(offset)\n    if expression:\n        return expression + '.' + starting\n    return starting"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the proposed object s docstring. Returns None if it can t be get.", "response": "def get_doc(self):\n        \"\"\"Get the proposed object's docstring.\n\n        Returns None if it can not be get.\n        \"\"\"\n        if not self.pyname:\n            return None\n        pyobject = self.pyname.get_object()\n        if not hasattr(pyobject, 'get_doc'):\n            return None\n        return self.pyname.get_object().get_doc()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a string representation of a param s default value. Returns None if there is no default value for this param.", "response": "def get_default(self):\n        \"\"\"Get a string representation of a param's default value.\n\n        Returns None if there is no default value for this param.\n        \"\"\"\n        definfo = functionutils.DefinitionInfo.read(self._function)\n        for arg, default in definfo.args_with_defaults:\n            if self.argname == arg:\n                return default\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of CodeAssistProposal objects sorted by scope.", "response": "def get_sorted_proposal_list(self):\n        \"\"\"Return a list of `CodeAssistProposal`\"\"\"\n        proposals = {}\n        for proposal in self.proposals:\n            proposals.setdefault(proposal.scope, []).append(proposal)\n        result = []\n        for scope in self.scopepref:\n            scope_proposals = proposals.get(scope, [])\n            scope_proposals = [proposal for proposal in scope_proposals\n                               if proposal.type in self.typerank]\n            scope_proposals.sort(key=self._proposal_key)\n            result.extend(scope_proposals)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_prefs(prefs):\n\n    # Specify which files and folders to ignore in the project.\n    # Changes to ignored resources are not added to the history and\n    # VCSs.  Also they are not returned in `Project.get_files()`.\n    # Note that ``?`` and ``*`` match all characters but slashes.\n    # '*.pyc': matches 'test.pyc' and 'pkg/test.pyc'\n    # 'mod*.pyc': matches 'test/mod1.pyc' but not 'mod/1.pyc'\n    # '.svn': matches 'pkg/.svn' and all of its children\n    # 'build/*.o': matches 'build/lib.o' but not 'build/sub/lib.o'\n    # 'build//*.o': matches 'build/lib.o' and 'build/sub/lib.o'\n    prefs['ignored_resources'] = ['*.pyc', '*~', '.ropeproject',\n                                  '.hg', '.svn', '_svn', '.git', '.tox']\n\n    # Specifies which files should be considered python files.  It is\n    # useful when you have scripts inside your project.  Only files\n    # ending with ``.py`` are considered to be python files by\n    # default.\n    # prefs['python_files'] = ['*.py']\n\n    # Custom source folders:  By default rope searches the project\n    # for finding source folders (folders that should be searched\n    # for finding modules).  You can add paths to that list.  Note\n    # that rope guesses project source folders correctly most of the\n    # time; use this if you have any problems.\n    # The folders should be relative to project root and use '/' for\n    # separating folders regardless of the platform rope is running on.\n    # 'src/my_source_folder' for instance.\n    # prefs.add('source_folders', 'src')\n\n    # You can extend python path for looking up modules\n    # prefs.add('python_path', '~/python/')\n\n    # Should rope save object information or not.\n    prefs['save_objectdb'] = True\n    prefs['compress_objectdb'] = False\n\n    # If `True`, rope analyzes each module when it is being saved.\n    prefs['automatic_soa'] = True\n    # The depth of calls to follow in static object analysis\n    prefs['soa_followed_calls'] = 0\n\n    # If `False` when running modules or unit tests \"dynamic object\n    # analysis\" is turned off.  This makes them much faster.\n    prefs['perform_doa'] = True\n\n    # Rope can check the validity of its object DB when running.\n    prefs['validate_objectdb'] = True\n\n    # How many undos to hold?\n    prefs['max_history_items'] = 32\n\n    # Shows whether to save history across sessions.\n    prefs['save_history'] = True\n    prefs['compress_history'] = False\n\n    # Set the number spaces used for indenting.  According to\n    # :PEP:`8`, it is best to use 4 spaces.  Since most of rope's\n    # unit-tests use 4 spaces it is more reliable, too.\n    prefs['indent_size'] = 4\n\n    # Builtin and c-extension modules that are allowed to be imported\n    # and inspected by rope.\n    prefs['extension_modules'] = []\n\n    # Add all standard c-extensions to extension_modules list.\n    prefs['import_dynload_stdmods'] = True\n\n    # If `True` modules with syntax errors are considered to be empty.\n    # The default value is `False`; When `False` syntax errors raise\n    # `rope.base.exceptions.ModuleSyntaxError` exception.\n    prefs['ignore_syntax_errors'] = False\n\n    # If `True`, rope ignores unresolvable imports.  Otherwise, they\n    # appear in the importing namespace.\n    prefs['ignore_bad_imports'] = False\n\n    # If `True`, rope will insert new module imports as\n    # `from <package> import <module>` by default.\n    prefs['prefer_module_from_imports'] = False\n\n    # If `True`, rope will transform a comma list of imports into\n    # multiple separate import statements when organizing\n    # imports.\n    prefs['split_imports'] = False\n\n    # If `True`, rope will remove all top-level import statements and\n    # reinsert them at the top of the module when making changes.\n    prefs['pull_imports_to_top'] = True\n\n    # If `True`, rope will sort imports alphabetically by module name instead\n    # of alphabetically by import statement, with from imports after normal\n    # imports.\n    prefs['sort_imports_alphabetically'] = False\n\n    # Location of implementation of\n    # rope.base.oi.type_hinting.interfaces.ITypeHintingFactory In general\n    # case, you don't have to change this value, unless you're an rope expert.\n    # Change this value to inject you own implementations of interfaces\n    # listed in module rope.base.oi.type_hinting.providers.interfaces\n    # For example, you can add you own providers for Django Models, or disable\n    # the search type-hinting in a class hierarchy, etc.\n    prefs['type_hinting_factory'] = (\n        'rope.base.oi.type_hinting.factory.default_type_hinting_factory')", "response": "This function is called before opening the project. It sets the prefs dictionary that will be used to populate the project s project preferences."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_resource(self, resource):\n        if resource.exists():\n            self.resources[resource] = self.timekeeper.get_indicator(resource)\n        else:\n            self.resources[resource] = None", "response": "Add a resource to the list of interesting resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the modification time and size of a Resource.", "response": "def get_indicator(self, resource):\n        \"\"\"Return the modification time and size of a `Resource`.\"\"\"\n        path = resource.real_path\n        # on dos, mtime does not change for a folder when files are added\n        if os.name != 'posix' and os.path.isdir(path):\n            return (os.path.getmtime(path),\n                    len(os.listdir(path)),\n                    os.path.getsize(path))\n        return (os.path.getmtime(path),\n                os.path.getsize(path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the resource at path", "response": "def path_to_resource(project, path, type=None):\n    \"\"\"Get the resource at path\n\n    You only need to specify `type` if `path` does not exist.  It can\n    be either 'file' or 'folder'.  If the type is `None` it is assumed\n    that the resource already exists.\n\n    Note that this function uses `Project.get_resource()`,\n    `Project.get_file()`, and `Project.get_folder()` methods.\n\n    \"\"\"\n    project_path = path_relative_to_project_root(project, path)\n    if project_path is None:\n        project_path = rope.base.project._realpath(path)\n        project = rope.base.project.get_no_project()\n    if type is None:\n        return project.get_resource(project_path)\n    if type == 'file':\n        return project.get_file(project_path)\n    if type == 'folder':\n        return project.get_folder(project_path)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef report_change(project, path, old_content):\n    resource = path_to_resource(project, path)\n    if resource is None:\n        return\n    for observer in list(project.observers):\n        observer.resource_changed(resource)\n    if project.pycore.automatic_soa:\n        rope.base.pycore.perform_soa_on_changed_scopes(project, resource,\n                                                       old_content)", "response": "Report that the contents of file at path was changed\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms static object analysis on all python files in the project", "response": "def analyze_modules(project, task_handle=taskhandle.NullTaskHandle()):\n    \"\"\"Perform static object analysis on all python files in the project\n\n    Note that this might be really time consuming.\n    \"\"\"\n    resources = project.get_python_files()\n    job_set = task_handle.create_jobset('Analyzing Modules', len(resources))\n    for resource in resources:\n        job_set.started_job(resource.path)\n        analyze_module(project, resource)\n        job_set.finished_job()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a PyObject object for the given code", "response": "def get_string_module(project, code, resource=None, force_errors=False):\n    \"\"\"Returns a `PyObject` object for the given code\n\n    If `force_errors` is `True`, `exceptions.ModuleSyntaxError` is\n    raised if module has syntax errors.  This overrides\n    ``ignore_syntax_errors`` project config.\n\n    \"\"\"\n    return pyobjectsdef.PyModule(project.pycore, code, resource,\n                                 force_errors=force_errors)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the changes this refactoring makes.", "response": "def get_changes(self, factory_name, global_factory=False, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get the changes this refactoring makes\n\n        `factory_name` indicates the name of the factory function to\n        be added.  If `global_factory` is `True` the factory will be\n        global otherwise a static method is added to the class.\n\n        `resources` can be a list of `rope.base.resource.File`\\s that\n        this refactoring should be applied on; if `None` all python\n        files in the project are searched.\n\n        \"\"\"\n        if resources is None:\n            resources = self.project.get_python_files()\n        changes = ChangeSet('Introduce factory method <%s>' % factory_name)\n        job_set = task_handle.create_jobset('Collecting Changes',\n                                            len(resources))\n        self._change_module(resources, changes, factory_name,\n                            global_factory, job_set)\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _search_type_in_type_comment(self, code):\n        for p in self.PEP0484_TYPE_COMMENT_PATTERNS:\n            match = p.search(code)\n            if match:\n                return [match.group(1)]", "response": "Search for type in type comment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef force_single_imports(self):\n        for import_stmt in self.imports[:]:\n            import_info = import_stmt.import_info\n            if import_info.is_empty() or import_stmt.readonly:\n                continue\n            if len(import_info.names_and_aliases) > 1:\n                for name_and_alias in import_info.names_and_aliases:\n                    if hasattr(import_info, \"module_name\"):\n                        new_import = importinfo.FromImport(\n                            import_info.module_name, import_info.level,\n                            [name_and_alias])\n                    else:\n                        new_import = importinfo.NormalImport([name_and_alias])\n                    self.add_import(new_import)\n                import_stmt.empty_import()", "response": "force a single import per statement"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves pyname when imported in from mod import x", "response": "def remove_pyname(self, pyname):\n        \"\"\"Removes pyname when imported in ``from mod import x``\"\"\"\n        visitor = actions.RemovePyNameVisitor(self.project, self.pymodule,\n                                              pyname, self._current_folder())\n        for import_stmt in self.imports:\n            import_stmt.accept(visitor)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_arguments(primary, pyfunction, call_node, scope):\n    args = list(call_node.args)\n    args.extend(call_node.keywords)\n    called = call_node.func\n    # XXX: Handle constructors\n    if _is_method_call(primary, pyfunction) and \\\n       isinstance(called, ast.Attribute):\n        args.insert(0, called.value)\n    return Arguments(args, scope)", "response": "A factory for creating Arguments"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value of key preference to value.", "response": "def set(self, key, value):\n        \"\"\"Set the value of `key` preference to `value`.\"\"\"\n        if key in self.callbacks:\n            self.callbacks[key](value)\n        else:\n            self.prefs[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an entry to a list preference.", "response": "def add(self, key, value):\n        \"\"\"Add an entry to a list preference\n\n        Add `value` to the list of entries for the `key` preference.\n\n        \"\"\"\n        if not key in self.prefs:\n            self.prefs[key] = []\n        self.prefs[key].append(value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_indentation(code, new_indents):\n    min_indents = find_minimum_indents(code)\n    return indent_lines(code, new_indents - min_indents)", "response": "Change the indentation of code to new_indents."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_body(pyfunction):\n    # FIXME scope = pyfunction.get_scope()\n    pymodule = pyfunction.get_module()\n    start, end = get_body_region(pyfunction)\n    return fix_indentation(pymodule.source_code[start:end], 0)", "response": "Return unindented function body"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the start and end offsets of function body", "response": "def get_body_region(defined):\n    \"\"\"Return the start and end offsets of function body\"\"\"\n    scope = defined.get_scope()\n    pymodule = defined.get_module()\n    lines = pymodule.lines\n    node = defined.get_ast()\n    start_line = node.lineno\n    if defined.get_doc() is None:\n        start_line = node.body[0].lineno\n    elif len(node.body) > 1:\n        start_line = node.body[1].lineno\n    start = lines.get_line_start(start_line)\n    scope_start = pymodule.logical_lines.logical_line_in(scope.start)\n    if scope_start[1] >= start_line:\n        # a one-liner!\n        # XXX: what if colon appears in a string\n        start = pymodule.source_code.index(':', start) + 1\n        while pymodule.source_code[start].isspace():\n            start += 1\n    end = min(lines.get_line_end(scope.end) + 1, len(pymodule.source_code))\n    return start, end"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do(self, changes, task_handle=taskhandle.NullTaskHandle()):\n        try:\n            self.current_change = changes\n            changes.do(change.create_job_set(task_handle, changes))\n        finally:\n            self.current_change = None\n        if self._is_change_interesting(changes):\n            self.undo_list.append(changes)\n            self._remove_extra_items()\n        del self.redo_list[:]", "response": "Perform the change and add it to the undo_list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nredo done changes from the history list.", "response": "def undo(self, change=None, drop=False,\n             task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Redo done changes from the history\n\n        When `change` is `None`, the last done change will be undone.\n        If change is not `None` it should be an item from\n        `self.undo_list`; this change and all changes that depend on\n        it will be undone.  In both cases the list of undone changes\n        will be returned.\n\n        If `drop` is `True`, the undone change will not be appended to\n        the redo list.\n\n        \"\"\"\n        if not self._undo_list:\n            raise exceptions.HistoryError('Undo list is empty')\n        if change is None:\n            change = self.undo_list[-1]\n        dependencies = self._find_dependencies(self.undo_list, change)\n        self._move_front(self.undo_list, dependencies)\n        self._perform_undos(len(dependencies), task_handle)\n        result = self.redo_list[-len(dependencies):]\n        if drop:\n            del self.redo_list[-len(dependencies):]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nredoing undone changes from the history", "response": "def redo(self, change=None, task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Redo undone changes from the history\n\n        When `change` is `None`, the last undone change will be\n        redone.  If change is not `None` it should be an item from\n        `self.redo_list`; this change and all changes that depend on\n        it will be redone.  In both cases the list of redone changes\n        will be returned.\n\n        \"\"\"\n        if not self.redo_list:\n            raise exceptions.HistoryError('Redo list is empty')\n        if change is None:\n            change = self.redo_list[-1]\n        dependencies = self._find_dependencies(self.redo_list, change)\n        self._move_front(self.redo_list, dependencies)\n        self._perform_redos(len(dependencies), task_handle)\n        return self.undo_list[-len(dependencies):]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_string_module(self, code, resource=None, force_errors=False):\n        return PyModule(self, code, resource, force_errors=force_errors)", "response": "Returns a PyObject object for the given code and resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Scope object for the given code and resource", "response": "def get_string_scope(self, code, resource=None):\n        \"\"\"Returns a `Scope` object for the given code\"\"\"\n        return rope.base.libutils.get_string_scope(code, resource)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_module(self, resource, args=None, stdin=None, stdout=None):\n        perform_doa = self.project.prefs.get('perform_doi', True)\n        perform_doa = self.project.prefs.get('perform_doa', perform_doa)\n        receiver = self.object_info.doa_data_received\n        if not perform_doa:\n            receiver = None\n        runner = rope.base.oi.doa.PythonFileRunner(\n            self, resource, args, stdin, stdout, receiver)\n        runner.add_finishing_observer(self.module_cache.forget_all_data)\n        runner.run()\n        return runner", "response": "Run the module with the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nanalyzing the module for static object inference.", "response": "def analyze_module(self, resource, should_analyze=lambda py: True,\n                       search_subscopes=lambda py: True, followed_calls=None):\n        \"\"\"Analyze `resource` module for static object inference\n\n        This function forces rope to analyze this module to collect\n        information about function calls.  `should_analyze` is a\n        function that is called with a `PyDefinedObject` argument.  If\n        it returns `True` the element is analyzed.  If it is `None` or\n        returns `False` the element is not analyzed.\n\n        `search_subscopes` is like `should_analyze`; The difference is\n        that if it returns `False` the sub-scopes are all ignored.\n        That is it is assumed that `should_analyze` returns `False`\n        for all of its subscopes.\n\n        `followed_calls` override the value of ``soa_followed_calls``\n        project config.\n        \"\"\"\n        if followed_calls is None:\n            followed_calls = self.project.prefs.get('soa_followed_calls', 0)\n        pymodule = self.resource_to_pyobject(resource)\n        self.module_cache.forget_all_data()\n        rope.base.oi.soa.analyze_module(\n            self, pymodule, should_analyze, search_subscopes, followed_calls)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntell if any of start till end lines have changed", "response": "def is_changed(self, start, end):\n        \"\"\"Tell whether any of start till end lines have changed\n\n        The end points are inclusive and indices start from 1.\n        \"\"\"\n        left, right = self._get_changed(start, end)\n        if left < right:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consume_changes(self, start, end):\n        left, right = self._get_changed(start, end)\n        if left < right:\n            del self.lines[left:right]\n        return left < right", "response": "Clear the changed status of lines from start till end"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns name PyName defined in this scope", "response": "def get_name(self, name):\n        \"\"\"Return name `PyName` defined in this scope\"\"\"\n        if name not in self.get_names():\n            raise exceptions.NameNotFoundError('name %s not found' % name)\n        return self.get_names()[name]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of tuples that are used to import the assist from the database.", "response": "def import_assist(self, starting):\n        \"\"\"Return a list of ``(name, module)`` tuples\n\n        This function tries to find modules that have a global name\n        that starts with `starting`.\n        \"\"\"\n        # XXX: breaking if gave up! use generators\n        result = []\n        for module in self.names:\n            for global_name in self.names[module]:\n                if global_name.startswith(starting):\n                    result.append((global_name, module))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of modules that have global name", "response": "def get_modules(self, name):\n        \"\"\"Return the list of modules that have global `name`\"\"\"\n        result = []\n        for module in self.names:\n            if name in self.names[module]:\n                result.append(module)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_names(self):\n        result = set()\n        for module in self.names:\n            result.update(set(self.names[module]))\n        return result", "response": "Return the list of all cached global names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of tuples containing the resource and line number of a named module.", "response": "def get_name_locations(self, name):\n        \"\"\"Return a list of ``(resource, lineno)`` tuples\"\"\"\n        result = []\n        for module in self.names:\n            if name in self.names[module]:\n                try:\n                    pymodule = self.project.get_module(module)\n                    if name in pymodule:\n                        pyname = pymodule[name]\n                        module, lineno = pyname.get_definition_location()\n                        if module is not None:\n                            resource = module.get_module().get_resource()\n                            if resource is not None and lineno is not None:\n                                result.append((resource, lineno))\n                except exceptions.ModuleNotFoundError:\n                    pass\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates global name cache for project files.", "response": "def generate_cache(self, resources=None, underlined=None,\n                       task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Generate global name cache for project files\n\n        If `resources` is a list of `rope.base.resource.File`\\s, only\n        those files are searched; otherwise all python modules in the\n        project are cached.\n\n        \"\"\"\n        if resources is None:\n            resources = self.project.get_python_files()\n        job_set = task_handle.create_jobset(\n            'Generatig autoimport cache', len(resources))\n        for file in resources:\n            job_set.started_job('Working on <%s>' % file.path)\n            self.update_resource(file, underlined)\n            job_set.finished_job()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates global name cache for modules listed in modules", "response": "def generate_modules_cache(self, modules, underlined=None,\n                               task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Generate global name cache for modules listed in `modules`\"\"\"\n        job_set = task_handle.create_jobset(\n            'Generatig autoimport cache for modules', len(modules))\n        for modname in modules:\n            job_set.started_job('Working on <%s>' % modname)\n            if modname.endswith('.*'):\n                mod = self.project.find_module(modname[:-2])\n                if mod:\n                    for sub in submodules(mod):\n                        self.update_resource(sub, underlined)\n            else:\n                self.update_module(modname, underlined)\n            job_set.finished_job()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nguess at what line the new import should be inserted", "response": "def find_insertion_line(self, code):\n        \"\"\"Guess at what line the new import should be inserted\"\"\"\n        match = re.search(r'^(def|class)\\s+', code)\n        if match is not None:\n            code = code[:match.start()]\n        try:\n            pymodule = libutils.get_string_module(self.project, code)\n        except exceptions.ModuleSyntaxError:\n            return 1\n        testmodname = '__rope_testmodule_rope'\n        importinfo = importutils.NormalImport(((testmodname, None),))\n        module_imports = importutils.get_module_imports(self.project,\n                                                        pymodule)\n        module_imports.add_import(importinfo)\n        code = module_imports.get_changed_source()\n        offset = code.index(testmodname)\n        lineno = code.count('\\n', 0, offset) + 1\n        return lineno"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_resource(self, resource, underlined=None):\n        try:\n            pymodule = self.project.get_pymodule(resource)\n            modname = self._module_name(resource)\n            self._add_names(pymodule, modname, underlined)\n        except exceptions.ModuleSyntaxError:\n            pass", "response": "Update the cache for global names in resource"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the cache for global names in modname.", "response": "def update_module(self, modname, underlined=None):\n        \"\"\"Update the cache for global names in `modname` module\n\n        `modname` is the name of a module.\n        \"\"\"\n        try:\n            pymodule = self.project.get_module(modname)\n            self._add_names(pymodule, modname, underlined)\n        except exceptions.ModuleNotFoundError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nuse by other refactorings", "response": "def replace(code, pattern, goal):\n    \"\"\"used by other refactorings\"\"\"\n    finder = similarfinder.RawSimilarFinder(code)\n    matches = list(finder.get_matches(pattern))\n    ast = patchedast.get_patched_ast(code)\n    lines = codeanalyze.SourceLinesAdapter(code)\n    template = similarfinder.CodeTemplate(goal)\n    computer = _ChangeComputer(code, ast, lines, template, matches)\n    result = computer.get_changed()\n    if result is None:\n        return code\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_changes(self, checks=None, imports=None, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get the changes needed by this restructuring\n\n        `resources` can be a list of `rope.base.resources.File`\\s to\n        apply the restructuring on.  If `None`, the restructuring will\n        be applied to all python files.\n\n        `checks` argument has been deprecated.  Use the `args` argument\n        of the constructor.  The usage of::\n\n          strchecks = {'obj1.type': 'mod.A', 'obj2': 'mod.B',\n                       'obj3.object': 'mod.C'}\n          checks = restructuring.make_checks(strchecks)\n\n        can be replaced with::\n\n          args = {'obj1': 'type=mod.A', 'obj2': 'name=mod.B',\n                  'obj3': 'object=mod.C'}\n\n        where obj1, obj2 and obj3 are wildcard names that appear\n        in restructuring pattern.\n\n        \"\"\"\n        if checks is not None:\n            warnings.warn(\n                'The use of checks parameter is deprecated; '\n                'use the args parameter of the constructor instead.',\n                DeprecationWarning, stacklevel=2)\n            for name, value in checks.items():\n                self.args[name] = similarfinder._pydefined_to_str(value)\n        if imports is not None:\n            warnings.warn(\n                'The use of imports parameter is deprecated; '\n                'use imports parameter of the constructor, instead.',\n                DeprecationWarning, stacklevel=2)\n            self.imports = imports\n        changes = change.ChangeSet('Restructuring <%s> to <%s>' %\n                                   (self.pattern, self.goal))\n        if resources is not None:\n            files = [resource for resource in resources\n                     if libutils.is_python_file(self.project, resource)]\n        else:\n            files = self.project.get_python_files()\n        job_set = task_handle.create_jobset('Collecting Changes', len(files))\n        for resource in files:\n            job_set.started_job(resource.path)\n            pymodule = self.project.get_pymodule(resource)\n            finder = similarfinder.SimilarFinder(pymodule,\n                                                 wildcards=self.wildcards)\n            matches = list(finder.get_matches(self.pattern, self.args))\n            computer = self._compute_changes(matches, pymodule)\n            result = computer.get_changed()\n            if result is not None:\n                imported_source = self._add_imports(resource, result,\n                                                    self.imports)\n                changes.add_change(change.ChangeContents(resource,\n                                                         imported_source))\n            job_set.finished_job()\n        return changes", "response": "Get the changes needed by this restructuring."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting str to str dicts to PyObject dicts", "response": "def make_checks(self, string_checks):\n        \"\"\"Convert str to str dicts to str to PyObject dicts\n\n        This function is here to ease writing a UI.\n\n        \"\"\"\n        checks = {}\n        for key, value in string_checks.items():\n            is_pyname = not key.endswith('.object') and \\\n                not key.endswith('.type')\n            evaluated = self._evaluate(value, is_pyname=is_pyname)\n            if evaluated is not None:\n                checks[key] = evaluated\n        return checks"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the changes this refactoring makes", "response": "def get_changes(self, extracted_name, similar=False, global_=False):\n        \"\"\"Get the changes this refactoring makes\n\n        :parameters:\n            - `similar`: if `True`, similar expressions/statements are also\n              replaced.\n            - `global_`: if `True`, the extracted method/variable will\n              be global.\n\n        \"\"\"\n        info = _ExtractInfo(\n            self.project, self.resource, self.start_offset, self.end_offset,\n            extracted_name, variable=self.kind == 'variable',\n            similar=similar, make_global=global_)\n        new_contents = _ExtractPerformer(info).extract()\n        changes = ChangeSet('Extract %s <%s>' % (self.kind,\n                                                 extracted_name))\n        changes.add_change(ChangeContents(self.resource, new_contents))\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes the extracted piece contain return statement?", "response": "def returned(self):\n        \"\"\"Does the extracted piece contain return statement\"\"\"\n        if self._returned is None:\n            node = _parse_text(self.extracted)\n            self._returned = usefunction._returns_last(node)\n        return self._returned"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of names and level tuples for assigned names", "response": "def get_name_levels(node):\n    \"\"\"Return a list of ``(name, level)`` tuples for assigned names\n\n    The `level` is `None` for simple assignments and is a list of\n    numbers for tuple assignments for example in::\n\n      a, (b, c) = x\n\n    The levels for for `a` is ``[0]``, for `b` is ``[1, 0]`` and for\n    `c` is ``[1, 1]``.\n\n    \"\"\"\n    visitor = _NodeNameCollector()\n    ast.walk(node, visitor)\n    return visitor.names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_pymodule(self):\n        msg = None\n        code = self.code\n        tries = 0\n        while True:\n            try:\n                if tries == 0 and self.resource is not None and \\\n                   self.resource.read() == code:\n                    return self.project.get_pymodule(self.resource,\n                                                     force_errors=True)\n                return libutils.get_string_module(\n                    self.project, code, resource=self.resource,\n                    force_errors=True)\n            except exceptions.ModuleSyntaxError as e:\n                if msg is None:\n                    msg = '%s:%s %s' % (e.filename, e.lineno, e.message_)\n                if tries < self.maxfixes:\n                    tries += 1\n                    self.commenter.comment(e.lineno)\n                    code = '\\n'.join(self.commenter.lines)\n                else:\n                    raise exceptions.ModuleSyntaxError(\n                        e.filename, e.lineno,\n                        'Failed to fix error: {0}'.format(msg))", "response": "Get a PyModule object from the code."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the number of basic changes a Change will make", "response": "def count_changes(change):\n    \"\"\"Counts the number of basic changes a `Change` will make\"\"\"\n    if isinstance(change, ChangeSet):\n        result = 0\n        for child in change.changes:\n            result += count_changes(child)\n        return result\n    return 1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the assignment of the specified class.", "response": "def get_super_assignment(pyname):\n    \"\"\"\n    :type pyname: rope.base.pynamesdef.AssignedName\n    :type: rope.base.pynamesdef.AssignedName\n    \"\"\"\n    try:\n        pyclass, attr_name = get_class_with_attr_name(pyname)\n    except TypeError:\n        return\n    else:\n        for super_pyclass in get_mro(pyclass)[1:]:\n            if attr_name in super_pyclass:\n                return super_pyclass[attr_name]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_class_with_attr_name(pyname):\n    lineno = get_lineno_for_node(pyname.assignments[0].ast_node)\n    holding_scope = pyname.module.get_scope().get_inner_scope_for_line(lineno)\n    pyobject = holding_scope.pyobject\n    if isinstance(pyobject, PyClass):\n        pyclass = pyobject\n    elif (isinstance(pyobject, PyFunction) and\n          isinstance(pyobject.parent, PyClass)):\n        pyclass = pyobject.parent\n    else:\n        return\n    for name, attr in pyclass.get_attributes().items():\n        if attr is pyname:\n            return (pyclass, name)", "response": "Returns the class and attribute name for the given pyname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresolve a type name to a resource object.", "response": "def resolve_type(type_name, pyobject):\n    \"\"\"\n    :type type_name: str\n    :type pyobject: rope.base.pyobjects.PyDefinedObject | rope.base.pyobjects.PyObject\n    :rtype: rope.base.pyobjects.PyDefinedObject | rope.base.pyobjects.PyObject or None\n    \"\"\"\n    if '.' not in type_name:\n        try:\n            return pyobject.get_module().get_scope().get_name(type_name).get_object()\n        except Exception:\n            pass\n    else:\n        mod_name, attr_name = type_name.rsplit('.', 1)\n        try:\n            mod_finder = ScopeNameFinder(pyobject.get_module())\n            mod = mod_finder._find_module(mod_name).get_object()\n            return mod.get_attribute(attr_name).get_object()\n        except Exception:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nanalyzes a module for static object inference.", "response": "def analyze_module(pycore, pymodule, should_analyze,\n                   search_subscopes, followed_calls):\n    \"\"\"Analyze `pymodule` for static object inference\n\n    Analyzes scopes for collecting object information.  The analysis\n    starts from inner scopes.\n\n    \"\"\"\n    _analyze_node(pycore, pymodule, should_analyze,\n                  search_subscopes, followed_calls)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_changes(self, changers, in_hierarchy=False, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get changes caused by this refactoring\n\n        `changers` is a list of `_ArgumentChanger`\\s.  If `in_hierarchy`\n        is `True` the changers are applyed to all matching methods in\n        the class hierarchy.\n        `resources` can be a list of `rope.base.resource.File`\\s that\n        should be searched for occurrences; if `None` all python files\n        in the project are searched.\n\n        \"\"\"\n        function_changer = _FunctionChangers(self.pyname.get_object(),\n                                             self._definfo(), changers)\n        return self._change_calls(function_changer, in_hierarchy,\n                                  resources, task_handle)", "response": "Get the changes caused by this refactoring."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of changes that should be applied to the given module names.", "response": "def get_changes(self, fixer=str.lower,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Fix module names\n\n        `fixer` is a function that takes and returns a `str`.  Given\n        the name of a module, it should return the fixed name.\n\n        \"\"\"\n        stack = changestack.ChangeStack(self.project, 'Fixing module names')\n        jobset = task_handle.create_jobset('Fixing module names',\n                                           self._count_fixes(fixer) + 1)\n        try:\n            while True:\n                for resource in self._tobe_fixed(fixer):\n                    jobset.started_job(resource.path)\n                    renamer = rename.Rename(self.project, resource)\n                    changes = renamer.get_changes(fixer(self._name(resource)))\n                    stack.push(changes)\n                    jobset.finished_job()\n                    break\n                else:\n                    break\n        finally:\n            jobset.started_job('Reverting to original state')\n            stack.pop_all()\n            jobset.finished_job()\n        return stack.merged()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef infer_returned_object(pyfunction, args):\n    object_info = pyfunction.pycore.object_info\n    result = object_info.get_exact_returned(pyfunction, args)\n    if result is not None:\n        return result\n    result = _infer_returned(pyfunction, args)\n    if result is not None:\n        if args and pyfunction.get_module().get_resource() is not None:\n            params = args.get_arguments(\n                pyfunction.get_param_names(special_args=False))\n            object_info.function_called(pyfunction, params, result)\n        return result\n    result = object_info.get_returned(pyfunction, args)\n    if result is not None:\n        return result\n    hint_return = get_type_hinting_factory(pyfunction.pycore.project).make_return_provider()\n    type_ = hint_return(pyfunction)\n    if type_ is not None:\n        return rope.base.pyobjects.PyObject(type_)", "response": "Infer the PyObject this PyFunction returns after calling"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef infer_parameter_objects(pyfunction):\n    object_info = pyfunction.pycore.object_info\n    result = object_info.get_parameter_objects(pyfunction)\n    if result is None:\n        result = _parameter_objects(pyfunction)\n    _handle_first_parameter(pyfunction, result)\n    return result", "response": "Infer the PyObject \\ s of parameters of this PyFunction"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nnormalizing so name to be a valid python file name.", "response": "def normalize_so_name(name):\n    \"\"\"\n    Handle different types of python installations\n    \"\"\"\n    if \"cpython\" in name:\n        return os.path.splitext(os.path.splitext(name)[0])[0]\n    # XXX: Special handling for Fedora python2 distribution\n    # See: https://github.com/python-rope/rope/issues/211\n    if name == \"timemodule.so\":\n        return \"time\"\n    return os.path.splitext(name)[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for the process to finish", "response": "def wait_process(self):\n        \"\"\"Wait for the process to finish\"\"\"\n        self.process.wait()\n        if self.analyze_data:\n            self.receiving_thread.join()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsearching for code in source and return a list of Match \\ es instances.", "response": "def get_matches(self, code, start=0, end=None, skip=None):\n        \"\"\"Search for `code` in source and return a list of `Match`\\es\n\n        `code` can contain wildcards.  ``${name}`` matches normal\n        names and ``${?name} can match any expression.  You can use\n        `Match.get_ast()` for getting the node that has matched a\n        given pattern.\n\n        \"\"\"\n        if end is None:\n            end = len(self.source)\n        for match in self._get_matched_asts(code):\n            match_start, match_end = match.get_region()\n            if start <= match_start and match_end <= end:\n                if skip is not None and (skip[0] < match_end and\n                                         skip[1] > match_start):\n                    continue\n                yield match"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns not ast. expr_context children of node", "response": "def _get_children(self, node):\n        \"\"\"Return not `ast.expr_context` children of `node`\"\"\"\n        children = ast.get_children(node)\n        return [child for child in children\n                if not isinstance(child, ast.expr_context)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_imports(project, pydefined):\n    pymodule = pydefined.get_module()\n    module = module_imports.ModuleImports(project, pymodule)\n    if pymodule == pydefined:\n        return [stmt.import_info for stmt in module.imports]\n    return module.get_used_imports(pydefined)", "response": "A shortcut for getting the ImportInfo \\ s used in a scope"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new Finder object based on the arguments it creates a list of filters.", "response": "def create_finder(project, name, pyname, only_calls=False, imports=True,\n                  unsure=None, docs=False, instance=None, in_hierarchy=False,\n                  keywords=True):\n    \"\"\"A factory for `Finder`\n\n    Based on the arguments it creates a list of filters.  `instance`\n    argument is needed only when you want implicit interfaces to be\n    considered.\n\n    \"\"\"\n    pynames_ = set([pyname])\n    filters = []\n    if only_calls:\n        filters.append(CallsFilter())\n    if not imports:\n        filters.append(NoImportsFilter())\n    if not keywords:\n        filters.append(NoKeywordsFilter())\n    if isinstance(instance, pynames.ParameterName):\n        for pyobject in instance.get_objects():\n            try:\n                pynames_.add(pyobject[name])\n            except exceptions.AttributeNotFoundError:\n                pass\n    for pyname in pynames_:\n        filters.append(PyNameFilter(pyname))\n        if in_hierarchy:\n            filters.append(InHierarchyFilter(pyname))\n    if unsure:\n        filters.append(UnsureFilter(unsure))\n    return Finder(project, name, filters=filters, docs=docs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef same_pyname(expected, pyname):\n    if expected is None or pyname is None:\n        return False\n    if expected == pyname:\n        return True\n    if type(expected) not in (pynames.ImportedModule, pynames.ImportedName) \\\n        and type(pyname) not in \\\n            (pynames.ImportedModule, pynames.ImportedName):\n        return False\n    return expected.get_definition_location() == \\\n        pyname.get_definition_location() and \\\n        expected.get_object() == pyname.get_object()", "response": "Check whether expected and pyname are the same"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if the given PyName is unsure.", "response": "def unsure_pyname(pyname, unbound=True):\n    \"\"\"Return `True` if we don't know what this name references\"\"\"\n    if pyname is None:\n        return True\n    if unbound and not isinstance(pyname, pynames.UnboundName):\n        return False\n    if pyname.get_object() == pyobjects.get_unknown():\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding the Occurrence instances that match the filters.", "response": "def find_occurrences(self, resource=None, pymodule=None):\n        \"\"\"Generate `Occurrence` instances\"\"\"\n        tools = _OccurrenceToolsCreator(self.project, resource=resource,\n                                        pymodule=pymodule, docs=self.docs)\n        for offset in self._textual_finder.find_offsets(tools.source_code):\n            occurrence = Occurrence(tools, offset)\n            for filter in self.filters:\n                result = filter(occurrence)\n                if result is None:\n                    continue\n                if result:\n                    yield occurrence\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_generate(kind, project, resource, offset):\n    generate = eval('Generate' + kind.title())\n    return generate(project, resource, offset)", "response": "A factory for creating objects\n    objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a module and returns a rope. base. resources. File object", "response": "def create_module(project, name, sourcefolder=None):\n    \"\"\"Creates a module and returns a `rope.base.resources.File`\"\"\"\n    if sourcefolder is None:\n        sourcefolder = project.root\n    packages = name.split('.')\n    parent = sourcefolder\n    for package in packages[:-1]:\n        parent = parent.get_child(package)\n    return parent.create_file(packages[-1] + '.py')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_package(project, name, sourcefolder=None):\n    if sourcefolder is None:\n        sourcefolder = project.root\n    packages = name.split('.')\n    parent = sourcefolder\n    for package in packages[:-1]:\n        parent = parent.get_child(package)\n    made_packages = parent.create_folder(packages[-1])\n    made_packages.create_file('__init__.py')\n    return made_packages", "response": "Creates a package and returns a folder"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrenaming the name of the occurrence in the specified module.", "response": "def rename_in_module(occurrences_finder, new_name, resource=None,\n                     pymodule=None, replace_primary=False, region=None,\n                     reads=True, writes=True):\n    \"\"\"Returns the changed source or `None` if there is no changes\"\"\"\n    if resource is not None:\n        source_code = resource.read()\n    else:\n        source_code = pymodule.source_code\n    change_collector = codeanalyze.ChangeCollector(source_code)\n    for occurrence in occurrences_finder.find_occurrences(resource, pymodule):\n        if replace_primary and occurrence.is_a_fixed_primary():\n            continue\n        if replace_primary:\n            start, end = occurrence.get_primary_range()\n        else:\n            start, end = occurrence.get_word_range()\n        if (not reads and not occurrence.is_written()) or \\\n           (not writes and occurrence.is_written()):\n            continue\n        if region is None or region[0] <= start < region[1]:\n            change_collector.add_change(start, end, new_name)\n    return change_collector.get_changed()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of ChangeSets that need to be applied to the current refactoring.", "response": "def get_changes(self, new_name, in_file=None, in_hierarchy=False,\n                    unsure=None, docs=False, resources=None,\n                    task_handle=taskhandle.NullTaskHandle()):\n        \"\"\"Get the changes needed for this refactoring\n\n        Parameters:\n\n        - `in_hierarchy`: when renaming a method this keyword forces\n          to rename all matching methods in the hierarchy\n        - `docs`: when `True` rename refactoring will rename\n          occurrences in comments and strings where the name is\n          visible.  Setting it will make renames faster, too.\n        - `unsure`: decides what to do about unsure occurrences.\n          If `None`, they are ignored.  Otherwise `unsure` is\n          called with an instance of `occurrence.Occurrence` as\n          parameter.  If it returns `True`, the occurrence is\n          considered to be a match.\n        - `resources` can be a list of `rope.base.resources.File`\\s to\n          apply this refactoring on.  If `None`, the restructuring\n          will be applied to all python files.\n        - `in_file`: this argument has been deprecated; use\n          `resources` instead.\n\n        \"\"\"\n        if unsure in (True, False):\n            warnings.warn(\n                'unsure parameter should be a function that returns '\n                'True or False', DeprecationWarning, stacklevel=2)\n\n            def unsure_func(value=unsure):\n                return value\n            unsure = unsure_func\n        if in_file is not None:\n            warnings.warn(\n                '`in_file` argument has been deprecated; use `resources` '\n                'instead. ', DeprecationWarning, stacklevel=2)\n            if in_file:\n                resources = [self.resource]\n        if _is_local(self.old_pyname):\n            resources = [self.resource]\n        if resources is None:\n            resources = self.project.get_python_files()\n        changes = ChangeSet('Renaming <%s> to <%s>' %\n                            (self.old_name, new_name))\n        finder = occurrences.create_finder(\n            self.project, self.old_name, self.old_pyname, unsure=unsure,\n            docs=docs, instance=self.old_instance,\n            in_hierarchy=in_hierarchy and self.is_method())\n        job_set = task_handle.create_jobset('Collecting Changes',\n                                            len(resources))\n        for file_ in resources:\n            job_set.started_job(file_.path)\n            new_content = rename_in_module(finder, new_name, resource=file_)\n            if new_content is not None:\n                changes.add_change(ChangeContents(file_, new_content))\n            job_set.finished_job()\n        if self._is_renaming_a_module():\n            resource = self.old_pyname.get_object().get_resource()\n            if self._is_allowed_to_move(resources, resource):\n                self._rename_module(resource, new_name, changes)\n        return changes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef real_code(source):\n    collector = codeanalyze.ChangeCollector(source)\n    for start, end in ignored_regions(source):\n        if source[start] == '#':\n            replacement = ' ' * (end - start)\n        else:\n            replacement = '\"%s\"' % (' ' * (end - start - 2))\n        collector.add_change(start, end, replacement)\n    source = collector.get_changed() or source\n    collector = codeanalyze.ChangeCollector(source)\n    parens = 0\n    for match in _parens.finditer(source):\n        i = match.start()\n        c = match.group()\n        if c in '({[':\n            parens += 1\n        if c in ')}]':\n            parens -= 1\n        if c == '\\n' and parens > 0:\n            collector.add_change(i, i + 1, ' ')\n    source = collector.get_changed() or source\n    return source.replace('\\\\\\n', '  ').replace('\\t', ' ').replace(';', '\\n')", "response": "Simplify source for analysis\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ignored_regions(source):\n    return [(match.start(), match.end()) for match in _str.finditer(source)]", "response": "Return a list of tuples containing start and end positions of all ignored regions in source"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef move(self, new_location):\n        self._perform_change(change.MoveResource(self, new_location),\n                             'Moving <%s> to <%s>' % (self.path, new_location))", "response": "Move resource to new location."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the children of this folder", "response": "def get_children(self):\n        \"\"\"Return the children of this folder\"\"\"\n        try:\n            children = os.listdir(self.real_path)\n        except OSError:\n            return []\n        result = []\n        for name in children:\n            try:\n                child = self.get_child(name)\n            except exceptions.ResourceNotFoundError:\n                continue\n            if not self.project.is_ignored(child):\n                result.append(self.get_child(name))\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the proper indexes for the proper fields of the current object.", "response": "def _create_indexes(self):\n        \"\"\"Ensures the proper fields are indexed\"\"\"\n        # WARNING: The collection will be locked during the index\n        # creation. If the collection has a large number of\n        # documents in it, the operation can take a long time.\n\n        # TODO: The creation of indexes can be moved to a Django\n        # management command or equivalent. There is also an option to\n        # run the indexing on the background, without locking.\n        self.collection.ensure_index([('time', pymongo.DESCENDING)])\n        self.collection.ensure_index('name')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, event):\n        try:\n            self.collection.insert(event, manipulate=False)\n        except (PyMongoError, BSONError):\n            # The event will be lost in case of a connection error or any error\n            # that occurs when trying to insert the event into Mongo.\n            # pymongo will re-connect/re-authenticate automatically\n            # during the next event.\n            msg = 'Error inserting to MongoDB event tracker backend'\n            log.exception(msg)", "response": "Insert the event into the Mongo collection and log an error if it occurs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_backends_from_settings(self):\n        config = getattr(settings, DJANGO_BACKEND_SETTING_NAME, {})\n\n        backends = self.instantiate_objects(config)\n\n        return backends", "response": "Creates a dictionary of backend engine configurations and returns them"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct an object given a dictionary containing an ENGINE key which contains the module path to the class and an OPTIONS key which contains the dictionary that will be passed in to the constructor as keyword args.", "response": "def instantiate_from_dict(self, values):\n        \"\"\"\n        Constructs an object given a dictionary containing an \"ENGINE\" key\n        which contains the full module path to the class, and an \"OPTIONS\"\n        key which contains a dictionary that will be passed in to the\n        constructor as keyword args.\n        \"\"\"\n\n        name = values['ENGINE']\n        options = values.get('OPTIONS', {})\n\n        # Parse the name\n        parts = name.split('.')\n        module_name = '.'.join(parts[:-1])\n        class_name = parts[-1]\n\n        # Get the class\n        try:\n            module = import_module(module_name)\n            cls = getattr(module, class_name)\n        except (ValueError, AttributeError, TypeError, ImportError):\n            raise ValueError('Cannot find class %s' % name)\n\n        options = self.instantiate_objects(options)\n\n        return cls(**options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_processors_from_settings(self):\n        config = getattr(settings, DJANGO_PROCESSOR_SETTING_NAME, [])\n\n        processors = self.instantiate_objects(config)\n\n        return processors", "response": "Creates a list of processors from the Django settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a new backend that will be called for each processed event.", "response": "def register_backend(self, name, backend):\n        \"\"\"\n        Register a new backend that will be called for each processed event.\n\n        Note that backends are called in the order that they are registered.\n        \"\"\"\n        if not hasattr(backend, 'send') or not callable(backend.send):\n            raise ValueError('Backend %s does not have a callable \"send\" method.' % backend.__class__.__name__)\n        else:\n            self.backends[name] = backend"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister a new processor.", "response": "def register_processor(self, processor):\n        \"\"\"\n        Register a new processor.\n\n        Note that processors are called in the order that they are registered.\n        \"\"\"\n        if not callable(processor):\n            raise ValueError('Processor %s is not callable.' % processor.__class__.__name__)\n        else:\n            self.processors.append(processor)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, event):\n        try:\n            processed_event = self.process_event(event)\n        except EventEmissionExit:\n            return\n        else:\n            self.send_to_backends(processed_event)", "response": "Process the event and send it to all registered backends."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_to_backends(self, event):\n\n        for name, backend in six.iteritems(self.backends):\n            try:\n                backend.send(event)\n            except Exception:  # pylint: disable=broad-except\n                LOG.exception(\n                    'Unable to send event to backend: %s', name\n                )", "response": "Sends the event to all registered backends."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef emit(self, name=None, data=None):\n        event = {\n            'name': name or UNKNOWN_EVENT_TYPE,\n            'timestamp': datetime.now(UTC),\n            'data': data or {},\n            'context': self.resolve_context()\n        }\n\n        self.routing_backend.send(event)", "response": "Emit an event to the routing backend."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_context(self):\n        merged = dict()\n        for context in self.located_context.values():\n            merged.update(context)\n        return merged", "response": "Create a new dictionary that corresponds to all of the context that has been entered but not exited at this point."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef context(self, name, ctx):\n        self.enter_context(name, ctx)\n        try:\n            yield\n        finally:\n            self.exit_context(name)", "response": "Execute the block with the given context applied."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend an event to segment. com.", "response": "def send(self, event):\n        \"\"\"Use the segment.com python API to send the event to segment.com\"\"\"\n        if analytics is None:\n            return\n\n        context = event.get('context', {})\n        user_id = context.get('user_id')\n        name = event.get('name')\n        if name is None or user_id is None:\n            return\n\n        segment_context = {}\n\n        ga_client_id = context.get('client_id')\n        if ga_client_id is not None:\n            segment_context['Google Analytics'] = {\n                'clientId': ga_client_id\n            }\n        ip_address = context.get('ip')\n        if ip_address is not None:\n            segment_context['ip'] = ip_address\n        user_agent = context.get('agent')\n        if user_agent is not None:\n            segment_context['userAgent'] = user_agent\n        path = context.get('path')\n        referer = context.get('referer')\n        page = context.get('page')\n\n        if path and not page:\n            # Try to put together a url from host and path, hardcoding the schema.\n            # (Segment doesn't care about the schema for GA, but will extract the host and path from the url.)\n            host = context.get('host')\n            if host:\n                parts = (\"https\", host, path, \"\", \"\")\n                page = urlunsplit(parts)\n\n        if path is not None or referer is not None or page is not None:\n            segment_context['page'] = {}\n            if path is not None:\n                segment_context['page']['path'] = path\n            if referer is not None:\n                segment_context['page']['referrer'] = referer\n            if page is not None:\n                segment_context['page']['url'] = page\n\n        analytics.track(\n            user_id,\n            name,\n            event,\n            context=segment_context\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a reference to a thread - specific context", "response": "def get(self):\n        \"\"\"Return a reference to a thread-specific context\"\"\"\n        if not self.thread_local_data:\n            self.thread_local_data = threading.local()\n\n        if not hasattr(self.thread_local_data, 'context'):\n            self.thread_local_data.context = OrderedDict()\n\n        return self.thread_local_data.context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, event):\n        event_str = json.dumps(event, cls=DateTimeJSONEncoder)\n\n        # TODO: do something smarter than simply dropping the event on\n        # the floor.\n        if self.max_event_size is None or len(event_str) <= self.max_event_size:\n            self.log(event_str)", "response": "Send the event to the standard python logger"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing datetime and date objects of iso format.", "response": "def default(self, obj):  # lint-amnesty, pylint: disable=arguments-differ, method-hidden\n        \"\"\"\n        Serialize datetime and date objects of iso format.\n\n        datatime objects are converted to UTC.\n        \"\"\"\n\n        if isinstance(obj, datetime):\n            if obj.tzinfo is None:\n                # Localize to UTC naive datetime objects\n                obj = UTC.localize(obj)  # pylint: disable=no-value-for-parameter\n            else:\n                # Convert to UTC datetime objects from other timezones\n                obj = obj.astimezone(UTC)\n            return obj.isoformat()\n        elif isinstance(obj, date):\n            return obj.isoformat()\n\n        return super(DateTimeJSONEncoder, self).default(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_html_theme_path():\n    theme_path = os.path.abspath(os.path.dirname(__file__))\n    return [theme_path]", "response": "Return list of HTML theme paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _dist_wrapper():\n    try:\n        # Copy select *.rst files to *.txt for build.\n        for rst_file, txt_file in zip(SDIST_RST_FILES, SDIST_TXT_FILES):\n            local(\"cp %s %s\" % (rst_file, txt_file))\n\n        # Perform action.\n        yield\n    finally:\n        # Clean up temp *.txt files.\n        for rst_file in SDIST_TXT_FILES:\n            local(\"rm -f %s\" % rst_file, capture=False)", "response": "Add temporary distribution build files and then clean up."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting build revision. @param tag Use git tag instead of hash?", "response": "def get_rev(tag=True):\n    \"\"\"Get build revision.\n\n    @param tag  Use git tag instead of hash?\n    \"\"\"\n    rev_cmd = \"git describe --always --tag\" if tag in (True, \"True\") else \\\n              \"git rev-parse HEAD\"\n    return local(rev_cmd, capture=True).strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate zip file upload bundles.", "response": "def zip_bundle(tag=True):\n    \"\"\"Create zip file upload bundles.\n\n    @param tag  Use git tag instead of hash?\n    \"\"\"\n    #rev = get_rev(tag)\n    rev = __version__\n\n    print(\"Cleaning old build files.\")\n    clean()\n\n    local(\"mkdir -p build\")\n\n    print(\"Bundling new files.\")\n    with lcd(\"sphinx_bootstrap_theme/bootstrap\"):\n        local(\"zip -r ../../build/bootstrap.zip .\")\n\n    dest = os.path.abspath(os.path.join(DL_DIR, rev))\n    with lcd(\"build\"):\n        local(\"mkdir -p %s\" % dest)\n        local(\"cp bootstrap.zip %s\" % dest)\n\n        print(\"Verifying contents.\")\n        local(\"unzip -l bootstrap.zip\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading file name without extension to string.", "response": "def read_file(name):\n    \"\"\"Read file name (without extension) to string.\"\"\"\n    cur_path = os.path.dirname(__file__)\n    exts = ('txt', 'rst')\n    for ext in exts:\n        path = os.path.join(cur_path, '.'.join((name, ext)))\n        if os.path.exists(path):\n            with open(path, 'rt') as file_obj:\n                return file_obj.read()\n\n    return ''"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the message for the recipient and creates the message for all possible recipients.", "response": "def save_model(self, request, obj, form, change):\n        \"\"\"\n        Saves the message for the recipient and looks in the form instance\n        for other possible recipients. Prevents duplication by excludin the\n        original recipient from the list of optional recipients.\n\n        When changing an existing message and choosing optional recipients,\n        the message is effectively resent to those users.\n        \"\"\"\n        obj.save()\n\n        if notification:\n            # Getting the appropriate notice labels for the sender and recipients.\n            if obj.parent_msg is None:\n                sender_label = 'messages_sent'\n                recipients_label = 'messages_received'\n            else:\n                sender_label = 'messages_replied'\n                recipients_label = 'messages_reply_received'\n\n            # Notification for the sender.\n            notification.send([obj.sender], sender_label, {'message': obj,})\n\n        if form.cleaned_data['group'] == 'all':\n            # send to all users\n            recipients = User.objects.exclude(pk=obj.recipient.pk)\n        else:\n            # send to a group of users\n            recipients = []\n            group = form.cleaned_data['group']\n            if group:\n                group = Group.objects.get(pk=group)\n                recipients.extend(\n                    list(group.user_set.exclude(pk=obj.recipient.pk)))\n        # create messages for all found recipients\n        for user in recipients:\n            obj.pk = None\n            obj.recipient = user\n            obj.save()\n\n            if notification:\n                # Notification for the recipient.\n                notification.send([user], recipients_label, {'message' : obj,})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the unread - count for a logged in user.", "response": "def do_print_inbox_count(parser, token):\n    \"\"\"\n    A templatetag to show the unread-count for a logged in user.\n    Returns the number of unread messages in the user's inbox.\n    Usage::\n\n        {% load inbox %}\n        {% inbox_count %}\n\n        {# or assign the value to a variable: #}\n\n        {% inbox_count as my_var %}\n        {{ my_var }}\n\n    \"\"\"\n    bits = token.contents.split()\n    if len(bits) > 1:\n        if len(bits) != 3:\n            raise TemplateSyntaxError(\"inbox_count tag takes either no arguments or exactly two arguments\")\n        if bits[1] != 'as':\n            raise TemplateSyntaxError(\"first argument to inbox_count tag must be 'as'\")\n        return InboxOutput(bits[2])\n    else:\n        return InboxOutput()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all messages that were either received or sent by the given user and are marked as deleted.", "response": "def trash_for(self, user):\n        \"\"\"\n        Returns all messages that were either received or sent by the given\n        user and are marked as deleted.\n        \"\"\"\n        return self.filter(\n            recipient=user,\n            recipient_deleted_at__isnull=False,\n        ) | self.filter(\n            sender=user,\n            sender_deleted_at__isnull=False,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisplaying a list of received messages for the current user.", "response": "def inbox(request, template_name='django_messages/inbox.html'):\n    \"\"\"\n    Displays a list of received messages for the current user.\n    Optional Arguments:\n        ``template_name``: name of the template to use.\n    \"\"\"\n    message_list = Message.objects.inbox_for(request.user)\n    return render(request, template_name, {\n        'message_list': message_list,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef outbox(request, template_name='django_messages/outbox.html'):\n    message_list = Message.objects.outbox_for(request.user)\n    return render(request, template_name, {\n        'message_list': message_list,\n    })", "response": "Displays a list of sent messages by the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay a list of deleted messages. Optional arguments: ``template_name``: name of the template to use Hint: A Cron-Job could periodicly clean up old messages, which are deleted by sender and recipient.", "response": "def trash(request, template_name='django_messages/trash.html'):\n    \"\"\"\n    Displays a list of deleted messages.\n    Optional arguments:\n        ``template_name``: name of the template to use\n    Hint: A Cron-Job could periodicly clean up old messages, which are deleted\n    by sender and recipient.\n    \"\"\"\n    message_list = Message.objects.trash_for(request.user)\n    return render(request, template_name, {\n        'message_list': message_list,\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compose(request, recipient=None, form_class=ComposeForm,\n        template_name='django_messages/compose.html', success_url=None, recipient_filter=None):\n    \"\"\"\n    Displays and handles the ``form_class`` form to compose new messages.\n    Required Arguments: None\n    Optional Arguments:\n        ``recipient``: username of a `django.contrib.auth` User, who should\n                       receive the message, optionally multiple usernames\n                       could be separated by a '+'\n        ``form_class``: the form-class to use\n        ``template_name``: the template to use\n        ``success_url``: where to redirect after successfull submission\n    \"\"\"\n    if request.method == \"POST\":\n        sender = request.user\n        form = form_class(request.POST, recipient_filter=recipient_filter)\n        if form.is_valid():\n            form.save(sender=request.user)\n            messages.info(request, _(u\"Message successfully sent.\"))\n            if success_url is None:\n                success_url = reverse('messages_inbox')\n            if 'next' in request.GET:\n                success_url = request.GET['next']\n            return HttpResponseRedirect(success_url)\n    else:\n        form = form_class()\n        if recipient is not None:\n            recipients = [u for u in User.objects.filter(**{'%s__in' % get_username_field(): [r.strip() for r in recipient.split('+')]})]\n            form.fields['recipient'].initial = recipients\n    return render(request, template_name, {\n        'form': form,\n    })", "response": "Displays and handles the compose form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reply(request, message_id, form_class=ComposeForm,\n        template_name='django_messages/compose.html', success_url=None,\n        recipient_filter=None, quote_helper=format_quote,\n        subject_template=_(u\"Re: %(subject)s\"),):\n    \"\"\"\n    Prepares the ``form_class`` form for writing a reply to a given message\n    (specified via ``message_id``). Uses the ``format_quote`` helper from\n    ``messages.utils`` to pre-format the quote. To change the quote format\n    assign a different ``quote_helper`` kwarg in your url-conf.\n\n    \"\"\"\n    parent = get_object_or_404(Message, id=message_id)\n\n    if parent.sender != request.user and parent.recipient != request.user:\n        raise Http404\n\n    if request.method == \"POST\":\n        sender = request.user\n        form = form_class(request.POST, recipient_filter=recipient_filter)\n        if form.is_valid():\n            form.save(sender=request.user, parent_msg=parent)\n            messages.info(request, _(u\"Message successfully sent.\"))\n            if success_url is None:\n                success_url = reverse('messages_inbox')\n            return HttpResponseRedirect(success_url)\n    else:\n        form = form_class(initial={\n            'body': quote_helper(parent.sender, parent.body),\n            'subject': subject_template % {'subject': parent.subject},\n            'recipient': [parent.sender,]\n            })\n    return render(request, template_name, {\n        'form': form,\n    })", "response": "Renders a reply to a message."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting a message from the database.", "response": "def delete(request, message_id, success_url=None):\n    \"\"\"\n    Marks a message as deleted by sender or recipient. The message is not\n    really removed from the database, because two users must delete a message\n    before it's save to remove it completely.\n    A cron-job should prune the database and remove old messages which are\n    deleted by both users.\n    As a side effect, this makes it easy to implement a trash with undelete.\n\n    You can pass ?next=/foo/bar/ via the url to redirect the user to a different\n    page (e.g. `/foo/bar/`) than ``success_url`` after deletion of the message.\n    \"\"\"\n    user = request.user\n    now = timezone.now()\n    message = get_object_or_404(Message, id=message_id)\n    deleted = False\n    if success_url is None:\n        success_url = reverse('messages_inbox')\n    if 'next' in request.GET:\n        success_url = request.GET['next']\n    if message.sender == user:\n        message.sender_deleted_at = now\n        deleted = True\n    if message.recipient == user:\n        message.recipient_deleted_at = now\n        deleted = True\n    if deleted:\n        message.save()\n        messages.info(request, _(u\"Message successfully deleted.\"))\n        if notification:\n            notification.send([user], \"messages_deleted\", {'message': message,})\n        return HttpResponseRedirect(success_url)\n    raise Http404"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef undelete(request, message_id, success_url=None):\n    user = request.user\n    message = get_object_or_404(Message, id=message_id)\n    undeleted = False\n    if success_url is None:\n        success_url = reverse('messages_inbox')\n    if 'next' in request.GET:\n        success_url = request.GET['next']\n    if message.sender == user:\n        message.sender_deleted_at = None\n        undeleted = True\n    if message.recipient == user:\n        message.recipient_deleted_at = None\n        undeleted = True\n    if undeleted:\n        message.save()\n        messages.info(request, _(u\"Message successfully recovered.\"))\n        if notification:\n            notification.send([user], \"messages_recovered\", {'message': message,})\n        return HttpResponseRedirect(success_url)\n    raise Http404", "response": "Undelete a message from trash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef view(request, message_id, form_class=ComposeForm, quote_helper=format_quote,\n        subject_template=_(u\"Re: %(subject)s\"),\n        template_name='django_messages/view.html'):\n    \"\"\"\n    Shows a single message.``message_id`` argument is required.\n    The user is only allowed to see the message, if he is either\n    the sender or the recipient. If the user is not allowed a 404\n    is raised.\n    If the user is the recipient and the message is unread\n    ``read_at`` is set to the current datetime.\n    If the user is the recipient a reply form will be added to the\n    tenplate context, otherwise 'reply_form' will be None.\n    \"\"\"\n    user = request.user\n    now = timezone.now()\n    message = get_object_or_404(Message, id=message_id)\n    if (message.sender != user) and (message.recipient != user):\n        raise Http404\n    if message.read_at is None and message.recipient == user:\n        message.read_at = now\n        message.save()\n\n    context = {'message': message, 'reply_form': None}\n    if message.recipient == user:\n        form = form_class(initial={\n            'body': quote_helper(message.sender, message.body),\n            'subject': subject_template % {'subject': message.subject},\n            'recipient': [message.sender,]\n            })\n        context['reply_form'] = form\n    return render(request, template_name, context)", "response": "Displays a single message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_quote(sender, body):\n    lines = wrap(body, 55).split('\\n')\n    for i, line in enumerate(lines):\n        lines[i] = \"> %s\" % line\n    quote = '\\n'.join(lines)\n    return ugettext(u\"%(sender)s wrote:\\n%(body)s\") % {\n        'sender': sender,\n        'body': quote\n    }", "response": "Formats the given text as a quote."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats a subject into a I18N - compliant string.", "response": "def format_subject(subject):\n    \"\"\"\n    Prepends 'Re:' to the subject. To avoid multiple 'Re:'s\n    a counter is added.\n    NOTE: Currently unused. First step to fix Issue #48.\n    FIXME: Any hints how to make this i18n aware are very welcome.\n\n    \"\"\"\n    subject_prefix_re = r'^Re\\[(\\d*)\\]:\\ '\n    m = re.match(subject_prefix_re, subject, re.U)\n    prefix = u\"\"\n    if subject.startswith('Re: '):\n        prefix = u\"[2]\"\n        subject = subject[4:]\n    elif m is not None:\n        try:\n            num = int(m.group(1))\n            prefix = u\"[%d]\" % (num+1)\n            subject = subject[6+len(str(num)):]\n        except:\n            # if anything fails here, fall back to the old mechanism\n            pass\n\n    return ugettext(u\"Re%(prefix)s: %(subject)s\") % {\n        'subject': subject,\n        'prefix': prefix\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_message_email(sender, instance, signal,\n        subject_prefix=_(u'New Message: %(subject)s'),\n        template_name=\"django_messages/new_message.html\",\n        default_protocol=None,\n        *args, **kwargs):\n    \"\"\"\n    This function sends an email and is called via Django's signal framework.\n    Optional arguments:\n        ``template_name``: the template to use\n        ``subject_prefix``: prefix for the email subject.\n        ``default_protocol``: default protocol in site URL passed to template\n    \"\"\"\n    if default_protocol is None:\n        default_protocol = getattr(settings, 'DEFAULT_HTTP_PROTOCOL', 'http')\n\n    if 'created' in kwargs and kwargs['created']:\n        try:\n            current_domain = Site.objects.get_current().domain\n            subject = subject_prefix % {'subject': instance.subject}\n            message = render_to_string(template_name, {\n                'site_url': '%s://%s' % (default_protocol, current_domain),\n                'message': instance,\n            })\n            if instance.recipient.email != \"\":\n                send_mail(subject, message, settings.DEFAULT_FROM_EMAIL,\n                    [instance.recipient.email,])\n        except Exception as e:\n            #print e\n            pass", "response": "This function sends an email to the user who created the message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, **params):\n        '''\n        Returns details for a specific airport.\n\n        .. code-block:: python\n\n            amadeus.reference_data.location('ALHR').get()\n\n        :rtype: amadeus.Response\n        :raises amadeus.ResponseError: if the request could not be completed\n        '''\n        return self.client.get('/v1/reference-data/locations/{0}'\n                               .format(self.location_id), **params)", "response": "Returns details for a specific airport."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef request(self, verb, path, **params):\n        '''\n        A helper function for making generic POST requests calls. It is used by\n        every namespaced API method. It can be used to make any generic API\n        call that is automatically authenticated using your API credentials:\n\n        .. code-block:: python\n\n          amadeus.request('GET', '/foo/bar', airline='1X')\n\n        :param verb: the HTTP verb to use\n        :paramtype verb: str\n\n        :param path: path the full path for the API call\n        :paramtype path: str\n\n        :param params: (optional) params to pass to the API\n        :paramtype params: dict\n\n        :rtype: amadeus.Response\n        :raises amadeus.ResponseError: when the request fails\n        '''\n        return self._unauthenticated_request(\n            verb, path, params,\n            self.__access_token()._bearer_token()\n        )", "response": "A generic POST request that uses the access token and bearer token to make the API call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, **params):\n        '''\n        Returns details for a specific offer.\n\n        .. code-block:: python\n\n            amadeus.shopping.hotel_offer('XXX').get\n\n        :rtype: amadeus.Response\n        :raises amadeus.ResponseError: if the request could not be completed\n        '''\n        return self.client.get('/v2/shopping/hotel-offers/{0}'\n                               .format(self.offer_id), **params)", "response": "Returns details for a specific offer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef buffer_typechecks(self, call_id, payload):\n        if self.currently_buffering_typechecks:\n            for note in payload['notes']:\n                self.buffered_notes.append(note)", "response": "Adds typecheck events to the buffer"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef buffer_typechecks_and_display(self, call_id, payload):\n        self.buffer_typechecks(call_id, payload)\n        self.editor.display_notes(self.buffered_notes)", "response": "Adds typecheck events to the buffer and displays them right away."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle a typecheck_complete event.", "response": "def handle_typecheck_complete(self, call_id, payload):\n        \"\"\"Handles ``NewScalaNotesEvent```.\n\n        Calls editor to display/highlight line notes and clears notes buffer.\n        \"\"\"\n        self.log.debug('handle_typecheck_complete: in')\n        if not self.currently_buffering_typechecks:\n            self.log.debug('Completed typecheck was not requested by user, not displaying notes')\n            return\n\n        self.editor.display_notes(self.buffered_notes)\n        self.currently_buffering_typechecks = False\n        self.buffered_notes = []"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_setting(self, key, default):\n        gkey = \"ensime_{}\".format(key)\n        return self._vim.vars.get(gkey, default)", "response": "Returns the value of a Vim variable if it is set and default otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef client_status(self, config_path):\n        c = self.client_for(config_path)\n        status = \"stopped\"\n        if not c or not c.ensime:\n            status = 'unloaded'\n        elif c.ensime.is_ready():\n            status = 'ready'\n        elif c.ensime.is_running():\n            status = 'startup'\n        elif c.ensime.aborted():\n            status = 'aborted'\n        return status", "response": "Get status of a client for a project given a path to its config."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_client(self, quiet, bootstrap_server, create_client):\n        current_file = self._vim.current.buffer.name\n        config_path = ProjectConfig.find_from(current_file)\n        if config_path:\n            return self.client_for(\n                config_path,\n                quiet=quiet,\n                bootstrap_server=bootstrap_server,\n                create_client=create_client)", "response": "Return the client for the current file in the editor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_for(self, config_path, quiet=False, bootstrap_server=False,\n                   create_client=False):\n        \"\"\"Get a cached client for a project, otherwise create one.\"\"\"\n        client = None\n        abs_path = os.path.abspath(config_path)\n        if abs_path in self.clients:\n            client = self.clients[abs_path]\n        elif create_client:\n            client = self.create_client(config_path)\n            if client.setup(quiet=quiet, bootstrap_server=bootstrap_server):\n                self.clients[abs_path] = client\n        return client", "response": "Get a cached client for a project. If create_client is True create a client for the project."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_client(self, config_path):\n        config = ProjectConfig(config_path)\n        editor = Editor(self._vim)\n        launcher = EnsimeLauncher(self._vim, config)\n\n        if self.using_server_v2:\n            client = EnsimeClientV2(editor, launcher)\n        else:\n            client = EnsimeClientV1(editor, launcher)\n\n        self._create_ticker()\n\n        return client", "response": "Create an instance of the appropriate client class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndisabling ensime - vim in the event of an error we can t usefully recover from.", "response": "def disable_plugin(self):\n        \"\"\"Disable ensime-vim, in the event of an error we can't usefully\n        recover from.\n\n        Todo:\n            This is incomplete and unreliable, see:\n            https://github.com/ensime/ensime-vim/issues/294\n\n            If used from a secondary thread, this may need to use threadsafe\n            Vim calls where available -- see :meth:`Editor.raw_message`.\n        \"\"\"\n        for path in self.runtime_paths():\n            self._vim.command('set runtimepath-={}'.format(path))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef runtime_paths(self):  # TODO: memoize\n        runtimepath = self._vim.options['runtimepath']\n        plugin = \"ensime-vim\"\n        paths = []\n\n        for path in runtimepath.split(','):\n            if plugin in path:\n                paths.append(os.path.expanduser(path))\n\n        return paths", "response": "All the runtime paths of ensime - vim plugin files."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrigger the periodic tick function in the client.", "response": "def tick_clients(self):\n        \"\"\"Trigger the periodic tick function in the client.\"\"\"\n        if not self._ticker:\n            self._create_ticker()\n\n        for client in self.clients.values():\n            self._ticker.tick(client)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append(self, text, afterline=None):\n        if afterline:\n            self._vim.current.buffer.append(text, afterline)\n        else:\n            self._vim.current.buffer.append(text)", "response": "Append text to the current buffer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a line from the current buffer.", "response": "def getline(self, lnum=None):\n        \"\"\"Get a line from the current buffer.\n\n        Args:\n            lnum (Optional[str]): Number of the line to get, current if ``None``.\n\n        Todo:\n            - Give this more behavior of Vim ``getline()``?\n            - ``buffer[index]`` is zero-based, this is probably too confusing\n        \"\"\"\n        return self._vim.current.buffer[lnum] if lnum else self._vim.current.line"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getlines(self, bufnr=None):\n        buf = self._vim.buffers[bufnr] if bufnr else self._vim.current.buffer\n        return buf[:]", "response": "Returns a list of all lines of a buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a point or offset in a file to a row and column position.", "response": "def point2pos(self, point):\n        \"\"\"Converts a point or offset in a file to a (row, col) position.\"\"\"\n        row = self._vim.eval('byte2line({})'.format(point))\n        col = self._vim.eval('{} - line2byte({})'.format(point, row))\n        return (int(row), int(col))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npresenting a selection menu and returns the user s choice.", "response": "def menu(self, prompt, choices):\n        \"\"\"Presents a selection menu and returns the user's choice.\n\n        Args:\n            prompt (str): Text to ask the user what to select.\n            choices (Sequence[str]): Values for the user to select from.\n\n        Returns:\n            The value selected by the user, or ``None``.\n\n        Todo:\n            Nice opportunity to provide a hook for Unite.vim, etc. here.\n        \"\"\"\n        menu = [prompt] + [\n            \"{0}. {1}\".format(*choice) for choice in enumerate(choices, start=1)\n        ]\n        command = 'inputlist({})'.format(repr(menu))\n        choice = int(self._vim.eval(command))\n\n        # Vim returns weird stuff if user clicks outside choices with mouse\n        if not 0 < choice < len(menu):\n            return\n\n        return choices[choice - 1]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_buffer_options(self, options, bufnr=None):\n        buf = self._vim.buffers[bufnr] if bufnr else self._vim.current.buffer\n\n        # Special case handling for filetype, see doc on ``set_filetype``\n        filetype = options.pop('filetype', None)\n        if filetype:\n            self.set_filetype(filetype)\n\n        for opt, value in options.items():\n            buf.options[opt] = value", "response": "Set options for a buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the filetype for a buffer.", "response": "def set_filetype(self, filetype, bufnr=None):\n        \"\"\"Set filetype for a buffer.\n\n        Note: it's a quirk of Vim's Python API that using the buffer.options\n        dictionary to set filetype does not trigger ``FileType`` autocommands,\n        hence this implementation executes as a command instead.\n\n        Args:\n            filetype (str): The filetype to set.\n            bufnr (Optional[int]): A Vim buffer number, current if ``None``.\n        \"\"\"\n        if bufnr:\n            self._vim.command(str(bufnr) + 'bufdo set filetype=' + filetype)\n        else:\n            self._vim.command('set filetype=' + filetype)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nopens file in a new split window.", "response": "def split_window(self, fpath, vertical=False, size=None, bufopts=None):\n        \"\"\"Open file in a new split window.\n\n        Args:\n            fpath (str): Path of the file to open. If ``None``, a new empty\n                split is created.\n            vertical (bool): Whether to open a vertical split.\n            size (Optional[int]): The height (or width) to set for the new window.\n            bufopts (Optional[dict]): Buffer-local options to set in the split window.\n                See :func:`.set_buffer_options`.\n        \"\"\"\n        command = 'split {}'.format(fpath) if fpath else 'new'\n        if vertical:\n            command = 'v' + command\n        if size:\n            command = str(size) + command\n\n        self._vim.command(command)\n\n        if bufopts:\n            self.set_buffer_options(bufopts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the file of the current buffer.", "response": "def write(self, noautocmd=False):\n        \"\"\"Writes the file of the current buffer.\n\n        Args:\n            noautocmd (bool): If true, write will skip autocommands.\n\n        Todo:\n            We should consider whether ``SourceFileInfo`` can replace most\n            usage of noautocmd. See #298\n        \"\"\"\n        cmd = 'noautocmd write' if noautocmd else 'write'\n        self._vim.command(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initialize(self):\n        # TODO: This seems wrong, the user setting value is never used anywhere.\n        if 'EnErrorStyle' not in self._vim.vars:\n            self._vim.vars['EnErrorStyle'] = 'EnError'\n        self._vim.command('highlight EnErrorStyle ctermbg=red gui=underline')\n\n        # TODO: this SHOULD be a buffer-local setting only, and since it should\n        # apply to all Scala files, ftplugin is the ideal place to set it. I'm\n        # not even sure how this is currently working when only set once.\n        self._vim.command('set omnifunc=EnCompleteFunc')\n\n        # TODO: custom filetype ftplugin\n        self._vim.command(\n            'autocmd FileType package_info nnoremap <buffer> <Space> :call EnPackageDecl()<CR>')\n        self._vim.command('autocmd FileType package_info setlocal splitright')", "response": "Sets up initial ensime - vim editor settings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_cursor(self, row, col):\n        self._vim.current.window.cursor = (row, col)", "response": "Set cursor position to given row and column in the current window."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef word_under_cursor_pos(self):\n        self._vim.command('normal e')\n        end = self.cursor()\n        self._vim.command('normal b')\n        beg = self.cursor()\n        return beg, end", "response": "Return start and end positions of the cursor respectively."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning start and end positions of the visual selection respectively.", "response": "def selection_pos(self):\n        \"\"\"Return start and end positions of the visual selection respectively.\"\"\"\n        buff = self._vim.current.buffer\n        beg = buff.mark('<')\n        end = buff.mark('>')\n        return beg, end"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ask_input(self, prompt):\n        self._vim.command('call inputsave()')\n        self._vim.command('let user_input = input(\"{} \")'.format(prompt))\n        self._vim.command('call inputrestore()')\n        response = self._vim.eval('user_input')\n        self._vim.command('unlet user_input')\n        return response", "response": "Prompt user for input and return the entered value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisplaying error when user is over it.", "response": "def lazy_display_error(self, filename):\n        \"\"\"Display error when user is over it.\"\"\"\n        position = self.cursor()\n        error = self.get_error_at(position)\n        if error:\n            report = error.get_truncated_message(position, self.width() - 1)\n            self.raw_message(report)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_error_at(self, cursor):\n        for error in self._errors:\n            if error.includes(self._vim.eval(\"expand('%:p')\"), cursor):\n                return error\n        return None", "response": "Return error at position cursor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclean errors and unhighlight them in vim.", "response": "def clean_errors(self):\n        \"\"\"Clean errors and unhighlight them in vim.\"\"\"\n        self._vim.eval('clearmatches()')\n        self._errors = []\n        self._matches = []\n        # Reset Syntastic notes - TODO: bufdo?\n        self._vim.current.buffer.vars['ensime_notes'] = []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays a message in the Vim status line.", "response": "def raw_message(self, message, silent=False):\n        \"\"\"Display a message in the Vim status line.\"\"\"\n        vim = self._vim\n        cmd = 'echo \"{}\"'.format(message.replace('\"', '\\\\\"'))\n        if silent:\n            cmd = 'silent ' + cmd\n\n        if self.isneovim:\n            vim.async_call(vim.command, cmd)\n        else:\n            vim.command(cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the fully - qualified name of the symbol on that line.", "response": "def symbol_for_inspector_line(self, lineno):\n        \"\"\"Given a line number for the Package Inspector window, returns the\n        fully-qualified name for the symbol on that line.\n        \"\"\"\n        def indent(line):\n            n = 0\n            for char in line:\n                if char == ' ':\n                    n += 1\n                else:\n                    break\n            return n / 2\n\n        lines = self._vim.current.buffer[:lineno]\n        i = indent(lines[-1])\n        fqn = [lines[-1].split()[-1]]\n\n        for line in reversed(lines):\n            if indent(line) == i - 1:\n                i -= 1\n                fqn.insert(0, line.split()[-1])\n\n        return \".\".join(fqn)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef display_notes(self, notes):\n\n        # TODO: this can probably be a cached property like isneovim\n        hassyntastic = bool(int(self._vim.eval('exists(\":SyntasticCheck\")')))\n\n        if hassyntastic:\n            self.__display_notes_with_syntastic(notes)\n        else:\n            self.__display_notes(notes)\n\n        self._vim.command('redraw!')", "response": "Renders notes reported by ENSIME such as typecheck errors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregenerating signature for methods. Return just the name otherwise", "response": "def formatted_completion_sig(completion):\n    \"\"\"Regenerate signature for methods. Return just the name otherwise\"\"\"\n    f_result = completion[\"name\"]\n    if is_basic_type(completion):\n        # It's a raw type\n        return f_result\n    elif len(completion[\"typeInfo\"][\"paramSections\"]) == 0:\n        return f_result\n\n    # It's a function type\n    sections = completion[\"typeInfo\"][\"paramSections\"]\n    f_sections = [formatted_param_section(ps) for ps in sections]\n    return u\"{}{}\".format(f_result, \"\".join(f_sections))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef formatted_param_section(section):\n    implicit = \"implicit \" if section[\"isImplicit\"] else \"\"\n    s_params = [(p[0], formatted_param_type(p[1])) for p in section[\"params\"]]\n    return \"({}{})\".format(implicit, concat_params(s_params))", "response": "Format a parameters list. Supports the implicit list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the short name for a type. Special treatment for by - name and var args.", "response": "def formatted_param_type(ptype):\n    \"\"\"Return the short name for a type. Special treatment for by-name and var args\"\"\"\n    pt_name = ptype[\"name\"]\n    if pt_name.startswith(\"<byname>\"):\n        pt_name = pt_name.replace(\"<byname>[\", \"=> \")[:-1]\n    elif pt_name.startswith(\"<repeated>\"):\n        pt_name = pt_name.replace(\"<repeated>[\", \"\")[:-1] + \"*\"\n    return pt_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register_responses_handlers(self):\n        self.handlers[\"SymbolInfo\"] = self.handle_symbol_info\n        self.handlers[\"IndexerReadyEvent\"] = self.handle_indexer_ready\n        self.handlers[\"AnalyzerReadyEvent\"] = self.handle_analyzer_ready\n        self.handlers[\"NewScalaNotesEvent\"] = self.buffer_typechecks\n        self.handlers[\"NewJavaNotesEvent\"] = self.buffer_typechecks_and_display\n        self.handlers[\"BasicTypeInfo\"] = self.show_type\n        self.handlers[\"ArrowTypeInfo\"] = self.show_type\n        self.handlers[\"FullTypeCheckCompleteEvent\"] = self.handle_typecheck_complete\n        self.handlers[\"StringResponse\"] = self.handle_string_response\n        self.handlers[\"CompletionInfoList\"] = self.handle_completion_info_list\n        self.handlers[\"TypeInspectInfo\"] = self.handle_type_inspect\n        self.handlers[\"SymbolSearchResults\"] = self.handle_symbol_search\n        self.handlers[\"SourcePositions\"] = self.handle_source_positions\n        self.handlers[\"DebugOutputEvent\"] = self.handle_debug_output\n        self.handlers[\"DebugBreakEvent\"] = self.handle_debug_break\n        self.handlers[\"DebugBacktrace\"] = self.handle_debug_backtrace\n        self.handlers[\"DebugVmError\"] = self.handle_debug_vm_error\n        self.handlers[\"RefactorDiffEffect\"] = self.apply_refactor\n        self.handlers[\"ImportSuggestions\"] = self.handle_import_suggestions\n        self.handlers[\"PackageInfo\"] = self.handle_package_info\n        self.handlers[\"FalseResponse\"] = self.handle_false_response", "response": "Register handlers for responses from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a registered handler for a given response and execute it.", "response": "def handle_incoming_response(self, call_id, payload):\n        \"\"\"Get a registered handler for a given response and execute it.\"\"\"\n        self.log.debug('handle_incoming_response: in [typehint: %s, call ID: %s]',\n                       payload['typehint'], call_id)  # We already log the full JSON response\n\n        typehint = payload[\"typehint\"]\n        handler = self.handlers.get(typehint)\n\n        def feature_not_supported(m):\n            msg = feedback[\"handler_not_implemented\"]\n            self.editor.raw_message(msg.format(typehint, self.launcher.ensime_version))\n\n        if handler:\n            with catch(NotImplementedError, feature_not_supported):\n                handler(call_id, payload)\n        else:\n            self.log.warning('Response has not been handled: %s', Pretty(payload))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle symbol search results", "response": "def handle_symbol_search(self, call_id, payload):\n        \"\"\"Handler for symbol search results\"\"\"\n        self.log.debug('handle_symbol_search: in %s', Pretty(payload))\n\n        syms = payload[\"syms\"]\n        qfList = []\n        for sym in syms:\n            p = sym.get(\"pos\")\n            if p:\n                item = self.editor.to_quickfix_item(str(p[\"file\"]),\n                                                    p[\"line\"],\n                                                    str(sym[\"name\"]),\n                                                    \"info\")\n                qfList.append(item)\n        self.editor.write_quickfix_list(qfList, \"Symbol Search\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_symbol_info(self, call_id, payload):\n        with catch(KeyError, lambda e: self.editor.message(\"unknown_symbol\")):\n            decl_pos = payload[\"declPos\"]\n            f = decl_pos.get(\"file\")\n            call_options = self.call_options[call_id]\n            self.log.debug('handle_symbol_info: call_options %s', call_options)\n            display = call_options.get(\"display\")\n            if display and f:\n                self.editor.raw_message(f)\n\n            open_definition = call_options.get(\"open_definition\")\n            if open_definition and f:\n                self.editor.clean_errors()\n                self.editor.doautocmd('BufLeave')\n                if call_options.get(\"split\"):\n                    vert = call_options.get(\"vert\")\n                    self.editor.split_window(f, vertical=vert)\n                else:\n                    self.editor.edit(f)\n                self.editor.doautocmd('BufReadPre', 'BufRead', 'BufEnter')\n                self.set_position(decl_pos)\n                del self.call_options[call_id]", "response": "Handler for response SymbolInfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle response for string response.", "response": "def handle_string_response(self, call_id, payload):\n        \"\"\"Handler for response `StringResponse`.\n\n        This is the response for the following requests:\n          1. `DocUriAtPointReq` or `DocUriForSymbolReq`\n          2. `DebugToStringReq`\n        \"\"\"\n        self.log.debug('handle_string_response: in [typehint: %s, call ID: %s]',\n                       payload['typehint'], call_id)\n\n        # :EnDocBrowse or :EnDocUri\n        url = payload['text']\n        if not url.startswith('http'):\n            port = self.ensime.http_port()\n            url = gconfig['localhost'].format(port, url)\n\n        options = self.call_options.get(call_id)\n        if options and options.get('browse'):\n            self._browse_doc(url)\n            del self.call_options[call_id]\n        else:\n            # TODO: make this return value of a Vim function synchronously, how?\n            self.log.debug('EnDocUri %s', url)\n            return url"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nshows type of a variable or scala type.", "response": "def show_type(self, call_id, payload):\n        \"\"\"Show type of a variable or scala type.\"\"\"\n        if self.full_types_enabled:\n            tpe = payload['fullName']\n        else:\n            tpe = payload['name']\n\n        self.log.info('Displayed type %s', tpe)\n        self.editor.raw_message(tpe)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_source_positions(self, call_id, payload):\n        self.log.debug('handle_source_positions: in %s', Pretty(payload))\n\n        call_options = self.call_options[call_id]\n        word_under_cursor = call_options.get(\"word_under_cursor\")\n\n        positions = payload[\"positions\"]\n\n        if not positions:\n            self.editor.raw_message(\"No usages of <{}> found\".format(word_under_cursor))\n            return\n\n        qf_list = []\n        for p in positions:\n            position = p[\"position\"]\n            preview = str(p[\"preview\"]) if \"preview\" in p else \"<no preview>\"\n            item = self.editor.to_quickfix_item(str(position[\"file\"]),\n                                                position[\"line\"],\n                                                preview,\n                                                \"info\")\n            qf_list.append(item)\n        qf_sorted = sorted(qf_list, key=itemgetter('filename', 'lnum'))\n        self.editor.write_quickfix_list(qf_sorted, \"Usages of <{}>\".format(word_under_cursor))", "response": "Handler for source positions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _start_refresh_timer(self):\n        if not self._timer:\n            self._timer = self._vim.eval(\n                \"timer_start({}, 'EnTick', {{'repeat': -1}})\"\n                .format(REFRESH_TIMER)\n            )", "response": "Start the refresh timer."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nput new messages on the queue.", "response": "def queue_poll(self, sleep_t=0.5):\n        \"\"\"Put new messages on the queue as they arrive. Blocking in a thread.\n\n        Value of sleep is low to improve responsiveness.\n        \"\"\"\n        connection_alive = True\n\n        while self.running:\n            if self.ws:\n                def logger_and_close(msg):\n                    self.log.error('Websocket exception', exc_info=True)\n                    if not self.running:\n                        # Tear down has been invoked\n                        # Prepare to exit the program\n                        connection_alive = False  # noqa: F841\n                    else:\n                        if not self.number_try_connection:\n                            # Stop everything.\n                            self.teardown()\n                            self._display_ws_warning()\n\n                with catch(websocket.WebSocketException, logger_and_close):\n                    result = self.ws.recv()\n                    self.queue.put(result)\n\n            if connection_alive:\n                time.sleep(sleep_t)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking the classpath and connect to the server if necessary.", "response": "def setup(self, quiet=False, bootstrap_server=False):\n        \"\"\"Check the classpath and connect to the server if necessary.\"\"\"\n        def lazy_initialize_ensime():\n            if not self.ensime:\n                called_by = inspect.stack()[4][3]\n                self.log.debug(str(inspect.stack()))\n                self.log.debug('setup(quiet=%s, bootstrap_server=%s) called by %s()',\n                               quiet, bootstrap_server, called_by)\n\n                installed = self.launcher.strategy.isinstalled()\n                if not installed and not bootstrap_server:\n                    if not quiet:\n                        scala = self.launcher.config.get('scala-version')\n                        msg = feedback[\"prompt_server_install\"].format(scala_version=scala)\n                        self.editor.raw_message(msg)\n                    return False\n\n                try:\n                    self.ensime = self.launcher.launch()\n                except InvalidJavaPathError:\n                    self.editor.message('invalid_java')  # TODO: also disable plugin\n\n            return bool(self.ensime)\n\n        def ready_to_connect():\n            if not self.ws and self.ensime.is_ready():\n                self.connect_ensime_server()\n            return True\n\n        # True if ensime is up and connection is ok, otherwise False\n        return self.running and lazy_initialize_ensime() and ready_to_connect()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, msg):\n        def reconnect(e):\n            self.log.error('send error, reconnecting...', exc_info=True)\n            self.connect_ensime_server()\n            if self.ws:\n                self.ws.send(msg + \"\\n\")\n\n        self.log.debug('send: in')\n        if self.running and self.ws:\n            with catch(websocket.WebSocketException, reconnect):\n                self.log.debug('send: sending JSON on WebSocket')\n                self.ws.send(msg + \"\\n\")", "response": "Send something to the ensime server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect_ensime_server(self):\n        self.log.debug('connect_ensime_server: in')\n        server_v2 = isinstance(self, EnsimeClientV2)\n\n        def disable_completely(e):\n            if e:\n                self.log.error('connection error: %s', e, exc_info=True)\n            self.shutdown_server()\n            self._display_ws_warning()\n\n        if self.running and self.number_try_connection:\n            self.number_try_connection -= 1\n            if not self.ensime_server:\n                port = self.ensime.http_port()\n                uri = \"websocket\" if server_v2 else \"jerky\"\n                self.ensime_server = gconfig[\"ensime_server\"].format(port, uri)\n            with catch(websocket.WebSocketException, disable_completely):\n                # Use the default timeout (no timeout).\n                options = {\"subprotocols\": [\"jerky\"]} if server_v2 else {}\n                options['enable_multithread'] = True\n                self.log.debug(\"About to connect to %s with options %s\",\n                               self.ensime_server, options)\n                self.ws = websocket.create_connection(self.ensime_server, **options)\n            if self.ws:\n                self.send_request({\"typehint\": \"ConnectionInfoReq\"})\n        else:\n            # If it hits this, number_try_connection is 0\n            disable_completely(None)", "response": "Start initial connection with the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshutting down server if it is alive.", "response": "def shutdown_server(self):\n        \"\"\"Shut down server if it is alive.\"\"\"\n        self.log.debug('shutdown_server: in')\n        if self.ensime and self.toggle_teardown:\n            self.ensime.stop()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntear down the server or keep it alive.", "response": "def teardown(self):\n        \"\"\"Tear down the server or keep it alive.\"\"\"\n        self.log.debug('teardown: in')\n        self.running = False\n        self.shutdown_server()\n        shutil.rmtree(self.tmp_diff_folder, ignore_errors=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_at_position(self, what, useSelection, where=\"range\"):\n        self.log.debug('send_at_position: in')\n        b, e = self.editor.selection_pos() if useSelection else self.editor.word_under_cursor_pos()\n        self.log.debug('useSelection: {}, beg: {}, end: {}'.format(useSelection, b, e))\n        beg = self.get_position(b[0], b[1])\n        end = self.get_position(e[0], e[1])\n        self.send_request(\n            {\"typehint\": what + \"AtPointReq\",\n             \"file\": self.editor.path(),\n             where: {\"from\": beg, \"to\": end}})", "response": "Send a request to the server to perform an operation on a range of words."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_position(self, decl_pos):\n        if decl_pos[\"typehint\"] == \"LineSourcePosition\":\n            self.editor.set_cursor(decl_pos['line'], 0)\n        else:  # OffsetSourcePosition\n            point = decl_pos[\"offset\"]\n            row, col = self.editor.point2pos(point + 1)\n            self.editor.set_cursor(row, col)", "response": "Set editor position from ENSIME declPos data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_position(self, row, col):\n        result = col\n        self.log.debug('%s %s', row, col)\n        lines = self.editor.getlines()[:row - 1]\n        result += sum([len(l) + 1 for l in lines])\n        self.log.debug(result)\n        return result", "response": "Get char position in all the text from row and column."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nask the server to perform an operation at a given point.", "response": "def send_at_point(self, what, row, col):\n        \"\"\"Ask the server to perform an operation at a given point.\"\"\"\n        pos = self.get_position(row, col)\n        self.send_request(\n            {\"typehint\": what + \"AtPointReq\",\n             \"file\": self._file_info(),\n             \"point\": pos})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef type_check_cmd(self, args, range=None):\n        self.log.debug('type_check_cmd: in')\n        self.start_typechecking()\n        self.type_check(\"\")\n        self.editor.message('typechecking')", "response": "Sets the flag to begin buffering typecheck notes & clears any\n        stale notes before requesting a typecheck from the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef doc_uri(self, args, range=None):\n        self.log.debug('doc_uri: in')\n        self.send_at_position(\"DocUri\", False, \"point\")", "response": "Request the doc of whatever at cursor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef usages(self):\n        row, col = self.editor.cursor()\n        self.log.debug('usages: in')\n        self.call_options[self.call_id] = {\n                \"word_under_cursor\": self.editor.current_word(),\n                \"false_resp_msg\": \"Not a valid symbol under the cursor\"}\n        self.send_at_point(\"UsesOfSymbol\", row, col)", "response": "Request usages of whatever at cursor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef doc_browse(self, args, range=None):\n        self.log.debug('browse: in')\n        self.call_options[self.call_id] = {\"browse\": True}\n        self.send_at_position(\"DocUri\", False, \"point\")", "response": "Browse the current document of whatever at cursor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename(self, new_name, range=None):\n        self.log.debug('rename: in')\n        if not new_name:\n            new_name = self.editor.ask_input(\"Rename to:\")\n        self.editor.write(noautocmd=True)\n        b, e = self.editor.word_under_cursor_pos()\n        current_file = self.editor.path()\n        self.editor.raw_message(current_file)\n        self.send_refactor_request(\n            \"RefactorReq\",\n            {\n                \"typehint\": \"RenameRefactorDesc\",\n                \"newName\": new_name,\n                \"start\": self.get_position(b[0], b[1]),\n                \"end\": self.get_position(e[0], e[1]) + 1,\n                \"file\": current_file,\n            },\n            {\"interactive\": False}\n        )", "response": "Request a rename to the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef symbol_search(self, search_terms):\n        self.log.debug('symbol_search: in')\n\n        if not search_terms:\n            self.editor.message('symbol_search_symbol_required')\n            return\n        req = {\n            \"typehint\": \"PublicSymbolSearchReq\",\n            \"keywords\": search_terms,\n            \"maxResults\": 25\n        }\n        self.send_request(req)", "response": "Search for symbols matching a set of keywords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a refactor request to the Ensime server.", "response": "def send_refactor_request(self, ref_type, ref_params, ref_options):\n        \"\"\"Send a refactor request to the Ensime server.\n\n        The `ref_params` field will always have a field `type`.\n        \"\"\"\n        request = {\n            \"typehint\": ref_type,\n            \"procId\": self.refactor_id,\n            \"params\": ref_params\n        }\n        f = ref_params[\"file\"]\n        self.refactorings[self.refactor_id] = f\n        self.refactor_id += 1\n        request.update(ref_options)\n        self.send_request(request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_refactor(self, call_id, payload):\n        supported_refactorings = [\"Rename\", \"InlineLocal\", \"AddImport\", \"OrganizeImports\"]\n\n        if payload[\"refactorType\"][\"typehint\"] in supported_refactorings:\n            diff_filepath = payload[\"diff\"]\n            path = self.editor.path()\n            bname = os.path.basename(path)\n            target = os.path.join(self.tmp_diff_folder, bname)\n            reject_arg = \"--reject-file={}.rej\".format(target)\n            backup_pref = \"--prefix={}\".format(self.tmp_diff_folder)\n            # Patch utility is prepackaged or installed with vim\n            cmd = [\"patch\", reject_arg, backup_pref, path, diff_filepath]\n            failed = Popen(cmd, stdout=PIPE, stderr=PIPE).wait()\n            if failed:\n                self.editor.message(\"failed_refactoring\")\n            # Update file and reload highlighting\n            self.editor.edit(self.editor.path())\n            self.editor.doautocmd('BufReadPre', 'BufRead', 'BufEnter')", "response": "Apply a refactor depending on its type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a request to the server.", "response": "def send_request(self, request):\n        \"\"\"Send a request to the server.\"\"\"\n        self.log.debug('send_request: in')\n\n        message = {'callId': self.call_id, 'req': request}\n        self.log.debug('send_request: %s', Pretty(message))\n        self.send(json.dumps(message))\n\n        call_id = self.call_id\n        self.call_id += 1\n        return call_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef buffer_leave(self, filename):\n        self.log.debug('buffer_leave: %s', filename)\n        # TODO: This is questionable, and we should use location list for\n        # single-file errors.\n        self.editor.clean_errors()", "response": "User is changing of buffer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating type checking when user saves buffer.", "response": "def type_check(self, filename):\n        \"\"\"Update type checking when user saves buffer.\"\"\"\n        self.log.debug('type_check: in')\n        self.editor.clean_errors()\n        self.send_request(\n            {\"typehint\": \"TypecheckFilesReq\",\n             \"files\": [self.editor.path()]})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unqueue(self, timeout=10, should_wait=False):\n        start, now = time.time(), time.time()\n        wait = self.queue.empty() and should_wait\n\n        while (not self.queue.empty() or wait) and (now - start) < timeout:\n            if wait and self.queue.empty():\n                time.sleep(0.25)\n                now = time.time()\n            else:\n                result = self.queue.get(False)\n                self.log.debug('unqueue: result received\\n%s', result)\n                if result and result != \"nil\":\n                    wait = None\n                    # Restart timeout\n                    start, now = time.time(), time.time()\n                    _json = json.loads(result)\n                    # Watch out, it may not have callId\n                    call_id = _json.get(\"callId\")\n                    if _json[\"payload\"]:\n                        self.handle_incoming_response(call_id, _json[\"payload\"])\n                else:\n                    self.log.debug('unqueue: nil or None received')\n\n        if (now - start) >= timeout:\n            self.log.warning('unqueue: no reply from server for %ss', timeout)", "response": "Unqueue all the received ensime responses for a given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unqueue_and_display(self, filename):\n        if self.running and self.ws:\n            self.editor.lazy_display_error(filename)\n            self.unqueue()", "response": "Unqueue messages and give feedback to user if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to connect and display messages in queue.", "response": "def tick(self, filename):\n        \"\"\"Try to connect and display messages in queue.\"\"\"\n        if self.connection_attempts < 10:\n            # Trick to connect ASAP when\n            # plugin is  started without\n            # user interaction (CursorMove)\n            self.setup(True, False)\n            self.connection_attempts += 1\n        self.unqueue_and_display(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up EnsimeClient when vim enters a new object.", "response": "def vim_enter(self, filename):\n        \"\"\"Set up EnsimeClient when vim enters.\n\n        This is useful to start the EnsimeLauncher as soon as possible.\"\"\"\n        success = self.setup(True, False)\n        if success:\n            self.editor.message(\"start_message\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_legacy_bootstrap():\n        home = os.environ['HOME']\n        old_base_dir = os.path.join(home, '.config', 'classpath_project_ensime')\n        if os.path.isdir(old_base_dir):\n            shutil.rmtree(old_base_dir, ignore_errors=True)", "response": "Remove bootstrap projects from old path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _start_process(self, classpath):\n        cache_dir = self.config['cache-dir']\n        java_flags = self.config['java-flags']\n        iswindows = os.name == 'nt'\n\n        Util.mkdir_p(cache_dir)\n        log_path = os.path.join(cache_dir, \"server.log\")\n        log = open(log_path, \"w\")\n        null = open(os.devnull, \"r\")\n        java = os.path.join(self.config['java-home'], 'bin', 'java.exe' if iswindows else 'java')\n\n        if not os.path.exists(java):\n            raise InvalidJavaPathError(errno.ENOENT, 'No such file or directory', java)\n        elif not os.access(java, os.X_OK):\n            raise InvalidJavaPathError(errno.EACCES, 'Permission denied', java)\n\n        args = (\n            [java, \"-cp\", (';' if iswindows else ':').join(classpath)] +\n            [a for a in java_flags if a] +\n            [\"-Densime.config={}\".format(self.config.filepath),\n             \"org.ensime.server.Server\"])\n        process = subprocess.Popen(\n            args,\n            stdin=null,\n            stdout=log,\n            stderr=subprocess.STDOUT)\n        pid_path = os.path.join(cache_dir, \"server.pid\")\n        Util.write_file(pid_path, str(process.pid))\n\n        def on_stop():\n            log.close()\n            null.close()\n            with catch(Exception):\n                os.remove(pid_path)\n\n        return EnsimeProcess(cache_dir, process, log_path, on_stop)", "response": "Starts a process that runs the server with the given classpath."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninstall a bootstrap sbt project and generates its classpath.", "response": "def install(self):\n        \"\"\"Installs ENSIME server with a bootstrap sbt project and generates its classpath.\"\"\"\n        project_dir = os.path.dirname(self.classpath_file)\n        sbt_plugin = \"\"\"addSbtPlugin(\"{0}\" % \"{1}\" % \"{2}\")\"\"\"\n\n        Util.mkdir_p(project_dir)\n        Util.mkdir_p(os.path.join(project_dir, \"project\"))\n        Util.write_file(\n            os.path.join(project_dir, \"build.sbt\"),\n            self.build_sbt())\n        Util.write_file(\n            os.path.join(project_dir, \"project\", \"build.properties\"),\n            \"sbt.version={}\".format(self.SBT_VERSION))\n        Util.write_file(\n            os.path.join(project_dir, \"project\", \"plugins.sbt\"),\n            sbt_plugin.format(*self.SBT_COURSIER_COORDS))\n\n        # Synchronous update of the classpath via sbt\n        # see https://github.com/ensime/ensime-vim/issues/29\n        cd_cmd = \"cd {}\".format(project_dir)\n        sbt_cmd = \"sbt -Dsbt.log.noformat=true -batch saveClasspath\"\n\n        if int(self.vim.eval(\"has('nvim')\")):\n            import tempfile\n            import re\n            tmp_dir = tempfile.gettempdir()\n            flag_file = \"{}/ensime-vim-classpath.flag\".format(tmp_dir)\n            self.vim.command(\"echo 'Waiting for generation of classpath...'\")\n            if re.match(\".+fish$\", self.vim.eval(\"&shell\")):\n                sbt_cmd += \"; echo $status > {}\".format(flag_file)\n                self.vim.command(\"terminal {}; and {}\".format(cd_cmd, sbt_cmd))\n            else:\n                sbt_cmd += \"; echo $? > {}\".format(flag_file)\n                self.vim.command(\"terminal ({} && {})\".format(cd_cmd, sbt_cmd))\n\n            # Block execution when sbt is run\n            waiting_for_flag = True\n            while waiting_for_flag:\n                waiting_for_flag = not os.path.isfile(flag_file)\n                if not waiting_for_flag:\n                    with open(flag_file, \"r\") as f:\n                        rtcode = f.readline()\n                    os.remove(flag_file)\n                    if rtcode and int(rtcode) != 0:  # error\n                        self.vim.command(\n                            \"echo 'Something wrong happened, check the \"\n                            \"execution log...'\")\n                        return None\n                else:\n                    time.sleep(0.2)\n        else:\n            self.vim.command(\"!({} && {})\".format(cd_cmd, sbt_cmd))\n\n        success = self.reorder_classpath(self.classpath_file)\n        if not success:\n            self.vim.command(\"echo 'Classpath ordering failed.'\")\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reorder_classpath(self, classpath_file):\n        success = False\n\n        with catch((IOError, OSError)):\n            with open(classpath_file, \"r\") as f:\n                classpath = f.readline()\n\n            # Proceed if classpath is non-empty\n            if classpath:\n                units = classpath.split(\":\")\n                reordered_units = []\n                for unit in units:\n                    if \"monkeys\" in unit:\n                        reordered_units.insert(0, unit)\n                    else:\n                        reordered_units.append(unit)\n                reordered_classpath = \":\".join(reordered_units)\n\n                with open(classpath_file, \"w\") as f:\n                    f.write(reordered_classpath)\n\n            success = True\n\n        return success", "response": "Reorder classpath and put monkeys - jar in the first place."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds path of an. ensime config in path.", "response": "def find_from(path):\n        \"\"\"Find path of an .ensime config, searching recursively upward from path.\n\n        Args:\n            path (str): Path of a file or directory from where to start searching.\n\n        Returns:\n            str: Canonical path of nearest ``.ensime``, or ``None`` if not found.\n        \"\"\"\n        realpath = os.path.realpath(path)\n        config_path = os.path.join(realpath, '.ensime')\n\n        if os.path.isfile(config_path):\n            return config_path\n        elif realpath == os.path.abspath('/'):\n            return None\n        else:\n            dirname = os.path.dirname(realpath)\n            return ProjectConfig.find_from(dirname)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse an. ensime config file into a dict.", "response": "def parse(path):\n        \"\"\"Parse an ``.ensime`` config file from S-expressions.\n\n        Args:\n            path (str): Path of an ``.ensime`` file to parse.\n\n        Returns:\n            dict: Configuration values with string keys.\n        \"\"\"\n\n        def paired(iterable):\n            \"\"\"s -> (s0, s1), (s2, s3), (s4, s5), ...\"\"\"\n            cursor = iter(iterable)\n            return zip(cursor, cursor)\n\n        def unwrap_if_sexp_symbol(datum):\n            \"\"\"Convert Symbol(':key') to ':key' (Symbol isn't hashable for dict keys).\n            \"\"\"\n            return datum.value() if isinstance(datum, sexpdata.Symbol) else datum\n\n        def sexp2dict(sexps):\n            \"\"\"Transforms a nested list structure from sexpdata to dict.\"\"\"\n            newdict = {}\n\n            # Turn flat list into associative pairs\n            for key, value in paired(sexps):\n                key = str(unwrap_if_sexp_symbol(key)).lstrip(':')\n\n                # Recursively transform nested lists\n                if isinstance(value, list) and value:\n                    if isinstance(value[0], list):\n                        newdict[key] = [sexp2dict(val) for val in value]\n                    elif isinstance(value[0], sexpdata.Symbol):\n                        newdict[key] = sexp2dict(value)\n                    else:\n                        newdict[key] = value\n                else:\n                    newdict[key] = value\n\n            return newdict\n\n        conf = sexpdata.loads(Util.read_file(path))\n        return sexp2dict(conf)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking JSON response from API and converts to ACL object", "response": "def from_acl_response(acl_response):\n        '''Takes JSON response from API and converts to ACL object'''\n        if 'read' in acl_response:\n            read_acl = AclType.from_acl_response(acl_response['read'])\n            return Acl(read_acl)\n        else:\n            raise ValueError('Response does not contain read ACL')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a directory optionally include Acl argument to set permissions", "response": "def create(self, acl=None):\n        '''Creates a directory, optionally include Acl argument to set permissions'''\n        parent, name = getParentAndBase(self.path)\n        json = { 'name': name }\n        if acl is not None:\n            json['acl'] = acl.to_api_param()\n        response = self.client.postJsonHelper(DataDirectory._getUrl(parent), json, False)\n        if (response.status_code != 200):\n            raise DataApiError(\"Directory creation failed: \" + str(response.content))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_permissions(self):\n        '''\n        Returns permissions for this directory or None if it's a special collection such as\n        .session or .algo\n        '''\n        response = self.client.getHelper(self.url, acl='true')\n        if response.status_code != 200:\n            raise DataApiError('Unable to get permissions:' + str(response.content))\n        content = response.json()\n        if 'acl' in content:\n            return Acl.from_acl_response(content['acl'])\n        else:\n            return None", "response": "Returns permissions for this directory or None if it s a special collection such as a special collection such as a session or algo."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _eight_byte_real(value):\n    if value == 0:\n        return b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\n    if value < 0:\n        byte1 = 0x80\n        value = -value\n    else:\n        byte1 = 0x00\n    fexp = numpy.log2(value) / 4\n    exponent = int(numpy.ceil(fexp))\n    if fexp == exponent:\n        exponent += 1\n    mantissa = int(value * 16.**(14 - exponent))\n    byte1 += exponent + 64\n    byte2 = (mantissa // 281474976710656)\n    short3 = (mantissa % 281474976710656) // 4294967296\n    long4 = mantissa % 4294967296\n    return struct.pack(\">HHL\", byte1 * 256 + byte2, short3, long4)", "response": "Convert a number into the GDSII 8 byte real format."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a number from GDSII 8 byte real format to float.", "response": "def _eight_byte_real_to_float(value):\n    \"\"\"\n    Convert a number from GDSII 8 byte real format to float.\n\n    Parameters\n    ----------\n    value : string\n        The GDSII binary string representation of the number.\n\n    Returns\n    -------\n    out : float\n        The number represented by ``value``.\n    \"\"\"\n    short1, short2, long3 = struct.unpack('>HHL', value)\n    exponent = (short1 & 0x7f00) // 256 - 64\n    mantissa = (((short1 & 0x00ff) * 65536 + short2) * 4294967296 +\n                long3) / 72057594037927936.0\n    if short1 & 0x8000:\n        return -mantissa * 16.**exponent\n    return mantissa * 16.**exponent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef slice(objects, position, axis, precision=1e-3, layer=0, datatype=0):\n    if not isinstance(layer, list):\n        layer = [layer]\n    if not isinstance(objects, list):\n        objects = [objects]\n    if not isinstance(position, list):\n        pos = [position]\n    else:\n        pos = sorted(position)\n    result = [[] for _ in range(len(pos) + 1)]\n    polygons = []\n    for obj in objects:\n        if isinstance(obj, PolygonSet):\n            polygons.extend(obj.polygons)\n        elif isinstance(obj, CellReference) or isinstance(obj, CellArray):\n            polygons.extend(obj.get_polygons())\n        else:\n            polygons.append(obj)\n    scaling = 1 / precision\n    for pol in polygons:\n        for r, p in zip(result, clipper._chop(pol, pos, axis, scaling)):\n            r.extend(p)\n    for i in range(len(result)):\n        result[i] = PolygonSet(result[i], layer[i % len(layer)], datatype)\n    return result", "response": "Slice polygons and polygon sets at given positions along a specified axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef offset(polygons,\n           distance,\n           join='miter',\n           tolerance=2,\n           precision=0.001,\n           join_first=False,\n           max_points=199,\n           layer=0,\n           datatype=0):\n    \"\"\"\n    Shrink or expand a polygon or polygon set.\n\n    Parameters\n    ----------\n    polygons : polygon or array-like\n        Polygons to be offset.  Must be a ``PolygonSet``,\n        ``CellReference``, ``CellArray``, or an array.  The array may\n        contain any of the previous objects or an array-like[N][2] of\n        vertices of a polygon.\n    distance : number\n        Offset distance.  Positive to expand, negative to shrink.\n    join : {'miter', 'bevel', 'round'}\n        Type of join used to create the offset polygon.\n    tolerance : number\n        For miter joints, this number must be at least 2 and it\n        represents the maximun distance in multiples of offset betwen\n        new vertices and their original position before beveling to\n        avoid spikes at acute joints.  For round joints, it indicates\n        the curvature resolution in number of points per full circle.\n    precision : float\n        Desired precision for rounding vertice coordinates.\n    join_first : bool\n        Join all paths before offseting to avoid unecessary joins in\n        adjacent polygon sides.\n    max_points : integer\n        If greater than 4, fracture the resulting polygons to ensure\n        they have at most ``max_points`` vertices.  This is not a\n        tessellating function, so this number should be as high as\n        possible.  For example, it should be set to 199 for polygons\n        being drawn in GDSII files.\n    layer : integer\n        The GDSII layer number for the resulting element.\n    datatype : integer\n        The GDSII datatype for the resulting element (between 0 and\n        255).\n\n    Returns\n    -------\n    out : ``PolygonSet`` or ``None``\n        Return the offset shape as a set of polygons.\n    \"\"\"\n    poly = []\n    if isinstance(polygons, PolygonSet):\n        poly.extend(polygons.polygons)\n    elif isinstance(polygons, CellReference) or isinstance(\n            polygons, CellArray):\n        poly.extend(polygons.get_polygons())\n    else:\n        for obj in polygons:\n            if isinstance(obj, PolygonSet):\n                poly.extend(obj.polygons)\n            elif isinstance(obj, CellReference) or isinstance(obj, CellArray):\n                poly.extend(obj.get_polygons())\n            else:\n                poly.append(obj)\n    result = clipper.offset(poly, distance, join, tolerance, 1 / precision, 1\n                            if join_first else 0)\n    return None if len(result) == 0 else PolygonSet(\n        result, layer, datatype, verbose=False).fracture(\n            max_points, precision)", "response": "Returns a new polygon set that is offset by the given distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes any boolean operation between 2 polygons or polygon sets. Parameters ---------- operandA : polygon or array-like First operand. Must be a ``PolygonSet``, ``CellReference``, ``CellArray``, or an array. The array may contain any of the previous objects or an array-like[N][2] of vertices of a polygon. operandB : polygon, array-like or ``None`` Second operand. Must be ``None``, a ``PolygonSet``, ``CellReference``, ``CellArray``, or an array. The array may contain any of the previous objects or an array-like[N][2] of vertices of a polygon. operation : {'or', 'and', 'xor', 'not'} Boolean operation to be executed. The 'not' operation returns the difference ``operandA - operandB``. precision : float Desired precision for rounding vertice coordinates. max_points : integer If greater than 4, fracture the resulting polygons to ensure they have at most ``max_points`` vertices. This is not a tessellating function, so this number should be as high as possible. For example, it should be set to 199 for polygons being drawn in GDSII files. layer : integer The GDSII layer number for the resulting element. datatype : integer The GDSII datatype for the resulting element (between 0 and 255). Returns ------- out : PolygonSet or ``None`` Result of the boolean operation.", "response": "def fast_boolean(operandA,\n                 operandB,\n                 operation,\n                 precision=0.001,\n                 max_points=199,\n                 layer=0,\n                 datatype=0):\n    \"\"\"\n    Execute any boolean operation between 2 polygons or polygon sets.\n\n    Parameters\n    ----------\n    operandA : polygon or array-like\n        First operand.  Must be a ``PolygonSet``, ``CellReference``,\n        ``CellArray``, or an array.  The array may contain any of the\n        previous objects or an array-like[N][2] of vertices of a\n        polygon.\n    operandB : polygon, array-like or ``None``\n        Second operand.  Must be ``None``, a ``PolygonSet``,\n        ``CellReference``, ``CellArray``, or an array.  The array may\n        contain any of the previous objects or an array-like[N][2] of\n        vertices of a polygon.\n    operation : {'or', 'and', 'xor', 'not'}\n        Boolean operation to be executed.  The 'not' operation returns\n        the difference ``operandA - operandB``.\n    precision : float\n        Desired precision for rounding vertice coordinates.\n    max_points : integer\n        If greater than 4, fracture the resulting polygons to ensure\n        they have at most ``max_points`` vertices.  This is not a\n        tessellating function, so this number should be as high as\n        possible.  For example, it should be set to 199 for polygons\n        being drawn in GDSII files.\n    layer : integer\n        The GDSII layer number for the resulting element.\n    datatype : integer\n        The GDSII datatype for the resulting element (between 0 and\n        255).\n\n    Returns\n    -------\n    out : PolygonSet or ``None``\n        Result of the boolean operation.\n    \"\"\"\n    polyA = []\n    polyB = []\n    for poly, obj in zip((polyA, polyB), (operandA, operandB)):\n        if isinstance(obj, PolygonSet):\n            poly.extend(obj.polygons)\n        elif isinstance(obj, CellReference) or isinstance(obj, CellArray):\n            poly.extend(obj.get_polygons())\n        elif obj is not None:\n            for inobj in obj:\n                if isinstance(inobj, PolygonSet):\n                    poly.extend(inobj.polygons)\n                elif isinstance(inobj, CellReference) or isinstance(\n                        inobj, CellArray):\n                    poly.extend(inobj.get_polygons())\n                else:\n                    poly.append(inobj)\n    if len(polyB) == 0:\n        polyB.append(polyA.pop())\n    result = clipper.clip(polyA, polyB, operation, 1 / precision)\n    return None if len(result) == 0 else PolygonSet(\n        result, layer, datatype, verbose=False).fracture(\n            max_points, precision)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests whether each of the points or point - groups are inside the given set of polygons.", "response": "def inside(points, polygons, short_circuit='any', precision=0.001):\n    \"\"\"\n    Test whether each of the points is within the given set of polygons.\n\n    Parameters\n    ----------\n    points : array-like[N][2] or list of array-like[N][2]\n        Coordinates of the points to be tested or groups of points to be\n        tested together.\n    polygons : polygon or array-like\n        Polygons to be tested against.  Must be a ``PolygonSet``,\n        ``CellReference``, ``CellArray``, or an array.  The array may\n        contain any of the previous objects or an array-like[N][2] of\n        vertices of a polygon.\n    short_circuit : {'any', 'all'}\n        If `points` is a list of point groups, testing within each group\n        will be short-circuited if any of the points in the group is\n        inside ('any') or outside ('all') the polygons.  If `points` is\n        simply a list of points, this parameter has no effect.\n    precision : float\n        Desired precision for rounding vertice coordinates.\n\n    Returns\n    -------\n    out : tuple\n        Tuple of booleans indicating if each of the points or point\n        groups is inside the set of polygons.\n    \"\"\"\n    poly = []\n    if isinstance(polygons, PolygonSet):\n        poly.extend(polygons.polygons)\n    elif isinstance(polygons, CellReference) or isinstance(\n            polygons, CellArray):\n        poly.extend(polygons.get_polygons())\n    else:\n        for obj in polygons:\n            if isinstance(obj, PolygonSet):\n                poly.extend(obj.polygons)\n            elif isinstance(obj, CellReference) or isinstance(obj, CellArray):\n                poly.extend(obj.get_polygons())\n            else:\n                poly.append(obj)\n    if hasattr(points[0][0], '__iter__'):\n        pts = points\n        sc = 1 if short_circuit == 'any' else -1\n    else:\n        pts = (points, )\n        sc = 0\n    return clipper.inside(pts, poly, sc, 1 / precision)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a copy of obj and translates it to a new location.", "response": "def copy(obj, dx, dy):\n    \"\"\"\n    Creates a copy of ``obj`` and translates the new object to a new\n    location.\n\n    Parameters\n    ----------\n    obj : ``obj``\n        any translatable geometery object.\n    dx  : float\n        distance to move in the x-direction\n    dy  : float\n        distance to move in the y-direction\n\n\n    Returns\n    -------\n    out : ``obj``\n        Translated copy of original ``obj``\n\n    Examples\n    --------\n    >>> rectangle = gdspy.Rectangle((0, 0), (10, 20))\n    >>> rectangle2 = gdspy.copy(rectangle, 2,0)\n    >>> myCell.add(rectangle)\n    >>> myCell.add(rectangle2)\n    \"\"\"\n\n    newObj = libCopy.deepcopy(obj)\n    newObj.translate(dx, dy)\n    return newObj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the GDSII library to a file.", "response": "def write_gds(outfile,\n              cells=None,\n              name='library',\n              unit=1.0e-6,\n              precision=1.0e-9):\n    \"\"\"\n    Write the current GDSII library to a file.\n\n    The dimensions actually written on the GDSII file will be the\n    dimensions of the objects created times the ratio\n    ``unit/precision``.  For example, if a circle with radius 1.5 is\n    created and we set ``unit=1.0e-6`` (1 um) and ``precision=1.0e-9``\n    (1 nm), the radius of the circle will be 1.5 um and the GDSII file\n    will contain the dimension 1500 nm.\n\n    Parameters\n    ----------\n    outfile : file or string\n        The file (or path) where the GDSII stream will be written.  It\n        must be opened for writing operations in binary format.\n    cells : array-like\n        The list of cells or cell names to be included in the library.\n        If ``None``, all cells are used.\n    name : string\n        Name of the GDSII library.\n    unit : number\n        Unit size for the objects in the library (in *meters*).\n    precision : number\n        Precision for the dimensions of the objects in the library (in\n        *meters*).\n    \"\"\"\n    current_library.name = name\n    current_library.unit = unit\n    current_library.precision = precision\n    current_library.write_gds(outfile, cells)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the a hash value for a GDSII file.", "response": "def gdsii_hash(filename, engine=None):\n    \"\"\"\n    Calculate the a hash value for a GDSII file.\n\n    The hash is generated based only on the contents of the cells in the\n    GDSII library, ignoring any timestamp records present in the file\n    structure.\n\n    Parameters\n    ----------\n    filename : string\n        Full path to the GDSII file.\n    engine : hashlib-like engine\n        The engine that executes the hashing algorithm.  It must provide\n        the methods ``update`` and ``hexdigest`` as defined in the\n        hashlib module.  If ``None``, the dafault ``hashlib.sha1()`` is\n        used.\n\n    Returns\n    -------\n    out : string\n        The hash correponding to the library contents in hex format.\n    \"\"\"\n    with open(filename, 'rb') as fin:\n        data = fin.read()\n    contents = []\n    start = pos = 0\n    while pos < len(data):\n        size, rec = struct.unpack('>HH', data[pos:pos + 4])\n        if rec == 0x0502:\n            start = pos + 28\n        elif rec == 0x0700:\n            contents.append(data[start:pos])\n        pos += size\n    h = hashlib.sha1() if engine is None else engine\n    for x in sorted(contents):\n        h.update(x)\n    return h.hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_bounding_box(self):\n        if len(self.polygons) == 0:\n            return None\n        return numpy.array(((min(pts[:, 0].min() for pts in self.polygons),\n                             min(pts[:, 1].min() for pts in self.polygons)),\n                            (max(pts[:, 0].max() for pts in self.polygons),\n                             max(pts[:, 1].max() for pts in self.polygons))))", "response": "Returns the bounding box of the polygon."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrotate the set of polygons around a center point.", "response": "def rotate(self, angle, center=(0, 0)):\n        \"\"\"\n        Rotate this object.\n\n        Parameters\n        ----------\n        angle : number\n            The angle of rotation (in *radians*).\n        center : array-like[2]\n            Center point for the rotation.\n\n        Returns\n        -------\n        out : ``PolygonSet``\n            This object.\n        \"\"\"\n        ca = numpy.cos(angle)\n        sa = numpy.sin(angle)\n        sa = numpy.array((-sa, sa))\n        c0 = numpy.array(center)\n        self.polygons = [(points - c0) * ca + (points - c0)[:, ::-1] * sa + c0\n                         for points in self.polygons]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scale(self, scalex, scaley=None, center=(0, 0)):\n        c0 = numpy.array(center)\n        s = scalex if scaley is None else numpy.array((scalex, scaley))\n        self.polygons = [(points - c0) * s + c0 for points in self.polygons]\n        return self", "response": "This method scales the set of polygon points along the first axis and along the second axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_gds(self, multiplier):\n        data = []\n        for ii in range(len(self.polygons)):\n            if len(self.polygons[ii]) > 4094:\n                raise ValueError(\"[GDSPY] Polygons with more than 4094 are \"\n                                 \"not supported by the GDSII format.\")\n            data.append(\n                struct.pack('>10h', 4, 0x0800, 6, 0x0D02, self.layers[ii], 6,\n                            0x0E02, self.datatypes[ii],\n                            12 + 8 * len(self.polygons[ii]), 0x1003))\n            data.extend(\n                struct.pack('>2l', int(round(point[0] * multiplier)),\n                            int(round(point[1] * multiplier)))\n                for point in self.polygons[ii])\n            data.append(\n                struct.pack('>2l2h',\n                            int(round(self.polygons[ii][0][0] * multiplier)),\n                            int(round(self.polygons[ii][0][1] * multiplier)),\n                            4, 0x1100))\n        return b''.join(data)", "response": "Convert this object to a series of GDSII elements."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the total area of the path.", "response": "def area(self, by_spec=False):\n        \"\"\"\n        Calculate the total area of the path(s).\n\n        Parameters\n        ----------\n        by_spec : bool\n            If ``True``, the return value is a dictionary with\n            ``{(layer, datatype): area}``.\n\n        Returns\n        -------\n        out : number, dictionary\n            Area of this object.\n        \"\"\"\n        if by_spec:\n            path_area = {}\n            for poly, key in zip(self.polygons, zip(self.layers,\n                                                    self.datatypes)):\n                poly_area = 0\n                for ii in range(1, len(poly) - 1):\n                    poly_area += (poly[0][0] - poly[ii + 1][0]) * (\n                        poly[ii][1] - poly[0][1]) - (\n                            poly[0][1] - poly[ii + 1][1]) * (\n                                poly[ii][0] - poly[0][0])\n                if key in path_area:\n                    path_area[key] += 0.5 * abs(poly_area)\n                else:\n                    path_area[key] = 0.5 * abs(poly_area)\n        else:\n            path_area = 0\n            for points in self.polygons:\n                poly_area = 0\n                for ii in range(1, len(points) - 1):\n                    poly_area += (points[0][0] - points[ii + 1][0]) * (\n                        points[ii][1] - points[0][1]) - (\n                            points[0][1] - points[ii + 1][1]) * (\n                                points[ii][0] - points[0][0])\n                path_area += 0.5 * abs(poly_area)\n        return path_area"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nslicing these polygons in the horizontal and vertical directions and return the resulting polygon set.", "response": "def fracture(self, max_points=199, precision=1e-3):\n        \"\"\"\n        Slice these polygons in the horizontal and vertical directions\n        so that each resulting piece has at most ``max_points``.  This\n        operation occurs in place.\n\n        Parameters\n        ----------\n        max_points : integer\n            Maximal number of points in each resulting polygon (must be\n            greater than 4).\n        precision : float\n            Desired precision for rounding vertice coordinates.\n\n        Returns\n        -------\n        out : ``PolygonSet``\n            This object.\n        \"\"\"\n        if max_points > 4:\n            ii = 0\n            while ii < len(self.polygons):\n                if len(self.polygons[ii]) > max_points:\n                    pts0 = sorted(self.polygons[ii][:, 0])\n                    pts1 = sorted(self.polygons[ii][:, 1])\n                    ncuts = len(pts0) // max_points\n                    if pts0[-1] - pts0[0] > pts1[-1] - pts1[0]:\n                        # Vertical cuts\n                        cuts = [\n                            pts0[int(i * len(pts0) / (ncuts + 1.0) + 0.5)]\n                            for i in range(1, ncuts + 1)\n                        ]\n                        chopped = clipper._chop(self.polygons[ii], cuts, 0,\n                                                1 / precision)\n                    else:\n                        # Horizontal cuts\n                        cuts = [\n                            pts1[int(i * len(pts1) / (ncuts + 1.0) + 0.5)]\n                            for i in range(1, ncuts + 1)\n                        ]\n                        chopped = clipper._chop(self.polygons[ii], cuts, 1,\n                                                1 / precision)\n                    self.polygons.pop(ii)\n                    layer = self.layers.pop(ii)\n                    datatype = self.datatypes.pop(ii)\n                    self.polygons.extend(\n                        numpy.array(x)\n                        for x in itertools.chain.from_iterable(chopped))\n                    npols = sum(len(c) for c in chopped)\n                    self.layers.extend(layer for _ in range(npols))\n                    self.datatypes.extend(datatype for _ in range(npols))\n                else:\n                    ii += 1\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fillet(self,\n               radius,\n               points_per_2pi=128,\n               max_points=199,\n               precision=1e-3):\n        \"\"\"\n        Round the corners of these polygons and fractures them into\n        polygons with less vertices if necessary.\n\n        Parameters\n        ----------\n        radius : number, list\n            Radius of the corners.  If number: All corners filleted by\n            that amount.  If list: Specify fillet radii on a per-corner\n            basis (list length must be equal to the number of points in\n            the polygon)\n        points_per_2pi : integer\n            Number of vertices used to approximate a full circle.  The\n            number of vertices in each corner of the polygon will be the\n            fraction of this number corresponding to the angle\n            encompassed by that corner with respect to 2 pi.\n        max_points : integer\n            Maximal number of points in each resulting polygon (must be\n            greater than 4).\n        precision : float\n            Desired precision for rounding vertice coordinates in case\n            of fracturing.\n\n        Returns\n        -------\n        out : ``PolygonSet``\n            This object.\n        \"\"\"\n        two_pi = 2 * numpy.pi\n        fracture = False\n\n        for jj in range(len(self.polygons)):\n            vec = self.polygons[jj].astype(float) - numpy.roll(\n                self.polygons[jj], 1, 0)\n            length = numpy.sqrt(numpy.sum(vec**2, 1))\n            ii = numpy.flatnonzero(length)\n            if len(ii) < len(length):\n                self.polygons[jj] = self.polygons[jj][ii]\n                vec = self.polygons[jj] - numpy.roll(self.polygons[jj], 1, 0)\n                length = numpy.sqrt(numpy.sum(vec**2, 1))\n            vec[:, 0] = vec[:, 0] / length\n            vec[:, 1] = vec[:, 1] / length\n            dvec = numpy.roll(vec, -1, 0) - vec\n            norm = numpy.sqrt(numpy.sum(dvec**2, 1))\n            ii = numpy.flatnonzero(norm)\n            dvec[ii, 0] = dvec[ii, 0] / norm[ii]\n            dvec[ii, 1] = dvec[ii, 1] / norm[ii]\n            theta = numpy.arccos(numpy.sum(numpy.roll(vec, -1, 0) * vec, 1))\n            ct = numpy.cos(theta * 0.5)\n            tt = numpy.tan(theta * 0.5)\n\n            new_points = []\n            for ii in range(-1, len(self.polygons[jj]) - 1):\n                if theta[ii] > 0:\n                    a0 = -vec[ii] * tt[ii] - dvec[ii] / ct[ii]\n                    a0 = numpy.arctan2(a0[1], a0[0])\n                    a1 = vec[ii + 1] * tt[ii] - dvec[ii] / ct[ii]\n                    a1 = numpy.arctan2(a1[1], a1[0])\n                    if a1 - a0 > numpy.pi:\n                        a1 -= two_pi\n                    elif a1 - a0 < -numpy.pi:\n                        a1 += two_pi\n                    n = max(\n                        int(\n                            numpy.ceil(abs(a1 - a0) / two_pi *\n                                       points_per_2pi) + 0.5), 2)\n                    a = numpy.linspace(a0, a1, n)\n                    l = radius * tt[ii]\n                    if l > 0.49 * length[ii]:\n                        r = 0.49 * length[ii] / tt[ii]\n                        l = 0.49 * length[ii]\n                    else:\n                        r = radius\n                    if l > 0.49 * length[ii + 1]:\n                        r = 0.49 * length[ii + 1] / tt[ii]\n                    new_points.extend(r * dvec[ii] / ct[ii] +\n                                      self.polygons[jj][ii] + numpy.vstack(\n                                          (r * numpy.cos(a),\n                                           r * numpy.sin(a))).transpose())\n                else:\n                    new_points.append(self.polygons[jj][ii])\n            self.polygons[jj] = numpy.array(new_points)\n            if len(new_points) > max_points:\n                fracture = True\n\n        if fracture:\n            self.fracture(max_points, precision)\n        return self", "response": "Flips the corners of the set of polygons into a single polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef translate(self, dx, dy):\n        vec = numpy.array((dx, dy))\n        self.polygons = [points + vec for points in self.polygons]\n        return self", "response": "Moves the polygons from one place to another."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrotate this path by the specified angle.", "response": "def rotate(self, angle, center=(0, 0)):\n        \"\"\"\n        Rotate this object.\n\n        Parameters\n        ----------\n        angle : number\n            The angle of rotation (in *radians*).\n        center : array-like[2]\n            Center point for the rotation.\n\n        Returns\n        -------\n        out : ``Path``\n            This object.\n        \"\"\"\n        ca = numpy.cos(angle)\n        sa = numpy.sin(angle)\n        sa = numpy.array((-sa, sa))\n        c0 = numpy.array(center)\n        if isinstance(self.direction, basestring):\n            self.direction = _directions_dict[self.direction] * numpy.pi\n        self.direction += angle\n        cur = numpy.array((self.x, self.y)) - c0\n        self.x, self.y = cur * ca + cur[::-1] * sa + c0\n        self.polygons = [(points - c0) * ca + (points - c0)[:, ::-1] * sa + c0\n                         for points in self.polygons]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef segment(self,\n                length,\n                direction=None,\n                final_width=None,\n                final_distance=None,\n                axis_offset=0,\n                layer=0,\n                datatype=0):\n        \"\"\"\n        Add a straight section to the path.\n\n        Parameters\n        ----------\n        length : number\n            Length of the section to add.\n        direction : {'+x', '-x', '+y', '-y'} or number\n            Direction or angle (in *radians*) of rotation of the\n            segment.\n        final_width : number\n            If set, the paths of this segment will have their widths\n            linearly changed from their current value to this one.\n        final_distance : number\n            If set, the distance between paths is linearly change from\n            its current value to this one along this segment.\n        axis_offset : number\n            If set, the paths will be offset from their direction by\n            this amount.\n        layer : integer, list\n            The GDSII layer numbers for the elements of each path.  If\n            the number of layers in the list is less than the number\n            of paths, the list is repeated.\n        datatype : integer, list\n            The GDSII datatype for the elements of each path (between\n            0 and 255).  If the number of datatypes in the list is less\n            than the number of paths, the list is repeated.\n\n        Returns\n        -------\n        out : ``Path``\n            This object.\n        \"\"\"\n        if direction is None:\n            direction = self.direction\n        else:\n            self.direction = direction\n        if direction == '+x':\n            ca = 1\n            sa = 0\n        elif direction == '-x':\n            ca = -1\n            sa = 0\n        elif direction == '+y':\n            ca = 0\n            sa = 1\n        elif direction == '-y':\n            ca = 0\n            sa = -1\n        else:\n            ca = numpy.cos(direction)\n            sa = numpy.sin(direction)\n        old_x = self.x\n        old_y = self.y\n        self.x += length * ca + axis_offset * sa\n        self.y += length * sa - axis_offset * ca\n        old_w = self.w\n        old_distance = self.distance\n        if final_width is not None:\n            self.w = final_width * 0.5\n        if final_distance is not None:\n            self.distance = final_distance\n        if (self.w != 0) or (old_w != 0):\n            for ii in range(self.n):\n                d0 = ii * self.distance - (self.n - 1) * self.distance * 0.5\n                old_d0 = ii * old_distance - (self.n - 1) * old_distance * 0.5\n                self.polygons.append(\n                    numpy.array([(old_x + (old_d0 - old_w) * sa,\n                                  old_y - (old_d0 - old_w) * ca),\n                                 (old_x + (old_d0 + old_w) * sa,\n                                  old_y - (old_d0 + old_w) * ca),\n                                 (self.x + (d0 + self.w) * sa,\n                                  self.y - (d0 + self.w) * ca),\n                                 (self.x + (d0 - self.w) * sa,\n                                  self.y - (d0 - self.w) * ca)]))\n                if self.w == 0:\n                    self.polygons[-1] = self.polygons[-1][:-1, :]\n                if old_w == 0:\n                    self.polygons[-1] = self.polygons[-1][1:, :]\n            self.length += numpy.sqrt(length**2 + axis_offset**2)\n            if isinstance(layer, list):\n                self.layers.extend(\n                    (layer * (self.n // len(layer) + 1))[:self.n])\n            else:\n                self.layers.extend(layer for _ in range(self.n))\n            if isinstance(datatype, list):\n                self.datatypes.extend(\n                    (datatype * (self.n // len(datatype) + 1))[:self.n])\n            else:\n                self.datatypes.extend(datatype for _ in range(self.n))\n        return self", "response": "Adds a straight section to the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a curved section to the path. Parameters ---------- radius : number Central radius of the section. initial_angle : number Initial angle of the curve (in *radians*). final_angle : number Final angle of the curve (in *radians*). number_of_points : integer or float If integer: number of vertices that form the object (polygonal approximation). If float: approximate curvature resolution. The actual number of points is automatically calculated. max_points : integer if ``number_of_points > max_points``, the element will be fractured in smaller polygons with at most ``max_points`` each. final_width : number If set, the paths of this segment will have their widths linearly changed from their current value to this one. final_distance : number If set, the distance between paths is linearly change from its current value to this one along this segment. layer : integer, list The GDSII layer numbers for the elements of each path. If the number of layers in the list is less than the number of paths, the list is repeated. datatype : integer, list The GDSII datatype for the elements of each path (between 0 and 255). If the number of datatypes in the list is less than the number of paths, the list is repeated. Returns ------- out : ``Path`` This object. Notes ----- The GDSII specification supports only a maximum of 199 vertices per polygon.", "response": "def arc(self,\n            radius,\n            initial_angle,\n            final_angle,\n            number_of_points=0.01,\n            max_points=199,\n            final_width=None,\n            final_distance=None,\n            layer=0,\n            datatype=0):\n        \"\"\"\n        Add a curved section to the path.\n\n        Parameters\n        ----------\n        radius : number\n            Central radius of the section.\n        initial_angle : number\n            Initial angle of the curve (in *radians*).\n        final_angle : number\n            Final angle of the curve (in *radians*).\n        number_of_points : integer or float\n            If integer: number of vertices that form the object\n            (polygonal approximation).  If float: approximate curvature\n            resolution.  The actual number of points is automatically\n            calculated.\n        max_points : integer\n            if ``number_of_points > max_points``, the element will be\n            fractured in smaller polygons with at most ``max_points``\n            each.\n        final_width : number\n            If set, the paths of this segment will have their widths\n            linearly changed from their current value to this one.\n        final_distance : number\n            If set, the distance between paths is linearly change from\n            its current value to this one along this segment.\n        layer : integer, list\n            The GDSII layer numbers for the elements of each path.  If\n            the number of layers in the list is less than the number of\n            paths, the list is repeated.\n        datatype : integer, list\n            The GDSII datatype for the elements of each path (between 0\n            and 255).  If the number of datatypes in the list is less\n            than the number of paths, the list is repeated.\n\n        Returns\n        -------\n        out : ``Path``\n            This object.\n\n        Notes\n        -----\n        The GDSII specification supports only a maximum of 199 vertices\n        per polygon.\n        \"\"\"\n        warn = True\n        cx = self.x - radius * numpy.cos(initial_angle)\n        cy = self.y - radius * numpy.sin(initial_angle)\n        self.x = cx + radius * numpy.cos(final_angle)\n        self.y = cy + radius * numpy.sin(final_angle)\n        if final_angle > initial_angle:\n            self.direction = final_angle + numpy.pi * 0.5\n        else:\n            self.direction = final_angle - numpy.pi * 0.5\n        old_w = self.w\n        old_distance = self.distance\n        if final_width is not None:\n            self.w = final_width * 0.5\n        if final_distance is not None:\n            self.distance = final_distance\n        if isinstance(number_of_points, float):\n            number_of_points = 2 * int(\n                abs((final_angle - initial_angle) *\n                    (radius + max(old_distance, self.distance) *\n                     (self.n - 1) * 0.5 + max(old_w, self.w)) /\n                    number_of_points) + 0.5) + 2\n        number_of_points = max(number_of_points, 3)\n        pieces = int(numpy.ceil(number_of_points / float(max_points)))\n        number_of_points = number_of_points // pieces\n        widths = numpy.linspace(old_w, self.w, pieces + 1)\n        distances = numpy.linspace(old_distance, self.distance, pieces + 1)\n        angles = numpy.linspace(initial_angle, final_angle, pieces + 1)\n        if (self.w != 0) or (old_w != 0):\n            for jj in range(pieces):\n                for ii in range(self.n):\n                    self.polygons.append(numpy.zeros((number_of_points, 2)))\n                    r0 = radius + ii * distances[jj + 1] - (\n                        self.n - 1) * distances[jj + 1] * 0.5\n                    old_r0 = radius + ii * distances[jj] - (\n                        self.n - 1) * distances[jj] * 0.5\n                    pts2 = number_of_points // 2\n                    pts1 = number_of_points - pts2\n                    ang = numpy.linspace(angles[jj], angles[jj + 1], pts1)\n                    rad = numpy.linspace(old_r0 + widths[jj],\n                                         r0 + widths[jj + 1], pts1)\n                    self.polygons[-1][:pts1, 0] = numpy.cos(ang) * rad + cx\n                    self.polygons[-1][:pts1, 1] = numpy.sin(ang) * rad + cy\n                    if widths[jj + 1] == 0:\n                        pts1 -= 1\n                        pts2 += 1\n                    if widths[jj] == 0:\n                        self.polygons[-1][:pts1 - 1, :] = numpy.array(\n                            self.polygons[-1][1:pts1, :])\n                        pts1 -= 1\n                        pts2 += 1\n                    ang = numpy.linspace(angles[jj + 1], angles[jj], pts2)\n                    rad = numpy.linspace(r0 - widths[jj + 1],\n                                         old_r0 - widths[jj], pts2)\n                    if (rad[0] <= 0 or rad[-1] <= 0) and warn:\n                        warnings.warn(\n                            \"[GDSPY] Path arc with width larger than radius \"\n                            \"created: possible self-intersecting polygon.\",\n                            stacklevel=2)\n                        warn = False\n                    self.polygons[-1][pts1:, 0] = numpy.cos(ang) * rad + cx\n                    self.polygons[-1][pts1:, 1] = numpy.sin(ang) * rad + cy\n                self.length += abs((angles[jj + 1] - angles[jj]) * radius)\n                if isinstance(layer, list):\n                    self.layers.extend(\n                        (layer * (self.n // len(layer) + 1))[:self.n])\n                else:\n                    self.layers.extend(layer for _ in range(self.n))\n                if isinstance(datatype, list):\n                    self.datatypes.extend(\n                        (datatype * (self.n // len(datatype) + 1))[:self.n])\n                else:\n                    self.datatypes.extend(datatype for _ in range(self.n))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef turn(self,\n             radius,\n             angle,\n             number_of_points=0.01,\n             max_points=199,\n             final_width=None,\n             final_distance=None,\n             layer=0,\n             datatype=0):\n        \"\"\"\n        Add a curved section to the path.\n\n        Parameters\n        ----------\n        radius : number\n            Central radius of the section.\n        angle : {'r', 'l', 'rr', 'll'} or number\n            Angle (in *radians*) of rotation of the path.  The values\n            'r' and 'l' represent 90-degree turns cw and ccw,\n            respectively; the values 'rr' and 'll' represent analogous\n            180-degree turns.\n        number_of_points : integer or float\n            If integer: number of vertices that form the object\n            (polygonal approximation).  If float: approximate curvature\n            resolution.  The actual number of points is automatically\n            calculated.\n        max_points : integer\n            if ``number_of_points > max_points``, the element will be\n            fractured in smaller polygons with at most ``max_points``\n            each.\n        final_width : number\n            If set, the paths of this segment will have their widths\n            linearly changed from their current value to this one.\n        final_distance : number\n            If set, the distance between paths is linearly change from\n            its current value to this one along this segment.\n        layer : integer, list\n            The GDSII layer numbers for the elements of each path.  If\n            the number of layers in the list is less than the number of\n            paths, the list is repeated.\n        datatype : integer, list\n            The GDSII datatype for the elements of each path (between 0\n            and 255).  If the number of datatypes in the list is less\n            than the number of paths, the list is repeated.\n\n        Returns\n        -------\n        out : ``Path``\n            This object.\n\n        Notes\n        -----\n        The GDSII specification supports only a maximum of 199 vertices\n        per polygon.\n        \"\"\"\n        exact = True\n        if angle == 'r':\n            delta_i = _halfpi\n            delta_f = 0\n        elif angle == 'rr':\n            delta_i = _halfpi\n            delta_f = -delta_i\n        elif angle == 'l':\n            delta_i = -_halfpi\n            delta_f = 0\n        elif angle == 'll':\n            delta_i = -_halfpi\n            delta_f = -delta_i\n        elif angle < 0:\n            exact = False\n            delta_i = _halfpi\n            delta_f = delta_i + angle\n        else:\n            exact = False\n            delta_i = -_halfpi\n            delta_f = delta_i + angle\n        if self.direction == '+x':\n            self.direction = 0\n        elif self.direction == '-x':\n            self.direction = numpy.pi\n        elif self.direction == '+y':\n            self.direction = _halfpi\n        elif self.direction == '-y':\n            self.direction = -_halfpi\n        elif exact:\n            exact = False\n        self.arc(radius, self.direction + delta_i, self.direction + delta_f,\n                 number_of_points, max_points, final_width, final_distance,\n                 layer, datatype)\n        if exact:\n            self.direction = _directions_list[int(\n                round(self.direction / _halfpi)) % 4]\n        return self", "response": "This function adds a curved section to the path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a parametric curve to the path.", "response": "def parametric(self,\n                   curve_function,\n                   curve_derivative=None,\n                   number_of_evaluations=99,\n                   max_points=199,\n                   final_width=None,\n                   final_distance=None,\n                   layer=0,\n                   datatype=0):\n        \"\"\"\n        Add a parametric curve to the path.\n\n        Parameters\n        ----------\n        curve_function : function\n            Function that defines the curve.  Must be a function of one\n            argument (that varies from 0 to 1) that returns a 2-element\n            list, tuple or array (x, y).\n        curve_derivative : function\n            If set, it should be the derivative of the curve function.\n            Must be a function of one argument (that varies from 0 to 1)\n            that returns a 2-element list, tuple or array (x,y).  If\n            ``None``, the derivative will be calculated numerically.\n        number_of_evaluations : integer\n            Number of points where the curve function will be evaluated.\n            The final segment will have twice this number of points.\n        max_points : integer\n            If ``2 * number_of_evaluations > max_points``, the element\n            will be fractured in smaller polygons with at most\n            ``max_points`` each.\n        final_width : number or function\n            If set to a number, the paths of this segment will have\n            their widths linearly changed from their current value to\n            this one.  If set to a function, it must be a function of\n            one argument (that varies from 0 to 1) and returns the width\n            of the path.\n        final_distance : number or function\n            If set to ta number, the distance between paths is linearly\n            change from its current value to this one.  If set to a\n            function, it must be a function of one argument (that varies\n            from 0 to 1) and returns the width of the path.\n        layer : integer, list\n            The GDSII layer numbers for the elements of each path.  If\n            the number of layers in the list is less than the number of\n            paths, the list is repeated.\n        datatype : integer, list\n            The GDSII datatype for the elements of each path (between 0\n            and 255).  If the number of datatypes in the list is less\n            than the number of paths, the list is repeated.\n\n        Returns\n        -------\n        out : ``Path``\n            This object.\n\n        Notes\n        -----\n        The norm of the vector returned by ``curve_derivative`` is not\n        important.  Only the direction is used.\n\n        The GDSII specification supports only a maximum of 199 vertices\n        per polygon.\n\n        Examples\n        --------\n        >>> def my_parametric_curve(t):\n        ...         return (2**t, t**2)\n        >>> def my_parametric_curve_derivative(t):\n        ...         return (0.69315 * 2**t, 2 * t)\n        >>> my_path.parametric(my_parametric_curve,\n        ...                    my_parametric_curve_derivative)\n        \"\"\"\n        pieces = int(numpy.ceil(2 * number_of_evaluations / float(max_points)))\n        number_of_evaluations = number_of_evaluations // pieces\n        boundaries = numpy.linspace(0, 1, pieces + 1)\n        if not callable(final_width):\n            old_w = self.w\n            if final_width is not None:\n                self.w = final_width * 0.5\n\n            def final_width(u):\n                return 2 * (old_w + u * (self.w - old_w))\n\n        if not callable(final_distance):\n            old_distance = self.distance\n            if final_distance is not None:\n                self.distance = final_distance\n\n            def final_distance(u):\n                return old_distance + u * (self.distance - old_distance)\n\n        if curve_derivative is None:\n\n            def numerical_diff(t):\n                delta = 0.5 / (number_of_evaluations - 1.0)\n                if t == 0:\n                    x0, y0 = curve_function(0)\n                    x1, y1 = curve_function(delta)\n                elif t == 1:\n                    x0, y0 = curve_function(1 - delta)\n                    x1, y1 = curve_function(1)\n                else:\n                    x0, y0 = curve_function(t - delta)\n                    x1, y1 = curve_function(t + delta)\n                return (x1 - x0, y1 - y0)\n\n            curve_derivative = numerical_diff\n        orgn = numpy.array((self.x, self.y))\n        refl = numpy.array((-1, 1))\n        for kk in range(pieces):\n            uu = numpy.linspace(boundaries[kk], boundaries[kk + 1],\n                                number_of_evaluations)\n            width = numpy.array([final_width(u) for u in uu]).reshape(\n                number_of_evaluations, 1) * 0.5\n            dist = numpy.array([final_distance(u) for u in uu]).reshape(\n                number_of_evaluations, 1)\n            x0 = numpy.array([curve_function(u) for u in uu]) + orgn\n            dx = numpy.array([curve_derivative(u) for u in uu])\n            dx = dx[:, ::-1] * refl / numpy.sqrt(\n                (dx * dx).sum(1)).reshape(number_of_evaluations, 1)\n            self.length += numpy.sqrt(((x0[1:, :] - x0[:-1, :])\n                                       **2).sum(1)).sum()\n            for ii in range(self.n):\n                p1 = x0 + dx * (dist * (ii - (self.n - 1) * 0.5) + width)\n                p2 = (x0 + dx * (dist * (ii -\n                                         (self.n - 1) * 0.5) - width))[::-1, :]\n                if width[number_of_evaluations - 1, 0] == 0:\n                    p2 = p2[1:]\n                if width[0, 0] == 0:\n                    p1 = p1[1:]\n                self.polygons.append(numpy.concatenate((p1, p2)))\n            if isinstance(layer, list):\n                self.layers.extend(\n                    (layer * (self.n // len(layer) + 1))[:self.n])\n            else:\n                self.layers.extend(layer for _ in range(self.n))\n            if isinstance(datatype, list):\n                self.datatypes.extend(\n                    (datatype * (self.n // len(datatype) + 1))[:self.n])\n            else:\n                self.datatypes.extend(datatype for _ in range(self.n))\n        self.x = x0[-1, 0]\n        self.y = x0[-1, 1]\n        self.direction = numpy.arctan2(-dx[-1, 0], dx[-1, 1])\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_gds(self, multiplier):\n        text = self.text\n        if len(text) % 2 != 0:\n            text = text + '\\0'\n        data = struct.pack('>11h', 4, 0x0C00, 6, 0x0D02, self.layer, 6, 0x1602,\n                           self.texttype, 6, 0x1701, self.anchor)\n        if (self.rotation is not None) or (self.magnification is\n                                           not None) or self.x_reflection:\n            word = 0\n            values = b''\n            if self.x_reflection:\n                word += 0x8000\n            if not (self.magnification is None):\n                # This flag indicates that the magnification is absolute, not\n                # relative (not supported).\n                #word += 0x0004\n                values += struct.pack('>2h', 12, 0x1B05) + _eight_byte_real(\n                    self.magnification)\n            if not (self.rotation is None):\n                # This flag indicates that the rotation is absolute, not\n                # relative (not supported).\n                #word += 0x0002\n                values += struct.pack('>2h', 12, 0x1C05) + _eight_byte_real(\n                    self.rotation)\n            data += struct.pack('>2hH', 6, 0x1A01, word) + values\n        return data + struct.pack(\n            '>2h2l2h', 12, 0x1003, int(round(self.position[0] * multiplier)),\n            int(round(self.position[1] * multiplier)), 4 + len(text),\n            0x1906) + text.encode('ascii') + struct.pack('>2h', 4, 0x1100)", "response": "Convert this label to a GDSII structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate(self, dx, dy):\n        self.position = numpy.array((dx + self.position[0],\n                                     dy + self.position[1]))\n\n        return self", "response": "Move the text from one place to another."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert this cell to a GDSII structure.", "response": "def to_gds(self, multiplier, timestamp=None):\n        \"\"\"\n        Convert this cell to a GDSII structure.\n\n        Parameters\n        ----------\n        multiplier : number\n            A number that multiplies all dimensions written in the GDSII\n            structure.\n        timestamp : datetime object\n            Sets the GDSII timestamp.  If ``None``, the current time is\n            used.\n\n        Returns\n        -------\n        out : string\n            The GDSII binary string that represents this cell.\n        \"\"\"\n        now = datetime.datetime.today() if timestamp is None else timestamp\n        name = self.name\n        if len(name) % 2 != 0:\n            name = name + '\\0'\n        return struct.pack(\n            '>16h', 28, 0x0502, now.year, now.month, now.day, now.hour,\n            now.minute, now.second, now.year, now.month, now.day, now.hour,\n            now.minute, now.second, 4 + len(name),\n            0x0606) + name.encode('ascii') + b''.join(\n                element.to_gds(multiplier)\n                for element in self.elements) + b''.join(\n                    label.to_gds(multiplier)\n                    for label in self.labels) + struct.pack('>2h', 4, 0x0700)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self, name, exclude_from_current=False, deep_copy=False):\n        new_cell = Cell(name, exclude_from_current)\n        if deep_copy:\n            new_cell.elements = libCopy.deepcopy(self.elements)\n            new_cell.labels = libCopy.deepcopy(self.labels)\n            for ref in new_cell.get_dependencies(True):\n                if ref._bb_valid:\n                    ref._bb_valid = False\n        else:\n            new_cell.elements = list(self.elements)\n            new_cell.labels = list(self.labels)\n        return new_cell", "response": "Creates a copy of this cell."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, element):\n        if isinstance(element, list):\n            for e in element:\n                if isinstance(e, Label):\n                    self.labels.append(e)\n                else:\n                    self.elements.append(e)\n        else:\n            if isinstance(element, Label):\n                self.labels.append(element)\n            else:\n                self.elements.append(element)\n        self._bb_valid = False\n        return self", "response": "Adds a new element or list of elements to this cell."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving polygons from this cell.", "response": "def remove_polygons(self, test):\n        \"\"\"\n        Remove polygons from this cell.\n\n        The function or callable ``test`` is called for each polygon in\n        the cell.  If its return value evaluates to ``True``, the\n        corresponding polygon is removed from the cell.\n\n        Parameters\n        ----------\n        test : callable\n            Test function to query whether a polygon should be removed.\n            The function is called with arguments:\n            ``(points, layer, datatype)``\n\n        Returns\n        -------\n        out : ``Cell``\n            This cell.\n\n        Examples\n        --------\n        Remove polygons in layer 1:\n\n        >>> cell.remove_polygons(lambda pts, layer, datatype:\n        ...                      layer == 1)\n\n        Remove polygons with negative x coordinates:\n\n        >>> cell.remove_polygons(lambda pts, layer, datatype:\n        ...                      any(pts[:, 0] < 0))\n        \"\"\"\n        empty = []\n        for element in self.elements:\n            if isinstance(element, PolygonSet):\n                ii = 0\n                while ii < len(element.polygons):\n                    if test(element.polygons[ii], element.layers[ii],\n                            element.datatypes[ii]):\n                        element.polygons.pop(ii)\n                        element.layers.pop(ii)\n                        element.datatypes.pop(ii)\n                    else:\n                        ii += 1\n                if len(element.polygons) == 0:\n                    empty.append(element)\n        for element in empty:\n            self.elements.remove(element)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving the labels from this cell.", "response": "def remove_labels(self, test):\n        \"\"\"\n        Remove labels from this cell.\n\n        The function or callable ``test`` is called for each label in\n        the cell.  If its return value evaluates to ``True``, the\n        corresponding label is removed from the cell.\n\n        Parameters\n        ----------\n        test : callable\n            Test function to query whether a label should be removed.\n            The function is called with the label as the only argument.\n\n        Returns\n        -------\n        out : ``Cell``\n            This cell.\n\n        Examples\n        --------\n        Remove labels in layer 1:\n\n        >>> cell.remove_labels(lambda lbl: lbl.layer == 1)\n        \"\"\"\n        ii = 0\n        while ii < len(self.labels):\n            if test(self.labels[ii]):\n                self.labels.pop(ii)\n            else:\n                ii += 1\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef area(self, by_spec=False):\n        if by_spec:\n            cell_area = {}\n            for element in self.elements:\n                element_area = element.area(True)\n                for ll in element_area.keys():\n                    if ll in cell_area:\n                        cell_area[ll] += element_area[ll]\n                    else:\n                        cell_area[ll] = element_area[ll]\n        else:\n            cell_area = 0\n            for element in self.elements:\n                cell_area += element.area()\n        return cell_area", "response": "Calculate the total area of the elements in this cell including the elements in the cell references and arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_layers(self):\n        layers = set()\n        for element in self.elements:\n            if isinstance(element, PolygonSet):\n                layers.update(element.layers)\n            elif isinstance(element, CellReference) or isinstance(\n                    element, CellArray):\n                layers.update(element.ref_cell.get_layers())\n        for label in self.labels:\n            layers.add(label.layer)\n        return layers", "response": "Returns a set of layers used in this cell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_datatypes(self):\n        datatypes = set()\n        for element in self.elements:\n            if isinstance(element, PolygonSet):\n                datatypes.update(element.datatypes)\n            elif isinstance(element, CellReference) or isinstance(\n                    element, CellArray):\n                datatypes.update(element.ref_cell.get_datatypes())\n        return datatypes", "response": "Returns a set of datatypes used in this cell."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the bounding box of this cell.", "response": "def get_bounding_box(self):\n        \"\"\"\n        Returns the bounding box for this cell.\n\n        Returns\n        -------\n        out : Numpy array[2,2] or ``None``\n            Bounding box of this cell [[x_min, y_min], [x_max, y_max]],\n            or ``None`` if the cell is empty.\n        \"\"\"\n        if len(self.elements) == 0:\n            return None\n        if not (self._bb_valid and\n                all(ref._bb_valid for ref in self.get_dependencies(True))):\n            bb = numpy.array(((1e300, 1e300), (-1e300, -1e300)))\n            all_polygons = []\n            for element in self.elements:\n                if isinstance(element, PolygonSet):\n                    all_polygons.extend(element.polygons)\n                elif isinstance(element, CellReference) or isinstance(\n                        element, CellArray):\n                    element_bb = element.get_bounding_box()\n                    if element_bb is not None:\n                        bb[0, 0] = min(bb[0, 0], element_bb[0, 0])\n                        bb[0, 1] = min(bb[0, 1], element_bb[0, 1])\n                        bb[1, 0] = max(bb[1, 0], element_bb[1, 0])\n                        bb[1, 1] = max(bb[1, 1], element_bb[1, 1])\n            if len(all_polygons) > 0:\n                all_points = numpy.concatenate(all_polygons).transpose()\n                bb[0, 0] = min(bb[0, 0], all_points[0].min())\n                bb[0, 1] = min(bb[0, 1], all_points[1].min())\n                bb[1, 0] = max(bb[1, 0], all_points[0].max())\n                bb[1, 1] = max(bb[1, 1], all_points[1].max())\n            self._bb_valid = True\n            _bounding_boxes[self] = bb\n        return _bounding_boxes[self]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_polygons(self, by_spec=False, depth=None):\n        if depth is not None and depth < 0:\n            bb = self.get_bounding_box()\n            if bb is None:\n                return {} if by_spec else []\n            pts = [\n                numpy.array([(bb[0, 0], bb[0, 1]), (bb[0, 0], bb[1, 1]),\n                             (bb[1, 0], bb[1, 1]), (bb[1, 0], bb[0, 1])])\n            ]\n            polygons = {self.name: pts} if by_spec else pts\n        else:\n            if by_spec:\n                polygons = {}\n                for element in self.elements:\n                    if isinstance(element, PolygonSet):\n                        for ii in range(len(element.polygons)):\n                            key = (element.layers[ii], element.datatypes[ii])\n                            if key in polygons:\n                                polygons[key].append(\n                                    numpy.array(element.polygons[ii]))\n                            else:\n                                polygons[key] = [\n                                    numpy.array(element.polygons[ii])\n                                ]\n                    else:\n                        cell_polygons = element.get_polygons(\n                            True, None if depth is None else depth - 1)\n                        for kk in cell_polygons.keys():\n                            if kk in polygons:\n                                polygons[kk].extend(cell_polygons[kk])\n                            else:\n                                polygons[kk] = cell_polygons[kk]\n            else:\n                polygons = []\n                for element in self.elements:\n                    if isinstance(element, PolygonSet):\n                        for points in element.polygons:\n                            polygons.append(numpy.array(points))\n                    else:\n                        polygons.extend(\n                            element.get_polygons(\n                                depth=None if depth is None else depth - 1))\n        return polygons", "response": "Returns a list of polygons in this cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_labels(self, depth=None):\n        labels = libCopy.deepcopy(self.labels)\n        if depth is None or depth > 0:\n            for element in self.elements:\n                if isinstance(element, CellReference):\n                    labels.extend(\n                        element.get_labels(None if depth is None else depth -\n                                           1))\n                elif isinstance(element, CellArray):\n                    labels.extend(\n                        element.get_labels(None if depth is None else depth -\n                                           1))\n        return labels", "response": "Returns a list with a copy of the labels in this cell and its elements."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of the cells referenced by this cell as references.", "response": "def get_dependencies(self, recursive=False):\n        \"\"\"\n        Returns a list of the cells included in this cell as references.\n\n        Parameters\n        ----------\n        recursive : bool\n            If True returns cascading dependencies.\n\n        Returns\n        -------\n        out : set of ``Cell``\n            List of the cells referenced by this cell.\n        \"\"\"\n        dependencies = set()\n        for element in self.elements:\n            if isinstance(element, CellReference) or isinstance(\n                    element, CellArray):\n                if recursive:\n                    dependencies.update(\n                        element.ref_cell.get_dependencies(True))\n                dependencies.add(element.ref_cell)\n        return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nflattening all CellReference and CellArray elements in this cell into real polygons and labels.", "response": "def flatten(self,\n                single_layer=None,\n                single_datatype=None,\n                single_texttype=None):\n        \"\"\"\n        Flatten all ``CellReference`` and ``CellArray`` elements in this\n        cell into real polygons and labels, instead of references.\n\n        Parameters\n        ----------\n        single_layer : integer or None\n            If not ``None``, all polygons will be transfered to the\n            layer indicated by this number.\n        single_datatype : integer or None\n            If not ``None``, all polygons will be transfered to the\n            datatype indicated by this number.\n        single_datatype : integer or None\n            If not ``None``, all labels will be transfered to the\n            texttype indicated by this number.\n\n        Returns\n        -------\n        out : ``Cell``\n            This cell.\n        \"\"\"\n        self.labels = self.get_labels()\n        if single_layer is not None:\n            for lbl in self.labels:\n                lbl.layer = single_layer\n        if single_texttype is not None:\n            for lbl in self.labels:\n                lbl.texttype = single_texttype\n        if single_layer is None or single_datatype is None:\n            poly_dic = self.get_polygons(True)\n            self.elements = []\n            if single_layer is None and single_datatype is None:\n                for ld in poly_dic.keys():\n                    self.add(PolygonSet(poly_dic[ld], *ld, verbose=False))\n            elif single_layer is None:\n                for ld in poly_dic.keys():\n                    self.add(\n                        PolygonSet(\n                            poly_dic[ld],\n                            ld[0],\n                            single_datatype,\n                            verbose=False))\n            else:\n                for ld in poly_dic.keys():\n                    self.add(\n                        PolygonSet(\n                            poly_dic[ld], single_layer, ld[1], verbose=False))\n        else:\n            polygons = self.get_polygons()\n            self.elements = []\n            self.add(\n                PolygonSet(\n                    polygons, single_layer, single_datatype, verbose=False))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_gds(self, multiplier):\n        name = self.ref_cell.name\n        if len(name) % 2 != 0:\n            name = name + '\\0'\n        data = struct.pack('>4h', 4, 0x0A00, 4 + len(name),\n                           0x1206) + name.encode('ascii')\n        if (self.rotation is not None) or (self.magnification is\n                                           not None) or self.x_reflection:\n            word = 0\n            values = b''\n            if self.x_reflection:\n                word += 0x8000\n            if not (self.magnification is None):\n                # This flag indicates that the magnification is absolute, not\n                # relative (not supported).\n                #word += 0x0004\n                values += struct.pack('>2h', 12, 0x1B05) + _eight_byte_real(\n                    self.magnification)\n            if not (self.rotation is None):\n                # This flag indicates that the rotation is absolute, not\n                # relative (not supported).\n                #word += 0x0002\n                values += struct.pack('>2h', 12, 0x1C05) + _eight_byte_real(\n                    self.rotation)\n            data += struct.pack('>2hH', 6, 0x1A01, word) + values\n        return data + struct.pack(\n            '>2h2l2h', 12, 0x1003, int(round(self.origin[0] * multiplier)),\n            int(round(self.origin[1] * multiplier)), 4, 0x1100)", "response": "Convert this object to a GDSII element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef area(self, by_spec=False):\n        if not isinstance(self.ref_cell, Cell):\n            return dict() if by_spec else 0\n        if self.magnification is None:\n            return self.ref_cell.area(by_spec)\n        else:\n            if by_spec:\n                factor = self.magnification**2\n                cell_area = self.ref_cell.area(True)\n                for kk in cell_area.keys():\n                    cell_area[kk] *= factor\n                return cell_area\n            else:\n                return self.ref_cell.area() * self.magnification**2", "response": "Calculate the total area of the referenced cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_polygons(self, by_spec=False, depth=None):\n        if not isinstance(self.ref_cell, Cell):\n            return dict() if by_spec else []\n        if self.rotation is not None:\n            ct = numpy.cos(self.rotation * numpy.pi / 180.0)\n            st = numpy.sin(self.rotation * numpy.pi / 180.0)\n            st = numpy.array([-st, st])\n        if self.x_reflection:\n            xrefl = numpy.array([1, -1], dtype='int')\n        if self.magnification is not None:\n            mag = numpy.array([self.magnification, self.magnification])\n        if self.origin is not None:\n            orgn = numpy.array(self.origin)\n        if by_spec:\n            polygons = self.ref_cell.get_polygons(True, depth)\n            for kk in polygons.keys():\n                for ii in range(len(polygons[kk])):\n                    if self.x_reflection:\n                        polygons[kk][ii] = polygons[kk][ii] * xrefl\n                    if self.magnification is not None:\n                        polygons[kk][ii] = polygons[kk][ii] * mag\n                    if self.rotation is not None:\n                        polygons[kk][ii] = (polygons[kk][ii] * ct +\n                                            polygons[kk][ii][:, ::-1] * st)\n                    if self.origin is not None:\n                        polygons[kk][ii] = polygons[kk][ii] + orgn\n        else:\n            polygons = self.ref_cell.get_polygons(depth=depth)\n            for ii in range(len(polygons)):\n                if self.x_reflection:\n                    polygons[ii] = polygons[ii] * xrefl\n                if self.magnification is not None:\n                    polygons[ii] = polygons[ii] * mag\n                if self.rotation is not None:\n                    polygons[ii] = (\n                        polygons[ii] * ct + polygons[ii][:, ::-1] * st)\n                if self.origin is not None:\n                    polygons[ii] = polygons[ii] + orgn\n        return polygons", "response": "Returns a list of polygons created by this reference."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_labels(self, depth=None):\n        if not isinstance(self.ref_cell, Cell):\n            return []\n        if self.rotation is not None:\n            ct = numpy.cos(self.rotation * numpy.pi / 180.0)\n            st = numpy.sin(self.rotation * numpy.pi / 180.0)\n            st = numpy.array([-st, st])\n        if self.x_reflection:\n            xrefl = numpy.array([1, -1], dtype='int')\n        if self.magnification is not None:\n            mag = numpy.array([self.magnification, self.magnification])\n        if self.origin is not None:\n            orgn = numpy.array(self.origin)\n        labels = self.ref_cell.get_labels(depth=depth)\n        for lbl in labels:\n            if self.x_reflection:\n                lbl.position = lbl.position * xrefl\n            if self.magnification is not None:\n                lbl.position = lbl.position * mag\n            if self.rotation is not None:\n                lbl.position = lbl.position * ct + lbl.position[::-1] * st\n            if self.origin is not None:\n                lbl.position = lbl.position + orgn\n        return labels", "response": "Returns a list of labels created by this reference."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the bounding box of this reference cell.", "response": "def get_bounding_box(self):\n        \"\"\"\n        Returns the bounding box for this reference.\n\n        Returns\n        -------\n        out : Numpy array[2,2] or ``None``\n            Bounding box of this cell [[x_min, y_min], [x_max, y_max]],\n            or ``None`` if the cell is empty.\n        \"\"\"\n        if not isinstance(self.ref_cell, Cell):\n            return None\n        if (self.rotation is None and self.magnification is None and\n                self.x_reflection is None):\n            key = self\n        else:\n            key = (self.ref_cell, self.rotation, self.magnification,\n                   self.x_reflection)\n        deps = self.ref_cell.get_dependencies(True)\n        if not (self.ref_cell._bb_valid and\n                all(ref._bb_valid for ref in deps) and key in _bounding_boxes):\n            for ref in deps:\n                ref.get_bounding_box()\n            self.ref_cell.get_bounding_box()\n            tmp = self.origin\n            self.origin = None\n            polygons = self.get_polygons()\n            self.origin = tmp\n            if len(polygons) == 0:\n                bb = None\n            else:\n                all_points = numpy.concatenate(polygons).transpose()\n                bb = numpy.array(((all_points[0].min(), all_points[1].min()),\n                                  (all_points[0].max(), all_points[1].max())))\n            _bounding_boxes[key] = bb\n        else:\n            bb = _bounding_boxes[key]\n        if self.origin is None or bb is None:\n            return bb\n        else:\n            return bb + numpy.array(((self.origin[0], self.origin[1]),\n                                     (self.origin[0], self.origin[1])))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmove the reference from one place to another.", "response": "def translate(self, dx, dy):\n        \"\"\"\n        Move the reference from one place to another\n\n        Parameters\n        ----------\n        dx : float\n            distance to move in the x-direction\n        dy : float\n            distance to move in the y-direction\n\n        Returns\n        -------\n        out : ``CellReference``\n            This object.\n        \"\"\"\n        self.origin = (self.origin[0] + dx, self.origin[1] + dy)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert this object to a GDSII element.", "response": "def to_gds(self, multiplier):\n        \"\"\"\n        Convert this object to a GDSII element.\n\n        Parameters\n        ----------\n        multiplier : number\n            A number that multiplies all dimensions written in the GDSII\n            element.\n\n        Returns\n        -------\n        out : string\n            The GDSII binary string that represents this object.\n        \"\"\"\n        name = self.ref_cell.name\n        if len(name) % 2 != 0:\n            name = name + '\\0'\n        data = struct.pack('>4h', 4, 0x0B00, 4 + len(name),\n                           0x1206) + name.encode('ascii')\n        x2 = self.origin[0] + self.columns * self.spacing[0]\n        y2 = self.origin[1]\n        x3 = self.origin[0]\n        y3 = self.origin[1] + self.rows * self.spacing[1]\n        if (self.rotation is not None) or (self.magnification is\n                                           not None) or self.x_reflection:\n            word = 0\n            values = b''\n            if self.x_reflection:\n                word += 0x8000\n                y3 = 2 * self.origin[1] - y3\n            if not (self.magnification is None):\n                # This flag indicates that the magnification is absolute, not\n                # relative (not supported).\n                #word += 0x0004\n                values += struct.pack('>2h', 12, 0x1B05) + _eight_byte_real(\n                    self.magnification)\n            if not (self.rotation is None):\n                # This flag indicates that the rotation is absolute, not\n                # relative (not supported).\n                #word += 0x0002\n                sa = numpy.sin(self.rotation * numpy.pi / 180.0)\n                ca = numpy.cos(self.rotation * numpy.pi / 180.0)\n                tmp = (x2 - self.origin[0]) * ca - (\n                    y2 - self.origin[1]) * sa + self.origin[0]\n                y2 = (x2 - self.origin[0]) * sa + (\n                    y2 - self.origin[1]) * ca + self.origin[1]\n                x2 = tmp\n                tmp = (x3 - self.origin[0]) * ca - (\n                    y3 - self.origin[1]) * sa + self.origin[0]\n                y3 = (x3 - self.origin[0]) * sa + (\n                    y3 - self.origin[1]) * ca + self.origin[1]\n                x3 = tmp\n                values += struct.pack('>2h', 12, 0x1C05) + _eight_byte_real(\n                    self.rotation)\n            data += struct.pack('>2hH', 6, 0x1A01, word) + values\n        return data + struct.pack(\n            '>6h6l2h', 8, 0x1302, self.columns, self.rows, 28, 0x1003,\n            int(round(self.origin[0] * multiplier)),\n            int(round(self.origin[1] * multiplier)), int(\n                round(x2 * multiplier)), int(round(y2 * multiplier)),\n            int(round(x3 * multiplier)), int(round(\n                y3 * multiplier)), 4, 0x1100)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_polygons(self, by_spec=False, depth=None):\n        if not isinstance(self.ref_cell, Cell):\n            return dict() if by_spec else []\n        if self.rotation is not None:\n            ct = numpy.cos(self.rotation * numpy.pi / 180.0)\n            st = numpy.sin(self.rotation * numpy.pi / 180.0)\n            st = numpy.array([-st, st])\n        if self.magnification is not None:\n            mag = numpy.array([self.magnification, self.magnification])\n        if self.origin is not None:\n            orgn = numpy.array(self.origin)\n        if self.x_reflection:\n            xrefl = numpy.array([1, -1], dtype='int')\n        if by_spec:\n            cell_polygons = self.ref_cell.get_polygons(True, depth)\n            polygons = {}\n            for kk in cell_polygons.keys():\n                polygons[kk] = []\n                for ii in range(self.columns):\n                    for jj in range(self.rows):\n                        spc = numpy.array(\n                            [self.spacing[0] * ii, self.spacing[1] * jj])\n                        for points in cell_polygons[kk]:\n                            if self.magnification:\n                                polygons[kk].append(points * mag + spc)\n                            else:\n                                polygons[kk].append(points + spc)\n                            if self.x_reflection:\n                                polygons[kk][-1] = polygons[kk][-1] * xrefl\n                            if self.rotation is not None:\n                                polygons[kk][-1] = (\n                                    polygons[kk][-1] * ct +\n                                    polygons[kk][-1][:, ::-1] * st)\n                            if self.origin is not None:\n                                polygons[kk][-1] = polygons[kk][-1] + orgn\n        else:\n            cell_polygons = self.ref_cell.get_polygons(depth=depth)\n            polygons = []\n            for ii in range(self.columns):\n                for jj in range(self.rows):\n                    spc = numpy.array(\n                        [self.spacing[0] * ii, self.spacing[1] * jj])\n                    for points in cell_polygons:\n                        if self.magnification:\n                            polygons.append(points * mag + spc)\n                        else:\n                            polygons.append(points + spc)\n                        if self.x_reflection:\n                            polygons[-1] = polygons[-1] * xrefl\n                        if self.rotation is not None:\n                            polygons[-1] = (\n                                polygons[-1] * ct + polygons[-1][:, ::-1] * st)\n                        if self.origin is not None:\n                            polygons[-1] = polygons[-1] + orgn\n        return polygons", "response": "Returns a list of polygons created by this reference."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_labels(self, depth=None):\n        if not isinstance(self.ref_cell, Cell):\n            return []\n        if self.rotation is not None:\n            ct = numpy.cos(self.rotation * numpy.pi / 180.0)\n            st = numpy.sin(self.rotation * numpy.pi / 180.0)\n            st = numpy.array([-st, st])\n        if self.magnification is not None:\n            mag = numpy.array([self.magnification, self.magnification])\n        if self.origin is not None:\n            orgn = numpy.array(self.origin)\n        if self.x_reflection:\n            xrefl = numpy.array([1, -1], dtype='int')\n        cell_labels = self.ref_cell.get_labels(depth=depth)\n        labels = []\n        for ii in range(self.columns):\n            for jj in range(self.rows):\n                spc = numpy.array([self.spacing[0] * ii, self.spacing[1] * jj])\n                for clbl in cell_labels:\n                    lbl = libCopy.deepcopy(clbl)\n                    if self.magnification:\n                        lbl.position = lbl.position * mag + spc\n                    else:\n                        lbl.position = lbl.position + spc\n                    if self.x_reflection:\n                        lbl.position = lbl.position * xrefl\n                    if self.rotation is not None:\n                        lbl.position = lbl.position * ct + lbl.position[::-1] * st\n                    if self.origin is not None:\n                        lbl.position = lbl.position + orgn\n                    labels.append(lbl)\n        return labels", "response": "Returns a list of labels created by this reference."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd one or more cells to the library.", "response": "def add(self, cell, overwrite_duplicate=False):\n        \"\"\"\n        Add one or more cells to the library.\n\n        Parameters\n        ----------\n        cell : ``Cell`` of list of ``Cell``\n            Cells to be included in the library.\n        overwrite_duplicate : bool\n            If True an existing cell with the same name in the library\n            will be overwritten.\n\n        Returns\n        -------\n        out : ``GdsLibrary``\n            This object.\n        \"\"\"\n        if isinstance(cell, Cell):\n            if (not overwrite_duplicate and cell.name in self.cell_dict and\n                    self.cell_dict[cell.name] is not cell):\n                raise ValueError(\"[GDSPY] cell named {0} already present in \"\n                                 \"library.\".format(cell.name))\n            self.cell_dict[cell.name] = cell\n        else:\n            for c in cell:\n                if (not overwrite_duplicate and c.name in self.cell_dict and\n                        self.cell_dict[c.name] is not c):\n                    raise ValueError(\"[GDSPY] cell named {0} already present \"\n                                     \"in library.\".format(c.name))\n                self.cell_dict[c.name] = c\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the GDSII library to a file.", "response": "def write_gds(self, outfile, cells=None, timestamp=None):\n        \"\"\"\n        Write the GDSII library to a file.\n\n        The dimensions actually written on the GDSII file will be the\n        dimensions of the objects created times the ratio\n        ``unit/precision``.  For example, if a circle with radius 1.5 is\n        created and we set ``unit=1.0e-6`` (1 um) and\n        ``precision=1.0e-9`` (1 nm), the radius of the circle will be\n        1.5 um and the GDSII file will contain the dimension 1500 nm.\n\n        Parameters\n        ----------\n        outfile : file or string\n            The file (or path) where the GDSII stream will be written.\n            It must be opened for writing operations in binary format.\n        cells : array-like\n            The list of cells or cell names to be included in the\n            library.  If ``None``, all cells are used.\n        timestamp : datetime object\n            Sets the GDSII timestamp.  If ``None``, the current time is\n            used.\n\n        Notes\n        -----\n        Only the specified cells are written.  The user is responsible\n        for ensuring all cell dependencies are satisfied.\n        \"\"\"\n        if isinstance(outfile, basestring):\n            outfile = open(outfile, 'wb')\n            close = True\n        else:\n            close = False\n        now = datetime.datetime.today() if timestamp is None else timestamp\n        name = self.name if len(self.name) % 2 == 0 else (self.name + '\\0')\n        outfile.write(\n            struct.pack('>19h', 6, 0x0002, 0x0258, 28, 0x0102, now.year,\n                        now.month, now.day, now.hour, now.minute, now.second,\n                        now.year, now.month, now.day, now.hour, now.minute,\n                        now.second, 4 + len(name), 0x0206) +\n            name.encode('ascii') + struct.pack('>2h', 20, 0x0305) +\n            _eight_byte_real(self.precision / self.unit) +\n            _eight_byte_real(self.precision))\n        if cells is None:\n            cells = self.cell_dict.values()\n        else:\n            cells = [self.cell_dict.get(c, c) for c in cells]\n        for cell in cells:\n            outfile.write(cell.to_gds(self.unit / self.precision))\n        outfile.write(struct.pack('>2h', 4, 0x0400))\n        if close:\n            outfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_gds(self,\n                 infile,\n                 units='skip',\n                 rename={},\n                 layers={},\n                 datatypes={},\n                 texttypes={}):\n        \"\"\"\n        Read a GDSII file into this library.\n\n        Parameters\n        ----------\n        infile : file or string\n            GDSII stream file (or path) to be imported.  It must be\n            opened for reading in binary format.\n        units : {'convert', 'import', 'skip'}\n            Controls how to scale and use the units in the imported\n            file.  ``'convert'``: the imported geometry is scaled to\n            this library units. ``'import'``: the unit and precision in\n            this library are replaced by those from the imported file.\n            ``'skip'``: the imported geometry is not scaled and units\n            are not replaced; the geometry is imported in the *user\n            units* of the file.\n        rename : dictionary\n            Dictionary used to rename the imported cells.  Keys and\n            values must be strings.\n        layers : dictionary\n            Dictionary used to convert the layers in the imported cells.\n            Keys and values must be integers.\n        datatypes : dictionary\n            Dictionary used to convert the datatypes in the imported\n            cells.  Keys and values must be integers.\n        texttypes : dictionary\n            Dictionary used to convert the text types in the imported\n            cells.  Keys and values must be integers.\n\n        Returns\n        -------\n        out : ``GdsLibrary``\n            This object.\n\n        Notes\n        -----\n        Not all features from the GDSII specification are currently\n        supported.  A warning will be produced if any unsuported\n        features are found in the imported file.\n        \"\"\"\n        self._references = []\n        if isinstance(infile, basestring):\n            infile = open(infile, 'rb')\n            close = True\n        else:\n            close = False\n        emitted_warnings = []\n        record = self._read_record(infile)\n        kwargs = {}\n        create_element = None\n        factor = 1\n        cell = None\n        while record is not None:\n            # LAYER\n            if record[0] == 0x0d:\n                kwargs['layer'] = layers.get(record[1][0], record[1][0])\n            # DATATYPE\n            elif record[0] == 0x0e:\n                kwargs['datatype'] = datatypes.get(record[1][0], record[1][0])\n            # TEXTTYPE\n            elif record[0] == 0x16:\n                kwargs['texttype'] = texttypes.get(record[1][0], record[1][0])\n            # XY\n            elif record[0] == 0x10:\n                kwargs['xy'] = factor * record[1]\n            # WIDTH\n            elif record[0] == 0x0f:\n                kwargs['width'] = factor * abs(record[1][0])\n                if record[1][0] < 0 and record[0] not in emitted_warnings:\n                    warnings.warn(\n                        \"[GDSPY] Paths with absolute width value are not \"\n                        \"supported.  Scaling these paths will also scale \"\n                        \"their width.\",\n                        stacklevel=2)\n                    emitted_warnings.append(record[0])\n            # ENDEL\n            elif record[0] == 0x11:\n                if create_element is not None:\n                    cell.add(create_element(**kwargs))\n                    create_element = None\n                kwargs = {}\n            # BOUNDARY\n            elif record[0] == 0x08:\n                create_element = self._create_polygon\n            # PATH\n            elif record[0] == 0x09:\n                create_element = self._create_path\n            # TEXT\n            elif record[0] == 0x0c:\n                create_element = self._create_label\n            # SNAME\n            elif record[0] == 0x12:\n                kwargs['ref_cell'] = rename.get(record[1], record[1])\n            # COLROW\n            elif record[0] == 0x13:\n                kwargs['columns'] = record[1][0]\n                kwargs['rows'] = record[1][1]\n            # STRANS\n            elif record[0] == 0x1a:\n                kwargs['x_reflection'] = ((int(record[1][0]) & 0x8000) > 0)\n                if (int(record[1][0]) &\n                        0x0006) and record[0] not in emitted_warnings:\n                    warnings.warn(\n                        \"[GDSPY] Absolute magnification or rotation of \"\n                        \"references is not supported.  Transformations will \"\n                        \"be interpreted as relative.\",\n                        stacklevel=2)\n                    emitted_warnings.append(record[0])\n            # MAG\n            elif record[0] == 0x1b:\n                kwargs['magnification'] = record[1][0]\n            # ANGLE\n            elif record[0] == 0x1c:\n                kwargs['rotation'] = record[1][0]\n            # SREF\n            elif record[0] == 0x0a:\n                create_element = self._create_reference\n            # AREF\n            elif record[0] == 0x0b:\n                create_element = self._create_array\n            # STRNAME\n            elif record[0] == 0x06:\n                name = rename.get(record[1], record[1])\n                cell = Cell(name, exclude_from_current=True)\n                self.cell_dict[name] = cell\n            # STRING\n            elif record[0] == 0x19:\n                kwargs['text'] = record[1]\n            # ENDSTR\n            elif record[0] == 0x07:\n                cell = None\n            # UNITS\n            elif record[0] == 0x03:\n                if units == 'skip':\n                    factor = record[1][0]\n                elif units == 'import':\n                    self.unit = record[1][1] / record[1][0]\n                    self.precision = record[1][1]\n                    factor = record[1][0]\n                elif units == 'convert':\n                    factor = record[1][1] / self.unit\n                else:\n                    raise ValueError(\"[GDSPY] units must be one of 'convert', \"\n                                     \"'import' or 'skip'.\")\n            # LIBNAME\n            elif record[0] == 0x02:\n                self.name = record[1]\n            # PRESENTATION\n            elif record[0] == 0x17:\n                kwargs['anchor'] = GdsLibrary._import_anchors[int(record[1][0])\n                                                              & 0x000f]\n            # PATHTYPE\n            elif record[0] == 0x21:\n                if record[1][0] > 2:\n                    if 0x21 not in emitted_warnings:\n                        warnings.warn(\n                            \"[GDSPY] Path ends with custom size are not \"\n                            \"supported.\",\n                            RuntimeWarning,\n                            stacklevel=2)\n                        emitted_warnings.append(0x21)\n                else:\n                    kwargs['ends'] = record[1][0]\n            # ENDLIB\n            elif record[0] == 0x04:\n                for ref in self._references:\n                    if ref.ref_cell in self.cell_dict:\n                        ref.ref_cell = self.cell_dict[ref.ref_cell]\n                    elif ref.ref_cell in current_library.cell_dict:\n                        ref.ref_cell = current_library.cell_dict[ref.ref_cell]\n            # Not supported\n            elif (record[0] not in emitted_warnings and\n                  record[0] not in GdsLibrary._unused_records):\n                warnings.warn(\n                    \"[GDSPY] Record type {0} ({1:02X}) is not \"\n                    \"supported.\".format(GdsLibrary._record_name[record[0]],\n                                        record[0]),\n                    RuntimeWarning,\n                    stacklevel=2)\n                emitted_warnings.append(record[0])\n            record = self._read_record(infile)\n        if close:\n            infile.close()\n        return self", "response": "Reads a GDSII file into this library."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a complete record from a GDSII stream file.", "response": "def _read_record(self, stream):\n        \"\"\"\n        Read a complete record from a GDSII stream file.\n\n        Parameters\n        ----------\n        stream : file\n            GDSII stream file to be imported.\n\n        Returns\n        -------\n        out : 2-tuple\n            Record type and data (as a numpy.array)\n        \"\"\"\n        header = stream.read(4)\n        if len(header) < 4:\n            return None\n        size, rec_type = struct.unpack('>HH', header)\n        data_type = (rec_type & 0x00ff)\n        rec_type = rec_type // 256\n        data = None\n        if size > 4:\n            if data_type == 0x01:\n                data = numpy.array(\n                    struct.unpack('>{0}H'.format((size - 4) // 2),\n                                  stream.read(size - 4)),\n                    dtype='uint')\n            elif data_type == 0x02:\n                data = numpy.array(\n                    struct.unpack('>{0}h'.format((size - 4) // 2),\n                                  stream.read(size - 4)),\n                    dtype='int')\n            elif data_type == 0x03:\n                data = numpy.array(\n                    struct.unpack('>{0}l'.format((size - 4) // 4),\n                                  stream.read(size - 4)),\n                    dtype='int')\n            elif data_type == 0x05:\n                data = numpy.array([\n                    _eight_byte_real_to_float(stream.read(8))\n                    for _ in range((size - 4) // 8)\n                ])\n            else:\n                data = stream.read(size - 4)\n                if str is not bytes:\n                    if data[-1] == 0:\n                        data = data[:-1].decode('ascii')\n                    else:\n                        data = data.decode('ascii')\n                elif data[-1] == '\\0':\n                    data = data[:-1]\n        return [rec_type, data]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts a cell from the GDSII file and adds it to the current global library and adds the referenced dependencies to the current global library.", "response": "def extract(self, cell):\n        \"\"\"\n        Extract a cell from the this GDSII file and include it in the\n        current global library, including referenced dependencies.\n\n        Parameters\n        ----------\n        cell : ``Cell`` or string\n            Cell or name of the cell to be extracted from the imported\n            file.  Referenced cells will be automatically extracted as\n            well.\n\n        Returns\n        -------\n        out : ``Cell``\n            The extracted cell.\n        \"\"\"\n        cell = self.cell_dict.get(cell, cell)\n        current_library.add(cell)\n        current_library.add(cell.get_dependencies(True))\n        return cell"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef top_level(self):\n        top = list(self.cell_dict.values())\n        for cell in self.cell_dict.values():\n            for dependency in cell.get_dependencies():\n                if dependency in top:\n                    top.remove(dependency)\n        return top", "response": "Return the top level cells from the GDSII data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_cell(self, cell):\n        self._outfile.write(cell.to_gds(self._res))\n        return self", "response": "Writes the specified cell to the file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfinalize the GDSII stream library.", "response": "def close(self):\n        \"\"\"\n        Finalize the GDSII stream library.\n        \"\"\"\n        self._outfile.write(struct.pack('>2h', 4, 0x0400))\n        if self._close:\n            self._outfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef taper(path,\n          length,\n          final_width,\n          final_distance,\n          direction=None,\n          layer=0,\n          datatype=0):\n    '''\n    Linear tapers for the lazy.\n\n    path        : `gdspy.Path` to append the taper\n    length      : total length\n    final_width : final width of th taper\n    direction   : taper direction\n    layer       : GDSII layer number (int or list)\n    datatype    : GDSII datatype number (int or list)\n\n    Parameters `layer` and `datatype` must be of the same type. If they\n    are lists, they must have the same length. Their length indicate the\n    number of pieces that compose the taper.\n\n    Return `path`.\n    '''\n    if layer.__class__ == datatype.__class__ == [].__class__:\n        assert len(layer) == len(datatype)\n    elif isinstance(layer, int) and isinstance(datatype, int):\n        layer = [layer]\n        datatype = [datatype]\n    else:\n        raise ValueError('Parameters layer and datatype must have the same '\n                         'type (either int or list) and length.')\n    n = len(layer)\n    w = numpy.linspace(2 * path.w, final_width, n + 1)[1:]\n    d = numpy.linspace(path.distance, final_distance, n + 1)[1:]\n    l = float(length) / n\n    for i in range(n):\n        path.segment(\n            l, direction, w[i], d[i], layer=layer[i], datatype=datatype[i])\n    return path", "response": "This function creates a taper for lazy path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a new structure for a single grating site.", "response": "def grating(period,\n            number_of_teeth,\n            fill_frac,\n            width,\n            position,\n            direction,\n            lda=1,\n            sin_theta=0,\n            focus_distance=-1,\n            focus_width=-1,\n            evaluations=99,\n            layer=0,\n            datatype=0):\n    '''\n    Straight or focusing grating.\n\n    period          : grating period\n    number_of_teeth : number of teeth in the grating\n    fill_frac       : filling fraction of the teeth (w.r.t. the period)\n    width           : width of the grating\n    position        : grating position (feed point)\n    direction       : one of {'+x', '-x', '+y', '-y'}\n    lda             : free-space wavelength\n    sin_theta       : sine of incidence angle\n    focus_distance  : focus distance (negative for straight grating)\n    focus_width     : if non-negative, the focusing area is included in\n                      the result (usually for negative resists) and this\n                      is the width of the waveguide connecting to the\n                      grating\n    evaluations     : number of evaluations of `path.parametric`\n    layer           : GDSII layer number\n    datatype        : GDSII datatype number\n\n    Return `PolygonSet`\n    '''\n    if focus_distance < 0:\n        path = gdspy.L1Path(\n            (position[0] - 0.5 * width,\n             position[1] + 0.5 * (number_of_teeth - 1 + fill_frac) * period),\n            '+x',\n            period * fill_frac, [width], [],\n            number_of_teeth,\n            period,\n            layer=layer,\n            datatype=datatype)\n    else:\n        neff = lda / float(period) + sin_theta\n        qmin = int(focus_distance / float(period) + 0.5)\n        path = gdspy.Path(period * fill_frac, position)\n        max_points = 199 if focus_width < 0 else 2 * evaluations\n        c3 = neff**2 - sin_theta**2\n        w = 0.5 * width\n        for q in range(qmin, qmin + number_of_teeth):\n            c1 = q * lda * sin_theta\n            c2 = (q * lda)**2\n            path.parametric(\n                lambda t: (width * t - w, (c1 + neff * numpy.sqrt(c2 - c3 * (\n                    width * t - w)**2)) / c3),\n                number_of_evaluations=evaluations,\n                max_points=max_points,\n                layer=layer,\n                datatype=datatype)\n            path.x = position[0]\n            path.y = position[1]\n        if focus_width >= 0:\n            path.polygons[0] = numpy.vstack(\n                (path.polygons[0][:evaluations, :],\n                 ([position] if focus_width == 0 else\n                  [(position[0] + 0.5 * focus_width, position[1]),\n                   (position[0] - 0.5 * focus_width, position[1])])))\n            path.fracture()\n    if direction == '-x':\n        return path.rotate(0.5 * numpy.pi, position)\n    elif direction == '+x':\n        return path.rotate(-0.5 * numpy.pi, position)\n    elif direction == '-y':\n        return path.rotate(numpy.pi, position)\n    else:\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new URL dispatcher based on the current context.", "response": "def make_flask_url_dispatcher():\n    \"\"\"Return an URL dispatcher based on the current :ref:`request context\n    <flask:request-context>`.\n\n    You generally don\u2019t need to call this directly.\n\n    The context is used when the dispatcher is first created but not\n    afterwards. It is not required after this function has returned.\n\n    Dispatch to the context\u2019s app URLs below the context\u2019s root URL.\n    If the app has a ``SERVER_NAME`` :ref:`config <flask:config>`, also\n    accept URLs that have that domain name or a subdomain thereof.\n\n    \"\"\"\n    def parse_netloc(netloc):\n        \"\"\"Return (hostname, port).\"\"\"\n        parsed = urlparse.urlsplit('http://' + netloc)\n        return parsed.hostname, parsed.port\n\n    app = current_app._get_current_object()\n    root_path = request.script_root\n\n    server_name = app.config.get('SERVER_NAME')\n    if server_name:\n        hostname, port = parse_netloc(server_name)\n\n        def accept(url):\n            \"\"\"Accept any URL scheme; also accept subdomains.\"\"\"\n            return url.hostname is not None and (\n                url.hostname == hostname or\n                url.hostname.endswith('.' + hostname))\n    else:\n        scheme = request.scheme\n        hostname, port = parse_netloc(request.host)\n        if (scheme, port) in DEFAULT_PORTS:\n            port = None\n\n        def accept(url):\n            \"\"\"Do not accept subdomains.\"\"\"\n            return (url.scheme, url.hostname) == (scheme, hostname)\n\n    def dispatch(url_string):\n        if isinstance(url_string, bytes):\n            url_string = url_string.decode('utf8')\n        url = urlparse.urlsplit(url_string)\n        url_port = url.port\n        if (url.scheme, url_port) in DEFAULT_PORTS:\n            url_port = None\n        if accept(url) and url_port == port and url.path.startswith(root_path):\n            netloc = url.netloc\n            if url.port and not url_port:\n                netloc = netloc.rsplit(':', 1)[0]  # remove default port\n            base_url = '%s://%s%s' % (url.scheme, netloc, root_path)\n            path = url.path[len(root_path):]\n            if url.query:\n                path += '?' + url.query\n            # Ignore url.fragment\n            return app, base_url, path\n\n    return dispatch"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_url_fetcher(dispatcher=None,\n                     next_fetcher=weasyprint.default_url_fetcher):\n    \"\"\"Return an function suitable as a ``url_fetcher`` in WeasyPrint.\n    You generally don\u2019t need to call this directly.\n\n    If ``dispatcher`` is not  provided, :func:`make_flask_url_dispatcher`\n    is called to get one. This requires a request context.\n\n    Otherwise, it must be a callable that take an URL and return either\n    ``None`` or a ``(wsgi_callable, base_url, path)`` tuple. For None\n    ``next_fetcher`` is used. (By default, fetch normally over the network.)\n    For a tuple the request is made at the WSGI level.\n    ``wsgi_callable`` must be a Flask application or another WSGI callable.\n    ``base_url`` is the root URL for the application while ``path``\n    is the path within the application.\n    Typically ``base_url + path`` is equal or equivalent to the passed URL.\n\n    \"\"\"\n    if dispatcher is None:\n        dispatcher = make_flask_url_dispatcher()\n\n    def flask_url_fetcher(url):\n        redirect_chain = set()\n        while 1:\n            result = dispatcher(url)\n            if result is None:\n                return next_fetcher(url)\n            app, base_url, path = result\n            client = Client(app, response_wrapper=Response)\n            if isinstance(path, unicode):\n                # TODO: double-check this. Apparently Werzeug %-unquotes bytes\n                # but not Unicode URLs. (IRI vs. URI or something.)\n                path = path.encode('utf8')\n            response = client.get(path, base_url=base_url)\n            if response.status_code == 200:\n                return dict(\n                    string=response.data,\n                    mime_type=response.mimetype,\n                    encoding=response.charset,\n                    redirected_url=url)\n            # The test client can follow redirects, but do it ourselves\n            # to get access to the redirected URL.\n            elif response.status_code in (301, 302, 303, 305, 307):\n                redirect_chain.add(url)\n                url = response.location\n                if url in redirect_chain:\n                    raise ClientRedirectError('loop detected')\n            else:\n                raise ValueError('Flask-WeasyPrint got HTTP status %s for %s%s'\n                                 % (response.status, base_url, path))\n    return flask_url_fetcher", "response": "Return a function suitable as a url_fetcher in WeasyPrint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrender a PDF to a response with the correct Content - Type header.", "response": "def render_pdf(html, stylesheets=None,\n               download_filename=None, automatic_download=True):\n    \"\"\"Render a PDF to a response with the correct ``Content-Type`` header.\n\n    :param html:\n        Either a :class:`weasyprint.HTML` object or an URL to be passed\n        to :func:`flask_weasyprint.HTML`. The latter case requires\n        a request context.\n    :param stylesheets:\n        A list of user stylesheets, passed to\n        :meth:`~weasyprint.HTML.write_pdf`\n    :param download_filename:\n        If provided, the ``Content-Disposition`` header is set so that most\n        web browser will show the \"Save as\u2026\" dialog with the value as the\n        default filename.\n    :param automatic_download:\n        If True, the browser will automatic download file.\n    :returns: a :class:`flask.Response` object.\n\n    \"\"\"\n    if not hasattr(html, 'write_pdf'):\n        html = HTML(html)\n    pdf = html.write_pdf(stylesheets=stylesheets)\n    response = current_app.response_class(pdf, mimetype='application/pdf')\n    if download_filename:\n        if automatic_download:\n            value = 'attachment'\n        else:\n            value = 'inline'\n        response.headers.add('Content-Disposition', value,\n                             filename=download_filename)\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a signature into a tuple of method name types dict return type", "response": "def _parse_sig(sig, arg_names, validate=False):\n    \"\"\"\n    Parses signatures into a ``OrderedDict`` of paramName => type.\n    Numerically-indexed arguments that do not correspond to an argument\n    name in python (ie: it takes a variable number of arguments) will be\n    keyed as the stringified version of it's index.\n\n      sig         the signature to be parsed\n      arg_names   a list of argument names extracted from python source\n\n    Returns a tuple of (method name, types dict, return type)\n    \"\"\"\n    d = SIG_RE.match(sig)\n    if not d:\n        raise ValueError('Invalid method signature %s' % sig)\n    d = d.groupdict()\n    ret = [(n, Any) for n in arg_names]\n    if 'args_sig' in d and type(\n            d['args_sig']) is str and d['args_sig'].strip():\n        for i, arg in enumerate(d['args_sig'].strip().split(',')):\n            _type_checking_available(sig, validate)\n            if '=' in arg:\n                if not type(ret) is OrderedDict:\n                    ret = OrderedDict(ret)\n                dk = KWARG_RE.match(arg)\n                if not dk:\n                    raise ValueError('Could not parse arg type %s in %s' %\n                                     (arg, sig))\n                dk = dk.groupdict()\n                if not sum(\n                    [(k in dk and type(dk[k]) is str and bool(dk[k].strip()))\n                         for k in ('arg_name', 'arg_type')]):\n                    raise ValueError('Invalid kwarg value %s in %s' %\n                                     (arg, sig))\n                ret[dk['arg_name']] = _eval_arg_type(dk['arg_type'], None, arg,\n                                                     sig)\n            else:\n                if type(ret) is OrderedDict:\n                    raise ValueError('Positional arguments must occur '\n                                     'before keyword arguments in %s' % sig)\n                if len(ret) < i + 1:\n                    ret.append((str(i), _eval_arg_type(arg, None, arg, sig)))\n                else:\n                    ret[i] = (ret[i][0], _eval_arg_type(arg, None, arg, sig))\n    if not type(ret) is OrderedDict:\n        ret = OrderedDict(ret)\n    return (d['method_name'], ret,\n            (_eval_arg_type(d['return_sig'], Any, 'return', sig) if\n             d['return_sig'] else Any))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _inject_args(sig, types):\n    if '(' in sig:\n        parts = sig.split('(')\n        sig = '%s(%s%s%s' % (\n            parts[0], ', '.join(types),\n            (', ' if parts[1].index(')') > 0 else ''), parts[1])\n    else:\n        sig = '%s(%s)' % (sig, ', '.join(types))\n    return sig", "response": "A function to inject arguments manually into a method signature before\n    it s been parsed."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps a function turns it into a json-rpc method. Adds several attributes to the function specific to the JSON-RPC machinery and adds it to the default jsonrpc_site if one isn't provided. You must import the module containing these functions in your urls.py. name The name of your method. IE: `namespace.methodName` The method name can include type information, like `ns.method(String, Array) -> Nil`. authenticated=False Adds `username` and `password` arguments to the beginning of your method if the user hasn't already been authenticated. These will be used to authenticate the user against `django.contrib.authenticate` If you use HTTP auth or other authentication middleware, `username` and `password` will not be added, and this method will only check against `request.user.is_authenticated`. You may pass a callable to replace `django.contrib.auth.authenticate` as the authentication method. It must return either a User or `None` and take the keyword arguments `username` and `password`. safe=False Designates whether or not your method may be accessed by HTTP GET. By default this is turned off. validate=False Validates the arguments passed to your method based on type information provided in the signature. Supply type information by including types in your method declaration. Like so: @jsonrpc_method('myapp.specialSauce(Array, String)', validate=True) def special_sauce(self, ingredients, instructions): return SpecialSauce(ingredients, instructions) Calls to `myapp.specialSauce` will now check each arguments type before calling `special_sauce`, throwing an `InvalidParamsError` when it encounters a discrepancy. This can significantly reduce the amount of code required to write JSON-RPC services. site=default_site Defines which site the jsonrpc method will be added to. Can be any object that provides a `register(name, func)` method.", "response": "def jsonrpc_method(name,\n                   authenticated=False,\n                   authentication_arguments=['username', 'password'],\n                   safe=False,\n                   validate=False,\n                   site=default_site):\n    \"\"\"\n    Wraps a function turns it into a json-rpc method. Adds several attributes\n    to the function specific to the JSON-RPC machinery and adds it to the default\n    jsonrpc_site if one isn't provided. You must import the module containing\n    these functions in your urls.py.\n\n        name\n\n            The name of your method. IE: `namespace.methodName` The method name\n            can include type information, like `ns.method(String, Array) -> Nil`.\n\n        authenticated=False\n\n            Adds `username` and `password` arguments to the beginning of your\n            method if the user hasn't already been authenticated. These will\n            be used to authenticate the user against `django.contrib.authenticate`\n            If you use HTTP auth or other authentication middleware, `username`\n            and `password` will not be added, and this method will only check\n            against `request.user.is_authenticated`.\n\n            You may pass a callable to replace `django.contrib.auth.authenticate`\n            as the authentication method. It must return either a User or `None`\n            and take the keyword arguments `username` and `password`.\n\n        safe=False\n\n            Designates whether or not your method may be accessed by HTTP GET.\n            By default this is turned off.\n\n        validate=False\n\n            Validates the arguments passed to your method based on type\n            information provided in the signature. Supply type information by\n            including types in your method declaration. Like so:\n\n            @jsonrpc_method('myapp.specialSauce(Array, String)', validate=True)\n            def special_sauce(self, ingredients, instructions):\n              return SpecialSauce(ingredients, instructions)\n\n            Calls to `myapp.specialSauce` will now check each arguments type\n            before calling `special_sauce`, throwing an `InvalidParamsError`\n            when it encounters a discrepancy. This can significantly reduce the\n            amount of code required to write JSON-RPC services.\n\n        site=default_site\n\n            Defines which site the jsonrpc method will be added to. Can be any\n            object that provides a `register(name, func)` method.\n\n    \"\"\"\n\n    def decorator(func):\n        arg_names = getargspec(func)[0][1:]\n        X = {'name': name, 'arg_names': arg_names}\n        if authenticated:\n            if authenticated is True or six.callable(authenticated):\n                # TODO: this is an assumption\n                X['arg_names'] = authentication_arguments + X['arg_names']\n                X['name'] = _inject_args(X['name'], ('String', 'String'))\n                from django.contrib.auth import authenticate as _authenticate\n                from django.contrib.auth.models import User\n            else:\n                authenticate = authenticated\n\n            @six.wraps(func)\n            def _func(request, *args, **kwargs):\n                user = getattr(request, 'user', None)\n                is_authenticated = getattr(user, 'is_authenticated',\n                                           lambda: False)\n                if ((user is not None and six.callable(is_authenticated) and\n                     not is_authenticated()) or user is None):\n                    user = None\n                    try:\n                        creds = args[:len(authentication_arguments)]\n                        if len(creds) == 0:\n                            raise IndexError\n                        # Django's authenticate() method takes arguments as dict\n                        user = _authenticate(username=creds[0],\n                                             password=creds[1], *creds[2:])\n                        if user is not None:\n                            args = args[len(authentication_arguments):]\n                    except IndexError:\n                        auth_kwargs = {}\n                        try:\n                            for auth_kwarg in authentication_arguments:\n                                auth_kwargs[auth_kwarg] = kwargs[auth_kwarg]\n                        except KeyError:\n                            raise InvalidParamsError(\n                                'Authenticated methods require at least '\n                                '[%(arguments)s] or {%(arguments)s} arguments' %\n                                {'arguments': ', '.join(authentication_arguments)})\n\n                        user = _authenticate(**auth_kwargs)\n                        if user is not None:\n                            for auth_kwarg in authentication_arguments:\n                                kwargs.pop(auth_kwarg)\n                    if user is None:\n                        raise InvalidCredentialsError\n                    request.user = user\n                return func(request, *args, **kwargs)\n        else:\n            _func = func\n\n        @six.wraps(_func)\n        def exc_printer(*a, **kw):\n            try:\n                return _func(*a, **kw)\n            except Exception as e:\n                try:\n                    print('JSONRPC SERVICE EXCEPTION')\n                    import traceback\n                    traceback.print_exc()\n                except:\n                    pass\n                six.reraise(*sys.exc_info())\n\n        ret_func = exc_printer\n\n        method, arg_types, return_type = \\\n      _parse_sig(X['name'], X['arg_names'], validate)\n        ret_func.json_args = X['arg_names']\n        ret_func.json_arg_types = arg_types\n        ret_func.json_return_type = return_type\n        ret_func.json_method = method\n        ret_func.json_safe = safe\n        ret_func.json_sig = X['name']\n        ret_func.json_validate = validate\n        site.register(method, ret_func)\n        return ret_func\n\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_payload(self, params):\n        data = dumps({\n            'jsonrpc': self.version,\n            'method': self.service_name,\n            'params': params,\n            'id': str(uuid.uuid1())\n        }).encode('utf-8')\n        headers = {\n            'Content-Type': 'application/json-rpc',\n            'Accept': 'application/json-rpc',\n            'Content-Length': len(data)\n        }\n        try:\n            req = urllib_request.Request(self.service_url, data, headers)\n            resp = urllib_request.urlopen(req)\n        except IOError as e:\n            if isinstance(e, urllib_error.HTTPError):\n                if e.code not in (\n                        401, 403\n                ) and e.headers['Content-Type'] == 'application/json-rpc':\n                    return e.read().decode('utf-8')  # we got a jsonrpc-formatted respnose\n                raise ServiceProxyException(e.code, e.headers, req)\n            else:\n                raise e\n        return resp.read().decode('utf-8')", "response": "Performs the actual sending action and returns the result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Exception data in a format for JSON - RPC", "response": "def json_rpc_format(self):\n        \"\"\" return the Exception data in a format for JSON-RPC \"\"\"\n\n        error = {\n            'name': smart_text(self.__class__.__name__),\n            'code': self.code,\n            'message': \"%s: %s\" %\n            (smart_text(self.__class__.__name__), smart_text(self.message)),\n            'data': self.data\n        }\n\n        from django.conf import settings\n\n        if settings.DEBUG:\n            import sys, traceback\n            error['stack'] = traceback.format_exc()\n            error['executable'] = sys.executable\n\n        return error"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches by users or channels on all server.", "response": "def directory(self, query, **kwargs):\n        \"\"\"Search by users or channels on all server.\"\"\"\n        if isinstance(query, dict):\n            query = str(query).replace(\"'\", '\"')\n\n        return self.__call_api_get('directory', query=query, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for users or rooms that are visible to the user.", "response": "def spotlight(self, query, **kwargs):\n        \"\"\"Searches for users or rooms that are visible to the user.\"\"\"\n        return self.__call_api_get('spotlight', query=query, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef users_set_preferences(self, user_id, data, **kwargs):\n        return self.__call_api_post('users.setPreferences', userId=user_id, data=data, kwargs=kwargs)", "response": "Set user s preferences."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a user s information limited to the caller s permissions.", "response": "def users_info(self, user_id=None, username=None, **kwargs):\n        \"\"\"Gets a user\u2019s information, limited to the caller\u2019s permissions.\"\"\"\n        if user_id:\n            return self.__call_api_get('users.info', userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_get('users.info', username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the online presence of the a user.", "response": "def users_get_presence(self, user_id=None, username=None, **kwargs):\n        \"\"\"Gets the online presence of the a user.\"\"\"\n        if user_id:\n            return self.__call_api_get('users.getPresence', userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_get('users.getPresence', username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a new user.", "response": "def users_register(self, email, name, password, username, **kwargs):\n        \"\"\"Register a new user.\"\"\"\n        return self.__call_api_post('users.register', email=email, name=name, password=password, username=username,\n                                    kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the URL for a user s avatar.", "response": "def users_get_avatar(self, user_id=None, username=None, **kwargs):\n        \"\"\"Gets the URL for a user\u2019s avatar.\"\"\"\n        if user_id:\n            return self.__call_api_get('users.getAvatar', userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_get('users.getAvatar', username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a user s avatar", "response": "def users_set_avatar(self, avatar_url, **kwargs):\n        \"\"\"Set a user\u2019s avatar\"\"\"\n        if avatar_url.startswith('http://') or avatar_url.startswith('https://'):\n            return self.__call_api_post('users.setAvatar', avatarUrl=avatar_url, kwargs=kwargs)\n        else:\n            avatar_file = {\"image\": open(avatar_url, \"rb\")}\n            return self.__call_api_post('users.setAvatar', files=avatar_file, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets a user s avatar", "response": "def users_reset_avatar(self, user_id=None, username=None, **kwargs):\n        \"\"\"Reset a user\u2019s avatar\"\"\"\n        if user_id:\n            return self.__call_api_post('users.resetAvatar', userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_post('users.resetAvatar', username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef users_create_token(self, user_id=None, username=None, **kwargs):\n        if user_id:\n            return self.__call_api_post('users.createToken', userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_post('users.createToken', username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')", "response": "Create a user authentication token."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate an existing user.", "response": "def users_update(self, user_id, **kwargs):\n        \"\"\"Update an existing user.\"\"\"\n        return self.__call_api_post('users.update', userId=user_id, data=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef users_forgot_password(self, email, **kwargs):\n        return self.__call_api_post('users.forgotPassword', email=email, data=kwargs)", "response": "Send email to reset your password."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nposts a new chat message.", "response": "def chat_post_message(self, text, room_id=None, channel=None, **kwargs):\n        \"\"\"Posts a new chat message.\"\"\"\n        if room_id:\n            return self.__call_api_post('chat.postMessage', roomId=room_id, text=text, kwargs=kwargs)\n        elif channel:\n            return self.__call_api_post('chat.postMessage', channel=channel, text=text, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or channel required')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef chat_delete(self, room_id, msg_id, **kwargs):\n        return self.__call_api_post('chat.delete', roomId=room_id, msgId=msg_id, kwargs=kwargs)", "response": "Deletes a chat message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef chat_update(self, room_id, msg_id, text, **kwargs):\n        return self.__call_api_post('chat.update', roomId=room_id, msgId=msg_id, text=text, kwargs=kwargs)", "response": "Updates the text of the chat message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chat_react(self, msg_id, emoji='smile', **kwargs):\n        return self.__call_api_post('chat.react', messageId=msg_id, emoji=emoji, kwargs=kwargs)", "response": "Updates the text of the chat message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for messages in a channel by id and text message.", "response": "def chat_search(self, room_id, search_text, **kwargs):\n        \"\"\"Search for messages in a channel by id and text message.\"\"\"\n        return self.__call_api_get('chat.search', roomId=room_id, searchText=search_text, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget Message Read Receipts", "response": "def chat_get_message_read_receipts(self, message_id, **kwargs):\n        \"\"\"Get Message Read Receipts\"\"\"\n        return self.__call_api_get('chat.getMessageReadReceipts', messageId=message_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef channels_info(self, room_id=None, channel=None, **kwargs):\n        if room_id:\n            return self.__call_api_get('channels.info', roomId=room_id, kwargs=kwargs)\n        elif channel:\n            return self.__call_api_get('channels.info', roomName=channel, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or channel required')", "response": "Gets a channel s information."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the messages from a channel.", "response": "def channels_history(self, room_id, **kwargs):\n        \"\"\"Retrieves the messages from a channel.\"\"\"\n        return self.__call_api_get('channels.history', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef channels_add_all(self, room_id, **kwargs):\n        return self.__call_api_post('channels.addAll', roomId=room_id, kwargs=kwargs)", "response": "Adds all of the users of the Rocket. Chat server to the channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a moderator role for a user in the current channel.", "response": "def channels_add_moderator(self, room_id, user_id, **kwargs):\n        \"\"\"Gives the role of moderator for a user in the current channel.\"\"\"\n        return self.__call_api_post('channels.addModerator', roomId=room_id, userId=user_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef channels_remove_moderator(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('channels.removeModerator', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Removes the role of moderator from a user in the current channel."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the role of owner for a user in the current channel.", "response": "def channels_add_owner(self, room_id, user_id=None, username=None, **kwargs):\n        \"\"\"Gives the role of owner for a user in the current channel.\"\"\"\n        if user_id:\n            return self.__call_api_post('channels.addOwner', roomId=room_id, userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_post('channels.addOwner', roomId=room_id, username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef channels_remove_owner(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('channels.removeOwner', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Removes the role of owner from a user in the current channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef channels_close(self, room_id, **kwargs):\n        return self.__call_api_post('channels.close', roomId=room_id, kwargs=kwargs)", "response": "Removes the channel from the user s list of channels."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the channel back to the user s list of channels.", "response": "def channels_open(self, room_id, **kwargs):\n        \"\"\"Adds the channel back to the user\u2019s list of channels.\"\"\"\n        return self.__call_api_post('channels.open', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new public channel optionally including users.", "response": "def channels_create(self, name, **kwargs):\n        \"\"\"Creates a new public channel, optionally including users.\"\"\"\n        return self.__call_api_post('channels.create', name=name, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the integrations which the channel has", "response": "def channels_get_integrations(self, room_id, **kwargs):\n        \"\"\"Retrieves the integrations which the channel has\"\"\"\n        return self.__call_api_get('channels.getIntegrations', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a user to the channel.", "response": "def channels_invite(self, room_id, user_id, **kwargs):\n        \"\"\"Adds a user to the channel.\"\"\"\n        return self.__call_api_post('channels.invite', roomId=room_id, userId=user_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a user from the channel.", "response": "def channels_kick(self, room_id, user_id, **kwargs):\n        \"\"\"Removes a user from the channel.\"\"\"\n        return self.__call_api_post('channels.kick', roomId=room_id, userId=user_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the callee from the channel.", "response": "def channels_leave(self, room_id, **kwargs):\n        \"\"\"Causes the callee to be removed from the channel.\"\"\"\n        return self.__call_api_post('channels.leave', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef channels_rename(self, room_id, name, **kwargs):\n        return self.__call_api_post('channels.rename', roomId=room_id, name=name, kwargs=kwargs)", "response": "Changes the name of the channel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the description for the channel.", "response": "def channels_set_description(self, room_id, description, **kwargs):\n        \"\"\"Sets the description for the channel.\"\"\"\n        return self.__call_api_post('channels.setDescription', roomId=room_id, description=description, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the code required to join the channel.", "response": "def channels_set_join_code(self, room_id, join_code, **kwargs):\n        \"\"\"Sets the code required to join the channel.\"\"\"\n        return self.__call_api_post('channels.setJoinCode', roomId=room_id, joinCode=join_code, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef channels_set_topic(self, room_id, topic, **kwargs):\n        return self.__call_api_post('channels.setTopic', roomId=room_id, topic=topic, kwargs=kwargs)", "response": "Sets the topic for the channel."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the type of room this channel should be.", "response": "def channels_set_type(self, room_id, a_type, **kwargs):\n        \"\"\"Sets the type of room this channel should be. The type of room this channel should be, either c or p.\"\"\"\n        return self.__call_api_post('channels.setType', roomId=room_id, type=a_type, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef channels_set_announcement(self, room_id, announce, **kwargs):\n        return self.__call_api_post('channels.setAnnouncement', roomId=room_id, announcement=announce, kwargs=kwargs)", "response": "Sets the announcement for the channel."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef channels_set_custom_fields(self, rid, custom_fields):\n        return self.__call_api_post('channels.setCustomFields', roomId=rid, customFields=custom_fields)", "response": "Sets the custom fields for the channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef channels_delete(self, room_id=None, channel=None, **kwargs):\n        if room_id:\n            return self.__call_api_post('channels.delete', roomId=room_id, kwargs=kwargs)\n        elif channel:\n            return self.__call_api_post('channels.delete', roomName=channel, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or channel required')", "response": "Delete a public channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets all the user mentions of a channel.", "response": "def channels_get_all_user_mentions_by_channel(self, room_id, **kwargs):\n        \"\"\"Gets all the mentions of a channel.\"\"\"\n        return self.__call_api_get('channels.getAllUserMentionsByChannel', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef groups_history(self, room_id, **kwargs):\n        return self.__call_api_get('groups.history', roomId=room_id, kwargs=kwargs)", "response": "Retrieves the messages from a private group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef groups_add_moderator(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('groups.addModerator', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Adds a moderator role for a user in the current groups."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef groups_remove_moderator(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('groups.removeModerator', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Removes the role of moderator from a user in the current groups."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groups_moderators(self, room_id=None, group=None, **kwargs):\n        if room_id:\n            return self.__call_api_get('groups.moderators', roomId=room_id, kwargs=kwargs)\n        elif group:\n            return self.__call_api_get('groups.moderators', roomName=group, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or group required')", "response": "Lists all moderators of a group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the role of owner for a user in the current Group.", "response": "def groups_add_owner(self, room_id, user_id, **kwargs):\n        \"\"\"Gives the role of owner for a user in the current Group.\"\"\"\n        return self.__call_api_post('groups.addOwner', roomId=room_id, userId=user_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the role of owner from a user in the current Group.", "response": "def groups_remove_owner(self, room_id, user_id, **kwargs):\n        \"\"\"Removes the role of owner from a user in the current Group.\"\"\"\n        return self.__call_api_post('groups.removeOwner', roomId=room_id, userId=user_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef groups_archive(self, room_id, **kwargs):\n        return self.__call_api_post('groups.archive', roomId=room_id, kwargs=kwargs)", "response": "Archives a private group only if you re part of the group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groups_unarchive(self, room_id, **kwargs):\n        return self.__call_api_post('groups.unarchive', roomId=room_id, kwargs=kwargs)", "response": "Unarchives a private group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef groups_close(self, room_id, **kwargs):\n        return self.__call_api_post('groups.close', roomId=room_id, kwargs=kwargs)", "response": "Removes the private group from the user s list of groups only if you re part of the group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new private group optionally including users only if you\u2019re part of the group.", "response": "def groups_create(self, name, **kwargs):\n        \"\"\"Creates a new private group, optionally including users, only if you\u2019re part of the group.\"\"\"\n        return self.__call_api_post('groups.create', name=name, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef groups_get_integrations(self, room_id, **kwargs):\n        return self.__call_api_get('groups.getIntegrations', roomId=room_id, kwargs=kwargs)", "response": "Retrieves the integrations which the group has"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef groups_invite(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('groups.invite', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Adds a user to the private group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef groups_kick(self, room_id, user_id, **kwargs):\n        return self.__call_api_post('groups.kick', roomId=room_id, userId=user_id, kwargs=kwargs)", "response": "Removes a user from the private group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving the callee from the private group if they are not the last owner.", "response": "def groups_leave(self, room_id, **kwargs):\n        \"\"\"Causes the callee to be removed from the private group, if they\u2019re part of it and are not the last owner.\"\"\"\n        return self.__call_api_post('groups.leave', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef groups_open(self, room_id, **kwargs):\n        return self.__call_api_post('groups.open', roomId=room_id, kwargs=kwargs)", "response": "Adds the private group back to the user s list of private groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef groups_rename(self, room_id, name, **kwargs):\n        return self.__call_api_post('groups.rename', roomId=room_id, name=name, kwargs=kwargs)", "response": "Changes the name of the private group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the description for the private group.", "response": "def groups_set_description(self, room_id, description, **kwargs):\n        \"\"\"Sets the description for the private group.\"\"\"\n        return self.__call_api_post('groups.setDescription', roomId=room_id, description=description, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting whether the group is read only or not.", "response": "def groups_set_read_only(self, room_id, read_only, **kwargs):\n        \"\"\"Sets whether the group is read only or not.\"\"\"\n        return self.__call_api_post('groups.setReadOnly', roomId=room_id, readOnly=bool(read_only), kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef groups_set_topic(self, room_id, topic, **kwargs):\n        return self.__call_api_post('groups.setTopic', roomId=room_id, topic=topic, kwargs=kwargs)", "response": "Sets the topic for the private group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the type of room this channel should be.", "response": "def groups_set_type(self, room_id, a_type, **kwargs):\n        \"\"\"Sets the type of room this group should be. The type of room this channel should be, either c or p.\"\"\"\n        return self.__call_api_post('groups.setType', roomId=room_id, type=a_type, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a private group.", "response": "def groups_delete(self, room_id=None, group=None, **kwargs):\n        \"\"\"Delete a private group.\"\"\"\n        if room_id:\n            return self.__call_api_post('groups.delete', roomId=room_id, kwargs=kwargs)\n        elif group:\n            return self.__call_api_post('groups.delete', roomName=group, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or group required')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all user s roles in the private group.", "response": "def groups_roles(self, room_id=None, room_name=None, **kwargs):\n        \"\"\"Lists all user\u2019s roles in the private group.\"\"\"\n        if room_id:\n            return self.__call_api_get('groups.roles', roomId=room_id, kwargs=kwargs)\n        elif room_name:\n            return self.__call_api_get('groups.roles', roomName=room_name, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or room_name required')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef im_history(self, room_id, **kwargs):\n        return self.__call_api_get('im.history', roomId=room_id, kwargs=kwargs)", "response": "Retrieves the history for a private im chat"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef im_create(self, username, **kwargs):\n        return self.__call_api_post('im.create', username=username, kwargs=kwargs)", "response": "Create a direct message session with another user."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the direct message back to the user s list of direct messages.", "response": "def im_open(self, room_id, **kwargs):\n        \"\"\"Adds the direct message back to the user\u2019s list of direct messages.\"\"\"\n        return self.__call_api_post('im.open', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef im_close(self, room_id, **kwargs):\n        return self.__call_api_post('im.close', roomId=room_id, kwargs=kwargs)", "response": "Removes the direct message from the user s list of direct messages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the messages from any direct message in the server", "response": "def im_messages_others(self, room_id, **kwargs):\n        \"\"\"Retrieves the messages from any direct message in the server\"\"\"\n        return self.__call_api_get('im.messages.others', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the topic for the direct message", "response": "def im_set_topic(self, room_id, topic, **kwargs):\n        \"\"\"Sets the topic for the direct message\"\"\"\n        return self.__call_api_post('im.setTopic', roomId=room_id, topic=topic, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef im_files(self, room_id=None, user_name=None, **kwargs):\n        if room_id:\n            return self.__call_api_get('im.files', roomId=room_id, kwargs=kwargs)\n        elif user_name:\n            return self.__call_api_get('im.files', username=user_name, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or username required')", "response": "Retrieves the files from a direct message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nposting a message with attached file to a dedicated room.", "response": "def rooms_upload(self, rid, file, **kwargs):\n        \"\"\"Post a message with attached file to a dedicated room.\"\"\"\n        files = {\n            'file': (os.path.basename(file), open(file, 'rb'), mimetypes.guess_type(file)[0]),\n        }\n        return self.__call_api_post('rooms.upload/' + rid, kwargs=kwargs, use_json=False, files=files)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rooms_clean_history(self, room_id, latest, oldest, **kwargs):\n        return self.__call_api_post('rooms.cleanHistory', roomId=room_id, latest=latest, oldest=oldest, kwargs=kwargs)", "response": "Cleans up a room removing messages from the provided time range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the information about the room.", "response": "def rooms_info(self, room_id=None, room_name=None):\n        \"\"\"Retrieves the information about the room.\"\"\"\n        if room_id is not None:\n            return self.__call_api_get('rooms.info', roomId=room_id)\n        elif room_name is not None:\n            return self.__call_api_get('rooms.info', roomName=room_name)\n        else:\n            raise RocketMissingParamException('roomId or roomName required')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef subscriptions_get_one(self, room_id, **kwargs):\n        return self.__call_api_get('subscriptions.getOne', roomId=room_id, kwargs=kwargs)", "response": "Get the subscription by room id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmarking messages as unread by roomId or from a message", "response": "def subscriptions_unread(self, room_id, **kwargs):\n        \"\"\"Mark messages as unread by roomId or from a message\"\"\"\n        return self.__call_api_post('subscriptions.unread', roomId=room_id, kwargs=kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef subscriptions_read(self, rid, **kwargs):\n        return self.__call_api_post('subscriptions.read', rid=rid, kwargs=kwargs)", "response": "Mark room as read"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting an asset image by name.", "response": "def assets_set_asset(self, asset_name, file, **kwargs):\n        \"\"\"Set an asset image by name.\"\"\"\n        content_type = mimetypes.MimeTypes().guess_type(file)\n        files = {\n            asset_name: (file, open(file, 'rb'), content_type[0], {'Expires': '0'}),\n        }\n        return self.__call_api_post('assets.setAsset', kwargs=kwargs, use_json=False, files=files)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_callback(args, kw=None):\n    'Return callback whether passed as a last argument or as a keyword'\n    if args and callable(args[-1]):\n        return args[-1], args[:-1]\n    try:\n        return kw['callback'], args\n    except (KeyError, TypeError):\n        return None, args", "response": "Return callback whether passed as a last argument or as a keyword"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef once(self, event, callback):\n        'Define a callback to handle the first event emitted by the server'\n        self._once_events.add(event)\n        self.on(event, callback)", "response": "Define a callback to handle the first event emitted by the server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an event handler", "response": "def off(self, event):\n        'Remove an event handler'\n        try:\n            self._once_events.remove(event)\n        except KeyError:\n            pass\n        self._callback_by_event.pop(event, None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait(self, seconds=None, **kw):\n        'Wait in a loop and react to events as defined in the namespaces'\n        # Use ping/pong to unblock recv for polling transport\n        self._heartbeat_thread.hurry()\n        # Use timeout to unblock recv for websocket transport\n        self._transport.set_timeout(seconds=1)\n        # Listen\n        warning_screen = self._yield_warning_screen(seconds)\n        for elapsed_time in warning_screen:\n            if self._should_stop_waiting(**kw):\n                break\n            try:\n                try:\n                    self._process_packets()\n                except TimeoutError:\n                    pass\n                except KeyboardInterrupt:\n                    self._close()\n                    raise\n            except ConnectionError as e:\n                self._opened = False\n                try:\n                    warning = Exception('[connection error] %s' % e)\n                    warning_screen.throw(warning)\n                except StopIteration:\n                    self._warn(warning)\n                try:\n                    namespace = self.get_namespace()\n                    namespace._find_packet_callback('disconnect')()\n                except PacketError:\n                    pass\n        self._heartbeat_thread.relax()\n        self._transport.set_timeout()", "response": "Wait in a loop and react to events as defined in the namespaces"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning function that acknowledges the server", "response": "def _prepare_to_send_ack(self, path, ack_id):\n        'Return function that acknowledges the server'\n        return lambda *args: self._ack(path, ack_id, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrotate the point around the X axis by the given angle in degrees.", "response": "def rotateX(self, angle):\n        \"\"\" Rotates the point around the X axis by the given angle in degrees. \"\"\"\n        rad = angle * math.pi / 180\n        cosa = math.cos(rad)\n        sina = math.sin(rad)\n        y = self.y * cosa - self.z * sina\n        z = self.y * sina + self.z * cosa\n        return Point3D(self.x, y, z)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef line(x1, y1, x2, y2):\n\n    x1 = normalize(x1)\n    y1 = normalize(y1)\n    x2 = normalize(x2)\n    y2 = normalize(y2)\n\n    xdiff = max(x1, x2) - min(x1, x2)\n    ydiff = max(y1, y2) - min(y1, y2)\n    xdir = 1 if x1 <= x2 else -1\n    ydir = 1 if y1 <= y2 else -1\n\n    r = max(xdiff, ydiff)\n\n    for i in range(r+1):\n        x = x1\n        y = y1\n\n        if ydiff:\n            y += (float(i) * ydiff) / r * ydir\n        if xdiff:\n            x += (float(i) * xdiff) / r * xdir\n\n        yield (x, y)", "response": "Returns the coords of the line between x1 y1 and x2 y2"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef animate(canvas, fn, delay=1./24, *args, **kwargs):\n\n    # python2 unicode curses fix\n    if not IS_PY3:\n        import locale\n        locale.setlocale(locale.LC_ALL, \"\")\n\n    def animation(stdscr):\n\n        for frame in fn(*args, **kwargs):\n            for x,y in frame:\n                canvas.set(x,y)\n\n            f = canvas.frame()\n            stdscr.addstr(0, 0, '{0}\\n'.format(f))\n            stdscr.refresh()\n            if delay:\n                sleep(delay)\n            canvas.clear()\n\n    curses.wrapper(animation)", "response": "Function to animate the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, x, y):\n        x = normalize(x)\n        y = normalize(y)\n        col, row = get_pos(x, y)\n\n        if type(self.chars[row][col]) != int:\n            return\n\n        self.chars[row][col] |= pixel_map[y % 4][x % 2]", "response": "Set a pixel of the Canvas object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unset(self, x, y):\n        x = normalize(x)\n        y = normalize(y)\n        col, row = get_pos(x, y)\n\n        if type(self.chars[row][col]) == int:\n            self.chars[row][col] &= ~pixel_map[y % 4][x % 2]\n\n        if type(self.chars[row][col]) != int or self.chars[row][col] == 0:\n            del(self.chars[row][col])\n\n        if not self.chars.get(row):\n            del(self.chars[row])", "response": "Unset a pixel of the Canvas object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the text to the given coords.", "response": "def set_text(self, x, y, text):\n        \"\"\"Set text to the given coords.\n\n        :param x: x coordinate of the text start position\n        :param y: y coordinate of the text start position\n        \"\"\"\n        col, row = get_pos(x, y)\n\n        for i,c in enumerate(text):\n            self.chars[row][col+i] = c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning bool. True if the state of a pixel is False otherwise.", "response": "def get(self, x, y):\n        \"\"\"Get the state of a pixel. Returns bool.\n\n        :param x: x coordinate of the pixel\n        :param y: y coordinate of the pixel\n        \"\"\"\n        x = normalize(x)\n        y = normalize(y)\n        dot_index = pixel_map[y % 4][x % 2]\n        col, row = get_pos(x, y)\n        char = self.chars.get(row, {}).get(col)\n\n        if not char:\n            return False\n\n        if type(char) != int:\n            return True\n\n        return bool(char & dot_index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rows(self, min_x=None, min_y=None, max_x=None, max_y=None):\n\n        if not self.chars.keys():\n            return []\n\n        minrow = min_y // 4 if min_y != None else min(self.chars.keys())\n        maxrow = (max_y - 1) // 4 if max_y != None else max(self.chars.keys())\n        mincol = min_x // 2 if min_x != None else min(min(x.keys()) for x in self.chars.values())\n        maxcol = (max_x - 1) // 2 if max_x != None else max(max(x.keys()) for x in self.chars.values())\n        ret = []\n\n        for rownum in range(minrow, maxrow+1):\n            if not rownum in self.chars:\n                ret.append('')\n                continue\n\n            maxcol = (max_x - 1) // 2 if max_x != None else max(self.chars[rownum].keys())\n            row = []\n\n            for x in  range(mincol, maxcol+1):\n                char = self.chars[rownum].get(x)\n\n                if not char:\n                    row.append(unichr(braille_char_offset))\n                elif type(char) != int:\n                    row.append(char)\n                else:\n                    row.append(unichr(braille_char_offset+char))\n\n            ret.append(''.join(row))\n\n        return ret", "response": "Returns a list of the current canvas object lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstrings representation of the current Canvas object pixels.", "response": "def frame(self, min_x=None, min_y=None, max_x=None, max_y=None):\n        \"\"\"String representation of the current :class:`Canvas` object pixels.\n\n        :param min_x: (optional) minimum x coordinate of the canvas\n        :param min_y: (optional) minimum y coordinate of the canvas\n        :param max_x: (optional) maximum x coordinate of the canvas\n        :param max_y: (optional) maximum y coordinate of the canvas\n        \"\"\"\n        ret = self.line_ending.join(self.rows(min_x, min_y, max_x, max_y))\n\n        if IS_PY3:\n            return ret\n\n        return ret.encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef forward(self, step):\n        x = self.pos_x + math.cos(math.radians(self.rotation)) * step\n        y = self.pos_y + math.sin(math.radians(self.rotation)) * step\n        prev_brush_state = self.brush_on\n        self.brush_on = True\n        self.move(x, y)\n        self.brush_on = prev_brush_state", "response": "Move the turtle forward."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef move(self, x, y):\n        if self.brush_on:\n            for lx, ly in line(self.pos_x, self.pos_y, x, y):\n                self.set(lx, ly)\n\n        self.pos_x = x\n        self.pos_y = y", "response": "Move the turtle to a coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef URL(base, path, segments=None, defaults=None):\n    # Make a copy of the Segments class\n    url_class = type(Segments.__name__, Segments.__bases__,\n                     dict(Segments.__dict__))\n    segments = [] if segments is None else segments\n    defaults = [] if defaults is None else defaults\n    # For each segment attach a property capable of getting and setting it\n    for segment in segments:\n        setattr(url_class, segment, url_class._segment(segment))\n    # Instantiate the class with the actual parameters\n    return url_class(base, path, segments, defaults)", "response": "This function creates a new URL segment handler capable of getting and setting segments by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a property capable of setting and getting a segment.", "response": "def _segment(cls, segment):\n        \"\"\"\n        Returns a property capable of setting and getting a segment.\n        \"\"\"\n        return property(\n            fget=lambda x: cls._get_segment(x, segment),\n            fset=lambda x, v: cls._set_segment(x, segment, v),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nyielding a list of Torrent objects for every torrent on page.", "response": "def items(self):\n        \"\"\"\n        Request URL and parse response. Yield a ``Torrent`` for every torrent\n        on page.\n        \"\"\"\n        request = get(str(self.url), headers={'User-Agent' : \"Magic Browser\",\"origin_req_host\" : \"thepiratebay.se\"})\n        root = html.fromstring(request.text)\n        items = [self._build_torrent(row) for row in\n                 self._get_torrent_rows(root)]\n        for item in items:\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_torrent(self, row):\n        # Scrape, strip and build!!!\n        cols = row.findall('.//td')  # split the row into it's columns\n\n        # this column contains the categories\n        [category, sub_category] = [c.text for c in cols[0].findall('.//a')]\n\n        # this column with all important info\n        links = cols[1].findall('.//a')  # get 4 a tags from this columns\n        title = unicode(links[0].text)\n        url = self.url.build().path(links[0].get('href'))\n        magnet_link = links[1].get('href')  # the magnet download link\n        try:\n            torrent_link = links[2].get('href')  # the torrent download link\n            if not torrent_link.endswith('.torrent'):\n                torrent_link = None\n        except IndexError:\n            torrent_link = None\n        comments = 0\n        has_cover = 'No'\n        images = cols[1].findall('.//img')\n        for image in images:\n            image_title = image.get('title')\n            if image_title is None:\n                continue\n            if \"comments\" in image_title:\n                comments = int(image_title.split(\" \")[3])\n            if \"cover\" in image_title:\n                has_cover = 'Yes'\n        user_status = \"MEMBER\"\n        if links[-2].get('href').startswith(\"/user/\"):\n            user_status = links[-2].find('.//img').get('title')\n        meta_col = cols[1].find('.//font').text_content()  # don't need user\n        match = self._meta.match(meta_col)\n        created = match.groups()[0].replace('\\xa0', ' ')\n        size = match.groups()[1].replace('\\xa0', ' ')\n        user = match.groups()[2]  # uploaded by user\n\n        # last 2 columns for seeders and leechers\n        seeders = int(cols[2].text)\n        leechers = int(cols[3].text)\n        t = Torrent(title, url, category, sub_category, magnet_link,\n                    torrent_link, comments, has_cover, user_status, created,\n                    size, user, seeders, leechers)\n        return t", "response": "Builds a Torrent object from a parsed row."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef items(self):\n        if self._multipage:\n            while True:\n                # Pool for more torrents\n                items = super(Paginated, self).items()\n                # Stop if no more torrents\n                first = next(items, None)\n                if first is None:\n                    raise StopIteration()\n                # Yield them if not\n                else:\n                    yield first\n                    for item in items:\n                        yield item\n                # Go to the next page\n                self.next()\n        else:\n            for item in super(Paginated, self).items():\n                yield item", "response": "Yields a list of Torrent objects for every torrent\n        on page."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies the URL correspondingly return the current page if number is given.", "response": "def page(self, number=None):\n        \"\"\"\n        If page is given, modify the URL correspondingly, return the current\n        page otherwise.\n        \"\"\"\n        if number is None:\n            return int(self.url.page)\n        self.url.page = str(number)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(self, query=None):\n        if query is None:\n            return self.url.query\n        self.url.query = query", "response": "Modify the URL correspondingly return the current\n        query otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying the URL correspondingly return the current order otherwise.", "response": "def order(self, order=None):\n        \"\"\"\n        If order is given, modify the URL correspondingly, return the current\n        order otherwise.\n        \"\"\"\n        if order is None:\n            return int(self.url.order)\n        self.url.order = str(order)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmodifying the URL correspondingly return the current category otherwise.", "response": "def category(self, category=None):\n        \"\"\"\n        If category is given, modify the URL correspondingly, return the\n        current category otherwise.\n        \"\"\"\n        if category is None:\n            return int(self.url.category)\n        self.url.category = str(category)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsearching TPB for query and returns a list of paginated Torrents capable of changing query categories and orders.", "response": "def search(self, query, page=0, order=7, category=0, multipage=False):\n        \"\"\"\n        Searches TPB for query and returns a list of paginated Torrents capable\n        of changing query, categories and orders.\n        \"\"\"\n        search = Search(self.base_url, query, page, order, category)\n        if multipage:\n            search.multipage()\n        return search"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to parse the human readable torrent creation datetime.", "response": "def created(self):\n        \"\"\"\n        Attempt to parse the human readable torrent creation datetime.\n        \"\"\"\n        timestamp, current = self._created\n        if timestamp.endswith('ago'):\n            quantity, kind, ago = timestamp.split()\n            quantity = int(quantity)\n            if 'sec' in kind:\n                current -= quantity\n            elif 'min' in kind:\n                current -= quantity * 60\n            elif 'hour' in kind:\n                current -= quantity * 60 * 60\n            return datetime.datetime.fromtimestamp(current)\n        current = datetime.datetime.fromtimestamp(current)\n        timestamp = timestamp.replace(\n            'Y-day', str(current.date() - datetime.timedelta(days=1)))\n        timestamp = timestamp.replace('Today', current.date().isoformat())\n        try:\n            return dateutil.parser.parse(timestamp)\n        except:\n            return current"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_torrent(self):\n        print('Title: %s' % self.title)\n        print('URL: %s' % self.url)\n        print('Category: %s' % self.category)\n        print('Sub-Category: %s' % self.sub_category)\n        print('Magnet Link: %s' % self.magnet_link)\n        print('Torrent Link: %s' % self.torrent_link)\n        print('Uploaded: %s' % self.created)\n        print('Comments: %d' % self.comments)\n        print('Has Cover Image: %s' % self.has_cover)\n        print('User Status: %s' % self.user_status)\n        print('Size: %s' % self.size)\n        print('User: %s' % self.user)\n        print('Seeders: %d' % self.seeders)\n        print('Leechers: %d' % self.leechers)", "response": "Print the details of a torrent"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_from_argv(self, argv):\n        option = '--testrunner='\n        for arg in argv[2:]:\n            if arg.startswith(option):\n                self.test_runner = arg[len(option):]\n                break\n        super(Command, self).run_from_argv(argv)", "response": "This method is called by the command line parser to extract the value of the test_runner option and then run the base command line."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncolor the text of the content of the content", "response": "def foreground(color, content, readline=False):\n    \"\"\" Color the text of the content\n\n    :param color: pick a constant, any constant\n    :type color: int\n    :param content: Whatever you want to say...\n    :type content: unicode\n    :return: ansi string\n    :rtype: unicode\n    \"\"\"\n    return encode(color, readline=readline) + content + encode(DEFAULT, readline=readline)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rgb(red, green, blue, content):\n    color = 16 + 36 * red + 6 * green + blue\n    return encode('38;5;' + str(color)) + content + encode(DEFAULT)", "response": "Colors a content using rgb for h\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquoting the string in a SQL table column name.", "response": "def quote(self, identifier, errors=\"strict\"):\n        \"\"\"\n        https://stackoverflow.com/questions/6514274/how-do-you-escape-strings-for-sqlite-table-column-names-in-python\n\n        :param identifier:\n        :param errors:\n        :return:\n        \"\"\"\n        if identifier is None:\n            return None\n\n        encodable = identifier.encode(\"utf-8\", errors).decode(\"utf-8\")\n\n        nul_index = encodable.find(\"\\x00\")\n\n        if nul_index >= 0:\n            error = UnicodeEncodeError(\n                \"NUL-terminated utf-8\",\n                encodable,\n                nul_index,\n                nul_index + 1,\n                \"NUL not allowed\"\n            )\n            error_handler = codecs.lookup_error(errors)\n            replacement, _ = error_handler(error)\n            encodable = encodable.replace(\"\\x00\", replacement)\n\n        return \"\\\"\" + encodable.replace(\"\\\"\", \"\\\"\\\"\") + \"\\\"\""}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nencoding the input dataframe into a dictionary with encoded values.", "response": "def encode_x(self, data):\n        \"\"\"\n        :param data: unencoded input dataframe\n        :return: a dict with encoded values\n        \"\"\"\n        encoded = OrderedDict()\n        if self.multiprocessing:\n            pool = multiprocessing.Pool(self.workers)\n            results = []\n            for encoder in self.encoders:\n                results.append((encoder, pool.apply_async(self.transform, (encoder, data))))\n\n            for encoder, result in results:\n                self.merged_transformed(encoded, encoder, result.get())\n\n        else:\n            for encoder in self.encoders:\n                self.merged_transformed(encoded, encoder, self.transform(encoder, data), append_twin=False)\n                if encoder.twin:\n                    self.merged_transformed(encoded, encoder, self.transform(encoder, data, append_twin = True), append_twin=True)\n\n        for column in self.index:\n            encoded[column] = self.read_column(data, column)\n\n        # Using a DataFrame as a container temporarily requires double the memory,\n        # as pandas copies all data on __init__. This is justified by having a\n        # type supported by all dependent libraries (heterogeneous dict is not)\n        dataframe = pandas.DataFrame(encoded)\n        if self.index:\n            dataframe.set_index(self.index)\n        return dataframe"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets an object or creates it if it does not exist.", "response": "def get_or_create(cls, **kwargs):\n        '''\n        Creates an object or returns the object if exists\n        credit to Kevin @ StackOverflow\n        from: http://stackoverflow.com/questions/2546207/does-sqlalchemy-have-an-equivalent-of-djangos-get-or-create\n        '''\n        session = Session()\n        instance = session.query(cls).filter_by(**kwargs).first()\n        session.close()\n\n        if not instance:\n            self = cls(**kwargs)\n            self.save()\n        else:\n            self = instance\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _features_as_kv(self):\n        self._data = self.get_data()\n        key_list = self.key()\n        values_list = self.values()\n        result = {}\n        for column in values_list:\n            key_prefix = self.cache_key_prefix() + \"#\" + column\n            self._data['cache_key'] = self._data[key_list].apply(lambda xdf: key_prefix + \"=\" + '#'.join(xdf.astype(str).values), axis=1)\n            result[column] = dict(zip(self._data.cache_key.values, self._data[column].values))\n        return result", "response": "Return features row as kv pairs so that they can be stored in memcache or redis"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_row_keys(self):\n        keys = self.key\n        columns = self.values\n        if not self._data:\n            self._data = self.get_data()\n\n        for column in columns:\n            key_prefix = self.cache_key_prefix() + \"#\" + column\n            self._data['cache_key'] = self._data[keys].apply(lambda xdf: key_prefix + \"=\" + '#'.join(xdf.astype(str).values),\n                                                 axis=1)\n        return list(self._data['cache_key'].values)", "response": "Method for generating the row keys for the user warehouse searches and conversions of the keys\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calling_logger(height=1):\n    stack = inspect.stack()\n    height = min(len(stack) - 1, height)\n    caller = stack[height]\n    scope = caller[0].f_globals\n    path = scope['__name__']\n    if path == '__main__':\n        path = scope['__package__'] or os.path.basename(sys.argv[0])\n    return logging.getLogger(path)", "response": "Obtain a logger for the calling module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting a naive model to the data", "response": "def fit(self, x, y, **kwargs):\n        \"\"\"\n        Fit a naive model\n        :param x: Predictors to use for fitting the data (this will not be used in naive models)\n        :param y: Outcome\n        \"\"\"\n        self.mean = numpy.mean(y)\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npredict the class of the user s income.", "response": "def predict(self, dataframe):\n        \"\"\"See :ref:`Base Estimator for Naive _naive_base_predict`\"\"\"\n        if self.mean > 0.5:\n            return numpy.ones(dataframe.shape[0])\n        else:\n            return numpy.zeros(dataframe.shape[0])"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npredicts probabilities using the model", "response": "def predict_proba(self, dataframe):\n        \"\"\"Predict probabilities using the model\n        :param dataframe: Dataframe against which to make predictions\n        \"\"\"\n        ret = numpy.ones((dataframe.shape[0], 2))\n        ret[:, 0] = (1 - self.mean)\n        ret[:, 1] = self.mean\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fillna(self, series, addition=0):\n        if series.dtype == numpy.object:\n            return series\n\n        return series.fillna(self.missing_value + addition).astype(self.dtype)", "response": "Fill in missing values with encoder specific default values."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms data to encode the sequence_length column.", "response": "def transform(self, data):\n        \"\"\"\n        :param data: DataFrame with column to encode\n        :return: encoded Series\n        \"\"\"\n        with timer('transform %s' % self.name, logging.DEBUG):\n            transformed = super(Token, self).transform(self.tokenize(data))\n            return transformed.reshape((len(data), self.sequence_length))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, data):\n        with timer('transform %s' % self.name, logging.DEBUG):\n            transformed = super(NestedUnique, self).transform(self.unnest(data))\n            return transformed.reshape((len(data), self.sequence_length))", "response": "Transform the data to encode the unique key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrandomize search hyper parameters", "response": "def hyper_parameter_search(\n            self,\n            param_distributions,\n            n_iter=10,\n            scoring=None,\n            fit_params={},\n            n_jobs=1,\n            iid=True,\n            refit=True,\n            cv=None,\n            verbose=0,\n            pre_dispatch='2*njobs',\n            random_state=None,\n            error_score='raise',\n            return_train_score=True,\n            test=True,\n            score=True,\n            save=True,\n            custom_data=None\n    ):\n        \"\"\"Random search hyper params\n        \"\"\"\n        params = locals()\n        params.pop('self')\n\n        self.fitting = lore.metadata.Fitting.create(\n            model=self.name,\n            custom_data=custom_data,\n            snapshot=lore.metadata.Snapshot(pipeline=self.pipeline.name,\n                                            head=str(self.pipeline.training_data.head(2)),\n                                            tail=str(self.pipeline.training_data.tail(2))\n                                            )\n        )\n\n        result = RandomizedSearchCV(\n            self.estimator,\n            param_distributions,\n            n_iter=n_iter,\n            scoring=scoring,\n            n_jobs=n_jobs,\n            iid=iid,\n            refit=refit,\n            cv=cv,\n            verbose=verbose,\n            pre_dispatch=pre_dispatch,\n            random_state=random_state,\n            error_score=error_score,\n            return_train_score=return_train_score\n        ).fit(\n            self.pipeline.encoded_training_data.x,\n            y=self.pipeline.encoded_training_data.y,\n            validation_x=self.pipeline.encoded_validation_data.x,\n            validation_y=self.pipeline.encoded_validation_data.y,\n            **fit_params\n        )\n        self.estimator = result.best_estimator_\n        self.stats = {}\n        self.estimator_kwargs = self.estimator.__getstate__()\n\n        if test:\n            self.stats['test'] = self.evaluate(self.pipeline.test_data)\n\n        if score:\n            self.stats['score'] = self.score(self.pipeline.test_data)\n\n        self.complete_fitting()\n\n        if save:\n            self.save()\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef require(packages):\n    global INSTALLED_PACKAGES, _new_requirements\n\n    if _new_requirements:\n        INSTALLED_PACKAGES = None\n\n    set_installed_packages()\n    if not INSTALLED_PACKAGES:\n        return\n\n    if not isinstance(packages, list):\n        packages = [packages]\n\n    missing = []\n    for package in packages:\n        name = re.split(r'[!<>=]', package)[0].lower()\n        if name not in INSTALLED_PACKAGES:\n            print(ansi.info() + ' %s is required.' % package)\n            missing += [package]\n\n    if missing:\n        mode = 'a' if os.path.exists(REQUIREMENTS) else 'w'\n        with open(REQUIREMENTS, mode) as requirements:\n            requirements.write('\\n' + '\\n'.join(missing) + '\\n')\n        print(ansi.info() + ' Dependencies added to requirements.txt. Rebooting.')\n        _new_requirements = True\n        import lore.__main__\n        lore.__main__.install(None, None)\n        reboot('--env-checked')", "response": "Ensures that a pypi package has been installed into the App s python environment."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef launched():\n    if not PREFIX:\n        return False\n\n    return os.path.realpath(sys.prefix) == os.path.realpath(PREFIX)", "response": "Test whether the current python environment is the correct lore env."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate():\n    if not os.path.exists(os.path.join(ROOT, APP, '__init__.py')):\n        message = ansi.error() + ' Python module not found.'\n        if os.environ.get('LORE_APP') is None:\n            message += ' $LORE_APP is not set. Should it be different than \"%s\"?' % APP\n        else:\n            message += ' $LORE_APP is set to \"%s\". Should it be different?' % APP\n        sys.exit(message)\n\n    if exists():\n        return\n\n    if len(sys.argv) > 1:\n        command = sys.argv[1]\n    else:\n        command = 'lore'\n    sys.exit(\n        ansi.error() + ' %s is only available in lore '\n                       'app directories (missing %s)' % (\n            ansi.bold(command),\n            ansi.underline(VERSION_PATH)\n        )\n    )", "response": "Display error messages and exit if no lore environment can be found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nensuring that python is running from the Lore virtualenv past this point.", "response": "def launch():\n    \"\"\"Ensure that python is running from the Lore virtualenv past this point.\n    \"\"\"\n    if launched():\n        check_version()\n        os.chdir(ROOT)\n        return\n\n    if not os.path.exists(BIN_LORE):\n        missing = ' %s virtualenv is missing.' % APP\n        if '--launched' in sys.argv:\n            sys.exit(ansi.error() + missing + ' Please check for errors during:\\n $ lore install\\n')\n        else:\n            print(ansi.warning() + missing)\n            import lore.__main__\n            lore.__main__.install(None, None)\n\n    reboot('--env-launched')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reboot(*args):\n    args = list(sys.argv) + list(args)\n    if args[0] == 'python' or not args[0]:\n        args[0] = BIN_PYTHON\n    elif os.path.basename(sys.argv[0]) in ['lore', 'lore.exe']:\n        args[0] = BIN_LORE\n    try:\n        os.execv(args[0], args)\n    except Exception as e:\n        if args[0] == BIN_LORE and args[1] == 'console' and JUPYTER_KERNEL_PATH:\n            print(ansi.error() + ' Your jupyter kernel may be corrupt. Please remove it so lore can reinstall:\\n $ rm ' + JUPYTER_KERNEL_PATH)\n        raise e", "response": "Reboot python in the Lore virtualenv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_requirements():\n    if not os.path.exists(REQUIREMENTS):\n        sys.exit(\n            ansi.error() + ' %s is missing. Please check it in.' % ansi.underline(REQUIREMENTS)\n        )\n\n    with open(REQUIREMENTS, 'r', encoding='utf-8') as f:\n        dependencies = f.readlines()\n\n    vcs = [d for d in dependencies if re.match(r'^(-e )?(git|svn|hg|bzr).*', d)]\n\n    dependencies = list(set(dependencies) - set(vcs))\n\n    missing = []\n    try:\n        pkg_resources.require(dependencies)\n    except (\n        pkg_resources.ContextualVersionConflict,\n        pkg_resources.DistributionNotFound,\n        pkg_resources.VersionConflict\n    ) as error:\n        missing.append(str(error))\n    except pkg_resources.RequirementParseError:\n        pass\n\n    if missing:\n        missing = ' missing requirement:\\n  ' + os.linesep.join(missing)\n        if '--env-checked' in sys.argv:\n            sys.exit(ansi.error() + missing + '\\nRequirement installation failure, please check for errors in:\\n $ lore install\\n')\n        else:\n            print(ansi.warning() + missing)\n            import lore.__main__\n            lore.__main__.install_requirements(None)\n            reboot('--env-checked')", "response": "Check that all listed packages from requirements. txt have been installed into the virtualenv at boot."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_config(path):\n    if configparser is None:\n        return None\n\n    # Check for env specific configs first\n    if os.path.exists(os.path.join(ROOT, 'config', NAME, path)):\n        path = os.path.join(ROOT, 'config', NAME, path)\n    else:\n        path = os.path.join(ROOT, 'config', path)\n\n    if not os.path.isfile(path):\n        return None\n\n    conf = open(path, 'rt').read()\n    conf = os.path.expandvars(conf)\n\n    config = configparser.SafeConfigParser()\n    if sys.version_info[0] == 2:\n        from io import StringIO\n        config.readfp(StringIO(unicode(conf)))\n    else:\n        config.read_string(conf)\n    return config", "response": "Load a config from disk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_version(path):\n    version = None\n    if os.path.exists(path):\n        version = open(path, 'r', encoding='utf-8').read().strip()\n\n    if version:\n        return re.sub(r'^python-', '', version)\n\n    return version", "response": "Attempts to read a python version string from a runtime. txt file and returns it"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extend_path():\n    if ROOT not in sys.path:\n        sys.path.insert(0, ROOT)\n\n    if LIB not in sys.path:\n        sys.path.insert(0, LIB)", "response": "Adds Lore App modules to the path to making importing easy including."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_env_file():\n    if not os.path.exists(ENV_FILE):\n        return\n\n    for line in open(ENV_FILE, 'r'):\n        line = line.strip()\n        if not line:\n            continue\n        name, value = line.split('=', 1)\n        if not name or not value or name.startswith('#') or len(name) == 0 or name.isspace():\n            continue\n        if re.match(r'^([\"\\']).*\\1$', value):\n            if value.startswith('\"'):\n                value = os.path.expandvars(value)\n            value = value[1:-1]\n        os.environ[name] = value", "response": "Loads environment variables defined in the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_env_directory():\n    for var in glob.glob(os.path.join(ENV_DIRECTORY, '*')):\n        if os.path.isfile(var):\n            os.environ[os.path.basename(var)] = os.path.expandvars(open(var, encoding='utf-8').read())", "response": "Load environment variables defined in the environment directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes the application with the specified settings.", "response": "def init_app(\n            self,\n            app,\n            feature_policy=DEFAULT_FEATURE_POLICY,\n            force_https=True,\n            force_https_permanent=False,\n            force_file_save=False,\n            frame_options=SAMEORIGIN,\n            frame_options_allow_from=None,\n            strict_transport_security=True,\n            strict_transport_security_preload=False,\n            strict_transport_security_max_age=ONE_YEAR_IN_SECS,\n            strict_transport_security_include_subdomains=True,\n            content_security_policy=DEFAULT_CSP_POLICY,\n            content_security_policy_report_uri=None,\n            content_security_policy_report_only=False,\n            content_security_policy_nonce_in=None,\n            referrer_policy=DEFAULT_REFERRER_POLICY,\n            session_cookie_secure=True,\n            session_cookie_http_only=True):\n        \"\"\"\n        Initialization.\n\n        Args:\n            app: A Flask application.\n            feature_policy: A string or dictionary describing the\n                feature policy for the response.\n            force_https: Redirects non-http requests to https, disabled in\n                debug mode.\n            force_https_permanent: Uses 301 instead of 302 redirects.\n            frame_options: Sets the X-Frame-Options header, defaults to\n                SAMEORIGIN.\n            frame_options_allow_from: Used when frame_options is set to\n                ALLOW_FROM and is a string of domains to allow frame embedding.\n            strict_transport_security: Sets HSTS headers.\n            strict_transport_security_preload: Enables HSTS preload. See\n                https://hstspreload.org.\n            strict_transport_security_max_age: How long HSTS headers are\n                honored by the browser.\n            strict_transport_security_include_subdomain: Whether to include\n                all subdomains when setting HSTS.\n            content_security_policy: A string or dictionary describing the\n                content security policy for the response.\n            content_security_policy_report_uri: A string indicating the report\n                URI used for CSP violation reports\n            content_security_policy_report_only: Whether to set the CSP header\n                as \"report-only\", which disables the enforcement by the browser\n                and requires a \"report-uri\" parameter with a backend to receive\n                the POST data\n            content_security_policy_nonce_in: A list of csp sections to include\n                a per-request nonce value in\n            referrer_policy: A string describing the referrer policy for the\n                response.\n            session_cookie_secure: Forces the session cookie to only be sent\n                over https. Disabled in debug mode.\n            session_cookie_http_only: Prevents JavaScript from reading the\n                session cookie.\n            force_file_save: Prevents the user from opening a file download\n                directly on >= IE 8\n\n        See README.rst for a detailed description of each option.\n        \"\"\"\n        if isinstance(feature_policy, dict):\n            self.feature_policy = OrderedDict(feature_policy)\n        else:\n            self.feature_policy = feature_policy\n        self.force_https = force_https\n        self.force_https_permanent = force_https_permanent\n\n        self.frame_options = frame_options\n        self.frame_options_allow_from = frame_options_allow_from\n\n        self.strict_transport_security = strict_transport_security\n        self.strict_transport_security_preload = \\\n            strict_transport_security_preload\n        self.strict_transport_security_max_age = \\\n            strict_transport_security_max_age\n        self.strict_transport_security_include_subdomains = \\\n            strict_transport_security_include_subdomains\n\n        if isinstance(content_security_policy, dict):\n            self.content_security_policy = OrderedDict(content_security_policy)\n        else:\n            self.content_security_policy = content_security_policy\n        self.content_security_policy_report_uri = \\\n            content_security_policy_report_uri\n        self.content_security_policy_report_only = \\\n            content_security_policy_report_only\n        if self.content_security_policy_report_only and \\\n                self.content_security_policy_report_uri is None:\n            raise ValueError(\n                'Setting content_security_policy_report_only to True also '\n                'requires a URI to be specified in '\n                'content_security_policy_report_uri')\n        self.content_security_policy_nonce_in = (\n            content_security_policy_nonce_in or []\n        )\n\n        app.jinja_env.globals['csp_nonce'] = self._get_nonce\n\n        self.referrer_policy = referrer_policy\n\n        self.session_cookie_secure = session_cookie_secure\n\n        if session_cookie_http_only:\n            app.config['SESSION_COOKIE_HTTPONLY'] = True\n\n        self.force_file_save = force_file_save\n\n        self.app = app\n\n        app.before_request(self._force_https)\n        app.before_request(self._make_nonce)\n        app.after_request(self._set_response_headers)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _force_https(self):\n\n        if self.session_cookie_secure:\n            if not self.app.debug:\n                self.app.config['SESSION_COOKIE_SECURE'] = True\n\n        criteria = [\n            self.app.debug,\n            flask.request.is_secure,\n            flask.request.headers.get('X-Forwarded-Proto', 'http') == 'https',\n        ]\n\n        local_options = self._get_local_options()\n\n        if local_options['force_https'] and not any(criteria):\n            if flask.request.url.startswith('http://'):\n                url = flask.request.url.replace('http://', 'https://', 1)\n                code = 302\n                if self.force_https_permanent:\n                    code = 301\n                r = flask.redirect(url, code=code)\n                return r", "response": "Redirect any non - https requests to https."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies all configured headers to the given response.", "response": "def _set_response_headers(self, response):\n        \"\"\"Applies all configured headers to the given response.\"\"\"\n        options = self._get_local_options()\n        self._set_feature_headers(response.headers, options)\n        self._set_frame_options_headers(response.headers, options)\n        self._set_content_security_policy_headers(response.headers, options)\n        self._set_hsts_headers(response.headers)\n        self._set_referrer_policy_headers(response.headers)\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_weapon_list():\n    twohand = []\n    for item in TWOHANDED_WEAPONS:\n        twohand.append([item])\n\n    onehand = []\n    for item in ONEHANDED_WEAPONS:\n        onehand.append([item])\n\n    shield = SHIELDS\n\n    dualwield_variations = []\n    weapon_shield_variations = []\n\n    for item in ONEHANDED_WEAPONS:\n        for i in ONEHANDED_WEAPONS:\n            dualwield_variations.append([item, i])\n        for j in shield:\n            weapon_shield_variations.append([j, item])\n\n    return twohand + onehand + dualwield_variations + weapon_shield_variations", "response": "Initialize the possible weapon list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting information from the hashcode to generate different colors. Returns a list of colors in r g b tupels.", "response": "def grind_hash_for_colors(hashcode):\n    \"\"\"Extracts information from the hashcode\n    to generate different colors. Returns a\n    list of colors in (r,g,b) tupels.\"\"\"\n\n    # When using smaller hash algorithms like MD5 or SHA1,\n    # the number of bits does not provide enough information\n    # to generate unique colors. Instead the hash is internally\n    # appended by itself to fit the MINIMUM_HASH_LEN.\n    # This leads to smaller hashes displaying less color\n    # variatons, depicting the insecurity of small hashes.\n    while (len(hashcode) < MINIMUM_HASH_LEN):\n        chardiff = diff(len(hashcode), MINIMUM_HASH_LEN)\n        if DEBUG:\n            print (\"Hashcode: %r with length: %d is too small. Appending difference.\" % (hashcode, len(hashcode)))\n        hashcode += hashcode[:chardiff]\n        if DEBUG:\n            print (\"Hash is now: %r with length: %d\" % (hashcode, len(hashcode)))\n\n    hashparts = split_sequence(hashcode, HEX_COLOR_LEN)\n\n    colors = []\n\n    for i in range(COLOR_QUANTITY):\n        colors.append(hex2rgb(hashparts[i]))\n    if DEBUG:\n        print (\"Generated colors: %r\" % colors)\n    return colors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split_sequence(seq, n):\n    tokens = []\n    while seq:\n        tokens.append(seq[:n])\n        seq = seq[n:]\n    return tokens", "response": "Generates tokens of length n from a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grind_hash_for_weapon(hashcode):\n\n    weaponlist = init_weapon_list()\n    # The second six characters of the hash\n    # control the weapon decision.\n    weapon_control = hashcode[ASPECT_CONTROL_LEN:(ASPECT_CONTROL_LEN * 2)]\n\n    # Decimal value of the hash chunk to map.\n    hash_dec_value = int(weapon_control, HEX_BASE)\n    decision = map_decision(MAX_DECISION_VALUE, len(weaponlist), hash_dec_value)\n\n    return choose_weapon(decision, weaponlist)", "response": "Grinds the given hashcode for a weapon to draw on\n    the pixelmap. Utilizes the second six characters from the hashcode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchoosing a weapon from a given list of weapons based on the decision.", "response": "def choose_weapon(decision, weapons):\n    \"\"\"Chooses a weapon from a given list\n    based on the decision.\"\"\"\n    choice = []\n    for i in range(len(weapons)):\n        if (i < decision):\n            choice = weapons[i]\n    return choice"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef choose_aspect(decision):\n    choice = []\n    for i in range(len(ASPECTSTYLES)):\n        if (i < decision):\n            choice = ASPECTSTYLES[i]\n    return choice", "response": "Chooses a style from ASPECTSTYLES based on the decision."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hex2rgb(hexvalue):\n\n    # Make sure a possible '#' char is eliminated\n    # before processing the color.\n    if ('#' in hexvalue):\n        hexcolor = hexvalue.replace('#', '')\n    else:\n        hexcolor = hexvalue\n\n    # Hex colors have a fixed length of 6 characters excluding the '#'\n    # TODO: Include custom exception here, even if it should never happen.\n    if (len(hexcolor) != 6):\n        print (\"Unexpected length of hex color value.\\nSix characters excluding \\'#\\' expected.\")\n        return 0\n\n    # Convert each two characters of\n    # the hex to an RGB color value.\n    r = int(hexcolor[0:2], HEX_BASE)\n    g = int(hexcolor[2:4], HEX_BASE)\n    b = int(hexcolor[4:6], HEX_BASE)\n\n    return r, g, b", "response": "Converts a given hex color to\n    its respective rgb color."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef index():\n    default = [\"pagan\", \"python\", \"avatar\", \"github\"]\n    slogan = request.forms.get(\"slogan\")\n\n    if not slogan:\n        if request.get_cookie(\"hist1\"):\n            slogan = request.get_cookie(\"hist1\")\n        else:\n            slogan = \"pagan\"\n\n    if not request.get_cookie(\"hist1\"):\n        hist1, hist2, hist3, hist4 = default[:]\n    else:\n        hist1 = request.get_cookie(\"hist1\")\n        hist2 = request.get_cookie(\"hist2\")\n        hist3 = request.get_cookie(\"hist3\")\n        hist4 = request.get_cookie(\"hist4\")\n\n    if slogan in (hist1, hist2, hist3, hist4):\n        history = [hist1, hist2, hist3, hist4]\n        history.remove(slogan)\n        hist1, hist2, hist3 = history[0], history[1], history[2]\n\n    response.set_cookie(\"hist1\", slogan, max_age=60*60*24*30, httponly=True)\n    response.set_cookie(\"hist2\", hist1, max_age=60*60*24*30, httponly=True)\n    response.set_cookie(\"hist3\", hist2, max_age=60*60*24*30, httponly=True)\n    response.set_cookie(\"hist4\", hist3, max_age=60*60*24*30, httponly=True)\n    # slogan, hist1, hist2, hist3 = escape(slogan), escape(hist1),\\\n    #     escape(hist2), escape(hist3)\n    md5 = hashlib.md5()\n    md5.update(slogan)\n    slogan_hash = md5.hexdigest()\n    md5.update(hist1)\n    hist1_hash = md5.hexdigest()\n    md5.update(hist2)\n    hist2_hash = md5.hexdigest()\n    md5.update(hist3)\n    hist3_hash = md5.hexdigest()\n    return template(TEMPLATEINDEX, slogan=slogan,\n                    hist1=hist1, hist2=hist2, hist3=hist3,\n                    sloganHash=slogan_hash, hist1Hash=hist1_hash,\n                    hist2Hash=hist2_hash, hist3Hash=hist3_hash)", "response": "main functionality of webserver"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating image by hash", "response": "def hashimage(hashvalue):\n    \"\"\"generate image by hash, usese tempfile :-/\"\"\"\n    tmpf = tempfile.mkstemp(\".png\")[1]\n    image = pagan.Avatar(\"\")\n    image.img = pagan.generator.generate_by_hash(hashvalue)\n    image.save(\"/\", tmpf)\n    return static_file(tmpf, root=\"/\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the avatar based on the input and the chosen hash function.", "response": "def __create_image(self, inpt, hashfun):\n        \"\"\"Creates the avatar based on the input and\n        the chosen hash function.\"\"\"\n        if hashfun not in generator.HASHES.keys():\n            print (\"Unknown or unsupported hash function. Using default: %s\"\n                   % self.DEFAULT_HASHFUN)\n            algo = self.DEFAULT_HASHFUN\n        else:\n            algo = hashfun\n        return generator.generate(inpt, algo)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change(self, inpt, hashfun=DEFAULT_HASHFUN):\n        self.img = self.__create_image(inpt, hashfun)", "response": "Change the avatar by providing a new input."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, path=DEFAULT_OUTPUT_PATH, filename=DEFAULT_FILENAME):\n\n        # Creates a path when it does not exist.\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        # Cut the .png file ending if one was omitted.\n        if filename[-4:] == \".png\":\n            filename = filename[:-4]\n\n        # Saves the image under the given filepath.\n        filepath = (\"%s%s.png\" % (path, filename))\n        filepath = os.path.join(path, \"%s.png\" % filename)\n        # FIXIT: filepath without SUFFIX, print writes false filename\n        print (\"Saving: %s\" % filepath)\n        self.img.save(filepath, 'PNG')", "response": "Saves the avatar to a given path to a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a hash from a given String with a specified algorithm and returns the hash in hexadecimal form. Default : sha256.", "response": "def hash_input(inpt, algo=HASH_SHA256):\n    \"\"\"Generates a hash from a given String\n    with a specified algorithm and returns the\n    hash in hexadecimal form. Default: sha256.\"\"\"\n    if (algo == HASH_MD5):\n        hashcode = hashlib.md5()\n    elif (algo == HASH_SHA1):\n        hashcode = hashlib.sha1()\n    elif (algo == HASH_SHA224):\n        hashcode = hashlib.sha224()\n    elif (algo == HASH_SHA256):\n        hashcode = hashlib.sha256()\n    elif (algo == HASH_SHA384):\n        hashcode = hashlib.sha384()\n    elif (algo == HASH_SHA512):\n        hashcode = hashlib.sha512()\n\n    if sys.version_info.major == 2:\n        inpt = bytes(inpt)\n    else:\n        inpt = bytes(inpt, \"utf-8\")\n\n    hashcode.update(inpt)\n    hexhash = hashcode.hexdigest()\n    return hexhash"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_shield_deco_layer(weapons, ip):\n    '''Reads the SHIELD_DECO.pgn file and creates\n    the shield decal layer.'''\n    layer = []\n    if weapons[0] in hashgrinder.SHIELDS:\n        layer = pgnreader.parse_pagan_file(FILE_SHIELD_DECO, ip, invert=False, sym=False)\n    return layer", "response": "Reads the SHIELD_DECO. pgn file and creates\n    the shield decal layer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_hair_layer(aspect, ip):\n    '''Reads the HAIR.pgn file and creates\n    the hair layer.'''\n    layer = []\n    if 'HAIR' in aspect:\n        layer = pgnreader.parse_pagan_file(FILE_HAIR, ip, invert=False, sym=True)\n    return layer", "response": "Reads the HAIR. pgn file and creates\n    the hair layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_torso_layer(aspect, ip):\n    '''Reads the TORSO.pgn file and creates\n    the torso layer.'''\n    layer = []\n    if 'TOP' in aspect:\n        layer = pgnreader.parse_pagan_file(FILE_TORSO, ip, invert=False, sym=True)\n    return layer", "response": "Reads the TORSO. pgn file and creates\n    the torso layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads the SUBFIELD. pgn file and creates the subfield layer.", "response": "def create_subfield_layer(aspect, ip):\n    '''Reads the SUBFIELD.pgn file and creates\n    the subfield layer.'''\n    layer = []\n    if 'PANTS' in aspect:\n        layer = pgnreader.parse_pagan_file(FILE_SUBFIELD, ip, invert=False, sym=True)\n    else:\n        layer = pgnreader.parse_pagan_file(FILE_MIN_SUBFIELD, ip, invert=False, sym=True)\n\n    return layer"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_boots_layer(aspect, ip):\n    '''Reads the BOOTS.pgn file and creates\n    the boots layer.'''\n    layer = []\n    if 'BOOTS' in aspect:\n        layer = pgnreader.parse_pagan_file(FILE_BOOTS, ip, invert=False, sym=True)\n    return layer", "response": "Reads the BOOTS. pgn file and creates\n    the boots layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_shield_layer(shield, hashcode):\n    return pgnreader.parse_pagan_file(('%s%spgn%s' % (PACKAGE_DIR, os.sep, os.sep)) + shield + '.pgn', hashcode, sym=False, invert=False)", "response": "Creates the layer for shields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate the layer for weapons.", "response": "def create_weapon_layer(weapon, hashcode, isSecond=False):\n    \"\"\"Creates the layer for weapons.\"\"\"\n    return pgnreader.parse_pagan_file(('%s%spgn%s' % (PACKAGE_DIR, os.sep, os.sep)) + weapon + '.pgn', hashcode, sym=False, invert=isSecond)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scale_pixels(color, layer):\n    pixelmap = []\n\n    # Scaling the pixel offsets.\n    for pix_x in range(MAX_X + 1):\n        for pix_y in range(MAX_Y + 1):\n\n            # Horizontal pixels\n            y1 = pix_y * dotsize[0]\n            x1 = pix_x * dotsize[1]\n\n            # Vertical pixels\n            y2 = pix_y * dotsize[0] + (dotsize[0] - 1)\n            x2 = pix_x * dotsize[1] + (dotsize[1] - 1)\n\n            if (y1 <= MAX_Y) and (y2 <= MAX_Y):\n                if (x1 <= MAX_X) and (x2 <= MAX_X):\n                    if (pix_x, pix_y) in layer:\n                        pixelmap.append([(y1, x1), (y2, x2), color])\n    return pixelmap", "response": "Scales the pixel to the virtual pixelmap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef draw_image(pixelmap, img):\n    '''Draws the image based on the given pixelmap.'''\n    for item in pixelmap:\n        color = item[2]\n\n        # Access the rectangle edges.\n        pixelbox = (item[0][0], item[0][1], item[1][0], item[1][1])\n        draw = ImageDraw.Draw(img)\n        draw.rectangle(pixelbox, fill=color)", "response": "Draws the image based on the given pixelmap."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and combines all required layers to build a pixelmap for creating the virtual pixels.", "response": "def setup_pixelmap(hashcode):\n    \"\"\"Creates and combines all required layers to\n    build a pixelmap for creating the virtual\n    pixels.\"\"\"\n\n    # Color distribution.\n    # colors = hashgrinder.grindIpForColors(ip)\n    colors = hashgrinder.grind_hash_for_colors(hashcode)\n    color_body = colors[0]\n    color_subfield = colors[1]\n    color_weapon_a = colors[2]\n    color_weapon_b = colors[3]\n    color_shield_deco = colors[4]\n    color_boots = colors[5]\n    color_hair = colors[6]\n    color_top = colors[7]\n\n    # Grinds for the aspect.\n    aspect = hashgrinder.grind_hash_for_aspect(hashcode)\n\n\n    #Determine weapons of the avatar.\n    weapons = hashgrinder.grind_hash_for_weapon(hashcode)\n\n\n\n    if DEBUG:\n        print (\"Current aspect: %r\" % aspect)\n        print (\"Current weapons: %r\" % weapons)\n\n    # There is just one body template. The optional pixels need to be mirrored so\n    # the body layout will be symmetric to avoid uncanny looks.\n    layer_body = pgnreader.parse_pagan_file(FILE_BODY, hashcode, invert=False, sym=True)\n\n    layer_hair = create_hair_layer(aspect, hashcode)\n    layer_boots = create_boots_layer(aspect, hashcode)\n    layer_torso = create_torso_layer(aspect, hashcode)\n\n    has_shield = (weapons[0] in hashgrinder.SHIELDS)\n\n    if has_shield:\n        layer_weapon_a = create_shield_layer(weapons[0], hashcode)\n        layer_weapon_b = create_weapon_layer(weapons[1], hashcode)\n    else:\n        layer_weapon_a = create_weapon_layer(weapons[0], hashcode)\n        if (len(weapons) == 2):\n            layer_weapon_b = create_weapon_layer(weapons[1], hashcode, True)\n\n    layer_subfield = create_subfield_layer(aspect, hashcode)\n    layer_deco = create_shield_deco_layer(weapons, hashcode)\n\n    pixelmap = scale_pixels(color_body, layer_body)\n    pixelmap += scale_pixels(color_top, layer_torso)\n    pixelmap += scale_pixels(color_hair, layer_hair)\n    pixelmap += scale_pixels(color_subfield, layer_subfield)\n    pixelmap += scale_pixels(color_boots, layer_boots)\n    pixelmap += scale_pixels(color_weapon_a, layer_weapon_a)\n    if (len(weapons) == 2):\n        pixelmap += scale_pixels(color_weapon_b, layer_weapon_b)\n    pixelmap += scale_pixels(color_shield_deco, layer_deco)\n\n    return pixelmap"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate(str, alg):\n    img = Image.new(IMAGE_MODE, IMAGE_SIZE, BACKGROUND_COLOR)\n    hashcode = hash_input(str, alg)\n    pixelmap = setup_pixelmap(hashcode)\n    draw_image(pixelmap, img)\n    return img", "response": "Generates an PIL image avatar based on the given\n    input String. Acts as the main accessor to pagan."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_by_hash(hashcode):\n    img = Image.new(IMAGE_MODE, IMAGE_SIZE, BACKGROUND_COLOR)\n    if len(hashcode) < 32:\n        print (\"hashcode must have lenght >= 32, %s\" % hashcode)\n        raise FalseHashError\n\n    allowed = \"0123456789abcdef\"\n    hashcheck = [c in allowed for c in hashcode]\n    if False in hashcheck:\n        print (\"hashcode has not allowed structure %s\" % hashcode)\n        raise FalseHashError\n\n    pixelmap = setup_pixelmap(hashcode)\n    draw_image(pixelmap, img)\n    return img", "response": "Generates an PIL image avatar based on the given hashcode. Acts as the main accessor to pagan."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef enforce_vertical_symmetry(pixmap):\n    '''Enforces vertical symmetry of the pixelmap.\n    Returns a pixelmap with all pixels mirrored in the middle.\n    The initial ones still remain.'''\n    mirror = []\n    for item in pixmap:\n        y = item[0]\n        x = item[1]\n\n        if x <= IMAGE_APEX:\n            diff_x = diff(x, IMAGE_APEX)\n            mirror.append((y, x + (2 * diff_x) - 1))\n\n        if x > IMAGE_APEX:\n            diff_x = diff(x, IMAGE_APEX)\n            mirror.append((y, x - (2 * diff_x) - 1))\n\n    return mirror + pixmap", "response": "Enforces vertical symmetry of the pixelmap."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchomping down company types to a more convention form.", "response": "def replace_types(text):\n    \"\"\"Chomp down company types to a more convention form.\"\"\"\n    if not hasattr(replace_types, '_replacer'):\n        replace_types._replacer = build_replacer()\n    return replace_types._replacer(text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clean_strict(text, boundary=WS):\n    # transliterate to ascii\n    text = ascii_text(text)\n    # replace punctuation and symbols\n    text = CHARACTERS_REMOVE_RE.sub('', text)\n    text = category_replace(text)\n    # pad out for company type replacements\n    text = ''.join((boundary, collapse_spaces(text), boundary))\n    return text", "response": "Super - hardcore string scrubbing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nauditing the system for issues.", "response": "def selfcheck(tools):\n    \"\"\"Audit the system for issues.\n\n    :param tools: Tools description. Use elevation.TOOLS to test elevation.\n    \"\"\"\n    msg = []\n    for tool_name, check_cli in collections.OrderedDict(tools).items():\n        try:\n            subprocess.check_output(check_cli, shell=True, stderr=subprocess.STDOUT)\n        except subprocess.CalledProcessError:\n            msg.append('%r not found or not usable.' % tool_name)\n    return '\\n'.join(msg) if msg else 'Your system is ready.'"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef seed(cache_dir=CACHE_DIR, product=DEFAULT_PRODUCT, bounds=None, max_download_tiles=9, **kwargs):\n    datasource_root, spec = ensure_setup(cache_dir, product)\n    ensure_tiles_names = list(spec['tile_names'](*bounds))\n    # FIXME: emergency hack to enforce the no-bulk-download policy\n    if len(ensure_tiles_names) > max_download_tiles:\n        raise RuntimeError(\"Too many tiles: %d. Please consult the providers' websites \"\n                           \"for how to bulk download tiles.\" % len(ensure_tiles_names))\n\n    with util.lock_tiles(datasource_root, ensure_tiles_names):\n        ensure_tiles(datasource_root, ensure_tiles_names, **kwargs)\n\n    with util.lock_vrt(datasource_root, product):\n        util.check_call_make(datasource_root, targets=['all'])\n    return datasource_root", "response": "Seed the DEM to given bounds."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clip(bounds, output=DEFAULT_OUTPUT, margin=MARGIN, **kwargs):\n    bounds = build_bounds(bounds, margin=margin)\n    datasource_root = seed(bounds=bounds, **kwargs)\n    do_clip(datasource_root, bounds, output, **kwargs)", "response": "Clip the DEM to given bounds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nshowing info about the product cache.", "response": "def info(cache_dir=CACHE_DIR, product=DEFAULT_PRODUCT):\n    \"\"\"Show info about the product cache.\n\n    :param cache_dir: Root of the DEM cache folder.\n    :param product: DEM product choice.\n    \"\"\"\n    datasource_root, _ = ensure_setup(cache_dir, product)\n    util.check_call_make(datasource_root, targets=['info'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the consumer and oauth tokens with three - legged OAuth process and save in a yaml file in the user s home directory.", "response": "def new_oauth(yaml_path):\n    '''\n    Return the consumer and oauth tokens with three-legged OAuth process and\n    save in a yaml file in the user's home directory.\n    '''\n\n    print('Retrieve consumer key and consumer secret from http://www.tumblr.com/oauth/apps')\n    consumer_key = input('Paste the consumer key here: ').strip()\n    consumer_secret = input('Paste the consumer secret here: ').strip()\n\n    request_token_url = 'http://www.tumblr.com/oauth/request_token'\n    authorize_url = 'http://www.tumblr.com/oauth/authorize'\n    access_token_url = 'http://www.tumblr.com/oauth/access_token'\n\n    # STEP 1: Obtain request token\n    oauth_session = OAuth1Session(consumer_key, client_secret=consumer_secret)\n    fetch_response = oauth_session.fetch_request_token(request_token_url)\n    resource_owner_key = fetch_response.get('oauth_token')\n    resource_owner_secret = fetch_response.get('oauth_token_secret')\n\n    # STEP 2: Authorize URL + Rresponse\n    full_authorize_url = oauth_session.authorization_url(authorize_url)\n\n    # Redirect to authentication page\n    print('\\nPlease go here and authorize:\\n{}'.format(full_authorize_url))\n    redirect_response = input('Allow then paste the full redirect URL here:\\n').strip()\n\n    # Retrieve oauth verifier\n    oauth_response = oauth_session.parse_authorization_response(redirect_response)\n\n    verifier = oauth_response.get('oauth_verifier')\n\n    # STEP 3: Request final access token\n    oauth_session = OAuth1Session(\n        consumer_key,\n        client_secret=consumer_secret,\n        resource_owner_key=resource_owner_key,\n        resource_owner_secret=resource_owner_secret,\n        verifier=verifier\n    )\n    oauth_tokens = oauth_session.fetch_access_token(access_token_url)\n\n    tokens = {\n        'consumer_key': consumer_key,\n        'consumer_secret': consumer_secret,\n        'oauth_token': oauth_tokens.get('oauth_token'),\n        'oauth_token_secret': oauth_tokens.get('oauth_token_secret')\n    }\n\n    yaml_file = open(yaml_path, 'w+')\n    yaml.dump(tokens, yaml_file, indent=2)\n    yaml_file.close()\n\n    return tokens"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhelps us validate the parameters for the request :param valid_options: a list of strings of valid options for the api request :param params: a dict, the key-value store which we really only care about the key which has tells us what the user is using for the API request :returns: None or throws an exception if the validation fails", "response": "def validate_params(valid_options, params):\n    \"\"\"\n    Helps us validate the parameters for the request\n\n    :param valid_options: a list of strings of valid options for the\n                          api request\n    :param params: a dict, the key-value store which we really only care about\n                   the key which has tells us what the user is using for the\n                   API request\n\n    :returns: None or throws an exception if the validation fails\n    \"\"\"\n    #crazy little if statement hanging by himself :(\n    if not params:\n        return\n\n    #We only allow one version of the data parameter to be passed\n    data_filter = ['data', 'source', 'external_url', 'embed']\n    multiple_data = [key for key in params.keys() if key in data_filter]\n    if len(multiple_data) > 1:\n        raise Exception(\"You can't mix and match data parameters\")\n\n    #No bad fields which are not in valid options can pass\n    disallowed_fields = [key for key in params.keys() if key not in valid_options]\n    if disallowed_fields:\n        field_strings = \",\".join(disallowed_fields)\n        raise Exception(\"{} are not allowed fields\".format(field_strings))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_blogname(fn):\n    @wraps(fn)\n    def add_dot_tumblr(*args, **kwargs):\n        if (len(args) > 1 and (\".\" not in args[1])):\n            args = list(args)\n            args[1] += \".tumblr.com\"\n        return fn(*args, **kwargs)\n    return add_dot_tumblr", "response": "Decorator to validate the blogname and return a function that accepts a blogname and returns a new object that can be used to query all the same blog."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, url, params):\n        url = self.host + url\n        if params:\n            url = url + \"?\" + urllib.parse.urlencode(params)\n\n        try:\n            resp = requests.get(url, allow_redirects=False, headers=self.headers, auth=self.oauth)\n        except TooManyRedirects as e:\n            resp = e.response\n\n        return self.json_parse(resp)", "response": "Issues a GET request against the API."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(self, url, params={}, files=[]):\n        url = self.host + url\n        try:\n            if files:\n                return self.post_multipart(url, params, files)\n            else:\n                data = urllib.parse.urlencode(params)\n                if not PY3:\n                    data = str(data)\n                resp = requests.post(url, data=data, headers=self.headers, auth=self.oauth)\n                return self.json_parse(resp)\n        except HTTPError as e:\n            return self.json_parse(e.response)", "response": "Issues a POST request against the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef json_parse(self, response):\n        try:\n            data = response.json()\n        except ValueError:\n            data = {'meta': { 'status': 500, 'msg': 'Server Error'}, 'response': {\"error\": \"Malformed JSON or HTML was returned.\"}}\n\n        # We only really care about the response if we succeed\n        # and the error if we fail\n        if 200 <= data['meta']['status'] <= 399:\n            return data['response']\n        else:\n            return data", "response": "Wraps and abstracts response validation and JSON parsing\nAttributeNames to make sure the user gets the correct response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_multipart(self, url, params, files):\n        resp = requests.post(\n            url,\n            data=params,\n            params=params,\n            files=files,\n            headers=self.headers,\n            allow_redirects=False,\n            auth=self.oauth\n        )\n        return self.json_parse(resp)", "response": "Generates and issues a multipart request for data files\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the url of the blog s avatar", "response": "def avatar(self, blogname, size=64):\n        \"\"\"\n        Retrieves the url of the blog's avatar\n\n        :param blogname: a string, the blog you want the avatar for\n\n        :returns: A dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/avatar/{}\".format(blogname, size)\n        return self.send_api_request(\"get\", url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tagged(self, tag, **kwargs):\n        kwargs.update({'tag': tag})\n        return self.send_api_request(\"get\", '/v2/tagged', kwargs, ['before', 'limit', 'filter', 'tag', 'api_key'], True)", "response": "Get a list of posts tagged with the given tag"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a list of posts from a particular blog.", "response": "def posts(self, blogname, type=None, **kwargs):\n        \"\"\"\n        Gets a list of posts from a particular blog\n\n        :param blogname: a string, the blogname you want to look up posts\n                         for. eg: codingjester.tumblr.com\n        :param id: an int, the id of the post you are looking for on the blog\n        :param tag: a string, the tag you are looking for on posts\n        :param limit: an int, the number of results you want\n        :param offset: an int, the offset of the posts you want to start at.\n        :param before: an int, the timestamp for posts you want before.\n        :param filter: the post format you want returned: HTML, text or raw.\n        :param type: the type of posts you want returned, e.g. video. If omitted returns all post types.\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        if type is None:\n            url = '/v2/blog/{}/posts'.format(blogname)\n        else:\n            url = '/v2/blog/{}/posts/{}'.format(blogname, type)\n        return self.send_api_request(\"get\", url, kwargs, ['id', 'tag', 'limit', 'offset', 'before', 'reblog_info', 'notes_info', 'filter', 'api_key'], True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef blog_info(self, blogname):\n        url = \"/v2/blog/{}/info\".format(blogname)\n        return self.send_api_request(\"get\", url, {}, ['api_key'], True)", "response": "Returns the information of the given blog"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef blog_following(self, blogname, **kwargs):\n        url = \"/v2/blog/{}/following\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['limit', 'offset'])", "response": "Get the publicly exposed list of blogs that a blog follows."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef followers(self, blogname, **kwargs):\n        url = \"/v2/blog/{}/followers\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['limit', 'offset'])", "response": "Gets the followers of the given blog."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the current user s likes.", "response": "def blog_likes(self, blogname, **kwargs):\n        \"\"\"\n        Gets the current given user's likes\n        :param limit: an int, the number of likes you want returned\n        (DEPRECATED) :param offset: an int, the like you want to start at, for pagination.\n        :param before: an int, the timestamp for likes you want before.\n        :param after: an int, the timestamp for likes you want after.\n\n            # Start at the 20th like and get 20 more likes.\n            client.blog_likes({'offset': 20, 'limit': 20})\n\n        :returns: A dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/likes\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['limit', 'offset', 'before', 'after'], True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef queue(self, blogname, **kwargs):\n        url = \"/v2/blog/{}/posts/queue\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['limit', 'offset', 'filter'])", "response": "Get the posts that are currently in the blog s queue."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the drafts of a blog", "response": "def drafts(self, blogname, **kwargs):\n        \"\"\"\n        Gets posts that are currently in the blog's drafts\n        :param filter: the post format that you want returned: HTML, text, raw.\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/posts/draft\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['filter'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the submission of a blog.", "response": "def submission(self, blogname, **kwargs):\n        \"\"\"\n        Gets posts that are currently in the blog's queue\n\n        :param offset: an int, the post you want to start at, for pagination.\n        :param filter: the post format that you want returned: HTML, text, raw.\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/posts/submission\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, [\"offset\", \"filter\"])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking the post with the given id and reblog key", "response": "def like(self, id, reblog_key):\n        \"\"\"\n        Like the post of the given blog\n\n        :param id: an int, the id of the post you want to like\n        :param reblog_key: a string, the reblog key of the post\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        url = \"/v2/user/like\"\n        params = {'id': id, 'reblog_key': reblog_key}\n        return self.send_api_request(\"post\", url, params, ['id', 'reblog_key'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a photo post or photoset on a blog", "response": "def create_photo(self, blogname, **kwargs):\n        \"\"\"\n        Create a photo post or photoset on a blog\n\n        :param blogname: a string, the url of the blog you want to post to.\n        :param state: a string, The state of the post.\n        :param tags: a list of tags that you want applied to the post\n        :param tweet: a string, the customized tweet that you want\n        :param date: a string, the GMT date and time of the post\n        :param format: a string, sets the format type of the post. html or markdown\n        :param slug: a string, a short text summary to the end of the post url\n        :param caption: a string, the caption that you want applied to the photo\n        :param link: a string, the 'click-through' url you want on the photo\n        :param source: a string, the photo source url\n        :param data: a string or a list of the path of photo(s)\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        kwargs.update({\"type\": \"photo\"})\n        return self._send_post(blogname, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a text post on a blog", "response": "def create_text(self, blogname, **kwargs):\n        \"\"\"\n        Create a text post on a blog\n\n        :param blogname: a string, the url of the blog you want to post to.\n        :param state: a string, The state of the post.\n        :param tags: a list of tags that you want applied to the post\n        :param tweet: a string, the customized tweet that you want\n        :param date: a string, the GMT date and time of the post\n        :param format: a string, sets the format type of the post. html or markdown\n        :param slug: a string, a short text summary to the end of the post url\n        :param title: a string, the optional title of a post\n        :param body: a string, the body of the text post\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        kwargs.update({\"type\": \"text\"})\n        return self._send_post(blogname, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a quote post on a blog", "response": "def create_quote(self, blogname, **kwargs):\n        \"\"\"\n        Create a quote post on a blog\n\n        :param blogname: a string, the url of the blog you want to post to.\n        :param state: a string, The state of the post.\n        :param tags: a list of tags that you want applied to the post\n        :param tweet: a string, the customized tweet that you want\n        :param date: a string, the GMT date and time of the post\n        :param format: a string, sets the format type of the post. html or markdown\n        :param slug: a string, a short text summary to the end of the post url\n        :param quote: a string, the full text of the quote\n        :param source: a string, the cited source of the quote\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        kwargs.update({\"type\": \"quote\"})\n        return self._send_post(blogname, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a link post on a blog", "response": "def create_link(self, blogname, **kwargs):\n        \"\"\"\n        Create a link post on a blog\n\n        :param blogname: a string, the url of the blog you want to post to.\n        :param state: a string, The state of the post.\n        :param tags: a list of tags that you want applied to the post\n        :param tweet: a string, the customized tweet that you want\n        :param date: a string, the GMT date and time of the post\n        :param format: a string, sets the format type of the post. html or markdown\n        :param slug: a string, a short text summary to the end of the post url\n        :param title: a string, the title of the link\n        :param url: a string, the url of the link you are posting\n        :param description: a string, the description of the link you are posting\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        kwargs.update({\"type\": \"link\"})\n        return self._send_post(blogname, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_chat(self, blogname, **kwargs):\n        kwargs.update({\"type\": \"chat\"})\n        return self._send_post(blogname, kwargs)", "response": "Create a chat post on a blog"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_audio(self, blogname, **kwargs):\n        kwargs.update({\"type\": \"audio\"})\n        return self._send_post(blogname, kwargs)", "response": "Create an audio post on a blog"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a video post on a blog", "response": "def create_video(self, blogname, **kwargs):\n        \"\"\"\n        Create a audio post on a blog\n\n        :param blogname: a string, the url of the blog you want to post to.\n        :param state: a string, The state of the post.\n        :param tags: a list of tags that you want applied to the post\n        :param tweet: a string, the customized tweet that you want\n        :param date: a string, the GMT date and time of the post\n        :param format: a string, sets the format type of the post. html or markdown\n        :param slug: a string, a short text summary to the end of the post url\n        :param caption: a string, the caption for the post\n        :param embed: a string, the emebed code that you'd like to upload\n        :param data: a string, the local filename path of the video you are uploading\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        kwargs.update({\"type\": \"video\"})\n        return self._send_post(blogname, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a reblog on the given blogname", "response": "def reblog(self, blogname, **kwargs):\n        \"\"\"\n        Creates a reblog on the given blogname\n\n        :param blogname: a string, the url of the blog you want to reblog to\n        :param id: an int, the post id that you are reblogging\n        :param reblog_key: a string, the reblog key of the post\n        :param comment: a string, a comment added to the reblogged post\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/post/reblog\".format(blogname)\n\n        valid_options = ['id', 'reblog_key', 'comment'] + self._post_valid_options(kwargs.get('type', None))\n        if 'tags' in kwargs and kwargs['tags']:\n            # Take a list of tags and make them acceptable for upload\n            kwargs['tags'] = \",\".join(kwargs['tags'])\n        return self.send_api_request('post', url, kwargs, valid_options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes a post with the given id", "response": "def delete_post(self, blogname, id):\n        \"\"\"\n        Deletes a post with the given id\n\n        :param blogname: a string, the url of the blog you want to delete from\n        :param id: an int, the post id that you want to delete\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n        url = \"/v2/blog/{}/post/delete\".format(blogname)\n        return self.send_api_request('post', url, {'id': id}, ['id'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _send_post(self, blogname, params):\n        url = \"/v2/blog/{}/post\".format(blogname)\n        valid_options = self._post_valid_options(params.get('type', None))\n\n        if len(params.get(\"tags\", [])) > 0:\n            # Take a list of tags and make them acceptable for upload\n            params['tags'] = \",\".join(params['tags'])\n\n        return self.send_api_request(\"post\", url, params, valid_options)", "response": "Formats and sends the API request off. Validates\n        Formats parameters and sends the API request off. Validates\n            Formats parameters and sends the API request off. Validates\n        common and per - post - type parameters and formats your tags for you."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends the url with parameters to the requested url", "response": "def send_api_request(self, method, url, params={}, valid_parameters=[], needs_api_key=False):\n        \"\"\"\n        Sends the url with parameters to the requested url, validating them\n        to make sure that they are what we expect to have passed to us\n\n        :param method: a string, the request method you want to make\n        :param params: a dict, the parameters used for the API request\n        :param valid_parameters: a list, the list of valid parameters\n        :param needs_api_key: a boolean, whether or not your request needs an api key injected\n\n        :returns: a dict parsed from the JSON response\n        \"\"\"\n        if needs_api_key:\n            params.update({'api_key': self.request.consumer_key})\n            valid_parameters.append('api_key')\n\n        files = {}\n        if 'data' in params:\n            if isinstance(params['data'], list):\n                for idx, data in enumerate(params['data']):\n                    files['data['+str(idx)+']'] =  open(params['data'][idx], 'rb')\n            else:\n                files = {'data': open(params['data'], 'rb')}\n            del params['data']\n\n        validate_params(valid_parameters, params)\n        if method == \"get\":\n            return self.request.get(url, params)\n        else:\n            return self.request.post(url, params, files)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run(path, code=None, params=None, **meta):\n        tree = compile(code, path, \"exec\", ast.PyCF_ONLY_AST)\n\n        McCabeChecker.max_complexity = int(params.get('complexity', 10))\n        return [\n            {'lnum': lineno, 'offset': offset, 'text': text, 'type': McCabeChecker._code}\n            for lineno, offset, text, _ in McCabeChecker(tree, path).run()\n        ]", "response": "MCCabe code checking.\n\n        :return list: List of errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_csp_str(val):\n    seen = set()\n    values = val if isinstance(val, (list, tuple)) else val.strip().split(',')\n    return [x for x in values if x and not (x in seen or seen.add(x))]", "response": "Split comma separated string into unique values keeping their order."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes choosen linters. : returns list of inited linters.", "response": "def parse_linters(linters):\n    \"\"\" Initialize choosen linters.\n\n    :returns: list of inited linters\n\n    \"\"\"\n    result = list()\n    for name in split_csp_str(linters):\n        linter = LINTERS.get(name)\n        if linter:\n            result.append((name, linter))\n        else:\n            logging.warning(\"Linter `%s` not found.\", name)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config_file(rootdir=None):\n    if rootdir is None:\n        return DEFAULT_CONFIG_FILE\n\n    for path in CONFIG_FILES:\n        path = os.path.join(rootdir, path)\n        if os.path.isfile(path) and os.access(path, os.R_OK):\n            return path", "response": "Search for configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse command line and configuration files.", "response": "def parse_options(args=None, config=True, rootdir=CURDIR, **overrides):  # noqa\n    \"\"\" Parse options from command line and configuration files.\n\n    :return argparse.Namespace:\n\n    \"\"\"\n    args = args or []\n\n    # Parse args from command string\n    options = PARSER.parse_args(args)\n    options.file_params = dict()\n    options.linters_params = dict()\n\n    # Compile options from ini\n    if config:\n        cfg = get_config(str(options.options), rootdir=rootdir)\n        for opt, val in cfg.default.items():\n            LOGGER.info('Find option %s (%s)', opt, val)\n            passed_value = getattr(options, opt, _Default())\n            if isinstance(passed_value, _Default):\n                if opt == 'paths':\n                    val = val.split()\n                if opt == 'skip':\n                    val = fix_pathname_sep(val)\n                setattr(options, opt, _Default(val))\n\n        # Parse file related options\n        for name, opts in cfg.sections.items():\n\n            if name == cfg.default_section:\n                continue\n\n            if name.startswith('pylama'):\n                name = name[7:]\n\n            if name in LINTERS:\n                options.linters_params[name] = dict(opts)\n                continue\n\n            mask = re.compile(fnmatch.translate(fix_pathname_sep(name)))\n            options.file_params[mask] = dict(opts)\n\n    # Override options\n    _override_options(options, **overrides)\n\n    # Postprocess options\n    for name in options.__dict__:\n        value = getattr(options, name)\n        if isinstance(value, _Default):\n            setattr(options, name, process_value(name, value.value))\n\n    if options.concurrent and 'pylint' in options.linters:\n        LOGGER.warning('Can\\'t parse code asynchronously with pylint enabled.')\n        options.concurrent = False\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads configuration from INI.", "response": "def get_config(ini_path=None, rootdir=None):\n    \"\"\" Load configuration from INI.\n\n    :return Namespace:\n\n    \"\"\"\n    config = Namespace()\n    config.default_section = 'pylama'\n\n    if not ini_path:\n        path = get_default_config_file(rootdir)\n        if path:\n            config.read(path)\n    else:\n        config.read(ini_path)\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setup_logger(options):\n    LOGGER.setLevel(logging.INFO if options.verbose else logging.WARN)\n    if options.report:\n        LOGGER.removeHandler(STREAM)\n        LOGGER.addHandler(logging.FileHandler(options.report, mode='w'))\n\n    if options.options:\n        LOGGER.info('Try to read configuration from: %r', options.options)", "response": "Do the logger setup with options."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(path, code=None, params=None, **meta):\n        import _ast\n\n        builtins = params.get(\"builtins\", \"\")\n\n        if builtins:\n            builtins = builtins.split(\",\")\n\n        tree = compile(code, path, \"exec\", _ast.PyCF_ONLY_AST)\n        w = checker.Checker(tree, path, builtins=builtins)\n        w.messages = sorted(w.messages, key=lambda m: m.lineno)\n        return [{\n            'lnum': m.lineno,\n            'text': m.message % m.message_args,\n            'type': m.message[0]\n        } for m in w.messages]", "response": "Check code with pyflakes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking path. :param rootdir: Root directory (for making relative file paths) :param options: Parsed pylama options (from pylama.config.parse_options) :returns: (list) Errors list", "response": "def check_path(options, rootdir=None, candidates=None, code=None):\n    \"\"\"Check path.\n\n    :param rootdir: Root directory (for making relative file paths)\n    :param options: Parsed pylama options (from pylama.config.parse_options)\n\n    :returns: (list) Errors list\n\n    \"\"\"\n    if not candidates:\n        candidates = []\n        for path_ in options.paths:\n            path = op.abspath(path_)\n            if op.isdir(path):\n                for root, _, files in walk(path):\n                    candidates += [op.relpath(op.join(root, f), CURDIR)\n                                   for f in files]\n            else:\n                candidates.append(path)\n\n    if rootdir is None:\n        rootdir = path if op.isdir(path) else op.dirname(path)\n\n    paths = []\n    for path in candidates:\n\n        if not options.force and not any(l.allow(path)\n                                         for _, l in options.linters):\n            continue\n\n        if not op.exists(path):\n            continue\n\n        paths.append(path)\n\n    if options.concurrent:\n        return check_async(paths, options, rootdir)\n\n    errors = []\n    for path in paths:\n        errors += run(path=path, code=code, rootdir=rootdir, options=options)\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_paths(options, candidates=None, error=True):\n    errors = check_path(options, rootdir=CURDIR, candidates=candidates)\n\n    if options.format in ['pycodestyle', 'pep8']:\n        pattern = \"%(filename)s:%(lnum)s:%(col)s: %(text)s\"\n    elif options.format == 'pylint':\n        pattern = \"%(filename)s:%(lnum)s: [%(type)s] %(text)s\"\n    else:  # 'parsable'\n        pattern = \"%(filename)s:%(lnum)s:%(col)s: [%(type)s] %(text)s\"\n\n    for er in errors:\n        if options.abspath:\n            er._info['filename'] = op.abspath(er.filename)\n        LOGGER.warning(pattern, er._info)\n\n    if error:\n        sys.exit(int(bool(errors)))\n\n    return errors", "response": "Process files and log errors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the scanner s state.", "response": "def reset(self, source):\n        \"\"\" Reset scanner's state.\n\n        :param source: Source for parsing\n\n        \"\"\"\n        self.tokens = []\n        self.source = source\n        self.pos = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scan(self):\n\n        self.pre_scan()\n\n        token = None\n        end = len(self.source)\n\n        while self.pos < end:\n\n            best_pat = None\n            best_pat_len = 0\n\n            # Check patterns\n            for p, regexp in self.patterns:\n                m = regexp.match(self.source, self.pos)\n                if m:\n                    best_pat = p\n                    best_pat_len = len(m.group(0))\n                    break\n\n            if best_pat is None:\n                raise SyntaxError(\n                    \"SyntaxError[@char {0}: {1}]\".format(\n                        self.pos, \"Bad token.\"))\n\n            # Ignore patterns\n            if best_pat in self.ignore:\n                self.pos += best_pat_len\n                continue\n\n            # Create token\n            token = (\n                best_pat,\n                self.source[self.pos:self.pos + best_pat_len],\n                self.pos,\n                self.pos + best_pat_len,\n            )\n\n            self.pos = token[-1]\n            self.tokens.append(token)", "response": "Scan source and grab tokens."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_scan(self):\n        escape_re = re.compile(r'\\\\\\n[\\t ]+')\n        self.source = escape_re.sub('', self.source)", "response": "Prepare string for scanning."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\niterates over the items in the cache.", "response": "def iteritems(self, raw=False):\n        \"\"\" Iterate self items. \"\"\"\n\n        for key in self:\n            yield key, self.__getitem__(key, raw=raw)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread and parse INI files.", "response": "def read(self, *files, **params):\n        \"\"\" Read and parse INI files.\n\n        :param *files: Files for reading\n        :param **params: Params for parsing\n\n        Set `update=False` for prevent values redefinition.\n\n        \"\"\"\n        for f in files:\n            try:\n                with io.open(f, encoding='utf-8') as ff:\n                    NS_LOGGER.info('Read from `{0}`'.format(ff.name))\n                    self.parse(ff.read(), **params)\n            except (IOError, TypeError, SyntaxError, io.UnsupportedOperation):\n                if not self.silent_read:\n                    NS_LOGGER.error('Reading error `{0}`'.format(ff.name))\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting namespace as INI file.", "response": "def write(self, f):\n        \"\"\" Write namespace as INI file.\n\n        :param f: File object or path to file.\n\n        \"\"\"\n        if isinstance(f, str):\n            f = io.open(f, 'w', encoding='utf-8')\n\n        if not hasattr(f, 'read'):\n            raise AttributeError(\"Wrong type of file: {0}\".format(type(f)))\n\n        NS_LOGGER.info('Write to `{0}`'.format(f.name))\n        for section in self.sections.keys():\n            f.write('[{0}]\\n'.format(section))\n            for k, v in self[section].items():\n                f.write('{0:15}= {1}\\n'.format(k, v))\n            f.write('\\n')\n        f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(self, source, update=True, **params):\n        scanner = INIScanner(source)\n        scanner.scan()\n\n        section = self.default_section\n        name = None\n\n        for token in scanner.tokens:\n            if token[0] == 'KEY_VALUE':\n                name, value = re.split('[=:]', token[1], 1)\n                name, value = name.strip(), value.strip()\n                if not update and name in self[section]:\n                    continue\n                self[section][name] = value\n\n            elif token[0] == 'SECTION':\n                section = token[1].strip('[]')\n\n            elif token[0] == 'CONTINUATION':\n                if not name:\n                    raise SyntaxError(\n                        \"SyntaxError[@char {0}: {1}]\".format(\n                            token[2], \"Bad continuation.\"))\n                self[section][name] += '\\n' + token[1].strip()", "response": "Parse INI source as string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(path='', code=None, rootdir=CURDIR, options=None):\n    errors = []\n    fileconfig = dict()\n    linters = LINTERS\n    linters_params = dict()\n    lname = 'undefined'\n    params = dict()\n    path = op.relpath(path, rootdir)\n\n    if options:\n        linters = options.linters\n        linters_params = options.linters_params\n        for mask in options.file_params:\n            if mask.match(path):\n                fileconfig.update(options.file_params[mask])\n\n        if options.skip and any(p.match(path) for p in options.skip):\n            LOGGER.info('Skip checking for path: %s', path)\n            return []\n\n    try:\n        with CodeContext(code, path) as ctx:\n            code = ctx.code\n            params = prepare_params(parse_modeline(code), fileconfig, options)\n            LOGGER.debug('Checking params: %s', params)\n\n            if params.get('skip'):\n                return errors\n\n            for item in params.get('linters') or linters:\n\n                if not isinstance(item, tuple):\n                    item = item, LINTERS.get(item)\n\n                lname, linter = item\n\n                if not linter:\n                    continue\n\n                lparams = linters_params.get(lname, dict())\n                LOGGER.info(\"Run %s %s\", lname, lparams)\n\n                ignore, select = merge_params(params, lparams)\n\n                linter_errors = linter.run(\n                    path, code=code, ignore=ignore, select=select, params=lparams)\n                if not linter_errors:\n                    continue\n\n                errors += filter_errors([\n                    Error(filename=path, linter=lname, **er) for er in linter_errors\n                ], ignore=ignore, select=select)\n\n    except IOError as e:\n        LOGGER.error(\"IOError %s\", e)\n        errors.append(Error(text=str(e), filename=path, linter=lname))\n\n    except SyntaxError as e:\n        LOGGER.error(\"SyntaxError %s\", e)\n        errors.append(\n            Error(linter='pylama', lnum=e.lineno, col=e.offset,\n                  text='E0100 SyntaxError: {}'.format(e.args[0]),\n                  filename=path))\n\n    except Exception as e:  # noqa\n        import traceback\n        LOGGER.error(traceback.format_exc())\n\n    errors = list(remove_duplicates(errors))\n\n    if code and errors:\n        errors = filter_skiplines(code, errors)\n\n    if options and options.sort:\n        sort = dict((v, n) for n, v in enumerate(options.sort, 1))\n\n        def key(e):\n            return (sort.get(e.type, 999), e.lnum)\n    else:\n        def key(e):\n            return e.lnum\n\n    return sorted(errors, key=key)", "response": "Run code checkers with given params."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses params from file s modeline.", "response": "def parse_modeline(code):\n    \"\"\"Parse params from file's modeline.\n\n    :return dict: Linter params.\n\n    \"\"\"\n    seek = MODELINE_RE.search(code)\n    if seek:\n        return dict(v.split('=') for v in seek.group(1).split(':'))\n\n    return dict()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_params(modeline, fileconfig, options):\n    params = dict(skip=False, ignore=[], select=[], linters=[])\n    if options:\n        params['ignore'] = list(options.ignore)\n        params['select'] = list(options.select)\n\n    for config in filter(None, [modeline, fileconfig]):\n        for key in ('ignore', 'select', 'linters'):\n            params[key] += process_value(key, config.get(key, []))\n        params['skip'] = bool(int(config.get('skip', False)))\n    # TODO: skip what? This is causing erratic behavior for linters.\n    params['skip'] = False\n\n    params['ignore'] = set(params['ignore'])\n    params['select'] = set(params['select'])\n\n    return params", "response": "Prepare and merge a params from modelines and configs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter errors by select and ignore options.", "response": "def filter_errors(errors, select=None, ignore=None, **params):\n    \"\"\"Filter errors by select and ignore options.\n\n    :return bool:\n\n    \"\"\"\n    select = select or []\n    ignore = ignore or []\n\n    for e in errors:\n        for s in select:\n            if e.number.startswith(s):\n                yield e\n                break\n        else:\n            for s in ignore:\n                if e.number.startswith(s):\n                    break\n            else:\n                yield e"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters lines by noqa.", "response": "def filter_skiplines(code, errors):\n    \"\"\"Filter lines by `noqa`.\n\n    :return list: A filtered errors\n\n    \"\"\"\n    if not errors:\n        return errors\n\n    enums = set(er.lnum for er in errors)\n    removed = set([\n        num for num, l in enumerate(code.split('\\n'), 1)\n        if num in enums and SKIP_PATTERN(l)\n    ])\n\n    if removed:\n        errors = [er for er in errors if er.lnum not in removed]\n\n    return errors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging global ignore and local params with linter local params.", "response": "def merge_params(params, lparams):\n    \"\"\"Merge global ignore/select with linter local params.\"\"\"\n    ignore = params.get('ignore', set())\n    if 'ignore' in lparams:\n        ignore = ignore | set(lparams['ignore'])\n\n    select = params.get('select', set())\n    if 'select' in lparams:\n        select = select | set(lparams['select'])\n\n    return ignore, select"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_async(paths, options, rootdir=None):\n    LOGGER.info('Async code checking is enabled.')\n    path_queue = Queue.Queue()\n    result_queue = Queue.Queue()\n\n    for num in range(CPU_COUNT):\n        worker = Worker(path_queue, result_queue)\n        worker.setDaemon(True)\n        LOGGER.info('Start worker #%s', (num + 1))\n        worker.start()\n\n    for path in paths:\n        path_queue.put((path, dict(options=options, rootdir=rootdir)))\n\n    path_queue.join()\n\n    errors = []\n    while True:\n        try:\n            errors += result_queue.get(False)\n        except Queue.Empty:\n            break\n\n    return errors", "response": "Check given paths asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        while True:\n            path, params = self.path_queue.get()\n            errors = run(path, **params)\n            self.result_queue.put(errors)\n            self.path_queue.task_done()", "response": "Run tasks from queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the output line", "response": "def _parse(self, line):\n        \"\"\"Parse the output line\"\"\"\n        try:\n            result = line.split(':', maxsplit=4)\n            filename, line_num_txt, column_txt, message_type, text = result\n        except ValueError:\n            return\n\n        try:\n            self.line_num = int(line_num_txt.strip())\n            self.column = int(column_txt.strip())\n        except ValueError:\n            return\n\n        self.filename = filename\n        self.message_type = message_type.strip()\n        self.text = text.strip()\n        self.valid = True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert to the Linter. run return value", "response": "def to_result(self):\n        \"\"\"Convert to the Linter.run return value\"\"\"\n        text = [self.text]\n        if self.note:\n            text.append(self.note)\n\n        return {\n            'lnum': self.line_num,\n            'col': self.column,\n            'text': ' - '.join(text),\n            'type': self.types.get(self.message_type, '')\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(path, code=None, params=None, **meta):\n        args = [path, '--follow-imports=skip', '--show-column-numbers']\n        stdout, stderr, status = api.run(args)\n        messages = []\n        for line in stdout.split('\\n'):\n            line.strip()\n            if not line:\n                continue\n            message = _MyPyMessage(line)\n            if message.valid:\n                if message.message_type == 'note':\n                    if messages[-1].line_num == message.line_num:\n                        messages[-1].add_note(message.text)\n                else:\n                    messages.append(message)\n\n        return [m.to_result() for m in messages]", "response": "Check code with mypy.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing value to pylint.", "response": "def prepare_value(value):\n        \"\"\"Prepare value to pylint.\"\"\"\n        if isinstance(value, (list, tuple, set)):\n            return \",\".join(value)\n\n        if isinstance(value, bool):\n            return \"y\" if value else \"n\"\n\n        return str(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(path, code=None, params=None, **meta):\n        parser = get_parser()\n        for option in parser.option_list:\n            if option.dest and option.dest in params:\n                value = params[option.dest]\n                if isinstance(value, str):\n                    params[option.dest] = option.convert_value(option, value)\n\n        for key in [\"filename\", \"exclude\", \"select\", \"ignore\"]:\n            if key in params and isinstance(params[key], str):\n                params[key] = _parse_multi_options(params[key])\n\n        P8Style = StyleGuide(reporter=_PycodestyleReport, **params)\n        buf = StringIO(code)\n        return P8Style.input_file(path, lines=buf.readlines())", "response": "Check code with pycodestyle.\n            and return list of errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_file(self, filename, lines, expected, line_offset):\n        super(_PycodestyleReport, self).init_file(\n            filename, lines, expected, line_offset)\n        self.errors = []", "response": "Prepare storage for errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(path, code=None, params=None, **meta):\n        if 'ignore_decorators' in params:\n            ignore_decorators = params['ignore_decorators']\n        else:\n            ignore_decorators = None\n        check_source_args = (code, path, ignore_decorators) if THIRD_ARG else (code, path)\n        return [{\n            'lnum': e.line,\n            # Remove colon after error code (\"D403: ...\" => \"D403 ...\").\n            'text': (e.message[0:4] + e.message[5:]\n                     if e.message[4] == ':' else e.message),\n            'type': 'D',\n            'number': e.code\n        } for e in PyDocChecker().check_source(*check_source_args)]", "response": "pydocstyle code checking.\n\n        :return list: List of errors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck code with Radon.", "response": "def run(path, code=None, params=None, ignore=None, select=None, **meta):\n        \"\"\"Check code with Radon.\n\n        :return list: List of errors.\n        \"\"\"\n        complexity = params.get('complexity', 10)\n        no_assert = params.get('no_assert', False)\n        show_closures = params.get('show_closures', False)\n\n        visitor = ComplexityVisitor.from_code(code, no_assert=no_assert)\n        blocks = visitor.blocks\n        if show_closures:\n            blocks = add_inner_blocks(blocks)\n\n        return [\n            {'lnum': block.lineno, 'col': block.col_offset, 'type': 'R', 'number': 'R709',\n             'text': 'R701: %s is too complex %d' % (block.name, block.complexity)}\n            for block in visitor.blocks if block.complexity > complexity\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns a shell command.", "response": "def run(command):\n    \"\"\"Run a shell command.\n\n    :return str: Stdout\n    \"\"\"\n    p = Popen(command.split(), stdout=PIPE, stderr=PIPE)\n    (stdout, stderr) = p.communicate()\n    return (p.returncode, [line.strip() for line in stdout.splitlines()],\n            [line.strip() for line in stderr.splitlines()])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef git_hook(error=True):\n    _, files_modified, _ = run(\"git diff-index --cached --name-only HEAD\")\n\n    options = parse_options()\n    setup_logger(options)\n    if sys.version_info >= (3,):\n        candidates = [f.decode('utf-8') for f in files_modified]\n    else:\n        candidates = [str(f) for f in files_modified]\n\n    if candidates:\n        process_paths(options, candidates=candidates, error=error)", "response": "Run pylama after git commit."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns pylama after mercurial commit.", "response": "def hg_hook(ui, repo, node=None, **kwargs):\n    \"\"\"Run pylama after mercurial commit.\"\"\"\n    seen = set()\n    paths = []\n    if len(repo):\n        for rev in range(repo[node], len(repo)):\n            for file_ in repo[rev].files():\n                file_ = op.join(repo.root, file_)\n                if file_ in seen or not op.exists(file_):\n                    continue\n                seen.add(file_)\n                paths.append(file_)\n\n    options = parse_options()\n    setup_logger(options)\n    if paths:\n        process_paths(options, candidates=paths)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef install_git(path):\n    hook = op.join(path, 'pre-commit')\n    with open(hook, 'w') as fd:\n        fd.write(\"\"\"#!/usr/bin/env python\nimport sys\nfrom pylama.hook import git_hook\n\nif __name__ == '__main__':\n    sys.exit(git_hook())\n\"\"\")\n    chmod(hook, 484)", "response": "Install hook in Git repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninstalls hook in Mercurial repository.", "response": "def install_hg(path):\n    \"\"\" Install hook in Mercurial repository. \"\"\"\n    hook = op.join(path, 'hgrc')\n    if not op.isfile(hook):\n        open(hook, 'w+').close()\n\n    c = ConfigParser()\n    c.readfp(open(hook, 'r'))\n    if not c.has_section('hooks'):\n        c.add_section('hooks')\n\n    if not c.has_option('hooks', 'commit'):\n        c.set('hooks', 'commit', 'python:pylama.hooks.hg_hook')\n\n    if not c.has_option('hooks', 'qrefresh'):\n        c.set('hooks', 'qrefresh', 'python:pylama.hooks.hg_hook')\n\n    c.write(open(hook, 'w+'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(path, code=None, params=None, **meta):\n        code = converter(code)\n        line_numbers = commented_out_code_line_numbers(code)\n        lines = code.split('\\n')\n\n        result = []\n        for line_number in line_numbers:\n            line = lines[line_number - 1]\n            result.append(dict(\n                lnum=line_number,\n                offset=len(line) - len(line.rstrip()),\n                # https://github.com/sobolevn/flake8-eradicate#output-example\n                text=converter('E800 Found commented out code: ') + line,\n                # https://github.com/sobolevn/flake8-eradicate#error-codes\n                type='E800',\n            ))\n        return result", "response": "Eradicate code checking.\n\n        :return list: List of errors."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_duplicates(errors):\n    passed = defaultdict(list)\n    for error in errors:\n        key = error.linter, error.number\n        if key in DUPLICATES:\n            if key in passed[error.lnum]:\n                continue\n            passed[error.lnum] = DUPLICATES[key]\n        yield error", "response": "Filter duplicates from given list of errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_up(self):\n        self.path.profile = self.path.gen.joinpath(\"profile\")\n\n        if not self.path.profile.exists():\n            self.path.profile.mkdir()\n\n        self.python = hitchpylibrarytoolkit.project_build(\n            \"strictyaml\",\n            self.path,\n            self.given[\"python version\"],\n            {\"ruamel.yaml\": self.given[\"ruamel version\"]},\n        ).bin.python\n\n        self.example_py_code = (\n            ExamplePythonCode(self.python, self.path.gen)\n            .with_code(self.given.get(\"code\", \"\"))\n            .with_setup_code(\n                self.given.get(\"setup\", \"\")\n            )\n            .with_terminal_size(160, 100)\n            .with_strings(\n                yaml_snippet_1=self.given.get(\"yaml_snippet_1\"),\n                yaml_snippet=self.given.get(\"yaml_snippet\"),\n                yaml_snippet_2=self.given.get(\"yaml_snippet_2\"),\n                modified_yaml_snippet=self.given.get(\"modified_yaml_snippet\"),\n            )\n        )", "response": "Set up your applications and the test environment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new chunk referring to the same location in a duplicated document.", "response": "def fork(self, strictindex, new_value):\n        \"\"\"\n        Return a chunk referring to the same location in a duplicated document.\n\n        Used when modifying a YAML chunk so that the modification can be validated\n        before changing it.\n        \"\"\"\n        forked_chunk = YAMLChunk(\n            deepcopy(self._ruamelparsed),\n            pointer=self.pointer,\n            label=self.label,\n            key_association=copy(self._key_association),\n        )\n        forked_chunk.contents[self.ruamelindex(strictindex)] = new_value.as_marked_up()\n        forked_chunk.strictparsed()[strictindex] = deepcopy(new_value.as_marked_up())\n        return forked_chunk"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_child_of(self, chunk):\n        if self.is_mapping():\n            for key, value in self.contents.items():\n                self.key(key, key).pointer.make_child_of(chunk.pointer)\n                self.val(key).make_child_of(chunk)\n        elif self.is_sequence():\n            for index, item in enumerate(self.contents):\n                self.index(index).make_child_of(chunk)\n        else:\n            self.pointer.make_child_of(chunk.pointer)", "response": "Link one YAML chunk to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a YAMLChunk referenced by a pointer.", "response": "def _select(self, pointer):\n        \"\"\"\n        Get a YAMLChunk referenced by a pointer.\n        \"\"\"\n        return YAMLChunk(\n            self._ruamelparsed,\n            pointer=pointer,\n            label=self._label,\n            strictparsed=self._strictparsed,\n            key_association=copy(self._key_association),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index(self, strictindex):\n        return self._select(self._pointer.index(self.ruamelindex(strictindex)))", "response": "Return a chunk in a sequence referenced by index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the ruamel equivalent of a strict parsed index.", "response": "def ruamelindex(self, strictindex):\n        \"\"\"\n        Get the ruamel equivalent of a strict parsed index.\n\n        E.g. 0 -> 0, 1 -> 2, parsed-via-slugify -> Parsed via slugify\n        \"\"\"\n        return (\n            self.key_association.get(strictindex, strictindex)\n            if self.is_mapping()\n            else strictindex\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a chunk referencing a value in a mapping with the key key.", "response": "def val(self, strictkey):\n        \"\"\"\n        Return a chunk referencing a value in a mapping with the key 'key'.\n        \"\"\"\n        ruamelkey = self.ruamelindex(strictkey)\n        return self._select(self._pointer.val(ruamelkey, strictkey))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a chunk referencing a key in a mapping with the name key.", "response": "def key(self, key, strictkey=None):\n        \"\"\"\n        Return a chunk referencing a key in a mapping with the name 'key'.\n        \"\"\"\n        return self._select(self._pointer.key(key, strictkey))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef textslice(self, start, end):\n        return self._select(self._pointer.textslice(start, end))", "response": "Return a chunk referencing a slice of a scalar text value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flatten(items):\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in flatten(x):\n                yield sub_x\n        else:\n            yield x", "response": "Yields items from any nested iterable."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of start and end positions of comma separated text items.", "response": "def comma_separated_positions(text):\n    \"\"\"\n    Start and end positions of comma separated text items.\n\n    Commas and trailing spaces should not be included.\n\n    >>> comma_separated_positions(\"ABC, 2,3\")\n    [(0, 3), (5, 6), (7, 8)]\n    \"\"\"\n    chunks = []\n    start = 0\n    end = 0\n    for item in text.split(\",\"):\n        space_increment = 1 if item[0] == \" \" else 0\n        start += space_increment  # Is there a space after the comma to ignore? \", \"\n        end += len(item.lstrip()) + space_increment\n        chunks.append((start, end))\n        start += len(item.lstrip()) + 1  # Plus comma\n        end = start\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ruamel_structure(data, validator=None):\n    if isinstance(data, dict):\n        if len(data) == 0:\n            raise exceptions.CannotBuildDocumentsFromEmptyDictOrList(\n                \"Document must be built with non-empty dicts and lists\"\n            )\n        return CommentedMap(\n            [\n                (ruamel_structure(key), ruamel_structure(value))\n                for key, value in data.items()\n            ]\n        )\n    elif isinstance(data, list):\n        if len(data) == 0:\n            raise exceptions.CannotBuildDocumentsFromEmptyDictOrList(\n                \"Document must be built with non-empty dicts and lists\"\n            )\n        return CommentedSeq([ruamel_structure(item) for item in data])\n    elif isinstance(data, bool):\n        return u\"yes\" if data else u\"no\"\n    elif isinstance(data, (int, float)):\n        return str(data)\n    else:\n        if not is_string(data):\n            raise exceptions.CannotBuildDocumentFromInvalidData(\n                (\n                    \"Document must be built from a combination of:\\n\"\n                    \"string, int, float, bool or nonempty list/dict\\n\\n\"\n                    \"Instead, found variable with type '{}': '{}'\"\n                ).format(type(data).__name__, data)\n            )\n        return data", "response": "Takes a dict and list and returns a ruamel. yaml style\n    structure of CommentedMaps CommentedSeqs and a ruamel. yaml style\n    and data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns story matching keywords and rewrite story if code changed.", "response": "def rbdd(*keywords):\n    \"\"\"\n    Run story matching keywords and rewrite story if code changed.\n    \"\"\"\n    settings = _personal_settings().data\n    settings[\"engine\"][\"rewrite\"] = True\n    _storybook(settings[\"engine\"]).with_params(\n        **{\"python version\": settings[\"params\"][\"python version\"]}\n    ).only_uninherited().shortcut(*keywords).play()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregresses a file in python 2 and 3.", "response": "def regressfile(filename):\n    \"\"\"\n    Run all stories in filename 'filename' in python 2 and 3.\n    \"\"\"\n    _storybook({\"rewrite\": False}).in_filename(filename).with_params(\n        **{\"python version\": \"2.7.14\"}\n    ).filter(\n        lambda story: not story.info.get(\"fails_on_python_2\")\n    ).ordered_by_name().play()\n\n    _storybook({\"rewrite\": False}).with_params(\n        **{\"python version\": \"3.7.0\"}\n    ).in_filename(filename).ordered_by_name().play()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning regression testing - lint and then run all tests.", "response": "def regression():\n    \"\"\"\n    Run regression testing - lint and then run all tests.\n    \"\"\"\n    lint()\n    doctests()\n    storybook = _storybook({}).only_uninherited()\n    storybook.with_params(**{\"python version\": \"2.7.14\"}).filter(\n        lambda story: not story.info.get(\"fails_on_python_2\")\n    ).ordered_by_name().play()\n    storybook.with_params(**{\"python version\": \"3.7.0\"}).ordered_by_name().play()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrerunning last example code block with specified version of python.", "response": "def rerun(version=\"3.7.0\"):\n    \"\"\"\n    Rerun last example code block with specified version of python.\n    \"\"\"\n    from commandlib import Command\n\n    Command(DIR.gen.joinpath(\"py{0}\".format(version), \"bin\", \"python\"))(\n        DIR.gen.joinpath(\"state\", \"examplepythoncode.py\")\n    ).in_dir(DIR.gen.joinpath(\"state\")).run()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating dicts lists and scalar values into a YAML object which can be dumped out.", "response": "def as_document(data, schema=None, label=u\"<unicode string>\"):\n    \"\"\"\n    Translate dicts/lists and scalar (string/bool/float/int/etc.) values into a\n    YAML object which can be dumped out.\n    \"\"\"\n    if schema is None:\n        schema = Any()\n\n    return schema(YAMLChunk(schema.to_yaml(data), label=label))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the first YAML document in a string and produce corresponding YAML object.", "response": "def dirty_load(\n    yaml_string, schema=None, label=u\"<unicode string>\", allow_flow_style=False\n):\n    \"\"\"\n    Parse the first YAML document in a string\n    and produce corresponding YAML object.\n\n    If allow_flow_style is set to True, then flow style is allowed.\n    \"\"\"\n    return generic_load(\n        yaml_string, schema=schema, label=label, allow_flow_style=allow_flow_style\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(yaml_string, schema=None, label=u\"<unicode string>\"):\n    return generic_load(yaml_string, schema=schema, label=label)", "response": "Parse a string containing a YAML document and produce a corresponding YAML object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the raw data representation of the document or document segment.", "response": "def data(self):\n        \"\"\"\n        Returns raw data representation of the document or document segment.\n\n        Mappings are rendered as ordered dicts, sequences as lists and scalar values\n        as whatever the validator returns (int, string, etc.).\n\n        If no validators are used, scalar values are always returned as strings.\n        \"\"\"\n        if isinstance(self._value, CommentedMap):\n            mapping = OrderedDict()\n            for key, value in self._value.items():\n                mapping[key.data] = value.data\n            return mapping\n        elif isinstance(self._value, CommentedSeq):\n            return [item.data for item in self._value]\n        else:\n            return self._value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_yaml(self):\n        dumped = dump(self.as_marked_up(), Dumper=StrictYAMLDumper, allow_unicode=True)\n        return dumped if sys.version_info[0] == 3 else dumped.decode(\"utf8\")", "response": "Render the YAML node and subnodes as string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns string value of scalar whatever value it was parsed as.", "response": "def text(self):\n        \"\"\"\n        Return string value of scalar, whatever value it was parsed as.\n        \"\"\"\n        if isinstance(self._value, CommentedMap):\n            raise TypeError(\"{0} is a mapping, has no text value.\".format(repr(self)))\n        if isinstance(self._value, CommentedSeq):\n            raise TypeError(\"{0} is a sequence, has no text value.\".format(repr(self)))\n        return self._text"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npartitioning source into a list of CodePartitions for import refactoring.", "response": "def partition_source(src):\n    \"\"\"Partitions source into a list of `CodePartition`s for import\n    refactoring.\n    \"\"\"\n    # In python2, ast.parse(text_string_with_encoding_pragma) raises\n    # SyntaxError: encoding declaration in Unicode string\n    ast_obj = ast.parse(src.encode('UTF-8'))\n    visitor = TopLevelImportVisitor()\n    visitor.visit(ast_obj)\n\n    line_offsets = get_line_offsets_by_line_no(src)\n\n    chunks = []\n    startpos = 0\n    pending_chunk_type = None\n    possible_ending_tokens = None\n    seen_import = False\n    for (\n            token_type, token_text, (srow, scol), (erow, ecol), _,\n    ) in tokenize.generate_tokens(io.StringIO(src).readline):\n        # Searching for a start of a chunk\n        if pending_chunk_type is None:\n            if not seen_import and token_type == tokenize.COMMENT:\n                if 'noreorder' in token_text:\n                    chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                    break\n                else:\n                    pending_chunk_type = CodeType.PRE_IMPORT_CODE\n                    possible_ending_tokens = TERMINATES_COMMENT\n            elif not seen_import and token_type == tokenize.STRING:\n                pending_chunk_type = CodeType.PRE_IMPORT_CODE\n                possible_ending_tokens = TERMINATES_DOCSTRING\n            elif scol == 0 and srow in visitor.top_level_import_line_numbers:\n                seen_import = True\n                pending_chunk_type = CodeType.IMPORT\n                possible_ending_tokens = TERMINATES_IMPORT\n            elif token_type == tokenize.NL:\n                # A NL token is a non-important newline, we'll immediately\n                # append a NON_CODE partition\n                endpos = line_offsets[erow] + ecol\n                srctext = src[startpos:endpos]\n                startpos = endpos\n                chunks.append(CodePartition(CodeType.NON_CODE, srctext))\n            elif token_type == tokenize.COMMENT:\n                if 'noreorder' in token_text:\n                    chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                    break\n                else:\n                    pending_chunk_type = CodeType.CODE\n                    possible_ending_tokens = TERMINATES_COMMENT\n            elif token_type == tokenize.ENDMARKER:\n                # Token ended right before end of file or file was empty\n                pass\n            else:\n                # We've reached a `CODE` block, which spans the rest of the\n                # file (intentionally timid).  Let's append that block and be\n                # done\n                chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n                break\n        # Attempt to find ending of token\n        elif token_type in possible_ending_tokens:\n            endpos = line_offsets[erow] + ecol\n            srctext = src[startpos:endpos]\n            startpos = endpos\n            chunks.append(CodePartition(pending_chunk_type, srctext))\n            pending_chunk_type = None\n            possible_ending_tokens = None\n        elif token_type == tokenize.COMMENT and 'noreorder' in token_text:\n            chunks.append(CodePartition(CodeType.CODE, src[startpos:]))\n            break\n\n    chunks = [chunk for chunk in chunks if chunk.src]\n\n    # Make sure we're not removing any code\n    assert _partitions_to_src(chunks) == src\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef separate_comma_imports(partitions):\n    def _inner():\n        for partition in partitions:\n            if partition.code_type is CodeType.IMPORT:\n                import_obj = import_obj_from_str(partition.src)\n                if import_obj.has_multiple_imports:\n                    for new_import_obj in import_obj.split_imports():\n                        yield CodePartition(\n                            CodeType.IMPORT, new_import_obj.to_text(),\n                        )\n                else:\n                    yield partition\n            else:\n                yield partition\n\n    return list(_inner())", "response": "Turns import a and import b into import a and import b"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all module names that would be imported due to this import - import", "response": "def _module_to_base_modules(s):\n    \"\"\"return all module names that would be imported due to this\n    import-import\n    \"\"\"\n    parts = s.split('.')\n    for i in range(1, len(parts)):\n        yield '.'.join(parts[:i])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_thresholds(input, thresholds, choices):\n    condlist = [input <= threshold for threshold in thresholds]\n    if len(condlist) == len(choices) - 1:\n        # If a choice is provided for input > highest threshold, last condition must be true to return it.\n        condlist += [True]\n    assert len(condlist) == len(choices), \\\n        \"apply_thresholds must be called with the same number of thresholds than choices, or one more choice\"\n    return np.select(condlist, choices)", "response": "Returns a single element of the list of choices depending on the input position compared to thresholds."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_parameter_file(file_path, name = ''):\n    if not os.path.exists(file_path):\n        raise ValueError(\"{} doest not exist\".format(file_path))\n    if os.path.isdir(file_path):\n        return ParameterNode(name, directory_path = file_path)\n    data = _load_yaml_file(file_path)\n    return _parse_child(name, data, file_path)", "response": "Load a parameter file or a directory containing YAML files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, period = None, start = None, stop = None, value = None):\n        if period is not None:\n            if start is not None or stop is not None:\n                raise TypeError(\"Wrong input for 'update' method: use either 'update(period, value = value)' or 'update(start = start, stop = stop, value = value)'. You cannot both use 'period' and 'start' or 'stop'.\")\n            if isinstance(period, str):\n                period = periods.period(period)\n            start = period.start\n            stop = period.stop\n        if start is None:\n            raise ValueError(\"You must provide either a start or a period\")\n        start_str = str(start)\n        stop_str = str(stop.offset(1, 'day')) if stop else None\n\n        old_values = self.values_list\n        new_values = []\n        n = len(old_values)\n        i = 0\n\n        # Future intervals : not affected\n        if stop_str:\n            while (i < n) and (old_values[i].instant_str >= stop_str):\n                new_values.append(old_values[i])\n                i += 1\n\n        # Right-overlapped interval\n        if stop_str:\n            if new_values and (stop_str == new_values[-1].instant_str):\n                pass  # such interval is empty\n            else:\n                if i < n:\n                    overlapped_value = old_values[i].value\n                    value_name = _compose_name(self.name, item_name = stop_str)\n                    new_interval = ParameterAtInstant(value_name, stop_str, data = {'value': overlapped_value})\n                    new_values.append(new_interval)\n                else:\n                    value_name = _compose_name(self.name, item_name = stop_str)\n                    new_interval = ParameterAtInstant(value_name, stop_str, data = {'value': None})\n                    new_values.append(new_interval)\n\n        # Insert new interval\n        value_name = _compose_name(self.name, item_name = start_str)\n        new_interval = ParameterAtInstant(value_name, start_str, data = {'value': value})\n        new_values.append(new_interval)\n\n        # Remove covered intervals\n        while (i < n) and (old_values[i].instant_str >= start_str):\n            i += 1\n\n        # Past intervals : not affected\n        while i < n:\n            new_values.append(old_values[i])\n            i += 1\n\n        self.values_list = new_values", "response": "Update the value of a specific legislation parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging another ParameterNode into the current node.", "response": "def merge(self, other):\n        \"\"\"\n        Merges another ParameterNode into the current node.\n\n        In case of child name conflict, the other node child will replace the current node child.\n        \"\"\"\n        for child_name, child in other.children.items():\n            self.add_child(child_name, child)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_child(self, name, child):\n        if name in self.children:\n            raise ValueError(\"{} has already a child named {}\".format(self.name, name))\n        if not (isinstance(child, ParameterNode) or isinstance(child, Parameter) or isinstance(child, Scale)):\n            raise TypeError(\"child must be of type ParameterNode, Parameter, or Scale. Instead got {}\".format(type(child)))\n        self.children[name] = child\n        setattr(self, name, child)", "response": "Add a child to the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_node_vectorisable(node):\n        MESSAGE_PART_1 = \"Cannot use fancy indexing on parameter node '{}', as\"\n        MESSAGE_PART_3 = \"To use fancy indexing on parameter node, its children must be homogenous.\"\n        MESSAGE_PART_4 = \"See more at <https://openfisca.org/doc/coding-the-legislation/legislation_parameters#computing-a-parameter-that-depends-on-a-variable-fancy-indexing>.\"\n\n        def raise_key_inhomogeneity_error(node_with_key, node_without_key, missing_key):\n            message = \" \".join([\n                MESSAGE_PART_1,\n                \"'{}' exists, but '{}' doesn't.\",\n                MESSAGE_PART_3,\n                MESSAGE_PART_4,\n                ]).format(\n                node._name,\n                '.'.join([node_with_key, missing_key]),\n                '.'.join([node_without_key, missing_key]),\n                )\n\n            raise ValueError(message)\n\n        def raise_type_inhomogeneity_error(node_name, non_node_name):\n            message = \" \".join([\n                MESSAGE_PART_1,\n                \"'{}' is a node, but '{}' is not.\",\n                MESSAGE_PART_3,\n                MESSAGE_PART_4,\n                ]).format(\n                node._name,\n                node_name,\n                non_node_name,\n                )\n\n            raise ValueError(message)\n\n        def raise_not_implemented(node_name, node_type):\n            message = \" \".join([\n                MESSAGE_PART_1,\n                \"'{}' is a '{}', and fancy indexing has not been implemented yet on this kind of parameters.\",\n                MESSAGE_PART_4,\n                ]).format(\n                node._name,\n                node_name,\n                node_type,\n                )\n            raise NotImplementedError(message)\n\n        def extract_named_children(node):\n            return {\n                '.'.join([node._name, key]): value\n                for key, value in node._children.items()\n                }\n\n        def check_nodes_homogeneous(named_nodes):\n            \"\"\"\n                Check than several nodes (or parameters, or baremes) have the same structure.\n            \"\"\"\n            names = list(named_nodes.keys())\n            nodes = list(named_nodes.values())\n            first_node = nodes[0]\n            first_name = names[0]\n            if isinstance(first_node, ParameterNodeAtInstant):\n                children = extract_named_children(first_node)\n                for node, name in list(zip(nodes, names))[1:]:\n                    if not isinstance(node, ParameterNodeAtInstant):\n                        raise_type_inhomogeneity_error(first_name, name)\n                    first_node_keys = first_node._children.keys()\n                    node_keys = node._children.keys()\n                    if not first_node_keys == node_keys:\n                        missing_keys = set(first_node_keys).difference(node_keys)\n                        if missing_keys:  # If the first_node has a key that node hasn't\n                            raise_key_inhomogeneity_error(first_name, name, missing_keys.pop())\n                        else:  # If If the node has a key that first_node doesn't have\n                            missing_key = set(node_keys).difference(first_node_keys).pop()\n                            raise_key_inhomogeneity_error(name, first_name, missing_key)\n                    children.update(extract_named_children(node))\n                check_nodes_homogeneous(children)\n            elif isinstance(first_node, float) or isinstance(first_node, int):\n                for node, name in list(zip(nodes, names))[1:]:\n                    if isinstance(node, int) or isinstance(node, float):\n                        pass\n                    elif isinstance(node, ParameterNodeAtInstant):\n                        raise_type_inhomogeneity_error(name, first_name)\n                    else:\n                        raise_not_implemented(name, type(node).__name__)\n\n            else:\n                raise_not_implemented(first_name, type(first_node).__name__)\n\n        check_nodes_homogeneous(extract_named_children(node))", "response": "Check that a node can be casted to a vectorial node."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreplace an existing OpenFisca variable in the tax and benefit system by a new one.", "response": "def replace_variable(self, variable):\n        \"\"\"\n        Replaces an existing OpenFisca variable in the tax and benefit system by a new one.\n\n        The new variable must have the same name than the replaced one.\n\n        If no variable with the given name exists in the tax and benefit system, no error will be raised and the new variable will be simply added.\n\n        :param Variable variable: New variable to add. Must be a subclass of Variable.\n        \"\"\"\n        name = variable.__name__\n        if self.variables.get(name) is not None:\n            del self.variables[name]\n        self.load_variable(variable, update = False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds all OpenFisca variables contained in a given file to the tax and benefit system.", "response": "def add_variables_from_file(self, file_path):\n        \"\"\"\n        Adds all OpenFisca variables contained in a given file to the tax and benefit system.\n        \"\"\"\n        try:\n            file_name = path.splitext(path.basename(file_path))[0]\n\n            #  As Python remembers loaded modules by name, in order to prevent collisions, we need to make sure that:\n            #  - Files with the same name, but located in different directories, have a different module names. Hence the file path hash in the module name.\n            #  - The same file, loaded by different tax and benefit systems, has distinct module names. Hence the `id(self)` in the module name.\n            module_name = '{}_{}_{}'.format(id(self), hash(path.abspath(file_path)), file_name)\n\n            module_directory = path.dirname(file_path)\n            try:\n                module = load_module(module_name, *find_module(file_name, [module_directory]))\n            except NameError as e:\n                logging.error(str(e) + \": if this code used to work, this error might be due to a major change in OpenFisca-Core. Checkout the changelog to learn more: <https://github.com/openfisca/openfisca-core/blob/master/CHANGELOG.md>\")\n                raise\n            potential_variables = [getattr(module, item) for item in dir(module) if not item.startswith('__')]\n            for pot_variable in potential_variables:\n                # We only want to get the module classes defined in this module (not imported)\n                if isclass(pot_variable) and issubclass(pot_variable, Variable) and pot_variable.__module__ == module_name:\n                    self.add_variable(pot_variable)\n        except Exception:\n            log.error('Unable to load OpenFisca variables from file \"{}\"'.format(file_path))\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_variables_from_directory(self, directory):\n        py_files = glob.glob(path.join(directory, \"*.py\"))\n        for py_file in py_files:\n            self.add_variables_from_file(py_file)\n        subdirectories = glob.glob(path.join(directory, \"*/\"))\n        for subdirectory in subdirectories:\n            self.add_variables_from_directory(subdirectory)", "response": "Recursively explores a directory and adds all OpenFisca variables found there to the tax and benefit system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_extension(self, extension):\n        # Load extension from installed pip package\n        try:\n            package = importlib.import_module(extension)\n            extension_directory = package.__path__[0]\n        except ImportError:\n            message = linesep.join([traceback.format_exc(),\n                                    'Error loading extension: `{}` is neither a directory, nor a package.'.format(extension),\n                                    'Are you sure it is installed in your environment? If so, look at the stack trace above to determine the origin of this error.',\n                                    'See more at <https://github.com/openfisca/openfisca-extension-template#installing>.'])\n            raise ValueError(message)\n\n        self.add_variables_from_directory(extension_directory)\n        param_dir = path.join(extension_directory, 'parameters')\n        if path.isdir(param_dir):\n            extension_parameters = ParameterNode(directory_path = param_dir)\n            self.parameters.merge(extension_parameters)", "response": "Loads an extension into the tax and benefit system."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_reform(self, reform_path):\n        from openfisca_core.reforms import Reform\n        try:\n            reform_package, reform_name = reform_path.rsplit('.', 1)\n        except ValueError:\n            raise ValueError('`{}` does not seem to be a path pointing to a reform. A path looks like `some_country_package.reforms.some_reform.`'.format(reform_path))\n        try:\n            reform_module = importlib.import_module(reform_package)\n        except ImportError:\n            message = linesep.join([traceback.format_exc(),\n                                    'Could not import `{}`.'.format(reform_package),\n                                    'Are you sure of this reform module name? If so, look at the stack trace above to determine the origin of this error.'])\n            raise ValueError(message)\n        reform = getattr(reform_module, reform_name, None)\n        if reform is None:\n            raise ValueError('{} has no attribute {}'.format(reform_package, reform_name))\n        if not issubclass(reform, Reform):\n            raise ValueError('`{}` does not seem to be a valid Openfisca reform.'.format(reform_path))\n\n        return reform(self)", "response": "Returns a new tax and benefit system with the reformed tax and benefit system."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a variable from the tax and benefit system.", "response": "def get_variable(self, variable_name, check_existence = False):\n        \"\"\"\n        Get a variable from the tax and benefit system.\n\n        :param variable_name: Name of the requested variable.\n        :param check_existence: If True, raise an error if the requested variable does not exist.\n        \"\"\"\n        variables = self.variables\n        found = variables.get(variable_name)\n        if not found and check_existence:\n            raise VariableNotFound(variable_name, self)\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef neutralize_variable(self, variable_name):\n        self.variables[variable_name] = get_neutralized_variable(self.get_variable(variable_name))", "response": "Neutralizes an OpenFisca variable existing in the tax and benefit system."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the legislation parameter for a directory containing YAML parameters files.", "response": "def load_parameters(self, path_to_yaml_dir):\n        \"\"\"\n        Loads the legislation parameter for a directory containing YAML parameters files.\n\n        :param path_to_yaml_dir: Absolute path towards the YAML parameter directory.\n\n        Example:\n\n        >>> self.load_parameters('/path/to/yaml/parameters/dir')\n        \"\"\"\n\n        parameters = ParameterNode('', directory_path = path_to_yaml_dir)\n\n        if self.preprocess_parameters is not None:\n            parameters = self.preprocess_parameters(parameters)\n\n        self.parameters = parameters"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the parameters of the legislation at a given instant.", "response": "def get_parameters_at_instant(self, instant):\n        \"\"\"\n        Get the parameters of the legislation at a given instant\n\n        :param instant: string of the format 'YYYY-MM-DD' or `openfisca_core.periods.Instant` instance.\n        :returns: The parameters of the legislation at a given instant.\n        :rtype: :any:`ParameterNodeAtInstant`\n        \"\"\"\n        if isinstance(instant, periods.Period):\n            instant = instant.start\n        elif isinstance(instant, (str, int)):\n            instant = periods.instant(instant)\n        else:\n            assert isinstance(instant, periods.Instant), \"Expected an Instant (e.g. Instant((2017, 1, 1)) ). Got: {}.\".format(instant)\n\n        parameters_at_instant = self._parameters_at_instant_cache.get(instant)\n        if parameters_at_instant is None and self.parameters is not None:\n            parameters_at_instant = self.parameters.get_at_instant(str(instant))\n            self._parameters_at_instant_cache[instant] = parameters_at_instant\n        return parameters_at_instant"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the metatada relative to the country package the tax and benefit system is built from.", "response": "def get_package_metadata(self):\n        \"\"\"\n            Gets metatada relative to the country package the tax and benefit system is built from.\n\n            :returns: Country package metadata\n            :rtype: dict\n\n            Example:\n\n            >>> tax_benefit_system.get_package_metadata()\n            >>> {\n            >>>    'location': '/path/to/dir/containing/package',\n            >>>    'name': 'openfisca-france',\n            >>>    'repository_url': 'https://github.com/openfisca/openfisca-france',\n            >>>    'version': '17.2.0'\n            >>>    }\n        \"\"\"\n        # Handle reforms\n        if self.baseline:\n            return self.baseline.get_package_metadata()\n\n        fallback_metadata = {\n            'name': self.__class__.__name__,\n            'version': '',\n            'repository_url': '',\n            'location': '',\n            }\n\n        module = inspect.getmodule(self)\n        if not module.__package__:\n            return fallback_metadata\n        package_name = module.__package__.split('.')[0]\n        try:\n            distribution = pkg_resources.get_distribution(package_name)\n        except pkg_resources.DistributionNotFound:\n            return fallback_metadata\n\n        location = inspect.getsourcefile(module).split(package_name)[0].rstrip('/')\n\n        home_page_metadatas = [\n            metadata.split(':', 1)[1].strip(' ')\n            for metadata in distribution._get_metadata(distribution.PKG_INFO) if 'Home-page' in metadata\n            ]\n        repository_url = home_page_metadatas[0] if home_page_metadatas else ''\n        return {\n            'name': distribution.key,\n            'version': distribution.version,\n            'repository_url': repository_url,\n            'location': location,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_variables(self, entity = None):\n        if not entity:\n            return self.variables\n        else:\n            return {\n                variable_name: variable\n                for variable_name, variable in self.variables.items()\n                # TODO - because entities are copied (see constructor) they can't be compared\n                if variable.entity.key == entity.key\n                }", "response": "Returns a dictionary containing all variables contained in a tax and benefit system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_from_dict(self, tax_benefit_system, input_dict):\n\n        input_dict = self.explicit_singular_entities(tax_benefit_system, input_dict)\n        if any(key in tax_benefit_system.entities_plural() for key in input_dict.keys()):\n            return self.build_from_entities(tax_benefit_system, input_dict)\n        else:\n            return self.build_from_variables(tax_benefit_system, input_dict)", "response": "Builds a simulation from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a simulation from a Python dict containing entities.", "response": "def build_from_entities(self, tax_benefit_system, input_dict):\n        \"\"\"\n            Build a simulation from a Python dict ``input_dict`` fully specifying entities.\n\n            Examples:\n\n            >>> simulation_builder.build_from_entities({\n                'persons': {'Javier': { 'salary': {'2018-11': 2000}}},\n                'households': {'household': {'parents': ['Javier']}}\n                })\n        \"\"\"\n        input_dict = deepcopy(input_dict)\n\n        simulation = Simulation(tax_benefit_system, tax_benefit_system.instantiate_entities())\n\n        # Register variables so get_variable_entity can find them\n        for (variable_name, _variable) in tax_benefit_system.variables.items():\n            self.register_variable(variable_name, simulation.get_variable_population(variable_name).entity)\n\n        check_type(input_dict, dict, ['error'])\n        axes = input_dict.pop('axes', None)\n\n        unexpected_entities = [entity for entity in input_dict if entity not in tax_benefit_system.entities_plural()]\n        if unexpected_entities:\n            unexpected_entity = unexpected_entities[0]\n            raise SituationParsingError([unexpected_entity],\n                ''.join([\n                    \"Some entities in the situation are not defined in the loaded tax and benefit system.\",\n                    \"These entities are not found: {0}.\",\n                    \"The defined entities are: {1}.\"]\n                    )\n                .format(\n                ', '.join(unexpected_entities),\n                ', '.join(tax_benefit_system.entities_plural())\n                    )\n                )\n        persons_json = input_dict.get(tax_benefit_system.person_entity.plural, None)\n\n        if not persons_json:\n            raise SituationParsingError([tax_benefit_system.person_entity.plural],\n                'No {0} found. At least one {0} must be defined to run a simulation.'.format(tax_benefit_system.person_entity.key))\n\n        persons_ids = self.add_person_entity(simulation.persons.entity, persons_json)\n\n        for entity_class in tax_benefit_system.group_entities:\n            instances_json = input_dict.get(entity_class.plural)\n            if instances_json is not None:\n                self.add_group_entity(self.persons_plural, persons_ids, entity_class, instances_json)\n            else:\n                self.add_default_group_entity(persons_ids, entity_class)\n\n        if axes:\n            self.axes = axes\n            self.expand_axes()\n\n        try:\n            self.finalize_variables_init(simulation.persons)\n        except PeriodMismatchError as e:\n            self.raise_period_mismatch(simulation.persons.entity, persons_json, e)\n\n        for entity_class in tax_benefit_system.group_entities:\n            try:\n                population = simulation.populations[entity_class.key]\n                self.finalize_variables_init(population)\n            except PeriodMismatchError as e:\n                self.raise_period_mismatch(population.entity, instances_json, e)\n\n        return simulation"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild a simulation from a Python dict input_dict describing variables values without expliciting entities.", "response": "def build_from_variables(self, tax_benefit_system, input_dict):\n        \"\"\"\n            Build a simulation from a Python dict ``input_dict`` describing variables values without expliciting entities.\n\n            This method uses :any:`build_default_simulation` to infer an entity structure\n\n            Example:\n\n            >>> simulation_builder.build_from_variables(\n                {'salary': {'2016-10': 12000}}\n                )\n        \"\"\"\n        count = _get_person_count(input_dict)\n        simulation = self.build_default_simulation(tax_benefit_system, count)\n        for variable, value in input_dict.items():\n            if not isinstance(value, dict):\n                if self.default_period is None:\n                    raise SituationParsingError([variable],\n                        \"Can't deal with type: expected object. Input variables should be set for specific periods. For instance: {'salary': {'2017-01': 2000, '2017-02': 2500}}, or {'birth_date': {'ETERNITY': '1980-01-01'}}.\")\n                simulation.set_input(variable, self.default_period, value)\n            else:\n                for period_str, dated_value in value.items():\n                    simulation.set_input(variable, period_str, dated_value)\n        return simulation"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild a simulation with the default number of members and persons.", "response": "def build_default_simulation(self, tax_benefit_system, count = 1):\n        \"\"\"\n            Build a simulation where:\n                - There are ``count`` persons\n                - There are ``count`` instances of each group entity, containing one person\n                - Every person has, in each entity, the first role\n        \"\"\"\n\n        simulation = Simulation(tax_benefit_system, tax_benefit_system.instantiate_entities())\n        for population in simulation.populations.values():\n            population.count = count\n            population.ids = np.array(range(count))\n            if not population.entity.is_person:\n                population.members_entity_id = population.ids  # Each person is its own group entity\n        return simulation"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_person_entity(self, entity, instances_json):\n        check_type(instances_json, dict, [entity.plural])\n        entity_ids = list(map(str, instances_json.keys()))\n        self.persons_plural = entity.plural\n        self.entity_ids[self.persons_plural] = entity_ids\n        self.entity_counts[self.persons_plural] = len(entity_ids)\n\n        for instance_id, instance_object in instances_json.items():\n            check_type(instance_object, dict, [entity.plural, instance_id])\n            self.init_variable_values(entity, instance_object, str(instance_id))\n\n        return self.get_ids(entity.plural)", "response": "Add the simulation s instances of the persons entity as described in instances_json."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding all instances of one of the model s entities as described in instances_json.", "response": "def add_group_entity(self, persons_plural, persons_ids, entity, instances_json):\n        \"\"\"\n            Add all instances of one of the model's entities as described in ``instances_json``.\n        \"\"\"\n        check_type(instances_json, dict, [entity.plural])\n        entity_ids = list(map(str, instances_json.keys()))\n\n        self.entity_ids[entity.plural] = entity_ids\n        self.entity_counts[entity.plural] = len(entity_ids)\n\n        persons_count = len(persons_ids)\n        persons_to_allocate = set(persons_ids)\n        self.memberships[entity.plural] = np.empty(persons_count, dtype = np.int32)\n        self.roles[entity.plural] = np.empty(persons_count, dtype = object)\n\n        self.entity_ids[entity.plural] = entity_ids\n        self.entity_counts[entity.plural] = len(entity_ids)\n\n        for instance_id, instance_object in instances_json.items():\n            check_type(instance_object, dict, [entity.plural, instance_id])\n\n            variables_json = instance_object.copy()  # Don't mutate function input\n\n            roles_json = {\n                role.plural or role.key: transform_to_strict_syntax(variables_json.pop(role.plural or role.key, []))\n                for role in entity.roles\n                }\n\n            for role_id, role_definition in roles_json.items():\n                check_type(role_definition, list, [entity.plural, instance_id, role_id])\n                for index, person_id in enumerate(role_definition):\n                    entity_plural = entity.plural\n                    self.check_persons_to_allocate(persons_plural, entity_plural,\n                                                   persons_ids,\n                                                   person_id, instance_id, role_id,\n                                                   persons_to_allocate, index)\n\n                    persons_to_allocate.discard(person_id)\n\n            entity_index = entity_ids.index(instance_id)\n            role_by_plural = {role.plural or role.key: role for role in entity.roles}\n\n            for role_plural, persons_with_role in roles_json.items():\n                role = role_by_plural[role_plural]\n\n                if role.max is not None and len(persons_with_role) > role.max:\n                    raise SituationParsingError([entity.plural, instance_id, role_plural], f\"There can be at most {role.max} {role_plural} in a {entity.key}. {len(persons_with_role)} were declared in '{instance_id}'.\")\n\n                for index_within_role, person_id in enumerate(persons_with_role):\n                    person_index = persons_ids.index(person_id)\n                    self.memberships[entity.plural][person_index] = entity_index\n                    person_role = role.subroles[index_within_role] if role.subroles else role\n                    self.roles[entity.plural][person_index] = person_role\n\n            self.init_variable_values(entity, variables_json, instance_id)\n\n        if persons_to_allocate:\n            entity_ids = entity_ids + list(persons_to_allocate)\n            for person_id in persons_to_allocate:\n                person_index = persons_ids.index(person_id)\n                self.memberships[entity.plural][person_index] = entity_ids.index(person_id)\n                self.roles[entity.plural][person_index] = entity.flattened_roles[0]\n            # Adjust previously computed ids and counts\n            self.entity_ids[entity.plural] = entity_ids\n            self.entity_counts[entity.plural] = len(entity_ids)\n\n        # Convert back to Python array\n        self.roles[entity.plural] = self.roles[entity.plural].tolist()\n        self.memberships[entity.plural] = self.memberships[entity.plural].tolist()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncombining all the MarginalRateTaxScales in the node into a single MarginalRateTaxScale.", "response": "def combine_tax_scales(node):\n    \"\"\"\n        Combine all the MarginalRateTaxScales in the node into a single MarginalRateTaxScale.\n    \"\"\"\n    combined_tax_scales = None\n    for child_name in node:\n        child = node[child_name]\n\n        if not isinstance(child, AbstractTaxScale):\n            log.info('Skipping {} with value {} because it is not a tax scale'.format(child_name, child))\n            continue\n\n        if combined_tax_scales is None:\n            combined_tax_scales = MarginalRateTaxScale(name = child_name)\n            combined_tax_scales.add_bracket(0, 0)\n        combined_tax_scales.add_tax_scale(child)\n    return combined_tax_scales"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an inverse MarginalRateTaxScale object for the current MarginalRateTaxSclae.", "response": "def inverse(self):\n        \"\"\"Returns a new instance of MarginalRateTaxScale\n\n        Invert a taxscale:\n            Assume tax_scale composed of bracket which thresholds are expressed in term of brut revenue.\n            The inverse is another MarginalTaxSclae which thresholds are expressed in terms of net revenue.\n            If net  = revbrut - tax_scale.calc(revbrut) then brut = tax_scale.inverse().calc(net)\n        \"\"\"\n        # threshold : threshold of brut revenue\n        # net_threshold: threshold of net revenue\n        # theta : ordonn\u00e9e \u00e0 l'origine des segments des diff\u00e9rents seuils dans une\n        #         repr\u00e9sentation du revenu imposable comme fonction lin\u00e9aire par\n        #         morceaux du revenu brut\n        # Actually 1 / (1- global_rate)\n        inverse = self.__class__(name = self.name + \"'\", option = self.option, unit = self.unit)\n        net_threshold = 0\n        for threshold, rate in zip(self.thresholds, self.rates):\n            if threshold == 0:\n                previous_rate = 0\n                theta = 0\n            # On calcule le seuil de revenu imposable de la tranche consid\u00e9r\u00e9e.\n            net_threshold = (1 - previous_rate) * threshold + theta\n            inverse.add_bracket(net_threshold, 1 / (1 - rate))\n            theta = (rate - previous_rate) * threshold + theta\n            previous_rate = rate\n        return inverse"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scale_tax_scales(self, factor):\n        assert isinstance(factor, (float, int))\n        scaled_tax_scale = self.copy()\n        return scaled_tax_scale.multiply_thresholds(factor)", "response": "Scale all the MarginalRateTaxScales in the node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode(cls, array):\n        if type(array) is EnumArray:\n            return array\n        if array.dtype.kind in {'U', 'S'}:  # String array\n            array = np.select([array == item.name for item in cls], [item.index for item in cls]).astype(ENUM_ARRAY_DTYPE)\n        elif array.dtype.kind == 'O':  # Enum items arrays\n            # Ensure we are comparing the comparable. The problem this fixes:\n            #  On entering this method \"cls\" will generally come from variable.possible_values,\n            #  while the array values may come from directly importing a module containing an Enum class.\n            #  However, variables (and hence their possible_values) are loaded by a call to load_module,\n            #  which gives them a different identity from the ones imported in the usual way.\n            #  So, instead of relying on the \"cls\" passed in, we use only its name to check that\n            #  the values in the array, if non-empty, are of the right type.\n            if len(array) > 0 and cls.__name__ is array[0].__class__.__name__:\n                cls = array[0].__class__\n            array = np.select([array == item for item in cls], [item.index for item in cls]).astype(ENUM_ARRAY_DTYPE)\n        return EnumArray(array, cls)", "response": "Encodes a string numpy array or an enum item numpy array into an EnumArray."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(self):\n        return np.select([self == item.index for item in self.possible_values], [item for item in self.possible_values])", "response": "Return the array of enum items corresponding to self\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_to_str(self):\n        return np.select([self == item.index for item in self.possible_values], [item.name for item in self.possible_values])", "response": "Return the array of string identifiers corresponding to self\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrecording that OpenFisca started computing a variable.", "response": "def record_calculation_start(self, variable_name, period, **parameters):\n        \"\"\"\n            Record that OpenFisca started computing a variable.\n\n            :param str variable_name: Name of the variable starting to be computed\n            :param Period period: Period for which the variable is being computed\n            :param list parameters: Parameter with which the variable is being computed\n        \"\"\"\n        key = self._get_key(variable_name, period, **parameters)\n\n        if self.stack:  # The variable is a dependency of another variable\n            parent = self.stack[-1]\n            self.trace[parent]['dependencies'].append(key)\n        else:  # The variable has been requested by the client\n            self.requested_calculations.add(key)\n\n        if not self.trace.get(key):\n            self.trace[key] = {'dependencies': [], 'parameters': {}}\n\n        self.stack.append(key)\n        self._computation_log.append((key, len(self.stack)))\n        self.usage_stats[variable_name]['nb_requests'] += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrecords that OpenFisca finished computing a variable.", "response": "def record_calculation_end(self, variable_name, period, result, **parameters):\n        \"\"\"\n            Record that OpenFisca finished computing a variable.\n\n            :param str variable_name: Name of the variable starting to be computed\n            :param Period period: Period for which the variable is being computed\n            :param numpy.ndarray result: Result of the computation\n            :param list parameters: Parameter with which the variable is being computed\n        \"\"\"\n        key = self._get_key(variable_name, period, **parameters)\n        expected_key = self.stack.pop()\n\n        if not key == expected_key:\n            raise ValueError(\n                \"Something went wrong with the simulation tracer: result of '{0}' was expected, got results for '{1}' instead. This does not make sense as the last variable we started computing was '{0}'.\"\n                .format(expected_key, key)\n                )\n        self.trace[key]['value'] = result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_trace(self, variable_name, period, max_depth = 1, aggregate = False, ignore_zero = False):\n        key = self._get_key(variable_name, period)\n\n        def _print_details(key, depth):\n            if depth > 0 and ignore_zero and np.all(self.trace[key]['value'] == 0):\n                return\n            yield self._print_node(key, depth, aggregate)\n            if depth < max_depth:\n                for dependency in self.trace[key]['dependencies']:\n                    return _print_details(dependency, depth + 1)\n\n        return _print_details(key, 0)", "response": "Print the trace for the given variable for the given period."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_computation_log(self, aggregate = False):\n        for line in self.computation_log(aggregate):\n            print(line)", "response": "Print the computation log of a simulation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_simulation(tax_benefit_system, nb_persons, nb_groups, **kwargs):\n    simulation = Simulation(tax_benefit_system = tax_benefit_system, **kwargs)\n    simulation.persons.ids = np.arange(nb_persons)\n    simulation.persons.count = nb_persons\n    adults = [0] + sorted(random.sample(range(1, nb_persons), nb_groups - 1))\n\n    members_entity_id = np.empty(nb_persons, dtype = int)\n\n    # A legacy role is an index that every person within an entity has. For instance, the 'demandeur' has legacy role 0, the 'conjoint' 1, the first 'child' 2, the second 3, etc.\n    members_legacy_role = np.empty(nb_persons, dtype = int)\n\n    id_group = -1\n    for id_person in range(nb_persons):\n        if id_person in adults:\n            id_group += 1\n            legacy_role = 0\n        else:\n            legacy_role = 2 if legacy_role == 0 else legacy_role + 1\n        members_legacy_role[id_person] = legacy_role\n        members_entity_id[id_person] = id_group\n\n    for entity in simulation.populations.values():\n        if not entity.is_person:\n            entity.members_entity_id = members_entity_id\n            entity.count = nb_groups\n            entity.members_role = np.where(members_legacy_role == 0, entity.flattened_roles[0], entity.flattened_roles[-1])\n    return simulation", "response": "Generate a simulation containing nb_persons persons spread in nb_groups groups."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialise a variable with random values for the given period.", "response": "def randomly_init_variable(simulation, variable_name, period, max_value, condition = None):\n    \"\"\"\n        Initialise a variable with random values (from 0 to max_value) for the given period.\n        If a condition vector is provided, only set the value of persons or groups for which condition is True.\n\n        Example:\n\n        >>> from openfisca_core.scripts.simulation_generator import make_simulation, randomly_init_variable\n        >>> from openfisca_france import CountryTaxBenefitSystem\n        >>> tbs = CountryTaxBenefitSystem()\n        >>> simulation = make_simulation(tbs, 400, 100)  # Create a simulation with 400 persons, spread among 100 families\n        >>> randomly_init_variable(simulation, 'salaire_net', 2017, max_value = 50000, condition = simulation.persons.has_role(simulation.famille.DEMANDEUR))  # Randomly set a salaire_net for all persons between 0 and 50000?\n        >>> simulation.calculate('revenu_disponible', 2017)\n        \"\"\"\n    if condition is None:\n        condition = True\n    variable = simulation.tax_benefit_system.get_variable(variable_name)\n    population = simulation.get_variable_population(variable_name)\n    value = (np.random.rand(population.count) * max_value * condition).astype(variable.dtype)\n    simulation.set_input(variable_name, period, value)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmodifying the parameters of the legislation object.", "response": "def modify_parameters(self, modifier_function):\n        \"\"\"\n        Make modifications on the parameters of the legislation\n\n        Call this function in `apply()` if the reform asks for legislation parameter modifications.\n\n        :param modifier_function: A function that takes an object of type :any:`ParameterNode` and should return an object of the same type.\n        \"\"\"\n        baseline_parameters = self.baseline.parameters\n        baseline_parameters_copy = copy.deepcopy(baseline_parameters)\n        reform_parameters = modifier_function(baseline_parameters_copy)\n        if not isinstance(reform_parameters, ParameterNode):\n            return ValueError(\n                'modifier_function {} in module {} must return a ParameterNode'\n                .format(modifier_function.__name__, modifier_function.__module__,)\n                )\n        self.parameters = reform_parameters\n        self._parameters_at_instant_cache = {}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef instant(instant):\n    if instant is None:\n        return None\n    if isinstance(instant, Instant):\n        return instant\n    if isinstance(instant, str):\n        if not INSTANT_PATTERN.match(instant):\n            raise ValueError(\"'{}' is not a valid instant. Instants are described using the 'YYYY-MM-DD' format, for instance '2015-06-15'.\".format(instant))\n        instant = Instant(\n            int(fragment)\n            for fragment in instant.split('-', 2)[:3]\n            )\n    elif isinstance(instant, datetime.date):\n        instant = Instant((instant.year, instant.month, instant.day))\n    elif isinstance(instant, int):\n        instant = (instant,)\n    elif isinstance(instant, list):\n        assert 1 <= len(instant) <= 3\n        instant = tuple(instant)\n    elif isinstance(instant, Period):\n        instant = instant.start\n    else:\n        assert isinstance(instant, tuple), instant\n        assert 1 <= len(instant) <= 3\n    if len(instant) == 1:\n        return Instant((instant[0], 1, 1))\n    if len(instant) == 2:\n        return Instant((instant[0], instant[1], 1))\n    return Instant(instant)", "response": "Return a new instant aka a triple of integers year month day."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef period(value):\n    if isinstance(value, Period):\n        return value\n\n    if isinstance(value, Instant):\n        return Period((DAY, value, 1))\n\n    def parse_simple_period(value):\n        \"\"\"\n        Parses simple periods respecting the ISO format, such as 2012 or 2015-03\n        \"\"\"\n        try:\n            date = datetime.datetime.strptime(value, '%Y')\n        except ValueError:\n            try:\n                date = datetime.datetime.strptime(value, '%Y-%m')\n            except ValueError:\n                try:\n                    date = datetime.datetime.strptime(value, '%Y-%m-%d')\n                except ValueError:\n                    return None\n                else:\n                    return Period((DAY, Instant((date.year, date.month, date.day)), 1))\n            else:\n                return Period((MONTH, Instant((date.year, date.month, 1)), 1))\n        else:\n            return Period((YEAR, Instant((date.year, date.month, 1)), 1))\n\n    def raise_error(value):\n        message = linesep.join([\n            \"Expected a period (eg. '2017', '2017-01', '2017-01-01', ...); got: '{}'.\".format(value),\n            \"Learn more about legal period formats in OpenFisca:\",\n            \"<https://openfisca.org/doc/coding-the-legislation/35_periods.html#periods-in-simulations>.\"\n            ])\n        raise ValueError(message)\n\n    if value == 'ETERNITY' or value == ETERNITY:\n        return Period(('eternity', instant(datetime.date.min), float(\"inf\")))\n\n    # check the type\n    if isinstance(value, int):\n        return Period((YEAR, Instant((value, 1, 1)), 1))\n    if not isinstance(value, str):\n        raise_error(value)\n\n    # try to parse as a simple period\n    period = parse_simple_period(value)\n    if period is not None:\n        return period\n\n    # complex period must have a ':' in their strings\n    if \":\" not in value:\n        raise_error(value)\n\n    components = value.split(':')\n\n    # left-most component must be a valid unit\n    unit = components[0]\n    if unit not in (DAY, MONTH, YEAR):\n        raise_error(value)\n\n    # middle component must be a valid iso period\n    base_period = parse_simple_period(components[1])\n    if not base_period:\n        raise_error(value)\n\n    # period like year:2015-03 have a size of 1\n    if len(components) == 2:\n        size = 1\n    # if provided, make sure the size is an integer\n    elif len(components) == 3:\n        try:\n            size = int(components[2])\n        except ValueError:\n            raise_error(value)\n    # if there is more than 2 \":\" in the string, the period is invalid\n    else:\n        raise_error(value)\n\n    # reject ambiguous period such as month:2014\n    if unit_weight(base_period.unit) > unit_weight(unit):\n        raise_error(value)\n\n    return Period((unit, base_period.start, size))", "response": "Return a new period from a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef key_period_size(period):\n\n    unit, start, size = period\n\n    return '{}_{}'.format(unit_weight(unit), size)", "response": "Returns a key for the given period"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef date(self):\n        instant_date = date_by_instant_cache.get(self)\n        if instant_date is None:\n            date_by_instant_cache[self] = instant_date = datetime.date(*self)\n        return instant_date", "response": "Convert instant to a date."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new period starting at the current time.", "response": "def period(self, unit, size = 1):\n        \"\"\"Create a new period starting at instant.\n\n        >>> instant(2014).period('month')\n        Period(('month', Instant((2014, 1, 1)), 1))\n        >>> instant('2014-2').period('year', 2)\n        Period(('year', Instant((2014, 2, 1)), 2))\n        >>> instant('2014-2-3').period('day', size = 2)\n        Period(('day', Instant((2014, 2, 3)), 2))\n        \"\"\"\n        assert unit in (DAY, MONTH, YEAR), 'Invalid unit: {} of type {}'.format(unit, type(unit))\n        assert isinstance(size, int) and size >= 1, 'Invalid size: {} of type {}'.format(size, type(size))\n        return Period((unit, self, size))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nincrementing or decrement the given offset units.", "response": "def offset(self, offset, unit):\n        \"\"\"Increment (or decrement) the given instant with offset units.\n\n        >>> instant(2014).offset(1, 'day')\n        Instant((2014, 1, 2))\n        >>> instant(2014).offset(1, 'month')\n        Instant((2014, 2, 1))\n        >>> instant(2014).offset(1, 'year')\n        Instant((2015, 1, 1))\n\n        >>> instant('2014-1-31').offset(1, 'day')\n        Instant((2014, 2, 1))\n        >>> instant('2014-1-31').offset(1, 'month')\n        Instant((2014, 2, 28))\n        >>> instant('2014-1-31').offset(1, 'year')\n        Instant((2015, 1, 31))\n\n        >>> instant('2011-2-28').offset(1, 'day')\n        Instant((2011, 3, 1))\n        >>> instant('2011-2-28').offset(1, 'month')\n        Instant((2011, 3, 28))\n        >>> instant('2012-2-29').offset(1, 'year')\n        Instant((2013, 2, 28))\n\n        >>> instant(2014).offset(-1, 'day')\n        Instant((2013, 12, 31))\n        >>> instant(2014).offset(-1, 'month')\n        Instant((2013, 12, 1))\n        >>> instant(2014).offset(-1, 'year')\n        Instant((2013, 1, 1))\n\n        >>> instant('2011-3-1').offset(-1, 'day')\n        Instant((2011, 2, 28))\n        >>> instant('2011-3-31').offset(-1, 'month')\n        Instant((2011, 2, 28))\n        >>> instant('2012-2-29').offset(-1, 'year')\n        Instant((2011, 2, 28))\n\n        >>> instant('2014-1-30').offset(3, 'day')\n        Instant((2014, 2, 2))\n        >>> instant('2014-10-2').offset(3, 'month')\n        Instant((2015, 1, 2))\n        >>> instant('2014-1-1').offset(3, 'year')\n        Instant((2017, 1, 1))\n\n        >>> instant(2014).offset(-3, 'day')\n        Instant((2013, 12, 29))\n        >>> instant(2014).offset(-3, 'month')\n        Instant((2013, 10, 1))\n        >>> instant(2014).offset(-3, 'year')\n        Instant((2011, 1, 1))\n\n        >>> instant(2014).offset('first-of', 'month')\n        Instant((2014, 1, 1))\n        >>> instant('2014-2').offset('first-of', 'month')\n        Instant((2014, 2, 1))\n        >>> instant('2014-2-3').offset('first-of', 'month')\n        Instant((2014, 2, 1))\n\n        >>> instant(2014).offset('first-of', 'year')\n        Instant((2014, 1, 1))\n        >>> instant('2014-2').offset('first-of', 'year')\n        Instant((2014, 1, 1))\n        >>> instant('2014-2-3').offset('first-of', 'year')\n        Instant((2014, 1, 1))\n\n        >>> instant(2014).offset('last-of', 'month')\n        Instant((2014, 1, 31))\n        >>> instant('2014-2').offset('last-of', 'month')\n        Instant((2014, 2, 28))\n        >>> instant('2012-2-3').offset('last-of', 'month')\n        Instant((2012, 2, 29))\n\n        >>> instant(2014).offset('last-of', 'year')\n        Instant((2014, 12, 31))\n        >>> instant('2014-2').offset('last-of', 'year')\n        Instant((2014, 12, 31))\n        >>> instant('2014-2-3').offset('last-of', 'year')\n        Instant((2014, 12, 31))\n        \"\"\"\n        year, month, day = self\n        assert unit in (DAY, MONTH, YEAR), 'Invalid unit: {} of type {}'.format(unit, type(unit))\n        if offset == 'first-of':\n            if unit == MONTH:\n                day = 1\n            elif unit == YEAR:\n                month = 1\n                day = 1\n        elif offset == 'last-of':\n            if unit == MONTH:\n                day = calendar.monthrange(year, month)[1]\n            elif unit == YEAR:\n                month = 12\n                day = 31\n        else:\n            assert isinstance(offset, int), 'Invalid offset: {} of type {}'.format(offset, type(offset))\n            if unit == DAY:\n                day += offset\n                if offset < 0:\n                    while day < 1:\n                        month -= 1\n                        if month == 0:\n                            year -= 1\n                            month = 12\n                        day += calendar.monthrange(year, month)[1]\n                elif offset > 0:\n                    month_last_day = calendar.monthrange(year, month)[1]\n                    while day > month_last_day:\n                        month += 1\n                        if month == 13:\n                            year += 1\n                            month = 1\n                        day -= month_last_day\n                        month_last_day = calendar.monthrange(year, month)[1]\n            elif unit == MONTH:\n                month += offset\n                if offset < 0:\n                    while month < 1:\n                        year -= 1\n                        month += 12\n                elif offset > 0:\n                    while month > 12:\n                        year += 1\n                        month -= 12\n                month_last_day = calendar.monthrange(year, month)[1]\n                if day > month_last_day:\n                    day = month_last_day\n            elif unit == YEAR:\n                year += offset\n                # Handle february month of leap year.\n                month_last_day = calendar.monthrange(year, month)[1]\n                if day > month_last_day:\n                    day = month_last_day\n\n        return self.__class__((year, month, day))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_subperiods(self, unit):\n        if unit_weight(self.unit) < unit_weight(unit):\n            raise ValueError('Cannot subdivide {0} into {1}'.format(self.unit, unit))\n\n        if unit == YEAR:\n            return [self.this_year.offset(i, YEAR) for i in range(self.size)]\n\n        if unit == MONTH:\n            return [self.first_month.offset(i, MONTH) for i in range(self.size_in_months)]\n\n        if unit == DAY:\n            return [self.first_day.offset(i, DAY) for i in range(self.size_in_days)]", "response": "Return the list of all the periods of unit contained in self.\n "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef offset(self, offset, unit = None):\n        return self.__class__((self[0], self[1].offset(offset, self[0] if unit is None else unit), self[2]))", "response": "Increment or decrement the given period with offset units."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef contains(self, other):\n        if not isinstance(other, Period):\n            other = period(other)\n        return self.start <= other.start and self.stop >= other.stop", "response": "Returns True if the period contains other."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the size of the period in months.", "response": "def size_in_months(self):\n        \"\"\"Return the size of the period in months.\n\n        >>> period('month', '2012-2-29', 4).size_in_months\n        4\n        >>> period('year', '2012', 1).size_in_months\n        12\n        \"\"\"\n        if (self[0] == MONTH):\n            return self[2]\n        if(self[0] == YEAR):\n            return self[2] * 12\n        raise ValueError(\"Cannot calculate number of months in {0}\".format(self[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the size of the period in days.", "response": "def size_in_days(self):\n        \"\"\"Return the size of the period in days.\n\n        >>> period('month', '2012-2-29', 4).size_in_days\n        28\n        >>> period('year', '2012', 1).size_in_days\n        366\n        \"\"\"\n        unit, instant, length = self\n\n        if unit == DAY:\n            return length\n        if unit in [MONTH, YEAR]:\n            last_day = self.start.offset(length, unit).offset(-1, DAY)\n            return (last_day.date - self.start.date).days + 1\n\n        raise ValueError(\"Cannot calculate number of days in {0}\".format(unit))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the current time - of - day of the current time - of - day.", "response": "def stop(self):\n        \"\"\"Return the last day of the period as an Instant instance.\n\n        >>> period('year', 2014).stop\n        Instant((2014, 12, 31))\n        >>> period('month', 2014).stop\n        Instant((2014, 12, 31))\n        >>> period('day', 2014).stop\n        Instant((2014, 12, 31))\n\n        >>> period('year', '2012-2-29').stop\n        Instant((2013, 2, 28))\n        >>> period('month', '2012-2-29').stop\n        Instant((2012, 3, 28))\n        >>> period('day', '2012-2-29').stop\n        Instant((2012, 2, 29))\n\n        >>> period('year', '2012-2-29', 2).stop\n        Instant((2014, 2, 28))\n        >>> period('month', '2012-2-29', 2).stop\n        Instant((2012, 4, 28))\n        >>> period('day', '2012-2-29', 2).stop\n        Instant((2012, 3, 1))\n        \"\"\"\n        unit, start_instant, size = self\n        year, month, day = start_instant\n        if unit == ETERNITY:\n            return Instant((float(\"inf\"), float(\"inf\"), float(\"inf\")))\n        if unit == 'day':\n            if size > 1:\n                day += size - 1\n                month_last_day = calendar.monthrange(year, month)[1]\n                while day > month_last_day:\n                    month += 1\n                    if month == 13:\n                        year += 1\n                        month = 1\n                    day -= month_last_day\n                    month_last_day = calendar.monthrange(year, month)[1]\n        else:\n            if unit == 'month':\n                month += size\n                while month > 12:\n                    year += 1\n                    month -= 12\n            else:\n                assert unit == 'year', 'Invalid unit: {} of type {}'.format(unit, type(unit))\n                year += size\n            day -= 1\n            if day < 1:\n                month -= 1\n                if month == 0:\n                    year -= 1\n                    month = 12\n                day += calendar.monthrange(year, month)[1]\n            else:\n                month_last_day = calendar.monthrange(year, month)[1]\n                if day > month_last_day:\n                    month += 1\n                    if month == 13:\n                        year += 1\n                        month = 1\n                    day -= month_last_day\n        return Instant((year, month, day))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a new neutralized variable.", "response": "def get_neutralized_variable(variable):\n    \"\"\"\n        Return a new neutralized variable (to be used by reforms).\n        A neutralized variable always returns its default value, and does not cache anything.\n    \"\"\"\n    result = variable.clone()\n    result.is_neutralized = True\n    result.label = '[Neutralized]' if variable.label is None else '[Neutralized] {}'.format(variable.label),\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a formula name into a date.", "response": "def parse_formula_name(self, attribute_name):\n        \"\"\"\n        Returns the starting date of a formula based on its name.\n\n        Valid dated name formats are : 'formula', 'formula_YYYY', 'formula_YYYY_MM' and 'formula_YYYY_MM_DD' where YYYY, MM and DD are a year, month and day.\n\n        By convention, the starting date of:\n            - `formula` is `0001-01-01` (minimal date in Python)\n            - `formula_YYYY` is `YYYY-01-01`\n            - `formula_YYYY_MM` is `YYYY-MM-01`\n        \"\"\"\n\n        def raise_error():\n            raise ValueError(\n                'Unrecognized formula name in variable \"{}\". Expecting \"formula_YYYY\" or \"formula_YYYY_MM\" or \"formula_YYYY_MM_DD where YYYY, MM and DD are year, month and day. Found: \"{}\".'\n                .format(self.name, attribute_name))\n\n        if attribute_name == FORMULA_NAME_PREFIX:\n            return date.min\n\n        FORMULA_REGEX = r'formula_(\\d{4})(?:_(\\d{2}))?(?:_(\\d{2}))?$'  # YYYY or YYYY_MM or YYYY_MM_DD\n\n        match = re.match(FORMULA_REGEX, attribute_name)\n        if not match:\n            raise_error()\n        date_str = '-'.join([match.group(1), match.group(2) or '01', match.group(3) or '01'])\n\n        try:\n            return datetime.datetime.strptime(date_str, '%Y-%m-%d').date()\n        except ValueError:  # formula_2005_99_99 for instance\n            raise_error()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget instrospection data about the code of the variable.", "response": "def get_introspection_data(cls, tax_benefit_system):\n        \"\"\"\n        Get instrospection data about the code of the variable.\n\n        :returns: (comments, source file path, source code, start line number)\n        :rtype: tuple\n\n        \"\"\"\n        comments = inspect.getcomments(cls)\n\n        # Handle dynamically generated variable classes or Jupyter Notebooks, which have no source.\n        try:\n            absolute_file_path = inspect.getsourcefile(cls)\n        except TypeError:\n            source_file_path = None\n        else:\n            source_file_path = absolute_file_path.replace(tax_benefit_system.get_package_metadata()['location'], '')\n        try:\n            source_lines, start_line_number = inspect.getsourcelines(cls)\n            source_code = textwrap.dedent(''.join(source_lines))\n        except (IOError, TypeError):\n            source_code, start_line_number = None, None\n\n        return comments, source_file_path, source_code, start_line_number"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the formula used to compute the variable at the given period.", "response": "def get_formula(self, period = None):\n        \"\"\"\n        Returns the formula used to compute the variable at the given period.\n\n        If no period is given and the variable has several formula, return the oldest formula.\n\n        :returns: Formula used to compute the variable\n        :rtype: function\n        \"\"\"\n\n        if not self.formulas:\n            return None\n\n        if period is None:\n            return self.formulas.peekitem(index = 0)[1]  # peekitem gets the 1st key-value tuple (the oldest start_date and formula). Return the formula.\n\n        if isinstance(period, periods.Period):\n            instant = period.start\n        else:\n            try:\n                instant = periods.period(period).start\n            except ValueError:\n                instant = periods.instant(period)\n\n        if self.end and instant.date > self.end:\n            return None\n\n        instant = str(instant)\n        for start_date in reversed(self.formulas):\n            if start_date <= instant:\n                return self.formulas[start_date]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef has_role(self, role):\n        self.entity.check_role_validity(role)\n        group_population = self.simulation.get_population(role.entity.plural)\n        if role.subroles:\n            return np.logical_or.reduce([group_population.members_role == subrole for subrole in role.subroles])\n        else:\n            return group_population.members_role == role", "response": "Check if a person has a given role within its group entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_rank(self, entity, criteria, condition = True):\n\n        # If entity is for instance 'person.household', we get the reference entity 'household' behind the projector\n        entity = entity if not isinstance(entity, Projector) else entity.reference_entity\n\n        positions = entity.members_position\n        biggest_entity_size = np.max(positions) + 1\n        filtered_criteria = np.where(condition, criteria, np.inf)\n        ids = entity.members_entity_id\n\n        # Matrix: the value in line i and column j is the value of criteria for the jth person of the ith entity\n        matrix = np.asarray([\n            entity.value_nth_person(k, filtered_criteria, default = np.inf)\n            for k in range(biggest_entity_size)\n            ]).transpose()\n\n        # We double-argsort all lines of the matrix.\n        # Double-argsorting gets the rank of each value once sorted\n        # For instance, if x = [3,1,6,4,0], y = np.argsort(x) is [4, 1, 0, 3, 2] (because the value with index 4 is the smallest one, the value with index 1 the second smallest, etc.) and z = np.argsort(y) is [2, 1, 4, 3, 0], the rank of each value.\n        sorted_matrix = np.argsort(np.argsort(matrix))\n\n        # Build the result vector by taking for each person the value in the right line (corresponding to its household id) and the right column (corresponding to its position)\n        result = sorted_matrix[ids, positions]\n\n        # Return -1 for the persons who don't respect the condition\n        return np.where(condition, result, -1)", "response": "Get the rank of a person within an entity according to a criteria."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ordered_members_map(self):\n        if self._ordered_members_map is None:\n            return np.argsort(self.members_entity_id)\n        return self._ordered_members_map", "response": "Mask to group the persons by entityID"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sum(self, array, role = None):\n        self.entity.check_role_validity(role)\n        self.members.check_array_compatible_with_entity(array)\n        if role is not None:\n            role_filter = self.members.has_role(role)\n            return np.bincount(\n                self.members_entity_id[role_filter],\n                weights = array[role_filter],\n                minlength = self.count)\n        else:\n            return np.bincount(self.members_entity_id, weights = array)", "response": "Return the sum of array for the members of the entity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef any(self, array, role = None):\n        sum_in_entity = self.sum(array, role = role)\n        return (sum_in_entity > 0)", "response": "Return True if array is True for any members of the entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef all(self, array, role = None):\n        return self.reduce(array, reducer = np.logical_and, neutral_element = True, role = role)", "response": "Return True if array is True for all members of the entity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the maximum value of array for the entity members.", "response": "def max(self, array, role = None):\n        \"\"\"\n            Return the maximum value of ``array`` for the entity members.\n\n            ``array`` must have the dimension of the number of persons in the simulation\n\n            If ``role`` is provided, only the entity member with the given role are taken into account.\n\n            Example:\n\n            >>> salaries = household.members('salary', '2018-01')  # e.g. [2000, 1500, 0, 0, 0]\n            >>> household.max(salaries)\n            >>> array([2000])\n        \"\"\"\n        return self.reduce(array, reducer = np.maximum, neutral_element = - np.infty, role = role)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef min(self, array, role = None):\n        return self.reduce(array, reducer = np.minimum, neutral_element = np.infty, role = role)", "response": "Return the minimum value of array for the entity members."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nb_persons(self, role = None):\n        if role:\n            if role.subroles:\n                role_condition = np.logical_or.reduce([self.members_role == subrole for subrole in role.subroles])\n            else:\n                role_condition = self.members_role == role\n            return self.sum(role_condition)\n        else:\n            return np.bincount(self.members_entity_id)", "response": "Returns the number of persons contained in the entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the value of array for the person with the unique role role.", "response": "def value_from_person(self, array, role, default = 0):\n        \"\"\"\n            Get the value of ``array`` for the person with the unique role ``role``.\n\n            ``array`` must have the dimension of the number of persons in the simulation\n\n            If such a person does not exist, return ``default`` instead\n\n            The result is a vector which dimension is the number of entities\n        \"\"\"\n        self.entity.check_role_validity(role)\n        if role.max != 1:\n            raise Exception(\n                'You can only use value_from_person with a role that is unique in {}. Role {} is not unique.'\n                .format(self.key, role.key)\n                )\n        self.members.check_array_compatible_with_entity(array)\n        members_map = self.ordered_members_map\n        result = self.filled_array(default, dtype = array.dtype)\n        if isinstance(array, EnumArray):\n            result = EnumArray(result, array.possible_values)\n        role_filter = self.members.has_role(role)\n        entity_filter = self.any(role_filter)\n\n        result[entity_filter] = array[members_map][role_filter[members_map]]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the value of array for the nth person in the entity.", "response": "def value_nth_person(self, n, array, default = 0):\n        \"\"\"\n            Get the value of array for the person whose position in the entity is n.\n\n            Note that this position is arbitrary, and that members are not sorted.\n\n            If the nth person does not exist, return  ``default`` instead.\n\n            The result is a vector which dimension is the number of entities.\n        \"\"\"\n        self.members.check_array_compatible_with_entity(array)\n        positions = self.members_position\n        nb_persons_per_entity = self.nb_persons()\n        members_map = self.ordered_members_map\n        result = self.filled_array(default, dtype = array.dtype)\n        # For households that have at least n persons, set the result as the value of criteria for the person for which the position is n.\n        # The map is needed b/c the order of the nth persons of each household in the persons vector is not necessarily the same than the household order.\n        result[nb_persons_per_entity > n] = array[members_map][positions[members_map] == n]\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the path to the temporary directory used to store intermediate calculation data in case of memory overflow.", "response": "def data_storage_dir(self):\n        \"\"\"\n        Temporary folder used to store intermediate calculation data in case the memory is saturated\n        \"\"\"\n        if self._data_storage_dir is None:\n            self._data_storage_dir = tempfile.mkdtemp(prefix = \"openfisca_\")\n            log.warn((\n                \"Intermediate results will be stored on disk in {} in case of memory overflow. \"\n                \"You should remove this directory once you're done with your simulation.\"\n                ).format(self._data_storage_dir))\n        return self._data_storage_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate(self, variable_name, period, **parameters):\n        population = self.get_variable_population(variable_name)\n        holder = population.get_holder(variable_name)\n        variable = self.tax_benefit_system.get_variable(variable_name, check_existence = True)\n\n        if period is not None and not isinstance(period, periods.Period):\n            period = periods.period(period)\n\n        if self.trace:\n            self.tracer.record_calculation_start(variable.name, period, **parameters)\n\n        self._check_period_consistency(period, variable)\n\n        # First look for a value already cached\n        cached_array = holder.get_array(period)\n        if cached_array is not None:\n            if self.trace:\n                self.tracer.record_calculation_end(variable.name, period, cached_array, **parameters)\n            return cached_array\n\n        array = None\n\n        # First, try to run a formula\n        try:\n            self._check_for_cycle(variable, period)\n            array = self._run_formula(variable, population, period)\n\n            # If no result, use the default value and cache it\n            if array is None:\n                array = holder.default_array()\n\n            array = self._cast_formula_result(array, variable)\n\n            holder.put_in_cache(array, period)\n        except SpiralError:\n            array = holder.default_array()\n        finally:\n            if self.trace:\n                self.tracer.record_calculation_end(variable.name, period, array, **parameters)\n            self._clean_cycle_detection_data(variable.name)\n\n        self.purge_cache_of_invalid_values()\n\n        return array", "response": "Calculates the variable for the period and returns the result of the calculation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the value of a variable using the calculate_output attribute of the variable.", "response": "def calculate_output(self, variable_name, period):\n        \"\"\"\n            Calculate the value of a variable using the ``calculate_output`` attribute of the variable.\n        \"\"\"\n\n        variable = self.tax_benefit_system.get_variable(variable_name, check_existence = True)\n\n        if variable.calculate_output is None:\n            return self.calculate(variable_name, period)\n\n        return variable.calculate_output(self, variable_name, period)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run_formula(self, variable, population, period):\n\n        formula = variable.get_formula(period)\n        if formula is None:\n            return None\n\n        if self.trace:\n            parameters_at = self.trace_parameters_at_instant\n        else:\n            parameters_at = self.tax_benefit_system.get_parameters_at_instant\n\n        if formula.__code__.co_argcount == 2:\n            array = formula(population, period)\n        else:\n            array = formula(population, period, parameters_at)\n\n        self._check_formula_result(array, variable, population, period)\n        return array", "response": "Run the formula for the given period and return the array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that a variable is consistent with the given period.", "response": "def _check_period_consistency(self, period, variable):\n        \"\"\"\n            Check that a period matches the variable definition_period\n        \"\"\"\n        if variable.definition_period == periods.ETERNITY:\n            return  # For variables which values are constant in time, all periods are accepted\n\n        if variable.definition_period == periods.MONTH and period.unit != periods.MONTH:\n            raise ValueError(\"Unable to compute variable '{0}' for period {1}: '{0}' must be computed for a whole month. You can use the ADD option to sum '{0}' over the requested period, or change the requested period to 'period.first_month'.\".format(\n                variable.name,\n                period\n                ))\n\n        if variable.definition_period == periods.YEAR and period.unit != periods.YEAR:\n            raise ValueError(\"Unable to compute variable '{0}' for period {1}: '{0}' must be computed for a whole year. You can use the DIVIDE option to get an estimate of {0} by dividing the yearly value by 12, or change the requested period to 'period.this_year'.\".format(\n                variable.name,\n                period\n                ))\n\n        if period.size != 1:\n            raise ValueError(\"Unable to compute variable '{0}' for period {1}: '{0}' must be computed for a whole {2}. You can use the ADD option to sum '{0}' over the requested period.\".format(\n                variable.name,\n                period,\n                'month' if variable.definition_period == periods.MONTH else 'year'\n                ))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_for_cycle(self, variable, period):\n        previous_periods = [_period for (_name, _period) in self.computation_stack if _name == variable.name]\n        self.computation_stack.append((variable.name, str(period)))\n        if str(period) in previous_periods:\n            raise CycleError(\"Circular definition detected on formula {}@{}\".format(variable.name, period))\n        spiral = len(previous_periods) >= self.max_spiral_loops\n        if spiral:\n            self.invalidate_spiral_variables(variable)\n            message = \"Quasicircular definition detected on formula {}@{} involving {}\".format(variable.name, period, self.computation_stack)\n            raise SpiralError(message, variable.name)", "response": "Raise an exception if a variable is in a cycle."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the value of variable_name for period.", "response": "def get_array(self, variable_name, period):\n        \"\"\"\n            Return the value of ``variable_name`` for ``period``, if this value is alreay in the cache (if it has been set as an input or previously calculated).\n\n            Unlike :any:`calculate`, this method *does not* trigger calculations and *does not* use any formula.\n        \"\"\"\n        if period is not None and not isinstance(period, periods.Period):\n            period = periods.period(period)\n        return self.get_holder(variable_name).get_array(period)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget data about the virtual memory usage of the simulation.", "response": "def get_memory_usage(self, variables = None):\n        \"\"\"\n            Get data about the virtual memory usage of the simulation\n        \"\"\"\n        result = dict(\n            total_nb_bytes = 0,\n            by_variable = {}\n            )\n        for entity in self.populations.values():\n            entity_memory_usage = entity.get_memory_usage(variables = variables)\n            result['total_nb_bytes'] += entity_memory_usage['total_nb_bytes']\n            result['by_variable'].update(entity_memory_usage['by_variable'])\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting a variable s value for a given period", "response": "def set_input(self, variable_name, period, value):\n        \"\"\"\n            Set a variable's value for a given period\n\n            :param variable: the variable to be set\n            :param value: the input value for the variable\n            :param period: the period for which the value is setted\n\n            Example:\n            >>> from openfisca_country_template import CountryTaxBenefitSystem\n            >>> simulation = Simulation(CountryTaxBenefitSystem())\n            >>> simulation.set_input('age', '2018-04', [12, 14])\n            >>> simulation.get_array('age', '2018-04')\n            array([12, 14], dtype=int32)\n\n            If a ``set_input`` property has been set for the variable, this method may accept inputs for periods not matching the ``definition_period`` of the variable. To read more about this, check the `documentation <https://openfisca.org/doc/coding-the-legislation/35_periods.html#automatically-process-variable-inputs-defined-for-periods-not-matching-the-definitionperiod>`_.\n        \"\"\"\n        variable = self.tax_benefit_system.get_variable(variable_name, check_existence = True)\n        period = periods.period(period)\n        if ((variable.end is not None) and (period.start.date > variable.end)):\n            return\n        self.get_holder(variable_name).set_input(period, value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clone(self, debug = False, trace = False):\n        new = empty_clone(self)\n        new_dict = new.__dict__\n\n        for key, value in self.__dict__.items():\n            if key not in ('debug', 'trace', 'tracer'):\n                new_dict[key] = value\n\n        new.persons = self.persons.clone(new)\n        setattr(new, new.persons.entity.key, new.persons)\n        new.populations = {new.persons.entity.key: new.persons}\n\n        for entity in self.tax_benefit_system.group_entities:\n            population = self.populations[entity.key].clone(new)\n            new.populations[entity.key] = population\n            setattr(new, entity.key, population)  # create shortcut simulation.household (for instance)\n\n        new.debug = debug\n        new.trace = trace\n\n        return new", "response": "Clone the simulation just enough to be able to run the copy without modifying the original simulation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone(self, population):\n        new = empty_clone(self)\n        new_dict = new.__dict__\n\n        for key, value in self.__dict__.items():\n            if key not in ('population', 'formula', 'simulation'):\n                new_dict[key] = value\n\n        new_dict['population'] = population\n        new_dict['simulation'] = population.simulation\n\n        return new", "response": "Clone the holder just enough to be able to run a new simulation without modifying the original simulation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete all known values of the variable.", "response": "def delete_arrays(self, period = None):\n        \"\"\"\n            If ``period`` is ``None``, remove all known values of the variable.\n\n            If ``period`` is not ``None``, only remove all values for any period included in period (e.g. if period is \"2017\", values for \"2017-01\", \"2017-07\", etc. would be removed)\n\n        \"\"\"\n\n        self._memory_storage.delete(period)\n        if self._disk_storage:\n            self._disk_storage.delete(period)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_array(self, period):\n        if self.variable.is_neutralized:\n            return self.default_array()\n        value = self._memory_storage.get(period)\n        if value is not None:\n            return value\n        if self._disk_storage:\n            return self._disk_storage.get(period)", "response": "Get the value of the variable for the given period."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_memory_usage(self):\n\n        usage = dict(\n            nb_cells_by_array = self.population.count,\n            dtype = self.variable.dtype,\n            )\n\n        usage.update(self._memory_storage.get_memory_usage())\n\n        if self.simulation.trace:\n            usage_stats = self.simulation.tracer.usage_stats[self.variable.name]\n            usage.update(dict(\n                nb_requests = usage_stats['nb_requests'],\n                nb_requests_by_array = usage_stats['nb_requests'] / float(usage['nb_arrays']) if usage['nb_arrays'] > 0 else np.nan\n                ))\n\n        return usage", "response": "Get data about the virtual memory usage of the holder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_known_periods(self):\n\n        return list(self._memory_storage.get_known_periods()) + list((\n            self._disk_storage.get_known_periods() if self._disk_storage else []))", "response": "Get the list of periods the variable value is known for."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_input(self, period, array):\n\n        period = periods.period(period)\n        if period.unit == ETERNITY and self.variable.definition_period != ETERNITY:\n            error_message = os.linesep.join([\n                'Unable to set a value for variable {0} for ETERNITY.',\n                '{0} is only defined for {1}s. Please adapt your input.',\n                ]).format(\n                    self.variable.name,\n                    self.variable.definition_period\n                )\n            raise PeriodMismatchError(\n                self.variable.name,\n                period,\n                self.variable.definition_period,\n                error_message\n                )\n        if self.variable.is_neutralized:\n            warning_message = \"You cannot set a value for the variable {}, as it has been neutralized. The value you provided ({}) will be ignored.\".format(self.variable.name, array)\n            return warnings.warn(\n                warning_message,\n                Warning\n                )\n        if self.variable.value_type in (float, int) and isinstance(array, str):\n            array = eval_expression(array)\n        if self.variable.set_input:\n            return self.variable.set_input(self, period, array)\n        return self._set(period, array)", "response": "Set a value for a given variable for a given period."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dump_simulation(simulation, directory):\n    parent_directory = os.path.abspath(os.path.join(directory, os.pardir))\n    if not os.path.isdir(parent_directory):  # To deal with reforms\n        os.mkdir(parent_directory)\n    if not os.path.isdir(directory):\n        os.mkdir(directory)\n\n    if os.listdir(directory):\n        raise ValueError(\"Directory '{}' is not empty\".format(directory))\n\n    entities_dump_dir = os.path.join(directory, \"__entities__\")\n    os.mkdir(entities_dump_dir)\n\n    for entity in simulation.populations.values():\n        # Dump entity structure\n        _dump_entity(entity, entities_dump_dir)\n\n        # Dump variable values\n        for holder in entity._holders.values():\n            _dump_holder(holder, directory)", "response": "Dump simulation data to directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef restore_simulation(directory, tax_benefit_system, **kwargs):\n    simulation = Simulation(tax_benefit_system, tax_benefit_system.instantiate_entities())\n\n    entities_dump_dir = os.path.join(directory, \"__entities__\")\n    for population in simulation.populations.values():\n        if population.entity.is_person:\n            continue\n        person_count = _restore_entity(population, entities_dump_dir)\n\n    for population in simulation.populations.values():\n        if not population.entity.is_person:\n            continue\n        _restore_entity(population, entities_dump_dir)\n        population.count = person_count\n\n    variables_to_restore = (variable for variable in os.listdir(directory) if variable != \"__entities__\")\n    for variable in variables_to_restore:\n        _restore_holder(simulation, variable, directory)\n\n    return simulation", "response": "Restore simulation from directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if all forms defined in form_classes are valid.", "response": "def are_forms_valid(self, forms):\n        \"\"\"\n        Check if all forms defined in `form_classes` are valid.\n        \"\"\"\n        for form in six.itervalues(forms):\n            if not form.is_valid():\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the context dictionary.", "response": "def get_context_data(self, **kwargs):\n        \"\"\"\n        Add forms into the context dictionary.\n        \"\"\"\n        context = {}\n        if 'forms' not in kwargs:\n            context['forms'] = self.get_forms()\n        else:\n            context['forms'] = kwargs['forms']\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding the keyword arguments required to instantiate the form.", "response": "def get_form_kwargs(self):\n        \"\"\"\n        Build the keyword arguments required to instantiate the form.\n        \"\"\"\n\n        kwargs = {}\n        for key in six.iterkeys(self.form_classes):\n            if self.request.method in ('POST', 'PUT'):\n                kwargs[key] = {\n                    'data': self.request.POST,\n                    'files': self.request.FILES,\n                }\n            else:\n                kwargs[key] = {}\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a copy of initial with empty initial data dictionaries for each form.", "response": "def get_initial(self):\n        \"\"\"\n        Returns a copy of `initial` with empty initial data dictionaries for each form.\n        \"\"\"\n        initial = super(MultiFormView, self).get_initial()\n        for key in six.iterkeys(self.form_classes):\n            initial[key] = {}\n        return initial"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post(self, request, **kwargs):\n        forms = self.get_forms()\n        if self.are_forms_valid(forms):\n            return self.forms_valid(forms)\n        else:\n            return self.forms_invalid(forms)", "response": "Uses are_forms_valid to call either forms_valid or forms_invalid to call either forms_valid or forms_invalid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves each form and call save on each form.", "response": "def forms_valid(self, forms):\n        \"\"\"\n        Calls `save()` on each form.\n        \"\"\"\n        for form in six.itervalues(forms):\n            form.save()\n        return super(MultiModelFormView, self).forms_valid(forms)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of all the forms defined in self. form_classes.", "response": "def get_forms(self):\n        \"\"\"\n        Initializes the forms defined in `form_classes` with initial data from `get_initial()`,\n        kwargs from get_form_kwargs() and form instance object from `get_objects()`.\n        \"\"\"\n        forms = {}\n        objects = self.get_objects()\n        initial = self.get_initial()\n        form_kwargs = self.get_form_kwargs()\n        for key, form_class in six.iteritems(self.form_classes):\n            forms[key] = form_class(instance=objects[key], initial=initial[key], **form_kwargs[key])\n        return forms"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary with the instance objects for each form. Keys should match the corresponding form. Values should match the corresponding form. Values should match the corresponding form.", "response": "def get_objects(self):\n        \"\"\"\n        Returns dictionary with the instance objects for each form. Keys should match the\n        corresponding form.\n        \"\"\"\n        objects = {}\n        for key in six.iterkeys(self.form_classes):\n            objects[key] = None\n        return objects"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ratio_split(amount, ratios):\n    ratio_total = sum(ratios)\n    divided_value = amount / ratio_total\n    values = []\n    for ratio in ratios:\n        value = divided_value * ratio\n        values.append(value)\n\n    # Now round the values, keeping track of the bits we cut off\n    rounded = [v.quantize(Decimal(\"0.01\")) for v in values]\n    remainders = [v - rounded[i] for i, v in enumerate(values)]\n    remainder = sum(remainders)\n    # Give the last person the (positive or negative) remainder\n    rounded[-1] = (rounded[-1] + remainder).quantize(Decimal(\"0.01\"))\n\n    assert sum(rounded) == amount\n\n    return rounded", "response": "Split in_value according to the ratios specified in ratios."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_columns(self):\n        reader = self._get_csv_reader()\n        headings = six.next(reader)\n        try:\n            examples = six.next(reader)\n        except StopIteration:\n            examples = []\n\n        found_fields = set()\n        for i, value in enumerate(headings):\n            if i >= 20:\n                break\n\n            infer_field = self.has_headings and value not in found_fields\n\n            to_field = (\n                {\n                    \"date\": \"date\",\n                    \"amount\": \"amount\",\n                    \"description\": \"description\",\n                    \"memo\": \"description\",\n                    \"notes\": \"description\",\n                }.get(value.lower(), \"\")\n                if infer_field\n                else \"\"\n            )\n\n            if to_field:\n                found_fields.add(to_field)\n\n            TransactionCsvImportColumn.objects.update_or_create(\n                transaction_import=self,\n                column_number=i + 1,\n                column_heading=value if self.has_headings else \"\",\n                to_field=to_field,\n                example=examples[i].strip() if examples else \"\",\n            )", "response": "For each column in file create a TransactionCsvImportColumn"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_num_similar_objects(self, obj):\n        return StatementLine.objects.filter(\n            date=obj.date, amount=obj.amount, description=obj.description\n        ).count()", "response": "Get any statement lines which would be considered a duplicate of obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_num_similar_rows(self, row, until=None):\n        return len(list(filter(lambda r: row == r, self.dataset[:until])))", "response": "Get the number of rows similar to row which precede the index until"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nimport data from teller. io account_uuid bank_account Returns the created StatementImport", "response": "def do_import(token, account_uuid, bank_account, since=None):\n    \"\"\"Import data from teller.io\n\n    Returns the created StatementImport\n    \"\"\"\n    response = requests.get(\n        url=\"https://api.teller.io/accounts/{}/transactions\".format(account_uuid),\n        headers={\"Authorization\": \"Bearer {}\".format(token)},\n    )\n    response.raise_for_status()\n    data = response.json()\n\n    statement_import = StatementImport.objects.create(\n        source=\"teller.io\", extra={\"account_uuid\": account_uuid}, bank_account=bank_account\n    )\n\n    for line_data in data:\n        uuid = UUID(hex=line_data[\"id\"])\n        if StatementLine.objects.filter(uuid=uuid):\n            continue\n\n        description = \", \".join(filter(bool, [line_data[\"counterparty\"], line_data[\"description\"]]))\n        date = datetime.date(*map(int, line_data[\"date\"].split(\"-\")))\n\n        if not since or date >= since:\n            StatementLine.objects.create(\n                uuid=uuid,\n                date=line_data[\"date\"],\n                statement_import=statement_import,\n                amount=line_data[\"amount\"],\n                type=line_data[\"type\"],\n                description=description,\n                source_data=line_data,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks that all accounts sum to 0", "response": "def validate_accounting_equation(cls):\n        \"\"\"Check that all accounts sum to 0\"\"\"\n        balances = [account.balance(raw=True) for account in Account.objects.root_nodes()]\n        if sum(balances, Balance()) != 0:\n            raise exceptions.AccountingEquationViolationError(\n                \"Account balances do not sum to zero. They sum to {}\".format(sum(balances))\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sign(self):\n        return -1 if self.type in (Account.TYPES.asset, Account.TYPES.expense) else 1", "response": "Returns 1 if a credit should increase the value of the\n        account or - 1 if a credit should decrease the value of the\n        account or - 1 if a credit should decrease the value of the\n        account."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef balance(self, as_of=None, raw=False, leg_query=None, **kwargs):\n        balances = [\n            account.simple_balance(as_of=as_of, raw=raw, leg_query=leg_query, **kwargs)\n            for account in self.get_descendants(include_self=True)\n        ]\n        return sum(balances, Balance())", "response": "Get the balance for this account including all child accounts including this account."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the balance for this account ignoring all child accounts", "response": "def simple_balance(self, as_of=None, raw=False, leg_query=None, **kwargs):\n        \"\"\"Get the balance for this account, ignoring all child accounts\n\n        Args:\n            as_of (Date): Only include transactions on or before this date\n            raw (bool): If true the returned balance should not have its sign\n                        adjusted for display purposes.\n            leg_query (models.Q): Django Q-expression, will be used to filter the transaction legs.\n                                  allows for more complex filtering than that provided by **kwargs.\n            kwargs (dict): Will be used to filter the transaction legs\n\n        Returns:\n            Balance\n        \"\"\"\n        legs = self.legs\n        if as_of:\n            legs = legs.filter(transaction__date__lte=as_of)\n\n        if leg_query or kwargs:\n            leg_query = leg_query or models.Q()\n            legs = legs.filter(leg_query, **kwargs)\n\n        return legs.sum_to_balance() * (1 if raw else self.sign) + self._zero_balance()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a transaction which transfers amount to to_account", "response": "def transfer_to(self, to_account, amount, **transaction_kwargs):\n        \"\"\"Create a transaction which transfers amount to to_account\n\n        This is a shortcut utility method which simplifies the process of\n        transferring between accounts.\n\n        This method attempts to perform the transaction in an intuitive manner.\n        For example:\n\n          * Transferring income -> income will result in the former decreasing and the latter increasing\n          * Transferring asset (i.e. bank) -> income will result in the balance of both increasing\n          * Transferring asset -> asset will result in the former decreasing and the latter increasing\n\n        .. note::\n\n            Transfers in any direction between ``{asset | expense} <-> {income | liability | equity}``\n            will always result in both balances increasing. This may change in future if it is\n            found to be unhelpful.\n\n            Transfers to trading accounts will always behave as normal.\n\n        Args:\n\n            to_account (Account): The destination account.\n            amount (Money): The amount to be transferred.\n            transaction_kwargs: Passed through to transaction creation. Useful for setting the\n                transaction `description` field.\n        \"\"\"\n        if not isinstance(amount, Money):\n            raise TypeError(\"amount must be of type Money\")\n\n        if to_account.sign == 1 and to_account.type != self.TYPES.trading:\n            # Transferring from two positive-signed accounts implies that\n            # the caller wants to reduce the first account and increase the second\n            # (which is opposite to the implicit behaviour)\n            direction = -1\n        elif self.type == self.TYPES.liability and to_account.type == self.TYPES.expense:\n            # Transfers from liability -> asset accounts should reduce both.\n            # For example, moving money from Rent Payable (liability) to your Rent (expense) account\n            # should use the funds you've built up in the liability account to pay off the expense account.\n            direction = -1\n        else:\n            direction = 1\n\n        transaction = Transaction.objects.create(**transaction_kwargs)\n        Leg.objects.create(transaction=transaction, account=self, amount=+amount * direction)\n        Leg.objects.create(transaction=transaction, account=to_account, amount=-amount * direction)\n        return transaction"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsum the Legs of the QuerySet to get a Balance object", "response": "def sum_to_balance(self):\n        \"\"\"Sum the Legs of the QuerySet to get a `Balance`_ object\n        \"\"\"\n        result = self.values(\"amount_currency\").annotate(total=models.Sum(\"amount\"))\n        return Balance([Money(r[\"total\"], r[\"amount_currency\"]) for r in result])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the balance of the account associated with this leg following the transaction", "response": "def account_balance_after(self):\n        \"\"\"Get the balance of the account associated with this leg following the transaction\"\"\"\n        # TODO: Consider moving to annotation, particularly once we can count on Django 1.11's subquery support\n        transaction_date = self.transaction.date\n        return self.account.balance(\n            leg_query=(\n                models.Q(transaction__date__lt=transaction_date)\n                | (\n                    models.Q(transaction__date=transaction_date)\n                    & models.Q(transaction_id__lte=self.transaction_id)\n                )\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef account_balance_before(self):\n        # TODO: Consider moving to annotation, particularly once we can count on Django 1.11's subquery support\n        transaction_date = self.transaction.date\n        return self.account.balance(\n            leg_query=(\n                models.Q(transaction__date__lt=transaction_date)\n                | (\n                    models.Q(transaction__date=transaction_date)\n                    & models.Q(transaction_id__lt=self.transaction_id)\n                )\n            )\n        )", "response": "Get the balance of the account associated with this leg before the transaction"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_transaction(self, to_account):\n        from_account = self.statement_import.bank_account\n\n        transaction = Transaction.objects.create()\n        Leg.objects.create(\n            transaction=transaction, account=from_account, amount=+(self.amount * -1)\n        )\n        Leg.objects.create(transaction=transaction, account=to_account, amount=-(self.amount * -1))\n\n        transaction.date = self.date\n        transaction.save()\n\n        self.transaction = transaction\n        self.save()\n        return transaction", "response": "Create a transaction for this statement amount and account into to_account\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef currency_exchange(\n    source,\n    source_amount,\n    destination,\n    destination_amount,\n    trading_account,\n    fee_destination=None,\n    fee_amount=None,\n    date=None,\n    description=None,\n):\n    \"\"\" Exchange funds from one currency to another\n\n    Use this method to represent a real world currency transfer. Note this\n    process doesn't care about exchange rates, only about the value\n    of currency going in and out of the transaction.\n\n    You can also record any exchange fees by syphoning off funds to ``fee_account`` of amount ``fee_amount``. Note\n    that the free currency must be the same as the source currency.\n\n    Examples:\n\n        For example, imagine our Canadian bank has obligingly transferred 120 CAD into our US bank account.\n        We sent CAD 120, and received USD 100. We were also changed 1.50 CAD in fees.\n\n        We can represent this exchange in Hordak as follows::\n\n            from hordak.utilities.currency import currency_exchange\n\n            currency_exchange(\n                # Source account and amount\n                source=cad_cash,\n                source_amount=Money(120, 'CAD'),\n                # Destination account and amount\n                destination=usd_cash,\n                destination_amount=Money(100, 'USD'),\n                # Trading account the exchange will be done through\n                trading_account=trading,\n                # We also incur some fees\n                fee_destination=banking_fees,\n                fee_amount=Money(1.50, 'CAD')\n            )\n\n        We should now find that:\n\n         1. ``cad_cash.balance()`` has decreased by ``CAD 120``\n         2. ``usd_cash.balance()`` has increased by ``USD 100``\n         3. ``banking_fees.balance()`` is ``CAD 1.50``\n         4. ``trading_account.balance()`` is ``USD 100, CAD -120``\n\n        You can perform ``trading_account.normalise()`` to discover your unrealised gains/losses\n        on currency traded through that account.\n\n    Args:\n        source (Account): The account the funds will be taken from\n        source_amount (Money): A ``Money`` instance containing the inbound amount and currency.\n        destination (Account): The account the funds will be placed into\n        destination_amount (Money): A ``Money`` instance containing the outbound amount and currency\n        trading_account (Account): The trading account to be used. The normalised balance of this account will indicate\n            gains/losses you have made as part of your activity via this account. Note that the normalised balance\n            fluctuates with the current exchange rate.\n        fee_destination (Account): Your exchange may incur fees. Specifying this will move incurred fees\n            into this account (optional).\n        fee_amount (Money): The amount and currency of any incurred fees (optional).\n        description (str): Description for the transaction. Will default to describing funds in/out & fees (optional).\n        date (datetime.date): The date on which the transaction took place. Defaults to today (optional).\n\n    Returns:\n        (Transaction): The transaction created\n\n    See Also:\n        You can see the above example in practice in ``CurrencyExchangeTestCase.test_fees`` in `test_currency.py`_.\n\n    .. _test_currency.py: https://github.com/adamcharnock/django-hordak/blob/master/hordak/tests/utilities/test_currency.py\n    \"\"\"\n    from hordak.models import Account, Transaction, Leg\n\n    if trading_account.type != Account.TYPES.trading:\n        raise TradingAccountRequiredError(\n            \"Account {} must be a trading account\".format(trading_account)\n        )\n\n    if (fee_destination or fee_amount) and not (fee_destination and fee_amount):\n        raise RuntimeError(\n            \"You must specify either neither or both fee_destination and fee_amount.\"\n        )\n\n    if fee_amount is None:\n        # If fees are not specified then set fee_amount to be zero\n        fee_amount = Money(0, source_amount.currency)\n    else:\n        # If we do have fees then make sure the fee currency matches the source currency\n        if fee_amount.currency != source_amount.currency:\n            raise InvalidFeeCurrency(\n                \"Fee amount currency ({}) must match source amount currency ({})\".format(\n                    fee_amount.currency, source_amount.currency\n                )\n            )\n\n    # Checks over and done now. Let's create the transaction\n    with db_transaction.atomic():\n        transaction = Transaction.objects.create(\n            date=date or datetime.date.today(),\n            description=description\n            or \"Exchange of {} to {}, incurring {} fees\".format(\n                source_amount, destination_amount, \"no\" if fee_amount is None else fee_amount\n            ),\n        )\n\n        # Source currency into trading account\n        Leg.objects.create(transaction=transaction, account=source, amount=source_amount)\n        Leg.objects.create(\n            transaction=transaction, account=trading_account, amount=-(source_amount - fee_amount)\n        )\n\n        # Any fees\n        if fee_amount and fee_destination:\n            Leg.objects.create(\n                transaction=transaction,\n                account=fee_destination,\n                amount=-fee_amount,\n                description=\"Fees\",\n            )\n\n        # Destination currency out of trading account\n        Leg.objects.create(\n            transaction=transaction, account=trading_account, amount=destination_amount\n        )\n        Leg.objects.create(transaction=transaction, account=destination, amount=-destination_amount)\n\n    return transaction", "response": "This method is used to represent a real world currency transfer from one currency to another currency."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncaches a rate for future use.", "response": "def cache_rate(self, currency, date, rate):\n        \"\"\"\n        Cache a rate for future use\n        \"\"\"\n        if not self.is_supported(defaults.INTERNAL_CURRENCY):\n            logger.info('Tried to cache unsupported currency \"{}\". Ignoring.'.format(currency))\n        else:\n            cache.set(_cache_key(currency, date), str(rate), _cache_timeout(date))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the exchange rate for currency against date", "response": "def get_rate(self, currency, date):\n        \"\"\"Get the exchange rate for ``currency`` against ``_INTERNAL_CURRENCY``\n\n        If implementing your own backend, you should probably override :meth:`_get_rate()`\n        rather than this.\n        \"\"\"\n        if str(currency) == defaults.INTERNAL_CURRENCY:\n            return Decimal(1)\n\n        cached = cache.get(_cache_key(currency, date))\n        if cached:\n            return Decimal(cached)\n        else:\n            # Expect self._get_rate() to implement caching\n            return Decimal(self._get_rate(currency, date))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the given money to to_currency using exchange rate on date.", "response": "def convert(self, money, to_currency, date=None):\n        \"\"\"Convert the given ``money`` to ``to_currency`` using exchange rate on ``date``\n\n        If ``date`` is omitted then the date given by ``money.date`` will be used.\n        \"\"\"\n        if str(money.currency) == str(to_currency):\n            return copy.copy(money)\n        return Money(\n            amount=money.amount\n            * self.rate(money.currency, to_currency, date or datetime.date.today()),\n            currency=to_currency,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the exchange rate between the specified currencies", "response": "def rate(self, from_currency, to_currency, date):\n        \"\"\"Get the exchange rate between the specified currencies\"\"\"\n        return (1 / self.backend.get_rate(from_currency, date)) * self.backend.get_rate(\n            to_currency, date\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef currencies(self):\n        return [m.currency.code for m in self.monies() if m.amount]", "response": "Get all currencies with non - zero values"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nnormalising this balance into a single currency", "response": "def normalise(self, to_currency):\n        \"\"\"Normalise this balance into a single currency\n\n        Args:\n            to_currency (str): Destination currency\n\n        Returns:\n            (Balance): A new balance object containing a single Money value in the specified currency\n        \"\"\"\n        out = Money(currency=to_currency)\n        for money in self._money_obs:\n            out += converter.convert(money, to_currency)\n        return Balance([out])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn only the active hits.", "response": "def filter_active(self, *args, **kwargs):\n        \"\"\"\n        Return only the 'active' hits.\n\n        How you count a hit/view will depend on personal choice: Should the\n        same user/visitor *ever* be counted twice?  After a week, or a month,\n        or a year, should their view be counted again?\n\n        The defaulf is to consider a visitor's hit still 'active' if they\n        return within a the last seven days..  After that the hit\n        will be counted again.  So if one person visits once a week for a year,\n        they will add 52 hits to a given object.\n\n        Change how long the expiration is by adding to settings.py:\n\n        HITCOUNT_KEEP_HIT_ACTIVE  = {'days' : 30, 'minutes' : 30}\n\n        Accepts days, seconds, microseconds, milliseconds, minutes,\n        hours, and weeks.  It's creating a datetime.timedelta object.\n\n        \"\"\"\n        grace = getattr(settings, 'HITCOUNT_KEEP_HIT_ACTIVE', {'days': 7})\n        period = timezone.now() - timedelta(**grace)\n        return self.filter(created__gte=period).filter(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_ip(request):\n\n    # if neither header contain a value, just use local loopback\n    ip_address = request.META.get('HTTP_X_FORWARDED_FOR',\n                                  request.META.get('REMOTE_ADDR', '127.0.0.1'))\n    if ip_address:\n        # make sure we have one and only one IP\n        try:\n            ip_address = IP_RE.match(ip_address)\n            if ip_address:\n                ip_address = ip_address.group(0)\n            else:\n                # no IP, probably from some dirty proxy or other device\n                # throw in some bogus IP\n                ip_address = '10.0.0.1'\n        except IndexError:\n            pass\n\n    return ip_address", "response": "Retrieves the remote IP address from the request data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_hit_count(request, hitcount):\n    warnings.warn(\n        \"hitcount.views._update_hit_count is deprecated. \"\n        \"Use hitcount.views.HitCountMixin.hit_count() instead.\",\n        RemovedInHitCount13Warning\n    )\n    return HitCountMixin.hit_count(request, hitcount)", "response": "Deprecated in 1. 2. Use hitcount. views. HitCountMixin. hit_count instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_hit_count_ajax(request, *args, **kwargs):\n    warnings.warn(\n        \"hitcount.views.update_hit_count_ajax is deprecated. \"\n        \"Use hitcount.views.HitCountJSONView instead.\",\n        RemovedInHitCount13Warning\n    )\n    view = HitCountJSONView.as_view()\n    return view(request, *args, **kwargs)", "response": "Deprecated in 1. 2. Use hitcount. views. HitCountJSONView instead."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling with a HttpRequest and HitCount object it will return a namedtuple: UpdateHitCountResponse(hit_counted=Boolean, hit_message='Message'). `hit_counted` will be True if the hit was counted and False if it was not. `'hit_message` will indicate by what means the Hit was either counted or ignored.", "response": "def hit_count(self, request, hitcount):\n        \"\"\"\n        Called with a HttpRequest and HitCount object it will return a\n        namedtuple:\n\n        UpdateHitCountResponse(hit_counted=Boolean, hit_message='Message').\n\n        `hit_counted` will be True if the hit was counted and False if it was\n        not.  `'hit_message` will indicate by what means the Hit was either\n        counted or ignored.\n        \"\"\"\n        UpdateHitCountResponse = namedtuple(\n            'UpdateHitCountResponse', 'hit_counted hit_message')\n\n        # as of Django 1.8.4 empty sessions are not being saved\n        # https://code.djangoproject.com/ticket/25489\n        if request.session.session_key is None:\n            request.session.save()\n\n        user = request.user\n        try:\n            is_authenticated_user = user.is_authenticated()\n        except:\n            is_authenticated_user = user.is_authenticated\n        session_key = request.session.session_key\n        ip = get_ip(request)\n        user_agent = request.META.get('HTTP_USER_AGENT', '')[:255]\n        hits_per_ip_limit = getattr(settings, 'HITCOUNT_HITS_PER_IP_LIMIT', 0)\n        exclude_user_group = getattr(settings, 'HITCOUNT_EXCLUDE_USER_GROUP', None)\n\n        # first, check our request against the IP blacklist\n        if BlacklistIP.objects.filter(ip__exact=ip):\n            return UpdateHitCountResponse(\n                False, 'Not counted: user IP has been blacklisted')\n\n        # second, check our request against the user agent blacklist\n        if BlacklistUserAgent.objects.filter(user_agent__exact=user_agent):\n            return UpdateHitCountResponse(\n                False, 'Not counted: user agent has been blacklisted')\n\n        # third, see if we are excluding a specific user group or not\n        if exclude_user_group and is_authenticated_user:\n            if user.groups.filter(name__in=exclude_user_group):\n                return UpdateHitCountResponse(\n                    False, 'Not counted: user excluded by group')\n\n        # eliminated first three possible exclusions, now on to checking our database of\n        # active hits to see if we should count another one\n\n        # start with a fresh active query set (HITCOUNT_KEEP_HIT_ACTIVE)\n        qs = Hit.objects.filter_active()\n\n        # check limit on hits from a unique ip address (HITCOUNT_HITS_PER_IP_LIMIT)\n        if hits_per_ip_limit:\n            if qs.filter(ip__exact=ip).count() >= hits_per_ip_limit:\n                return UpdateHitCountResponse(\n                    False, 'Not counted: hits per IP address limit reached')\n\n        # create a generic Hit object with request data\n        hit = Hit(session=session_key, hitcount=hitcount, ip=get_ip(request),\n                  user_agent=request.META.get('HTTP_USER_AGENT', '')[:255],)\n\n        # first, use a user's authentication to see if they made an earlier hit\n        if is_authenticated_user:\n            if not qs.filter(user=user, hitcount=hitcount):\n                hit.user = user  # associate this hit with a user\n                hit.save()\n\n                response = UpdateHitCountResponse(\n                    True, 'Hit counted: user authentication')\n            else:\n                response = UpdateHitCountResponse(\n                    False, 'Not counted: authenticated user has active hit')\n\n        # if not authenticated, see if we have a repeat session\n        else:\n            if not qs.filter(session=session_key, hitcount=hitcount):\n                hit.save()\n                response = UpdateHitCountResponse(\n                    True, 'Hit counted: session key')\n            else:\n                response = UpdateHitCountResponse(\n                    False, 'Not counted: session key has active hit')\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a string such as days = 1 seconds = 30 and returns a dictionary with the key value pairs as keys and values as values.", "response": "def return_period_from_string(arg):\n    \"\"\"\n    Takes a string such as \"days=1,seconds=30\" and strips the quotes\n    and returns a dictionary with the key/value pairs\n\n    \"\"\"\n    period = {}\n\n    if arg[0] == '\"' and arg[-1] == '\"':\n        opt = arg[1:-1]  # remove quotes\n    else:\n        opt = arg\n\n    for o in opt.split(\",\"):\n        key, value = o.split(\"=\")\n        period[str(key)] = int(value)\n\n    return period"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _start(self):\n        params = self._translate(self._options)\n        self._resp = self._r_session.get(self._url, params=params, stream=True)\n        self._resp.raise_for_status()\n        self._lines = self._resp.iter_lines(self._chunk_size)", "response": "Starts streaming the feed using the provided session and feed options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _translate(self, options):\n        translation = dict()\n        for key, val in iteritems_(options):\n            self._validate(key, val, feed_arg_types(self._source))\n            try:\n                if isinstance(val, STRTYPE):\n                    translation[key] = val\n                elif not isinstance(val, NONETYPE):\n                    arg_converter = TYPE_CONVERTERS.get(type(val), json.dumps)\n                    translation[key] = arg_converter(val)\n            except Exception as ex:\n                raise CloudantArgumentError(115, key, ex)\n        return translation", "response": "Perform translation of feed options passed in as keyword\n        arguments to CouchDB / Cloudant equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate(self, key, val, arg_types):\n        if key in arg_types:\n            arg_type = arg_types[key]\n        else:\n            if ANY_ARG not in arg_types:\n                raise CloudantArgumentError(116, key)\n            arg_type = arg_types[ANY_ARG]\n\n        if arg_type == ANY_TYPE:\n            return\n        if (not isinstance(val, arg_type) or\n                (isinstance(val, bool) and int in arg_type)):\n            raise CloudantArgumentError(117, key, arg_type)\n        if isinstance(val, int) and val < 0 and not isinstance(val, bool):\n            raise CloudantArgumentError(118, key, val)\n        if key == 'feed':\n            valid_vals = ('continuous', 'normal', 'longpoll')\n            if self._source == 'CouchDB':\n                valid_vals = ('continuous', 'longpoll')\n            if val not in valid_vals:\n                raise CloudantArgumentError(119, val, valid_vals)\n        if key == 'style' and val not in ('main_only', 'all_docs'):\n            raise CloudantArgumentError(120, val)", "response": "Validates that the key and value are valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the iteration by pulling the next line from the stream and processing the data.", "response": "def next(self):\n        \"\"\"\n        Handles the iteration by pulling the next line out of the stream,\n        attempting to convert the response to JSON if necessary.\n\n        :returns: Data representing what was seen in the feed\n        \"\"\"\n        while True:\n            if not self._resp:\n                self._start()\n            if self._stop:\n                raise StopIteration\n            skip, data = self._process_data(next_(self._lines))\n            if not skip:\n                break\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_data(self, line):\n        skip = False\n        if self._raw_data:\n            return skip, line\n        line = unicode_(line)\n        if not line:\n            if (self._options.get('heartbeat', False) and\n                    self._options.get('feed') in ('continuous', 'longpoll') and\n                    not self._last_seq):\n                line = None\n            else:\n                skip = True\n        elif line in ('{\"results\":[', '],'):\n            skip = True\n        elif line[-1] == ',':\n            line = line[:-1]\n        elif line[:10] == ('\"last_seq\"'):\n            line = '{' + line\n        try:\n            if line:\n                data = json.loads(line)\n                if data.get('last_seq'):\n                    self._last_seq = data['last_seq']\n                    skip = True\n            else:\n                data = None\n        except ValueError:\n            data = {\"error\": \"Bad JSON line\", \"line\": line}\n        return skip, data", "response": "Validates and processes the line passed in and converts it to a\n        Python object if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the key and value are valid.", "response": "def _validate(self, key, val, arg_types):\n        \"\"\"\n        Ensures that the key and the value are valid arguments to be used with\n        the feed.\n        \"\"\"\n        if key == 'feed' and val != 'continuous':\n            raise CloudantArgumentError(121, val)\n        super(InfiniteFeed, self)._validate(key, val, arg_types)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling the iteration by pulling the next line from the stream and converting the response to JSON.", "response": "def next(self):\n        \"\"\"\n        Handles the iteration by pulling the next line out of the stream and\n        converting the response to JSON.\n\n        :returns: Data representing what was seen in the feed\n        \"\"\"\n        while True:\n            if self._source == 'CouchDB':\n                raise CloudantFeedException(101)\n            if self._last_seq:\n                self._options.update({'since': self._last_seq})\n                self._resp = None\n                self._last_seq = None\n            if not self._resp:\n                self._start()\n            if self._stop:\n                raise StopIteration\n            skip, data = self._process_data(next_(self._lines))\n            if not skip:\n                break\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef features(self):\n        if self._features is None:\n            metadata = self.metadata()\n            if \"features\" in metadata:\n                self._features = metadata[\"features\"]\n            else:\n                self._features = []\n        return self._features", "response": "lazy fetch and cache features\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect(self):\n        if self.r_session:\n            self.session_logout()\n\n        if self.admin_party:\n            self._use_iam = False\n            self.r_session = ClientSession(\n                timeout=self._timeout\n            )\n        elif self._use_basic_auth:\n            self._use_iam = False\n            self.r_session = BasicSession(\n                self._user,\n                self._auth_token,\n                self.server_url,\n                timeout=self._timeout\n            )\n        elif self._use_iam:\n            self.r_session = IAMSession(\n                self._auth_token,\n                self.server_url,\n                auto_renew=self._auto_renew,\n                client_id=self._iam_client_id,\n                client_secret=self._iam_client_secret,\n                timeout=self._timeout\n            )\n        else:\n            self.r_session = CookieSession(\n                self._user,\n                self._auth_token,\n                self.server_url,\n                auto_renew=self._auto_renew,\n                timeout=self._timeout\n            )\n\n        # If a Transport Adapter was supplied add it to the session\n        if self.adapter is not None:\n            self.r_session.mount(self.server_url, self.adapter)\n        if self._client_user_header is not None:\n            self.r_session.headers.update(self._client_user_header)\n\n        self.session_login()\n\n        # Utilize an event hook to append to the response message\n        # using :func:`~cloudant.common_util.append_response_error_content`\n        self.r_session.hooks['response'].append(append_response_error_content)", "response": "Connects to the Cloudant API."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndisconnects from the server.", "response": "def disconnect(self):\n        \"\"\"\n        Ends a client authentication session, performs a logout and a clean up.\n        \"\"\"\n        if self.r_session:\n            self.session_logout()\n\n        self.r_session = None\n        self.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef session_login(self, user=None, passwd=None):\n        self.change_credentials(user=user, auth_token=passwd)", "response": "Performs a session login by posting the auth information\n        to the _session endpoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef change_credentials(self, user=None, auth_token=None):\n        self.r_session.set_credentials(user, auth_token)\n        self.r_session.login()", "response": "Change login credentials.\n\n        :param str user: Username used to connect to server.\n        :param str auth_token: Authentication token used to connect to server."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve a list of all database names for the current client.", "response": "def all_dbs(self):\n        \"\"\"\n        Retrieves a list of all database names for the current client.\n\n        :returns: List of database names for the client\n        \"\"\"\n        url = '/'.join((self.server_url, '_all_dbs'))\n        resp = self.r_session.get(url)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new database on the remote server with the specified name and adds it to the locally cached dictionary.", "response": "def create_database(self, dbname, partitioned=False, **kwargs):\n        \"\"\"\n        Creates a new database on the remote server with the name provided\n        and adds the new database object to the client's locally cached\n        dictionary before returning it to the caller.  The method will\n        optionally throw a CloudantClientException if the database\n        exists remotely.\n\n        :param str dbname: Name used to create the database.\n        :param bool throw_on_exists: Boolean flag dictating whether or\n            not to throw a CloudantClientException when attempting to\n            create a database that already exists.\n        :param bool partitioned: Create as a partitioned database. Defaults to\n            ``False``.\n\n        :returns: The newly created database object\n        \"\"\"\n        new_db = self._DATABASE_CLASS(self, dbname, partitioned=partitioned)\n        try:\n            new_db.create(kwargs.get('throw_on_exists', False))\n        except CloudantDatabaseException as ex:\n            if ex.status_code == 412:\n                raise CloudantClientException(412, dbname)\n        super(CouchDB, self).__setitem__(dbname, new_db)\n        return new_db"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_database(self, dbname):\n        db = self._DATABASE_CLASS(self, dbname)\n        if not db.exists():\n            raise CloudantClientException(404, dbname)\n        db.delete()\n        if dbname in list(self.keys()):\n            super(CouchDB, self).__delitem__(dbname)", "response": "Removes the named database remotely and locally."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves the remote server metadata dictionary.", "response": "def metadata(self):\n        \"\"\"\n        Retrieves the remote server metadata dictionary.\n\n        :returns: Dictionary containing server metadata details\n        \"\"\"\n        resp = self.r_session.get(self.server_url)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keys(self, remote=False):\n        if not remote:\n            return list(super(CouchDB, self).keys())\n        return self.all_dbs()", "response": "Returns the list of database names for this client."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\noverrides dictionary get behavior to retrieve database objects with the specified key.", "response": "def get(self, key, default=None, remote=False):\n        \"\"\"\n        Overrides dictionary get behavior to retrieve database objects with\n        support for returning a default.  If remote=True then a remote\n        request is made to retrieve the database from the remote server,\n        otherwise the client's locally cached database object is returned.\n\n        :param str key: Database name used to retrieve the database object.\n        :param str default: Default database name.  Defaults to None.\n        :param bool remote: Dictates whether the locally cached\n            database is returned or a remote request is made to retrieve\n            the database from the server.  Defaults to False.\n\n        :returns: Database object\n        \"\"\"\n        if not remote:\n            return super(CouchDB, self).get(key, default)\n        db = self._DATABASE_CLASS(self, key)\n        if db.exists():\n            super(CouchDB, self).__setitem__(key, db)\n            return db\n\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bill(self, year=None, month=None):\n        endpoint = '/'.join((self.server_url, '_api', 'v2', 'bill'))\n        return self._usage_endpoint(endpoint, year, month)", "response": "Retrieves Cloudant billing data optionally for a given year and month."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving Cloudant volume usage data optionally for a given year and month.", "response": "def volume_usage(self, year=None, month=None):\n        \"\"\"\n        Retrieves Cloudant volume usage data, optionally for a given\n        year and month.\n\n        :param int year: Year to query against, for example 2014.\n            Optional parameter.  Defaults to None.  If used, it must be\n            accompanied by ``month``.\n        :param int month: Month to query against that must be an integer\n            between 1 and 12.  Optional parameter.  Defaults to None.\n            If used, it must be accompanied by ``year``.\n\n        :returns: Volume usage data in JSON format\n        \"\"\"\n        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'usage', 'data_volume'))\n        return self._usage_endpoint(endpoint, year, month)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving Cloudant requests usage data optionally for a given year and month.", "response": "def requests_usage(self, year=None, month=None):\n        \"\"\"\n        Retrieves Cloudant requests usage data, optionally for a given\n        year and month.\n\n        :param int year: Year to query against, for example 2014.\n            Optional parameter.  Defaults to None.  If used, it must be\n            accompanied by ``month``.\n        :param int month: Month to query against that must be an integer\n            between 1 and 12.  Optional parameter.  Defaults to None.\n            If used, it must be accompanied by ``year``.\n\n        :returns: Requests usage data in JSON format\n        \"\"\"\n        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'usage', 'requests'))\n        return self._usage_endpoint(endpoint, year, month)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a list containing the names of databases shared with this account.", "response": "def shared_databases(self):\n        \"\"\"\n        Retrieves a list containing the names of databases shared\n        with this account.\n\n        :returns: List of database names\n        \"\"\"\n        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'user', 'shared_databases'))\n        resp = self.r_session.get(endpoint)\n        resp.raise_for_status()\n        data = response_to_json_dict(resp)\n        return data.get('shared_databases', [])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and returns a new API Key and pass pair.", "response": "def generate_api_key(self):\n        \"\"\"\n        Creates and returns a new API Key/pass pair.\n\n        :returns: API key/pass pair in JSON format\n        \"\"\"\n        endpoint = '/'.join((self.server_url, '_api', 'v2', 'api_keys'))\n        resp = self.r_session.post(endpoint)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cors_configuration(self):\n        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'user', 'config', 'cors'))\n        resp = self.r_session.get(endpoint)\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)", "response": "Retrieves the current CORS configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disable_cors(self):\n        return self.update_cors_configuration(\n            enable_cors=False,\n            allow_credentials=False,\n            origins=[],\n            overwrite_origins=True\n        )", "response": "Switches CORS off.\n\n        :returns: CORS status in JSON format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the CORS configuration of the current object with new values.", "response": "def update_cors_configuration(\n            self,\n            enable_cors=True,\n            allow_credentials=True,\n            origins=None,\n            overwrite_origins=False):\n        \"\"\"\n        Merges existing CORS configuration with updated values.\n\n        :param bool enable_cors: Enables/disables CORS.  Defaults to True.\n        :param bool allow_credentials: Allows authentication credentials.\n            Defaults to True.\n        :param list origins: List of allowed CORS origin(s).  Special cases are\n            a list containing a single \"*\" which will allow any origin and\n            an empty list which will not allow any origin.  Defaults to None.\n        :param bool overwrite_origins: Dictates whether the origins list is\n            overwritten of appended to.  Defaults to False.\n\n        :returns: CORS configuration update status in JSON format\n        \"\"\"\n        if origins is None:\n            origins = []\n\n        cors_config = {\n            'enable_cors': enable_cors,\n            'allow_credentials': allow_credentials,\n            'origins': origins\n        }\n\n        if overwrite_origins:\n            return self._write_cors_configuration(cors_config)\n\n        old_config = self.cors_configuration()\n\n        # update config values\n        updated_config = old_config.copy()\n\n        updated_config['enable_cors'] = cors_config.get('enable_cors')\n        updated_config['allow_credentials'] = cors_config.get('allow_credentials')\n\n        if cors_config.get('origins') == [\"*\"]:\n            updated_config['origins'] = [\"*\"]\n        elif old_config.get('origins') != cors_config.get('origins'):\n            new_origins = list(\n                set(old_config.get('origins')).union(\n                    set(cors_config.get('origins')))\n            )\n            updated_config['origins'] = new_origins\n\n        return self._write_cors_configuration(updated_config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_cors_configuration(self, config):\n        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'user', 'config', 'cors'))\n        resp = self.r_session.put(\n            endpoint,\n            data=json.dumps(config, cls=self.encoder),\n            headers={'Content-Type': 'application/json'}\n        )\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)", "response": "Overwrites the entire CORS config with the values updated in\n            update_cors_configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a Cloudant session using a VCAP_SERVICES environment variable.", "response": "def bluemix(cls, vcap_services, instance_name=None, service_name=None, **kwargs):\n        \"\"\"\n        Create a Cloudant session using a VCAP_SERVICES environment variable.\n\n        :param vcap_services: VCAP_SERVICES environment variable\n        :type vcap_services: dict or str\n        :param str instance_name: Optional Bluemix instance name. Only required\n            if multiple Cloudant instances are available.\n        :param str service_name: Optional Bluemix service name.\n\n        Example usage:\n\n        .. code-block:: python\n\n            import os\n            from cloudant.client import Cloudant\n\n            client = Cloudant.bluemix(os.getenv('VCAP_SERVICES'),\n                                      'Cloudant NoSQL DB')\n\n            print client.all_dbs()\n        \"\"\"\n        service_name = service_name or 'cloudantNoSQLDB'  # default service\n        try:\n            service = CloudFoundryService(vcap_services,\n                                          instance_name=instance_name,\n                                          service_name=service_name)\n        except CloudantException:\n            raise CloudantClientException(103)\n\n        if hasattr(service, 'iam_api_key'):\n            return Cloudant.iam(service.username,\n                                service.iam_api_key,\n                                url=service.url,\n                                **kwargs)\n        return Cloudant(service.username,\n                        service.password,\n                        url=service.url,\n                        **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a Cloudant client that uses IAM authentication.", "response": "def iam(cls, account_name, api_key, **kwargs):\n        \"\"\"\n        Create a Cloudant client that uses IAM authentication.\n\n        :param account_name: Cloudant account name.\n        :param api_key: IAM authentication API key.\n        \"\"\"\n        return cls(None,\n                   api_key,\n                   account=account_name,\n                   auto_renew=kwargs.get('auto_renew', True),\n                   use_iam=True,\n                   **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconstructing and returns the Query URL.", "response": "def url(self):\n        \"\"\"\n        Constructs and returns the Query URL.\n\n        :returns: Query URL\n        \"\"\"\n        if self._partition_key:\n            base_url = self._database.database_partition_url(\n                self._partition_key)\n        else:\n            base_url = self._database.database_url\n\n        return base_url + '/_find'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_doc(self, doc_id):\n        resp = self._r_session.get('/'.join([self._scheduler, 'docs', '_replicator', doc_id]))\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Get the replication document state for a given replication document ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_jobs(self, limit=None, skip=None):\n        params = dict()\n        if limit is not None:\n            params[\"limit\"] = limit\n        if skip is not None:\n            params[\"skip\"] = skip\n        resp = self._r_session.get('/'.join([self._scheduler, 'jobs']), params=params)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Lists all the jobs in the replication cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the design document partition URL.", "response": "def document_partition_url(self, partition_key):\n        \"\"\"\n        Retrieve the design document partition URL.\n\n        :param str partition_key: Partition key.\n        :return: Design document partition URL.\n        :rtype: str\n        \"\"\"\n        return '/'.join((\n            self._database.database_partition_url(partition_key),\n            '_design',\n            url_quote_plus(self['_id'][8:], safe='')\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_view(self, view_name, map_func, reduce_func=None, **kwargs):\n        if self.get_view(view_name) is not None:\n            raise CloudantArgumentError(107, view_name)\n        if self.get('language', None) == QUERY_LANGUAGE:\n            raise CloudantDesignDocumentException(101)\n\n        view = View(self, view_name, map_func, reduce_func, **kwargs)\n        self.views.__setitem__(view_name, view)", "response": "Adds a MapReduce view to the locally cached DesignDocument View\n        dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a show function to the locally cached DesignDocument shows dictionary.", "response": "def add_show_function(self, show_name, show_func):\n        \"\"\"\n        Appends a show function to the locally cached DesignDocument\n        shows dictionary.\n\n        :param show_name: Name used to identify the show function.\n        :param show_func: Javascript show function.\n        \"\"\"\n        if self.get_show_function(show_name) is not None:\n            raise CloudantArgumentError(110, show_name)\n\n        self.shows.__setitem__(show_name, show_func)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update_view(self, view_name, map_func, reduce_func=None, **kwargs):\n        view = self.get_view(view_name)\n        if view is None:\n            raise CloudantArgumentError(111, view_name)\n        if isinstance(view, QueryIndexView):\n            raise CloudantDesignDocumentException(102)\n\n        view = View(self, view_name, map_func, reduce_func, **kwargs)\n        self.views.__setitem__(view_name, view)", "response": "Updates an existing MapReduce view definition in the DesignDocument View dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_search_index(self, index_name, search_func, analyzer=None):\n        search = self.get_index(index_name)\n        if search is None:\n            raise CloudantArgumentError(112, index_name)\n        if analyzer is not None:\n            search = {'index': codify(search_func), 'analyzer': analyzer}\n        else:\n            search = {'index': codify(search_func)}\n\n        self.indexes.__setitem__(index_name, search)", "response": "Updates an existing Cloudant search index in the DesignDocument indexes dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates an existing Javascript list function in the DesignDocument indexes dictionary.", "response": "def update_list_function(self, list_name, list_func):\n        \"\"\"\n        Modifies/overwrites an existing list function in the\n        locally cached DesignDocument indexes dictionary.\n\n        :param str list_name: Name used to identify the list function.\n        :param str list_func: Javascript list function.\n        \"\"\"\n        if self.get_list_function(list_name) is None:\n            raise CloudantArgumentError(113, list_name)\n\n        self.lists.__setitem__(list_name, codify(list_func))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_show_function(self, show_name, show_func):\n        if self.get_show_function(show_name) is None:\n            raise CloudantArgumentError(114, show_name)\n\n        self.shows.__setitem__(show_name, show_func)", "response": "Updates the show function in the DesignDocument shows dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_view(self, view_name):\n        view = self.get_view(view_name)\n        if view is None:\n            return\n        if isinstance(view, QueryIndexView):\n            raise CloudantDesignDocumentException(103)\n\n        self.views.__delitem__(view_name)", "response": "Removes an existing MapReduce view definition from the locally cached DesignDocument View dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete_index(self, index_name):\n        index = self.get_index(index_name)\n        if index is None:\n            return\n\n        self.indexes.__delitem__(index_name)", "response": "Removes an existing index in the locally cached DesignDocument\n        indexes dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an existing show function in the locally cached DesignDocument shows dictionary.", "response": "def delete_show_function(self, show_name):\n        \"\"\"\n        Removes an existing show function in the locally cached DesignDocument\n        shows dictionary.\n\n        :param show_name: Name used to identify the list.\n        \"\"\"\n        if self.get_show_function(show_name) is None:\n            return\n\n        self.shows.__delitem__(show_name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the remote design document content and populates the locally - cached DesignDocument dictionary.", "response": "def fetch(self):\n        \"\"\"\n        Retrieves the remote design document content and populates the locally\n        cached DesignDocument dictionary.  View content is stored either as\n        View or QueryIndexView objects which are extensions of the ``dict``\n        type.  All other design document data are stored directly as\n        ``dict`` types.\n        \"\"\"\n        super(DesignDocument, self).fetch()\n        if self.views:\n            for view_name, view_def in iteritems_(self.get('views', dict())):\n                if self.get('language', None) != QUERY_LANGUAGE:\n                    self['views'][view_name] = View(\n                        self,\n                        view_name,\n                        view_def.pop('map', None),\n                        view_def.pop('reduce', None),\n                        **view_def\n                    )\n                else:\n                    self['views'][view_name] = QueryIndexView(\n                        self,\n                        view_name,\n                        view_def.pop('map', None),\n                        view_def.pop('reduce', None),\n                        **view_def\n                    )\n\n        for prop in self._nested_object_names:\n            # Ensure dict for each sub-object exists in locally cached DesignDocument.\n            getattr(self, prop, self.setdefault(prop, dict()))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self):\n        if self.views:\n            if self.get('language', None) != QUERY_LANGUAGE:\n                for view_name, view in self.iterviews():\n                    if isinstance(view, QueryIndexView):\n                        raise CloudantDesignDocumentException(104, view_name)\n            else:\n                for view_name, view in self.iterviews():\n                    if not isinstance(view, QueryIndexView):\n                        raise CloudantDesignDocumentException(105, view_name)\n\n        if self.indexes:\n            if self.get('language', None) != QUERY_LANGUAGE:\n                for index_name, search in self.iterindexes():\n                    # Check the instance of the javascript search function\n                    if not isinstance(search['index'], STRTYPE):\n                        raise CloudantDesignDocumentException(106, index_name)\n            else:\n                for index_name, index in self.iterindexes():\n                    if not isinstance(index['index'], dict):\n                        raise CloudantDesignDocumentException(107, index_name)\n\n        for prop in self._nested_object_names:\n            if not getattr(self, prop):\n                # Ensure empty dict for each sub-object is not saved remotely.\n                self.__delitem__(prop)\n\n        super(DesignDocument, self).save()\n\n        for prop in self._nested_object_names:\n            # Ensure views, indexes, and lists dict exist in locally cached DesignDocument.\n            getattr(self, prop, self.setdefault(prop, dict()))", "response": "Saves changes made to the locally cached DesignDocument object to the remote database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef info(self):\n        ddoc_info = self.r_session.get(\n            '/'.join([self.document_url, '_info']))\n        ddoc_info.raise_for_status()\n        return response_to_json_dict(ddoc_info)", "response": "Retrieves the design document view information data returns dictionary"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the search index information for a specified search index within the design document", "response": "def search_info(self, search_index):\n        \"\"\"\n        Retrieves information about a specified search index within the design\n        document, returns dictionary\n\n        GET databasename/_design/{ddoc}/_search_info/{search_index}\n        \"\"\"\n        ddoc_search_info = self.r_session.get(\n            '/'.join([self.document_url, '_search_info', search_index]))\n        ddoc_search_info.raise_for_status()\n        return response_to_json_dict(ddoc_search_info)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve disk size information about a specified search index within the design document, returns dictionary GET databasename/_design/{ddoc}/_search_disk_size/{search_index}", "response": "def search_disk_size(self, search_index):\n        \"\"\"\n        Retrieves disk size information about a specified search index within\n        the design document, returns dictionary\n\n        GET databasename/_design/{ddoc}/_search_disk_size/{search_index}\n        \"\"\"\n        ddoc_search_disk_size = self.r_session.get(\n            '/'.join([self.document_url, '_search_disk_size', search_index]))\n        ddoc_search_disk_size.raise_for_status()\n        return response_to_json_dict(ddoc_search_disk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary containing useful authentication information that can be used to authenticate against this database.", "response": "def creds(self):\n        \"\"\"\n        Retrieves a dictionary of useful authentication information\n        that can be used to authenticate against this database.\n\n        :returns: Dictionary containing authentication information\n        \"\"\"\n        session = self.client.session()\n        if session is None:\n            return None\n\n        return {\n            \"basic_auth\": self.client.basic_auth_str(),\n            \"user_ctx\": session.get('userCtx')\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the remote database exists.", "response": "def exists(self):\n        \"\"\"\n        Performs an existence check on the remote database.\n\n        :returns: Boolean True if the database exists, False otherwise\n        \"\"\"\n        resp = self.r_session.head(self.database_url)\n        if resp.status_code not in [200, 404]:\n            resp.raise_for_status()\n\n        return resp.status_code == 200"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef metadata(self):\n        resp = self.r_session.get(self.database_url)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Retrieves the remote database metadata dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves the metadata dictionary for the remote database partition.", "response": "def partition_metadata(self, partition_key):\n        \"\"\"\n        Retrieves the metadata dictionary for the remote database partition.\n\n        :param str partition_key: Partition key.\n        :returns: Metadata dictionary for the database partition.\n        :rtype: dict\n        \"\"\"\n        resp = self.r_session.get(self.database_partition_url(partition_key))\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_document(self, data, throw_on_exists=False):\n        docid = data.get('_id', None)\n        doc = None\n        if docid and docid.startswith('_design/'):\n            doc = DesignDocument(self, docid)\n        else:\n            doc = Document(self, docid)\n        doc.update(data)\n        try:\n            doc.create()\n        except HTTPError as error:\n            if error.response.status_code == 409:\n                if throw_on_exists:\n                    raise CloudantDatabaseException(409, docid)\n            else:\n                raise\n        super(CouchDatabase, self).__setitem__(doc['_id'], doc)\n        return doc", "response": "Creates a new document in the remote and locally cached database using the provided data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_document(self):\n        doc = Document(self, None)\n        doc.create()\n        super(CouchDatabase, self).__setitem__(doc['_id'], doc)\n        return doc", "response": "Creates a new empty document in the remote and locally cached database and auto - generating the _id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef design_documents(self):\n        url = '/'.join((self.database_url, '_all_docs'))\n        query = \"startkey=\\\"_design\\\"&endkey=\\\"_design0\\\"&include_docs=true\"\n        resp = self.r_session.get(url, params=query)\n        resp.raise_for_status()\n        data = response_to_json_dict(resp)\n        return data['rows']", "response": "Retrieves the JSON content for all design documents in this database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_design_document(self, ddoc_id):\n        ddoc = DesignDocument(self, ddoc_id)\n        try:\n            ddoc.fetch()\n        except HTTPError as error:\n            if error.response.status_code != 404:\n                raise\n\n        return ddoc", "response": "Retrieves a design document."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_partitioned_view_result(self, partition_key, ddoc_id, view_name,\n                                    raw_result=False, **kwargs):\n        \"\"\"\n        Retrieves the partitioned view result based on the design document and\n        view name.\n\n        See :func:`~cloudant.database.CouchDatabase.get_view_result` method for\n        further details.\n\n        :param str partition_key: Partition key.\n        :param str ddoc_id: Design document id used to get result.\n        :param str view_name: Name of the view used to get result.\n        :param bool raw_result: Dictates whether the view result is returned\n            as a default Result object or a raw JSON response.\n            Defaults to False.\n        :param kwargs: See\n            :func:`~cloudant.database.CouchDatabase.get_view_result` method for\n            available keyword arguments.\n        :returns: The result content either wrapped in a QueryResult or\n            as the raw response JSON content.\n        :rtype: QueryResult, dict\n        \"\"\"\n        ddoc = DesignDocument(self, ddoc_id)\n        view = View(ddoc, view_name, partition_key=partition_key)\n\n        return self._get_view_result(view, raw_result, **kwargs)", "response": "Retrieves the partitioned view result based on the design document and view name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the view result for a design document and view name.", "response": "def get_view_result(self, ddoc_id, view_name, raw_result=False, **kwargs):\n        \"\"\"\n        Retrieves the view result based on the design document and view name.\n        By default the result is returned as a\n        :class:`~cloudant.result.Result` object which provides a key\n        accessible, sliceable, and iterable interface to the result collection.\n        Depending on how you are accessing, slicing or iterating through your\n        result collection certain query parameters are not permitted.  See\n        :class:`~cloudant.result.Result` for additional details.\n\n        However, by setting ``raw_result=True``, the result will be returned as\n        the raw JSON response content for the view requested.  With this setting\n        there are no restrictions on the query parameters used but it also\n        means that the result collection key access, slicing, and iteration is\n        the responsibility of the developer.\n\n        For example:\n\n        .. code-block:: python\n\n            # get Result based on a design document view\n            result = db.get_view_result('_design/ddoc_id_001', 'view_001')\n\n            # get a customized Result based on a design document view\n            result = db.get_view_result('_design/ddoc_id_001', 'view_001',\n                include_docs=True, reduce=False)\n\n            # get raw response content based on a design document view\n            result = db.get_view_result('_design/ddoc_id_001', 'view_001',\n                raw_result=True)\n\n            # get customized raw response content for a design document view\n            db.get_view_result('_design/ddoc_id_001', 'view_001',\n                raw_result=True, include_docs=True, skip=100, limit=100)\n\n        For more detail on key access, slicing and iteration, refer to the\n        :class:`~cloudant.result.Result` documentation.\n\n        :param str ddoc_id: Design document id used to get result.\n        :param str view_name: Name of the view used to get result.\n        :param bool raw_result: Dictates whether the view result is returned\n            as a default Result object or a raw JSON response.\n            Defaults to False.\n        :param bool descending: Return documents in descending key order.\n        :param endkey: Stop returning records at this specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param str endkey_docid: Stop returning records when the specified\n            document id is reached.\n        :param bool group: Using the reduce function, group the results to a\n            group or single row.\n        :param group_level: Only applicable if the view uses complex keys: keys\n            that are lists. Groups reduce results for the specified number\n            of list fields.\n        :param bool include_docs: Include the full content of the documents.\n        :param bool inclusive_end: Include rows with the specified endkey.\n        :param key: Return only documents that match the specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param list keys: Return only documents that match the specified keys.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param int limit: Limit the number of returned documents to the\n            specified count.  Not valid when used with\n            :class:`~cloudant.result.Result` iteration.\n        :param int page_size: Sets the page size for result iteration.\n            Only valid if used with ``raw_result=False``.\n        :param bool reduce: True to use the reduce function, false otherwise.\n        :param int skip: Skip this number of rows from the start.\n            Not valid when used with :class:`~cloudant.result.Result` iteration.\n        :param bool stable: Whether or not the view results should be returned\n            from a \"stable\" set of shards.\n        :param str stale: Allow the results from a stale view to be used. This\n            makes the request return immediately, even if the view has not been\n            completely built yet. If this parameter is not given, a response is\n            returned only after the view has been built. Note that this\n            parameter is deprecated and the appropriate combination of `stable`\n            and `update` should be used instead.\n        :param startkey: Return records starting with the specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param str startkey_docid: Return records starting with the specified\n            document ID.\n        :param str update: Determine whether the view in question should be\n            updated prior to or after responding to the user. Valid values are:\n            false: return results before updating the view; true: Return results\n            after updating the view; lazy: Return the view results without\n            waiting for an update, but update them immediately after the request.\n\n        :returns: The result content either wrapped in a QueryResult or\n            as the raw response JSON content\n        \"\"\"\n        ddoc = DesignDocument(self, ddoc_id)\n        view = View(ddoc, view_name)\n\n        return self._get_view_result(view, raw_result, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_view_result(view, raw_result, **kwargs):\n        if raw_result:\n            return view(**kwargs)\n        if kwargs:\n            return Result(view, **kwargs)\n\n        return view.result", "response": "Get view results helper."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new database object.", "response": "def create(self, throw_on_exists=False):\n        \"\"\"\n        Creates a database defined by the current database object, if it\n        does not already exist and raises a CloudantException if the operation\n        fails.  If the database already exists then this method call is a no-op.\n\n        :param bool throw_on_exists: Boolean flag dictating whether or\n            not to throw a CloudantDatabaseException when attempting to\n            create a database that already exists.\n\n        :returns: The database object\n        \"\"\"\n        if not throw_on_exists and self.exists():\n            return self\n\n        resp = self.r_session.put(self.database_url, params={\n            'partitioned': TYPE_CONVERTERS.get(bool)(self._partitioned)\n        })\n        if resp.status_code == 201 or resp.status_code == 202:\n            return self\n\n        raise CloudantDatabaseException(\n            resp.status_code, self.database_url, resp.text\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the current database from the remote instance.", "response": "def delete(self):\n        \"\"\"\n        Deletes the current database from the remote instance.\n        \"\"\"\n        resp = self.r_session.delete(self.database_url)\n        resp.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef all_docs(self, **kwargs):\n        resp = get_docs(self.r_session,\n                        '/'.join([self.database_url, '_all_docs']),\n                        self.client.encoder,\n                        **kwargs)\n        return response_to_json_dict(resp)", "response": "This method wraps the _all_docs endpoint and returns the results as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping for the _all_docs primary index on the database partition and returns the results by value.", "response": "def partitioned_all_docs(self, partition_key, **kwargs):\n        \"\"\"\n        Wraps the _all_docs primary index on the database partition, and returns\n        the results by value.\n\n        See :func:`~cloudant.database.CouchDatabase.all_docs` method for further\n        details.\n\n        :param str partition_key: Partition key.\n        :param kwargs: See :func:`~cloudant.database.CouchDatabase.all_docs`\n            method for available keyword arguments.\n        :returns: Raw JSON response content from ``_all_docs`` endpoint.\n        :rtype: dict\n        \"\"\"\n        resp = get_docs(self.r_session,\n                        '/'.join([\n                            self.database_partition_url(partition_key),\n                            '_all_docs'\n                        ]),\n                        self.client.encoder,\n                        **kwargs)\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprovide a context manager that can be used to customize the ``_all_docs`` behavior and wrap the output as a :class:`~cloudant.result.Result`. :param bool descending: Return documents in descending key order. :param endkey: Stop returning records at this specified key. Not valid when used with :class:`~cloudant.result.Result` key access and key slicing. :param str endkey_docid: Stop returning records when the specified document id is reached. :param bool include_docs: Include the full content of the documents. :param bool inclusive_end: Include rows with the specified endkey. :param key: Return only documents that match the specified key. Not valid when used with :class:`~cloudant.result.Result` key access and key slicing. :param list keys: Return only documents that match the specified keys. Not valid when used with :class:`~cloudant.result.Result` key access and key slicing. :param int page_size: Sets the page size for result iteration. :param startkey: Return records starting with the specified key. Not valid when used with :class:`~cloudant.result.Result` key access and key slicing. :param str startkey_docid: Return records starting with the specified document ID. For example: .. code-block:: python with database.custom_result(include_docs=True) as rslt: data = rslt[100: 200]", "response": "def custom_result(self, **options):\n        \"\"\"\n        Provides a context manager that can be used to customize the\n        ``_all_docs`` behavior and wrap the output as a\n        :class:`~cloudant.result.Result`.\n\n        :param bool descending: Return documents in descending key order.\n        :param endkey: Stop returning records at this specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param str endkey_docid: Stop returning records when the specified\n            document id is reached.\n        :param bool include_docs: Include the full content of the documents.\n        :param bool inclusive_end: Include rows with the specified endkey.\n        :param key: Return only documents that match the specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param list keys: Return only documents that match the specified keys.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param int page_size: Sets the page size for result iteration.\n        :param startkey: Return records starting with the specified key.\n            Not valid when used with :class:`~cloudant.result.Result` key\n            access and key slicing.\n        :param str startkey_docid: Return records starting with the specified\n            document ID.\n\n        For example:\n\n        .. code-block:: python\n\n            with database.custom_result(include_docs=True) as rslt:\n                data = rslt[100: 200]\n        \"\"\"\n        rslt = Result(self.all_docs, **options)\n        yield rslt\n        del rslt"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the list of document ids in the database.", "response": "def keys(self, remote=False):\n        \"\"\"\n        Retrieves the list of document ids in the database.  Default is\n        to return only the locally cached document ids, specify remote=True\n        to make a remote request to include all document ids from the remote\n        database instance.\n\n        :param bool remote: Dictates whether the list of locally cached\n            document ids are returned or a remote request is made to include\n            an up to date list of document ids from the server.\n            Defaults to False.\n\n        :returns: List of document ids\n        \"\"\"\n        if not remote:\n            return list(super(CouchDatabase, self).keys())\n        docs = self.all_docs()\n        return [row['id'] for row in docs.get('rows', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform multiple documents inserts or updates through a single single request.", "response": "def bulk_docs(self, docs):\n        \"\"\"\n        Performs multiple document inserts and/or updates through a single\n        request.  Each document must either be or extend a dict as\n        is the case with Document and DesignDocument objects.  A document\n        must contain the ``_id`` and ``_rev`` fields if the document\n        is meant to be updated.\n\n        :param list docs: List of Documents to be created/updated.\n\n        :returns: Bulk document creation/update status in JSON format\n        \"\"\"\n        url = '/'.join((self.database_url, '_bulk_docs'))\n        data = {'docs': docs}\n        headers = {'Content-Type': 'application/json'}\n        resp = self.r_session.post(\n            url,\n            data=json.dumps(data, cls=self.client.encoder),\n            headers=headers\n        )\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef missing_revisions(self, doc_id, *revisions):\n        url = '/'.join((self.database_url, '_missing_revs'))\n        data = {doc_id: list(revisions)}\n\n        resp = self.r_session.post(\n            url,\n            headers={'Content-Type': 'application/json'},\n            data=json.dumps(data, cls=self.client.encoder)\n        )\n        resp.raise_for_status()\n\n        resp_json = response_to_json_dict(resp)\n        missing_revs = resp_json['missing_revs'].get(doc_id)\n        if missing_revs is None:\n            missing_revs = []\n\n        return missing_revs", "response": "Returns a list of document revision values that do not exist in the current remote database for the specified document id and list of revision values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef revisions_diff(self, doc_id, *revisions):\n        url = '/'.join((self.database_url, '_revs_diff'))\n        data = {doc_id: list(revisions)}\n\n        resp = self.r_session.post(\n            url,\n            headers={'Content-Type': 'application/json'},\n            data=json.dumps(data, cls=self.client.encoder)\n        )\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)", "response": "Returns the differences in the current remote database for the specified document id and list of revision values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_revision_limit(self):\n        url = '/'.join((self.database_url, '_revs_limit'))\n        resp = self.r_session.get(url)\n        resp.raise_for_status()\n\n        try:\n            ret = int(resp.text)\n        except ValueError:\n            raise CloudantDatabaseException(400, response_to_json_dict(resp))\n\n        return ret", "response": "Retrieves the limit of historical revisions to store for any single\n        document in the current remote database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_revision_limit(self, limit):\n        url = '/'.join((self.database_url, '_revs_limit'))\n\n        resp = self.r_session.put(url, data=json.dumps(limit, cls=self.client.encoder))\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)", "response": "Sets the limit of historical revisions for any single document\n            in the current remote database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove view files that are not used by any design document in the remote database.", "response": "def view_cleanup(self):\n        \"\"\"\n        Removes view files that are not used by any design document in the\n        remote database.\n\n        :returns: View cleanup status in JSON format\n        \"\"\"\n        url = '/'.join((self.database_url, '_view_cleanup'))\n        resp = self.r_session.post(\n            url,\n            headers={'Content-Type': 'application/json'}\n        )\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_list_function_result(self, ddoc_id, list_name, view_name, **kwargs):\n        ddoc = DesignDocument(self, ddoc_id)\n        headers = {'Content-Type': 'application/json'}\n        resp = get_docs(self.r_session,\n                        '/'.join([ddoc.document_url, '_list', list_name, view_name]),\n                        self.client.encoder,\n                        headers,\n                        **kwargs)\n        return resp.text", "response": "Returns a customized MapReduce view result from the specified design document id list name and view name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_show_function_result(self, ddoc_id, show_name, doc_id):\n        ddoc = DesignDocument(self, ddoc_id)\n        headers = {'Content-Type': 'application/json'}\n        resp = get_docs(self.r_session,\n                        '/'.join([ddoc.document_url, '_show', show_name, doc_id]),\n                        self.client.encoder,\n                        headers)\n        return resp.text", "response": "Retrieves a formatted document from the specified database based on the show function provided."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a design document based on the update handler function provided.", "response": "def update_handler_result(self, ddoc_id, handler_name, doc_id=None, data=None, **params):\n        \"\"\"\n        Creates or updates a document from the specified database based on the\n        update handler function provided.  Update handlers are used, for\n        example, to provide server-side modification timestamps, and document\n        updates to individual fields without the latest revision. You can\n        provide query parameters needed by the update handler function using\n        the ``params`` argument.\n\n        Create a document with a generated ID:\n\n        .. code-block:: python\n\n            # Assuming that 'update001' update handler exists as part of the\n            # 'ddoc001' design document in the remote database...\n            # Execute 'update001' to create a new document\n            resp = db.update_handler_result('ddoc001', 'update001', data={'name': 'John',\n                                            'message': 'hello'})\n\n        Create or update a document with the specified ID:\n\n        .. code-block:: python\n\n            # Assuming that 'update001' update handler exists as part of the\n            # 'ddoc001' design document in the remote database...\n            # Execute 'update001' to update document 'doc001' in the database\n            resp = db.update_handler_result('ddoc001', 'update001', 'doc001',\n                                            data={'month': 'July'})\n\n        For more details, see the `update handlers documentation\n        <https://console.bluemix.net/docs/services/Cloudant/api/design_documents.html#update-handlers>`_.\n\n        :param str ddoc_id: Design document id used to get result.\n        :param str handler_name: Name used in part to identify the\n            update handler function.\n        :param str doc_id: Optional document id used to specify the\n            document to be handled.\n\n        :returns: Result of update handler function in text format\n        \"\"\"\n        ddoc = DesignDocument(self, ddoc_id)\n        if doc_id:\n            resp = self.r_session.put(\n                '/'.join([ddoc.document_url, '_update', handler_name, doc_id]),\n                params=params, data=data)\n        else:\n            resp = self.r_session.post(\n                '/'.join([ddoc.document_url, '_update', handler_name]),\n                params=params, data=data)\n        resp.raise_for_status()\n        return resp.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_query_indexes(self, raw_result=False):\n\n        url = '/'.join((self.database_url, '_index'))\n        resp = self.r_session.get(url)\n        resp.raise_for_status()\n\n        if raw_result:\n            return response_to_json_dict(resp)\n\n        indexes = []\n        for data in response_to_json_dict(resp).get('indexes', []):\n            if data.get('type') == JSON_INDEX_TYPE:\n                indexes.append(Index(\n                    self,\n                    data.get('ddoc'),\n                    data.get('name'),\n                    partitioned=data.get('partitioned', False),\n                    **data.get('def', {})\n                ))\n            elif data.get('type') == TEXT_INDEX_TYPE:\n                indexes.append(TextIndex(\n                    self,\n                    data.get('ddoc'),\n                    data.get('name'),\n                    partitioned=data.get('partitioned', False),\n                    **data.get('def', {})\n                ))\n            elif data.get('type') == SPECIAL_INDEX_TYPE:\n                indexes.append(SpecialIndex(\n                    self,\n                    data.get('ddoc'),\n                    data.get('name'),\n                    partitioned=data.get('partitioned', False),\n                    **data.get('def', {})\n                ))\n            else:\n                raise CloudantDatabaseException(101, data.get('type'))\n        return indexes", "response": "Retrieves the query indexes from the remote database."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_query_index(\n            self,\n            design_document_id=None,\n            index_name=None,\n            index_type='json',\n            partitioned=False,\n            **kwargs\n    ):\n        \"\"\"\n        Creates either a JSON or a text query index in the remote database.\n\n        :param str index_type: The type of the index to create.  Can\n            be either 'text' or 'json'.  Defaults to 'json'.\n        :param str design_document_id: Optional identifier of the design\n            document in which the index will be created. If omitted the default\n            is that each index will be created in its own design document.\n            Indexes can be grouped into design documents for efficiency.\n            However, a change to one index in a design document will invalidate\n            all other indexes in the same document.\n        :param str index_name: Optional name of the index. If omitted, a name\n            will be generated automatically.\n        :param list fields: A list of fields that should be indexed.  For JSON\n            indexes, the fields parameter is mandatory and should follow the\n            'sort syntax'.  For example ``fields=['name', {'age': 'desc'}]``\n            will create an index on the 'name' field in ascending order and the\n            'age' field in descending order.  For text indexes, the fields\n            parameter is optional.  If it is included then each field element\n            in the fields list must be a single element dictionary where the\n            key is the field name and the value is the field type.  For example\n            ``fields=[{'name': 'string'}, {'age': 'number'}]``.  Valid field\n            types are ``'string'``, ``'number'``, and ``'boolean'``.\n        :param dict default_field: Optional parameter that specifies how the\n            ``$text`` operator can be used with the index.  Only valid when\n            creating a text index.\n        :param dict selector: Optional parameter that can be used to limit the\n            index to a specific set of documents that match a query. It uses\n            the same syntax used for selectors in queries.  Only valid when\n            creating a text index.\n\n        :returns: An Index object representing the index created in the\n            remote database\n        \"\"\"\n        if index_type == JSON_INDEX_TYPE:\n            index = Index(self, design_document_id, index_name,\n                          partitioned=partitioned, **kwargs)\n        elif index_type == TEXT_INDEX_TYPE:\n            index = TextIndex(self, design_document_id, index_name,\n                              partitioned=partitioned, **kwargs)\n        else:\n            raise CloudantArgumentError(103, index_type)\n        index.create()\n        return index", "response": "Creates either a JSON or text query index in the remote database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes the query index identified by the design document id index type and index name from the remote database.", "response": "def delete_query_index(self, design_document_id, index_type, index_name):\n        \"\"\"\n        Deletes the query index identified by the design document id,\n        index type and index name from the remote database.\n\n        :param str design_document_id: The design document id that the index\n            exists in.\n        :param str index_type: The type of the index to be deleted.  Must\n            be either 'text' or 'json'.\n        :param str index_name: The index name of the index to be deleted.\n        \"\"\"\n        if index_type == JSON_INDEX_TYPE:\n            index = Index(self, design_document_id, index_name)\n        elif index_type == TEXT_INDEX_TYPE:\n            index = TextIndex(self, design_document_id, index_name)\n        else:\n            raise CloudantArgumentError(103, index_type)\n        index.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_partitioned_query_result(self, partition_key, selector, fields=None,\n                                     raw_result=False, **kwargs):\n        \"\"\"\n        Retrieves the partitioned query result from the specified database based\n        on the query parameters provided.\n\n        See :func:`~cloudant.database.CouchDatabase.get_query_result` method for\n        further details.\n\n        :param str partition_key: Partition key.\n        :param str selector: Dictionary object describing criteria used to\n            select documents.\n        :param list fields: A list of fields to be returned by the query.\n        :param bool raw_result: Dictates whether the query result is returned\n            wrapped in a QueryResult or if the response JSON is returned.\n            Defaults to False.\n        :param kwargs: See\n            :func:`~cloudant.database.CouchDatabase.get_query_result` method for\n            available keyword arguments.\n        :returns: The result content either wrapped in a QueryResult or\n            as the raw response JSON content.\n        :rtype: QueryResult, dict\n        \"\"\"\n        query = Query(self,\n                      selector=selector,\n                      fields=fields,\n                      partition_key=partition_key)\n\n        return self._get_query_result(query, raw_result, **kwargs)", "response": "Returns the partitioned query result from the specified database based on the query parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_query_result(self, selector, fields=None, raw_result=False,\n                         **kwargs):\n        \"\"\"\n        Retrieves the query result from the specified database based on the\n        query parameters provided.  By default the result is returned as a\n        :class:`~cloudant.result.QueryResult` which uses the ``skip`` and\n        ``limit`` query parameters internally to handle slicing and iteration\n        through the query result collection.  Therefore ``skip`` and ``limit``\n        cannot be used as arguments to get the query result when\n        ``raw_result=False``.  However, by setting ``raw_result=True``, the\n        result will be returned as the raw JSON response content for the query\n        requested.  Using this setting requires the developer to manage their\n        own slicing and iteration.  Therefore ``skip`` and ``limit`` are valid\n        arguments in this instance.\n\n        For example:\n\n        .. code-block:: python\n\n            # Retrieve documents where the name field is 'foo'\n            selector = {'name': {'$eq': 'foo'}}\n            docs = db.get_query_result(selector)\n            for doc in docs:\n                print doc\n\n            # Retrieve documents sorted by the age field in ascending order\n            docs = db.get_query_result(selector, sort=['name'])\n            for doc in docs:\n                print doc\n\n            # Retrieve JSON response content, limiting response to 100 documents\n            resp = db.get_query_result(selector, raw_result=True, limit=100)\n            for doc in resp['docs']:\n                print doc\n\n        For more detail on slicing and iteration, refer to the\n        :class:`~cloudant.result.QueryResult` documentation.\n\n        :param dict selector: Dictionary object describing criteria used to\n            select documents.\n        :param list fields: A list of fields to be returned by the query.\n        :param bool raw_result: Dictates whether the query result is returned\n            wrapped in a QueryResult or if the response JSON is returned.\n            Defaults to False.\n        :param str bookmark: A string that enables you to specify which page of\n            results you require. Only valid for queries using indexes of type\n            *text*.\n        :param int limit: Maximum number of results returned.  Only valid if\n            used with ``raw_result=True``.\n        :param int page_size: Sets the page size for result iteration.  Default\n            is 100.  Only valid with ``raw_result=False``.\n        :param int r: Read quorum needed for the result.  Each document is read\n            from at least 'r' number of replicas before it is returned in the\n            results.\n        :param int skip: Skip the first 'n' results, where 'n' is the value\n            specified.  Only valid if used with ``raw_result=True``.\n        :param list sort: A list of fields to sort by.  Optionally the list can\n            contain elements that are single member dictionary structures that\n            specify sort direction.  For example\n            ``sort=['name', {'age': 'desc'}]`` means to sort the query results\n            by the \"name\" field in ascending order and the \"age\" field in\n            descending order.\n        :param str use_index: Identifies a specific index for the query to run\n            against, rather than using the Cloudant Query algorithm which finds\n            what it believes to be the best index.\n\n        :returns: The result content either wrapped in a QueryResult or\n            as the raw response JSON content\n        \"\"\"\n        query = Query(self,\n                      selector=selector,\n                      fields=fields)\n\n        return self._get_query_result(query, raw_result, **kwargs)", "response": "Retrieves the query result from the specified database based on the query parameters provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets query results helper.", "response": "def _get_query_result(query, raw_result, **kwargs):\n        \"\"\" Get query results helper. \"\"\"\n        if raw_result:\n            return query(**kwargs)\n        if kwargs:\n            return QueryResult(query, **kwargs)\n\n        return query.result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsharing the current remote database with the given username.", "response": "def share_database(self, username, roles=None):\n        \"\"\"\n        Shares the current remote database with the username provided.\n        You can grant varying degrees of access rights,\n        default is to share read-only, but additional\n        roles can be added by providing the specific roles as a\n        ``list`` argument.  If the user already has this database shared with\n        them then it will modify/overwrite the existing permissions.\n\n        :param str username: Cloudant user to share the database with.\n        :param list roles: A list of\n            `roles\n            <https://console.bluemix.net/docs/services/Cloudant/api/authorization.html#roles>`_\n            to grant to the named user.\n\n        :returns: Share database status in JSON format\n        \"\"\"\n        if roles is None:\n            roles = ['_reader']\n        valid_roles = [\n            '_reader',\n            '_writer',\n            '_admin',\n            '_replicator',\n            '_db_updates',\n            '_design',\n            '_shards',\n            '_security'\n        ]\n        doc = self.security_document()\n        data = doc.get('cloudant', {})\n        perms = []\n        if all(role in valid_roles for role in roles):\n            perms = list(set(roles))\n\n        if not perms:\n            raise CloudantArgumentError(102, roles, valid_roles)\n\n        data[username] = perms\n        doc['cloudant'] = data\n        resp = self.r_session.put(\n            self.security_url,\n            data=json.dumps(doc, cls=self.client.encoder),\n            headers={'Content-Type': 'application/json'}\n        )\n        resp.raise_for_status()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unshare_database(self, username):\n        doc = self.security_document()\n        data = doc.get('cloudant', {})\n        if username in data:\n            del data[username]\n        doc['cloudant'] = data\n        resp = self.r_session.put(\n            self.security_url,\n            data=json.dumps(doc, cls=self.client.encoder),\n            headers={'Content-Type': 'application/json'}\n        )\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Unshare the database with the named user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shards(self):\n        url = '/'.join((self.database_url, '_shards'))\n        resp = self.r_session.get(url)\n        resp.raise_for_status()\n\n        return response_to_json_dict(resp)", "response": "Retrieves information about the shards in the current remote database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves the raw JSON content from the remote database based on the partitioned search index on the server, using the query_params provided as query parameters. See :func:`~cloudant.database.CouchDatabase.get_search_result` method for further details. :param str partition_key: Partition key. :param str ddoc_id: Design document id used to get the search result. :param str index_name: Name used in part to identify the index. :param query_params: See :func:`~cloudant.database.CloudantDatabase.get_search_result` method for available keyword arguments. :returns: Search query result data in JSON format. :rtype: dict", "response": "def get_partitioned_search_result(self, partition_key, ddoc_id, index_name,\n                                      **query_params):\n        \"\"\"\n        Retrieves the raw JSON content from the remote database based on the\n        partitioned search index on the server, using the query_params provided\n        as query parameters.\n\n        See :func:`~cloudant.database.CouchDatabase.get_search_result` method\n        for further details.\n\n        :param str partition_key: Partition key.\n        :param str ddoc_id: Design document id used to get the search result.\n        :param str index_name: Name used in part to identify the index.\n        :param query_params: See\n            :func:`~cloudant.database.CloudantDatabase.get_search_result` method\n            for available keyword arguments.\n        :returns: Search query result data in JSON format.\n        :rtype: dict\n        \"\"\"\n        ddoc = DesignDocument(self, ddoc_id)\n\n        return self._get_search_result(\n            '/'.join((\n                ddoc.document_partition_url(partition_key),\n                '_search',\n                index_name\n            )),\n            **query_params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the raw JSON content from the remote database based on the search index on the server, using the query_params provided as query parameters. A ``query`` parameter containing the Lucene query syntax is mandatory. Example for search queries: .. code-block:: python # Assuming that 'searchindex001' exists as part of the # 'ddoc001' design document in the remote database... # Retrieve documents where the Lucene field name is 'name' and # the value is 'julia*' resp = db.get_search_result('ddoc001', 'searchindex001', query='name:julia*', include_docs=True) for row in resp['rows']: # Process search index data (in JSON format). Example if the search query requires grouping by using the ``group_field`` parameter: .. code-block:: python # Assuming that 'searchindex001' exists as part of the # 'ddoc001' design document in the remote database... # Retrieve JSON response content, limiting response to 10 documents resp = db.get_search_result('ddoc001', 'searchindex001', query='name:julia*', group_field='name', limit=10) for group in resp['groups']: for row in group['rows']: # Process search index data (in JSON format). :param str ddoc_id: Design document id used to get the search result. :param str index_name: Name used in part to identify the index. :param str bookmark: Optional string that enables you to specify which page of results you require. Only valid for queries that do not specify the ``group_field`` query parameter. :param list counts: Optional JSON array of field names for which counts should be produced. The response will contain counts for each unique value of this field name among the documents matching the search query. Requires the index to have faceting enabled. :param list drilldown: Optional list of fields that each define a pair of a field name and a value. This field can be used several times. The search will only match documents that have the given value in the field name. It differs from using ``query=fieldname:value`` only in that the values are not analyzed. :param str group_field: Optional string field by which to group search matches. Fields containing other data (numbers, objects, arrays) can not be used. :param int group_limit: Optional number with the maximum group count. This field can only be used if ``group_field`` query parameter is specified. :param group_sort: Optional JSON field that defines the order of the groups in a search using ``group_field``. The default sort order is relevance. This field can have the same values as the sort field, so single fields as well as arrays of fields are supported. :param int limit: Optional number to limit the maximum count of the returned documents. In case of a grouped search, this parameter limits the number of documents per group. :param query/q: A Lucene query in the form of ``name:value``. If name is omitted, the special value ``default`` is used. The ``query`` parameter can be abbreviated as ``q``. :param ranges: Optional JSON facet syntax that reuses the standard Lucene syntax to return counts of results which fit into each specified category. Inclusive range queries are denoted by brackets. Exclusive range queries are denoted by curly brackets. For example ``ranges={\"price\":{\"cheap\":\"[0 TO 100]\"}}`` has an inclusive range of 0 to 100. Requires the index to have faceting enabled. :param sort: Optional JSON string of the form ``fieldname<type>`` for ascending or ``-fieldname<type>`` for descending sort order. Fieldname is the name of a string or number field and type is either number or string or a JSON array of such strings. The type part is optional and defaults to number. :param str stale: Optional string to allow the results from a stale index to be used. This makes the request return immediately, even if the index has not been completely built yet. :param list highlight_fields: Optional list of fields which should be highlighted. :param str highlight_pre_tag: Optional string inserted before the highlighted word in the highlights output. Defaults to ``<em>``. :param str highlight_post_tag: Optional string inserted after the highlighted word in the highlights output. Defaults to ``</em>``. :param int highlight_number: Optional number of fragments returned in highlights. If the search term occurs less often than the number of fragments specified, longer fragments are returned. Default is 1. :param int highlight_size: Optional number of characters in each fragment for highlights. Defaults to 100 characters. :param list include_fields: Optional list of field names to include in search results. Any fields included must have been indexed with the ``store:true`` option. :returns: Search query result data in JSON format", "response": "def get_search_result(self, ddoc_id, index_name, **query_params):\n        \"\"\"\n        Retrieves the raw JSON content from the remote database based on the\n        search index on the server, using the query_params provided as query\n        parameters. A ``query`` parameter containing the Lucene query\n        syntax is mandatory.\n\n        Example for search queries:\n\n        .. code-block:: python\n\n            # Assuming that 'searchindex001' exists as part of the\n            # 'ddoc001' design document in the remote database...\n            # Retrieve documents where the Lucene field name is 'name' and\n            # the value is 'julia*'\n            resp = db.get_search_result('ddoc001', 'searchindex001',\n                                        query='name:julia*',\n                                        include_docs=True)\n            for row in resp['rows']:\n                # Process search index data (in JSON format).\n\n        Example if the search query requires grouping by using\n        the ``group_field`` parameter:\n\n        .. code-block:: python\n\n            # Assuming that 'searchindex001' exists as part of the\n            # 'ddoc001' design document in the remote database...\n            # Retrieve JSON response content, limiting response to 10 documents\n            resp = db.get_search_result('ddoc001', 'searchindex001',\n                                        query='name:julia*',\n                                        group_field='name',\n                                        limit=10)\n            for group in resp['groups']:\n                for row in group['rows']:\n                # Process search index data (in JSON format).\n\n        :param str ddoc_id: Design document id used to get the search result.\n        :param str index_name: Name used in part to identify the index.\n        :param str bookmark: Optional string that enables you to specify which\n            page of results you require. Only valid for queries that do not\n            specify the ``group_field`` query parameter.\n        :param list counts: Optional JSON array of field names for which\n            counts should be produced. The response will contain counts for each\n            unique value of this field name among the documents matching the\n            search query.\n            Requires the index to have faceting enabled.\n        :param list drilldown:  Optional list of fields that each define a\n            pair of a field name and a value. This field can be used several\n            times.  The search will only match documents that have the given\n            value in the field name. It differs from using\n            ``query=fieldname:value`` only in that the values are not analyzed.\n        :param str group_field: Optional string field by which to group\n            search matches.  Fields containing other data\n            (numbers, objects, arrays) can not be used.\n        :param int group_limit: Optional number with the maximum group count.\n            This field can only be used if ``group_field`` query parameter\n            is specified.\n        :param group_sort: Optional JSON field that defines the order of the\n            groups in a search using ``group_field``. The default sort order\n            is relevance. This field can have the same values as the sort field,\n            so single fields as well as arrays of fields are supported.\n        :param int limit: Optional number to limit the maximum count of the\n            returned documents. In case of a grouped search, this parameter\n            limits the number of documents per group.\n        :param query/q: A Lucene query in the form of ``name:value``.\n            If name is omitted, the special value ``default`` is used.\n            The ``query`` parameter can be abbreviated as ``q``.\n        :param ranges: Optional JSON facet syntax that reuses the standard\n            Lucene syntax to return counts of results which fit into each\n            specified category. Inclusive range queries are denoted by brackets.\n            Exclusive range queries are denoted by curly brackets.\n            For example ``ranges={\"price\":{\"cheap\":\"[0 TO 100]\"}}`` has an\n            inclusive range of 0 to 100.\n            Requires the index to have faceting enabled.\n        :param sort: Optional JSON string of the form ``fieldname<type>`` for\n            ascending or ``-fieldname<type>`` for descending sort order.\n            Fieldname is the name of a string or number field and type is either\n            number or string or a JSON array of such strings. The type part is\n            optional and defaults to number.\n        :param str stale: Optional string to allow the results from a stale\n            index to be used. This makes the request return immediately, even\n            if the index has not been completely built yet.\n        :param list highlight_fields: Optional list of fields which should be\n            highlighted.\n        :param str highlight_pre_tag: Optional string inserted before the\n            highlighted word in the highlights output.  Defaults to ``<em>``.\n        :param str highlight_post_tag: Optional string inserted after the\n            highlighted word in the highlights output.  Defaults to ``</em>``.\n        :param int highlight_number: Optional number of fragments returned in\n            highlights. If the search term occurs less often than the number of\n            fragments specified, longer fragments are returned.  Default is 1.\n        :param int highlight_size: Optional number of characters in each\n            fragment for highlights.  Defaults to 100 characters.\n        :param list include_fields: Optional list of field names to include in\n            search results. Any fields included must have been indexed with the\n            ``store:true`` option.\n\n        :returns: Search query result data in JSON format\n        \"\"\"\n        ddoc = DesignDocument(self, ddoc_id)\n        return self._get_search_result(\n            '/'.join((ddoc.document_url, '_search', index_name)),\n            **query_params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_search_result(self, query_url, **query_params):\n        param_q = query_params.get('q')\n        param_query = query_params.get('query')\n        # Either q or query parameter is required\n        if bool(param_q) == bool(param_query):\n            raise CloudantArgumentError(104, query_params)\n\n        # Validate query arguments and values\n        for key, val in iteritems_(query_params):\n            if key not in list(SEARCH_INDEX_ARGS.keys()):\n                raise CloudantArgumentError(105, key)\n            if not isinstance(val, SEARCH_INDEX_ARGS[key]):\n                raise CloudantArgumentError(106, key, SEARCH_INDEX_ARGS[key])\n        # Execute query search\n        headers = {'Content-Type': 'application/json'}\n        resp = self.r_session.post(\n            query_url,\n            headers=headers,\n            data=json.dumps(query_params, cls=self.client.encoder)\n        )\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Get search results helper."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_replication(self, source_db=None, target_db=None,\n                           repl_id=None, **kwargs):\n        \"\"\"\n        Creates a new replication task.\n\n        :param source_db: Database object to replicate from.  Can be either a\n            ``CouchDatabase`` or ``CloudantDatabase`` instance.\n        :param target_db: Database object to replicate to.  Can be either a\n            ``CouchDatabase`` or ``CloudantDatabase`` instance.\n        :param str repl_id: Optional replication id.  Generated internally if\n            not explicitly set.\n        :param dict user_ctx: Optional user to act as.  Composed internally\n            if not explicitly set.\n        :param bool create_target: Specifies whether or not to\n            create the target, if it does not already exist.\n        :param bool continuous: If set to True then the replication will be\n            continuous.\n\n        :returns: Replication document as a Document instance\n        \"\"\"\n        if source_db is None:\n            raise CloudantReplicatorException(101)\n\n        if target_db is None:\n            raise CloudantReplicatorException(102)\n\n        data = dict(\n            _id=repl_id if repl_id else str(uuid.uuid4()),\n            **kwargs\n        )\n\n        # replication source\n\n        data['source'] = {'url': source_db.database_url}\n        if source_db.admin_party:\n            pass  # no credentials required\n        elif source_db.client.is_iam_authenticated:\n            data['source'].update({'auth': {\n                'iam': {'api_key': source_db.client.r_session.get_api_key}\n            }})\n        else:\n            data['source'].update({'headers': {\n                'Authorization': source_db.creds['basic_auth']\n            }})\n\n        # replication target\n\n        data['target'] = {'url': target_db.database_url}\n        if target_db.admin_party:\n            pass  # no credentials required\n        elif target_db.client.is_iam_authenticated:\n            data['target'].update({'auth': {\n                'iam': {'api_key': target_db.client.r_session.get_api_key}\n            }})\n        else:\n            data['target'].update({'headers': {\n                'Authorization': target_db.creds['basic_auth']\n            }})\n\n        # add user context delegation\n\n        if not data.get('user_ctx') and self.database.creds and \\\n            self.database.creds.get('user_ctx'):\n            data['user_ctx'] = self.database.creds['user_ctx']\n\n        return self.database.create_document(data, throw_on_exists=True)", "response": "Creates a new replication task."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_replications(self):\n        docs = self.database.all_docs(include_docs=True)['rows']\n        documents = []\n        for doc in docs:\n            if doc['id'].startswith('_design/'):\n                continue\n            document = Document(self.database, doc['id'])\n            document.update(doc['doc'])\n            documents.append(document)\n        return documents", "response": "Returns a list of all replication documents in the replication database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the state of a given replication. Possible values are triggered completed error and None.", "response": "def replication_state(self, repl_id):\n        \"\"\"\n        Retrieves the state for the given replication. Possible values are\n        ``triggered``, ``completed``, ``error``, and ``None`` (meaning not yet\n        triggered).\n\n        :param str repl_id: Replication id used to identify the replication to\n            inspect.\n\n        :returns: Replication state as a ``str``\n        \"\"\"\n        if \"scheduler\" in self.client.features():\n            try:\n                repl_doc = Scheduler(self.client).get_doc(repl_id)\n            except HTTPError as err:\n                raise CloudantReplicatorException(err.response.status_code, repl_id)\n            state = repl_doc['state']\n        else:\n            try:\n                repl_doc = self.database[repl_id]\n            except KeyError:\n                raise CloudantReplicatorException(404, repl_id)\n            repl_doc.fetch()\n            state = repl_doc.get('_replication_state')\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef follow_replication(self, repl_id):\n        def update_state():\n            \"\"\"\n            Retrieves the replication state.\n            \"\"\"\n            if \"scheduler\" in self.client.features():\n                try:\n                    arepl_doc = Scheduler(self.client).get_doc(repl_id)\n                    return arepl_doc, arepl_doc['state']\n                except HTTPError:\n                    return None, None\n            else:\n                try:\n                    arepl_doc = self.database[repl_id]\n                    arepl_doc.fetch()\n                    return arepl_doc, arepl_doc.get('_replication_state')\n                except KeyError:\n                    return None, None\n\n        while True:\n            # Make sure we fetch the state up front, just in case it moves\n            # too fast and we miss it in the changes feed.\n            repl_doc, state = update_state()\n            if repl_doc:\n                yield repl_doc\n            if state is not None and state in ['error', 'completed']:\n                return\n\n            # Now listen on changes feed for the state\n            for change in self.database.changes():\n                if change.get('id') == repl_id:\n                    repl_doc, state = update_state()\n                    if repl_doc is not None:\n                        yield repl_doc\n                    if state is not None and state in ['error', 'completed']:\n                        return", "response": "Follows a given replication and returns a generator of documents and their replication state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping a replication based on the provided replication id.", "response": "def stop_replication(self, repl_id):\n        \"\"\"\n        Stops a replication based on the provided replication id by deleting\n        the replication document from the replication database.  The\n        replication can only be stopped if it has not yet completed.  If it has\n        already completed then the replication document is still deleted from\n        replication database.\n\n        :param str repl_id: Replication id used to identify the replication to\n            stop.\n        \"\"\"\n\n        try:\n            repl_doc = self.database[repl_id]\n        except KeyError:\n            raise CloudantReplicatorException(404, repl_id)\n\n        repl_doc.fetch()\n        repl_doc.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef base64_user_pass(self):\n        if self._username is None or self._password is None:\n            return None\n\n        hash_ = base64.urlsafe_b64encode(bytes_(\"{username}:{password}\".format(\n            username=self._username,\n            password=self._password\n        )))\n        return \"Basic {0}\".format(unicode_(hash_))", "response": "Returns a base64 encoded http authentication string suitable for use with the\n            database and other places that need it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding requests. Session. request to set the timeout.", "response": "def request(self, method, url, **kwargs):\n        \"\"\"\n        Overrides ``requests.Session.request`` to set the timeout.\n        \"\"\"\n        resp = super(ClientSession, self).request(\n            method, url, timeout=self._timeout, **kwargs)\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef info(self):\n        if self._session_url is None:\n            return None\n\n        resp = self.get(self._session_url)\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "response": "Get the current session information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset a new username and password.", "response": "def set_credentials(self, username, password):\n        \"\"\"\n        Set a new username and password.\n\n        :param str username: New username.\n        :param str password: New password.\n        \"\"\"\n        if username is not None:\n            self._username = username\n\n        if password is not None:\n            self._password = password"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noverride requests. Session. request to provide basic access to the object.", "response": "def request(self, method, url, **kwargs):\n        \"\"\"\n        Overrides ``requests.Session.request`` to provide basic access\n        authentication.\n        \"\"\"\n        auth = None\n        if self._username is not None and self._password is not None:\n            auth = (self._username, self._password)\n\n        return super(BasicSession, self).request(\n            method, url, auth=auth, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform cookie based user login.", "response": "def login(self):\n        \"\"\"\n        Perform cookie based user login.\n        \"\"\"\n        resp = super(CookieSession, self).request(\n            'POST',\n            self._session_url,\n            data={'name': self._username, 'password': self._password},\n        )\n        resp.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request(self, method, url, **kwargs):\n        resp = super(CookieSession, self).request(method, url, **kwargs)\n\n        if not self._auto_renew:\n            return resp\n\n        is_expired = any((\n            resp.status_code == 403 and\n            response_to_json_dict(resp).get('error') == 'credentials_expired',\n            resp.status_code == 401\n        ))\n\n        if is_expired:\n            self.login()\n            resp = super(CookieSession, self).request(method, url, **kwargs)\n\n        return resp", "response": "Override requests. Session. request to renew the cookie and retry the original request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform IAM cookie based user login.", "response": "def login(self):\n        \"\"\"\n        Perform IAM cookie based user login.\n        \"\"\"\n        access_token = self._get_access_token()\n        try:\n            super(IAMSession, self).request(\n                'POST',\n                self._session_url,\n                headers={'Content-Type': 'application/json'},\n                data=json.dumps({'access_token': access_token})\n            ).raise_for_status()\n\n        except RequestException:\n            raise CloudantException(\n                'Failed to exchange IAM token with Cloudant')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef request(self, method, url, **kwargs):\n        # The CookieJar API prevents callers from getting an individual Cookie\n        # object by name.\n        # We are forced to use the only exposed method of discarding expired\n        # cookies from the CookieJar. Internally this involves iterating over\n        # the entire CookieJar and calling `.is_expired()` on each Cookie\n        # object.\n        self.cookies.clear_expired_cookies()\n\n        if self._auto_renew and 'IAMSession' not in self.cookies.keys():\n            self.login()\n\n        resp = super(IAMSession, self).request(method, url, **kwargs)\n\n        if not self._auto_renew:\n            return resp\n\n        if resp.status_code == 401:\n            self.login()\n            resp = super(IAMSession, self).request(method, url, **kwargs)\n\n        return resp", "response": "Override requests. Session. request to renew the IAM cookie and retry the original request if required."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget IAM access token using API key.", "response": "def _get_access_token(self):\n        \"\"\"\n        Get IAM access token using API key.\n        \"\"\"\n        err = 'Failed to contact IAM token service'\n        try:\n            resp = super(IAMSession, self).request(\n                'POST',\n                self._token_url,\n                auth=self._token_auth,\n                headers={'Accepts': 'application/json'},\n                data={\n                    'grant_type': 'urn:ibm:params:oauth:grant-type:apikey',\n                    'response_type': 'cloud_iam',\n                    'apikey': self._api_key\n                }\n            )\n            err = response_to_json_dict(resp).get('errorMessage', err)\n            resp.raise_for_status()\n\n            return response_to_json_dict(resp)['access_token']\n\n        except KeyError:\n            raise CloudantException('Invalid response from IAM token service')\n\n        except RequestException:\n            raise CloudantException(err)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs and returns the document URL.", "response": "def document_url(self):\n        \"\"\"\n        Constructs and returns the document URL.\n\n        :returns: Document URL\n        \"\"\"\n        if '_id' not in self or self['_id'] is None:\n            return None\n\n        # handle design document url\n        if self['_id'].startswith('_design/'):\n            return '/'.join((\n                self._database_host,\n                url_quote_plus(self._database_name),\n                '_design',\n                url_quote(self['_id'][8:], safe='')\n            ))\n\n        # handle document url\n        return '/'.join((\n            self._database_host,\n            url_quote_plus(self._database_name),\n            url_quote(self['_id'], safe='')\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exists(self):\n        if '_id' not in self or self['_id'] is None:\n            return False\n\n        resp = self.r_session.head(self.document_url)\n        if resp.status_code not in [200, 404]:\n            resp.raise_for_status()\n\n        return resp.status_code == 200", "response": "Returns True if the document exists in the remote database False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self):\n\n        # Ensure that an existing document will not be \"updated\"\n        doc = dict(self)\n        if doc.get('_rev') is not None:\n            doc.__delitem__('_rev')\n\n        headers = {'Content-Type': 'application/json'}\n        resp = self.r_session.post(\n            self._database.database_url,\n            headers=headers,\n            data=json.dumps(doc, cls=self.encoder)\n        )\n        resp.raise_for_status()\n        data = response_to_json_dict(resp)\n        super(Document, self).__setitem__('_id', data['id'])\n        super(Document, self).__setitem__('_rev', data['rev'])", "response": "Creates the locally cached Document object and updates the locally cached Document object with the _id and _rev fields set to the locally cached Document object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch(self):\n        if self.document_url is None:\n            raise CloudantDocumentException(101)\n        resp = self.r_session.get(self.document_url)\n        resp.raise_for_status()\n        self.clear()\n        self.update(response_to_json_dict(resp, cls=self.decoder))", "response": "Fetches the content of the current document from the remote database and populates the locally cached Document object with the content of that document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self):\n        headers = {}\n        headers.setdefault('Content-Type', 'application/json')\n        if not self.exists():\n            self.create()\n            return\n        put_resp = self.r_session.put(\n            self.document_url,\n            data=self.json(),\n            headers=headers\n        )\n        put_resp.raise_for_status()\n        data = response_to_json_dict(put_resp)\n        super(Document, self).__setitem__('_rev', data['rev'])\n        return", "response": "Saves the locally cached Document object to the remote database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_field_append(doc, field, value):\n        if doc.get(field) is None:\n            doc[field] = []\n        if not isinstance(doc[field], list):\n            raise CloudantDocumentException(102, field)\n        if value is not None:\n            doc[field].append(value)", "response": "Appends a value to a locally cached Document object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nremove a value from a locally cached Document s list field.", "response": "def list_field_remove(doc, field, value):\n        \"\"\"\n        Removes a value from a list field in a locally cached Document object.\n\n        :param Document doc: Locally cached Document object that can be a\n            Document, DesignDocument or dict.\n        :param str field: Name of the field list to remove from.\n        :param value: Value to remove from the field list.\n        \"\"\"\n        if not isinstance(doc[field], list):\n            raise CloudantDocumentException(102, field)\n        doc[field].remove(value)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef field_set(doc, field, value):\n        if value is None:\n            doc.__delitem__(field)\n        else:\n            doc[field] = value", "response": "Sets or replaces a value for a locally cached Document object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a field in the remote database.", "response": "def update_field(self, action, field, value, max_tries=10):\n        \"\"\"\n        Updates a field in the remote document. If a conflict exists,\n        the document is re-fetched from the remote database and the update\n        is retried.  This is performed up to ``max_tries`` number of times.\n\n        Use this method when you want to update a single field in a document,\n        and don't want to risk clobbering other people's changes to\n        the document in other fields, but also don't want the caller\n        to implement logic to deal with conflicts.\n\n        For example:\n\n        .. code-block:: python\n\n            # Append the string 'foo' to the 'words' list of Document doc.\n            doc.update_field(\n                action=doc.list_field_append,\n                field='words',\n                value='foo'\n            )\n\n        :param callable action: A routine that takes a Document object,\n            a field name, and a value. The routine should attempt to\n            update a field in the locally cached Document object with the\n            given value, using whatever logic is appropriate.\n            Valid actions are\n            :func:`~cloudant.document.Document.list_field_append`,\n            :func:`~cloudant.document.Document.list_field_remove`,\n            :func:`~cloudant.document.Document.field_set`\n        :param str field: Name of the field to update\n        :param value: Value to update the field with\n        :param int max_tries: In the case of a conflict, the number of retries\n            to attempt\n        \"\"\"\n        self._update_field(action, field, value, max_tries)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete(self):\n        if not self.get(\"_rev\"):\n            raise CloudantDocumentException(103)\n\n        del_resp = self.r_session.delete(\n            self.document_url,\n            params={\"rev\": self[\"_rev\"]},\n        )\n        del_resp.raise_for_status()\n        _id = self['_id']\n        self.clear()\n        self['_id'] = _id", "response": "Removes the document from the remote database and clears the content of the locally cached Document object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_attachment(\n            self,\n            attachment,\n            headers=None,\n            write_to=None,\n            attachment_type=None):\n        \"\"\"\n        Retrieves a document's attachment and optionally writes it to a file.\n        If the content_type of the attachment is 'application/json' then the\n        data returned will be in JSON format otherwise the response content will\n        be returned as text or binary.\n\n        :param str attachment: Attachment file name used to identify the\n            attachment.\n        :param dict headers: Optional, additional headers to be sent\n            with request.\n        :param file write_to: Optional file handler to write the attachment to.\n            The write_to file must be opened for writing prior to including it\n            as an argument for this method.\n        :param str attachment_type: Optional setting to define how to handle the\n            attachment when returning its contents from this method.  Valid\n            values are ``'text'``, ``'json'``, and ``'binary'``  If\n            omitted then the returned content will be based on the\n            response Content-Type.\n\n        :returns: The attachment content\n        \"\"\"\n        # need latest rev\n        self.fetch()\n        attachment_url = '/'.join((self.document_url, attachment))\n        if headers is None:\n            headers = {'If-Match': self['_rev']}\n        else:\n            headers['If-Match'] = self['_rev']\n\n        resp = self.r_session.get(attachment_url, headers=headers)\n        resp.raise_for_status()\n\n        if attachment_type is None:\n            if resp.headers['Content-Type'].startswith('text/'):\n                attachment_type = 'text'\n            elif resp.headers['Content-Type'] == 'application/json':\n                attachment_type = 'json'\n            else:\n                attachment_type = 'binary'\n\n        if write_to is not None:\n            if attachment_type in ('text', 'json'):\n                write_to.write(resp.text)\n            else:\n                write_to.write(resp.content)\n        if attachment_type == 'text':\n            return resp.text\n        if attachment_type == 'json':\n            return response_to_json_dict(resp)\n\n        return resp.content", "response": "Retrieves a document s attachment and optionally writes it to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves an attachment from a remote document and refreshes the locally cached document object.", "response": "def delete_attachment(self, attachment, headers=None):\n        \"\"\"\n        Removes an attachment from a remote document and refreshes the locally\n        cached document object.\n\n        :param str attachment: Attachment file name used to identify the\n            attachment.\n        :param dict headers: Optional, additional headers to be sent\n            with request.\n\n        :returns: Attachment deletion status in JSON format\n        \"\"\"\n        # need latest rev\n        self.fetch()\n        attachment_url = '/'.join((self.document_url, attachment))\n        if headers is None:\n            headers = {'If-Match': self['_rev']}\n        else:\n            headers['If-Match'] = self['_rev']\n\n        resp = self.r_session.delete(\n            attachment_url,\n            headers=headers\n        )\n        resp.raise_for_status()\n        super(Document, self).__setitem__('_rev', response_to_json_dict(resp)['rev'])\n        # Execute logic only if attachment metadata exists locally\n        if self.get('_attachments'):\n            # Remove the attachment metadata for the specified attachment\n            if self['_attachments'].get(attachment):\n                self['_attachments'].__delitem__(attachment)\n            # Remove empty attachment metadata from the local dictionary\n            if not self['_attachments']:\n                super(Document, self).__delitem__('_attachments')\n\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a new attachment to the locally cached version of the document.", "response": "def put_attachment(self, attachment, content_type, data, headers=None):\n        \"\"\"\n        Adds a new attachment, or updates an existing attachment, to\n        the remote document and refreshes the locally cached\n        Document object accordingly.\n\n        :param attachment: Attachment file name used to identify the\n            attachment.\n        :param content_type: The http ``Content-Type`` of the attachment used\n            as an additional header.\n        :param data: Attachment data defining the attachment content.\n        :param headers: Optional, additional headers to be sent\n            with request.\n\n        :returns: Attachment addition/update status in JSON format\n        \"\"\"\n        # need latest rev\n        self.fetch()\n        attachment_url = '/'.join((self.document_url, attachment))\n        if headers is None:\n            headers = {\n                'If-Match': self['_rev'],\n                'Content-Type': content_type\n            }\n        else:\n            headers['If-Match'] = self['_rev']\n            headers['Content-Type'] = content_type\n\n        resp = self.r_session.put(\n            attachment_url,\n            data=data,\n            headers=headers\n        )\n        resp.raise_for_status()\n        self.fetch()\n        return response_to_json_dict(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef python_to_couch(options):\n    translation = dict()\n    for key, val in iteritems_(options):\n        py_to_couch_validate(key, val)\n        translation.update(_py_to_couch_translate(key, val))\n    return translation", "response": "Translate query options from python style parameters into CouchDB query parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef py_to_couch_validate(key, val):\n    if key not in RESULT_ARG_TYPES:\n        raise CloudantArgumentError(116, key)\n    # pylint: disable=unidiomatic-typecheck\n    # Validate argument values and ensure that a boolean is not passed in\n    # if an integer is expected\n    if (not isinstance(val, RESULT_ARG_TYPES[key]) or\n            (type(val) is bool and int in RESULT_ARG_TYPES[key])):\n        raise CloudantArgumentError(117, key, RESULT_ARG_TYPES[key])\n    if key == 'keys':\n        for key_list_val in val:\n            if (not isinstance(key_list_val, RESULT_ARG_TYPES['key']) or\n                    type(key_list_val) is bool):\n                raise CloudantArgumentError(134, RESULT_ARG_TYPES['key'])\n    if key == 'stale':\n        if val not in ('ok', 'update_after'):\n            raise CloudantArgumentError(135, val)", "response": "Validate the parameter key and value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _py_to_couch_translate(key, val):\n    try:\n        if key in ['keys', 'endkey_docid', 'startkey_docid', 'stale', 'update']:\n            return {key: val}\n        if val is None:\n            return {key: None}\n        arg_converter = TYPE_CONVERTERS.get(type(val))\n        return {key: arg_converter(val)}\n    except Exception as ex:\n        raise CloudantArgumentError(136, key, ex)", "response": "Converts a Python parameter value to its CouchDB\n    equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_docs(r_session, url, encoder=None, headers=None, **params):\n    keys_list = params.pop('keys', None)\n    keys = None\n    if keys_list is not None:\n        keys = json.dumps({'keys': keys_list}, cls=encoder)\n    f_params = python_to_couch(params)\n    resp = None\n    if keys is not None:\n        # If we're using POST we are sending JSON so add the header\n        if headers is None:\n            headers = {}\n        headers['Content-Type'] = 'application/json'\n        resp = r_session.post(url, headers=headers, params=f_params, data=keys)\n    else:\n        resp = r_session.get(url, headers=headers, params=f_params)\n    resp.raise_for_status()\n    return resp", "response": "Provides a helper for functions that require GET POST or text requests with a JSON response containing documents."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_response_error_content(response, **kwargs):\n    if response.status_code >= 400:\n        try:\n            resp_dict = response_to_json_dict(response)\n            error = resp_dict.get('error', '')\n            reason = resp_dict.get('reason', '')\n            # Append to the existing response's reason\n            response.reason += ' {0} {1}'.format(error, reason)\n        except ValueError:\n            pass\n    return response", "response": "Adds a HTTP response error with reason message to the response. reason."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cloudant(user, passwd, **kwargs):\n    cloudant_session = Cloudant(user, passwd, **kwargs)\n    cloudant_session.connect()\n    yield cloudant_session\n    cloudant_session.disconnect()", "response": "Context manager for Cloudant."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprovides a context manager to create a Cloudant session using IAM authentication and provide access to databases, docs etc. :param account_name: Cloudant account name. :param api_key: IAM authentication API key. For example: .. code-block:: python # cloudant context manager from cloudant import cloudant_iam with cloudant_iam(ACCOUNT_NAME, API_KEY) as client: # Context handles connect() and disconnect() for you. # Perform library operations within this context. Such as: print client.all_dbs() # ...", "response": "def cloudant_iam(account_name, api_key, **kwargs):\n    \"\"\"\n    Provides a context manager to create a Cloudant session using IAM\n    authentication and provide access to databases, docs etc.\n\n    :param account_name: Cloudant account name.\n    :param api_key: IAM authentication API key.\n\n    For example:\n\n    .. code-block:: python\n\n        # cloudant context manager\n        from cloudant import cloudant_iam\n\n        with cloudant_iam(ACCOUNT_NAME, API_KEY) as client:\n            # Context handles connect() and disconnect() for you.\n            # Perform library operations within this context.  Such as:\n            print client.all_dbs()\n            # ...\n\n    \"\"\"\n    cloudant_session = Cloudant.iam(account_name, api_key, **kwargs)\n\n    cloudant_session.connect()\n    yield cloudant_session\n    cloudant_session.disconnect()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cloudant_bluemix(vcap_services, instance_name=None, service_name=None, **kwargs):\n    cloudant_session = Cloudant.bluemix(\n        vcap_services,\n        instance_name=instance_name,\n        service_name=service_name,\n        **kwargs\n    )\n    cloudant_session.connect()\n    yield cloudant_session\n    cloudant_session.disconnect()", "response": "Creates a context manager that can be used to create a Cloudant instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef couchdb(user, passwd, **kwargs):\n    couchdb_session = CouchDB(user, passwd, **kwargs)\n    couchdb_session.connect()\n    yield couchdb_session\n    couchdb_session.disconnect()", "response": "Context manager for CouchDB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef couchdb_admin_party(**kwargs):\n    couchdb_session = CouchDB(None, None, True, **kwargs)\n    couchdb_session.connect()\n    yield couchdb_session\n    couchdb_session.disconnect()", "response": "A context manager that provides a context manager for CouchDB admin Party mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles processing when the result argument provided is an integer.", "response": "def _handle_result_by_index(self, idx):\n        \"\"\"\n        Handle processing when the result argument provided is an integer.\n        \"\"\"\n        if idx < 0:\n            return None\n        opts = dict(self.options)\n        skip = opts.pop('skip', 0)\n        limit = opts.pop('limit', None)\n        py_to_couch_validate('skip', skip)\n        py_to_couch_validate('limit', limit)\n        if limit is not None and idx >= limit:\n            # Result is out of range\n            return dict()\n        return self._ref(skip=skip+idx, limit=1, **opts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles processing when the result argument provided is a document key.", "response": "def _handle_result_by_key(self, key):\n        \"\"\"\n        Handle processing when the result argument provided is a document key.\n        \"\"\"\n        invalid_options = ('key', 'keys', 'startkey', 'endkey')\n        if any(x in invalid_options for x in self.options):\n            raise ResultException(102, invalid_options, self.options)\n        return self._ref(key=key, **self.options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_result_by_idx_slice(self, idx_slice):\n        opts = dict(self.options)\n        skip = opts.pop('skip', 0)\n        limit = opts.pop('limit', None)\n        py_to_couch_validate('skip', skip)\n        py_to_couch_validate('limit', limit)\n        start = idx_slice.start\n        stop = idx_slice.stop\n        data = None\n        # start and stop cannot be None and both must be greater than 0\n        if all(i is not None and i >= 0 for i in [start, stop]) and start < stop:\n            if limit is not None:\n                if start >= limit:\n                    # Result is out of range\n                    return dict()\n                if stop > limit:\n                    # Ensure that slice does not extend past original limit\n                    return self._ref(skip=skip+start, limit=limit-start, **opts)\n            data = self._ref(skip=skip+start, limit=stop-start, **opts)\n        elif start is not None and stop is None and start >= 0:\n            if limit is not None:\n                if start >= limit:\n                    # Result is out of range\n                    return dict()\n                # Ensure that slice does not extend past original limit\n                data = self._ref(skip=skip+start, limit=limit-start, **opts)\n            else:\n                data = self._ref(skip=skip+start, **opts)\n        elif start is None and stop is not None and stop >= 0:\n            if limit is not None and stop > limit:\n                # Ensure that slice does not extend past original limit\n                data = self._ref(skip=skip, limit=limit, **opts)\n            else:\n                data = self._ref(skip=skip, limit=stop, **opts)\n        return data", "response": "Handle processing when the result argument provided is an index slice."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nhandle processing when the result argument provided is a key slice.", "response": "def _handle_result_by_key_slice(self, key_slice):\n        \"\"\"\n        Handle processing when the result argument provided is a key slice.\n        \"\"\"\n        invalid_options = ('key', 'keys', 'startkey', 'endkey')\n        if any(x in invalid_options for x in self.options):\n            raise ResultException(102, invalid_options, self.options)\n\n        if isinstance(key_slice.start, ResultByKey):\n            start = key_slice.start()\n        else:\n            start = key_slice.start\n\n        if isinstance(key_slice.stop, ResultByKey):\n            stop = key_slice.stop()\n        else:\n            stop = key_slice.stop\n\n        if (start is not None and stop is not None and\n                isinstance(start, type(stop))):\n            data = self._ref(startkey=start, endkey=stop, **self.options)\n        elif start is not None and stop is None:\n            data = self._ref(startkey=start, **self.options)\n        elif start is None and stop is not None:\n            data = self._ref(endkey=stop, **self.options)\n        else:\n            data = None\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\niterate through view data.", "response": "def _iterator(self, response):\n        '''\n        Iterate through view data.\n        '''\n\n        while True:\n            result = deque(self._parse_data(response))\n            del response\n            if result:\n                doc_count = len(result)\n                last = result.pop()\n                while result:\n                    yield result.popleft()\n\n                # We expect doc_count = self._page_size + 1 results, if\n                # we have self._page_size or less it means we are on the\n                # last page and need to return the last result.\n                if doc_count < self._real_page_size:\n                    yield last\n                    break\n                del result\n\n                # if we are in a view, keys could be duplicate so we\n                # need to start from the right docid\n                if last['id']:\n                    response = self._call(startkey=last['key'],\n                                          startkey_docid=last['id'])\n                # reduce result keys are unique by definition\n                else:\n                    response = self._call(startkey=last['key'])\n\n            else:\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _iterator(self, response):\n        '''\n        Iterate through query data.\n        '''\n\n        while True:\n            result = self._parse_data(response)\n            bookmark = response.get('bookmark')\n            if result:\n                for row in result:\n                    yield row\n\n                del result\n\n                if not bookmark:\n                    break\n\n                response = self._call(bookmark=bookmark)\n\n            else:\n                break", "response": "Iterate through query data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs and returns the View URL.", "response": "def url(self):\n        \"\"\"\n        Constructs and returns the View URL.\n\n        :returns: View URL\n        \"\"\"\n        if self._partition_key:\n            base_url = self.design_doc.document_partition_url(\n                self._partition_key)\n        else:\n            base_url = self.design_doc.document_url\n\n        return '/'.join((\n            base_url,\n            '_view',\n            self.view_name\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_a_dict(self):\n        index_dict = {\n            'ddoc': self._ddoc_id,\n            'name': self._name,\n            'type': self._type,\n            'def': self._def\n        }\n\n        if self._partitioned:\n            index_dict['partitioned'] = True\n\n        return index_dict", "response": "Displays the index as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create(self):\n        payload = {'type': self._type}\n        if self._ddoc_id and self._ddoc_id != '':\n            if isinstance(self._ddoc_id, STRTYPE):\n                if self._ddoc_id.startswith('_design/'):\n                    payload['ddoc'] = self._ddoc_id[8:]\n                else:\n                    payload['ddoc'] = self._ddoc_id\n            else:\n                raise CloudantArgumentError(122, self._ddoc_id)\n        if self._name and self._name != '':\n            if isinstance(self._name, STRTYPE):\n                payload['name'] = self._name\n            else:\n                raise CloudantArgumentError(123, self._name)\n        self._def_check()\n        payload['index'] = self._def\n\n        if self._partitioned:\n            payload['partitioned'] = True\n\n        headers = {'Content-Type': 'application/json'}\n        resp = self._r_session.post(\n            self.index_url,\n            data=json.dumps(payload, cls=self._database.client.encoder),\n            headers=headers\n        )\n        resp.raise_for_status()\n        self._ddoc_id = response_to_json_dict(resp)['id']\n        self._name = response_to_json_dict(resp)['name']", "response": "Creates the current index in the remote database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves the current index from the remote database.", "response": "def delete(self):\n        \"\"\"\n        Removes the current index from the remote database.\n        \"\"\"\n        if not self._ddoc_id:\n            raise CloudantArgumentError(125)\n        if not self._name:\n            raise CloudantArgumentError(126)\n        ddoc_id = self._ddoc_id\n        if ddoc_id.startswith('_design/'):\n            ddoc_id = ddoc_id[8:]\n        url = '/'.join((self.index_url, ddoc_id, self._type, self._name))\n        resp = self._r_session.delete(url)\n        resp.raise_for_status()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _def_check(self):\n        if self._def != dict():\n            for key, val in iteritems_(self._def):\n                if key not in list(TEXT_INDEX_ARGS.keys()):\n                    raise CloudantArgumentError(127, key)\n                if not isinstance(val, TEXT_INDEX_ARGS[key]):\n                    raise CloudantArgumentError(128, key, TEXT_INDEX_ARGS[key])", "response": "Checks that the definition contains only valid arguments for a\n        text index."}
